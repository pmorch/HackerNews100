<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 08 Jan 2024 13:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Visualizing Ext4 (235 pts)]]></title>
            <link>https://buredoranna.github.io/linux/ext4/2020/01/09/ext4-viz.html</link>
            <guid>38907821</guid>
            <pubDate>Mon, 08 Jan 2024 03:13:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buredoranna.github.io/linux/ext4/2020/01/09/ext4-viz.html">https://buredoranna.github.io/linux/ext4/2020/01/09/ext4-viz.html</a>, See on <a href="https://news.ycombinator.com/item?id=38907821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<center><h2>What does ext4 look like?</h2></center>


<div><p>That is... if I start with a blank drive, a drive made completely of 0x00s, and then do mkfs.ext4, what does the drive, now embossed with ext4, look like?</p></div>

<div><p>I mean, what I wanted to see, is what it takes to transmogrify a bunch 0x00s, from "nothing" into the purposeful assemblage of bytes that is an ext4 filesystem.</p></div>

<div><p>At first I figured I’d try visualizing a live drive, like /dev/sda... but quickly figured 'dd' + 'live drive' could get me into trouble, so opted for adding a small secondary drive to my VM.</p></div>

<div><p>Then I thought, even with a virtual machine, working with 'dd' and /dev/sdX would be more trouble than it was worth. I then remembered I didn’t have to use drives at all, virtual or otherwise, I could just work with a regular file, configured as a loop device.</p></div>

<div><p>And it turns out mount/umount have evolved since I last experimented with loop devices... you don’t have to 'losetup' the loop device anymore just a simple:</p></div>

<pre># mount -o loop &lt;foo_file&gt; &lt;bar_dir&gt;
# umount &lt;bar_dir&gt;
</pre>



<div><p>Using a loop device simplified my efforts, and diminished the likelihood of a<br><a href="https://askubuntu.com/questions/982552/accidentally-did-dd-dev-sda">dd accident</a>.</p></div>

<div><p>So a little about what we’re looking at.</p></div>

<p>I always start with a blank file... that is a file created with 'dd' and a source of '/dev/zero'... the calculated size of the file correspond to a final image with eight blocks, each 64-pixels/bytes high, and 1024-pixels/bytes wide.<br></p>

<pre>$ dd if=/dev/zero of=blockfile.ext4 bs=$((64 * 1024)) count=8
</pre>

<p>The output of this is predictable<br></p>
<pre>$ od -x -A x blockfile.ext4
000000 0000 0000 0000 0000 0000 0000 0000 0000
*
080000
</pre>

<div><p>But I wanted to see the difference between a zero file, and one with whatever structure mkfs.ext4 adds to the drive...</p></div>

<div><p>Of note: the size of drive I’m working with is too small for a journal... but thats okay... Doing a visualization which includes a journal I’m leaving for a future project.</p></div>

<p>So now the output of 'od' of the blockfile which mkfs.ext4 is run against, is a little more interesting... here we begin to see structure:<br></p>

<pre>$ od -x -Ax blockfile.ext4
000000 0000 0000 0000 0000 0000 0000 0000 0000
*
000400 0040 0000 0200 0000 0019 0000 01e2 0000
000410 0035 0000 0001 0000 0000 0000 0000 0000
000420 2000 0000 2000 0000 0040 0000 0000 0000
000430 c737 5e10 0000 ffff ef53 0001 0001 0000
000440 c737 5e10 0000 0000 0000 0000 0001 0000
000450 0000 0000 000b 0000 0080 0000 0038 0000
000460 02c2 0000 046b 0000 927f 9037 d060 5e4c
000470 1b83 287a 7389 0001 0000 0000 0000 0000
000480 0000 0000 0000 0000 0000 0000 0000 0000
*
0004c0 0000 0000 0000 0000 0000 0000 0000 0003
0004d0 0000 0000 0000 0000 0000 0000 0000 0000
0004e0 0000 0000 0000 0000 0000 0000 7da5 5d6c
0004f0 72c1 5b42 719d b2ee 63d5 d142 0001 0040
000500 000c 0000 0000 0000 c737 5e10 0000 0000
000510 0000 0000 0000 0000 0000 0000 0000 0000
*
000560 0001 0000 0000 0000 0000 0000 0000 0000
000570 0000 0000 0104 0000 0015 0000 0000 0000
000580 0000 0000 0000 0000 0000 0000 0000 0000
*
0007f0 0000 0000 0000 0000 0000 0000 7d3f 9ace
000800 0006 0000 0016 0000 0026 0000 01e2 0035
000810 0002 0004 0000 0000 c571 4e0a 0035 4797
000820 0000 0000 0000 0000 0000 0000 0000 0000
000830 0000 0000 0000 0000 96a2 c509 0000 0000
000840 0000 0000 0000 0000 0000 0000 0000 0000
*
001800 ffff 002f 1fe0 0000 0000 0000 0000 0000
001810 0000 0000 0000 0000 0000 0000 0000 0000
*
001830 0000 0000 0000 0000 0000 0000 0000 8000
001840 ffff ffff ffff ffff ffff ffff ffff ffff
*
001c00 0002 0000 000c 0201 002e 0000 0002 0000
001c10 000c 0202 2e2e 0000 000b 0000 03dc 020a
001c20 6f6c 7473 662b 756f 646e 0000 0000 0000
001c30 0000 0000 0000 0000 0000 0000 0000 0000
*
001ff0 0000 0000 0000 0000 000c de00 e669 11f0
002000 000b 0000 000c 0201 002e 0000 0002 0000
002010 03e8 0202 2e2e 0000 0000 0000 0000 0000
002020 0000 0000 0000 0000 0000 0000 0000 0000
*
0023f0 0000 0000 0000 0000 000c de00 0f7a 7b5d
002400 0000 0000 03f4 0000 0000 0000 0000 0000
002410 0000 0000 0000 0000 0000 0000 0000 0000
*
0027f0 0000 0000 0000 0000 000c de00 3e04 8f88
002800 0000 0000 03f4 0000 0000 0000 0000 0000
002810 0000 0000 0000 0000 0000 0000 0000 0000
*
002bf0 0000 0000 0000 0000 000c de00 3e04 8f88
002c00 0000 0000 03f4 0000 0000 0000 0000 0000
002c10 0000 0000 0000 0000 0000 0000 0000 0000
*
002ff0 0000 0000 0000 0000 000c de00 3e04 8f88
003000 0000 0000 03f4 0000 0000 0000 0000 0000
003010 0000 0000 0000 0000 0000 0000 0000 0000
*
0033f0 0000 0000 0000 0000 000c de00 3e04 8f88
003400 0000 0000 03f4 0000 0000 0000 0000 0000
003410 0000 0000 0000 0000 0000 0000 0000 0000
*
[... snip ...]
</pre>

<div><p>But at the byte density provided by the output of 'od', trying to visualize the ext4 structure is like trying to visualize the structure of three deciduous forests by examining the leaves of a single tree... I wanted a picture which would let me "zoom out", giving me a better idea of what I was looking at...</p></div>

<div><p>So I came up with this... each blue block is 1024 pixels wide, and 64 pixels high... each pixel represents a single byte... Nothing much to see here, except a drive made entirely of 0x00s.</p></div>

<p><img src="https://buredoranna.github.io/assets/images/blockfile.nulls.ext4.png" alt="I miss you, image =("></p>

<div><p>It starts to get interesting after creating the ext4 filesystem, and see this...</p></div>

<p><img src="https://buredoranna.github.io/assets/images/blockfile.nouserdata.ext4.png" alt="I miss you, image"></p>

<div><p>With this image we can can see the structure added by mkfs.ext4, and where on the drive the ext4 data is located.</p></div>

<p>Its worth noting this image doesn’t actually differentiate between "ext4 bytes" and "non-ext4 bytes". That is, there could be bytes owned by ext4, but if they are 0x00s they are color coded the same as any other 0x00... But even with this limitation, the image is interesting.
<br></p>

<div><p>But I still wanted an image which differentiated between ext4 data and "user" data. My solution was to create a file 1024 bytes in size from /dev/urandom, and copy that file to the mounted loop device. Then, in my visualization code, when reading the blockfile, I test if "the next 1024 bytes to be read" match "the 1024 bytes of the reference file", and if they match, color code those 1024 pixels accordingly.</p></div>

<div><p>And with user data copied to the drive, we get this:</p></div>

<p><img src="https://buredoranna.github.io/assets/images/blockfile.ext4.120.png" alt="I miss you, image"></p>

<div><p>Which I find very satisfying... But still, I wanted an animation. So I built an animated GIF.</p></div>

<div><p>Between each frame, the "user data" file is copied to the drive three times... so there are three copies written each frame... This makes for a more expressive animation and a smaller GIF than if each frame was a single 'cp' of the file.</p></div>

<div><p>I hope you enjoy this as much as I do.</p></div>



<div><p>And by way of comparison, here is a similar animation, but with ext2</p></div>


<hr>

<p><br>Here begins the ext4 rabbit hole...<br>
<a href="https://en.wikipedia.org/wiki/Ext4">Wikipedia</a>
<a href="https://ext4.wiki.kernel.org/index.php/Main_Page">ext4 wiki</a><a></a>
<a href="https://www.kernel.org/doc/html/latest/admin-guide/ext4.html">Admin Guide</a>
<a href="http://e2fsprogs.sourceforge.net/">e2fsprogs</a>
<a href="https://www.kernel.org/doc/html/latest/filesystems/ext4/index.html">ext4 Data Structures and Algorithms</a></p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database of 16,000 Artists Used to Train Midjourney AI Goes Viral (141 pts)]]></title>
            <link>https://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/</link>
            <guid>38907729</guid>
            <pubDate>Mon, 08 Jan 2024 02:57:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/">https://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/</a>, See on <a href="https://news.ycombinator.com/item?id=38907729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-1234691955">
		<header>
			<div>
	<nav data-dropdown="">

	
	<ul data-dropdown-list="">
					<li data-dropdown-list-item="">
				<span>
	<a href="https://www.artnews.com/">

	Home
	</a>
</span>
			</li>
					<li data-dropdown-list-item="">
				<span>
	<a href="https://www.artnews.com/c/art-news/">

	ARTnews
	</a>
</span>
			</li>
					<li data-dropdown-list-item="">
				<span>
	<a href="https://www.artnews.com/c/art-news/news/">

	News
	</a>
</span>
			</li>
			</ul>
</nav>
</div>

		</header>

		

		

		<div>

			<article>
				<div>
	

<figure>

			
<div>
	
			<p><img src="https://www.artnews.com/wp-content/uploads/2024/01/4f6e27fd85d4a899633e2228d076124f_1920_KR-wide.jpg?w=2000" alt="Metalwork Colossus by Hyan Tran, age 6, for a special drop of Magic the Gathering benefitting the Seattle Children's Hospital in 2021." srcset="https://www.artnews.com/wp-content/uploads/2024/01/4f6e27fd85d4a899633e2228d076124f_1920_KR-wide.jpg 2800w, https://www.artnews.com/wp-content/uploads/2024/01/4f6e27fd85d4a899633e2228d076124f_1920_KR-wide.jpg?resize=400,183 400w" sizes="(min-width: 87.5rem) 1000px, (min-width: 78.75rem) 681px, (min-width: 48rem) 450px, (max-width: 48rem) 250px" height="" width="" decoding="async">
			
			</p>
	
	</div>
	
			
			
<figcaption>
	
					<span>A database shows this image by Hyan Tran, age 6, was also scraped by Midjourney for its AI image generator.</span>
		
									<cite>©2023 Scalefast Inc.
Courtesy of Magic the Gathering Secret Lair</cite>
					
	</figcaption>

			
</figure>

</div>

				<div>
					<p>
	For many, a new year includes resolutions to do better and build better habits. For <a href="https://www.artnews.com/t/midjourney/" id="auto-tag_midjourney" data-tag="midjourney">Midjourney</a>, the start of 2024 meant having to deal with a circulating list of artists whose work the company used to train its generative artificial intelligence program.</p>



<p>
	During the New Year’s weekend, artists linked to a Google Sheet on the social media platforms X (formerly known as Twitter) and Bluesky, alleging that it showed how Midjourney developed a database of time periods, styles, genres, movements, mediums, techniques, and thousands of artists to train its AI text-to-image generator. Jon Lam, a senior storyboard artist at Riot Games, also posted several screenshots of Midjourney software developers discussing the creation of a database of artists to train its AI image generator to emulate.

	</p>
<section>
	

	<h2 id="section-heading">

	
		Related Articles
	
	</h2>


	
</section>




<p>
	https://x.com/JonLamArt/status/1741545927435784424?s=20</p>



<p>
	The 24-page list of artists’ names used by Midjourney as the training foundation for its AI image generator (Exhibit J) includes modern and contemporary blue-chip names,as well as commercially successfully illustrators for companies like Hasbro and Nintendo. Notable artists include Cy Twombly, Andy Warhol, Anish Kapoor, Yayoi Kusama, Gerhard Richter, Frida Kahlo, Andy Warhol, Ellsworth Kelly, Damien Hirst, Amedeo Modigliani, Pablo Picasso, Paul Signac, Norman Rockwell, Paul Cézanne, Banksy, Walt Disney, and Vincent van Gogh.

</p>



<p>
	Midjourney’s dataset also includes artists who contributed art to the popular trading card game Magic the Gathering, including Hyan Tran, a six-year-old child and one-time art contributor who <a rel="nofollow" href="https://secretlair.wizards.com/us/en/product/694985/extra-life-2021" target="_blank">participated in a fundraiser for the Seattle Children’s Hospital in 2021</a>. </p>



<p>
	<a rel="nofollow" href="https://bsky.app/profile/thephilfoglio.bsky.social/post/3khxc4wqgmt2e" target="_blank">Phil Foglio</a> encouraged other artists to search the list to see if their names were included and to seek legal representation if they did not already have a lawyer. </p>



<p>
	Access to the Google file was soon restricted, but <a rel="nofollow" href="https://web.archive.org/web/20231231203837/https://docs.google.com/spreadsheets/d/1MEglfejpqgVcaf-I-cgZ5ngV_MlaOTeGXAoBPJO69FM/htmlview#" target="_blank">a version has been uploaded to the Internet Archive</a>. </p>



<p>
	The list of 16,000 artists was included as part of a lawsuit amendment to a class-action complaint<a href="https://www.artnews.com/art-news/news/artists-class-action-lawsuit-against-ai-image-generator-midjourney-stability-deviantart-1234653892/"> targeted at Stability AI, Midjourney, and DeviantArt </a>and the submission of <a rel="nofollow" href="https://stablediffusionlitigation.com/pdf/00201/1-1-stable-diffusion-complaint-exhibits.pdf" target="_blank">455-pages of supplementary evidence</a> filed on November 29 last year. </p>



<p>
	The amendment was filed after a judge in California federal court <a href="https://www.artnews.com/art-news/news/judge-dismisses-several-copyright-allegations-against-ai-companies-in-artist-class-action-1234685299/">dismissed several claims</a> brought forth by a group of artists against Midjourney and DeviantArt on October 30. 

</p>



<p>
	The class-action copyright lawsuit was<a href="https://www.artnews.com/art-news/news/artists-class-action-lawsuit-against-ai-image-generator-midjourney-stability-deviantart-1234653892/"> first filed almost a year ago</a> in the United States District Court of the Northern District of California.</p>



<p>
	Last September, the US Copyright Review Board decided that an image generated using Midjourney’s software could not be copyright due to how it was produced. Jason M. Allen’s image had garnered the $750 top prize in the digital category for art at the Colorado State Fair in 2022. The win went viral online, but <a href="https://www.artnews.com/art-news/news/colorado-state-fair-ai-generated-artwork-controversy-1234638022/">prompted intense worry and anxiety among artists</a> about the future of their careers. </p>



<p>
	Concern about artworks being scraped without permission and used to train AI image generators also prompted researchers from the University of Chicago to create a digital tool for artists <a href="https://www.artnews.com/art-news/news/new-data-poisoning-tool-enables-artists-to-fight-back-against-image-generating-ai-companies-1234684663/">to help “poison” massive image sets</a> and destabilize text-to-image outputs.</p>



<p>
	At publication time, Midjourney did not respond to requests for comment from <em>ARTnews</em>.</p>










				</div>

				<div>
	<nav data-dropdown="">

						<h4 data-dropdown-list-item=""> Read More About:</h4>
			
	
</nav>
</div>
	

			</article>

			
		</div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone that fell from hole in Alaska 737 MAX flight is found, still open to Mail (416 pts)]]></title>
            <link>https://twitter.com/SeanSafyre/status/1744138937239822685</link>
            <guid>38907620</guid>
            <pubDate>Mon, 08 Jan 2024 02:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SeanSafyre/status/1744138937239822685">https://twitter.com/SeanSafyre/status/1744138937239822685</a>, See on <a href="https://news.ycombinator.com/item?id=38907620">Hacker News</a></p>
Couldn't get https://twitter.com/SeanSafyre/status/1744138937239822685: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[MotorOS: a Rust-first operating system for x64 VMs (248 pts)]]></title>
            <link>https://github.com/moturus/motor-os</link>
            <guid>38907568</guid>
            <pubDate>Mon, 08 Jan 2024 02:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/moturus/motor-os">https://github.com/moturus/motor-os</a>, See on <a href="https://news.ycombinator.com/item?id=38907568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Motūrus OS</h2>
<p dir="auto">Motūrus project builds a simple, fast, and secure operating system (Motūrus OS) for the cloud.</p>
<p dir="auto">In more specific terms, Motūrus OS (sometimes called Motor OS),
is a new operating system targeting virtual machine-based workloads such as web serving, "serverless", edge caching, etc.</p>
<p dir="auto"><a href="https://github.com/moturus/motor-os/blob/main/docs/screenshot.md">Screenshot</a></p>
<h2 tabindex="-1" dir="auto">Why?</h2>
<p dir="auto">At the moment, most virtualized production workloads run Linux.
While Linux has many advanced features that in many
situations mean it is the only reasonable OS choice, there are
several complications that make it not ideal, in theory,
for some virtualized workloads:</p>
<ul dir="auto">
<li>Linux is optimized for baremetal, which leads to inefficiencies
when it is used inside a VM that is running on a Linux host:
<ul dir="auto">
<li>duplicate block caches</li>
<li>duplicate page table walks</li>
<li>the host scheduler can preempt the VCPU holding a spinlock in the VM's kernel</li>
</ul>
</li>
<li>Linux is difficult to use:
<ul dir="auto">
<li>Docker, Nix OS, "serverless", etc. all exist because of Linux's complexity</li>
</ul>
</li>
<li>Linux has, historically, not been very secure</li>
</ul>
<p dir="auto">A new operating system built from ground-up with the focus
on virtualized workloads can be made much simpler and more
secure than Linux, while matching or exceeding its
performance and/or efficiency.</p>
<h2 tabindex="-1" dir="auto">What?</h2>
<p dir="auto">Motūrus OS is a microkernel-based operating system, built in
Rust, that targets virtualized workloads exclusively. It
currently supports x64 KVM-based virtual machines, and can
run in either Qemu or Cloud Hypervisor.</p>
<p dir="auto">Rust is <em>the</em> language of Motūrus OS: not only it is
implemented in Rust, it also exposes its ABI in Rust, not C.</p>
<h3 tabindex="-1" dir="auto">What works</h3>
<p dir="auto">While at the moment most of the subsystems are working in only
POC/MVP mode, they <strong>are</strong> working, and you can run, say, a web
server.</p>
<p dir="auto">More specifically, these things work:</p>
<ul dir="auto">
<li>boots via MBR (Qemu) or PVH (Cloud Hypervisor) in about 200ms</li>
<li>himem micro-kernel</li>
<li>scheduling:
<ul dir="auto">
<li>a simple multi-processor round robin (SMP)</li>
<li>in-kernel scheduling is cooperative
<ul dir="auto">
<li>the kernel is very small and does not block, so does not need to be preemptible</li>
</ul>
</li>
<li>the userspace is preemptible</li>
</ul>
</li>
<li>memory management:
<ul dir="auto">
<li>only 4K pages at the moment</li>
<li>stacks are guarded</li>
<li>page faults in the userspace work and are properly handled (only stack memory allocations are currently lazy)</li>
</ul>
</li>
<li>I/O subsystem (in the userspace)
<ul dir="auto">
<li>VirtIO-BLK and VirtIO-NET <a href="https://github.com/moturus/motor-os/tree/main/src/lib/virtio">drivers</a></li>
<li>two simple filesystems
(<a href="https://crates.io/crates/srfs" rel="nofollow">srfs</a> and
<a href="https://crates.io/crates/flatfs" rel="nofollow">flatfs</a>)</li>
<li><a href="https://crates.io/crates/smoltcp" rel="nofollow">smoltcp</a>-based networking (TCP only at the moment)
<ul dir="auto">
<li>a simple <a href="https://github.com/moturus/motor-os/tree/main/src/bin/httpd">httpd</a> is provided</li>
</ul>
</li>
</ul>
</li>
<li>the userspace:
<ul dir="auto">
<li>multiple processes, with preemption</li>
<li>threads, TLS</li>
<li>Rust's standard library <a href="https://github.com/moturus/rust/tree/moturus-2023-12-16">mostly ported</a>
<ul dir="auto">
<li>Rust programs that use Rust standard library and do not
depend, directly or indirectly, on Unix or Windows FFI,
will cross-compile for Motūrus OS and run, subject to
"what does not work" below</li>
</ul>
</li>
<li>a simple <a href="https://github.com/moturus/rush">unix-like shell</a> in the serial console</li>
</ul>
</li>
</ul>
<h3 tabindex="-1" dir="auto">What does not work</h3>
<p dir="auto">Most pieces are not yet ready for production use. No security
audit has been made. It is very easy to hit a "not implemented"
panic in sys-io (the userspace I/O subsystem).</p>
<p dir="auto">More specifically:</p>
<ul dir="auto">
<li>Filesystem: most Rust std::fs APIs have been implemented as
proof-of-concept, but are slow (synchronous) and will
have to be reimplemented using Motūrus async I/O</li>
<li>Networking:
<ul dir="auto">
<li>std::net::TcpStream is mostly implemented, but there are
todo! panics</li>
<li>other protocols are not implemented yet</li>
<li>performance can (and will) be better</li>
</ul>
</li>
<li>The ecosystem outside Rust std:
<ul dir="auto">
<li>crates like rand or rustls can be compiled and used
with minor tweaks</li>
<li>crates depending on async runtimes (e.g.
<a href="https://tokio.rs/" rel="nofollow">Tokio</a>) will not compile at the moment
<ul dir="auto">
<li><a href="https://github.com/tokio-rs/mio">Tokio Mio</a> should be
not too difficult to port</li>
</ul>
</li>
<li>crates that are wrappers around native Linux or Windows APIs
will not work, obviously</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">How can I build/run it?</h2>
<p dir="auto">See <a href="https://github.com/moturus/motor-os/blob/main/docs/build.md">docs/build.md</a>.</p>
<h2 tabindex="-1" dir="auto">Thanks</h2>
<p dir="auto">Big thanks to Philipp Oppermann for his great <a href="https://os.phil-opp.com/" rel="nofollow">Writing an OS in Rust</a> blog series - it has inspired a lot of people to experiment in this space.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a HTMX Playground 100% in the browser (331 pts)]]></title>
            <link>https://lassebomh.github.io/htmx-playground/</link>
            <guid>38906989</guid>
            <pubDate>Mon, 08 Jan 2024 01:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lassebomh.github.io/htmx-playground/">https://lassebomh.github.io/htmx-playground/</a>, See on <a href="https://news.ycombinator.com/item?id=38906989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="popup-top">
        <main id="popup">
            <article>
                
                
                <p>
                    A simple code sandbox for playing around with HTMX. No setup needed!
                </p>
                <p>
                    It allows you to write code in a backend-like environment, running entirely inside the browser. You can define endpoints within server.js and render your own templates. It will run a mock server that intersepts outgoing requests from HTMX. The request handling and templating engine should be very familiar to people who use Django. In principle, this project isn't specific to HTMX, so you are free to try out other libraries as well.
                </p>
                
                <p>
                    Check out the examples! I've adapted them from the original <a href="https://htmx.org/examples/">htmx.org examples</a>.
                </p>
                <h2>Saving &amp; sharing</h2>
                <ol>
                    <li>Press "Copy as JSON" in the top right.</li>
                    <li>Upload the contents as a Gist, and enter the raw URL in "Load Playground"</li>
                    <li>The URL on this page will update, and can now be shared.</li>
                </ol>
                <p>
                    The code is available <a target="_blank" href="https://github.com/lassebomh/htmx-playground">on GitHub</a>.
                </p>

                <h2>Limitations</h2>
                <ul>
                    <li>No page navigation</li>
                    <li>Limited mobile support</li>
                </ul>

                <h2>Libraries used</h2>
                <ul>
                    <li>Svelte</li>
                    <li>Ace (code editor)</li>
                    <li>PollyJS (mock server)</li>
                    <li>Nunjucks (templating engine)</li>
                </ul>
                
            </article>
        </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I made an app that runs Mistral 7B 0.2 LLM locally on iPhone Pros (230 pts)]]></title>
            <link>https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941</link>
            <guid>38906966</guid>
            <pubDate>Mon, 08 Jan 2024 01:00:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941">https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941</a>, See on <a href="https://news.ycombinator.com/item?id=38906966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!---->
<!---->
<!---->
    


<!---->
    


  <div dir="" data-test-bidi=""><p>Introducing Offline Chat, the next-generation AI ChatBot that runs entirely on your device without the Internet.&nbsp;You can use it anywhere, and your data stays private and secure.</p><p>While Offline Chat might not match the prowess of top-tier online models due to inherent memory and processing constraints, it stands out as an engaging and versatile tool. It's perfect for sparking creativity and assisting in various tasks such as writing, though it's advisable to verify facts independently.</p><p>The app requires a Pro iPhone with a minimum of 6GB of RAM. Only the following devices meet the requirement:</p><p>- iPhone 15 Pro, iPhone 14 Pro, iPhone 13 Pro, iPhone 12 Pro.<br>- iPads: Please check. RAM varies based on model and year. </p><p>For the technically oriented, the AI is a fine tuned large language model based on Mistral 7B 0.1, quantized to 3-bit.</p></div>

<!---->
  <section>
    <div>
      <h2>What’s New</h2>
        

    </div>
    <div>
          <p dir="false" data-test-bidi="">Updated model to Mistral 7B 0.2. This new model is truly extraordinary considering its size.</p>


      </div>
  </section>

      <section>
      <p>
        <h2>
          Ratings and Reviews
        </h2>

        <!---->
      </p>

        


      

<!---->    </section>


<!---->
<!---->
<!---->
  <section>
  <div>
    <h2>
      App Privacy
    </h2>

    


  </div>

  <p>
    The developer, <span>Opus Noma LLC</span>, indicated that the app’s privacy practices may include handling of data as described below. For more information, see the <a href="http://opusnoma.com/privacy">developer’s privacy policy</a>.
  </p>

  <div>
        
        <h3>Data Not Collected</h3>
        <p>The developer does not collect any data from this app.</p>
<!---->      </div>

    <p>Privacy practices may vary, for example, based on the features you use or your age. <a href="https://apps.apple.com/story/id1538632801">Learn&nbsp;More</a></p>
</section>


<section>
  <div>
    <h2>Information</h2>
    <dl>
        <p>
          <dt>Seller</dt>
          <dd>
              Opus Noma LLC
          </dd>
        </p>
        <p>
          <dt>Size</dt>
          <dd aria-label="3.3 gigabytes">3.3 GB</dd>
        </p>
        <p>
          <dt>Category</dt>
          <dd>
              <a href="https://itunes.apple.com/us/genre/id6000" data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;actionUrl&quot;:&quot;https://itunes.apple.com/us/genre/id6000&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;GenrePage&quot;}">
                Business
              </a>
          </dd>
        </p>
      <div>
        <dt>Compatibility</dt>
          <dd>
              <dl>
                <dt>
                  iPhone
                </dt>
                <dd>Requires iOS 17.0 or later.
                </dd>
              </dl>
              <dl>
                <dt>
                  iPad
                </dt>
                <dd>Requires iPadOS 17.0 or later.
                </dd>
              </dl>
              <dl>
                <dt>
                  Mac
                </dt>
                <dd>Requires macOS&nbsp;14.0 or later and a Mac with Apple&nbsp;M1&nbsp;chip or later.
                </dd>
              </dl>
          </dd>
      </div>
<!---->      
      <p>
        <dt>Age Rating</dt>
        <dd>
             12+
              <span>Infrequent/Mild Medical/Treatment Information</span>
              <span>Infrequent/Mild Mature/Suggestive Themes</span>
        </dd>
      </p>
<!---->      <p>
        <dt>Copyright</dt>
        <dd>© 2023 Opus Noma LLC</dd>
      </p>
        <p>
          <dt>Price</dt>
          <dd>$1.99</dd>
        </p>
<!---->
    </dl>
  </div>
  <div>
    <ul>
<!---->        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="http://opusnoma.com/">
            App Support
          </a>
        </li>
<!---->        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="http://opusnoma.com/privacy">
            Privacy Policy
          </a>
        </li>
    </ul>
  </div>
</section>

<section>
  <ul>
<!---->      <li>
        <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="http://opusnoma.com/">
          App Support
        </a>
      </li>
<!---->      <li>
        <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="http://opusnoma.com/privacy">
          Privacy Policy
        </a>
      </li>
  </ul>
</section>

  <section>
    <p>
      <h2>Supports</h2>
    </p>
    <ul>
        <li>
          <img src="https://apps.apple.com/assets/images/supports/supports-FamilySharing@2x-f58f31bc78fe9fe7be3565abccbecb34.png" alt="" role="presentation">
          <div>
              <h3 dir="ltr">
    Family Sharing
</h3>


              <h4 dir="">
        

                    <p data-test-bidi="">Up to six family members can use this app with Family&nbsp;Sharing enabled.</p>

    


<!----></h4>


          </div>
        </li>
    </ul>
  </section>

<!---->
    <section>
      <p>
        <h2>
          More By This Developer
        </h2>
        <!---->
      </p>

      
    </section>

    <section>
      <p>
        <h2>
          You Might Also Like
        </h2>
        <!---->
      </p>

      
    </section>


<!---->

<!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How An ASML Lithography Machine Moves a Wafer [video] (113 pts)]]></title>
            <link>https://www.youtube.com/watch?v=1fOA85xtYxs</link>
            <guid>38906881</guid>
            <pubDate>Mon, 08 Jan 2024 00:46:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=1fOA85xtYxs">https://www.youtube.com/watch?v=1fOA85xtYxs</a>, See on <a href="https://news.ycombinator.com/item?id=38906881">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How We Handle Cap Table Information (110 pts)]]></title>
            <link>https://henrysward.medium.com/how-we-handle-captable-information-c98d85d79277</link>
            <guid>38906749</guid>
            <pubDate>Mon, 08 Jan 2024 00:31:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://henrysward.medium.com/how-we-handle-captable-information-c98d85d79277">https://henrysward.medium.com/how-we-handle-captable-information-c98d85d79277</a>, See on <a href="https://news.ycombinator.com/item?id=38906749">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://henrysward.medium.com/?source=post_page-----c98d85d79277--------------------------------"><div aria-hidden="false"><p><img alt="Henry Ward" src="https://miro.medium.com/v2/resize:fill:88:88/1*-fzeFCY7lgBp4RqEXdKdVw.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="9bd8">On Friday we had an internal policy violation that affected three companies. I’ve been in touch with the founders and I’m appalled we made that mistake and it should never have happened. It is unacceptable and we’ve dealt with the violation on Saturday morning and are continuing the investigation to make sure it never happens again.</p><p id="dba1">Let me share our framework on data privacy and access controls to hopefully address concerns from this weekend. For a deeper dive, I will bucket data privacy into four buckets with different rules that I will cover separately.</p><p id="779c">1. <strong>Public Disclosures:</strong> We can only publish <em>aggregate and anonymous data</em>. So we can say things like there are 34K startups on Carta, or the average Series A startup has 25 employees, etc… However, we cannot say Acme Startup has 41 shareholders or the PPS is $13.24. You will see this type of aggregate anonymous information frequently in our data reports.</p><p id="8e1c">2. <strong>Internal Systems Disclosures:</strong> We can use cap table data for onboarding and internal systems development. So for example, we can load cap table data into dashboards for audit, we can write health checks to make sure cap table reports are correct, we can run machine learning algorithms to predict when you need a 409A, etc… We can use cap table data <em>to help us improve the software or customer experience</em>. This also includes things like when support teams access cap tables (through an approval and audit system) or when a customer needs help correcting or updating their cap table. All human access to cap tables is tracked and audited.</p><p id="e651">3. <strong>Sales &amp; Marketing:</strong> Lastly, we can market to our customers and users. For example, we can offer new products to help companies with employee compensation, taxes, and expense reporting. Occasionally we have offered products directly to employee shareholders. For example, in the past we have offered stock based loan products to employees of certain companies where employees can access loans to exercise their stock. But when we offer these products to employees we only do it in collaboration with the company. The company has to approve the program for their employees for us to offer it.</p><p id="5023"><strong>4. CartaX: </strong>CartaX is a separate product that operates as an opt-in marketplace where investors are invited to enter bids and asks on different companies. At any given time we have about one hundred companies that are in the marketplace. Where CartaX and the cap table business converge is if we match a trade in the marketplace, we go to the company and ask if they will allow it. If the company allows it, we use their cap table to execute the trade. If the company doesn’t allow it, we stop the trade. We do not and will never trade without company consent.</p><p id="81c5">In the case of Linear and two other companies, we had an internal breach of protocol and we contacted someone directly on the cap table. That never should have happened and is absolutely a breach of our privacy protocols. And we have addressed it over the weekend.</p><p id="2c11">The second mistake might be whether we are too close to the cap table business to be helping on liquidity. We started CartaX five years ago to help founders and companies with liquidity and it has mostly been a net positive for founders, employees, and shareholders. But even if we do everything perfectly and make zero mistakes, perhaps just the appearance of being in the liquidity business makes us seem compromised. Everything we do must be grounded in trust and if being in the liquidity business compromises that trust, perhaps we need to reevaluate that offering.</p><p id="62dc">I will think about this and come back with more thoughts in the coming months. If you have a perspective on whether Carta should be helping companies with liquidity, please reach out to me. I’d love to hear them.</p><p id="83f6">I’m sorry for scaring everybody about this. After ten years of managing cap tables across 40,000 startups, I promise we aren’t compromising anyone’s data. We won’t be here if you don’t trust us. Trust, transparency, and integrity is our most important currency. If you would like to chat with me more one-on-one, please email me at <a href="mailto:henry.ward@carta.com" rel="noopener ugc nofollow" target="_blank">henry.ward@carta.com</a> and we can set up a zoom.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Buffett once bet $1M that he could beat a group of hedge funds over 10 years (104 pts)]]></title>
            <link>https://finance.yahoo.com/news/warren-buffett-once-bet-1m-113000485.html</link>
            <guid>38906586</guid>
            <pubDate>Mon, 08 Jan 2024 00:07:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finance.yahoo.com/news/warren-buffett-once-bet-1m-113000485.html">https://finance.yahoo.com/news/warren-buffett-once-bet-1m-113000485.html</a>, See on <a href="https://news.ycombinator.com/item?id=38906586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><div><p><img alt="Warren Buffett once bet $1M that he could beat a group of fancy hedge funds over 10 years — and he crushed them with a technique requiring absolutely no investing skill. Here's what he did" src="https://s.yimg.com/ny/api/res/1.2/PAPJyRp1wWXzoalzy.3PIA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTQyNw--/https://media.zenfs.com/en/moneywise_327/2ced5299e5954d8176723c1e63fb13e7" data-src="https://s.yimg.com/ny/api/res/1.2/PAPJyRp1wWXzoalzy.3PIA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTQyNw--/https://media.zenfs.com/en/moneywise_327/2ced5299e5954d8176723c1e63fb13e7"></p></div><p><figcaption>Warren Buffett once bet $1M that he could beat a group of fancy hedge funds over 10 years — and he crushed them with a technique requiring absolutely no investing skill. Here's what he did</figcaption></p></figure><p>Known for their complex investment strategies, hedge funds are typically seen as exclusive options for the ultra-rich. However, you don’t have to be among the elite to achieve comparable, or even superior, returns.</p><p>According to legendary investor Warren Buffett, there’s a very simple strategy that has the potential to outperform these complex hedge funds. Buffett was at one point so confident about this strategy that he was willing to wager a million dollars on its effectiveness.</p><h2>Don’t miss</h2><ul><li><p>Commercial real estate has outperformed the S&amp;P 500 over 25 years. Here's how to diversify your portfolio <a href="https://moneywise.com/investing/alternative-investments/hedge-your-portfolio-with-commercial-real-estate?throw=C1DM1&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_without+the+headache+of+being+a+landlord" rel="nofollow noopener" target="_blank" data-ylk="slk:without the headache of being a landlord;elm:context_link;itc:0">without the headache of being a landlord</a></p></li><li><p>Finish 2023 stronger than you started: <a href="https://moneywise.com/retirement/retirement/alt-5-money-moves-you-should-make-before-the-end-of-2023?throw=C1DM2&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_5+money+moves+you+should+make+before+the+end+of+th" rel="nofollow noopener" target="_blank" data-ylk="slk:5 money moves you should make before the end of the year;elm:context_link;itc:0">5 money moves you should make before the end of the year</a></p></li><li><p>The US dollar has lost 87% of its purchasing power since 1971 — <a href="https://moneywise.com/investing/alternative-investments/gold-ira-secure-financial-future?throw=C1DM3&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_invest+in+this+stable+asset+before+you+lose+your+r" rel="nofollow noopener" target="_blank" data-ylk="slk:invest in this stable asset before you lose your retirement fund;elm:context_link;itc:0">invest in this stable asset before you lose your retirement fund</a></p></li></ul><p>In 2007, <a href="https://www.usatoday.com/story/money/markets/2018/01/02/warren-buffett-bet-against-hedge-funds-girls-charity/996993001/" rel="nofollow noopener" target="_blank" data-ylk="slk:Buffett bet a million dollars;elm:context_link;itc:0">Buffett bet a million dollars</a> that over the course of a decade, a simple S&amp;P 500 index fund would outperform a basket of hand-picked hedge funds. He picked the Vanguard 500 Index Fund Admiral Shares (VFIAX).</p><p>Hedge fund manager Ted Seides from Protégé Partners accepted the bet and picked five funds-of-funds. A fund-of-funds is a portfolio of funds that charges two layers of management fees.</p><p>The outcome? Buffett triumphed decisively.</p><p>Buffett <a href="https://www.berkshirehathaway.com/letters/2017ltr.pdf" rel="nofollow noopener" target="_blank" data-ylk="slk:shared;elm:context_link;itc:0">shared</a> the final scorecard of the bet in his 2017 shareholder letter. The S&amp;P 500 index fund he selected delivered a total gain of 125.8% during the decade, while the five funds-of-funds reported respective gains of 21.7%, 42.3%, 87.7%, 2.8% and 27.0% during the same period.</p><p>Buffett gave all proceeds to charity — and Girls Inc. of Omaha turned out to be the biggest winner of the bet.</p><h2>High returns, low fees</h2><p>This decade-long bet challenged the notion that complex and expensive investment methods always yield the best results.</p><p>After all, anyone can replicate Buffett’ strategy at a very low cost. The Vanguard index fund he picked has an expense ratio of just 0.04%.</p><p>The hedge funds, Buffett pointed out, come at a much higher cost to investors.</p><p>“Even if the funds lost money for their investors during the decade, their managers could grow very rich,” he wrote in the shareholder letter. “That would occur because fixed fees averaging a staggering 2.5% of assets or so were paid every year by the fund-of-funds’ investors, with part of these fees going to the managers at the five funds-of-funds and the balance going to the 200-plus managers of the underlying hedge funds.”</p><p>In the investing world, fees should not be overlooked — they can eat into your returns. In an op-ed for Bloomberg titled “Why I Lost My Bet With Warren Buffett,” Seides agreed with Buffett on the subject of hedge funds’ management fees.</p><p>“He is correct that hedge-fund fees are high, and his reasoning is convincing. Fees matter in investing, no doubt about it,” he <a href="https://www.bloomberg.com/view/articles/2017-05-03/why-i-lost-my-bet-with-warren-buffett" rel="nofollow noopener" target="_blank" data-ylk="slk:wrote;elm:context_link;itc:0">wrote</a>.</p><p><strong>Read more:</strong> 'It's not taxed at all': Warren Buffett shares the <a href="https://moneywise.com/life/lifestyle/hybrid-its-not-taxed-at-all?throw=C1HALF&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_%27best+investment%27+you+can+make+when+battling+infla" rel="nofollow noopener" target="_blank" data-ylk="slk:'best investment' you can make when battling inflation;elm:context_link;itc:0">'best investment' you can make when battling inflation</a></p><p>These days, many ETFs enable investors to track benchmark indices at minimal costs. For instance, the Vanguard S&amp;P 500 ETF (VOO), which follows the S&amp;P 500, has a low expense ratio of 0.03%. Similarly, the SPDR S&amp;P 500 ETF Trust (SPY) tracks the same index and carries an expense ratio of 0.0945%.</p><p>Does that mean every investor should abandon stock-picking and put all their money into index funds?</p><p>The answer varies based on the individual.</p><p>For someone like Buffett, making their own investment decisions could lead to significantly greater success. Consider this: from 1964 to 2022, Buffett’s Berkshire Hathaway <a href="https://www.berkshirehathaway.com/letters/2022ltr.pdf" rel="nofollow noopener" target="_blank" data-ylk="slk:delivered;elm:context_link;itc:0">delivered</a> an astounding overall gain of 3,787,464%, substantially outperforming the S&amp;P 500’s already impressive 24,708% return in the same timeframe.</p><h2>What to read next</h2><ul><li><p>Thanks to Jeff Bezos, you can now <a href="https://moneywise.com/investing/real-estate/invest-vacation-rental-homes?throw=C1WTRN1&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_cash+in+on+prime+real+estate" rel="nofollow noopener" target="_blank" data-ylk="slk:cash in on prime real estate;elm:context_link;itc:0">cash in on prime real estate</a> — without the headache of being a landlord. Here's how</p></li><li><p>Worried about the economy? Here are <a href="https://moneywise.com/top/alternative-investments?throw=C1WTRN2&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_the+best+shock-proof+assets" rel="nofollow noopener" target="_blank" data-ylk="slk:the best shock-proof assets;elm:context_link;itc:0">the best shock-proof assets</a> for your portfolio. (They’re all outside of the stock market.)</p></li><li><p>Rising prices are throwing off Americans' retirement plans — here’s <a href="https://moneywise.com/managing-money/retirement-planning/how-to-retire-early?throw=C1WTRN3&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_how+to+get+your+savings+back+on+track" rel="nofollow noopener" target="_blank" data-ylk="slk:how to get your savings back on track;elm:context_link;itc:0">how to get your savings back on track</a></p></li></ul><p><em>This article provides information only and should not be construed as advice. It is provided without warranty of any kind.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["LibreOffice is better at reading old Word files than Word" (368 pts)]]></title>
            <link>https://eldritch.cafe/@sfwrtr/111716610017454919</link>
            <guid>38906331</guid>
            <pubDate>Sun, 07 Jan 2024 23:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eldritch.cafe/@sfwrtr/111716610017454919">https://eldritch.cafe/@sfwrtr/111716610017454919</a>, See on <a href="https://news.ycombinator.com/item?id=38906331">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MK1 Flywheel Unlocks the Full Potential of AMD Instinct for LLM Inference (113 pts)]]></title>
            <link>https://mkone.ai/blog/mk1-flywheel-amd</link>
            <guid>38906208</guid>
            <pubDate>Sun, 07 Jan 2024 23:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mkone.ai/blog/mk1-flywheel-amd">https://mkone.ai/blog/mk1-flywheel-amd</a>, See on <a href="https://news.ycombinator.com/item?id=38906208">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-yui_3_17_2_1_1703802076508_12216">
  <h3>AMD Enters AI Market with Instinct Series Accelerators</h3><p>The 2023 holiday season marked a significant milestone for the AI community with the launch of AMD’s eagerly anticipated Instinct MI300 series accelerator, showcasing their advanced CDNA 3 architecture. On paper, the MI300 has the potential to challenge NVIDIA’s market dominance for cloud AI workloads, bringing hope for genuine performance competition and leveling the playing field. AMD’s Achilles heel up to this point has been its lagging software ecosystem, however, recently there have been inroads into natively supporting AMD hardware on popular AI frameworks.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1703802231758_5288">
  <p>Once we realized what the AMD Instinct series cards were capable of, we challenged ourselves to port our LLM inference engine MK1 Flywheel to AMD. Having just achieved the best inference performance on NVIDIA hardware (see <a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready">our companion post</a>), we rolled up our sleeves and got to work.</p><p>To jump straight to the conclusion: we believe that with Flywheel, AMD looks to be a force to be reckoned with. For now, our results are on the AMD Instinct MI210, and we are excited to benchmark on MI300 as they become widely available.</p><h3>How does AMD Stack Up to NVIDIA? </h3><p>Before we take you through our journey with AMD Instinct and ROCm, let's start with the results. </p><p>For reasons explained later on, we profiled the AMD Instinct MI100 and MI210 cards, and here focus on the newer MI210 for comparison. On the NVIDIA side, we chose the RTX A6000 since it has a similar hardware specification based on TFLOPS, memory and power. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1704576951001_27949">
  <p><em>Inference performance for MK1 Flywheel and vLLM  (v0.2.6) for Llama-2-13B. The benchmarks measure the throughput (requests/second) against average latency (end-to-end round trip time to service the request) across an increasing number of asynchronous workers (mimicking users) for a given input prompt and output generation distribution. Typical distribution (left) has a max token context of 960 and max token generations of 80; Long prompt distribution (right) has max token context of 1800 and a max token generation of 120. </em></p><p>The results are clear: MK1 Flywheel on an AMD Instinct MI210 now rivals a compute-matched NVIDIA GPU (also running Flywheel). Moreover, Flywheel shows higher throughput across all tested workloads on both AMD and NVIDIA compared to vLLM. The increased performance translates into significant cost savings as the same GPU instance can now service more users. </p><p>For clients considering using AMD Instinct for LLM inference workloads at scale, <strong>please reach out.</strong></p><h3>Recap of MK1 Flywheel</h3><p>In <a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready">our companion post</a>, we introduced MK1 Flywheel, our enterprise LLM inference engine built for real-world enterprise applications. Working with our early adopters, we forged a performant inference solution that is now in commercial use servicing millions of active users for LLM chat applications. As a quick recap, Flywheel has</p><ol data-rte-list="default"><li><p>Unparalleled Throughput vs Latency characteristics.</p></li><li><p>Rapid auto scaling for optimized scaling up of inference on cloud platforms.</p></li><li><p>For enterprise customers: seamless integration into your stack with native PyTorch compatibility, and a drop in replacement for inference backends like vLLM, TensorRT-LLM or Hugging Face TGI. </p></li></ol><p>You can take Flywheel for a spin on <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-wkonsb76xsee6"><strong>AWS SageMaker</strong></a><strong>. </strong>Currently it runs on NVIDIA backend, and we look forward to offering Flywheel on AMD backends.</p><p>Next up, we want to give you a behind-the-scenes journey of building the hardware and software components that brought Flywheel to life on AMD. Hope you have as much fun reading it as we had doing it!</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1703804286743_15230">
  <h3>Our Journey: Building out the Hardware</h3><p>Our early exploration of the AMD Instinct series began with the harsh reality check that these cards aren’t ubiquitous across cloud platforms (for now). In order to chalk up a quick existential proof, we grabbed an MI100 (CDNA 1 architecture) from the bargain bin on eBay. The exercise was fairly touch-and-go at the start. Since the card does not have active cooling, we jury-rigged a 3D-printed fan hood onto the card to boot the card on our typical workstation desktops. Once everything was plugged in, only then did we realize that the system had no video output! Fortunately, we rummaged a relic in the form of an NVIDIA TitanX, and strapped it in. The workstation was rounded out with an Intel Xeon CPU, for reasons that will require its own blog post. Once we had this chimera of a system up and running, the absurdly loud high-static pressure fan did nothing to curb our enthusiasm when we generated text off a LLama-2-7B. We have come a long way since then.</p><p>A few weeks later we were able to land an MI210 (CDNA 2 architecture). With the lessons learned from the MI100, we had it up and running in no time!</p><h3>Our Journey: Building out the Software</h3><p>AMD has truly stepped up their game on their ROCm software stack over the past year, and it shows. We now have tight integration into bread and butter AI frameworks like PyTorch and TensorFlow. And, for the first time, you can effortlessly run LLMs right off of Hugging Face with a few lines of python. </p><p>As we designed MK1 Flywheel for the NVIDIA platform, we learned priceless lessons along the way and grew the intuition and engineering necessary to distinctly push NVIDIA GPUs to their limits. Energized by the momentum we saw on the AMD platform and community, we challenged ourselves to put our theories and engineering to the test. Once again, staying true to our performance-obsessed nature, we took no shortcuts and built the framework backend from first principles for the AMD stack.</p><p>While the interesting idiosyncrasies of the CDNA architecture flavored our ROCm kernel stack to be understandably different from our CUDA stack, the fundamentals that unlocked the true potential of the GPU hardware remained consistent. To start, we had to grok the CDNA architecture and its history (GCN - Graphics Core Next), alongside the complementary graphics-forward RDNA architecture. In parallel, we had to discover the strengths and limits of the compiler, especially on a compute pipeline that relies on instruction counting. Certain system-level techniques like optimized kernel scheduling worked right away due to being platform agnostic by nature. However, all core device kernels that used platform-specific features (say, NVIDIA Tensor Cores) had to be written from scratch to use the equivalent on the AMD hardware (AMD Matrix Cores).</p><p>In our experience so far, ROCm is in great shape to build <em>functional</em> kernels right off the bat on AMD hardware. Yet, to extract the most out of the CDNA architecture, we've had to go full manual, in comparison to our experience with CUDA. In all fairness, this can be marked up to the fact that NVIDIA has had a significant head start with CUDA for AI workloads. We believe what our work demonstrates, is that after equalizing the playing field on the software front with MK1 Flywheel, the AMD Instinct Series is a serious contender for today’s cloud inference workloads. We are excited to continue developing on ROCm, and extending Flywheel for MI300, as well as further optimizing for finetuning and training workloads. </p><h3>Results: MK1 Flywheel on AMD Instinct MI210 and MI100 </h3><p>We further benchmarked Flywheel on AMD Instinct across Mistral-7B and Llama-2-13B for various workloads. These results confirm that Flywheel has excellent performance across different LLM use cases.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1704576951001_125452">
  <p><em>Inference performance for MK1 Flywheel and vLLM  (v0.2.6) for popular models. Data distribution characteristics are described further in </em><a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready"><em>our companion post</em></a><em> . MI100 measurements are only provided for MK1 Flywheel, since vLLM does not officially support the CDNA 1 architecture.</em></p><h3>Dev Notes for ROCm</h3><p>There are a few things that we do miss from the CUDA stack. For example, the tight-loop performance debugging afforded by NVIDIA Nsight Compute was invaluable as we developed our inference stack for NVIDIA. Omnitrace and Omniperf on the ROCm stack got us some of the way but weren’t as polished as their counterparts. The documentation could use some love as well, as our current experience working on the ROCm stack was like driving a stick shift. On the plus side, the fact that ROCm toolkit and the AMD LLVM project are entirely open source allowed us to parse the true nature of the hardware and build design patterns. Conversely, CUDA is entirely closed.</p><p>The ROCm platform offers <a href="https://github.com/ROCm/HIPIFY">HIPIFY</a>, a convenient utility that converts CUDA code to cross-platform HIP code. However, to extract the most out of the CDNA architecture, we couldn’t “<em>hipify</em>” our CUDA kernels tuned for current NVIDIA architectures. This is where our efforts of building a framework from first principles really paid off, as we applied our learnings and theories onto the new architecture and reaped the results. </p><h3>What's next?</h3><p>The journey has only begun, and there is a lot more work to be done. Despite the VRMs screeching for their dear lives, the GPUs are not on fire… yet. As indicated in <a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready">our companion post</a>, there are a good deal of stack optimizations left on the table in order to maximize inference performance. MK1 Flywheel aims to be platform agnostic, offering performance parity across GPUs with similar hardware specifications, i.e. <em>you will get your TFLOPs worth</em>. This opens up a wider range of cloud hardware platforms and economic mobility to serve your inference applications, while retaining the familiar frontend and user experience. For users planning enterprise deployments, we have different options for integrating MK1 Flywheel natively into your production stack, regardless of platform. <a href="https://mkone.ai/contact"><strong>Don't hesitate to reach out!</strong></a></p><p>We are eager to explore the MI300X accelerator, and in theory, MK1 Flywheel should perform out-of-the-box for CDNA3, as we have prepared for it. The bigger hurdle is access to the new hardware. The MI300A APU has us looking forward to the converged strength of the Zen4 CPU and CDNA3 GPU.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1704517194203_204206">
  <p>A <a href="https://embeddedllm.com/blog/vllm_rocm/">post from EmbeddedLLM dated Oct-27-2023</a> claims that AMD Instinct MI210 achieves LLM inference parity with the NVIDIA A100. There seems to be an unexplained discrepancy, considering that the NVIDIA A100 has 312 TFLOPS compared to the 181 TFLOPS of the Instinct MI210. In contrast, MK1 Flywheel achieves a proportional increase in performance on the A100 which has 1.7x the horsepower of the Instinct MI210, which can be seen in Throughput vs Latency comparisons. </p><p>We believe the EmbeddedLLM results may simply be out-of-date as an older version of vLLM (v0.1.4) was used as the base to develop ROCm support, and may not have included performance improvements from more recent versions. As of <a href="https://docs.vllm.ai/en/latest/getting_started/amd-installation.html">v0.2.4</a>, vLLM natively supports AMD Instinct MI200 series GPUs.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Does the Cerebellum Do? (273 pts)]]></title>
            <link>https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway</link>
            <guid>38905898</guid>
            <pubDate>Sun, 07 Jan 2024 22:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway">https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway</a>, See on <a href="https://news.ycombinator.com/item?id=38905898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png" width="750" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:750,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:102187,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>This is the cerebellum. Its name means “little brain” — it’s a whole other brain under your “big” one.</p><p>If you vaguely remember something about what the cerebellum does, you’re probably thinking something to do with balance. Medical students have to learn the “cerebellar gait” that results from cerebellar injury (it’s the same staggering gait that drunk people have, because alcohol impairs the cerebellum.)</p><div id="youtube2-FFki8FtaByw" data-attrs="{&quot;videoId&quot;:&quot;FFki8FtaByw&quot;,&quot;startTime&quot;:&quot;63s&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/FFki8FtaByw?start=63s&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>A more detailed neurological exam of a patient with cerebellar disease shows a wider variety of motor problems.</p><div id="youtube2-Gn3AcxSn-Dc" data-attrs="{&quot;videoId&quot;:&quot;Gn3AcxSn-Dc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/Gn3AcxSn-Dc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>Here you’ll notice that the patient can’t bring his finger to his nose or clap his hands without a wobbling back-and-forth motion; that his eyes “wobble” back and forth (which is called </span><a href="https://en.wikipedia.org/wiki/Nystagmus" rel="">nystagmus</a><span>); and that he wobbles backwards and forwards while standing or walking, so that he nearly falls over and needs a broad-based gait to support himself.</span></p><p><span>In cerebellar disease, muscle tone is diminished (people are “floppy”), movements are not fluent (each individual sub-movement is separate), there’s </span><a href="https://en.wikipedia.org/wiki/Dysmetria" rel="">dysmetria </a><span>(failure to “aim” or estimate the distance to move, overshoot or undershoot) and there’s “</span><a href="https://www.ncbi.nlm.nih.gov/books/NBK560642/#:~:text=Intention%20tremor%20is%20defined%20as,worsening%20before%20reaching%20the%20endpoint." rel="">intention tremor</a><span>” (high amplitude, relatively slow wobbles that arise when the patient starts to move, as contrasted with the “resting tremor” characteristic of e.g. Parkinson’s disease.)  </span></p><p><span>Clearly, the cerebellum does something to control movement, and movement is impaired when it is damaged. But </span><em>why do we need a whole other “little brain” to control these aspects of movement</em><span>? </span></p><p>There are already regions of the cerebrum (or “forebrain”) dedicated to movement, like the motor cortex and the basal ganglia. And you can do a lot of movement using just those!</p><p><span>Even in the rare cases known as cerebellar agenesis, where a person is born totally lacking a cerebellum</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-1-140098227" target="_self" rel="">1</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-2-140098227" target="_self" rel="">2</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-3-140098227" target="_self" rel="">3</a></span><span>, movement is still possible, just impaired: slow motor and speech development in childhood, abnormal spoken pronunciation, wobbly limb movements, and mild-to-moderate intellectual disability. But </span><em>not </em><span>paralysis, and not even particularly bad disability overall — a lot of these people were able to live independently and work at jobs.</span></p><p>So…that’s weird. </p><p><span>What is the cerebellum’s job?  It seems weird to have a </span><em>whole separate organ </em><span>for “make motor and cognitive skills work somewhat better.”  </span></p><p>The other weird thing about the cerebellum is anatomical.</p><p>These very large, complex neurons  are the Purkinje cells, which exist only in the cerebellum.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg" width="611" height="715" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:715,&quot;width&quot;:611,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:186474,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Illustration by Santiago Ramon y Cajal</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png" width="462" height="376" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:376,&quot;width&quot;:462,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:90330,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Notice how the Purkinje cell (e) is much more complex than the other neuron types. </figcaption></figure></div><p>They have hundreds of synapses each, unlike the neurons of the cerebrum which only have a few.</p><p><span>Most of the other cells in the cerebellum are the small granule cells — in fact, they are so numerous that they comprise more than half of all neurons in the whole human brain.  In total, the cerebellum contains 80% of all neurons!</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-4-140098227" target="_self" rel="">4</a></span></p><p>If you were an alien with a microscope who knew nothing about neurology, your first assumption would be “ah yes, the thinking happens in the cerebellum.”</p><p>Why is there so much neuronal complexity dedicated to….making movement a bit smoother and “higher” cognition a bit better?</p><p><span>The third weird fact is that the </span><em>size </em><span>of the cerebellum has been growing throughout primate evolution and human prehistory, </span><em>faster </em><span>than overall brain size.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-5-140098227" target="_self" rel="">5</a></span></p><p><span>Great ape brains are distinguished from monkey brains by their larger frontal </span><em>and cerebellar </em><span>lobes.  The Neanderthals had bigger brains than us but smaller cerebella. And, most strikingly, modern humans have much bigger cerebella than “anatomically modern” Cro-Magnon humans of only 50,000 years ago (but relatively </span><em>smaller </em><span>cerebral hemispheres!) </span></p><p>An alien paleontologist could be forgiven for assuming “ah yes, the cerebellum, the seat of the higher intellect.”</p><p><span>The cerebellum </span><em>looks </em><span>like it should have some crucial unique function. Something key to “what makes us human.”  But what could it be?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg" width="600" height="588" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:116735,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Some of Ivan Pavlov’s famous dogs</figcaption></figure></div><p>Classical conditioning — the thing that makes a dog salivate when it hears a bell it’s learned to associate with food — is a very low-level process.</p><p>You don’t need a lot of brain for classical conditioning. </p><p><span>You can classically condition the sea slug </span><em>Aplysia, </em><span>which has only 20,000 neurons, to flinch from a neutral sensation it’s learned to associate with a painful one.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-6-140098227" target="_self" rel="">6</a></span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg" width="400" height="289" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:289,&quot;width&quot;:400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26821,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Aplysia californica </em><span>releasing a toxic cloud in self-defense</span></figcaption></figure></div><p><span>Even single cells can exhibit learning. The giant slime mold amoeba </span><em>Physarum</em><span> can be “trained” to cross a noxious part of a petri dish to reach food on the other side.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-7-140098227" target="_self" rel="">7</a></span></p><p><span>However, in vertebrates, classical conditioning is highly localized in the nervous system: the cerebellum is </span><em>necessary and sufficient </em><span>for learning conditioned responses.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-8-140098227" target="_self" rel="">8</a></span></p><p>A standard test of classical conditioning is the eyeblink test: can the subject learn to associate a neutral stimulus with an unpleasant one (like a puff of air to the eye) and automatically blink in response to the neutral stimulus.</p><p><span>If you lesion the cerebellum in animals, they no longer exhibit eyeblink conditioning. Humans with cerebellar damage also have no eyeblink conditioning.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-9-140098227" target="_self" rel="">9</a></span></p><p><span>Conversely, if you want classical conditioning, </span><em>all you need </em><span>is a cerebellum — in fact, all you need for classical conditioning is the Purkinje cells!</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-10-140098227" target="_self" rel="">10</a></span><span> </span></p><p>The eyeblink response is governed by the Purkinje cells. When an animal feels a puff of air to the eye, the Purkinje cells stop firing temporarily, resulting in an eyeblink. </p><p>So, in 2007, some Swedish researchers decided to study this response in isolation. Here’s how it works.</p><p><span>You take a “decerebrate” ferret — i.e. a ferret with the whole cerebrum severed from the rest of the brain (consisting of the brainstem and cerebellum). You condition individual Purkinje cells with electrode stimulation of two types of neurons that form their inputs: the climbing fibers (unconditioned stimulus) and the mossy fibers (conditioned stimulus.)  Stimulating the climbing fibers (the same thing that happens naturally when a puff of air hits the eye) causes a temporary suppression of Purkinje cell firing; stimulating the mossy fibers does not. </span><em>But, </em><span>if the mossy fibers are stimulated </span><em>right before </em><span>the climbing fibers, the Purkinje neurons </span><em>learn </em><span>to anticipate the association and suppress their firing in response to the mossy fiber stimulation.  This is a textbook example of classical conditioning.</span></p><p>Are cerebellar Purkinje cells the only individual neurons capable of single-cell learning?</p><p><span>Not in all animals; the sensory neurons of the sea slug </span><em>Aplysia </em><span>can be classically conditioned.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-11-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-11-140098227" target="_self" rel="">11</a></span><span>  But I haven’t been able to find a study in vertebrates of single-cell classical conditioning or associative learning outside the cerebellum. (Other regions of the brain, of course, are involved in learning, but the learning might be occurring  through </span><em>multicellular </em><span>information like the relationships between nearby neurons’ firing patterns.)</span></p><p>Why do we care?</p><div><p><span>First of all, this gives us at least one “unique job” for the cerebellum: it is the place in the brain where conditioned associations are learned.</span></p><p><span>Second of all, it </span><em>disproves </em><span>the </span><a href="https://en.wikipedia.org/wiki/Long-term_potentiation" rel="">long-term potentiation </a><span>theory that learning in the brain happens exclusively through strengthening synaptic connections between neurons. (The old dictum that “neurons that fire together, wire together.”)  The brain is </span><em>not </em><span>like a neural network where the only thing that is “learned” or “updated” is the weights between neurons. At least some learning evidently happens </span><em>within individual neurons. </em></p></div><p><span>That’s bad news for anyone hoping to simulate a brain digitally.  It means there’s a lot more relevant stuff to simulate (like the learning that goes on within cells) than the </span><a href="https://en.wikipedia.org/wiki/Connectionism" rel="">connectionist </a><span>paradigm of treating each biological neuron like a neural-net “neuron” would imply, and thus the computational requirements of simulating a brain are higher — maybe vastly higher — than connectionists hope.</span></p><p><span>On the other hand, intracellular learning is </span><em>great </em><span>news for neuroscientists trying to discover exactly how learning works inside a brain. If you’re studying an example of learning (or classical conditioning) that occurs within a single neuron, then the long-hypothesized “engram”, or physical object corresponding to a piece of newly learned information, has to reside </span><em>inside that neuron</em><span>. Something needs to physically or chemically </span><em>change </em><span>in that neuron, representing the newly learned information, and  causing the corresponding change in the neuron’s firing behavior.</span></p><p><span>Not only can individual Purkinje cells in the cerebellum be classically conditioned, they can also learn information about the </span><em>timing </em><span>of stimuli.</span></p><p><span>If the time interval between the conditioned stimulus and unconditioned stimulus is varied, the Purkinje cells learn to suppress their firing at </span><em>different times </em><span>to match.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-12-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-12-140098227" target="_self" rel="">12</a></span><span> </span></p><p>In other words, if the Purkinje cell has “learned” from experience that an aversive stimulus will typically follow the neutral stimulus in exactly 50 milliseconds, then the Purkinje cell will delay roughly that long after receiving the neutral stimulus before pausing its firing.  If the Purkinje cell “learns” a longer delay, it’ll wait longer before the pause. </p><p><span>This means that something inside the Purkinje cell is capable of representing </span><em>number </em><span>or </span><em>quantity</em><span>. </span></p><p><strong>Cerebellar Purkinje cells, in other words, can count. (Or measure.)</strong></p><p>The fact that individual cerebellar Purkinje neurons contain the ability to measure quantities is highly suggestive about the function of the cerebellum, given the symptoms of cerebellar disease. </p><p><span>Dysmetria, or failure to estimate the right distance and/or speed to move, is a typical symptom of damage to the cerebellum. If </span><em>measurement </em><span>(especially motor timing) is localized to the cerebellum, then dysmetria as a cerebellar deficit makes perfect sense.</span></p><p>Other symptoms of cerebellar damage are easy to understand as consequences of dysmetria. Intention tremor (those big back-and-forth wobbles when the patient tries to move) might simply be what happens when dysmetria causes the movement to initially overshoot, and then an attempt to correct the overshoot itself overshoots and swings in the other direction, and so on. Nystagmus could be the same thing, with the muscles of the eye.  </p><p><span>Even the non-motor symptoms of cerebellar damage, like poor performance on cognitive tests, have sometimes been referred to as a kind of “dysmetria of thought.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-13-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-13-140098227" target="_self" rel="">13</a></span><span> </span></p><p><span>Cerebellar patients have trouble with planning tasks (like the </span><a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi" rel="">Tower of Hanoi</a><span> puzzle), spatial reasoning tasks, and executive function tasks; you might suppose that mentally “gauging” how long to spend doing things, or “relating” subtasks to the whole, as well as understanding how objects fit together in space, are aspects of mental “measurement”.  </span></p><p><span>Cerebellar patients have normal abilities to make grammatical sentences, but make strange errors in generating </span><em>logical </em><span>sentences.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-14-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-14-140098227" target="_self" rel="">14</a></span><span> Things like:</span></p><ul><li><p>“It’s a big job, but it’s not easy.”</p></li><li><p>“If you drive, it will be less crowded”</p></li><li><p>“Although it’s the wrong size for you, I’ll ask to have one that will fit you.”</p></li><li><p>“I never thought I would meet you here; Nor did I, because everything seems so fresh here to buy.”</p></li></ul><p>Conjunctions (like “but”, “although”, “because”, “if”) represent particular logical relationships between parts of sentences. The cerebellar patients’ errors imply a specific impairment in the ability to make those conjunctional relationships.</p><p><span>A sentence that begins “it’s a big job, </span><em>but</em><span>” ought to end with something that would ordinarily seem to conflict with the claim “it’s a big job”; the cerebellar patient instead ended the sentence with something that typically </span><em>reinforces </em><span>the claim (“big jobs” are </span><em>usually </em><span>“not easy”). This error violates the expected logical relationship between clauses.</span></p><p>In a sense this is analogous to a problem in “spatial” or “part-whole” relating — the cerebellar patient has trouble constructing sentences whose clauses relate in the right way to match the conjunction between them. It’s analogous to the way cerebellar patients have the basic sensorimotor ability to do all the same individual movements that healthy people do, but they have trouble sequencing them fluently, “putting the pieces together” in the right way.</p><p>Of course, other areas of the brain are involved in “measurement” or “quantity” activities too. Visual areas of the brain “measure” spatial distances, auditory areas “measure” pitch and rhythm, various parts of the cerebral cortex are active in mental arithmetic, etc. So it’s not that the cerebellum is the sole region that “measures” or “relates” things. But there is something measurement-related going on.</p><p><span>Another broad theory of what the cerebellum is “for” is </span><em>anticipation </em><span>or </span><em>preparation</em><span>.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-15-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-15-140098227" target="_self" rel="">15</a></span></p><p><span>This fits well with the fact that classical conditioning happens in the cerebellum. Classical conditioning consists of learning to </span><em>expect </em><span>one stimulus to follow another, and respond </span><em>in anticipation </em><span>of the expected stimulus.</span></p><p><span>It also fits well with the cerebellum’s role in motor planning and sequencing.  Fluent movement requires unconsciously, rapidly forming intentions to do </span><em>multiple different things in sequence </em><span>without having to stop to think between steps.</span></p><blockquote><p>Gordon Holmes (1939) quotes a cerebellar-lesioned patient as saying, "The movements of my left (unaffected) arm are done subconsciously, but I have to think out each movement of the right (affected) arm. I come to a dead stop in turning and have to think before I start again."</p></blockquote><p><span>This kind of cerebellar damage symptom represents a failure of the </span><em>anticipatory </em><span>or </span><em>preparatory </em><span>function that automatically “gets ready” for the next step before the last one is complete.</span></p><p><span>Patients with cerebellar damage have impaired ability to switch their attentional focus (for instance, between a task that requires watching for visual cues and one that requires listening for auditory cues) but unimpaired ability to perform similar tasks that don’t require shifting focus. Patients with brain lesions </span><em>outside </em><span>the cerebellum didn’t have problems with task-switching.</span></p><p>This also suggests that the cerebellum is involved in the “readiness” or “anticipation” prior to making a mental action.</p><p>Also, experimentally stimulating the cerebellum in animals makes them more sensitive to subsequent sensory stimulus, further supporting the hypothesis that the cerebellum helps organisms “prepare to pay attention”.</p><p>The Purkinje neurons of the cerebellum, in particular, are involved in “preparatory” motor adjustments, such as altering one’s grip on an object in anticipation of the experimenter moving it.</p><p>If the function of the cerebellum is fully general “anticipatory” or “predictive” modeling, this would explain why it’s so important, especially in primate and hominid evolution. Dexterity (tool use, throwing) has obviously been selected for in our primate and hominid ancestors, and so have the general cognitive abilities to predict and make sense of a changing world.</p><p>It also would explain why people without a cerebellum are still capable of most of the same tasks as healthy people, just with worse performance. </p><p><span>People who lack a cerebellum are </span><em>wholly </em><span>incapable of eyeblink classical conditioning, but they’re not wholly incapable of learning or memory; they learn new skills </span><em>slowly</em><span> but they do learn them and they don’t have total amnesia. </span></p><p><span>This would make sense if they are unable to learn to </span><em>instantly anticipate </em><span>context to prepare for upcoming actions, but they are able to learn and form memories through a slower, noisier route using the cerebrum.  They can perform most of the same skills as healthy people, but they have to adjust “by trial and error” instead of using anticipation to get things right the first time, so they’re slower and less accurate.</span></p><p><span>Another way of phrasing this is that the cerebellum is a </span><em>forward model</em><span> of the consequences of action.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-16-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-16-140098227" target="_self" rel="">16</a></span><span> </span></p><p><span>The cerebellum receives an </span><a href="https://en.wikipedia.org/wiki/Efference_copy#:~:text=In%20physiology%2C%20an%20efference%20copy,by%20an%20organism's%20motor%20system." rel="">efference copy </a><span>of motor commands generated by the motor cortex, and uses its model to predict the sensory consequences; it can then compare predicted vs. actual data and error-correct.  A fast, subconscious path for sensorimotor prediction and learning (including classical conditioning) is confined to the cerebellum alone; a slower path includes both the cerebellum and cerebrum. </span></p><p><span>It takes hundreds of milliseconds to consciously perceive sensory information — far too slow a timescale to allow finely tuned and responsive motion that adapts to sensory feedback. Motion in real time has to be shaped and controlled by something faster than sensory processing through the long chains of neurons used for e.g. image recognition in the cortex — but moving totally “blind” without feedback control from </span><em>anything </em><span>would result in unacceptably crude, sloppy, choppy movement. The solution, perhaps, is the “virtual reality” generated by the cerebellum, a predictive model of the world that runs faster than the senses.</span></p><p><span>All vertebrates have a cerebellum.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-17-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-17-140098227" target="_self" rel="">17</a></span></p><p>If you remove the cerebellum from a dogfish, it can still swim, but it has a tendency to “stall” and difficulty judging its turns, so that it often bumps into the sides of the tank.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg" width="800" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:44566,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Pacific spiny dogfish</figcaption></figure></div><p><span>The cerebellum is especially enlarged in fish with electrolocation abilities, like the Peters’s elephant-nose fish </span><em>Gnathonemus petersii, </em><span>an African freshwater fish that uses electricity-sensing receptors all over its body to navigate around obstacles.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg" width="736" height="608" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:608,&quot;width&quot;:736,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:59387,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Peter’s elephant-nose fish</figcaption></figure></div><p>This fish uses its huge cerebellum to monitor the electrical signatures of moving objects in the water around it. Particular Purkinje cells in the elephant-nose fish cerebellum are sensitive to particular target distances and speeds, just as particular neurons in the mammalian visual cortex are sensitive to the shape, location, and angle of visual features.</p><p>This unusual electrosensing function reinforces that the cerebellum is not just a motor control organ, but has a more general function related to spatial and environmental awareness.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif" width="1400" height="809" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:809,&quot;width&quot;:1400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108045,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/avif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The platypus also uses its cerebellum to keep track of electrical signals </figcaption></figure></div><p>Monotremes such as platypuses also have an unusually large cerebellum; like the elephant-nose fish, the platypus is electrosensitive (in its beak) which it uses to detect swimming prey in the water.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg" width="1024" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109585,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The fin whale, whose cerebellum is massively expanded</figcaption></figure></div><p>Aquatic mammals — whales, dolphins, seals, and sea lions — are also notable for their enlarged cerebellums, especially the baleen whales. In marine mammals, the cerebellum is used for echolocation.  Echolocating bats also use the cerebellum for navigation, though their overall cerebellum size relative to brain size is small.</p><p>In many mammals, large areas of the cerebellum are devoted to processing sensory and motor information for parts of the body that are particularly dexterous and used in exploration: the snout in rodents, the hands in primates, the tip of the tail in arboreal monkeys.</p><p>Across animals, the cerebellum seems to be involved in both motion and sensory perception, and intriguingly, seems to be particularly enlarged in animals that use echolocation or electrosensing in the water, for spatial awareness of object locations in all directions.</p><p>This is suggestive of something like “spatial world modeling” going on in the cerebellum, and is consistent with the theory that the cerebellum’s job is anticipation and preparation. </p><p><span>Echolocation and electrosensing are both sensory modalities that involve an organism generating a “field” around itself (of electric or sound waves) and perceiving objects in the patterns of disruption in that field. Unlike vision, hearing, and smell, they are “</span><a href="https://en.wikipedia.org/wiki/Active_sensory_systems" rel="">active sensory systems</a><span>”, in which the organism can control the intensity, direction, and timing of the “probe” signal (the sound or electric signal they emit). </span></p><p>Touch can have an analogous “actively controllable” quality, as the animal reaches out a snout, hand, or tail to explore its nearby surroundings. But truly active sensory systems, unlike touch, allow an animal to explore and probe at a distance, and gain a 3D model of its whole surrounding world. </p><p>While humans don’t have these kinds of sensory systems, it may provide some intuition for what our cerebellum is doing; perhaps building implicit anticipations of where everything in the physical world around us, and how it will respond if “poked”. </p><p><span>If you try to map regions of the cerebellum by function and by functional connectivity</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-18-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-18-140098227" target="_self" rel="">18</a></span><span> you get a close one-to-one match with the cerebrum, even down to the localization of language in one hemisphere (the left hemisphere of the cerebrum and the right hemisphere of the cerebellum; everything’s flipped.)</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-19-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-19-140098227" target="_self" rel="">19</a></span></p><p><span>The only parts of the cerebral cortex </span><em>without </em><span>a corresponding cerebellar region are the auditory and visual cortices. (Suggestive, given that hearing and vision are </span><em>passive </em><span>senses, and my hypothesis in the previous section that the cerebellum has something to do with </span><em>active </em><span>sensing.)</span></p><p><span>The cerebellum has a repeated, almost crystal-like neural structure: it’s divided into multiple identical parallel modules. </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-20-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-20-140098227" target="_self" rel="">20</a></span><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png" width="351" height="585.1986417657046" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:982,&quot;width&quot;:589,&quot;resizeWidth&quot;:351,&quot;bytes&quot;:352847,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The Purkinje cells (PC, in orange) project down into the core of the cerebellum, where they connect to deep nuclei. Climbing fibers (red) feed back up to the Purkinje cells. Mossy fibers (yellow) also feed into the Purkinje cells indirectly, via the granular cells (beige), and the Golgi, stellate, and basket cells (blue). The blue cells inhibit the Purkinje cells, while the red, yellow, and beige ones are excitatory.  </p><p>Basically, this is a feedforward excitatory chain, plus inhibitory feedback loops. The main chain goes: </p><ul><li><p>Afferent neurons (receiving input from the rest of the brain) → </p></li><li><p>mossy fibers →</p></li><li><p>granular cells → </p></li><li><p>parallel fibers →</p></li><li><p>Purkinje cells →</p></li><li><p>output to the rest of the brain</p></li></ul><p>but then there are lots of additional loops where this pathway can be self-inhibiting.</p><p>The primary feedforward chain, though, is a reasonable candidate for the mechanism behind the super-fast “forward model” that generates predictions to inform action faster than sensory processing can generate conscious perceptions.</p><p>On a slightly larger scale, nearby patches of neurons in the cerebellum form pretty much self-contained modules without much connection to cells in other modules.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png" width="1182" height="853" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f4079330-721b-46fb-93d0-ce825523a261_1182x853.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:853,&quot;width&quot;:1182,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:774398,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This is in contrast to the cerebral cortex, which varies a lot in cell composition between regions, has lots of recurrent loops, and lots of cross-connections between neurons from different local columns. The cerebellum is “one and done” — information goes </span><em>in</em><span>, </span><em>through </em><span>the Purkinje cells, and </span><em>out. </em></p><p>There are lots of different such modules, looping the cerebellum together with different parts of the cerebrum. </p><ul><li><p>Loops through the parietal lobes are involved in visual-motor coordination (like reaching the hand out to grasp something.) </p></li><li><p>Loops through the oculomotor cortex are involved in controlling eye movements.</p></li><li><p><span>Loops through the prefrontal cortex are involved in control of attention and working memory, fear extinction learning</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-21-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-21-140098227" target="_self" rel="">21</a></span><span>, mental preparation of imminent actions, and procedural learning.  </span></p></li><li><p>There are other loops (including through the basal ganglia and limbic system) but these have less well understood functions.</p></li></ul><p>The independence of cerebellar modules makes sense given the need for speed — you can’t have long chains of interconnected neurons messing around if you want to give near-real-time model responses to control immediate action.</p><p>People often talk as though “higher” intelligence lives in the cortex, especially the frontal lobes. The “hindbrain” is for boring, animal stuff, like controlling heart rate and hormones. Real thinking happens behind your noble brow.</p><p>This picture, it turns out, is wrong.</p><p><span>Modern </span><em>Homo sapiens </em><span>is as much characterized by our big cerebellums as by our big frontal lobes. </span></p><p><span>Even the most iconically “thought-like” thinking — the phonological loop, i.e. imagined or inner speech — passes through both the cerebrum </span><em>and </em><span>cerebellum. (Speech, after all, is motion, and imagined speech is simulated motion.  I can literally feel subtle movements and tension in my tongue and jaw when I think.)</span></p><p><span>Most things we do have a cerebellar component, what some neuroscientists call a cerebellar </span><em>transform</em><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-22-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-22-140098227" target="_self" rel="">22</a></span><span>, smoothing and tuning and relating and fluently switching between the basic building-block abilities (sensory perception, motion, comprehension) that reside in the cerebrum.  </span></p><p>The cerebellum regulates rate, rhythm, speed, contextual appropriateness; damage it, and the same building block actions are still possible, but all out of whack, clumsy and disproportionate. </p><p><span>This is consistent with the “embodied cognition” worldview where sensorimotor functions are </span><em>on a continuum with </em><span>or </span><em>of the same kind as </em><span>cognitive functions; thought is just “inner” motion and/or “inner” sensation. (And even abstract thought is built up of motions and sensations, via analogy or metaphor.)</span></p><p>The cerebellum may also inspire artificial-intelligence approaches somewhat, especially approaches to robotics or other control, in that it may be be beneficial to include a fast feedforward-only predictive modeling step to control real-time actions, alongside a slower training/updating pathway for model retraining. (I may formalize this more in a later post).</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The optimal amount of fraud is non-zero (2022) (115 pts)]]></title>
            <link>https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/</link>
            <guid>38905889</guid>
            <pubDate>Sun, 07 Jan 2024 22:33:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/">https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=38905889">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>I was recently interviewed by NPR’s Planet Money (<a href="https://www.npr.org/2022/08/26/1119606931/wake-up-and-smell-the-fraud">podcast</a>, <a href="https://www.npr.org/transcripts/1119606931">transcript</a>) regarding a particular form of credit card fraud. One comment which tragically ended on the cutting room floor: "the optimal amount of fraud is greater than zero."</p><p>This is counterintuitive and sounds like it is trying a bit too hard to be clever. You should believe it.</p><h2 id="crime-rates-are-a-policy-choice">Crime rates are a policy choice<br></h2><p>If you enjoy simulation games, you might be familiar with the mechanic where you click a button and some statistic in your civilization moves radically in response. In real life, cause and effect is more subtle, but this relationship exists, and there are (both historically and at this very moment) legal regimes which are radically different than your status quo, and which achieve(d) very different outcomes as a direct consequence of policy decisions.</p><p>A glib way to phrase this is that crime is a policy choice, both definitionally (you could simply agree something was not a crime anymore and bam, crime down) and, more interestingly, because crime responds directly to things which are within your control. Most of the world has taken most of the easy policy choices which have few tradeoffs available! But there are still arbitrarily severe options to control crime from where you are, from “increase the police budget” to “ban alcohol totally” to “implement an Orwellian dystopia.”</p><p>Fraud is a unique subset of crime which occurs, to a major degree, subject to the enforcement efforts of non-state actors. A commanding majority of all fraud which is stopped, detected, adjudicated, and even punished (!) gets those done to it by one or more private sector actors. And the private sector has, in this case, policy decisions to make, which, like the public sector’s decisions, balance the undesirability of fraud against the desirability of social goods such as an open society, easy access to services, and (not least!) making money.</p><h2 id="scoping-down-to-payments-fraud">Scoping down to payments fraud<br></h2><p>To prevent this conversation from being painfully abstract, let’s scope it to one particular type of fraud against one particular type of actor: the bad guy steals a payment credential, like a credit card number, and uses it to extract valuable goods or services from a business. This is an extremely common fraud, costing the world something like $10 to $20 billion a year, and yet it is actually fairly constrained relative to all types of fraud.</p><p>This fraud is possible <em>by design</em>. The very best minds in government, the financial industry, the payments industry, and business have gotten together and decided that they want this fraud to be possible. That probably strikes you as an extraordinary claim, and yet it is true.</p><p>Before we get into the how, let’s get into the why.</p><h2 id="who-pays-for-payments-fraud">Who pays for payments fraud?<br></h2><p>Liability for payments fraud happens in a waterfall, established by a combination of regulation, contracts, and business practice. The specifics get complicated but, for ability to concretely visualize this, consider the case of consumer credit card users in the United States.</p><p>You might assume that, if a credit card is stolen/hacked and used by a bad actor to buy something, the cardholder would be liable. They will suffer the first loss, certainly, but society has decided by regulation (specifically, <a href="https://www.consumerfinance.gov/rules-policy/regulations/1005/">Regulation E</a>) that that loss should flow to their financial institution, less a $50 I-can’t-believe-it’s-not-deductible. As a marketing decision, the U.S. financial industry virtually universally waives that $50.</p><p>The card issuer will, following the credit card brand’s rules (which developed in symbiosis with regulation), automatically seek recovery of the loss from the business’s payments processor. It will, similarly, automatically seek recovery of the loss from the business itself.</p><p>In the overwhelming majority of cases, that is where the waterfall ends. While insurance is available (both specialized chargeback insurance and general business insurance), overwhelmingly businesses simply absorb fraud costs in the same way that they absorb their office rent, staff salaries, and marketing expenses.</p><p>That $10 to $20 billion number we threw around earlier? This is what happens to it, in the ordinary course of business. This allocation of loss is mostly automatic, virtually never involves a court or lawyer, and only sometimes takes human effort at the margin at all.</p><h2 id="fraud-as-a-necessary-business-expense">Fraud as a necessary business expense<br></h2><p>Pretend you are the newly hired Director of Fraud for Business, Inc. You know you are ultimately liable for most fraud that happens in this pattern. What target do you take to the CEO for how much fraud you should suffer?</p><p>Zero?! Do you think the Director of Marketing desires to spend zero on marketing!? That would be an objectively silly goal. They would clearly be fired and replaced with someone who understands marginal returns.</p><p><strong>The marginal return of permitting fraud against you is plausibly greater than zero, and therefore, you should </strong><em><strong>welcome</strong></em><strong> greater than zero fraud.</strong> You can think of it as a necessary expense, just like rent or salary or advertising is. You can even write it off on your taxes. (Ask your accountant; businesses frequently misunderstand the rules here.)</p><p>The reason for this is that Directors of Fraud are aware that the policy choices available to them impact the user experience of <em>fraudsters</em> <em>and legitimate users alike</em>. They want to choose policies which balance the tradeoff of lowering fraud against the ease for legitimate users to transact.</p><h2 id="costs-and-benefits-of-policy-choices-around-trust">Costs and benefits of policy choices around trust</h2><p>Maybe the frame of talking about fraud predisposes people to view the space of choices here negatively. Here’s an equivalent function with different emotional valence: how much do you trust people, and under what circumstances?</p><p>All fraud is a) an abuse of trust causing b) monetary losses for the defrauded and c) monetary gain for the fraudster. You could zero fraud by never trusting anyone in any circumstance.</p><p>Trust, though, is an immensely socially useful technology. Human civilization has a fundamental limitation in that all humans can be trivially killed while sleeping. Huge portions of society’s efforts go toward establishing conditions where this trivial vulnerability virtually never gets exploited. God has, reportedly, closed all bug reports claiming that it is a feature and won’t be patched any time soon.</p><p>Anyhow, trust is also fundamental in commerce, where it’s a layered concept, with different people having different levels of trust in different situations. To increase trust generally tends to frontload the cost to generate that trust, and decrease transactional friction afterwards. You trust your accountant more than most regular employees, you trust your employees more than your customers, you trust your customers more than a person you’ve never met, etc.</p><p>This cost falls on both parties in a trust relationship. To employ an accountant, you (the business) need to identify and interview several prospective accountants and employ one winner for years, and you (the accountant) need to have spent years of your life to get a professional credential and then to have worked your entire career to demonstrate yourself worthy of trust. This is one reason why accountants are routinely trusted with the holiest-of-holies secrets of companies and governments.</p><p>Clearly, e-commerce would cease if, prior to buying a pair of sneakers online, you required someone to go to that degree of effort. You’d almost never lose a pair of sneakers to a fraudster again, but you’d also sell very few sneakers.</p><h2 id="making-a-customer-of-someone-you%E2%80%99ve-never-met">Making a customer of someone you’ve never met<br></h2><p>The payments industry has to solve many foundational problems. One of the core ones is quickly bootstrapping a business over the decision to trust someone they’ve never met, enough to allow them to consume valuable goods and services, based on nothing more than a promise of future payment.</p><p>A promise! Mere words! Billions upon billions of dollars have been spent on marketing to make you think that a payment is more than a promise. It’s a lie, and it’s a lie we all choose to believe in part because it’s a vastly more effective model to run the world under than the truth is.</p><p>Businesses prefer attracting new customers to not attracting new customers, citation hopefully not needed. They have a choice as to how much friction they want that new customer to need to go through prior to being offered goods and services. Many businesses have found that decreasing friction results in getting more new customers, who spend more, and who stick around for more transactions. (These are, incidentally, the “only three goals of marketing.”)</p><p>You could subject first-time customers (or even repeat customers), to an elaborate underwriting process, in part to increase your trust in them / decrease your perception of the risk that they would defraud you. You could, for example, ask them to give you a firm handshake as a condition of doing business.</p><p>The requirement for a firm handshake is, actually, an effective anti-fraud measure. The requirement that it happen face-to-face decreases the number of international professionalized fraud gangs which can target you, because they’re not physically close enough to shake your hand. Unfortunately, for the same reason, it also decreases how many customers you can sell to; most people don’t live within commuting distance of your retail presence.</p><h2 id="anti-fraud-loops-used-in-online-commerce">Anti-fraud loops used in online commerce</h2><p>You’ve probably had a shopping experience impacted by an anti-fraud loop, though you might not have recognized it as such. Ever been asked for billing address in addition to shipping address? That’s for AVS verification. There is an obvious user-experience hit there, and it’s quantifiable; removing fields from checkout forms increases conversion rates nearly as a rule. (Conversion rates are an industry term-of-art describing the percentage of prospects who successfully purchase something.)</p><p>Wonder why everyone under the sun wants you to have an account on their site? One major reason is that it gives customers a history that allows a business to direct more of its anti-fraud attention to (more risky) first-time users than (less risky) multi-year regular customers. Allowing guest checkouts is a business decision to accept more fraud (and less ability to market to the customer) in return for marginal sales.</p><p>Some of the savvier interventions operate in the background or don’t surface for all users. For example, you could imagine asking the purchasers of especially high-risk orders to first confirm possession of a phone number (via typing in a code you text them), or even to talk to a human in your fraud department before completing the transaction. Both of these are aimed at breaking the economics of scaled fraud; phone numbers and voice calls are expensive relative to synthetic identities and tend to leak information about fraud operations, which can further inform defenses.</p><p>We’ll talk about this some other time; risk scoring and marginal interventions is a fascinatingly deep topic.</p><h2 id="different-businesses-have-different-tolerance-for-fraud">Different businesses have different tolerance for fraud</h2><p>Margins create margins. A business with high margins will, all else equal, tend to spend more on marketing and sales than a business with low margins; if they don’t, their competitors will “bid up” the cost of attracting customers out of their own fat margins.</p><p>Businesses with high margins also tend to be more accepting of payments fraud than businesses with low margins. Consider businesses which sell IP, like video game companies, streaming services, or SaaS. Because their margins are often 90%+, if you were to present them with a menu of strategies which traded off conversion rate and fraud rate, they’d maximize for conversion rates until fraud at the margin reached levels not seen in even the most corrupt places imaginable.</p><p>Businesses selling valuable resalable goods with much lower margins, such as Apple hardware or game consoles, have to be <em>much</em> more careful about who they transact with. When they’re offered a conceptual slider for who to do secondary transaction screening on, they screen more marginal orders. They accept painful tradeoffs like, “We’ll have a fraud department review every new order and hold every first-time order for shipping until we can talk to the purchaser.”</p><p>Between these two there exists a spectrum of fraud regimes, and this is broadly a good thing. Society gets to make choices, and here it is choosing through the activities of private agents. It is optimizing for how many resources to let leak to bad actors and much societal effort to burn on policing them versus how much low-friction commerce to enable by good actors. This is often missed in discussions of fraud; one reason it has increased over the past few decades is that legitimate commerce has <em>exploded</em>, as the world becomes richer and as barriers to commerce have come down.</p><h2 id="this-extends-beyond-payments">This extends beyond payments</h2><p>So hopefully you buy that Internet merchants can happily accept non-zero levels of fraud. This argument generalizes, and it has some important ethical considerations. We should, as a society, accept non-zero amounts of benefits fraud. We should accept non-zero amounts of cheating on taxes. You personally have benefited from the financial industry’s decision to not expend the maximum possible effort on defending against so-called identity theft.</p><p>These tradeoffs are often <em>intensely</em> difficult to pursue openly. Who wants to be known as the politician in favor of benefits fraud or the financial CEO who thinks they are not laundering enough money?</p><p>One of the interesting questions here is who gets to resolve tensions like this. Generally speaking, it will be private actors applying their own cost-benefits decisions. There is substantial space for regulations to help with cases, like identity theft, where actors can choose to spend other people’s risk budgets to maximize for their own interests.</p><p>If you have other fraud subtopics you’d love to cover, drop me a line.</p>

        

        <div>
          <h2>Want more essays in your inbox?</h2>
          <p>I write about the intersection of tech and finance, approximately biweekly. It's free.</p>
                  </div>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When random numbers are too random: Low discrepancy sequences (2017) (119 pts)]]></title>
            <link>https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/</link>
            <guid>38905280</guid>
            <pubDate>Sun, 07 Jan 2024 21:24:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/">https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/</a>, See on <a href="https://news.ycombinator.com/item?id=38905280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p>Random numbers can be useful in graphics and game development, but they have a pesky and sometimes undesirable habit of clumping together.</p>
<p>This is a problem in path tracing and monte carlo integration when you take N samples, but the samples aren’t well spread across the sampling range.</p>
<p>This can also be a problem for situations like when you are randomly placing objects in the world or generating treasure for a treasure chest.  You don’t want your randomly placed trees to only be in one part of the forest, and you don’t want a player to get only trash items or only godly items when they open a treasure chest.  Ideally you want to have some randomness, but you don’t want the random number generator to give you all of the same or similar random numbers.</p>
<p>The problem is that random numbers can be TOO random, like in the below where you can see clumps and large gaps between the 100 samples.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniformrandom.png?w=800"></p>
<p>For cases like that, when you want random numbers that are a little bit more well distributed, you might find some use in low discrepancy sequences.</p>
<p>The standalone C++ code (one source file, standard headers, no libraries to link to) I used to generate the data and images are at the bottom of this post, as well as some links to more resources.</p>
<h2>What Is Discrepancy?</h2>
<p>In this context, discrepancy is a measurement of the highest or lowest density of points in a sequence.  High discrepancy means that there is either a large area of empty space, or that there is an area that has a high density of points.  Low discrepancy means that there are neither, and that your points are more or less pretty evenly distributed.</p>
<p>The lowest discrepancy possible has no randomness at all, and in the 1 dimensional case means that the points are evenly distributed on a grid.  For monte carlo integration and the game dev usage cases I mentioned, we do want some randomness, we just want the random points to be spread out a little more evenly.</p>
<p>If more formal math notation is your thing, discrepancy is defined as:<br>
<img src="https://s0.wp.com/latex.php?latex=D_%7BN%7D%28P%29%3D%5Csup+_%7B%7BB%5Cin+J%7D%7D%5Cleft%7C%7B%5Cfrac++%7BA%28B%3BP%29%7D%7BN%7D%7D-%5Clambda+_%7Bs%7D%28B%29%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_%7BN%7D%28P%29%3D%5Csup+_%7B%7BB%5Cin+J%7D%7D%5Cleft%7C%7B%5Cfrac++%7BA%28B%3BP%29%7D%7BN%7D%7D-%5Clambda+_%7Bs%7D%28B%29%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_%7BN%7D%28P%29%3D%5Csup+_%7B%7BB%5Cin+J%7D%7D%5Cleft%7C%7B%5Cfrac++%7BA%28B%3BP%29%7D%7BN%7D%7D-%5Clambda+_%7Bs%7D%28B%29%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="D_{N}(P)=\sup _{{B\in J}}\left|{\frac  {A(B;P)}{N}}-\lambda _{s}(B)\right|"></p>
<p>You can read more about the formal definition here: <a target="_blank" href="https://en.wikipedia.org/wiki/Equidistributed_sequence#Discrepancy">Wikipedia:<br>
 Equidistributed sequence</a></p>
<p>For monte carlo integration specifically, this is the behavior each thing gives you:</p>
<ul>
<li><b>High Discrepancy:</b> Random Numbers / White Noise aka Uniform Distribution – At lower sample counts, convergance is slower (and have higher variance) due to the possibility of not getting good coverage over the area you integrating. At higher sample counts, this problem disappears. (Hint: real time graphics and preview renderings use a smaller number of samples)</li>
<li><b>Lowest Discrepancy:</b> Regular Grid – This will cause aliasing, unlike the other “random” based sampling, which trade aliasing for noise.  Noise is preferred over aliasing.</li>
<li><b>Low Discrepancy:</b> Low Discrepancy Sequences – In lower numbers of samples, this will have faster convergence by having better coverage of the sampling space, but will use randomness to get rid of aliasing by introducing noise.</li>
</ul>
<p>Also interesting to note, <a target="_blank" href="https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method">Quasi Monte Carlo</a> has provably better asymptotic convergence than regular monte carlo integration.</p>
<h2>1 Dimensional Sequences</h2>
<p>We’ll first look at 1 dimensional sequences.</p>
<h2>Grid</h2>
<p>Here are 100 samples evenly spaced:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniform.png?w=800"></p>
<h2>Random Numbers (White Noise)</h2>
<p>This is actually a high discrepancy sequence. To generate this, you just use a standard random number generator to pick 100 points between 0 and 1.  I used std::mt19937 with a std::uniform_real_distribution from 0 to 1:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniformrandom.png?w=800"></p>
<h2>Subrandom Numbers</h2>
<p>Subrandom numbers are ways to decrease the discrepancy of white noise.</p>
<p>One way to do this is to break the sampling space in half.  You then generate even numbered samples in the first half of the space, and odd numbered samples in the second half of the space.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_2.png?w=800"></p>
<p>There’s no reason you can’t generalize this into more divisions of space though.</p>
<p>This splits the space into 4 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_4.png?w=800"></p>
<p>8 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_8.png?w=800"></p>
<p>16 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_16.png?w=800"></p>
<p>32 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_32.png?w=800"></p>
<p>There are other ways to generate subrandom numbers though.  One way is to generate random numbers between 0 and 0.5, and add them to the last sample, plus 0.5.  This gives you a random walk type setup.</p>
<p>Here is that:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandomb.png?w=800"></p>
<h2>Uniform Sampling + Jitter</h2>
<p>If you take the first subrandom idea to the logical maximum, you break your sample space up into N sections and place one point within those N sections to make a low discrepancy sequence made up of N points.</p>
<p>Another way to look at this is that you do uniform sampling, but add some random jitter to the samples, between +/- half a uniform sample size, to keep the samples in their own areas.</p>
<p>This is that:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniformjitter.png?w=800"></p>
<p>I have heard that Pixar invented this technique interestingly.</p>
<h2>Irrational Numbers</h2>
<p>Rational numbers are numbers which can be described as fractions, such as 0.75 which can be expressed as 3/4.  Irrational numbers are numbers which CANNOT be described as fractions, such as pi, or the golden ratio, or the square root of a prime number.</p>
<p>Interestingly you can use irrational numbers to generate low discrepancy sequences.  You start with some value (could be 0, or could be a random number), add the irrational number, and modulus against 1.0.  To get the next sample you add the irrational value again, and modulus against 1.0 again.  Rinse and repeat until you get as many samples as you want.</p>
<p>Some values work better than others though, and apparently the golden ratio is provably the best choice (1.61803398875…), says <a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence#Additive_recurrence" target="_blank">Wikipedia</a>.</p>
<p>Here is the golden ratio, using 4 different random (white noise) starting values:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_000000.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_287194.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_385180.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_775719.png?w=800"></p>
<p>Here I’ve used the square root of 2, with 4 different starting random numbers again:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_000000.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_287194.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_385180.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_775719.png?w=800"></p>
<p>Lastly, here is pi, with 4 random starting values:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_000000.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_287194.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_385180.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_775719.png?w=800"></p>
<h2>Van der Corput Sequence</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Van_der_Corput_sequence" target="_blank">Van der Corput sequence</a> is the 1d equivelant of the Halton sequence which we’ll talk about later.</p>
<p>How you generate values in the Van der Corput sequence is you convert the index of your sample into some base.</p>
<p>For instance if it was base 2, you would convert your index to binary.  If it was base 16, you would convert your index to hexadecimal.</p>
<p>Now, instead of treating the digits as if they are <img src="https://s0.wp.com/latex.php?latex=B%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^0">, <img src="https://s0.wp.com/latex.php?latex=B%5E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^1">, <img src="https://s0.wp.com/latex.php?latex=B%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^2">, etc (where B is the base), you instead treat them as <img src="https://s0.wp.com/latex.php?latex=B%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^{-1}">, <img src="https://s0.wp.com/latex.php?latex=B%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^{-2}">, <img src="https://s0.wp.com/latex.php?latex=B%5E%7B-3%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E%7B-3%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E%7B-3%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^{-3}"> and so on.  In other words, you multiply each digit by a fraction and add up the results.</p>
<p>To show a couple quick examples, let’s say we wanted sample 6 in the sequence of base 2.</p>
<p>First we convert 6 to binary which is 110.  From right to left, we have 3 digits: a 0 in the 1’s place, a 1 in the 2’s place, and a 1 in the 4’s place.  <img src="https://s0.wp.com/latex.php?latex=0%2A1+%2B+1%2A2+%2B+1%2A4+%3D+6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0%2A1+%2B+1%2A2+%2B+1%2A4+%3D+6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0%2A1+%2B+1%2A2+%2B+1%2A4+%3D+6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0*1 + 1*2 + 1*4 = 6">, so we can see that 110 is in fact 6 in binary.</p>
<p>To get the Van der Corput value for this, instead of treating it as the 1’s, 2’s and 4’s digit, we treat it as the 1/2, 1/4 and 1/8’s digit.</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%2A+1%2F2+%2B+1+%2A+1%2F4+%2B+1+%2A+1%2F8+%3D+3%2F8&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0+%2A+1%2F2+%2B+1+%2A+1%2F4+%2B+1+%2A+1%2F8+%3D+3%2F8&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+%2A+1%2F2+%2B+1+%2A+1%2F4+%2B+1+%2A+1%2F8+%3D+3%2F8&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0 * 1/2 + 1 * 1/4 + 1 * 1/8 = 3/8">.</p>
<p>So, sample 6 in the Van der Corput sequence using base 2 is 3/8.</p>
<p>Let’s try sample 21 in base 3.</p>
<p>First we convert 21 to base 3 which is 210.  We can verify this is right by seeing that <img src="https://s0.wp.com/latex.php?latex=0+%2A+1+%2B+1+%2A+3+%2B+2+%2A+9+%3D+21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0+%2A+1+%2B+1+%2A+3+%2B+2+%2A+9+%3D+21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+%2A+1+%2B+1+%2A+3+%2B+2+%2A+9+%3D+21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0 * 1 + 1 * 3 + 2 * 9 = 21">.</p>
<p>Instead of a 1’s, 3’s and 9’s digit, we are going to treat it like a 1/3, 1/9 and 1/27 digit.</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%2A+1%2F3+%2B+1+%2A+1%2F9+%2B+2+%2A+1%2F27+%3D+5%2F27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0+%2A+1%2F3+%2B+1+%2A+1%2F9+%2B+2+%2A+1%2F27+%3D+5%2F27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+%2A+1%2F3+%2B+1+%2A+1%2F9+%2B+2+%2A+1%2F27+%3D+5%2F27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0 * 1/3 + 1 * 1/9 + 2 * 1/27 = 5/27"></p>
<p>So, sample 21 in the Van der Corput sequence using base 3 is 5/27.</p>
<p>Here is the Van der Corput sequence for base 2:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_2.png?w=800"></p>
<p>Here it is for base 3:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_3.png?w=800"></p>
<p>Base 4:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_4.png?w=800"></p>
<p>Base 5:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_5.png?w=800"></p>
<h2>Sobol</h2>
<p>One dimensional Sobol is actually just the Van der Corput sequence base 2 re-arranged a little bit, but it’s generated differently.</p>
<p>You start with 0 (either using it as sample 0 or sample -1, doesn’t matter which), and for each sample you do this:</p>
<ol>
<li>Calculate the Ruler function value for the current sample’s index(more info in a second)</li>
<li>Make the direction vector by shifting 1 left (in binary) 31 – ruler times.</li>
<li>XOR the last sample by the direction vector to get the new sample</li>
<li>To interpret the sample as a floating point number you divide it by <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="2^{32}"></li>
</ol>
<p>That might sound completely different than the Van der Corput sequence but it actually is the same thing – just re-ordered.</p>
<p>In the final step when dividing by <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="2^{32}">, we are really just interpreting the binary number as a fraction just like before, but it’s the LEFT most digit that is the 1/2 spot, not the RIGHT most digit.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Thomae%27s_function#The_ruler_function" target="_blank">Ruler Function</a> goes like:  0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, …</p>
<p>It’s pretty easy to calculate too.  Calculating the ruler function for an index (starting at 1) is just the zero based index of the right most 1’s digit after converting the number to binary.</p>
<p>1 in binary is 001 so Ruler(1) is 0.<br>
2 in binary is 010 so Ruler(2) is 1.<br>
3 in binary is 011 so Ruler(3) is 0.<br>
4 in binary is 100 so Ruler(4) is 2.<br>
5 in binary is 101 so Ruler(5) is 0.<br>
and so on.</p>
<p>Here is 1D Sobol:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dsobol.png?w=800"></p>
<h2>Hammersley</h2>
<p>In one dimension, the Hammersley sequence is the same as the base 2 Van der Corput sequence, and in the same order.  If that sounds strange that it’s the same, it’s a 2d sequence I broke down into a 1d sequence for comparison.  The one thing Hammersley has that makes it unique in the 1d case is that you can truncate bits.</p>
<p>It doesn’t seem that useful for 1d Hammersley to truncate bits but knowing that is useful info too I guess.  Look at the 2d version of Hammersley to get a fairer look at it, because it’s meant to be a 2d sequence.</p>
<p>Here is Hammersley:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dhammersley_0.png?w=800"></p>
<p>With 1 bit truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dhammersley_1.png?w=800"></p>
<p>With 2 bits truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dhammersley_2.png?w=800"></p>
<h2>Poisson Disc</h2>
<p>Poisson disc points are points which are densely packed, but have a minimum distance from each other.</p>
<p>Computer scientists are still working out good algorithms to generate these points efficiently.</p>
<p>I use “Mitchell’s Best-Candidate” which means that when you want to generate a new point in the sequence, you generate N new points, and choose whichever point is farthest away from the other points you’ve generated so far.</p>
<p>Here it is where N is 100:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dpoisson.png?w=800"></p>
<h2>2 Dimensional Sequences</h2>
<p>Next up, let’s look at some 2 dimensional sequences.</p>
<h2>Grid</h2>
<p>Below is 2d uniform samples on a grid.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2duniform.png?w=800"></p>
<p>Note that uniform grid is not particularly low discrepancy for the 2d case! More info here: <a href="https://math.stackexchange.com/questions/2283671/is-it-expected-that-uniform-points-would-have-non-zero-discrepancy/2284163#2284163" target="_blank">Is it expected that uniform points would have non zero discrepancy?</a></p>

<p>Here are 100 random points:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2duniformrandom.png?w=800"></p>
<h2>Uniform Grid + Jitter</h2>
<p>Here is a uniform grid that has random jitter applied to the points.  Jittered grid is a pretty commonly used low discrepancy sampling technique that has good success.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2duniformjitter.png?w=800"></p>
<h2>Subrandom</h2>
<p>Just like in 1 dimensions, you can apply the subrandom ideas to 2 dimensions where you divide the X and Y axis into so many sections, and randomly choose points in the sections.</p>
<p>If you divide X and Y into the same number of sections though, you are going to have a problem because some areas are not going to have any points in them.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_2_2.png?w=800"></p>
<p><a target="_blank" href="https://twitter.com/Reedbeta">@Reedbeta</a> pointed out that instead of using i%x and i%y, that you could use i%x and (i/x)%y to make it pick points in all regions.</p>
<p>Picking different numbers for X and Y can be another way to give good results.  Here’s dividing X and Y into 2 and 3 sections respectively:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_2_3.png?w=800"></p>
<p>If you choose co-prime numbers for divisions for each axis you can get maximal period of repeats.  2 and 3 are coprime so the last example is a good example of that, but here is 3 and 11:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_3_11.png?w=800"></p>
<p>Here is 3 and 97.  97 is large enough that with only doing 100 samples, we are almost doing jittered grid on the y axis.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_3_97.png?w=800"></p>
<p>Here is the other subrandom number from 1d, where we start with a random value for X and Y, and then add a random number between 0 and 0.5 to each, also adding 0.5, to make a “random walk” type setup again:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandomb.png?w=800"></p>
<h2>Halton</h2>
<p>The Halton sequence is just the Van der Corput sequence, but using a different base on each axis.</p>
<p>Here is the Halton sequence where X and Y use bases 2 and 3:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dhalton_2_3.png?w=800"></p>
<p>Here it is using bases 5 and 7:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dhalton_5_7.png?w=800"></p>
<p>Here are bases 13 and 9:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dhalton_13_9.png?w=800"></p>
<h2>Irrational Numbers</h2>
<p>The irrational numbers technique can be used for 2d as well but I wasn’t able to find out how to make it give decent looking output that didn’t have an obvious diagonal pattern in them.  <a target="_blank" href="https://twitter.com/BartWronsk">Bart Wronski</a> shared a neat paper that explains how to use the golden ratio in 2d with great success: <a target="_blank" href="https://www.graphics.rwth-aachen.de/publication/2/jgt.pdf">Golden Ratio Sequences For Low-Discrepancy Sampling</a></p>
<p>This uses the golden ratio for the X axis and the square root of 2 for the Y axis.  Below that is the same, with a random starting point, to make it give a different sequence.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_618034_414214_000000_000000.png?w=800"></p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_618034_414214_775719_264045.png?w=800"></p>
<p>Here X axis uses square root of 2 and Y axis uses square root of 3.  Below that is a random starting point, which gives the same discrepancy.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_414214_732051_000000_000000.png?w=800"></p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_414214_732051_775719_264045.png?w=800"></p>
<h2>Hammersley</h2>
<p>In 2 dimensions, the Hammersley sequence uses the 1d Hammersley sequence for the X axis: Instead of treating the binary version of the index as binary, you treat it as fractions like you do for Van der Corput and sum up the fractions.</p>
<p>For the Y axis, you just reverse the bits and then do the same!</p>
<p>Here is the Hammersley sequence. Note we would have to take 128 samples (not just the 100 we did) if we wanted it to fill the entire square with samples.<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2dhammersley_0.png?w=800"></p>
<p>Truncating bits in 2d is a bit useful. Here is 1 bit truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2dhammersley_1.png?w=800"></p>
<p>2 bits truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2dhammersley_2.png?w=800"></p>
<h2>Poisson Disc</h2>
<p>Using the same method we did for 1d, we can generate points in 2d space:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dpoisson.png?w=800"></p>
<h2>N Rooks</h2>
<p>There is a sampling pattern called N-Rooks where you put N rooks onto a chess board and arrange them such that no two are in the same row or column.</p>
<p>A way to generate these samples is to realize that there will be only one rook per row, and that none of them will ever be in the same column.  So, you make an array that has numbers 0 to N-1, and then shuffle the array.  The index into the array is the row, and the value in the array is the column.</p>
<p>Here are 100 rooks:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2drooks.png?w=800"></p>
<h2>Sobol</h2>
<p>Sobol in two dimensions is more complex to explain so I’ll link you to the source I used: <a href="http://papa.bretmulvey.com/post/153648811993/sobol-sequences-made-simple" target="_blank">Sobol Sequences Made Simple</a>.</p>
<p>The 1D sobol already covered is used for the X axis, and then something more complex was used for the Y axis:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsobol.png?w=800"></p>
<h2>Links</h2>
<p>Bart Wronski has a really great series on a related topic: <a target="_blank" href="https://bartwronski.com/2016/10/30/dithering-in-games-mini-series/">Dithering in Games</a></p>
<p><a target="_blank" href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">Wikipedia: Low Discrepancy Sequence</a></p>
<p><a target="_blank" href="https://en.m.wikipedia.org/wiki/Halton_sequence">Wikipedia: Halton Sequence</a></p>
<p><a target="_blank" href="https://en.m.wikipedia.org/wiki/Van_der_Corput_sequence">Wikipedia: Van der Corput Sequence</a></p>
<p><a target="_blank" href="http://martin.ankerl.com/2009/12/09/how-to-create-random-colors-programmatically/">Using Fibonacci Sequence To Generate Colors</a></p>
<p><a target="_blank" href="http://gruenschloss.org/">Deeper info and usage cases for low discrepancy sequences</a></p>
<p><a target="_blank" href="https://www.jasondavies.com/poisson-disc/">Poisson-Disc Sampling</a></p>
<p>Low discrepancy sequences are related to blue noise.  Where white noise contains all frequencies evenly, blue noise has more high frequencies and fewer low frequencies.  Blue noise is essentially the ultimate in low discrepancy, but can be expensive to compute.  Here are some pages on blue noise:</p>
<p><a target="_blank" href="http://momentsingraphics.de/?p=127">Free Blue Noise Textures</a></p>
<p><a target="_blank" href="http://momentsingraphics.de/?p=148">The problem with 3D blue noise</a></p>
<p><a target="_blank" href="http://www.joesfer.com/?p=108">Stippling and Blue Noise</a></p>
<p><a target="_blank" href="https://mollyrocket.com/casey/stream_0015.html">Vegetation placement in “The Witness”</a></p>
<p>Here are some links from  <a target="_blank" href="https://twitter.com/marc_b_reynolds">@marc_b_reynolds</a>:<br>
Sobol (low-discrepancy) sequence in 1-3D, stratified in 2-4D.<br>
Classic binary-reflected gray code.<br>
<a target="_blank" href="https://github.com/Marc-B-Reynolds/Stand-alone-junk/blob/master/src/SFH/Sobol.h">Sobol.h</a></p>
<p><a target="_blank" href="http://marc-b-reynolds.github.io/math/2016/02/24/weyl.html">Weyl Sequence</a></p>
<h2>Code</h2>
<pre title="">#define _CRT_SECURE_NO_WARNINGS

#include &lt;windows.h&gt;  // for bitmap headers and performance counter.  Sorry non windows people!
#include &lt;vector&gt;
#include &lt;stdint.h&gt;
#include &lt;random&gt;
#include &lt;array&gt;
#include &lt;algorithm&gt;
#include &lt;stdlib.h&gt;
#include &lt;set&gt;

typedef uint8_t uint8;

#define NUM_SAMPLES 100  // to simplify some 2d code, this must be a square
#define NUM_SAMPLES_FOR_COLORING 100

// Turning this on will slow things down significantly because it's an O(N^5) operation for 2d!
#define CALCULATE_DISCREPANCY 0

#define IMAGE1D_WIDTH 600
#define IMAGE1D_HEIGHT 50
#define IMAGE2D_WIDTH 300
#define IMAGE2D_HEIGHT 300
#define IMAGE_PAD   30

#define IMAGE1D_CENTERX ((IMAGE1D_WIDTH+IMAGE_PAD*2)/2)
#define IMAGE1D_CENTERY ((IMAGE1D_HEIGHT+IMAGE_PAD*2)/2)
#define IMAGE2D_CENTERX ((IMAGE2D_WIDTH+IMAGE_PAD*2)/2)
#define IMAGE2D_CENTERY ((IMAGE2D_HEIGHT+IMAGE_PAD*2)/2)

#define AXIS_HEIGHT 40
#define DATA_HEIGHT 20
#define DATA_WIDTH 2

#define COLOR_FILL SColor(255,255,255)
#define COLOR_AXIS SColor(0, 0, 0)

//======================================================================================
struct SImageData
{
    SImageData ()
        : m_width(0)
        , m_height(0)
    { }
  
    size_t m_width;
    size_t m_height;
    size_t m_pitch;
    std::vector&lt;uint8&gt; m_pixels;
};

struct SColor
{
    SColor (uint8 _R = 0, uint8 _G = 0, uint8 _B = 0)
        : R(_R), G(_G), B(_B)
    { }

    uint8 B, G, R;
};

//======================================================================================
bool SaveImage (const char *fileName, const SImageData &amp;image)
{
    // open the file if we can
    FILE *file;
    file = fopen(fileName, "wb");
    if (!file) {
        printf("Could not save %s\n", fileName);
        return false;
    }
  
    // make the header info
    BITMAPFILEHEADER header;
    BITMAPINFOHEADER infoHeader;
  
    header.bfType = 0x4D42;
    header.bfReserved1 = 0;
    header.bfReserved2 = 0;
    header.bfOffBits = 54;
  
    infoHeader.biSize = 40;
    infoHeader.biWidth = (LONG)image.m_width;
    infoHeader.biHeight = (LONG)image.m_height;
    infoHeader.biPlanes = 1;
    infoHeader.biBitCount = 24;
    infoHeader.biCompression = 0;
    infoHeader.biSizeImage = (DWORD) image.m_pixels.size();
    infoHeader.biXPelsPerMeter = 0;
    infoHeader.biYPelsPerMeter = 0;
    infoHeader.biClrUsed = 0;
    infoHeader.biClrImportant = 0;
  
    header.bfSize = infoHeader.biSizeImage + header.bfOffBits;
  
    // write the data and close the file
    fwrite(&amp;header, sizeof(header), 1, file);
    fwrite(&amp;infoHeader, sizeof(infoHeader), 1, file);
    fwrite(&amp;image.m_pixels[0], infoHeader.biSizeImage, 1, file);
    fclose(file);
 
    return true;
}

//======================================================================================
void ImageInit (SImageData&amp; image, size_t width, size_t height)
{
    image.m_width = width;
    image.m_height = height;
    image.m_pitch = 4 * ((width * 24 + 31) / 32);
    image.m_pixels.resize(image.m_pitch * image.m_width);
    std::fill(image.m_pixels.begin(), image.m_pixels.end(), 0);
}

//======================================================================================
void ImageClear (SImageData&amp; image, const SColor&amp; color)
{
    uint8* row = &amp;image.m_pixels[0];
    for (size_t rowIndex = 0; rowIndex &lt; image.m_height; ++rowIndex)
    {
        SColor* pixels = (SColor*)row;
        std::fill(pixels, pixels + image.m_width, color);

        row += image.m_pitch;
    }
}

//======================================================================================
void ImageBox (SImageData&amp; image, size_t x1, size_t x2, size_t y1, size_t y2, const SColor&amp; color)
{
    for (size_t y = y1; y &lt; y2; ++y)
    {
        uint8* row = &amp;image.m_pixels[y * image.m_pitch];
        SColor* start = &amp;((SColor*)row)[x1];
        std::fill(start, start + x2 - x1, color);
    }
}

//======================================================================================
float Distance (float x1, float y1, float x2, float y2)
{
    float dx = (x2 - x1);
    float dy = (y2 - y1);

    return std::sqrtf(dx*dx + dy*dy);
}

//======================================================================================
SColor DataPointColor (size_t sampleIndex)
{
    SColor ret;
    float percent = (float(sampleIndex) / (float(NUM_SAMPLES_FOR_COLORING) - 1.0f));

    ret.R = uint8((1.0f - percent) * 255.0f);
    ret.G = 0;
    ret.B = uint8(percent * 255.0f);

    float mag = (float)sqrt(ret.R*ret.R + ret.G*ret.G + ret.B*ret.B);
    ret.R = uint8((float(ret.R) / mag)*255.0f);
    ret.G = uint8((float(ret.G) / mag)*255.0f);
    ret.B = uint8((float(ret.B) / mag)*255.0f);

    return ret;
}

//======================================================================================
float RandomFloat (float min, float max)
{
    static std::random_device rd;
    static std::mt19937 mt(rd());
    std::uniform_real_distribution&lt;float&gt; dist(min, max);
    return dist(mt);
}

//======================================================================================
size_t Ruler (size_t n)
{
    size_t ret = 0;
    while (n != 0 &amp;&amp; (n &amp; 1) == 0)
    {
        n /= 2;
        ++ret;
    }
    return ret;
}

//======================================================================================
float CalculateDiscrepancy1D (const std::array&lt;float, NUM_SAMPLES&gt;&amp; samples)
{
    // some info about calculating discrepancy
    // https://math.stackexchange.com/questions/1681562/how-to-calculate-discrepancy-of-a-sequence

    // Calculates the discrepancy of this data.
    // Assumes the data is [0,1) for valid sample range
    std::array&lt;float, NUM_SAMPLES&gt; sortedSamples = samples;
    std::sort(sortedSamples.begin(), sortedSamples.end());

    float maxDifference = 0.0f;
    for (size_t startIndex = 0; startIndex &lt;= NUM_SAMPLES; ++startIndex)
    {
        // startIndex 0 = 0.0f.  startIndex 1 = sortedSamples[0]. etc

        float startValue = 0.0f;
        if (startIndex &gt; 0)
            startValue = sortedSamples[startIndex - 1];

        for (size_t stopIndex = startIndex; stopIndex &lt;= NUM_SAMPLES; ++stopIndex)
        {
            // stopIndex 0 = sortedSamples[0].  startIndex[N] = 1.0f. etc

            float stopValue = 1.0f;
            if (stopIndex &lt; NUM_SAMPLES)
                stopValue = sortedSamples[stopIndex];

            float length = stopValue - startValue;

            // open interval (startValue, stopValue)
            size_t countInside = 0;
            for (float sample : samples)
            {
                if (sample &gt; startValue &amp;&amp;
                    sample &lt; stopValue)
                {
                    ++countInside;
                }
            }
            float density = float(countInside) / float(NUM_SAMPLES);
            float difference = std::abs(density - length);
            if (difference &gt; maxDifference)
                maxDifference = difference;

            // closed interval [startValue, stopValue]
            countInside = 0;
            for (float sample : samples)
            {
                if (sample &gt;= startValue &amp;&amp;
                    sample &lt;= stopValue)
                {
                    ++countInside;
                }
            }
            density = float(countInside) / float(NUM_SAMPLES);
            difference = std::abs(density - length);
            if (difference &gt; maxDifference)
                maxDifference = difference;
        }
    }
    return maxDifference;
}

//======================================================================================
float CalculateDiscrepancy2D (const std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt;&amp; samples)
{
    // some info about calculating discrepancy
    // https://math.stackexchange.com/questions/1681562/how-to-calculate-discrepancy-of-a-sequence

    // Calculates the discrepancy of this data.
    // Assumes the data is [0,1) for valid sample range.

    // Get the sorted list of unique values on each axis
    std::set&lt;float&gt; setSamplesX;
    std::set&lt;float&gt; setSamplesY;
    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
    {
        setSamplesX.insert(sample[0]);
        setSamplesY.insert(sample[1]);
    }
    std::vector&lt;float&gt; sortedXSamples;
    std::vector&lt;float&gt; sortedYSamples;
    sortedXSamples.reserve(setSamplesX.size());
    sortedYSamples.reserve(setSamplesY.size());
    for (float f : setSamplesX)
        sortedXSamples.push_back(f);
    for (float f : setSamplesY)
        sortedYSamples.push_back(f);

    // Get the sorted list of samples on the X axis, for faster interval testing
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; sortedSamplesX = samples;
    std::sort(sortedSamplesX.begin(), sortedSamplesX.end(),
        [] (const std::array&lt;float, 2&gt;&amp; itemA, const std::array&lt;float, 2&gt;&amp; itemB)
        {
            return itemA[0] &lt; itemB[0];
        }
    );

    // calculate discrepancy
    float maxDifference = 0.0f;
    for (size_t startIndexY = 0; startIndexY &lt;= sortedYSamples.size(); ++startIndexY)
    {
        float startValueY = 0.0f;
        if (startIndexY &gt; 0)
            startValueY = *(sortedYSamples.begin() + startIndexY - 1);

        for (size_t startIndexX = 0; startIndexX &lt;= sortedXSamples.size(); ++startIndexX)
        {
            float startValueX = 0.0f;
            if (startIndexX &gt; 0)
                startValueX = *(sortedXSamples.begin() + startIndexX - 1);

            for (size_t stopIndexY = startIndexY; stopIndexY &lt;= sortedYSamples.size(); ++stopIndexY)
            {
                float stopValueY = 1.0f;
                if (stopIndexY &lt; sortedYSamples.size())
                    stopValueY = sortedYSamples[stopIndexY];

                for (size_t stopIndexX = startIndexX; stopIndexX &lt;= sortedXSamples.size(); ++stopIndexX)
                {
                    float stopValueX = 1.0f;
                    if (stopIndexX &lt; sortedXSamples.size())
                        stopValueX = sortedXSamples[stopIndexX];

                    // calculate area
                    float length = stopValueX - startValueX;
                    float height = stopValueY - startValueY;
                    float area = length * height;

                    // open interval (startValue, stopValue)
                    size_t countInside = 0;
                    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
                    {
                        if (sample[0] &gt; startValueX &amp;&amp;
                            sample[1] &gt; startValueY &amp;&amp;
                            sample[0] &lt; stopValueX &amp;&amp;
                            sample[1] &lt; stopValueY)
                        {
                            ++countInside;
                        }
                    }
                    float density = float(countInside) / float(NUM_SAMPLES);
                    float difference = std::abs(density - area);
                    if (difference &gt; maxDifference)
                        maxDifference = difference;

                    // closed interval [startValue, stopValue]
                    countInside = 0;
                    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
                    {
                        if (sample[0] &gt;= startValueX &amp;&amp;
                            sample[1] &gt;= startValueY &amp;&amp;
                            sample[0] &lt;= stopValueX &amp;&amp;
                            sample[1] &lt;= stopValueY)
                        {
                            ++countInside;
                        }
                    }
                    density = float(countInside) / float(NUM_SAMPLES);
                    difference = std::abs(density - area);
                    if (difference &gt; maxDifference)
                        maxDifference = difference;
                }
            }
        }
    }

    return maxDifference;
}

//======================================================================================
void Test1D (const char* fileName, const std::array&lt;float, NUM_SAMPLES&gt;&amp; samples)
{
    // create and clear the image
    SImageData image;
    ImageInit(image, IMAGE1D_WIDTH + IMAGE_PAD * 2, IMAGE1D_HEIGHT + IMAGE_PAD * 2);

    // setup the canvas
    ImageClear(image, COLOR_FILL);

    // calculate the discrepancy
    #if CALCULATE_DISCREPANCY
        float discrepancy = CalculateDiscrepancy1D(samples);
        printf("%s Discrepancy = %0.2f%%\n", fileName, discrepancy*100.0f);
    #endif

    // draw the sample points
    size_t i = 0;
    for (float f: samples)
    {
        size_t pos = size_t(f * float(IMAGE1D_WIDTH)) + IMAGE_PAD;
        ImageBox(image, pos, pos + 1, IMAGE1D_CENTERY - DATA_HEIGHT / 2, IMAGE1D_CENTERY + DATA_HEIGHT / 2, DataPointColor(i));
        ++i;
    }

    // draw the axes lines. horizontal first then the two vertical
    ImageBox(image, IMAGE_PAD, IMAGE1D_WIDTH + IMAGE_PAD, IMAGE1D_CENTERY, IMAGE1D_CENTERY + 1, COLOR_AXIS);
    ImageBox(image, IMAGE_PAD, IMAGE_PAD + 1, IMAGE1D_CENTERY - AXIS_HEIGHT / 2, IMAGE1D_CENTERY + AXIS_HEIGHT / 2, COLOR_AXIS);
    ImageBox(image, IMAGE1D_WIDTH + IMAGE_PAD, IMAGE1D_WIDTH + IMAGE_PAD + 1, IMAGE1D_CENTERY - AXIS_HEIGHT / 2, IMAGE1D_CENTERY + AXIS_HEIGHT / 2, COLOR_AXIS);

    // save the image
    SaveImage(fileName, image);
}

//======================================================================================
void Test2D (const char* fileName, const std::array&lt;std::array&lt;float,2&gt;, NUM_SAMPLES&gt;&amp; samples)
{
    // create and clear the image
    SImageData image;
    ImageInit(image, IMAGE2D_WIDTH + IMAGE_PAD * 2, IMAGE2D_HEIGHT + IMAGE_PAD * 2);
    
    // setup the canvas
    ImageClear(image, COLOR_FILL);

    // calculate the discrepancy
    #if CALCULATE_DISCREPANCY
        float discrepancy = CalculateDiscrepancy2D(samples);
        printf("%s Discrepancy = %0.2f%%\n", fileName, discrepancy*100.0f);
    #endif

    // draw the sample points
    size_t i = 0;
    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
    {
        size_t posx = size_t(sample[0] * float(IMAGE2D_WIDTH)) + IMAGE_PAD;
        size_t posy = size_t(sample[1] * float(IMAGE2D_WIDTH)) + IMAGE_PAD;
        ImageBox(image, posx - 1, posx + 1, posy - 1, posy + 1, DataPointColor(i));
        ++i;
    }

    // horizontal lines
    ImageBox(image, IMAGE_PAD - 1, IMAGE2D_WIDTH + IMAGE_PAD + 1, IMAGE_PAD - 1, IMAGE_PAD, COLOR_AXIS);
    ImageBox(image, IMAGE_PAD - 1, IMAGE2D_WIDTH + IMAGE_PAD + 1, IMAGE2D_HEIGHT + IMAGE_PAD, IMAGE2D_HEIGHT + IMAGE_PAD + 1, COLOR_AXIS);

    // vertical lines
    ImageBox(image, IMAGE_PAD - 1, IMAGE_PAD, IMAGE_PAD - 1, IMAGE2D_HEIGHT + IMAGE_PAD + 1, COLOR_AXIS);
    ImageBox(image, IMAGE_PAD + IMAGE2D_WIDTH, IMAGE_PAD + IMAGE2D_WIDTH + 1, IMAGE_PAD - 1, IMAGE2D_HEIGHT + IMAGE_PAD + 1, COLOR_AXIS);

    // save the image
    SaveImage(fileName, image);
}

//======================================================================================
void TestUniform1D (bool jitter)
{
    // calculate the sample points
    const float c_cellSize = 1.0f / float(NUM_SAMPLES+1);
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i] = float(i+1) / float(NUM_SAMPLES+1);
        if (jitter)
            samples[i] += RandomFloat(-c_cellSize*0.5f, c_cellSize*0.5f);
    }

    // save bitmap etc
    if (jitter)
        Test1D("1DUniformJitter.bmp", samples);
    else
        Test1D("1DUniform.bmp", samples);
}

//======================================================================================
void TestUniformRandom1D ()
{
    // calculate the sample points
    const float c_halfJitter = 1.0f / float((NUM_SAMPLES + 1) * 2);
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
        samples[i] = RandomFloat(0.0f, 1.0f);

    // save bitmap etc
    Test1D("1DUniformRandom.bmp", samples);
}

//======================================================================================
void TestSubRandomA1D (size_t numRegions)
{
    const float c_randomRange = 1.0f / float(numRegions);

    // calculate the sample points
    const float c_halfJitter = 1.0f / float((NUM_SAMPLES + 1) * 2);
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i] = RandomFloat(0.0f, c_randomRange);
        samples[i] += float(i % numRegions) / float(numRegions);
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "1DSubRandomA_%zu.bmp", numRegions);
    Test1D(fileName, samples);
}

//======================================================================================
void TestSubRandomB1D ()
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    float sample = RandomFloat(0.0f, 0.5f);
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        sample = std::fmodf(sample + 0.5f + RandomFloat(0.0f, 0.5f), 1.0f);
        samples[i] = sample;
    }

    // save bitmap etc
    Test1D("1DSubRandomB.bmp", samples);
}

//======================================================================================
void TestVanDerCorput (size_t base)
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i] = 0.0f;
        float denominator = float(base);
        size_t n = i;
        while (n &gt; 0)
        {
            size_t multiplier = n % base;
            samples[i] += float(multiplier) / denominator;
            n = n / base;
            denominator *= base;
        }
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "1DVanDerCorput_%zu.bmp", base);
    Test1D(fileName, samples);
}

//======================================================================================
void TestIrrational1D (float irrational, float seed)
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    float sample = seed;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        sample = std::fmodf(sample + irrational, 1.0f);
        samples[i] = sample;
    }

    // save bitmap etc
    char irrationalStr[256];
    sprintf(irrationalStr, "%f", irrational);
    char seedStr[256];
    sprintf(seedStr, "%f", seed);
    char fileName[256];
    sprintf(fileName, "1DIrrational_%s_%s.bmp", &amp;irrationalStr[2], &amp;seedStr[2]);
    Test1D(fileName, samples);
}

//======================================================================================
void TestSobol1D ()
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        size_t ruler = Ruler(i + 1);
        size_t direction = size_t(size_t(1) &lt;&lt; size_t(31 - ruler));
        sampleInt = sampleInt ^ direction;
        samples[i] = float(sampleInt) / std::pow(2.0f, 32.0f);
    }

    // save bitmap etc
    Test1D("1DSobol.bmp", samples);
}

//======================================================================================
void TestHammersley1D (size_t truncateBits)
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        size_t n = i &gt;&gt; truncateBits;
        float base = 1.0f / 2.0f;
        samples[i] = 0.0f;
        while (n)
        {
            if (n &amp; 1)
                samples[i] += base;
            n /= 2;
            base /= 2.0f;
        }
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "1DHammersley_%zu.bmp", truncateBits);
    Test1D(fileName, samples);
}

//======================================================================================
float MinimumDistance1D (const std::array&lt;float, NUM_SAMPLES&gt;&amp; samples, size_t numSamples, float x)
{
    // Used by poisson.
    // This returns the minimum distance that point (x) is away from the sample points, from [0, numSamples).
    float minimumDistance = 0.0f;
    for (size_t i = 0; i &lt; numSamples; ++i)
    {
        float distance = std::abs(samples[i] - x);
        if (i == 0 || distance &lt; minimumDistance)
            minimumDistance = distance;
    }
    return minimumDistance;
}

//======================================================================================
void TestPoisson1D ()
{
    // every time we want to place a point, we generate this many points and choose the one farthest away from all the other points (largest minimum distance)
    const size_t c_bestOfAttempts = 100;

    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t sampleIndex = 0; sampleIndex &lt; NUM_SAMPLES; ++sampleIndex)
    {
        // generate some random points and keep the one that has the largest minimum distance from any of the existing points
        float bestX = 0.0f;
        float bestMinDistance = 0.0f;
        for (size_t attempt = 0; attempt &lt; c_bestOfAttempts; ++attempt)
        {
            float attemptX = RandomFloat(0.0f, 1.0f);
            float minDistance = MinimumDistance1D(samples, sampleIndex, attemptX);

            if (minDistance &gt; bestMinDistance)
            {
                bestX = attemptX;
                bestMinDistance = minDistance;
            }
        }
        samples[sampleIndex] = bestX;
    }

    // save bitmap etc
    Test1D("1DPoisson.bmp", samples);
}

//======================================================================================
void TestUniform2D (bool jitter)
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    const size_t c_oneSide = size_t(std::sqrt(NUM_SAMPLES));
    const float c_cellSize = 1.0f / float(c_oneSide+1);
    for (size_t iy = 0; iy &lt; c_oneSide; ++iy)
    {
        for (size_t ix = 0; ix &lt; c_oneSide; ++ix)
        {
            size_t sampleIndex = iy * c_oneSide + ix;

            samples[sampleIndex][0] = float(ix + 1) / (float(c_oneSide + 1));
            if (jitter)
                samples[sampleIndex][0] += RandomFloat(-c_cellSize*0.5f, c_cellSize*0.5f);

            samples[sampleIndex][1] = float(iy + 1) / (float(c_oneSide) + 1.0f);
            if (jitter)
                samples[sampleIndex][1] += RandomFloat(-c_cellSize*0.5f, c_cellSize*0.5f);
        }
    }

    // save bitmap etc
    if (jitter)
        Test2D("2DUniformJitter.bmp", samples);
    else
        Test2D("2DUniform.bmp", samples);
}

//======================================================================================
void TestUniformRandom2D ()
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    const size_t c_oneSide = size_t(std::sqrt(NUM_SAMPLES));
    const float c_halfJitter = 1.0f / float((c_oneSide + 1) * 2);
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i][0] = RandomFloat(0.0f, 1.0f);
        samples[i][1] = RandomFloat(0.0f, 1.0f);
    }

    // save bitmap etc
    Test2D("2DUniformRandom.bmp", samples);
}

//======================================================================================
void TestSubRandomA2D (size_t regionsX, size_t regionsY)
{
    const float c_randomRangeX = 1.0f / float(regionsX);
    const float c_randomRangeY = 1.0f / float(regionsY);

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i][0] = RandomFloat(0.0f, c_randomRangeX);
        samples[i][0] += float(i % regionsX) / float(regionsX);

        samples[i][1] = RandomFloat(0.0f, c_randomRangeY);
        samples[i][1] += float(i % regionsY) / float(regionsY);
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "2DSubRandomA_%zu_%zu.bmp", regionsX, regionsY);
    Test2D(fileName, samples);
}

//======================================================================================
void TestSubRandomB2D ()
{
    // calculate the sample points
    float samplex = RandomFloat(0.0f, 0.5f);
    float sampley = RandomFloat(0.0f, 0.5f);
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samplex = std::fmodf(samplex + 0.5f + RandomFloat(0.0f, 0.5f), 1.0f);
        sampley = std::fmodf(sampley + 0.5f + RandomFloat(0.0f, 0.5f), 1.0f);
        samples[i][0] = samplex;
        samples[i][1] = sampley;
    }
    
    // save bitmap etc
    Test2D("2DSubRandomB.bmp", samples);
}

//======================================================================================
void TestHalton (size_t basex, size_t basey)
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    const size_t c_oneSide = size_t(std::sqrt(NUM_SAMPLES));
    const float c_halfJitter = 1.0f / float((c_oneSide + 1) * 2);
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        // x axis
        samples[i][0] = 0.0f;
        {
            float denominator = float(basex);
            size_t n = i;
            while (n &gt; 0)
            {
                size_t multiplier = n % basex;
                samples[i][0] += float(multiplier) / denominator;
                n = n / basex;
                denominator *= basex;
            }
        }

        // y axis
        samples[i][1] = 0.0f;
        {
            float denominator = float(basey);
            size_t n = i;
            while (n &gt; 0)
            {
                size_t multiplier = n % basey;
                samples[i][1] += float(multiplier) / denominator;
                n = n / basey;
                denominator *= basey;
            }
        }
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "2DHalton_%zu_%zu.bmp", basex, basey);
    Test2D(fileName, samples);
}

//======================================================================================
void TestSobol2D ()
{
    // calculate the sample points

    // x axis
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        size_t ruler = Ruler(i + 1);
        size_t direction = size_t(size_t(1) &lt;&lt; size_t(31 - ruler));
        sampleInt = sampleInt ^ direction;
        samples[i][0] = float(sampleInt) / std::pow(2.0f, 32.0f);
    }

    // y axis
    // Code adapted from http://web.maths.unsw.edu.au/~fkuo/sobol/
    // uses numbers: new-joe-kuo-6.21201

    // Direction numbers
    std::vector&lt;size_t&gt; V;
    V.resize((size_t)ceil(log((double)NUM_SAMPLES) / log(2.0)));
    V[0] = size_t(1) &lt;&lt; size_t(31);
    for (size_t i = 1; i &lt; V.size(); ++i)
        V[i] = V[i - 1] ^ (V[i - 1] &gt;&gt; 1);

    // Samples
    sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i) {
        size_t ruler = Ruler(i + 1);
        sampleInt = sampleInt ^ V[ruler];
        samples[i][1] = float(sampleInt) / std::pow(2.0f, 32.0f);
    }

    // save bitmap etc
    Test2D("2DSobol.bmp", samples);
}

//======================================================================================
void TestHammersley2D (size_t truncateBits)
{
    // figure out how many bits we are working in.
    size_t value = 1;
    size_t numBits = 0;
    while (value &lt; NUM_SAMPLES)
    {
        value *= 2;
        ++numBits;
    }

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        // x axis
        samples[i][0] = 0.0f;
        {
            size_t n = i &gt;&gt; truncateBits;
            float base = 1.0f / 2.0f;
            while (n)
            {
                if (n &amp; 1)
                    samples[i][0] += base;
                n /= 2;
                base /= 2.0f;
            }
        }

        // y axis
        samples[i][1] = 0.0f;
        {
            size_t n = i &gt;&gt; truncateBits;
            size_t mask = size_t(1) &lt;&lt; (numBits - 1 - truncateBits);

            float base = 1.0f / 2.0f;
            while (mask)
            {
                if (n &amp; mask)
                    samples[i][1] += base;
                mask /= 2;
                base /= 2.0f;
            }
        }
    }


    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "2DHammersley_%zu.bmp", truncateBits);
    Test2D(fileName, samples);
}

//======================================================================================
void TestRooks2D ()
{
    // make and shuffle rook positions
    std::random_device rd;
    std::mt19937 mt(rd());
    std::array&lt;size_t, NUM_SAMPLES&gt; rookPositions;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
        rookPositions[i] = i;
    std::shuffle(rookPositions.begin(), rookPositions.end(), mt);

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        // x axis
        samples[i][0] = float(rookPositions[i]) / float(NUM_SAMPLES-1);

        // y axis
        samples[i][1] = float(i) / float(NUM_SAMPLES - 1);
    }

    // save bitmap etc
    Test2D("2DRooks.bmp", samples);
}

//======================================================================================
void TestIrrational2D (float irrationalx, float irrationaly, float seedx, float seedy)
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    float samplex = seedx;
    float sampley = seedy;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samplex = std::fmodf(samplex + irrationalx, 1.0f);
        sampley = std::fmodf(sampley + irrationaly, 1.0f);

        samples[i][0] = samplex;
        samples[i][1] = sampley;
    }

    // save bitmap etc
    char irrationalxStr[256];
    sprintf(irrationalxStr, "%f", irrationalx);
    char irrationalyStr[256];
    sprintf(irrationalyStr, "%f", irrationaly);
    char seedxStr[256];
    sprintf(seedxStr, "%f", seedx);
    char seedyStr[256];
    sprintf(seedyStr, "%f", seedy);
    char fileName[256];
    sprintf(fileName, "2DIrrational_%s_%s_%s_%s.bmp", &amp;irrationalxStr[2], &amp;irrationalyStr[2], &amp;seedxStr[2], &amp;seedyStr[2]);
    Test2D(fileName, samples);
}

//======================================================================================
float MinimumDistance2D (const std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt;&amp; samples, size_t numSamples, float x, float y)
{
    // Used by poisson.
    // This returns the minimum distance that point (x,y) is away from the sample points, from [0, numSamples).
    float minimumDistance = 0.0f;
    for (size_t i = 0; i &lt; numSamples; ++i)
    {
        float distance = Distance(samples[i][0], samples[i][1], x, y);
        if (i == 0 || distance &lt; minimumDistance)
            minimumDistance = distance;
    }
    return minimumDistance;
}

//======================================================================================
void TestPoisson2D ()
{
    // every time we want to place a point, we generate this many points and choose the one farthest away from all the other points (largest minimum distance)
    const size_t c_bestOfAttempts = 100;

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t sampleIndex = 0; sampleIndex &lt; NUM_SAMPLES; ++sampleIndex)
    {
        // generate some random points and keep the one that has the largest minimum distance from any of the existing points
        float bestX = 0.0f;
        float bestY = 0.0f;
        float bestMinDistance = 0.0f;
        for (size_t attempt = 0; attempt &lt; c_bestOfAttempts; ++attempt)
        {
            float attemptX = RandomFloat(0.0f, 1.0f);
            float attemptY = RandomFloat(0.0f, 1.0f);
            float minDistance = MinimumDistance2D(samples, sampleIndex, attemptX, attemptY);

            if (minDistance &gt; bestMinDistance)
            {
                bestX = attemptX;
                bestY = attemptY;
                bestMinDistance = minDistance;
            }
        }
        samples[sampleIndex][0] = bestX;
        samples[sampleIndex][1] = bestY;
    }

    // save bitmap etc
    Test2D("2DPoisson.bmp", samples);
}

//======================================================================================
int main (int argc, char **argv)
{
    // 1D tests
    {
        TestUniform1D(false);
        TestUniform1D(true);

        TestUniformRandom1D();

        TestSubRandomA1D(2);
        TestSubRandomA1D(4);
        TestSubRandomA1D(8);
        TestSubRandomA1D(16);
        TestSubRandomA1D(32);

        TestSubRandomB1D();

        TestVanDerCorput(2);
        TestVanDerCorput(3);
        TestVanDerCorput(4);
        TestVanDerCorput(5);

        // golden ratio mod 1 aka (sqrt(5) - 1)/2
        TestIrrational1D(0.618034f, 0.0f);
        TestIrrational1D(0.618034f, 0.385180f);
        TestIrrational1D(0.618034f, 0.775719f);
        TestIrrational1D(0.618034f, 0.287194f);

        // sqrt(2) - 1
        TestIrrational1D(0.414214f, 0.0f);
        TestIrrational1D(0.414214f, 0.385180f);
        TestIrrational1D(0.414214f, 0.775719f);
        TestIrrational1D(0.414214f, 0.287194f);

        // PI mod 1
        TestIrrational1D(0.141593f, 0.0f);
        TestIrrational1D(0.141593f, 0.385180f);
        TestIrrational1D(0.141593f, 0.775719f);
        TestIrrational1D(0.141593f, 0.287194f);
        
        TestSobol1D();

        TestHammersley1D(0);
        TestHammersley1D(1);
        TestHammersley1D(2);

        TestPoisson1D();
    }

    // 2D tests
    {
        TestUniform2D(false);
        TestUniform2D(true);

        TestUniformRandom2D();

        TestSubRandomA2D(2, 2);
        TestSubRandomA2D(2, 3);
        TestSubRandomA2D(3, 11);
        TestSubRandomA2D(3, 97);

        TestSubRandomB2D();

        TestHalton(2, 3);
        TestHalton(5, 7);
        TestHalton(13, 9);

        TestSobol2D();

        TestHammersley2D(0);
        TestHammersley2D(1);
        TestHammersley2D(2);

        TestRooks2D();

        // X axis = golden ratio mod 1 aka (sqrt(5)-1)/2
        // Y axis = sqrt(2) mod 1
        TestIrrational2D(0.618034f, 0.414214f, 0.0f, 0.0f);
        TestIrrational2D(0.618034f, 0.414214f, 0.775719f, 0.264045f);

        // X axis = sqrt(2) mod 1
        // Y axis = sqrt(3) mod 1
        TestIrrational2D(std::fmodf((float)std::sqrt(2.0f), 1.0f), std::fmodf((float)std::sqrt(3.0f), 1.0f), 0.0f, 0.0f);
        TestIrrational2D(std::fmodf((float)std::sqrt(2.0f), 1.0f), std::fmodf((float)std::sqrt(3.0f), 1.0f), 0.775719f, 0.264045f);

        TestPoisson2D();
    }

    #if CALCULATE_DISCREPANCY
        printf("\n");
        system("pause");
    #endif
}
</pre>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shecc: A self-hosting and educational C optimizing compiler (114 pts)]]></title>
            <link>https://github.com/sysprog21/shecc</link>
            <guid>38905182</guid>
            <pubDate>Sun, 07 Jan 2024 21:12:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sysprog21/shecc">https://github.com/sysprog21/shecc</a>, See on <a href="https://news.ycombinator.com/item?id=38905182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">shecc : self-hosting and educational C optimizing compiler</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/18013815/91671374-b2f0db00-eb58-11ea-8d55-858e9fb160c0.png"><img src="https://user-images.githubusercontent.com/18013815/91671374-b2f0db00-eb58-11ea-8d55-858e9fb160c0.png" alt="logo image" width="40%"></a></p>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto"><code>shecc</code> is built from scratch, targeting both 32-bit Arm and RISC-V architectures,
as a self-compiling compiler for a subset of the C language.
Despite its simplistic nature, it is capable of performing basic optimization strategies as a standalone optimizing compiler.</p>
<h3 tabindex="-1" dir="auto">Features</h3>
<ul dir="auto">
<li>Generate executable Linux ELF binaries for ARMv7-A and RV32IM.</li>
<li>Provide a minimal C standard library for basic I/O on GNU/Linux.</li>
<li>The cross-compiler is written in ANSI C, making it compatible with most platforms.</li>
<li>Include a self-contained C front-end with an integrated machine code generator; no external assembler or linker needed.</li>
<li>Utilize a two-pass compilation process: the first pass checks syntax and breaks down complex statements into basic operations,
while the second pass translates these operations into Arm/RISC-V machine code.</li>
<li>Develop a register allocation system that is compatible with RISC-style architectures.</li>
<li>Implement an architecture-independent, single static assignment (SSA)-based middle-end for enhanced optimizations.</li>
</ul>
<h2 tabindex="-1" dir="auto">Compatibility</h2>
<p dir="auto"><code>shecc</code> is capable of compiling C source files written in the following
syntax:</p>
<ul dir="auto">
<li>data types: char, int, struct, and pointer</li>
<li>condition statements: if, while, for, switch, case, break, return, and
general expressions</li>
<li>compound assignments: <code>+=</code>, <code>-=</code>, <code>*=</code></li>
<li>global/local variable initializations for supported data types
<ul dir="auto">
<li>e.g. <code>int i = [expr]</code></li>
</ul>
</li>
<li>limited support for preprocessor directives: <code>#define</code>, <code>#ifdef</code>, <code>#elif</code>, <code>#endif</code>, <code>#undef</code>, and <code>#error</code></li>
<li>non-nested variadic macros with <code>__VA_ARGS__</code> identifier</li>
</ul>
<p dir="auto">The backend targets armv7hf with Linux ABI, verified on Raspberry Pi 3,
and also supports RISC-V 32-bit architecture, verified with QEMU.</p>
<h2 tabindex="-1" dir="auto">Bootstrapping</h2>
<p dir="auto">The steps to validate <code>shecc</code> bootstrapping:</p>
<ol dir="auto">
<li><code>stage0</code>: <code>shecc</code> source code is initially compiled using an ordinary compiler
which generates a native executable. The generated compiler can be used as a
cross-compiler.</li>
<li><code>stage1</code>: The built binary reads its own source code as input and generates an
ARMv7-A/RV32IM  binary.</li>
<li><code>stage2</code>: The generated ARMv7-A/RV32IM binary is invoked (via QEMU or running on
Arm and RISC-V devices) with its own source code as input and generates another
ARMv7-A/RV32IM binary.</li>
<li><code>bootstrap</code>: Build the <code>stage1</code> and <code>stage2</code> compilers, and verify that they are
byte-wise identical. If so, <code>shecc</code> can compile its own source code and produce
new versions of that same program.</li>
</ol>
<h2 tabindex="-1" dir="auto">Prerequisites</h2>
<p dir="auto">Code generator in <code>shecc</code> does not rely on external utilities. You only need
ordinary C compilers such as <code>gcc</code> and <code>clang</code>. However, <code>shecc</code> would bootstrap
itself, and Arm/RISC-V ISA emulation is required. Install QEMU for Arm/RISC-V user
emulation on GNU/Linux:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo apt-get install qemu-user"><pre>$ sudo apt-get install qemu-user</pre></div>
<p dir="auto">It is still possible to build <code>shecc</code> on macOS or Microsoft Windows. However,
the second stage bootstrapping would fail due to <code>qemu-arm</code> absence.</p>
<p dir="auto">To execute the snapshot test, install the packages below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo apt-get install graphviz jq"><pre>$ sudo apt-get install graphviz jq</pre></div>
<h2 tabindex="-1" dir="auto">Build and Verify</h2>
<p dir="auto">Configure which backend you want, <code>shecc</code> supports ARMv7-A and RV32IM backend:</p>
<div data-snippet-clipboard-copy-content="$ make config ARCH=arm
# Target machine code switch to Arm

$ make config ARCH=riscv
# Target machine code switch to RISC-V"><pre><code>$ make config ARCH=arm
# Target machine code switch to Arm

$ make config ARCH=riscv
# Target machine code switch to RISC-V
</code></pre></div>
<p dir="auto">Run <code>make</code> and you should see this:</p>
<div data-snippet-clipboard-copy-content="  CC+LD	out/inliner
  GEN	out/libc.inc
  CC	out/src/main.o
  LD	out/shecc
  SHECC	out/shecc-stage1.elf
  SHECC	out/shecc-stage2.elf"><pre><code>  CC+LD	out/inliner
  GEN	out/libc.inc
  CC	out/src/main.o
  LD	out/shecc
  SHECC	out/shecc-stage1.elf
  SHECC	out/shecc-stage2.elf
</code></pre></div>
<p dir="auto">File <code>out/shecc</code> is the first stage compiler. Its usage:</p>
<div data-snippet-clipboard-copy-content="shecc [-o output] [--no-libc] [--dump-ir] <infile.c>"><pre><code>shecc [-o output] [--no-libc] [--dump-ir] &lt;infile.c&gt;
</code></pre></div>
<p dir="auto">Compiler options:</p>
<ul dir="auto">
<li><code>-o</code> : output file name (default: out.elf)</li>
<li><code>--no-libc</code> : Exclude embedded C library (default: embedded)</li>
<li><code>--dump-ir</code> : Dump intermediate representation (IR)</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ out/shecc -o fib tests/fib.c
$ chmod +x fib
$ qemu-arm fib"><pre>$ out/shecc -o fib tests/fib.c
$ chmod +x fib
$ qemu-arm fib</pre></div>
<p dir="auto">Verify that the emitted IRs are identical to the snapshots by specifying <code>check-snapshots</code> target when invoking <code>make</code>:</p>

<p dir="auto"><code>shecc</code> comes with unit tests. To run the tests, give <code>check</code> as an argument:</p>

<p dir="auto">Reference output:</p>
<div data-snippet-clipboard-copy-content="...
int main(int argc, int argv) { exit(sizeof(char)); } => 1
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; case 3: a = 10; break; case 1: return 0; } exit(a); } => 10
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; default: a = 10; break; } exit(a); } => 10
OK"><pre><code>...
int main(int argc, int argv) { exit(sizeof(char)); } =&gt; 1
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; case 3: a = 10; break; case 1: return 0; } exit(a); } =&gt; 10
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; default: a = 10; break; } exit(a); } =&gt; 10
OK
</code></pre></div>
<p dir="auto">To clean up the generated compiler files, execute the command <code>make clean</code>.
For resetting architecture configurations, use the command <code>make distclean</code>.</p>
<h2 tabindex="-1" dir="auto">Intermediate Representation</h2>
<p dir="auto">Once the option <code>--dump-ir</code> is passed to <code>shecc</code>, the intermediate representation (IR)
will be generated. Take the file <code>tests/fib.c</code> for example. It consists of a recursive
Fibonacci sequence function.</p>
<div dir="auto" data-snippet-clipboard-copy-content="int fib(int n)
{
    if (n == 0)
        return 0;
    else if (n == 1)
        return 1;
    return fib(n - 1) + fib(n - 2);
}"><pre><span>int</span> <span>fib</span>(<span>int</span> <span>n</span>)
{
    <span>if</span> (<span>n</span> <span>==</span> <span>0</span>)
        <span>return</span> <span>0</span>;
    <span>else</span> <span>if</span> (<span>n</span> <span>==</span> <span>1</span>)
        <span>return</span> <span>1</span>;
    <span>return</span> <span>fib</span>(<span>n</span> <span>-</span> <span>1</span>) <span>+</span> <span>fib</span>(<span>n</span> <span>-</span> <span>2</span>);
}</pre></div>
<p dir="auto">Execute the following to generate IR:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ out/shecc --dump-ir -o fib tests/fib.c"><pre>$ out/shecc --dump-ir -o fib tests/fib.c</pre></div>
<p dir="auto">Line-by-line explanation between C source and IR:</p>
<div dir="auto" data-snippet-clipboard-copy-content=" C Source                IR                                         Explanation
------------------+---------------------------------------+----------------------------------------------------------------------------------

int fib(int n)      def int @fib(int %n)                    Indicate a function definition
{                   {
  if (n == 0)         const %.t1001, $0                     Load constant 0 into a temporary variable &quot;.t1001&quot;
                      %.t1002 = eq %n, %.t1001              Test &quot;n&quot; equals &quot;.t1001&quot; or not, and write the result in temporary variable &quot;.t1002&quot;
                      br %.t1002, .label.1177, .label.1178  If &quot;.t1002&quot; equals zero, goto false label &quot;.label.1178&quot;, otherwise,
                                                            goto true label &quot;.label.1177&quot;
                    .label.1177
    return 0;         const %.t1003, $0                     Load constant 0 into a temporary variable &quot;.t1003&quot;
                      ret %.t1003                           Return &quot;.t1003&quot;
                      j .label.1184                         Jump to endif label &quot;.label.1184&quot;
                    .label.1178
  else if (n == 1)    const %.t1004, $1                     Load constant 1 into a temporary variable &quot;.t1004&quot;
                      %.t1005 = eq %n, %.t1004              Test &quot;n&quot; equals &quot;.t1004&quot; or not, and write the result in temporary variable &quot;.t1005&quot;
                      br %.t1005, .label.1183, .label.1184  If &quot;.t1005&quot; equals zero, goto false label &quot;.label.1184&quot;. Otherwise,
                                                            goto true label &quot;.label.1183&quot;
                    .label.1183
    return 1;         const %.t1006, $1                     Load constant 1 into a temporary variable &quot;.t1006&quot;
                      ret %.t1006                           Return &quot;.t1006&quot;
                    .label.1184
  return
    fib(n - 1)        const %.t1007, $1                     Load constant 1 into a temporary variable &quot;.t1007&quot;
                      %.t1008 = sub %n, %.t1007             Subtract &quot;.t1007&quot; from &quot;n&quot;, and store the result in temporary variable &quot;.t1008&quot;
                      push %.t1008                          Prepare parameter for function call
                      call @fib, 1                          Call function &quot;fib&quot; with one parameter
    +                 retval %.t1009                        Store return value in temporary variable &quot;.t1009&quot;
    fib(n - 2);       const %.t1010, $2                     Load constant 2 into a temporary variable &quot;.t1010&quot;
                      %.t1011 = sub %n, %.t1010             Subtract &quot;.t1010&quot; from &quot;n&quot;, and store the result in temporary variable &quot;.t1011&quot;
                      push %.t1011                          Prepare parameter for function call
                      call @fib, 1                          Call function &quot;fib&quot; with one parameter
                      retval %.t1012                        Store return value in temporary variable &quot;.t1012&quot;
                      %.t1013 = add %.t1009, %.t1012        Add &quot;.t1009&quot; and &quot;.t1012&quot;, and store the result in temporary variable &quot;.t1013&quot;
                      ret %.t1013                           Return &quot;.t1013&quot;
}                   }"><pre> <span>C</span> <span>Source</span>                <span>IR</span>                                         <span>Explanation</span>
<span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>+</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>-</span><span>+</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span>

<span>int</span> <span>fib</span>(<span>int</span> <span>n</span>)      <span>def</span> <span>int</span> @<span>fib</span>(<span>int</span> %<span>n</span>)                    <span>Indicate</span> <span>a</span> <span>function</span> <span>definition</span>
{                   {
  <span>if</span> (<span>n</span> <span>==</span> <span>0</span>)         <span>const</span> %.<span>t1001</span>, $<span>0</span>                     <span>Load</span> <span>constant</span> <span>0</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> <span>".t1001"</span>
                      %.<span>t1002</span> <span>=</span> <span>eq</span> %<span>n</span>, %.<span>t1001</span>              <span>Test</span> <span>"n"</span> <span>equals</span> <span>".t1001"</span> <span>or</span> <span>not</span>, <span>and</span> <span>write</span> <span>the</span> <span>result</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1002"</span>
                      <span>br</span> %.<span>t1002</span>, .<span>label</span>.<span>1177</span>, .<span>label</span>.<span>1178</span>  <span>If</span> <span>".t1002"</span> <span>equals</span> <span>zero</span>, <span>goto</span> <span>false</span> <span>label</span> <span>".label.1178"</span>, <span>otherwise</span>,
                                                            <span>goto</span> <span>true</span> <span>label</span> <span>".label.1177"</span>
                    .<span>label</span>.<span>1177</span>
    <span>return</span> <span>0</span>;         <span>const</span> %.<span>t1003</span>, $<span>0</span>                     <span>Load</span> <span>constant</span> <span>0</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1003</span>"
                      <span>ret</span> %.<span>t1003</span>                           <span>Return</span> ".<span>t1003</span>"
                      <span>j</span> .<span>label</span><span>.1184</span>                         <span>Jump</span> <span>to</span> <span>endif</span> <span>label</span> <span>".label.1184"</span>
                    .<span>label</span>.<span>1178</span>
  <span>else</span> <span>if</span> (<span>n</span> <span>==</span> <span>1</span>)    <span>const</span> %.<span>t1004</span>, $<span>1</span>                     <span>Load</span> <span>constant</span> <span>1</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> <span>".t1004"</span>
                      %.<span>t1005</span> <span>=</span> <span>eq</span> %<span>n</span>, %.<span>t1004</span>              <span>Test</span> <span>"n"</span> <span>equals</span> <span>".t1004"</span> <span>or</span> <span>not</span>, <span>and</span> <span>write</span> <span>the</span> <span>result</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1005"</span>
                      <span>br</span> %.<span>t1005</span>, .<span>label</span>.<span>1183</span>, .<span>label</span>.<span>1184</span>  <span>If</span> <span>".t1005"</span> <span>equals</span> <span>zero</span>, <span>goto</span> <span>false</span> <span>label</span> <span>".label.1184"</span>. <span>Otherwise</span>,
                                                            <span>goto</span> <span>true</span> <span>label</span> <span>".label.1183"</span>
                    .<span>label</span>.<span>1183</span>
    <span>return</span> <span>1</span>;         <span>const</span> %.<span>t1006</span>, $<span>1</span>                     <span>Load</span> <span>constant</span> <span>1</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1006</span>"
                      <span>ret</span> %.<span>t1006</span>                           <span>Return</span> ".<span>t1006</span>"
                    .<span>label</span><span>.1184</span>
  <span>return</span>
    <span>fib</span>(<span>n</span> <span>-</span> <span>1</span>)        <span>const</span> %.<span>t1007</span>, $<span>1</span>                     <span>Load</span> <span>constant</span> <span>1</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1007</span>"
                      %.<span>t1008</span> <span>=</span> <span>sub</span> %<span>n</span>, %.<span>t1007</span>             <span>Subtract</span> ".<span>t1007</span><span>" from "</span><span>n</span><span>", and store the result in temporary variable "</span>.<span>t1008</span>"
                      <span>push</span> %.<span>t1008</span>                          <span>Prepare</span> <span>parameter</span> <span>for</span> <span>function</span> <span>call</span>
                      <span>call</span> @<span>fib</span>, <span>1</span>                          <span>Call</span> <span>function</span> "<span>fib</span><span>" with one parameter</span>
    <span>+</span>                 <span>retval</span> %.<span>t1009</span>                        <span>Store</span> <span>return</span> <span>value</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1009"</span>
    <span>fib</span>(<span>n</span> <span>-</span> <span>2</span>);       <span>const</span> %.<span>t1010</span>, $<span>2</span>                     <span>Load</span> <span>constant</span> <span>2</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1010</span>"
                      %.<span>t1011</span> <span>=</span> <span>sub</span> %<span>n</span>, %.<span>t1010</span>             <span>Subtract</span> ".<span>t1010</span><span>" from "</span><span>n</span><span>", and store the result in temporary variable "</span>.<span>t1011</span>"
                      <span>push</span> %.<span>t1011</span>                          <span>Prepare</span> <span>parameter</span> <span>for</span> <span>function</span> <span>call</span>
                      <span>call</span> @<span>fib</span>, <span>1</span>                          <span>Call</span> <span>function</span> "<span>fib</span>" with one parameter
                      <span>retval</span> %.<span>t1012</span>                        <span>Store</span> <span>return</span> <span>value</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1012"</span>
                      %.<span>t1013</span> <span>=</span> <span>add</span> %.<span>t1009</span>, %.<span>t1012</span>        <span>Add</span> <span>".t1009"</span> <span>and</span> <span>".t1012"</span>, <span>and</span> <span>store</span> <span>the</span> <span>result</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1013"</span>
                      <span>ret</span> %.<span>t1013</span>                           <span>Return</span> <span>".t1013"</span>
}                   }</pre></div>
<h2 tabindex="-1" dir="auto">Known Issues</h2>
<ol dir="auto">
<li>The generated ELF lacks of .bss and .rodata section</li>
<li>The support of varying number of function arguments is incomplete. No <code>&lt;stdarg.h&gt;</code> can be used.
Alternatively, check the implementation <code>printf</code> in source <code>lib/c.c</code> for <code>var_arg</code>.</li>
<li>The C front-end is a bit dirty because there is no effective AST.</li>
</ol>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><code>shecc</code> is freely redistributable under the BSD 2 clause license.
Use of this source code is governed by a BSD-style license that can be found in the <code>LICENSE</code> file.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mouse filmed tidying man's shed every night (151 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/jan/07/mouse-secretly-filmed-tidying-mans-shed-every-night</link>
            <guid>38905178</guid>
            <pubDate>Sun, 07 Jan 2024 21:11:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/jan/07/mouse-secretly-filmed-tidying-mans-shed-every-night">https://www.theguardian.com/world/2024/jan/07/mouse-secretly-filmed-tidying-mans-shed-every-night</a>, See on <a href="https://news.ycombinator.com/item?id=38905178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A mouse has been filmed secretly tidying up a man’s shed almost every night for two months.</p><p>Wildlife photographer Rodney Holbrook noticed that objects he left out of place were being mysteriously put back where they belonged overnight.</p><p>Holbrook, from Builth Wells in Powys, <a href="https://www.theguardian.com/uk/wales" data-link-name="in body link" data-component="auto-linked-tag">Wales</a>, set up a night vision camera on his workbench to find out what was happening, and captured footage reminiscent of the 2007 animated movie Ratatouille, where a rodent secretly cooks at a restaurant.</p><p>Holbrook, 75, told the BBC: “It has been going on for months. I call him Welsh Tidy Mouse. At first, I noticed that some food that I was putting out for the birds was ending up in some old shoes I was storing in the shed, so I set up a camera.”</p><figure id="98d56567-bcf1-43ff-940b-562d57d515bd" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Rodney Holbrook in his shed. " src="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="267.10355718085106" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Rodney Holbrook was baffled by what was happening, so he set up a camera.</span> Photograph: Animal News Agency</figcaption></figure><p>Night vision footage showed the seemingly conscientious rodent gathering clothes pegs, corks, nuts and bolts, and placing them in a tray on Holbrook’s workbench.</p><p>Holbrook even experimented with leaving out different objects to see if the mouse could lift them, but the creature was undeterred and was even seen carrying cable ties to the pot.</p><p>“I couldn’t believe it when I saw that the mouse was tidying up,” Holbrook said.</p><p>“He moved all sorts of things into the box, bits of plastic, nuts and bolts. I don’t bother to tidy up now, as I know he will see to it. I leave things out of the box and they put it back in its place by the morning. Ninety-nine times out of 100 the mouse will tidy up throughout the night.”</p><p>A similar incident occurred in 2019, when a <a href="https://www.bbc.co.uk/news/uk-england-bristol-47625283" data-link-name="in body link">viral video showed a mouse “stockpiling” items in a man’s shed near Bristol.</a></p><p>Steve Mckears told reporters he thought he “was going mad” when screws and metal objects kept reappearing in a box containing bird feed. He set up a camera and captured footage of the mouse putting screws and other metal objects in the container.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitUI (367 pts)]]></title>
            <link>https://github.com/extrawurst/gitui</link>
            <guid>38905019</guid>
            <pubDate>Sun, 07 Jan 2024 20:53:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/extrawurst/gitui">https://github.com/extrawurst/gitui</a>, See on <a href="https://news.ycombinator.com/item?id=38905019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/extrawurst/gitui/blob/master/assets/logo.png"><img width="300px" src="https://github.com/extrawurst/gitui/raw/master/assets/logo.png"></a>
<p dir="auto"><a href="https://github.com/extrawurst/gitui/actions"><img src="https://github.com/extrawurst/gitui/workflows/CI/badge.svg" alt="CI"></a> <a href="https://crates.io/crates/gitui" rel="nofollow"><img src="https://camo.githubusercontent.com/fa710137de71d3027317b88573a00165cc985cf9e0b1e1c8340b69f0c51314a8/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f67697475692e737667" alt="crates" data-canonical-src="https://img.shields.io/crates/v/gitui.svg"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2bb6ac78e5a9f4f688a6a066cc71b62012101802fcdb478e6e4c6b6ec75dc694/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667"><img src="https://camo.githubusercontent.com/2bb6ac78e5a9f4f688a6a066cc71b62012101802fcdb478e6e4c6b6ec75dc694/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"></a> <a href="https://github.com/rust-secure-code/safety-dance/"><img src="https://camo.githubusercontent.com/4cae2784e68f964e197a4f5792390949288d7335430e3b4da3d0adb0a197bafb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f756e736166652d666f7262696464656e2d737563636573732e737667" alt="UNSAFE" data-canonical-src="https://img.shields.io/badge/unsafe-forbidden-success.svg"></a> <a href="https://extrawurst.itch.io/gitui" rel="nofollow"><img src="https://camo.githubusercontent.com/abe58c6a1baab77730331d1c95aba28b67a44aeccc609b483dbd600ca2ffadb0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f697463682e696f2d6f6b2d677265656e" alt="ITCH" data-canonical-src="https://img.shields.io/badge/itch.io-ok-green"></a> <a href="https://twitter.com/intent/follow?screen_name=extrawurst" rel="nofollow"><img src="https://camo.githubusercontent.com/f6045d21c7c8a8ccb64e31b309e50db34422083e9fd81f982839bd87312fe6b9/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f657874726177757273743f6c6162656c3d666f6c6c6f77267374796c653d736f6369616c" alt="TWEET" data-canonical-src="https://img.shields.io/twitter/follow/extrawurst?label=follow&amp;style=social"></a> <a href="https://deps.rs/repo/github/extrawurst/gitui" rel="nofollow"><img src="https://camo.githubusercontent.com/14d4c73d6a4109c7c11c19f4c3eb19dc3f5daf88325c1bde0c15603c04ac9d84/68747470733a2f2f646570732e72732f7265706f2f6769746875622f657874726177757273742f67697475692f7374617475732e737667" alt="dep_status" data-canonical-src="https://deps.rs/repo/github/extrawurst/gitui/status.svg"></a></p>
</h2>
<h5 tabindex="-1" dir="auto">GitUI provides you with the comfort of a git GUI but right in your terminal</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/extrawurst/gitui/blob/master/demo.gif"><img src="https://github.com/extrawurst/gitui/raw/master/demo.gif" alt="" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto"><a name="user-content-table-of-contents"></a> Table of Contents</h2>
<ol dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#bench">Benchmarks</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#build">Build</a></li>
<li><a href="#faqs">FAQs</a></li>
<li><a href="#diagnostics">Diagnostics</a></li>
<li><a href="#theme">Color Theme</a></li>
<li><a href="#bindings">Key Bindings</a></li>
<li><a href="#sponsoring">Sponsoring</a></li>
<li><a href="#inspiration">Inspiration</a></li>
</ol>
<h2 tabindex="-1" dir="auto">1. <a name="user-content-features"></a> Features <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<ul dir="auto">
<li>Fast and intuitive <strong>keyboard only</strong> control</li>
<li>Context based help (<strong>no need to memorize</strong> tons of hot-keys)</li>
<li>Inspect, commit, and amend changes (incl. hooks: <em>pre-commit</em>,<em>commit-msg</em>,<em>post-commit</em>,<em>prepare-commit-msg</em>)</li>
<li>Stage, unstage, revert and reset files, hunks and lines</li>
<li>Stashing (save, pop, apply, drop, and inspect)</li>
<li>Push / Fetch to / from remote</li>
<li>Branch List (create, rename, delete, checkout, remotes)</li>
<li>Browse / <strong>Search</strong> commit log, diff committed changes</li>
<li>Responsive terminal UI</li>
<li>Async git API for fluid control</li>
<li>Submodule support</li>
</ul>
<h2 tabindex="-1" dir="auto">2. <a name="user-content-motivation"></a> Motivation <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">I do most of my git work in a terminal but I frequently found myself using git GUIs for some use-cases like: index, commit, diff, stash, blame and log.</p>
<p dir="auto">Unfortunately popular git GUIs all fail on giant repositories or become unresponsive and unusable.</p>
<p dir="auto">GitUI provides you with the user experience and comfort of a git GUI but right in your terminal while being portable, fast, free and opensource.</p>
<h2 tabindex="-1" dir="auto">3. <a name="user-content-bench"></a> Benchmarks <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">For a <a href="https://youtu.be/rpilJV-eIVw?t=5334" rel="nofollow">RustBerlin meetup presentation</a> (<a href="https://github.com/extrawurst/gitui-presentation">slides</a>) I compared <code>lazygit</code>,<code>tig</code> and <code>gitui</code> by parsing the entire Linux git repository (which contains over 900k commits):</p>
<table>
<thead>
<tr>
<th></th>
<th>Time</th>
<th>Memory (GB)</th>
<th>Binary (MB)</th>
<th>Freezes</th>
<th>Crashes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>gitui</code></td>
<td><strong>24 s</strong> ✅</td>
<td><strong>0.17</strong> ✅</td>
<td>1.4</td>
<td><strong>No</strong> ✅</td>
<td><strong>No</strong> ✅</td>
</tr>
<tr>
<td><code>lazygit</code></td>
<td>57 s</td>
<td>2.6</td>
<td>16</td>
<td>Yes</td>
<td>Sometimes</td>
</tr>
<tr>
<td><code>tig</code></td>
<td>4 m 20 s</td>
<td>1.3</td>
<td><strong>0.6</strong> ✅</td>
<td>Sometimes</td>
<td><strong>No</strong> ✅</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">4. <a name="user-content-roadmap"></a> Road(map) to 1.0 <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">These are the high level goals before calling out <code>1.0</code>:</p>
<ul dir="auto">
<li>visualize branching structure in log tab (<a href="https://github.com/extrawurst/gitui/issues/81" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/81/hovercard">#81</a>)</li>
<li>interactive rebase (<a href="https://github.com/extrawurst/gitui/issues/32" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/32/hovercard">#32</a>)</li>
</ul>
<h2 tabindex="-1" dir="auto">5. <a name="user-content-limitations"></a> Known Limitations <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<ul dir="auto">
<li>no sparse repo support (see <a href="https://github.com/extrawurst/gitui/issues/1226" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/1226/hovercard">#1226</a>)</li>
<li>no support for GPG signing (see <a href="https://github.com/extrawurst/gitui/issues/97" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/97/hovercard">#97</a>)</li>
<li>no git-lfs support (see <a href="https://github.com/extrawurst/gitui/discussions/1089" data-hovercard-type="discussion" data-hovercard-url="/extrawurst/gitui/discussions/1089/hovercard">#1089</a>)</li>
<li><em>credential.helper</em> for https needs to be <strong>explicitly</strong> configured (see <a href="https://github.com/extrawurst/gitui/issues/800" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/800/hovercard">#800</a>)</li>
</ul>
<p dir="auto">Currently, this tool does not fully substitute the <em>git shell</em>, however both tools work well in tandem.</p>
<p dir="auto">The priorities for <code>gitui</code> are on features that are making me mad when done on the <em>git shell</em>, like stashing, staging lines or hunks. Eventually, I will be able to work on making <code>gitui</code> a one stop solution - but for that I need help - this is just a spare time project for now.</p>
<p dir="auto">All support is welcomed! Sponsors as well! ❤️</p>
<h2 tabindex="-1" dir="auto">6. <a name="user-content-installation"></a> Installation <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">GitUI is in beta and may contain bugs and missing features. However, for personal use it is reasonably stable and is being used while developing itself.</p>
<a href="https://repology.org/project/gitui/versions" rel="nofollow">
    <img src="https://camo.githubusercontent.com/0c4c1bfaf62801fbb97ab015e176e45525a135c2483440964fffd9ad7696189e/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f67697475692e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/gitui.svg">
</a>
<h3 tabindex="-1" dir="auto">Various Package Managers</h3>
<details>
  <summary>Install Instructions</summary>
<h5 tabindex="-1" dir="auto"><a href="https://archlinux.org/packages/extra/x86_64/gitui/" rel="nofollow">Arch Linux</a></h5>

<h5 tabindex="-1" dir="auto">Fedora</h5>

<h5 tabindex="-1" dir="auto">Gentoo</h5>
<p dir="auto">Available in <a href="https://github.com/gentoo-mirror/dm9pZCAq">dm9pZCAq overlay</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo eselect repository enable dm9pZCAq
sudo emerge --sync dm9pZCAq
sudo emerge dev-vcs/gitui::dm9pZCAq"><pre>sudo eselect repository <span>enable</span> dm9pZCAq
sudo emerge --sync dm9pZCAq
sudo emerge dev-vcs/gitui::dm9pZCAq</pre></div>
<h5 tabindex="-1" dir="auto"><a href="https://software.opensuse.org/package/gitui" rel="nofollow">openSUSE</a></h5>
<div dir="auto" data-snippet-clipboard-copy-content="sudo zypper install gitui"><pre>sudo zypper install gitui</pre></div>
<h5 tabindex="-1" dir="auto">Homebrew (macOS)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://ports.macports.org/port/gitui/details/" rel="nofollow">MacPorts (macOS)</a></h5>

<h5 tabindex="-1" dir="auto"><a href="https://github.com/microsoft/winget-pkgs/tree/master/manifests/s/StephanDilly/gitui">Winget</a> (Windows)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://github.com/ScoopInstaller/Main/blob/master/bucket/gitui.json">Scoop</a> (Windows)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://chocolatey.org/packages/gitui" rel="nofollow">Chocolatey</a> (Windows)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://search.nixos.org/packages?channel=unstable&amp;show=gitui&amp;from=0&amp;size=50&amp;sort=relevance&amp;query=gitui" rel="nofollow">Nix</a> (Nix/NixOS)</h5>
<p dir="auto">Nixpkg</p>
<div data-snippet-clipboard-copy-content="nix-env -iA nixpkgs.gitui"><pre><code>nix-env -iA nixpkgs.gitui
</code></pre></div>
<p dir="auto">NixOS</p>

<h5 tabindex="-1" dir="auto"><a href="https://github.com/termux/termux-packages/tree/master/packages/gitui">Termux</a> (Android)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://anaconda.org/conda-forge/gitui" rel="nofollow">Anaconda</a></h5>
<div data-snippet-clipboard-copy-content="conda install -c conda-forge gitui "><pre><code>conda install -c conda-forge gitui 
</code></pre></div>
</details>
<h3 tabindex="-1" dir="auto">Release Binaries</h3>
<p dir="auto"><a href="https://github.com/extrawurst/gitui/releases">Available for download in releases</a></p>
<p dir="auto">Binaries available for:</p>
<h3 tabindex="-1" dir="auto">Linux</h3>
<ul dir="auto">
<li>gitui-linux-musl.tar.gz (linux on x86_64)</li>
<li>gitui-linux-aarch64.tar.gz (linux on 64 bit arm)</li>
<li>gitui-linux-arm.tar.gz</li>
<li>gitui-linux-armv7.tar.gz</li>
</ul>
<p dir="auto">All contain a single binary file</p>
<h3 tabindex="-1" dir="auto">macOS</h3>
<ul dir="auto">
<li>gitui-mac.tar.gz (intel Mac, uses Rosetta on Apple silicon, single binary)</li>
</ul>
<h3 tabindex="-1" dir="auto">Windows</h3>
<ul dir="auto">
<li>gitui-win.tar.gz (single 64bit binary)</li>
<li>gitui.msi (64bit Installer package)</li>
</ul>
<h2 tabindex="-1" dir="auto">7. <a name="user-content-build"></a> Build <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<h3 tabindex="-1" dir="auto">Requirements</h3>
<ul dir="auto">
<li>
<p dir="auto">Minimum supported <code>rust</code>/<code>cargo</code> version: <code>1.65</code></p>
<ul dir="auto">
<li>See <a href="https://www.rust-lang.org/tools/install" rel="nofollow">Install Rust</a></li>
</ul>
</li>
<li>
<p dir="auto">To build openssl dependency (see <a href="https://docs.rs/openssl/latest/openssl/" rel="nofollow">https://docs.rs/openssl/latest/openssl/</a>)</p>
<ul dir="auto">
<li>perl &gt;= 5.12 (strawberry perl works for windows <a href="https://strawberryperl.com/" rel="nofollow">https://strawberryperl.com/</a>)</li>
<li>a c compiler (msvc, gcc or clang, cargo will find it)</li>
</ul>
</li>
<li>
<p dir="auto">To run the complete test suite python is required (and it must be invokable as <code>python</code>)</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Cargo Install</h3>
<p dir="auto">The simplest way to start playing around with <code>gitui</code> is to have <code>cargo</code> build and install it with <code>cargo install gitui</code>. If you are not familiar with rust and cargo: <a href="https://doc.rust-lang.org/book/ch01-00-getting-started.html" rel="nofollow">Getting Started with Rust</a></p>
<h3 tabindex="-1" dir="auto">Cargo Features</h3>
<h4 tabindex="-1" dir="auto">trace-libgit</h4>
<p dir="auto">enable <code>libgit2</code> tracing</p>
<p dir="auto">works if <code>libgit2</code> builded with <code>-DENABLE_TRACE=ON</code></p>
<p dir="auto">this feature enabled by default, to disable: <code>cargo install --no-default-features</code></p>
<h2 tabindex="-1" dir="auto">8. <a name="user-content-faqs"></a> FAQs <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">see <a href="https://github.com/extrawurst/gitui/blob/master/FAQ.md">FAQs page</a></p>
<h2 tabindex="-1" dir="auto">9. <a name="user-content-diagnostics"></a> Diagnostics <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">To run with logging enabled run <code>gitui -l</code>.</p>
<p dir="auto">This will log to:</p>
<ul dir="auto">
<li>macOS: <code>$HOME/Library/Caches/gitui/gitui.log</code></li>
<li>Linux using <code>XDG</code>: <code>$XDG_CACHE_HOME/gitui/gitui.log</code></li>
<li>Linux: <code>$HOME/.cache/gitui/gitui.log</code></li>
<li>Windows: <code>%LOCALAPPDATA%/gitui/gitui.log</code></li>
</ul>
<h2 tabindex="-1" dir="auto">10. <a name="user-content-theme"></a> Color Theme <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/extrawurst/gitui/blob/master/assets/light-theme.png"><img src="https://github.com/extrawurst/gitui/raw/master/assets/light-theme.png" alt=""></a></p>
<p dir="auto"><code>gitui</code> should automatically work on both light and dark terminal themes.</p>
<p dir="auto">However, you can customize everything to your liking: See <a href="https://github.com/extrawurst/gitui/blob/master/THEMES.md">Themes</a>.</p>
<h2 tabindex="-1" dir="auto">11. <a name="user-content-bindings"></a> Key Bindings <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">The key bindings can be customized: See <a href="https://github.com/extrawurst/gitui/blob/master/KEY_CONFIG.md">Key Config</a> on how to set them to <code>vim</code>-like bindings.</p>
<h2 tabindex="-1" dir="auto">12. <a name="user-content-sponsoring"></a> Sponsoring <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto"><a href="https://github.com/sponsors/extrawurst"><img src="https://camo.githubusercontent.com/a714ba0a49f7b58991cd598e0a5e29f6b5597ad5228ef84bde3b25479fa5a756/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d47697448756225323053706f6e736f72732d6661666266633f6c6f676f3d47697448756225323053706f6e736f7273" alt="github" data-canonical-src="https://img.shields.io/badge/-GitHub%20Sponsors-fafbfc?logo=GitHub%20Sponsors"></a></p>
<p dir="auto"><a href="https://liberapay.com/extrawurst/donate" rel="nofollow"><img alt="Donate using Liberapay" src="https://camo.githubusercontent.com/18cfbc8e266770af00126e8d82b29d1c2f047e6efb5243141f0810496f7c1c66/68747470733a2f2f6c69626572617061792e636f6d2f6173736574732f776964676574732f646f6e6174652e737667" data-canonical-src="https://liberapay.com/assets/widgets/donate.svg"></a></p>
<p dir="auto"><a href="https://ko-fi.com/B0B6GMW1T" rel="nofollow"><img height="36" src="https://camo.githubusercontent.com/1cccbe7e15949c13dca33cfe9142aed92e86101adc8778d3c3f695443a73de06/68747470733a2f2f73746f726167652e6b6f2d66692e636f6d2f63646e2f6b6f6669342e706e673f763d33" alt="Buy Me a Coffee at ko-fi.com" data-canonical-src="https://storage.ko-fi.com/cdn/kofi4.png?v=3"></a></p>
<h2 tabindex="-1" dir="auto">13. <a name="user-content-inspiration"></a> Inspiration <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<ul dir="auto">
<li><a href="https://github.com/jesseduffield/lazygit">lazygit</a></li>
<li><a href="https://github.com/jonas/tig">tig</a></li>
<li><a href="https://github.com/git-up/GitUp">GitUp</a>
<ul dir="auto">
<li>It would be nice to come up with a way to have the map view available in a terminal tool</li>
</ul>
</li>
<li><a href="https://github.com/andys8/git-brunch">git-brunch</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A concrete example of what goes wrong with Apple's docs (143 pts)]]></title>
            <link>https://www.amimetic.co.uk/blog/a-concrete-example-of-why-apples-docs-are-terrible/</link>
            <guid>38904721</guid>
            <pubDate>Sun, 07 Jan 2024 20:16:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amimetic.co.uk/blog/a-concrete-example-of-why-apples-docs-are-terrible/">https://www.amimetic.co.uk/blog/a-concrete-example-of-why-apples-docs-are-terrible/</a>, See on <a href="https://news.ycombinator.com/item?id=38904721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Apple's documentations is famously dreadful. In particular it is often missing or incomplete. <a href="https://nooverviewavailable.com/">No overview available</a> gives you some numbers on this but I think that actually understates the problem. Even when it is available it is often terrible. Here is a small concrete example I recently encountered while doing some MacOS development (an area I'm less familiar with).</p>
<p>I want to prompt the user to open a directory. How do I do that? Obviously I Google it (throwing in <code>Swift</code> to avoid ancient <code>Obj-C</code> answers).</p>
<p>Seems like I want something called <code>NSOpenPanel</code>.</p>
<p>I find <a href="https://developer.apple.com/documentation/appkit/nsopenpanel">Apple's docs</a> and it seems to be fairly clear. I configure with some straightforward options.</p>
<p>Okay how do I actually use it?</p>
<p>Hmmm. That is far from clear. There is literally nothing on the page that explains even the most basic use case. Sigh. Hit google again finding that I want to call <code>begin</code> and that that this accepts a callback which is given a result.</p>
<p>The example is out of date (a perpertual issue with Swift) but eventually sort that out in the editor (we now have a nice enum with the result).</p>
<p>Okay but how do I get the actual directory selected? The success value is <code>.OK</code>. Is there some kind of data with that? No. Just a result.</p>
<p>Hmmm...</p>
<p>There is a <a href="https://developer.apple.com/documentation/appkit/nsopensavepaneldelegate">delegate</a> that I can set. Maybe that gets the result.</p>
<p>Waste a few minutes on that, basically appears to be for configuration.</p>
<p>Ah, it inherits from <code>NSSavePanel</code>! That was not at all obvious (and I'm sorry OOP devotees, that makes damn all sense in principle). Oh, look, loads of new functions and properties. Ah, so I'm meant to query the NSOpenPanel object for the directory.</p>
<p>Not really clear how. Xcode offers a <code>url</code>, <code>urls</code>, <code>directoryUrl</code>, <code>representedUrl</code> among others (I'm not making any of those up and most didn't seem to be in documentation). Let's be optimistic and chose <code>url</code>. Success! It appears to work.</p>
<p>Oh, also missing (before I continue): in recent versions of MacOS need to add a capability for reading (and writing) to user selected directories. Again completely missing from documentation and yet another frustration for someone not already very familiar with the ecosystem.</p>
<p>This can be so much better. Apple developers sneer at Electron, but look at how <a href="https://electronjs.org/docs/api/dialog#dialogshowopendialogbrowserwindow-options">clear Electron's open dialog docs</a> are.</p>
<h2>What goes wrong with Apple's docs?</h2>
<ol>
<li>Missing and incomplete</li>
<li>Outdated (Obj-C or older Swift versioned docs are common)</li>
<li>Badly written (assumes a level of familiarity that would make reading the docs unnecessary). Lack of explanation.</li>
<li>Lack of examples of even basic usage.</li>
<li>Badly designed, legacy OOP APIs: Swift can sometimes get away without much documentation through good use of types. Sadly a lot of MacOS API's are a blend of old fashioned OOP with surprisingly low level details.</li>
</ol>
<h2>What if you are doing Apple development?</h2>
<p>Avoid Apple's docs (even where they are top Google results). They will likely be pretty bad. Stack Overflow is sometime good (but often an old Swift version). Some third party docs and good (Paul Hudson, Ray Wenderlich).</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Higher fees, more ads: streaming cashes in by using the old tactics of cable TV (132 pts)]]></title>
            <link>https://theconversation.com/with-higher-fees-and-more-ads-streaming-services-like-netflix-disney-and-hulu-are-cashing-in-by-using-the-old-tactics-of-cable-tv-215048</link>
            <guid>38904418</guid>
            <pubDate>Sun, 07 Jan 2024 19:39:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/with-higher-fees-and-more-ads-streaming-services-like-netflix-disney-and-hulu-are-cashing-in-by-using-the-old-tactics-of-cable-tv-215048">https://theconversation.com/with-higher-fees-and-more-ads-streaming-services-like-netflix-disney-and-hulu-are-cashing-in-by-using-the-old-tactics-of-cable-tv-215048</a>, See on <a href="https://news.ycombinator.com/item?id=38904418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>There’s one thing that television viewers can count on in 2024: higher fees and more commercials.</p>

<p>The major streaming services – Amazon, Netflix, Hulu, Disney+ and Max – have <a href="https://www.axios.com/2023/12/28/amazon-prime-netflix-disney-peacock-streaming-subscription">all announced</a> rate hikes and new advertising policies.</p>

<p>As I show in my new book, “<a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">24/7 Politics: Cable Television and the Fragmenting of America from Watergate to Fox News</a>,” the streaming boom that has imperiled cable television is actually built upon the very same business model that made television viewers pay for monthly subscriptions decades ago. </p>

<p>Like their cable predecessors, streaming companies have lured people in with promises of a better and cheaper viewing experience. Now that they have a robust subscriber base, they’re in the process of raising rates while also introducing more commercials and bundling programming to make customers pay more and more.</p>

<p>There is a difference, though. When cable companies tried similar tactics in the late 1980s, there was an uproar from politicians who called such business practices “unfair” to their constituents. Now, there’s nary a peep – a sign of just how inured Americans have become to the whims of corporations trying to squeeze their customers.</p>

<h2>Stemming the tide of ‘toll television’</h2>

<p>Like streaming companies, cable TV’s entrepreneurs in the 1960s saw the business potential of framing cable television as a path for more choice with fewer commercials.</p>

<p>At the time, <a href="https://www.worldradiohistory.com/BOOKSHELF-ARH/Regulatory/Television's-Guardians-The-FCC-1958-1967-Baughman-1985.pdf">federal regulations</a> squashed competition by allowing the “Big Three” broadcast networks — CBS, NBC and ABC — to dominate the airwaves as long as they also served a vaguely defined “<a href="https://www.rutgersuniversitypress.org/public-interests/9780813572291/">public interest</a>.” Advertisers underwrote the cost of programs, which meant that while viewers didn’t have to pay a monthly TV bill, they did have to endure commercials.</p>

<p>This business structure also encouraged programming with mass appeal in order to deliver the broadest possible audiences to advertisers. But not all TV viewers were happy with the formulaic quiz shows and sitcoms that dominated the airwaves. Sensing an untapped opportunity, TV entrepreneurs tried to concoct ways to circumvent the dominance of the Big Three. </p>

<p>Cable television actually dates back to the late 1940s. It was initially known as “<a href="https://tupress.temple.edu/books/blue-skies">community antenna television</a>,” or CATV, because it was used to bring broadcast signals to smaller communities that couldn’t get signals from the big cities.</p>

<p>At first, this technology simply expanded the reach of CBS, NBC and ABC rather than providing a competing service.  </p>

<figure>
            <p><img alt="Elderly man with mustache wearing suit smiles as he stands next to a young woman in black dress, who gazes his way." data-src="https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=772&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=772&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=772&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=970&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=970&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=970&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>Former NBC executive Pat Weaver – here with his daughter, actress Sigourney Weaver – was an early proponent of subscription TV.</span>
              <span><a href="https://www.gettyimages.com/detail/news-photo/sigourney-weaver-and-sylvester-pat-weaver-during-the-year-news-photo/105909150?adppopup=true">Ron Galella Collection/Getty Images</a></span>
            </figcaption>
          </figure>

<p>But in 1963, a former NBC executive named Pat Weaver <a href="https://www.theatlantic.com/magazine/archive/1964/10/why-suppress-pay-tv-the-fight-in-california/658253/">proposed subscription television</a>, in which people would pay a monthly fee for access to specialized channels through a wired connection. </p>

<p>His company, STV, offered a way to sidestep the “vast land of advertising trivia” that beamed into living rooms across the nation, Weaver explained during one public forum. Weaver dreamed of how giving individual subscribers more choices could forge a business model that could break through the programming limitations of broadcast.</p>

<p>In the end, STV didn’t last. Broadcasters and theater owners mobilized to convince the public that such experiments would turn all television into pay TV, dividing Americans into those with television access and those without it. </p>

<p><a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">Broadcasting lobbyists warned</a> that “toll television” would “have an undemocratic and divisive effect” by depriving viewers of their right to consume television for free. <a href="https://go.gale.com/ps/i.do?p=AONE&amp;u=googlescholar&amp;id=GALE%7CA70451817&amp;v=2.1&amp;it=r&amp;asid=23eb4633">One flyer featured</a> a devastated young boy with a football helmet who didn’t have enough coins to insert in the television. </p>

<p>“Pop says he don’t have any more Dollar and a halfs for me to watch each ball game,” the caption read.</p>

<p>The dire warnings about the end of free TV worked, and voters supported a <a href="https://ballotpedia.org/California_Proposition_15,_Prohibition_of_Paid_Television_Programming_Initiative_(1964)">state ballot initiative</a> in 1964 that outlawed subscription television. While the courts <a href="https://caselaw.findlaw.com/court/ca-supreme-court/1820874.html">overturned the new law</a> for violating the First Amendment, STV didn’t survive. </p>

<h2>Cable catches on</h2>

<p>But the idea of wired television delivering more choices to viewers persisted.  </p>

<p>As frustrations with the limits of broadcast television intensified <a href="https://www.cambridge.org/core/journals/modern-american-history/article/watergate-the-bipartisan-struggle-for-media-access-and-the-growth-of-cable-television/64F2A0E3B8D3EAD28F8E6449DEF11BDE">across the political spectrum during the 1970s</a>, consumers, elected officials and regulators all embraced the potential of cable television to offer an alternative.</p>

<p>By the mid-1970s, experiments with programming disseminated via satellite on cable systems tested new types of niche channels and shows – like nonstop movies, sports, music or the weather – to see if audiences might be interested. In 1975, HBO gambled that a live international boxing match between Muhammad Ali and Joe Frazier, “<a href="https://www.cnn.com/2016/06/04/sport/thrilla-in-manila-remembered/index.html">Thrilla in Manila</a>,” would boost its struggling pay-TV operation. </p>

<p>It did: Income from pay television services like HBO, which first launched in 1972, <a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">soared</a> from US$29 million in 1975 to $769 million in 1980. </p>

<p>Like STV before them, cable companies tapped into frustrations with broadcasting and its advertising model. They sold subscriptions by promising that premium channels like HBO could provide movies with “no cuts, no commercials.” </p>

<p>Millions of people eagerly signed up for cable subscriptions and premium channels like HBO that cost even extra.</p>

<h2>Deregulation nation</h2>

<p>Niche cable channels soon emerged that appealed to specific demographic groups. Black Entertainment Television created new opportunities for programming geared toward Black audiences. The Daytime Channel offered entertainment and news directed at women, while MTV connected a younger generation through music videos.</p>

<p>Then there was C-SPAN, a cable industry-funded initiative that put the cameras on the House of Representatives starting in 1979. In a 1984 letter to the network, an enthusiastic viewer praised the public affairs channel for providing “over-the-back-fence discussion with your neighbors on matters of common interest, but with the scope that the neighborhood extends to encompass all areas of the United States.”</p>

<p>Cable’s popularity buoyed the lobbying efforts of the industry, which was pushing Congress to deregulate key aspects of their business operations. In 1984, they succeeded: <a href="https://www.congress.gov/bill/98th-congress/senate-bill/66">The Cable Communications Policy Act of 1984</a> notably removed local government caps on what companies could charge for subscription services. </p>

<p>The consequences quickly became clear: price hikes and poor customer service. In the next few years, basic cable rates skyrocketed, <a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">increasing by an average of 90%</a>.</p>

<h2>Playing political football</h2>

<p>Al Gore, then an ambitious senator representing Tennessee, saw an opportunity. He pounced on the issue, decrying how cable companies and lobbyists had leveraged consumer demand in ways that amounted to <a href="https://www.c-span.org/video/?9959-1/cable-telecommunications-act-day-1-part-1">what he described as</a> “total domination of the marketplace.”</p>

<figure>
            <p><img alt="Man with graying hair wearing suit being interviewed." data-src="https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=738&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=738&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=738&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=927&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=927&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=927&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>John Malone served as CEO of TCI for over 20 years.</span>
              <span><a href="https://www.gettyimages.com/detail/news-photo/john-malone-news-photo/1039963656?adppopup=true">Rick Maiman/Sygma via Getty Images</a></span>
            </figcaption>
          </figure>

<p><a href="https://www.c-span.org/video/?9959-1/cable-telecommunications-act-day-1-part-1">He condemned the industry</a> as an American “Cosa Nostra,” and having likened Tele-Communications Inc. (TCI) executive <a href="https://www.thegentlemansjournal.com/article/john-malone-everything-need-know-americas-single-largest-land-owner/">John Malone</a> to “Darth Vader,” Gore then lashed out at him during a 1989 congressional hearing for “shaking down” average Americans.</p>

<p>Malone pushed back, highlighting the unprecedented choice that people now had on cable. Rate increases allowed for experimentation with niche programming that never stood a chance on network broadcast television, he added. And they also helped pay the costs of laying – and then later upgrading – wires across the country to deliver such services.</p>

<h2>Everything old is new again</h2>

<p>Cable-bashing was effective on the campaign trail for Gore and his top-of-the-ticket running mate, Arkansas Gov. Bill Clinton. But, once in office, they changed tack. They wanted private industry to build the information highway they saw as <a href="https://www.dissentmagazine.org/article/let-them-eat-tech/">central to their governing agenda</a>, and cable companies were the ones who owned the coaxial wires going into millions of homes. </p>

<p>Four years later, Gore and Clinton celebrated the <a href="https://www.fcc.gov/general/telecommunications-act-1996">1996 Telecommunications Act</a>, which slashed many price regulatory measures Gore had championed while on the campaign trail in 1992.</p>

<p>The rationale? That the marketplace competition and programming choice alone could deliver for the public interest. </p>

<p>The result? The expansion of a media landscape forged on the terrain of private businesses and their profit margins.</p>

<p>Despite today’s frustrations with changes designed to boost bottom lines – rate hikes, <a href="https://www.washingtonpost.com/technology/2023/05/27/netflix-password-sharing-why-users-mad/">limits on password sharing</a>, <a href="https://www.theatlantic.com/technology/archive/2022/08/sports-streaming-makes-losers-us-all/671231/">exclusive streaming contracts for sporting events</a> – people no longer look to politicians to help them navigate and address these concerns as they once did. The bipartisan belief in deregulation has seemingly closed down these conversations about policy alternatives. </p>

<p>That’s why cable didn’t just blaze a path for a new business model. It also convinced elected officials and constituents to embrace a different understanding of the public interest, one where the market reigns supreme.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just because you can doesn't mean you should: the <meter> element (101 pts)]]></title>
            <link>https://localghost.dev/blog/just-because-you-can-doesn-t-mean-you-should-the-meter-element/</link>
            <guid>38903869</guid>
            <pubDate>Sun, 07 Jan 2024 18:41:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://localghost.dev/blog/just-because-you-can-doesn-t-mean-you-should-the-meter-element/">https://localghost.dev/blog/just-because-you-can-doesn-t-mean-you-should-the-meter-element/</a>, See on <a href="https://news.ycombinator.com/item?id=38903869">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>I came across Sara Joy's (very cool) demo of <a href="https://codepen.io/sarajw/details/xxBGmRZ">CSS theming without classes</a> today, and looking through the code spotted a couple of elements I hadn't come across before: <code>&lt;progress&gt;</code> and <code>&lt;meter&gt;</code>. Granted, I've probably seen the <code>&lt;progress&gt;</code> element plenty of times, but I struggled to see what the <code>&lt;meter&gt;</code> was for.</p>
<p>While <code>&lt;progress&gt;</code> is fairly self-explanatory – it shows how far along something is, such as your progress through a form – the <code>&lt;meter&gt;</code> element was less obvious to me, so I had a look at the MDN page to see what they suggested.</p>
<figure><img src="https://localghost.dev/img/blog/meter.png" alt="A screenshot from MDN. HTML:  Heat the oven to 350 degrees.  Result:  &quot;Heat the oven to&quot; and then a half-full progress bar that doesn't show you any indication of the actual number." loading="lazy" decoding="async"></figure>
<p>This is a prime example of <strong>following the letter, not the spirit, of semantic HTML</strong>. Yes, technically the cooking temperature is somewhere between the lowest and highest temperature you can set the oven to, but is this actually helping people understand the recipe? Quite the opposite, it's making the recipe less accessible for anyone <em>not</em> using a screen reader.</p>
<p>Now, chances are the person who wrote this article simply couldn't think of a better example, and isn't necessarily proposing that everyone starts using meters instead of numbers in their recipes, but a <em>lot</em> of developers rely on MDN to tell them what is good practice, so I don't think this is particularly useful. (I'm going to try and contribute a better example.)</p>
<p>Inspired by this, and with a burning desire to build something truly terrible, I've created this recipe page using the <code>&lt;meter&gt;</code> element for every numerical value.</p>
<p data-height="265" data-theme-id="dark" data-default-tab="result" data-user="sophiekoonin" data-slug-hash="ZEPWxLL" data-preview="true" data-pen-title="CodePen Home
A recipe, but all the numbers are <meter> bars">
  <span>See the Pen 
    <a href="https://codepen.io/sophiekoonin/pen/ZEPWxLL">
      CodePen Home
A recipe, but all the numbers are <meter> bars</meter></a> by <a href="https://codepen.io/sophiekoonin">@sophiekoonin</a>
  on <a href="https://codepen.io/">CodePen</a>.</span>
</p>

<p>The <a href="https://www.w3.org/TR/2011/WD-html5-author-20110809/the-meter-element.html">spec</a> has a few more sensible examples, though I'm not entirely persuaded that a meter is a good illustration of newsgroup activity.</p>
<p>A more tangible use case for the <code>&lt;meter&gt;</code> element would be to indicate something like available storage space, or percentage of remaining budget on a service where your plan only allows you a certain number of events or entities: an example of this would be <a href="https://sentry.io/welcome/">Sentry</a>, where your plan has a limit to the number of events/errors it'll accept, depending on how much money you throw at them.</p>
<p>The key UX thing here, though, is that if it's important information it should be accompanied by a numerical value. A meter is good for an at-a-glance sense of how much of something has been used, but you need to present it alongside the actual value for it to be at all useful.</p>
<figure><img src="https://localghost.dev/img/blog/google-drive-meter.png" alt="A screenshot from Google Drive with a cloud icon next to text that says 'Storage 74% full', a graphical meter that is approx 74% full, then underneath the text '11.2 GB of 15 GB used'." loading="lazy" decoding="async"></figure>
<p>I checked both Dropbox and Google Drive, and both of them have a meter accompanied by a numerical description of how much space I've used; in both cases those meters are, of course, <code>&lt;div&gt;</code>s. Usually I'd complain about using a <code>&lt;div&gt;</code> when there's a semantic element available, but... it's not immediately clear to me what the advantage of using a <code>&lt;meter&gt;</code> would be from an accessibility viewpoint, if you've got the written description right there.</p>
<p>When you're choosing the right element for the job, it's entirely possible to go too far the other way, and <em>overuse</em> semantic elements when actually they hinder more than they help.</p>
<p>As my good pal and accessibility specialist Helen put it:</p>
<blockquote>
<p>It’s a really good example of thinking about what you’re trying to communicate and to who and whether your “semantic” choices actually enable that.</p>
</blockquote>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows XP 2024 Edition is everything I want from a new OS (204 pts)]]></title>
            <link>https://overclock3d.net/news/software/windows-xp-2024-edition-is-everything-i-want-from-a-new-os/</link>
            <guid>38903314</guid>
            <pubDate>Sun, 07 Jan 2024 17:48:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overclock3d.net/news/software/windows-xp-2024-edition-is-everything-i-want-from-a-new-os/">https://overclock3d.net/news/software/windows-xp-2024-edition-is-everything-i-want-from-a-new-os/</a>, See on <a href="https://news.ycombinator.com/item?id=38903314">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>




 
<p><a href="https://media.overclock3d.net/2024/01/Windows-XP-2024-Edition.jpg" target="_blank"><img fetchpriority="high" decoding="async" src="https://media.overclock3d.net/2024/01/Windows-XP-2024-Edition.jpg" alt="" width="1200" height="675"></a></p>
<h2>Windows XP 2024 is the OS I wish was real</h2>
<p>2024 is here, and rumour has it that Microsoft plans to announce/release Windows 12 this year. Yes, another new OS from Microsoft. Windows XP may be a dead OS, but it still holds a place in our hearts, and with Windows XP 2024 Edition, we can see what a revived Windows XP could look like.</p><p><a data-no-instant="1" href="https://www.scan.co.uk/products/deepcool-assassin-iv-cooler" rel="noopener" target="_blank"><img decoding="async" src="https://media.overclock3d.net/2023/09/728x90.jpg" alt="" width="728" height="90"></a></p>
<p><a href="https://www.youtube.com/watch?v=YLFUl9MW_Ks"><strong>AR 4789</strong></a> has designed and showcased a modern interpretation of Windows XP 2024 Edition. The OS’ design is familiar, yet new, and feels remarkably simple. There is no Cortana, no AI Co-Pilot, or an extreme amount of unwanted, pre-installed software. XP 2024 Edition is a call-back to simpler times, before your OS could show you ads, and when the Windows search function was a useful feature (No, I don’t want you to search using Bing!).</p>
<p>Honestly, now that I have seen it, I am a little sad that Windows XP 2024 Edition doesn’t exist. It is a concept, nothing more. It is a shame that Microsoft cannot create a new version of Windows that runs as smoothly as AR 4789’s concepts. Watch the video below to see it for yourself.</p>
<p><iframe title="Windows XP 2024" width="500" height="281" src="https://www.youtube.com/embed/YLFUl9MW_Ks?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p>With a simple UI, a nice looking dark mode and its iconic wallpaper, Windows XP looks great in its modern form. Like many PC users, I remember Windows XP and Windows 7 more fondly than their successors. While nostalgia plays a role here, so do other factors. Many aspects of Windows 11 are unwanted. Login requirements, changes to Windows Search, and taskbar changes, are all bugbears. Windows 12 is unlikely to address these issues, and moving to Linux is a step too far for most PC users.</p><p><a data-no-instant="1" href="https://msi.gm/S96E4205" rel="noopener" target="_blank"><img loading="lazy" decoding="async" src="https://media.overclock3d.net/2023/12/image_2023_12_06T15_16_39_674Z.jpg" alt="" width="728" height="90"></a></p>
<p>Today, Windows XP is practically extinct. Even newer OS’ like Windows 7 and 8 are becoming an increasingly rare sight. This week <a href="https://overclock3d.net/news/software/steam-has-dropped-support-for-windows-7-8-and-8-1/"><strong>Steam dropped support for pre-Windows 10 OS’</strong></a>. Windows 7 will be 15 years old this year, which means that it is getting close to “Retro” status. By comparison, Windows XP is practically ancient. Even so, I still like it, and I love the idea of its 2024 reimagining.</p>
<p>You can join the discussion on <a href="https://forum.overclock3d.net/showthread.php?t=101213"><strong>Windows XP 2024 Edition on the OC3D Forums</strong></a>.</p>

<div>
<p><img src="https://media.overclock3d.net/2023/09/l9qmlVQ4_400x400.png" alt="Mark Campbell">
</p>

</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The immediate victims of the con would rather act as if the con never happened (214 pts)]]></title>
            <link>https://statmodeling.stat.columbia.edu/2024/01/07/french-bio-lab-research-scandal/</link>
            <guid>38903145</guid>
            <pubDate>Sun, 07 Jan 2024 17:32:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statmodeling.stat.columbia.edu/2024/01/07/french-bio-lab-research-scandal/">https://statmodeling.stat.columbia.edu/2024/01/07/french-bio-lab-research-scandal/</a>, See on <a href="https://news.ycombinator.com/item?id=38903145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>Dorothy Bishop <a rel="nofollow" href="http://deevybee.blogspot.com/2023/02/open-letter-to-cnrs.html">has the story</a> about “a chemistry lab in CNRS-Université Sorbonne Paris Nord”:</p>
<blockquote><p>More than 20 scientific articles from the lab of one principal investigator have been shown to contain recycled and doctored graphs and electron microscopy images. That is, results from different experiments that should have distinctive results are illustrated by identical figures, with changes made to the axis legends by copying and pasting numbers on top of previous numbers. . . . the problematic data are well-documented in a number of PubPeer comments on the articles (see links in Appendix 1 of <a rel="nofollow" href="https://docs.google.com/document/d/1di9gvmdMDi81PPgX-7jK8QDD_nREHDVKpuNvOi3rjbM/edit">this document</a>).</p>
<p>The response by CNRS [Centre National de la Recherche Scientifique] to this case . . . was to request correction rather than retraction of what were described as “shortcomings and errors”, to accept the scientist’s account that there was no intentionality, despite clear evidence of a remarkable amount of manipulation and reuse of figures; a disciplinary sanction of exclusion from duties was imposed for just one month.</p></blockquote>
<p>I’m not surprised.  The sorts of people who will cheat on their research are likely to be the same sorts of people who will instigate lawsuits, start media campaigns, and attack in other ways.  These are researchers who’ve already shown a lack of scruple and a willingness to risk their careers; in short, they’re loose cannons, scary people, so it can seem like the safest strategy to not try to upset them too much, not trap them into a corner where they’ll fight like trapped rats.  I’m not speaking specifically of this CNRS researcher—I know nothing of the facts of this case beyond what’s reported in Bishop’s post—I’m just speaking to the mindset of the academic administrators who would just like the problem to go away so they can get on with their regular jobs.</p>
<p>But Bishop and her colleagues were annoyed.  If even blatant examples of scientific misconduct cannot be handled straightforwardly, what does this say about the academic and scientific process more generally?  Is science just a form of social media, where people can make any sort of claim and evidence doesn’t matter?</p>
<p>They write:</p>
<blockquote><p>So what should happen when fraud is suspected?  We propose that there should be a prompt investigation, with all results transparently reported. Where there are serious errors in the scientific record, then the research articles should immediately be retracted, any research funding used for fraudulent research should be returned to the funder, and the person responsible for the fraud should not be allowed to run a research lab or supervise students. The whistleblower should be protected from repercussions.</p>
<p>In practice, this seldom happens. Instead, we typically see, as in this case, prolonged and secret investigations by institutions, journals and/or funders. There is a strong bias to minimize the severity of malpractice, and to recommend that published work be “corrected” rather than retracted.</p></blockquote>
<p>Bishop and her colleagues continue:</p>
<blockquote><p>One can see why this happens. First, all of those concerned are reluctant to believe that researchers are dishonest, and are more willing to assume that the concerns have been exaggerated. It is easy to dismiss whistleblowers as deluded, overzealous or jealous of another’s success. Second, there are concerns about reputational risk to an institution if accounts of fraudulent research are publicised. And third, there is a genuine risk of litigation from those who are accused of data manipulation. So in practice, research misconduct tends to be played down.</p></blockquote>
<p>But:</p>
<blockquote><p>This failure to act effectively has serious consequences:</p>
<p>1.   It gives credibility to fictitious results, slowing down the progress of science by encouraging others to pursue false leads. . . . [and] erroneous data pollutes the databases on which we depend.</p>
<p>2.   Where the research has potential for clinical or commercial application, there can be direct damage to patients or businesses.</p>
<p>3.   It allows those who are prepared to cheat to compete with other scientists to gain positions of influence, and so perpetuate further misconduct, while damaging the prospects of honest scientists who obtain less striking results.  </p>
<p>4.   It is particularly destructive when data manipulation involves the Principal Investigator of a lab. . . . CNRS has a mission to support research training: it is hard to see how this can be achieved if trainees are placed in a lab where misconduct occurs.</p>
<p>5.   It wastes public money from research grants.</p>
<p>6.   It damages public trust in science and trust between scientists.</p>
<p>7.   It damages the reputation of the institutions, funders, journals and publishers associated with the fraudulent work.</p>
<p>8.   Whistleblowers, who should be praised by their institution for doing the right thing, are often made to feel that they are somehow letting the side down by drawing attention to something unpleasant. . . .</p></blockquote>
<p><strong>What happened next?</strong></p>
<p>It’s the usual bad stuff.  They receive a series of stuffy bureaucratic responses, none of which address any of items 1 through 8 above, let alone the problem of the data which apparently have obviously been faked.  Just disgusting.</p>
<p>But I’m not surprised.  We’ve seen it many times before:</p>
<p>– The University of California’s unresponsive response when informed of research misconduct by their star sleep expert.</p>
<p>– The American Political Science Association refusing to retract an award given to an author for a book with plagiarized material, or even to retroactively have the award shared with the people whose material was copied without acknowledgment.</p>
<p>– The London Times never acknowledging the blatant and repeated plagiarism by its celebrity chess columnist.</p>
<p>– The American Statistical Association refusing to retract an award given to a professor who plagiarized multiple times, including from wikipedia (in an amusing case where he created negative value by introducing an error into the material he’d copied, so damn lazy that he couldn’t even be bothered to proofread his pasted material).</p>
<p>– Cornell University . . . ok they finally canned the pizzagate dude, but only after emitting some platitudes.  Kind of amazing that they actually moved on that one.</p>
<p>– The Association for Psychological Science:  this one’s personal for me, as they ran an article that flat-out lied about me and then refused to correct it just because, hey, they didn’t want to.</p>
<p>– Lots and lots of examples of people finding errors or fraud in published papers and journals refusing to run retractions or corrections or even to publish letters pointing out what went wrong.</p>
<p>Anyway, this is one more story.</p>
<p><strong>What gets my goat</strong></p>
<p>What really annoys me in these situations is how the institutions show loyalty to the people who did research misconduct.  When researcher X works at or publishes with institution Y, and it turns out that X did something wrong, why does Y so often try to bury the problem and attack the messenger?  Y should be mad at X; after all, it’s X who has leveraged the reputation of Y for his personal gain.  I’d think that the leaders of Y would be <em>really</em> angry at X, even angrier than people from the outside.  But it doesn’t happen that way.  The immediate victims of the con would rather act as if the con never happened.  Instead, they’re mad at the outsiders who showed them that they were being fooled.  I’m sure that <a href="https://statmodeling.stat.columbia.edu/2023/07/07/cheating-in-science-sports-journalism-business-and-art-how-do-they-differ/">Dan Davies would have something to say</a> about all this.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disney backs down from 'Steamboat Willie' YouTube copyright claim (146 pts)]]></title>
            <link>https://mashable.com/article/disney-pulls-youtube-steamboat-willie-copyright-claim</link>
            <guid>38903074</guid>
            <pubDate>Sun, 07 Jan 2024 17:23:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mashable.com/article/disney-pulls-youtube-steamboat-willie-copyright-claim">https://mashable.com/article/disney-pulls-youtube-steamboat-willie-copyright-claim</a>, See on <a href="https://news.ycombinator.com/item?id=38903074">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article" data-autopogo="">
<p>This feels significant: <a href="https://mashable.com/category/disney" target="_self">Disney</a> has officially retracted a copyright claim on a third-party's <em>Steamboat Willie</em> video on YouTube.</p><p>On Thursday, <a href="https://mashable.com/article/youtube-demontizes-public-domain-steamboat-willie-disney-copyright-claim" target="_self"><u>Mashable reported</u></a> that YouTuber and voice actor Brock Baker had uploaded a video to his channel with over 1 million subscribers which was almost immediately hit with a copyright claim from Disney.</p><div>
<p><img src="https://helios-i.mashable.com/imagery/articles/07IiyhhCWb2WlXJ9Pfyxgjs/images-1.fill.size_2000x950.v1704492840.png" alt="Disney releases &quot;Steamboat Willie&quot; copyright claim" width="2000" height="950" loading="lazy" srcset="https://helios-i.mashable.com/imagery/articles/07IiyhhCWb2WlXJ9Pfyxgjs/images-1.fill.size_800x380.v1704492840.png 800w, https://helios-i.mashable.com/imagery/articles/07IiyhhCWb2WlXJ9Pfyxgjs/images-1.fill.size_1400x665.v1704492840.png 1400w, https://helios-i.mashable.com/imagery/articles/07IiyhhCWb2WlXJ9Pfyxgjs/images-1.fill.size_2000x950.v1704492840.png 2000w" sizes="(max-width: 1408px) 100vw, 1408px">
</p>
<p><span>YouTube's message to Brock Baker regarding Disney releasing the copyright claim on his "Steamboat Willie" upload.</span>
<span>Credit: Brock Baker</span>
</p>
</div>
<p>Baker's video featured the entirety of the 1928 Disney animated short <em>Steamboat Willie</em>. He had remixed the film, which stars Mickey Mouse,&nbsp;with his own comedic audio track playing over the nearly 8-minute cartoon, and released it under the title "Steamboat Willie (Brock's Dub)."</p><p>After being hit with the claim, Baker's upload became demonetized, meaning the YouTuber could not make any money off of it. The claim also blocked the ability to embed the video on third-party websites. In addition, the YouTube video was given limited visibility, including being blocked from view entirely in certain countries.&nbsp;</p>
<p>Baker disputed the copyright claim shortly after receiving it. His case appeared strong, as <em>Steamboat Willie</em> entered the public domain on January 1, 2024, allowing a broad range of creative usage of the film and its contents without Disney's permission — including for profit.</p><p>He was successful.</p><p>"Disney released their claim and it's now embeddable and shareable worldwide," Baker told Mashable on Friday along with a screenshot of the email alert he received from YouTube letting him know the copyright claim was released.</p><p>"Good news! After reviewing your dispute, Disney has decided to release their copyright claim on your YouTube video," reads the YouTube email message.</p><p>As a result of Disney pulling the claim, Baker's video is now monetizable, embeddable, and viewable worldwide.</p>

<p>"I'm honestly glad it took 24 hours and not 30 days, still frustrating though," Baker told us, referencing YouTube's policies which gave Disney an entire month to respond to his dispute to their copyright claim. "I wish I knew what goes on behind the scenes."</p><p>There has been lots of speculation online about what exactly can be done with <em>Steamboat Willie</em> that won't draw the ire of or potential lawsuit from Disney, which still holds the trademark (which is different from a copyright) for uses of the iconic Mickey Mouse character in certain contexts. According to <a href="https://www.techdirt.com/2024/01/03/youtube-still-blocking-access-to-steamboat-willie-on-behalf-of-disney-in-some-countries/" target="_blank" title="(opens in a new window)"><u>TechDirt</u></a>, other <em>Steamboat Willie</em> videos have also reportedly received copyright claims over the past few days.</p>
<p>YouTube, for its part, historically asserts that it does not mediate copyright claims. It's up to the copyright holder to make claims via its Content ID tool, and it's up to uploaders to dispute those claims when they believe they were incorrectly made. According to YouTube, the responsibility to release claims on content that has fallen into the public domain is with the Content ID user, who in this case is Disney.</p><p>Based on how quickly Baker's video was flagged, Disney's copyright claim on his upload was likely automated, drawing from YouTube's Content ID database. Mashable has reached out to Disney for more information and will update this piece if we hear back.</p><p>But, the way this has played out will likely be genuinely helpful to those looking to create new creative works based on the newly public domain title <em>Steamboat Willie</em>. By releasing the YouTube copyright claim on Baker's video, Disney has made what could be its first official public action recognizing that <em>Steamboat Willie</em> along with this version of Mickey Mouse is indeed public domain. </p><p>In other words, content like Baker's is allowed, with or without Disney's input.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NHS to investigate Palantir influencer campaign as possible contract breach (232 pts)]]></title>
            <link>https://goodlawproject.org/nhs-to-investigate-palantir-influencer-campaign-as-possible-contract-breach/</link>
            <guid>38902983</guid>
            <pubDate>Sun, 07 Jan 2024 17:12:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://goodlawproject.org/nhs-to-investigate-palantir-influencer-campaign-as-possible-contract-breach/">https://goodlawproject.org/nhs-to-investigate-palantir-influencer-campaign-as-possible-contract-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=38902983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">


		
			<div>
						<p><span>News</span></p>
												<p><span>6th January 2024</span></p><p><span>NHS England has confirmed that it will investigate whether Palantir violated the terms of its contract to run the Federated Data Platform, after the tech giant covertly launched an influencer campaign which targeted Good Law Project.</span></p>
					</div>

			<div>
						      
<p><span>We can reveal </span><span>that Palantir was required to but failed to seek prior approval from NHS England (NHSE) for its campaign to promote its contract to run the Federated Data Platform and brief against Good Law Project. Now NHSE has confirmed to <a href="https://www.bloomberg.com/news/articles/2024-01-05/nhs-probes-whether-palantir-influencer-campaign-breached-contract-terms?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcwNDUyMzA4MSwiZXhwIjoxNzA1MTI3ODgxLCJhcnRpY2xlSWQiOiJTNlFYVVBUMVVNMFcwMCIsImJjb25uZWN0SWQiOiIzMkI3NjREMjRGNDQ0OTEyQjE0Mzc1OTA4ODY4N0FFNiJ9.EzD2aOa0WmaF2BaPJBRRmxt5p5vebXePUNh0ZRHNS-g" target="_blank" rel="noopener">Bloomberg UK</a> that it will be investigating whether Palantir violated the contract terms just weeks after signing it.</span><span><br>
</span></p>
<p><span>A leaked briefing and emails from the campaign shows that Palantir used Tory-linked PR agency, Topham Guerin and marketing agency, Disrupt, to <a href="https://goodlawproject.org/how-palantir-and-topham-guerins-plan-to-discredit-us-unravelled/" target="_blank" rel="noopener">approach social media influencers</a> to ask them what they would like to be paid to take part. This was done at arms-length, with the briefing asking influencers not to mention Palantir in their content.<br>
</span><span><br>
</span><span><img fetchpriority="high" decoding="async" src="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png" alt="Quoted text from the 'Things to avoid' section of the Topham Guerin Briefing. Quoted text starts: &quot;Things to avoid ● When the content is posted, please keep the brand confidential and not tag Palantir. ● If responding to comments, please refrain from mentioning the brand. Timings DEADLINE: 22nd December &quot; Quoted text ends." width="640" height="159" srcset="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png 1024w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-300x74.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-768x190.png 768w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755.png 1183w" sizes="(max-width: 640px) 100vw, 640px" data-srcset="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png 1024w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-300x74.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-768x190.png 768w, https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755.png 1183w" data-src="https://goodlawproject.org/wp-content/uploads/2024/01/TG2-Copy-e1704536512755-1024x254.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></span><span>The Federated Data Platform <a href="https://www.contractsfinder.service.gov.uk/notice/2e8c61c0-faab-4f99-ae69-b00df6bae165?origin=SearchResults&amp;p=1" target="_blank" rel="noopener">contract</a> was finally published on the last working day before Christmas.</span> <span>Page after page of this three-part contract has been redacted – including a section on the protection of personal data.</span><span><br>
</span><span><br>
</span><span>But one of the sections we can actually see – which covers ‘Publicity and Branding’ – states that Palantir is not permitted to use the Authority’s name or brand in any marketing or publicise the contract without the prior written consent of NHS England.&nbsp;</span></p>
<p><span>An NHS spokesperson told Bloomberg that the NHS takes any potential breach of contract by a supplier seriously and is investigating what happened.</span></p>
<p><img decoding="async" src="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png" alt="An excerpt from the Federated Data Contract signed between the NHS and Palantir which reads: &quot;24 PUBLICITY AND BRANDING 24.1 The Supplier shall not: (a) make any press announcements or publicise this Agreement or its contents in any way; or (b) use the Authority's or any Authority Service Recipients name or brand in any promotion or marketing or announcement of orders; without the prior written consent of the Authority or relevant Authority Service Recipient, which shall not be unreasonably withheld or delayed.&quot;" width="541" height="174" srcset="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png 990w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-300x96.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-768x247.png 768w" sizes="(max-width: 541px) 100vw, 541px" data-srcset="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png 990w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-300x96.png 300w, https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy-768x247.png 768w" data-src="https://goodlawproject.org/wp-content/uploads/2024/01/FDPcontract01-Copy.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p>
<p><span>“Palantir is not – and frankly never has been – a company that can be trusted with this nationally important contract with our NHS” says Good Law Project’s Executive Director, Jo Maugham.</span></p>
<p><span>“By its own behaviour it is telling us exactly that.”</span><span><br>
</span><span><br>
</span><span>“Within weeks, it commissioned a covert smear campaign against a prominent critic and appears to have broken the terms of that contract. If this Government won’t act to protect the national interest, the next one must.”</span></p>
<p><span>Responding to Bloomberg, Palantir has claimed that its campaign was an “exploratory project” so it did not need to consult NHSE. Palantir’s Executive Vice President for UK and Europe, Louis Mosely said, “We decided not to pursue the project — as such, the campaign was not discussed with NHS England”.</span></p>
<p><span>But it is clear that the campaign did seek to use the NHS name and brand in marketing and publicized its agreement with the NHS.</span></p>
<p><span>The campaign materials were dated 28 November 2023, gave as a deadline 22 December and stated “We are on a tight timeline for this one and would require a response as soon as possible with content going live before the new year”, so it is clear that there was no intention to seek the NHS England’s consent.</span></p>
<p><span>Palantir’s conduct in commissioning a covert, paid-for smear campaign against a prominent critic; its decision to breach its contract with the NHS within weeks of signing it; and subsequent dissembling about its intention to breach, will serve to substantiate the concerns of those who believe Palantir was an entity that ought never to have been given the contract in the first place.<br>
</span><span><br>
Good Law Project is attacked because of the work it does – and the threat it poses to the likes of Palantir and the Government. We can only do this because of donations from people like you, right across the UK. If you can, please </span><a href="https://goodlawproject.org/donate/"><span>support our work.</span></a><span><br>
</span></p>

    
  					</div>

		


	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Irish State announce plan to build a porn preference register for most of the EU (225 pts)]]></title>
            <link>https://www.thegist.ie/the-gist-wtf-commission/</link>
            <guid>38902407</guid>
            <pubDate>Sun, 07 Jan 2024 16:16:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thegist.ie/the-gist-wtf-commission/">https://www.thegist.ie/the-gist-wtf-commission/</a>, See on <a href="https://news.ycombinator.com/item?id=38902407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://www.thegist.ie/tag/ireland/">Ireland</a>
            
                <p>The Irish State announced its new plan to build a porn preference register for most of the EU, amongst other mad things. This is the Gist.</p>

            

                <figure>
        <img srcset="https://www.thegist.ie/content/images/size/w320/2024/01/photo-1599663252656-f7054ba72a44.jpeg 320w,
                    https://www.thegist.ie/content/images/size/w600/2024/01/photo-1599663252656-f7054ba72a44.jpeg 600w,
                    https://www.thegist.ie/content/images/size/w960/2024/01/photo-1599663252656-f7054ba72a44.jpeg 960w,
                    https://www.thegist.ie/content/images/size/w1200/2024/01/photo-1599663252656-f7054ba72a44.jpeg 1200w,
                    https://www.thegist.ie/content/images/size/w2000/2024/01/photo-1599663252656-f7054ba72a44.jpeg 2000w" src="https://www.thegist.ie/content/images/size/w1200/2024/01/photo-1599663252656-f7054ba72a44.jpeg" alt="The Gist: Coimisiún na Meán go off the rails">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@karsten116?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Karsten Winegeart</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>

        </header>

        <section>
            <p>Let's start with where things stand right now and then go from there. </p><p>The first thing I should say: <strong>this is not a joke. </strong></p><p>I say that now, because later on, as you're reading the rest of this piece, you're going to keep asking yourself "Is this some kind of a joke?" </p><p>Hence, this helpful reference statement for you to return to repeatedly throughout. </p><p>This week the Executive Chairman of the new Irish regulator for internet content, Jeremy Godfrey, made himself available for comment to the press to discuss the <a href="https://www.cnam.ie/" rel="noreferrer">Coimisiún na Meán</a> (Media Commission) <a href="https://www.cnam.ie/coimisiun-na-mean-opens-public-consultation-on-irelands-first-online-safety-code/" rel="noreferrer">plan</a> to introduce a code of conduct to control how adults would access websites.</p><p>He outlined <a href="https://www.irishexaminer.com/news/arid-41300860.html" rel="noreferrer">the plan set out in page 17 of their Consultation and proposal document to the Irish Examiner.</a> </p><p>This body regulates content (and in particular video content) for the largest internet companies in the world due to the fact so many of them are based in Ireland.</p><p>Mr Godfrey suggested that his Commission would require adults and minors (children under 18) to send a copy of their passport to websites- including porn sites- and then, also, send them a live selfie so the porn sites could see what they looked like right now. And then the porn sites would run biometric data processing on those images (details unspecified) to confirm they were over 18. Call this the Nightclub Bouncer plan. </p><p>This is the national internet regulator proposing that it would require that everyone, adult and children alike, would upload their state ID and live selfies, to porn sites to have biometric processing of their facial images performed. Resulting, amongst other things, in an effective register of porn preferences for adults and a collection of selfies of children kept by the porn sites for six years (required to prove they have complied with the regulation, you see). </p><p>I refer you now to the top of the newsletter. I am still not joking.  </p><div><p>💡</p><p>Hey! Good news! This crackpot scheme isn't law yet. And you can do something to stop it. The Commission has extended its time for feedback on this plan to the 31st January 2024. You can just email what you think of it, giving as many reasons why you think it is a bad idea to <a href="mailto:VSPSregulation@cnam.ie" target="_blank" rel="noreferrer noopener">VSPSregulation@cnam.ie</a> Please do drop them a line, and tell your friends too.</p></div><div><p>But wait! That's not all! The CnaM Executive Chairman wanted to talk about porn sites because that's the least popular class of entities covered by this regulation. But the age-verification requirement actually can cover any <a href="https://www.cnam.ie/designation-notices/" rel="noreferrer">video-sharing platform</a> under the jurisdiction of the Irish State (link to the designation notice under section 139E and section 139G of the Broadcasting 2009 Act). That's a list that includes Facebook, WhatsApp, XTwitter and YouTube, just to pick four household names (because of <a href="https://www.irishstatutebook.ie/eli/2022/act/41/section/5/enacted/en/html?q=video-sharing+platform&amp;search_type=all" rel="noreferrer">Section 5 of the Online Safety and Media Regulation Act 2022</a>). It might also mean homegrown platforms such as <a href="https://mastodon.ie/" rel="noreferrer">Mastodon.ie</a>, the most prominent Irish part of the <a href="https://www.theverge.com/23990974/social-media-2023-fediverse-mastodon-threads-activitypub" rel="noreferrer">Fediverse,</a> who also allow videos to be shared. </p><p>In fact, Section 2 of the Broadcasting Act 2009 (as amended) casts the net wide enough to cover almost anywhere that lets you post a video. </p></div><blockquote>In this Act, ‘video-sharing platform service’ means, subject to subsection (3), a service, within the meaning of Articles 56 and 57 of the Treaty on the Functioning of the European Union, where—<br>(a) the principal purpose of the service is devoted to,<br>(b) the principal purpose of a dissociable section of the service is devoted to, or<br>(c) an essential functionality of the service is devoted to,<br>providing audiovisual programmes or user-generated videos, or both, by electronic communications networks, to the general public, in order to inform, entertain or educate.</blockquote><p>Also, these restrictions won't just limit and record access to porn sites. They can be applied to any sites which contains material the Commission decides may be legal, but on the other hand, oughtn't be seen by children. In other countries, this has been the kind of legal provision which has seen libraries restricting access to <a href="https://www.nytimes.com/2023/09/21/books/book-ban-rise-libraries.html" rel="noreferrer">books relating to LGBTQ+ themes</a>, racial justice themes and anything else you could imagine the Burke family objecting to. </p><p>The exact description of what is to be restricted behind the Commission's Nightclub Bouncer plan is simply 'age-inappropriate content', defined broadly in <a href="https://www.irishstatutebook.ie/eli/2022/act/41/section/45/enacted/en/html#sec45" rel="noreferrer">Section 45 of the Online Safety Act 2023</a> as "online content that is likely to be unsuitable for children (either generally or below a particular age), having regard to their capabilities, their development, and their rights and interests". It goes on to give examples of pornography and acts of violence, but the controls aren't limited to them. </p><p>This plan goes far beyond the requirement of the EU Audio-visual directive, which the EU Commission suggests "could be done by the use of PIN codes". It therefore must be measured against the twin EU law requirements of "necessity and proportionality" under the Charter of Fundamental Rights and the GDPR. It fails both tests, of course. </p><p>If there is an alternative method of meeting the requirement of age restriction (as the European Commission's suggestion of the use of PIN numbers demonstrates) then it fails the test of necessity and cannot be in compliance with EU law. </p><p>And even if there were no other method, it has to be considered whether creating a distributed database of internet use, including porn preferences, for all EU adults is proportionate to the aim being pursued. Then consider it also requires the additional security risk of sending copies of sensitive personal documents such as passports to platforms such as Elon Musk's X AND also requiring they perform biometric processing of <a href="https://gdpr-info.eu/art-9-gdpr/" rel="noreferrer">article 9 GDPR</a> facial data of both adults and minors. I think the question of proportionality answers itself by the time you reach the end of that sentence. </p><p>It is at this point that I feel justified in pointing to my <a href="https://www.rte.ie/radio/radio1/clips/21574302/" rel="noreferrer">interview on Morning Ireland</a> from 2019, where I warned that the Broadcasting Authority of Ireland was the <a href="https://www.rte.ie/news/ireland/2019/0624/1057148-online-safety/" rel="noreferrer">wrong body</a> to put in charge regulating the internet, because it had neither the experience nor understanding of the medium necessary. </p><p>In creating Coimisiún na Meán the Government did just that. The fresh new regulator is now demonstrating exactly how unfit for this role it is. </p><p>I take it back. In a way, this is a joke. It just isn't funny.</p><hr><p>Hello, dear reader. Many thanks for your interest in our first Gist of 2024. A reminder that you can <a href="https://www.thegist.ie/#/portal/signup/free" rel="noreferrer">subscribe for free</a> or, if you would like to support the production of more Gists into the future on a self-hosted <a href="https://www.nytimes.com/2023/12/22/business/substack-nazis-content-moderation.html" rel="noreferrer">non-nazi </a> system you can opt to contribute (from €2 a month) at <a href="https://www.thegist.ie/#/portal/signup" rel="noreferrer">this link</a>. But please know, I will always love you equally, regardless of which subscription option that's right for you. </p>
        </section>

    </article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Behind the scenes: the struggle for each paper (2021) (138 pts)]]></title>
            <link>https://jeffhuang.com/struggle_for_each_paper/</link>
            <guid>38902258</guid>
            <pubDate>Sun, 07 Jan 2024 15:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jeffhuang.com/struggle_for_each_paper/">https://jeffhuang.com/struggle_for_each_paper/</a>, See on <a href="https://news.ycombinator.com/item?id=38902258">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
	The start of my sabbatical has given me a moment to reflect on my publications. But <a href="https://jeffhuang.com/CurriculumVitae.pdf">my CV</a> only shows a list of neatly cataloged papers: title, authors, conference. Each one appearing no different from another. But how each paper ended up published is its own story, a story about people and opportunity.
</p><p>
	Using notes from <a href="https://jeffhuang.com/productivity_text_file/">my research journal</a> and conference records, I reassembled the "Behind the scenes" story for each of my full research papers: 15 papers as a student, and 15 papers after becoming a professor (excluding papers not led by my research group, as I feel it's not my place to tell those stories). This is as much a <a href="#introspection">reflective exercise for myself</a> than it is for an audience.
</p><p>
	If you read this page in its entirety, it will take about 30 minutes. But you can skip any story, and it should still make sense. The first half are my <a href="#student_papers">student stories</a>, and the second half are my <a href="#professor_papers">professor stories</a>, so you can even just read the half that's more interesting to you. This would probably be more enticing as a series of Tweets or a Substack newsletter, but I'd rather post it all at once.
</p><p>
	I'd love to read the backstories of other peoples' publications too. So if you feel comfortable sharing, post yours and email me if you want it linked at the bottom of this page so we can start a collection. Anyways, here goes.
</p><div><li>
I first stumbled upon research in September 2003 while I was a second-year undergraduate student. My inbox popped up the message, <em>"call me or track me down in class, 'cause a full explanation will take a lot of typing. -Bo"</em>.<p>
Bo was a friend who needed some programming help for an automated help generator as part of a research project, and off I went. What I thought would be a one week task turned into three rewrites of an application, two studies, and one long faculty mentorship. Neither of us could predict that Bo would graduate before the main study began, so he did not even become a co-author of a project he started (though in hindsight, I could have just put him as second author).</p><p>
Because I was learning as I went, I made all the novice mistakes from study design to paper structure. It was a long 3.5 years between when I got that first message and when it was published, after my graduation. And the acceptance decision came only after eeking out a lucky coin-flip, as the initial metareview score was "3-Borderline".</p><p>
But this paper was critical to getting me accepted to a Ph.D. program. Why do I think that? Well I was rejected by every Ph.D. program I applied to before this publication (but that's another story). So with this paper published, I left my job, squeezed my belongings into my car, and drove up the I-5 highway from California to Seattle.
</p></li><hr><li>
My next paper was sadly my first and only full paper with my original advisor, <a href="https://efthi.ischool.uw.edu/">Efthi, before he passed</a>. I don't remember coming up with this idea to study query reformulations, so it must have been he who led me to it. It was intended to be something easy but original—a starter project analyzing the sequences in existing data released by AOL.<p>
It was rejected from SIGIR on my first try, and I was starting to worry the topic would become stale, especially with the controversy about the original dataset that led to AOL closing their research department. So I was relieved this wrapped up quickly, and it felt good to have a paper under my belt in my first year.</p><p>
It puzzles me that this is my most cited full paper, but I think it's due to its topic rather than because of its contribution; though when I checked recently, a few citations are from people using the source code even 12 years later. I guess there's something to be said about the compound interest for citations.
</p></li><hr><li>
I met Ryen from Microsoft Research when he was a visiting speaker in Efthi's class, and he must have found my internship application that mentioned my paper about query reformulations. Well it turned out he was interested in search trails, which are essentially a series of reformulations, so I was able to continue along that line of work.<p>
The gold standard for these internships was to do a paper from start to finish in the twelve weeks, and I was desperately trying to do paper-worthy work to prove myself. I carried out the analysis as best I could, however I struggled to do enough for a full paper by the end of the internship. Fortunately, Ryen finished up and expanded on it substantially after I left, so I'm grateful he didn't just give it up as unfinished.</p><p>
To my complete surprise, this paper won the best paper award that year at SIGIR. Even after being nominated for the award, I felt it was so unlikely to win that I didn't attend the conference. In fact, at that time I thought my other related paper at that conference (the one below) was a better paper overall, but now I see that the best paper award committee probably felt that evaluation was a messier topic so wanted to reward that effort. This award boosted my confidence during my Ph.D. and opened some doors later, so I'm both lucky and thankful it happened.
</p></li><hr><li>
This related paper came out of the same summer internship with Ryen. I was lucky to be included in the author list because I had a small role and never even met the first author. Besides my initial work over the summer that was moreso for the paper above this, I contributed only a paragraph of original writing. After reading this paper, I felt like it was better than the one I had contributed more, but it has received fewer than half the citations.
</li><hr><li>
I met a brilliant intern during that same summer, Anna, who was doing her <em>first of two</em> Math PhDs at that time and often seemed idle. She explained, "the people who are always busy never seem to get much done." So sensing her free time, I asked her to help solve a made-up hypothetical problem that simplified assumptions from the information retrieval community.<p>
We had a good time working out the math, mostly me asking questions while watching her think aloud on the whiteboards. I learned about different strategies for deriving proofs in a real setting, which were unlike problem sets, where you knew a proof existed and were roughly the right difficulty for you to do. During this summer, I was mathematically in my best shape, as I could follow along enough to write out the solution and check for errors, whereas today it would take a while to even familiarize myself with the equations in the paper.
</p></li><hr><li>
After the summer, I was looking for new ideas to pursue outside my usual information retrieval topics. A friend in the Ph.D. program Gifford came to me with the suggestion that watching people playing video games was an overlooked phenomenon. But no one was paying attention. At that time, you could really only watch low-resolution videos of Korean players with amateur commentary dubbed in English.<p>
That idea clicked with me, so I helped out with a qualitative study to understand why. Gifford taught me most of what I know about grounded theory (and gave me a second lesson a few years ago while collaborating on a more recent 2018 paper).</p><p>
At that time, I had in my mind that I was helping him rescue a rejected idea, but now in retrospect, it's clear that it was he who gave me the opportunity to help out on work he already had a vision for. I'm quite proud of this work, which is my third most-cited paper, but mainly because what we thought would become a phenomenon did indeed come to pass.
</p></li><hr><li>
This became the most important paper during my Ph.D., and it was minutes from not happening. The idea was one of three that I tossed around during my interview to be a Research Intern at Bing. But to apply this idea to the search engine required a complex series of software integration steps that I was not too familiar with; yet it had to be committed by deadlines that occurred every two weeks.<p>
I aimed for a commit deadline during the second half of my internship, and the day it had to ship by 5pm, my code simply wouldn't pass the unit tests. I tried to force it by (naively) changing the unit tests, but that just broke other parts of the build process. It was 4pm and I even had to be somewhere else across the bridge in 30 minutes; I was desperate. I ran in the hallways panicking and found a software developer, Sarvesh, who took a look and gave me some tips. But by the time I had to leave, I still could not push the code. Sarvesh again came to my rescue and assured me, "you take off, I've got this" and sat down at my desk to fix the problem and ship my code while I drove out of the parking lot. Without his help, I would have had to ship it during the next cycle, and would not have left enough time to analyze the data before my internship was over. Not only that, but because it was during a corporate internship, I would have no rights to the intellectual property of any of the work.</p><p>
But my 20 lines of JavaScript did ship and luckily my was bug-free (after I pored over every line a hundred times), so this paper became the first paper I published at the main conference in my field; it was nominated for a best paper award, and became the foundational chapter in my dissertation. Not only that, but it was this work that led to a Google Research Grant, Facebook Ph.D. Fellowship, and Microsoft patent.
</p></li><hr><li>
Back at school, and with momentum from our previous paper about gaming, Gifford and I looked at one of his rejected papers about Texas Hold'em, and expanded it with data from another game, Halo 2. We repeated our formula of qualitative analysis, and submitted to CSCW. It received borderline ratings from our reviewers, but CSCW was experimenting with a "revise and resubmit" process that we were lucky to go through.
</li><hr><li>
At this point, one of my friends from the summer internship, Abdi, felt like I had "the magic touch" so I offered to help out with a rejected internship paper. I only helped brainstorm and edit, but it did end up getting accepted without much fanfare. While this one worked out, a year later I tried to rescue another paper with him, but it only ended up in the bin. Same effort, opposite outcomes.
</li><hr><li>
Now it was the end of my third year, and I felt guilty that I only had one dissertation chapter ready. I had co-founded a startup as part of Techstars Seattle while teaching a class in the evening, so those things had occupied all of my time. Our startup eventually ran out of money and my co-founders left for other opportunities, so in the summer I returned to Microsoft Research for my third internship. Again I joined a different group, this one managed by Sue who a year later became my Ph.D. advisor.<p>
Her group was probably the closest to my core research interests, and it just happened that Ryen had moved to this group. So with this combination of good circumstances, I aimed to write three papers with Ryen and others to get enough material for my dissertation.</p><p>
This first one was difficult for me, using a technique I was unfamiliar with, and required a lot of compute. My compute jobs would compete with higher priority jobs during the day so often timed out. So I had to stay many late evenings to kick off the 8-hour distributed processes that could only finish at night when the cluster was not in heavy use. My efforts paid off; reviewers were generally favorable and it was a clean accept.</p><p>
Now I have to confess, this is the paper I am most skeptical about. I don't fully trust the model, even though I kept checking it over. The results showed a modest improvement, but I can't seem to shake the feeling that they were due to a secondary factor like a collinearity, or even worse that there was a calculation error somewhere in there. But now my code is probably gone, and it seems like others were able to show practical improvements using similar models.
</p></li><hr><li>
A Bing employee, Georg, had collected a nice dataset from a study for one purpose, but it seemed like it could be used to study something closer to what I had done before. He graciously lent me the data, and I did enough analysis to produce this paper. However, one reviewer felt that the descriptive results were not that novel (just a bigger study), nor were the predictive analysis that successful, summarizing "with their paper they now try to add more to this field, but I don't see the important contribution that would justify the publication at CHI." The opinions were ultimately divided.<p>
I sweated over the rebuttal and promised changes, and luckily convinced the metareviewers to let this one slip through. But it actually turned out to be a fairly influential paper, with 223 citations as of today, and served as a baseline for some of my students' work later. So this was paper number two from that summer. I think it hit a trend of papers about user attention that came out the next few years, but this trend has dwindled since then.
</p></li><hr><li>
The third paper of that summer was an easier analysis with a novel idea—that sophisticated use of browser tabs was a central part to finding information. I recruited another student from my university, Tom, who happened to be there interning the same summer. The metareviewer remarked quite correctly, "The paper is not rocket science but [...] to my knowledge has not really been looked at, at least not at this large scale."<p>
While browser tabs did continue to be a phenomenon, this paper got fewer citations than a lighter short paper I wrote earlier on this topic. I think partly because the title was too clever to be easily recognizable.
</p></li><hr><li>
As a bonus for the summer, Georg was writing his own paper, and I had a minor contribution that he deemed enough for a co-authorship. It got a decent number of citations and filled a nice gap in the research field, as well as in my dissertation. So it became a lucky 4-paper summer. I was exhausted by the end of it and took a nice long break.
</li><hr><li>
Besides the two papers with Gifford about gaming, I wasn't doing any research during the school year, as all my other papers were from internships. Without an advisor, I served as a teaching assistant every quarter (sometimes for two or even three classes at once), and got a bit distracted with some other activities.<p>
But one day my opportunity to be a TA vanished (itself another story), and I begged over to Oren, a professor from the computer science department for an office and funding. To my surprise, he immediately agreed within hours of my email. So I started to learn about natural language processing and got to see how he ran his lab.</p><p>
This resulting paper was a combination of his interests and mine. A couple of undergraduate students joined in under my supervision, which was also my first time mentoring students. The study almost did not happen, because I had trouble fitting the procedure under the rules of our human subjects guidance. But after last-minute discussions with Oren and an HCI professor, we found a way to thread the needle.</p><p>
Its publication helped launch one of the undergraduate students into the Ph.D. program at MIT, and led to my interest in involving undergraduate students in research for many years to come.
</p></li><hr><li>
I wasn't originally planning to do another research internship at Microsoft in my last summer, but Tom and Nachi reached out to me about an opportunity I couldn't say no to—a summer to study gaming with large-scale Xbox data. So in my last summer, I joined their gaming research initiative. It was a collaboration between Microsoft Research and folks from Xbox, and along the way I learned a few techniques for time-series analysis.<p>I barely finished the final analysis in the paper the day that I had my farewell lunch. Reviewers loved the work more than I expected, and this paper led to a few opportunities later so I'm glad it worked out. It also brought closure to my Ph.D., as I ended up with no leftover working papers in the pipeline, hence the 2-year gap until my next paper.
</p></li>
<hr id="professor_papers">
<h3>Part 2: Papers as a faculty author</h3>
<hr><li>
Up to this point, I was feeling comfortable leading a paper from start to finish, but the job changed when I became faculty. While I could come up with the idea and advise the process, the initial drafts would be written by students.<p>
So fast forward past my move to Providence and a few false starts at unfinished projects. My first published paper as a faculty member came from a student referred by a professor at another university. The student was Eddie, an undergraduate student from UCLA who reached out to the professor about conducting research analyzing patterns in StarCraft replays. That professor thought I fit the topic better but cautioned, "he [I] probably doesn't have the bandwidth to supervise external students at this time". While that would probably be true now, back then I took the chance and steered him towards an adjacent investigation.</p><p>
I invited Gifford (yes, my classmate from before) to help out, and the work from start to finish was about 8 months of intense analysis and figure-making. The paper ended up with two strong ratings (4.5/5, 5/5) and two unenthusiastic ratings (2.5/5, 3/5), so the compromise was that it ended up shepherded (a paper deemed borderline but asked to make specific changes to be acceptable) to guide us to "accept". This made us nervous for longer, but after this paper got in, I wrote to Eddie, "You've earned your golden ticket to grad school :-) congrats!" and he chose to do a Ph.D. at the University of Washington, my own alma mater. 
</p></li><hr><li>
While teaching my graduate seminar, I was overwhelmed by the enthusiasm of the students so I started assigning projects that students could recycle into research. This became the first of a few papers that were born from class projects in my HCI seminars. Each student worked on their own mini-study, and we combined it into a meta analysis, which is a formula that worked for a couple more papers in later years as well.<p>
The timing for this particular paper was a bit lucky because the reviewers nominated it for a best paper award, but our follow-up work was not as successful; we still had more to say on this topic, but met a lot of resistance in writing the sequels after years of trying to publish newer findings with only rejections.
</p></li><hr><li>
I led my first Ph.D. student Alexandra in a few unfruitful directions trying to continue ideas from my Ph.D. that ended up with two unpublished papers worth of work (so I'm grateful for her patience). But one day while laying in bed I realized we could flip the story from estimating attention with the cursor, to using the webcam while the cursor auto-calibrates the webcam model during regular web browsing. We could deploy this as a library, basically shipping a product.<p>
This paper was the first of many product-style papers that have become the norm in our research group. The work was initially rejected at multiple conferences because while the overall system was effective and the functionality was novel, the technique was not innovative and the results were numerically worse than some of our competitors. I was frustrated about being compared against competitors who only reported data from the users for whom they get good results from (even when they are upfront about omitting results from most of their users), while we were reporting full results from every user.</p><p>
Anyways, it took over two years to build and publish, but I'm proud that the system in our paper is <a href="https://github.com/brownhci/webgazer">used by a sizable community</a>. It has become <a href="https://www.jspsych.org/overview/eye-tracking/">part of a popular psychology library</a> used for many research studies, and adopted by a few startups including one which bought a non-exclusive commercial license. We knew this work would have impact later, as Alexandra sent me one of my favorite acceptance notifications, "It got in!!!! I am going back to sleep, I'll email the rest of the authors tomorrow! :D Very excited, Alexandra" This paper was the foundation for her dissertation, and we are both still working on the project now seven years since it began.
</p></li><hr><li>
I admitted a second student, Nedi, to our Ph.D. program who had prior work on sleep diary research during her undergrad. My brainstorming notes in the months before she arrived was, "we will use data-driven techniques across a large populations sleep data to make (personalized) prescriptive sleep recommendations". By coincidence, I met some new collaborators who had clinical research expertise for this, so Nedi and her team of research assistants set out with these collaborators to develop the software and the study.<p>
We had a tough start and ate a few rejections at both UIST and CHI before we published the paper at the following UIST, 2 years after the initial idea. Even then, the paper almost didn't happen because the reviewers were skeptical (borderline ratings) but Nedi wrote a convincing rebuttal, as a metareviewer summarized, "I re-read the paper in light of the rebuttal. The proposed changes [...] pushed me into the slightly positive end of the spectrum. The submission was discussed at length at the PC meeting and received additional input from another PC member who reviewed the submission at a prior venue. The overall feeling is that this isn't a perfect paper, but it is a difficult area in which to do research and we do learn something from the submission."</p><p>
Close call, but this became the foundation for the rest of Nedi's Ph.D. work. We were lucky to publish it sooner than later because I later found out there were other research groups working on similar ideas.
</p></li><hr><li>
This paper followed from Alexandra's previous one, so was a bit more straightforward as a follow-on application to our previous work. Reviewers like it, and it received an honorable mention. I wish I had more to say, but it was one of the rare times where the idea was universally agreeable and the results were as expected. Later I learned that during my tenure application, an external letter writer remarked that I didn't have many follow-up papers (sequels) at that time, in fact just this one.
</li><hr><li>
Work on this paper started mid-2015, led by Shaun, a Masters student who became a Ph.D. student later. It was a fairly complex product so it took the team substantial time to build it out, and the paper was not published until 2 years later. What's nice is the paper had some broad impacts: two of the undergraduate students working on it are in Ph.D. programs now, and there are still active users of the online web application 6 years since the work started.<p>This paper set the standard that we would try to include undergraduate students in every paper, and so far that still holds true—100% of the papers from our group have included undergraduate authors.
</p></li><hr><li>
We followed a similar formula as before, having students in the HCI seminar run mini-studies, which became a meta study for this paper. But things were not so easy this time around, as the first version of the paper with one cohort struggled to reveal enough compelling findings. So we had to develop a new procedure for students in the seminar in another year, and combined the results from both cohorts for the submission.<p>
We submitted to CHI 2017 and while two reviewers rated it highly (4.5/5 and 4/5), the third wrote a scathing review; the two metareviewers examined the paper closely, and ultimately decided to reject. It was a little frustrating to be so close, as this paper had the highest average rating of all the rejected papers that year. However, we revised it and ultimately published it at IMWUT the following year after a cycle of major revisions.
</p></li><hr><li>
This paper was an exhausting amount of work for Alexandra, collecting a high-quality dataset with the hopes to release it as a contribution. The setup, lengthy procedure, and large number of participants were meant to provide stronger validity for the general topic of eye tracking during interactions.<p>
However, even after the data collection, there were a few snags. We learned that video frames did not inherently have timestamps associated with them, and it was nearly impossible to retroactively infer them to millisecond-level accuracy. While the dataset itself still felt like a strong contribution in the end, it was harder for other researchers to apply immediately so hasn't been as broadly used as I hoped. I still feel like this paper is a bit underrated today, and someone could write one or two other papers from the dataset we collected.
</p></li><hr><li>
My most recent Ph.D. admit, Jing, came with a unique design and technical background, so started working on an ambitious virtual reality project. However, that idea had trouble producing consistent results in practice, but I noted in my research journal on October 2016 that the "motion movement physical device that Jing built that can be used for replay too". So we pivoted to building the product for a different use case which became this paper.<p>
The paper was accepted on its second submission and one of the rare times I've encountered an "accept" decision without having to do a major revisions beforehand. But as a product it has been disappointing; we attempted to deploy it to usability professionals, the target audience, but it turns out that very few people were willing or capable of 3D-printing their own components. We learned from this so the project has not ended here, and we are nearly finished with a sequel, 5 years after the initial idea to reach our original vision.
</p></li><hr><li>
After meeting with Eda, a Masters student who wanted to work on a research project with me, the first note in my research journal was "discussed rewind: cool idea but low chance of publishing". I had no idea how true that would be, as this became the most challenging paper my group would publish. What started in January 2015 was published December 2018, 4 years later, and was passed from student to student after each one graduated. I lost count of the number of rejections.<p>
Many of the authors had never met each other, and it was a bittersweet moment for me when by pure coincidence, the original author, Eda, was standing in the hallway outside my office with the final author, Neilly, without knowing one other (which I immediately corrected by introducing them).</p><p>
What was challenging about this paper was the engineering work used existing known techniques, so we had to emphasize how the experience was a contribution on its own. This was hard to do in a study, as it wasn't about directly improving any specific aspect of life, but being able to experience it differently. I begged my old colleague Gifford to help in early 2018, and what put it over the finish line was a careful mixed methods descriptive writing based on the detailed analysis Gifford directed.
</p></li><hr><li>
This was the first time I had been involved in a project with both hardware and software components, so the system itself took longer than expected, and we were writing this paper down to the deadline. The paper had to be carefully crafted to describe a complex configuration, with 3D-printed parts, augmented reality, cameras and sensors, computer vision, heating and energy issues, and both wired and wireless networking.<p>
Reviews were mixed, but we thankfully had support from our metareviewer, "I am looking forward to hopefully a strong rebuttal so I can be your advocate at the UIST 2019 PC meeting." This encouragement was exactly what we needed in that moment.</p><p>
The effort was worth it, because this system led to a few other projects, and serves as a foundational paper for Jing's Ph.D. What we are still trying to figure out today is how to deploy this as a product to regular people, as the hardware requirements again posed a barrier for adoption.
</p></li><hr><li>
Nedi turned her earlier SleepCoacher paper into a fully automated process, completing our vision from the seed of an idea 6 years ago. The product described in this paper was shipped to the app stores and used by whoever would come across and download it, basically real usage by people we did not recruit. We maintained Android and iOS apps separately, and a server to do the calculations. It was a costly mistake to build out three separate systems; we should have started with something cross-platform and performed the calculations in the app itself to reduce the software maintenance from three systems to one.<p>
The paper was hard to publish, because unlike recruited and paid participants, our 5,000 app store installs (now 7,700) led to messy data—a lot of people never opened the app, or did so only once. Reviewers were unimpressed that thousands of installs only led to a couple hundred active participants, of which only about a fifth of the users tracked for enough nights to get useful information.</p><p>
In the end, it was a close decision but the CHI program committee decided that it could be acceptable if shepherded, "I am still leaning positive given the difficulty of the method, the importance of the topic, and complexity of the project as a whole." Being on the program committee myself that year, I wondered to another faculty member why our papers always seem to only barely get in, and they responded matter-of-factly, "all accepted papers barely get in," referring to the declining average scores at CHI over the years.
</p></li><hr><li>
This idea grew out of my NSF CAREER Award as the required educational component, where I proposed a classroom tool for large-class simultaneous design activities. But the code was written and rewritten several times by different teams even after that, because running a real-time collaborative system with over 100 simultaneous users introduced its own share of problems.<p>
There were many nervous moments leading up to each attempt. The tool failed the first semester or two that we tried it; the server would crash or some of the data would be lost or corrupted, and we would lose our chance to get data that semester.</p><p>
Submission-wise, reviewers always wanted more, so the paper itself went through multiple different narratives before being accepted in a tough revise and resubmit cycle. The mood around this revise and resubmit is best portrayed by a reviewer, "I find this to be a mostly solidly executed project, but I don't see a substantial contribution to CSCW / creativity support tools here. I am not sure if this can be fixed in a revision cycle, but if the authors are keen, ..."</p><p>
Well, we were keen.
</p></li><hr><li>
In 2014, I met with a faculty member in Psychiatry and Human Behavior to discuss a collaboration, where we concluded that the area of mental health lacked innovative computational techniques. I did a rough prototype of our initial idea in 2015, then students picked it up the following year. The work was passed between teams of students, mainly supporting the application development and real usage from a growing list of collaborators. It turns out that many clinical researchers felt the same, and this became our most funded project.<p>
However, the papers took a while and the one led by my group was rejected throughout 2018 and 2019. The hard part was because while we were using a fairly unique approach, I didn't have much experience writing about this topic. Reviewers would have varying opinions of what needed to change, so the text would waffle back and forth; we finally arrived at a version of the paper that was satisfying enough, and it was accepted for publication in 2020.</p><p>
About 20 people were involved at different points in time, but the study didn't start until 2017 so the paper itself was about 3 years in preparation. I feel like the overall goal of computational interventions for mental health is a good one, but it feels like we've only taken a small step.
</p><p><a href="https://jeffhuang.com/papers/Sochiatrist_CSCW20.pdf">Sochiatrist: Signals of Affect in Messaging Data</a>.
Talie Massachi, Grant Fong, Varun Mathur, Sachin Pendse, Gabriela Hoefer, Jessica Fu, Chong Wang, Nikita Ramoji, Nicole Nugent, Megan Ranney, Daniel Dickstein, Michael Armey, Ellie Pavlick, Jeff Huang.
CSCW 2020.
</p></li><hr><li>
We had been wanting to expand Nedi's SleepBandits app beyond sleep to all sorts of self-experimentation, and finally got a chance when NSF extended one of my expiring grants to fund this paper.<p>
Our first attempt to publish at CHI 2020 was rejected partly due to weak findings, but Nedi had already been preparing a second study to complement what we had. While it still required major edits, CHI 2021 accepted the paper after a straightforward rebuttal. Did I finally end a long streak of struggling to publish? We considered this our easiest paper, but it still took an 8-person team nearly 30 months.</p><p>
But maybe publishing faster or publishing more is not what it's about. I care more about this project as an app that people can use, so I rebuilt it in the cross-platform Flutter framework, with hopes to use it as a foundation for later work. Hopefully the papers are just a milestone towards people making their lives better through self-experimentation.
</p></li></div><p>After reviewing these notes, I'm a bit ambivalent. When I was a clueless student, I got lucky with my collaborators and acceptance decisions, which made all the difference. After becoming a professor, I had the experience yet the papers take even longer to publish.</p><p>Part of <em>why</em> is the focus on systems papers, which are <a href="http://dubfuture.blogspot.com/2009/11/i-give-up-on-chiuist.html">known to take 3-4 years</a>, but shipping them as products has been an even longer 4+ year agenda. Worth it, sure, but we'll never be like most groups that publish 5+ papers a year.</p><p>But I also think about how my group has encountered a lot of rejection, and keeping up morale was sometimes difficult. Bad news injects doubt and discouragement into students' minds, who then have to rally the team to continue the work in hope that acceptance is just around the corner. This dissonance is hard to manage.</p><p>The other thing I noticed is that papers that get the most citations later often got poor reviews or multiple rejections. They're usually about a new phenomenon, but the novelty can always be recast as "old thing, but just on the web" or "mostly engineering work, with so-so results". With this in mind, I should probably be generous in my own interpretation of what's novel when I review papers.</p><p>In retrospect, I am grateful for many key collaborators for their extra help in those times, and that some conferences like UIST accepted papers despite some obvious flaw because those papers ended up defining long-term research programs for multiple young researchers.</p><p>Thanks to Alexandra Papoutsaki, Bo Lu, Gifford Cheung, Tongyu Zhou, and Zainab Iftikhar for their comments on earlier drafts.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made an app that consolidated 18 apps (doc, sheet, form, site, chat…) (654 pts)]]></title>
            <link>https://nino.app</link>
            <guid>38901504</guid>
            <pubDate>Sun, 07 Jan 2024 14:34:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nino.app">https://nino.app</a>, See on <a href="https://news.ycombinator.com/item?id=38901504">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[First new vax in 30 years? (2021) (220 pts)]]></title>
            <link>https://mail-index.netbsd.org/port-vax/2021/07/03/msg003899.html</link>
            <guid>38901012</guid>
            <pubDate>Sun, 07 Jan 2024 13:27:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003899.html">https://mail-index.netbsd.org/port-vax/2021/07/03/msg003899.html</a>, See on <a href="https://news.ycombinator.com/item?id=38901012">Hacker News</a></p>
<div id="readability-page-1" class="page">
<!--X-Body-Begin-->
<!--X-User-Header-->
<address>
Port-vax archive
</address>
<!--X-User-Header-End-->
<!--X-TopPNI-->
<hr>
[<a href="https://mail-index.netbsd.org/port-vax/2021/05/03/msg003898.html">Date Prev</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003900.html">Date Next</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/04/17/msg003867.html">Thread Prev</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003906.html">Thread Next</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/date1.html#003899">Date Index</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/thread1.html#003899">Thread Index</a>][<a href="https://mail-index.netbsd.org/port-vax/2021/07/oindex.html">Old Index</a>]

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->

<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul>
<li><strong>To</strong>: <strong>port-vax List &lt;<a href="mailto:port-vax%NetBSD.org@localhost">port-vax%NetBSD.org@localhost</a>&gt;</strong></li>
<li><strong>Subject</strong>: <strong>First new vax in ...30 years? :-)</strong></li>
<li><strong>From</strong>: <strong>Anders Magnusson &lt;<a href="mailto:ragge%tethuvudet.se@localhost">ragge%tethuvudet.se@localhost</a>&gt;</strong></li>
<li>Date: Sat, 3 Jul 2021 12:14:02 +0200</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre>Hi,

</pre><tt>some time ago I ended up in an architectural discussion (risc vs cisc 
</tt><tt>etc...) and started to think about vax.
</tt><tt>Even though the vax is considered the "ultimate cisc" I wondered if its 
</tt><tt>cleanliness and nice instruction set still could be implemented 
</tt><tt>efficient enough.
</tt><tt>Well, the only way to know would be to try to implement it :-)&nbsp; I had an 
</tt><tt>15-year-old demo board with a small low-end FPGA (Xilinx XC3S400), so I 
</tt><tt>just had to learn Verilog and try to implement something.&nbsp; And it just 
</tt><tt>passed EVKAA.EXE:
</tt><pre>
&gt;FR00000000 200
&gt;G
EVKAA V10.4 Hardcore Instruction Test

Hit any key to continue
EVKAA&nbsp;&nbsp; V10.4 pass # 1(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 19(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 32(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 4B(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 64(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 7D(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # 96(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # AF(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # C8(X) done!
EVKAA&nbsp;&nbsp; V10.4 pass # E1(X) done!^C
Input breakpoint: 000000001
&gt;

</pre><tt>(the microcode console works like the Nova4 microcode console - simpler 
</tt><tt>to implement than the VAX style...)
</tt><tt>It runs at 50MHz, but could easily be increased to about 80, just to 
</tt><tt>program DCM.
</tt><pre>
Photo of the "vax": <a rel="nofollow" href="https://www.ludd.ltu.se/~ragge/pics/IMG_0837.jpg">https://www.ludd.ltu.se/~ragge/pics/IMG_0837.jpg</a>
</pre><tt>I had to get a new FPGA board, since I started to get bit errors on the 
</tt><tt>old one, so I bought a chinese board with essentially the same FPGA 
</tt><tt>(XC3S500E):
</tt><pre>
</pre><tt>I have implemented all addressing modes, the interrupt hierarchy, 
</tt><tt>timers, and 151 instructions.
</tt><pre>No memory management yet though, but that should be quite straight-forward.

</pre><tt>Currently it uses about 70% of available FPGA resources, which is around 
</tt><tt>6000 LUTs (which is quite inefficient implemented, since I have learned 
</tt><tt>how to do verilog programming while writing the code...)
</tt><pre>
</pre><tt>I'll follow up this mail with two more, one about the implementation and 
</tt><tt>one about how to go forward with the vax architecture :-)
</tt><pre>
-- R
</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="003992" href="https://mail-index.netbsd.org/port-vax/2021/07/07/msg003992.html">Re: First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Michael Parson</li></ul></li>
<li><strong><a name="003941" href="https://mail-index.netbsd.org/port-vax/2021/07/05/msg003941.html">RE: [EXTERNAL] First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Luke Brennan</li></ul></li>
<li><strong><a name="003912" href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003912.html">Re: First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Paul Koning</li></ul></li>
<li><strong><a name="003906" href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003906.html">Re: First new vax in ...30 years? :-)</a></strong>
<ul><li><em>From:</em> Dave McGuire</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->
<hr>
<ul>
<li>Prev by Date:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/05/03/msg003898.html">Re: Bountysource campaign for gcc-vax</a></strong>
</li>
<li>Next by Date:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003900.html">New vax - implementation :-)</a></strong>
</li>

<li>Previous by Thread:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/04/17/msg003867.html">HEADS UP: GCC 10 now default on several ports</a></strong>
</li>
<li>Next by Thread:
<strong><a href="https://mail-index.netbsd.org/port-vax/2021/07/03/msg003906.html">Re: First new vax in ...30 years? :-)</a></strong>
</li>

<li>Indexes:
<ul>
<li><a href="https://mail-index.netbsd.org/port-vax/2021/07/date1.html#003899">
<strong>reverse Date</strong></a></li>
<li><a href="https://mail-index.netbsd.org/port-vax/2021/07/thread1.html#003899">
<strong>reverse Thread</strong></a></li>
<li><a href="https://mail-index.netbsd.org/port-vax/2021/07/oindex.html">
<strong>Old Index</strong></a></li>
</ul>
</li>
</ul>

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<strong>
<a href="https://mail-index.netbsd.org/index.html">Home</a> |
<a href="https://mail-index.netbsd.org/port-vax/index.html">Main Index</a> |
<a href="https://mail-index.netbsd.org/port-vax/tindex.html">Thread Index</a> |
<a href="https://mail-index.netbsd.org/port-vax/oindex.html">Old Index</a>
</strong>
<!--X-User-Footer-End-->


</div>]]></description>
        </item>
    </channel>
</rss>