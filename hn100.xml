<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 06 Apr 2024 14:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How we communicate (110 pts)]]></title>
            <link>https://37signals.com/how-we-communicate</link>
            <guid>39951086</guid>
            <pubDate>Sat, 06 Apr 2024 09:08:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://37signals.com/how-we-communicate">https://37signals.com/how-we-communicate</a>, See on <a href="https://news.ycombinator.com/item?id=39951086">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>The how, where, why, and when we communicate. Long form asynchronous? Real-time chat? In-person? Video? Verbal? Written? Via email? In Basecamp? How do we keep everyone in the loop without everyone getting tangled in everyone else’s business? It’s all in here.</p>
  </div><div>

    <h2 id="rules-of-thumb-and-general-philosophy">Rules of thumb, and general philosophy</h2>
    <p>Below you’ll find a collection of general principles we try to keep in mind at 37signals when communicating with teammates, within departments, across the company, and with the public. They aren’t requirements, but they serve to create boundaries and shared practices to draw upon when we do the one thing that affects everything else we do: communicate.</p>
    <ol>
      <li>You can not not communicate. Not discussing the elephant in the room is communicating. Few things are as important to study, practice, and perfect as clear communication.</li>
      <li>Real-time sometimes, asynchronous most of the time.</li>
      <li>Internal communication based on long-form writing, rather than a verbal tradition of meetings, speaking, and chatting, leads to a welcomed reduction in meetings, video conferences, calls, or other real-time opportunities to interrupt and be interrupted.</li>
      <li>Give meaningful discussions a meaningful amount of time to develop and unfold. Rushing to judgement, or demanding immediate responses, only serves to increase the odds of poor decision making.</li>
      <li>Meetings are the last resort, not the first option.</li>
      <li>Writing solidifies, chat dissolves. Substantial decisions start and end with an exchange of complete thoughts, not one-line-at-a-time jousts. If it’s important, critical, or fundamental, write it up, don’t chat it down.</li>
      <li>Speaking only helps who’s in the room, writing helps everyone. This includes people who couldn’t make it, or future employees who join years from now.</li>
      <li>If your words can be perceived in different ways, they’ll be understood in the way which does the most harm.</li>
      <li>Never expect or require someone to get back to you immediately unless it’s a true emergency. The expectation of immediate response is toxic.</li>
      <li>If you have to repeat yourself, you weren’t clear enough the first time. However, if you’re talking about something brand new, you may have to repeat yourself for years before you’re heard. Pick your repeats wisely.</li>
      <li>Poor communication creates more work.</li>
      <li>Companies don’t have communication problems, they have miscommunication problems. The smaller the company, group, or team, the fewer opportunities for miscommunication.</li>
      <li>Five people in a room for an hour isn’t a one hour meeting, it’s a five hour meeting. Be mindful of the tradeoffs.</li>
      <li>Be proactive about “wait, what?” questions by providing factual context and spatial context. Factual are the things people also need to know. Spatial is where the communication happens (for example, if it’s about a specific to-do, discuss it right under the to-do, not somewhere else).</li>
      <li>Communication shouldn’t require schedule synchronization. Calendars have nothing to do with communication. Writing, rather than speaking or meeting, is independent of schedule and far more direct.</li>
      <li>“Now” is often the wrong time to say what just popped into your head. It’s better to let it filter it through the sieve of time. What’s left is the part worth saying.</li>
      <li>Ask yourself if others will feel compelled to rush their response if you rush your approach.</li>
      <li>The end of the day has a way of convincing you what you’ve done is good, but the next morning has a way of telling you the truth. If you aren’t sure, sleep on it before saying it.</li>
      <li>If you want an answer, you have to ask a question. People typically have a lot to say, but they’ll volunteer little. Automatic questions on a regular schedule help people practice sharing, writing, and communicating.</li>
      <li>Occasionally pick random words, sentences, or paragraphs and hit delete. Did it matter?</li>
      <li>Urgency is overrated, ASAP is poison.</li>
      <li>If something’s going to be difficult to hear or share, invite questions at the end. Ending without the invitation will lead to public silence but private conjecture. This is where rumors breed.</li>
      <li>Where you put something, and what you call it, matters. When titling something, lead with the most important information. Keep in mind that many technical systems truncate long text or titles.</li>
      <li>Write at the right time. Sharing something at 5pm may keep someone at work longer. You may have some spare time on a Sunday afternoon to write something, but putting it out there on Sunday may pull people back into work on the weekends. Early Monday morning communication may be buried by other things. There may not be a perfect time, but there’s certainly a wrong time. Keep that in mind when you hit send.</li>
      <li>Great news delivered on the heels of bad news makes both bits worse. The bad news feels like it’s being buried, the good news feels like it’s being injected to change the mood. Be honest with each by giving them adequate space.</li>
      <li>Time is on your side, rushing makes conversations worse.</li>
      <li>Communication is lossy, especially verbal communication. Every  hearsay hop adds static and chips at fidelity. Whenever possible, communicate directly with those you’re addressing rather than passing the message through intermediaries.</li>
      <li>Ask if things are clear. Ask what you left out. Ask if there was anything someone was expecting that you didn’t cover. Address the gaps before they widen with time.</li>
      <li>Consider where you put things. The right communication in the wrong place might as well not exist at all. When someone relies on search to find something it’s often because it wasn’t where they expected something to be.</li>
      <li>Communication often interrupts, so good communication is often about saying the right thing at the right time in the right way with the fewest side effects.</li>
    </ol>

    <h2 id="communicating-day-to-day">Communicating day-to-day</h2>
    <p>This section includes specific examples of how we apply our philosophy day-to-day across the company. Since communication often interrupts, valuing each other’s time and attention is a critical consideration. Keeping people in the loop is important, but asking them to follow along with everything is a distraction. That’s why we follow reliable, predictable methods to share the right kind of information at the right time in the right place.</p>

    <h4 id="basic-toolset">Basic toolset</h4>
    <p>98% of our internal communication happens inside Basecamp. That means all company-wide discussions, social chatter, project-related work, sharing of ideas, internal debates, <a href="https://basecamp.com/features/automatic-check-ins">automatic check-ins</a>, status updates, policy updates, and all official decisions and announcements all happen in Basecamp. A single centralized tool keeps everything together and creates a single source of truth for everyone across the company. We don’t use email internally (we do externally), we don’t use separate chat tools like Slack or Teams, and we rarely have in-person meetings. We do use Zoom or Skype for the occasional video conference between two or three people. And we occasionally discuss a pull request in GitHub.</p>

    <h4 id="automatic-daily-what-did-you-work-on-today">Automatic daily: “What did you work on today?</h4>
    <p>Every workday at 16:30, Basecamp (the product) automatically asks every employee “What did you work on today?” Whatever people write up is shared with everyone in the company. Everyone’s responses are displayed on a single page, grouped by date, so anyone who’s curious about what’s happening across the company can simply read from top to bottom. And if you have a question about anything, you can comment on anyone’s “what did you work on today?” check-in to keep the conversation in context.</p>
    <p>This routine is about loose accountability and strong reflection. Writing up what you did every day is a great way to think back about what you accomplished and how you spent your time.</p>
    <p>Some people just jot down a few bullets. Others write multi-paragraph stories to share — and document — the thinking behind their work. There are no requirements here. We just ask everyone to write in their own style.</p>

    <h4 id="automatic-weekly-what-will-you-be-working-on-this-week">Automatic weekly: “What will you be working on this week?”</h4>
    <p>Every Monday morning, Basecamp automatically asks everyone “What will you be working on this week?” This is a chance for everyone to lay out the big picture of their week. It’s not about regurgitating individual tasks, or diving headlong into the minutia of the week. It’s generally just your 10,000 foot view of the week ahead. The big picture items, the general themes. It sets your mind up for the work ahead, and, collectively, it gives everyone a good sense of what’s happening across the company this week.</p>

    
    <p>Every few weeks, or once a month, Basecamp will automatically ask everyone a social-style question. “What books are you reading?” Or “Try anything new lately?” Or “Anything inspire you lately?” Or “Seen any great design recently?” Or “What did you do this weekend?” These entirely optional questions are meant to shake loose some stuff that you’d love to share with everyone else, but you hadn’t had an opportunity to do so. This kind of internal communication helps grease the social gears. This is especially useful for remote teams, like ours. When we know each other a little better, we work a little better together.</p>

    <h4 id="-reflect-every-6-weeks-heartbeats">← Reflect every 6 weeks: Heartbeats</h4>
    <p>Heartbeats summarize the last ~6-weeks of work for a given team, department, or individual (if that person is a department of one). They’re written by the lead of the group, and they’re meant for everyone in the company to read. They summarize the big picture accomplishments, they detail the little things that mattered, and they generally highlight the importance of the work. They’ll also shine a light on challenges and difficulties along the way. They’re a good reminder that it’s not all sunshine all the time. On balance, Heartbeats are wonderful to write, fun to read, and they help everyone — including those not directly involved with the work — reflect on jobs well done and progress well made.</p>

    <h4 id="-project-every-6-weeks-kickoffs">→ Project every 6 weeks: Kickoffs</h4>
    <p>Kickoffs are essentially the opposites of Heartbeats. Rather than reflect, they project. They’re all about what the team plans on taking on over the next 6 weeks. Projects, initiatives, revamps, whatever it might be, if it’s on the slate, it gets summarized in the Kickoff. While Kickoffs detail specific work for a specific group, they’re also meant for full-company consumption. Like Heartbeats, they’re written by the team lead. Kickoffs are broad in scope, so they don’t cover all the details in the work ahead — the teams doing the work are the ones that wade into those weeds. We don’t want to overwhelm everyone with details that don’t matter. If anyone’s curious about something included in a Kickoff, they’re free to post a comment and ask a question.</p>

    <h4 id="whenever-relevant-announcements">Whenever relevant: Announcements</h4>
    <p>Occasionally we update an internal policy. Something about vacation time, or a new benefit, or reiterating that 40 hour weeks means 40 hour weeks. When we have something to announce company-wide, we don’t send an email. Email is decentralized and there’s no permanent record in a permanent place everyone can see. Instead, we post it either to the 37signals HQ message board or as a comment on an existing policy document stored in Basecamp. This means everyone sees the same thing, everyone hears the same thing, and everyone knows the same thing — including future employees who are yet to join. We now have a shared truth.</p>

    <h4 id="day-to-day-project-work-in-context">Day-to-day project work: In context</h4>
    <p>Effective communication requires context. Saying the right thing in the wrong place, or without proper detail, leads to double work and messages being missed. That’s why we spin up a separate Basecamp project for every project we work on. Everything related to that project is communicated inside that project. All the tasks, all the discussions, all the documents, all the debates, and all the decisions happen inside those walls. Everyone who needs access, has access. Every Basecamp project is a capsule of everything someone needs to know about that work project.</p>
    <p>Further, we take spatial context seriously. If we’re discussing a specific task, we discuss it in the comment section below the task itself. If we’re talking about a specific document, we discuss it in the comments attached to the document. Communications stay attached to the thing we’re discussing. This provides the full story in one reliable place. The alternative is terrible — communication detached from the original source material, discussions all over the place, fragmented conversations missing entire chunks of time and detail, etc. Basecamp’s “everything is commentable” feature is what makes this possible for us.</p>

    <h2 id="other-resources">Other resources</h2>
    <p>We’ve detailed the pros and cons of chat vs. long form writing in our infamous “<a href="https://37signals.com/group-chat-problems">Group Chat: Group Stress</a>” guide. We definitely recommend checking it out.</p>
    <p>You’ll also find a detailed explanation of how our teams work day-to-day on software projects in “<a href="https://basecamp.com/shapeup">Shape Up: Stop Running in Circles and Ship Work that Matters</a>”.</p>
    <p>Basecamp-exclusive <a href="https://basecamp.com/features/hill-charts">Hill Charts</a> help us see where projects really stand without having to rely on inefficient status meetings, daily standups, or regular in-person check-ins.</p>
    <p>The <a href="https://basecamp.com/handbook">37signals Employee Handbook</a> is also worth checking out. It explains how we’re structured, how we define titles and roles, our full benefits package, our company values, the responsibilities of individual contributors, managers, and executives, and other essential bits.</p>

    <h2 id="anything-else">Anything else?</h2>
    <p>We hope this guide was useful, but we’re sure we’re missing something. What questions do you still have? What did you hope to learn that you didn’t? Was anything more confusing than clarifying? What would have made this guide more helpful? It’s a work in progress, and we’ll update as necessary based on your feedback. Please send questions, suggestions, and thoughts directly to the author, Jason Fried, at <a href="mailto:jason@37signals.com">jason@37signals.com</a>. Thanks!</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What I think about when I edit (2019) (152 pts)]]></title>
            <link>https://evaparish.com/blog/how-i-edit</link>
            <guid>39950760</guid>
            <pubDate>Sat, 06 Apr 2024 07:48:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evaparish.com/blog/how-i-edit">https://evaparish.com/blog/how-i-edit</a>, See on <a href="https://news.ycombinator.com/item?id=39950760">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="44" id="item-5d0ba6c71b9e0500014435be" data-layout-label="Post Body" data-type="item"><p>I’m often asked to edit friends’ or coworkers’ writing, anything from emails to short stories to documentation. Recently, someone asked me <em>how</em> I edit. What am I looking for? How do I know what changes to make? That made me stop and think about what I’ve been doing semi-instinctually.</p>
<p>In this post, I want to distill the major points of editing that I believe in but haven’t spelled out until now. Much of this advice applies across genres. Personally, I wrote academic papers in college, write poetry and short fiction in my free time, and write technical documentation for work, and I’ve applied the same basic editing techniques to all.</p>
<p><strong>I also think that different genres inform each other.</strong> There are principles I’ve taken from fiction writing that make my technical language even clearer, and learning just how much people skim when reading technical documentation has improved how I format and write things like emails.</p>
<p>Here are my recommendations:</p>
<ul>
<li><a href="#decide">Decide what you’re actually saying</a></li>
<li><a href="#repeat">Repeat yourself (within reason)</a></li>
<li><a href="#simplify">Simplify</a></li>
<li><a href="#passive-voice">Eliminate passive voice</a></li>
<li><a href="#adverbs">Don’t use adverbs</a></li>
<li><a href="#assume">Don’t assume knowledge</a></li>
<li><a href="#tone">Be aware of your tone</a></li>
<li><a href="#jargon">Avoid jargon and cliches</a></li>
<li><a href="#whitespace">Make use of whitespace</a></li>
</ul>
<hr>

<h2 id="decide-what-you-re-actually-saying">Decide what you’re actually saying</h2>
<p>Before you ever get to editing at the sentence level, you have to determine whether you’ve said what you meant to say.</p>
<p>I recommend writing a <em>preamble</em>*, just for your own use, on everything you write. Take a few minutes and consider what you’re trying to say. <strong>What is your main point? Who are you writing for?</strong> Then <em>actually write this information down</em> at the top of your document (or notebook, or cafe napkin) so it’s there staring you in the face as you work. As you write, and as you edit, you can compare what you have on the page to what you set out to do.</p>
<p>It also forces you to think about what your message actually is. Say you want to write a blog post: if you can’t summarize the point you want to make in a sentence or two, how are you going to write a coherent post?</p>
<p>*I took this name from something we do in my creative writing classes at the <a href="https://www.writerstudio.com/about/">Writers Studio</a> in New York. As part of every assignment, we have to write a preamble: a few sentences at the top of the page mentioning what we intended to do, including the kind of narrator, tone, and mood. When other students critique your work in class, they’re holding it up to this model and evaluating whether you achieved your aim, focusing on <em>craft</em>, not whether they like your main character.</p>

<h2 id="repeat-yourself-within-reason-">Repeat yourself (within reason)</h2>
<p>Even if you think you’ve made your point very clearly, it’s worth restating it at the beginning and end of what you’re writing to make sure the reader gets it.</p>
<p>This principle applies across most genres. In documentation, a good tutorial will have a brief introduction to what you’re going to do, then the actual procedure, and finally a way for you to verify that you’ve done the thing correctly. In a blog post, you should introduce what you’re going to discuss in the post, then actually do that, and have a short summary at the end. And so on.</p>
<p>“Repeat yourself” applies at the language level too. One of the best writing tips I've ever gotten was to avoid using <a href="https://www.gingersoftware.com/content/grammar-rules/demonstrative-pronouns/">demonstrative pronouns</a>. Instead of saying “this” or “that,” you should add a noun to spell out exactly what you’re referring to, <em>even if you’ve just mentioned it</em>. </p>
<blockquote>
<p><em>Example:</em> We only have two boxes left. To solve <strong>this</strong>, we should order more.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> We only have two boxes left. To solve this shortage, we should order more.</p>
</blockquote>
<blockquote>
<p><em>Example:</em> Click next and enter your credentials when prompted. <strong>That</strong> will take you to the home screen.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Click next and enter your credentials when prompted. Successfully authenticating will take you to the home screen.</p>
</blockquote>
<p>This duplication can feel repetitive when you’re writing it, but it won’t feel repetitive to your reader—it’ll make your writing clearer and easier to follow.</p>
<p>In summary: when editing, <strong>look for ways that you can restate your point, clarify, or provide closure for the reader</strong>.</p>

<h2 id="simplify">Simplify</h2>
<p>When I edit someone else’s work, my number one quest is to remove words. Eliminate the fluff. Are there constructions that can be shortened? Any extraneous words that don’t add to the meaning of the sentence?</p>
<blockquote>
<p><em>Example:</em> You will need to run this script.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Run this script.</p>
</blockquote>


<blockquote>
<p><em>Example:</em> You can aid in readability by making sure that the names of things properly communicate what they do.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Make sure the names of things communicate what they do.</p>
</blockquote>
<p><strong>Unless you have a very specific reason not to, strive to get to the point as quickly as possible.</strong> Don’t bury your meaning in excess words and flowery constructions.</p>
<p>Other ways to simplify:</p>
<h3 id="-you-should-you-can-">“You should”/“You can”</h3>
<p>When writing instructions, anywhere you say “<em>You should X</em>” or “<em>You can X,</em>” replace it with the <a href="http://www.cws.illinois.edu/workshop/writers/verbmood/">imperative mood of the verb</a>.</p>
<blockquote>
<p><em>Example:</em> You should save the file to your home directory.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Save the file to your home directory.</p>
</blockquote>
<p>This change eliminates a couple words, making the sentence easier to read, and brings the reader straight to the point.</p>
<h3 id="-of-and-for-clauses">“Of” and “for” clauses</h3>
<p>Instead of using constructions with “of” or “for,” rewrite the sentence to put more information before the noun. This ordering makes the sentence more efficient.</p>
<blockquote>
<p><em>Example:</em> The manager of the team responsible for marketing</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> The marketing team’s manager</p>
</blockquote>
<p>In the rearranged version above, the reader can more quickly grasp what you mean, instead of having to revise her understanding after each clause.</p>
<h3 id="split-it-up">Split it up</h3>
<p>Break up long sentences into multiple shorter sentences.</p>
<blockquote>
<p><em>Example:</em> Due to the Acme project which just completed a major milestone of having all non-staging servers running in the Foobaz environment, we now see build times of sub-10 minutes which were previously taking over an hour when running with the XYZ plan.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> The Acme project recently completed a major milestone: all non-staging servers are now running in the Foobaz environment. Builds now take fewer than 10 minutes to complete. This change is a significant improvement, as builds on the XYZ plan previously took over an hour to complete.</p>
</blockquote>
<p>Also, break up sentences by adding commas <a href="https://owl.purdue.edu/owl/general_writing/punctuation/commas/extended_rules_for_commas.html">where appropriate</a>. For example, I’ve noticed a trend towards people dropping commas after subordinate clauses. I always add them back when I edit:</p>
<blockquote>
<p><em>Example:</em> If you’re looking for me I’ll be in my office.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> If you’re looking for me, I’ll be in my office.</p>
</blockquote>


<blockquote>
<p><em>Example:</em> Due to the fog our flight was delayed.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Due to the fog, our flight was delayed.</p>
</blockquote>
<p>I suspect that this is because, when we speak colloquially, we don’t pause at that point in the sentence. However, grammatically, <a href="https://owl.purdue.edu/owl/general_writing/punctuation/independent_and_dependent_clauses/index.html">you need a comma after a subordinate (or dependent) clause when it comes at the start of a sentence</a>. Besides being “correct,” the comma helps the reader pause and process what they’ve just read before moving on to the rest of the sentence. Using commas after subordinate clauses improves reader comprehension.</p>

<h2 id="eliminate-passive-voice">Eliminate passive voice</h2>
<p>You’ve heard this advice before. But you should understand <em>why</em> you shouldn’t use passive voice in your writing. It’s not just “bad style.”</p>
<p><strong>Passive voice obscures who or what is performing the action.</strong> Rewriting a passive construction to be active <em>almost always</em> makes what you’re saying clearer and makes the sentence easier to read, because your reader can attribute the action to the right person or thing.</p>
<blockquote>
<p><em>Example:</em> The fire alarm was pulled and the building was evacuated.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> The fire marshal pulled the alarm and the employees evacuated the building.</p>
</blockquote>


<blockquote>
<p><em>Example:</em> Millions of dollars were embezzled from the company.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Two executives embezzled millions of dollars from the company.</p>
</blockquote>
<p>It may even help <em>you</em> understand what you’re saying better. If you’re describing a system you built, and you say “An alert is triggered and the job is started”—do you know <em>how</em> those things happen? Which service triggers the alert? Which component is responsible for executing the job? In rewriting, you may realize that something doesn’t work as you expected, or that you don’t know how it works.</p>
<p>In technical documentation, you lose precision when you don’t attribute the action to someone or something. And in all writing, refining your language refines your understanding of the world.</p>

<h2 id="don-t-use-adverbs">Don’t use adverbs</h2>
<p>This aversion to adverbs is one of the principles I’ve taken from fiction-writing.</p>
<p><strong>You can almost always replace an adverb with a better, more specific verb, or describe what you mean instead.</strong> Being more specific is especially key in fiction, but I believe in stripping out adverbs in all types of writing.</p>
<p>There is nothing <em>inherently</em> wrong with adverbs. They are just part of a category of things that I believe are lazy in writing. When I say “He laughed loudly,” I’m relying on my reader somehow intuiting the precise volume of his laughter. “Loudly” could mean a million things, but what I really had in mind is that “He laughed with the kind of booming abandon that made the whole restaurant turn around and look.”</p>
<p>People also often use adverbs as a hedge: “Basically, it's this.” “Essentially, this is what I’m saying.” Is it, or isn’t it? Remove the adverb and commit to saying whatever you’re saying.</p>

<h2 id="don-t-assume-knowledge">Don’t assume knowledge</h2>
<p>It’s easy to fall into this trap when you’re writing about something you know well: you forget to consider what <em>you</em> know that your audience <em>doesn’t</em>. You don’t take a step back and provide relevant context. Imagine how much more pleasant it would be to read emails, documentation, etc., if people actually spelled out those TLAs (three-letter acronyms)!</p>
<p>Let’s start with an example, and look at a few ways we can improve it.</p>
<blockquote>
<p><em>Example:</em> This chart shows the TTFB for our website over the past week.</p>
</blockquote>
<p>To some people, this sentence makes perfect sense. To many people… not so much.</p>
<ul>
<li><p><em>Spell out acronyms on first use.</em> Any time you introduce an acronym or an initialism in a document, spell out what it means and put the acronym in parentheses. Thereafter, you can use the acronym by itself.</p>
<blockquote>
<p><em>Revision 1:</em> This chart shows the <u data-preserve-html-node="true">time to first byte (TTFB) metric</u> for our website over the past week.</p>
</blockquote>
<p>  You might think it’s obvious what an acronym means, but a new reader may not.</p>
</li>
</ul>
<ul>
<li><p><em>Add a phrase or a sentence briefly explaining a concept when you introduce it.</em></p>
<blockquote>
<p><em>Revision 2:</em> This chart shows the time to first byte (TTFB) metric for our website over the past week. <u data-preserve-html-node="true">TTFB measures how long it takes from when a user makes an HTTP request to when the user’s browser loads the first byte of data. It’s used as an indicator of how responsive a website is</u>.</p>
</blockquote>
</li>
</ul>
<ul>
<li><p><em>Link out to further reading.</em> Once you define a concept and the corresponding acronym, provide a link to somewhere that the reader can learn more about that concept if they’re still curious. You don’t have to call attention to the link.</p>
<blockquote>
<p><em>Revision 3:</em> This chart shows the <a href="https://en.wikipedia.org/wiki/Time_to_first_byte">time to first byte (TTFB) metric</a> for our website over the past week. TTFB measures how long it takes from when a user makes an HTTP request to when the user’s browser loads the first byte of data. It’s used as an indicator of how responsive a website is.</p>
</blockquote>
</li>
</ul>
<p>By now, your readers are with you, and they’re ready to proceed, feeling confident that they have an idea of what you’re talking about.</p>

<h2 id="be-aware-of-your-tone">Be aware of your tone</h2>
<p>Know what kind of tone you’re going for, and be consistent. You can be colloquial or formal, but not both.</p>
<blockquote>
<p><em>Example:</em> We were really into this new framework that we found for like a minute or two, but the metrics captured by the system do not correspond precisely enough to our investigative goals to be useful.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> We were initially enthusiastic about the X framework, but we found that it did not capture the metrics we were looking for.</p>
</blockquote>
<p>The original sentence starts out <em>very</em> colloquial and then morphs into formal, almost academic language. In the best case, you’ll confuse your readers about why you switched. In the worst case, you’ll completely distract them from what you’re saying.</p>

<h2 id="avoid-jargon-and-cliches">Avoid jargon and cliches</h2>
<p>In the business world, <a href="https://www.grantthornton.com/-/media/content-page-files/press-releases/2018/Jargon-Index-2018.ashx?la=en">jargon</a> means things like “deep dive“ and “low-hanging fruit”. Elsewhere, we love to use cliches. Especially baseball metaphors, for some reason: “step up to the plate,” “hit it out of the park,” “take a swing at it.”</p>
<p>It will always be better and clearer when you say exactly what you mean. <strong>Using jargon is lazy, and it assumes that the reader is part of the in-group that uses that jargon</strong> (see also: Don’t assume knowledge, above). It can be difficult for non-native English speakers (or non-Americans, when it comes to baseball) to follow your writing when you use jargon and cliches.</p>
<blockquote>
<p><em>Example:</em> tl;dr, if you can hack something together by EOD, that would be great.</p>
</blockquote>
<blockquote>
<p><em>Revision:</em> Can you deliver a prototype by the end of today?</p>
</blockquote>
<p>The original sentence has incomprehensible acronyms and tech slang, and doesn’t even sound like a request. The second one is straightforward, and asks for what the writer needs and by when.</p>

<h2 id="make-use-of-whitespace">Make use of whitespace</h2>
<p>Whitespace is key for technical documentation but can also be used to great effect in blog posts, emails, and elsewhere. It’s hard for people to read long paragraphs, especially on a computer screen. They will zone out. Ensure that readers stay with you by visually breaking up the page and making your key points easy to identify.</p>
<p>A few suggestions:</p>
<ul>
<li>Break up long paragraphs into multiple shorter ones.</li>
<li>Use useful <strong>subheadings</strong> to give your document some structure and allow readers to skip ahead to the section they’re interested in.</li>
<li>Use <strong>lists</strong> where relevant, because it’s easier to read a bulleted list of items than to read a paragraph with the same information.<ul>
<li>When you need to convey large amounts of information (for example, in reference documentation), <strong>tables</strong> are even better than lists.</li>
</ul>
</li>
<li>Use <strong>bold</strong> so that readers who skim (i.e., everyone) will still pick out your main points. (For an example, see the body of this post.)</li>
</ul>
<hr>
<h2 id="conclusion">Conclusion</h2>
<p>To simplify what you’ve just read, my editing philosophy can be reduced to two tenets:</p>
<ul>
<li><em>Say exactly what you mean</em>, which means not relying on adverbs, jargon, cliches, or hedges, and</li>
<li><em>Take out all unnecessary words</em>.</li>
</ul>
<p>Keeping these two tenets in mind will help you in your own writing, and will give you a framework for evaluating other people’s writing. Over time, as you practice, you’ll develop your own style and preferences. You may end up diverging from some of my recommendations, and that’s great, as long as you know <em>why</em> you’re doing so. <strong>The point of editing is to think about how you’re using language and to make choices that suit the message you want to deliver</strong>, <em>not</em> to unquestioningly follow rules—mine or anyone else’s.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft blocks even more customization apps in Windows 11 version 24H2 (117 pts)]]></title>
            <link>https://www.neowin.net/news/microsoft-blocks-even-more-customization-apps-in-windows-11-version-24h2/</link>
            <guid>39950557</guid>
            <pubDate>Sat, 06 Apr 2024 06:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.neowin.net/news/microsoft-blocks-even-more-customization-apps-in-windows-11-version-24h2/">https://www.neowin.net/news/microsoft-blocks-even-more-customization-apps-in-windows-11-version-24h2/</a>, See on <a href="https://news.ycombinator.com/item?id=39950557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                    <span>When you purchase through links on our site, we may earn an affiliate commission. <a href="https://www.neowin.net/terms">Here’s how it works</a>.</span>
                        
                        
                        <p>
    
    <time datetime="Apr 5, 2024 14:48 EDT" pubdate="pubdate">
    Apr 5, 2024 14:48 EDT
    </time>
         · <span>Hot!</span>    
    </p>

                    </div><div itemprop="articleBody">
                                                                        <p><img alt="Windows 11 version 24H2 image" src="https://cdn.neowin.com/news/images/uploaded/2024/02/1707477938_windows_11_24h2_story.jpg"></p>

<p>Users recently noticed that third-party apps for customizing the user interface no longer work in <a href="https://www.neowin.net/news/alleged-windows-11-24h2-rtm-is-almost-upon-us-final-release-schedule-out-too/">the upcoming Windows 11 version 24H2</a>. Not only does Microsoft not allow you to run those apps, but it even blocks you from upgrading to newer builds. StartAllBack, a popular tool for tweaking the taskbar and Start menu in Windows 11, <a href="https://www.neowin.net/news/microsoft-is-blocking-windows-11-build-upgrades-on-systems-with-startallback/">was among the first to fail on 24H2</a>. Sadly, it is not the only one. ExplorerPatcher also no longer works in Windows 11 24H2.</p>

<p><a href="https://github.com/valinet/ExplorerPatcher">ExplorerPatcher</a> from Valinet is quite a popular app that lets you bring back the old Windows 10 taskbar in Windows 11, apply additional modifications to make Windows 11 slightly better, and restore some of its missing features. Windows 11 version 24H2 is now flagging ExplorerPatcher as incompatible due to "security or performance issues" with the following message:</p>

<blockquote>
<p>This app can't run because it causes security or performance issues on Windows. A new version may be available. Check with your software provider for an updated version that runs on this version of Windows.</p>
</blockquote>

                            <!-- PLACE THIS SECTION INSIDE OF YOUR BODY WHERE YOU WANT THE VIDEO PLAYER TO RENDER -->
            <figure><a href="https://cdn.neowin.com/news/images/uploaded/2024/04/1712341497_explorerpatcher.jpg"><img alt="ExplorerPatcher error in Windows 11 version 24h2" src="https://cdn.neowin.com/news/images/uploaded/2024/04/1712341497_explorerpatcher.jpg"></a></figure><p>Like in the case of StartAllBack, you can bypass the block by simply renaming the executable to something else. If you want to upgrade to a newer build, delete the app, update your system, and then launch it using a renamed executable.</p>

<p>Windows 11 version 23H2 does not mind you using StartAllBack or ExplorerPatcher—both applications launch just fine without any warnings. It is possible that Microsoft blocked those apps due to a higher number of crashes on build 26100, which is allegedly version 24H2 RTM build.</p>

<p>Windows 11 version 24H2 may cause some headaches for those relying on third-party apps for user interface customization. The latest builds ship with the flag that prevents restoring the old taskbar from the Windows 10 era enabled by default. This could be a sign of Microsoft wanting to remove old components from Windows 11 as it moves forward.</p>
                        
                        
                                                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NIST researchers use cellphone compass to measure glucose (114 pts)]]></title>
            <link>https://www.nist.gov/news-events/news/2024/04/nist-researchers-use-cellphone-compass-measure-tiny-concentrations</link>
            <guid>39948840</guid>
            <pubDate>Sat, 06 Apr 2024 00:18:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nist.gov/news-events/news/2024/04/nist-researchers-use-cellphone-compass-measure-tiny-concentrations">https://www.nist.gov/news-events/news/2024/04/nist-researchers-use-cellphone-compass-measure-tiny-concentrations</a>, See on <a href="https://news.ycombinator.com/item?id=39948840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  
  
      <p>Nearly every modern cellphone has a built-in compass, or magnetometer, that detects the direction of Earth’s magnetic field, providing critical information for navigation. Now a team of researchers at the National Institute of Standards and Technology (NIST) has developed a technique that uses an ordinary cellphone magnetometer for an entirely different purpose — to measure the concentration of glucose, a marker for diabetes, to high accuracy.</p><p>The same technique, which uses the magnetometer in conjunction with magnetic materials designed to change their shape in response to biological or environmental cues, could be used to rapidly and cheaply measure a host of other biomedical properties for monitoring or diagnosing human disease. The method also has the potential to detect environmental toxins, said NIST scientist Gary Zabow.</p><p>In their proof-of-concept study, Zabow and fellow NIST researcher Mark Ferris clamped to a cellphone a tiny well containing the solution to be tested and a strip of hydrogel — a porous material that swells when immersed in water. The researchers embedded tiny magnetic particles within the hydrogel, which they had engineered to react either to the presence of glucose or to pH levels (a measure of acidity) by expanding or contracting. Changing pH levels can be associated with a variety of biological disorders.</p> 
      <figure role="group"><p><img alt="magnetometer illustration" height="812" loading="lazy" src="https://www.nist.gov/sites/default/files/styles/960_x_960_limit/public/images/2024/02/28/cell_phone_magnetometer_v9_22.png?itok=ggIfqAY3" typeof="foaf:Image" width="960"></p>
<figcaption>Illustration shows how a smartphone magnetometer can measure a host of biomedical properties in liquid samples using a magnetized hydrogel.<p><span>Credit:</span>
          
  K. Dill/NIST

        </p></figcaption></figure><p>As the hydrogels enlarged or shrunk, they moved the magnetic particles closer to or farther from the cellphone’s magnetometer, which detected the corresponding changes in the strength of the magnetic field. Employing this strategy, the researchers measured glucose concentrations as small as a few millionths of a mole (the scientific unit for a certain number of atoms or molecules in a substance). Although such high sensitivity is not required for at-home monitoring of glucose levels using a drop of blood, it might in the future enable routine testing for glucose in saliva, which contains a much smaller concentration of the sugar.</p><p>The researchers reported their findings in the March 30, 2024 edition of <a href="https://www.nature.com/articles/s41467-024-47073-2"><em>Nature Communications</em></a>.</p><p>Engineered, or “smart,” hydrogels like the ones the NIST team employed are inexpensive and relatively easy to fabricate, Ferris said, and can be tailored to react to a host of different compounds that medical researchers may want to measure. In their experiments, he and Zabow stacked single layers of two different hydrogels, each of which contracted and expanded at different rates in response to pH or glucose. These bilayers amplified the motion of the hydrogels, making it easier for the magnetometer to track changes in magnetic field strength.</p><p>Because the technique does not require any electronics or power source beyond that of the cellphone nor call for any special processing of the sample, it offers an inexpensive way to conduct testing — even in locations with relatively few resources.</p><p>Future efforts to improve the accuracy of such measurements using cellphone magnetometers might allow detection of DNA strands, specific proteins and histamines — compounds involved in the body’s immune response — at concentrations as low as a few tens of nanomoles (billionths of a mole).</p><p>That improvement could have substantial benefit. For instance, measuring histamines, which are typically detected in urine at concentrations ranging from about 45 to 190 nanomoles, would ordinarily require a 24-hour urine collection and a sophisticated laboratory analysis.&nbsp;</p><p>“An at-home test using a cellphone magnetometer sensitive to nanomolar concentrations would allow measurements to be done with much less hassle,” said Ferris. More generally, enhanced sensitivity would be essential when only a small amount of a substance is available for testing in extremely dilute quantities, Zabow added.</p><p>Similarly, the team’s study suggests that a cellphone magnetometer can measure pH levels with the same sensitivity as a thousand-dollar benchtop meter but at a fraction of the cost. A home-brewer or a baker could use the magnetometer to quickly test the pH of various liquids to perfect their craft, and an environmental scientist could measure the pH of ground water samples on-site with higher accuracy than a litmus test strip could provide.</p><p>In order to make the cellphone measurements a commercial success, engineers will need to develop a method to mass produce the hydrogel test strips and ensure that they have a long shelf life, Zabow said. Ideally, he added, the hydrogel strips should be designed to react more quickly to environmental cues in order to speed up measurements.&nbsp;</p><hr><p>Paper: Mark Ferris and Gary Zabow. Quantitative, High Sensitivity Measurement of Liquid Analytes using a Smartphone Compass. <em>Nature Communications</em>. Published online March 30, 2024.&nbsp;DOI: <a href="https://doi.org/10.1038/s41467-024-47073-2">https://doi.org/10.1038/s41467-024-47073-2</a></p>
  
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why CockroachDB doesn't use EvalPlanQual (104 pts)]]></title>
            <link>https://www.cockroachlabs.com/blog/why-cockroachdb-doesnt-use-evalplanqual/</link>
            <guid>39948686</guid>
            <pubDate>Fri, 05 Apr 2024 23:55:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cockroachlabs.com/blog/why-cockroachdb-doesnt-use-evalplanqual/">https://www.cockroachlabs.com/blog/why-cockroachdb-doesnt-use-evalplanqual/</a>, See on <a href="https://news.ycombinator.com/item?id=39948686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <div>

            
            <p><img src="https://www.cockroachlabs.com/img/Ben-r23.png"></p><h3>
              Tech Talk
            </h3>
            
            <p>Co-Founder Ben Darnell on Van Halen, brown M&amp;Ms, and isolation.</p>
  
             
              <p><a href="https://www.cockroachlabs.com/roachfest/2023/no-brown-mms-on-foreign-keys-isolation-levels-and-van-halen/">Watch now</a> 
            
          </p></div>
            

        <p>Here’s a surprising behavior of PostgreSQL you might not know about: under READ
COMMITTED isolation, <strong>PostgreSQL can sometimes miss rows</strong> when performing
UPDATE, DELETE, SELECT FOR UPDATE, or SELECT FOR SHARE statements. This is due
to the <em>EvalPlanQual</em> recheck PostgreSQL adds to these statements to prevent
lost-update anomalies.</p>
<p>For CockroachDB’s new implementation of READ COMMITTED isolation, we considered
building our own version of <em>EvalPlanQual</em>, but decided to use a different
technique instead which doesn’t miss rows. By not missing rows, CockroachDB
alleviates the need for application-level retries.</p>




<h2 id="what-is-evalplanqual">What is EvalPlanQual?</h2>
<p>Under READ COMMITTED isolation, PostgreSQL adds a special recheck step to
UPDATE, DELETE, SELECT FOR UPDATE, and SELECT FOR SHARE statements. This
recheck is known as <a href="https://github.com/postgres/postgres/blob/ada87a4d95fc39dfb1214edf6653390314b6f0df/src/backend/executor/README#L350" target="_blank" rel="noopener"><em>EvalPlanQual</em></a>, and consists of a re-evaluation
of part of the statement after all qualifying rows are locked. PostgreSQL uses
<em>EvalPlanQual</em> to prevent lost updates under READ COMMITTED isolation.</p>
<p>To make this concrete, let’s walk through an example.</p>
<h2 id="an-example-of-conflicting-updates-in-postgresql-and-cockroachdb">An example of conflicting updates in PostgreSQL and CockroachDB</h2>
<p>Suppose we’re using SQL to organize a 3-on-3 basketball league. One of the
tables in our schema holds information about every player in the league,
including (a) their ID, (b) their name, (c) their skill level, and (d) their
team assignment if they have one.</p>
<div><pre tabindex="0"><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> player (
    id    integer <span>NOT</span> <span>NULL</span>,
    name  text <span>NOT</span> <span>NULL</span>,
    <span>level</span> text <span>NOT</span> <span>NULL</span>,
    team  text,
    <span>PRIMARY</span> <span>KEY</span> (id)
);

<span>INSERT</span> <span>INTO</span> player <span>VALUES</span>
    (<span>1</span>, <span>'Gray'</span>,         <span>'A'</span>,  <span>'Dolphins'</span>),
    (<span>2</span>, <span>'Mohan'</span>,        <span>'A'</span>,  <span>'Dolphins'</span>),
    (<span>3</span>, <span>'Stonebreaker'</span>, <span>'A'</span>,  <span>'Dolphins'</span>),
    (<span>4</span>, <span>'Lamport'</span>,      <span>'A'</span>,  <span>'Gophers'</span>),
    (<span>5</span>, <span>'Ullman'</span>,       <span>'A'</span>,  <span>'Gophers'</span>),
    (<span>6</span>, <span>'Lynch'</span>,        <span>'A'</span>,  <span>'Gophers'</span>),
    (<span>7</span>, <span>'Bernstein'</span>,    <span>'AA'</span>, <span>'Elephants'</span>),
    (<span>8</span>, <span>'Liskov'</span>,       <span>'AA'</span>, <span>'Elephants'</span>),
    (<span>9</span>, <span>'Codd'</span>,         <span>'AA'</span>, <span>'Elephants'</span>);
</code></pre></div><p>When modifying this table, we’ll try to maintain two invariants:</p>
<ol>
<li>teams must have exactly 3 players</li>
<li>all players on a team must have the same skill level</li>
</ol>
<p>After a wildly successful season, we decide to move all of the Gophers up to
skill level AA. We use a single UPDATE statement in an implicit transaction,
which should atomically modify all three players and thus maintain our
invariants.</p>
<div><pre tabindex="0"><code data-lang="sql"><span>UPDATE</span> player <span>SET</span> <span>level</span> <span>=</span> <span>'AA'</span> <span>WHERE</span> team <span>=</span> <span>'Gophers'</span>;
</code></pre></div><p>(But to more easily demonstrate, let’s sleep for 5 seconds at the beginning of
the update.)</p>
<div><pre tabindex="0"><code data-lang="sql"><span>WITH</span> sleep <span>AS</span> (<span>SELECT</span> pg_sleep(<span>5</span>))
    <span>UPDATE</span> player <span>SET</span> <span>level</span> <span>=</span> <span>'AA'</span> <span>FROM</span> sleep <span>WHERE</span> team <span>=</span> <span>'Gophers'</span>;
</code></pre></div><p>While that update is running, the Gophers and Dolphins decide to exchange
players 3 and 4 in a trade. Again, we use a single UPDATE statement in an
implicit transaction. This should atomically swap the skill level and team of
the two players and thus, again, maintain our invariants.</p>
<div><pre tabindex="0"><code data-lang="sql"><span>UPDATE</span> player p1 <span>SET</span> (<span>level</span>, team) <span>=</span> 
        (<span>SELECT</span> <span>level</span>, team <span>FROM</span> player p2
         <span>WHERE</span> p2.id <span>IN</span> (<span>3</span>, <span>4</span>) <span>AND</span> p2.id <span>!=</span> p1.id)
    <span>WHERE</span> p1.id <span>IN</span> (<span>3</span>, <span>4</span>);
</code></pre></div><p>The second update succeeds immediately. What happens to the first update? Let’s
consider the result by both database and isolation level.</p>
<h3 id="postgresqls-result">PostgreSQL’s result</h3>
<p>In PostgreSQL, under SERIALIZABLE isolation, the first update fails with a
“could not serialize” error. During execution, PostgreSQL detects that another
transaction has concurrently modified players 3 and 4, and aborts.</p>
<p>Under READ COMMITTED isolation, the first update succeeds, but the result is
anomalous: player 3 is now on the Gophers but is <em>still at level A</em>. Only two
of the Gophers were moved up to level AA.</p>
<pre tabindex="0"><code>michae2=# SELECT * FROM player ORDER BY id;
 id |     name     | level |   team
----+--------------+-------+-----------
  1 | Gray         | A     | Dolphins
  2 | Mohan        | A     | Dolphins
  3 | Stonebreaker | A     | Gophers     &lt;- still at level A
  4 | Lamport      | A     | Dolphins
  5 | Ullman       | AA    | Gophers
  6 | Lynch        | AA    | Gophers
  7 | Bernstein    | AA    | Elephants
  8 | Liskov       | AA    | Elephants
  9 | Codd         | AA    | Elephants
(9 rows)
</code></pre><p>This violates invariant 2, which is surprising, because when considered
individually both updates should have maintained our invariants.</p>
<h3 id="cockroachdbs-result">CockroachDB’s result</h3>
<p>In CockroachDB, under both SERIALIZABLE and READ COMMITTED isolation the result
is the same: the first update succeeds, player 3 is now on the Gophers, and is
correctly at level AA.</p>
<pre tabindex="0"><code>demo@127.0.0.1:26257/demoapp/defaultdb&gt; SELECT * FROM player ORDER BY id;
  id |     name     | level |   team
-----+--------------+-------+------------
   1 | Gray         | A     | Dolphins
   2 | Mohan        | A     | Dolphins
   3 | Stonebreaker | AA    | Gophers     &lt;- correctly at level AA
   4 | Lamport      | A     | Dolphins
   5 | Ullman       | AA    | Gophers
   6 | Lynch        | AA    | Gophers
   7 | Bernstein    | AA    | Elephants
   8 | Liskov       | AA    | Elephants
   9 | Codd         | AA    | Elephants
(9 rows)
</code></pre><p>How does PostgreSQL arrive at its anomalous result, and how does CockroachDB
avoid the anomaly? To answer these questions let’s examine execution of the
first UPDATE statement under READ COMMITTED isolation in both databases.</p>
<h2 id="a-closer-look-at-postgresqls-update-under-read-committed-isolation">A closer look at PostgreSQL’s UPDATE under READ COMMITTED isolation</h2>
<p>We’ll start by looking at EXPLAIN in PostgreSQL. The plan for the first UPDATE
statement seems quite simple: a scan of the table followed by a filter.</p>
<pre tabindex="0"><code>michae2=# EXPLAIN UPDATE player SET level = 'AA' WHERE team = 'Gophers';
                         QUERY PLAN
-------------------------------------------------------------
 Update on player  (cost=0.00..1.11 rows=0 width=0)
   -&gt;  Seq Scan on player  (cost=0.00..1.11 rows=3 width=38)
         Filter: (team = 'Gophers'::text)
</code></pre><p>But that <code>Update on player</code> operation hides a lot of complexity. We can think
of PostgreSQL executing the update under READ COMMITTED isolation in roughly
seven steps.</p>
<ol>
<li>Establish a read snapshot for the statement. This snapshot is before the
second update begins.</li>
<li>Scan <code>player</code> at the read snapshot. (<code>Seq Scan</code> in the plan.)</li>
<li>Filter on <code>team = 'Gophers'</code>. (<code>Filter</code> in the plan.) Players 4, 5, and 6
qualify since the read snapshot is before the second update.</li>
<li>Lock all qualifying rows. (Part of <code>Update</code> in the plan.)</li>
<li><strong>Re-read the latest committed version of all locked rows. (<code>Seq Scan</code> in
the plan again.)</strong> This picks up the change to player 4.</li>
<li><strong>Re-run the filter on the latest committed versions. (<code>Filter</code> in the plan
again.)</strong> Now only players 5 and 6 qualify.</li>
<li>Write a new version of each qualifying row. (<code>Update</code> in the plan.)</li>
</ol>
<p>Steps 5 and 6 are <strong>PostgreSQL’s strategy for preventing lost updates</strong> under
READ COMMITTED isolation. After locking all qualifying rows, PostgreSQL
<em>re-evaluates the query steps a second time</em> on the latest version of each
locked row. This is the <em>EvalPlanQual</em> recheck. By doing this recheck,
PostgreSQL picks up any changes made to those rows between reading them and
locking them, which in this case correctly prevents modifying the skill level
of player 4. Unfortunately, because PostgreSQL only rechecks the locked rows,
it <strong>misses the change to player 3</strong>. We were expecting at least one of those
two players to move up to skill level AA, but neither did.</p>
<p>This behavior is <a href="https://www.postgresql.org/docs/16/transaction-iso.html#XACT-READ-COMMITTED" target="_blank" rel="noopener">documented</a>, but it can still <a href="https://www.cybertec-postgresql.com/en/transaction-anomalies-with-select-for-update/" target="_blank" rel="noopener">cause</a>
<a href="https://dev.to/aws-heroes/read-committed-anomalies-in-postgresql-1ieg" target="_blank" rel="noopener">surprises</a>.</p>
<h2 id="a-closer-look-at-cockroachdbs-update-under-read-committed-isolation">A closer look at CockroachDB’s UPDATE under READ COMMITTED isolation</h2>
<p>Next, let’s look at EXPLAIN in CockroachDB. Again, the plan for the UPDATE
statement seems quite simple: a scan of the table followed by a filter.</p>
<pre tabindex="0"><code>demo@127.0.0.1:26257/demoapp/defaultdb&gt; EXPLAIN UPDATE player SET level = 'AA' WHERE team = 'Gophers';
                                           info
-------------------------------------------------------------------------------------------
  distribution: local
  vectorized: true

  • update
  │ table: player
  │ set: level
  │ auto commit
  │
  └── • render
      │
      └── • filter
          │ estimated row count: 3
          │ filter: team = 'Gophers'
          │
          └── • scan
                estimated row count: 9 (100% of the table; stats collected 8 seconds ago)
                table: player@player_pkey
                spans: FULL SCAN
</code></pre><p>Again, that <code>update</code> operation hides a lot of complexity. We can think of
CockroachDB executing the update under READ COMMITTED isolation in roughly six
steps.</p>
<ol>
<li>Create a savepoint in the current transaction.</li>
<li>Establish a read snapshot for the statement. This snapshot is before the
second update begins.</li>
<li>Scan <code>player</code> at the read snapshot. (<code>scan</code> in the plan.)</li>
<li>Filter on <code>team = 'Gophers'</code>. (<code>filter</code> in the plan.) Players 4, 5, and 6
qualify since the read snapshot is before the second update.</li>
<li>Write an intent for each qualifying row.<sup><a href="#footnote1" target="_blank" rel="noopener">1</a></sup> (<code>update</code> in the plan.)</li>
<li>While writing intents, if the latest committed version of a row is newer
than our read snapshot, <strong>rollback to the savepoint and go back to step
1.</strong> This gives us a new read snapshot which is <em>after</em> the second update.</li>
</ol>
<p>Step 6 is <strong>CockroachDB’s strategy for preventing lost updates</strong> under READ
COMMITTED isolation. CockroachDB <a href="https://github.com/cockroachdb/cockroach/blob/04e1faeb6b674729e989e001d37429786b584c6e/pkg/sql/conn_executor_exec.go#L1696" target="_blank" rel="noopener"><em>retries the entire UPDATE
statement</em></a> with a new read snapshot if it encounters a newer
version of a row while writing an intent.<sup><a href="#footnote2" target="_blank" rel="noopener">2</a></sup> By executing the entire statement
again with a new read snapshot, CockroachDB can pick up <em>any</em> changes made
after the previous read snapshot, which in this case includes changes to <em>both</em>
players 3 and 4. Crucially, CockroachDB holds onto locks and intents across
retries, which helps it make progress even during periods of heavy contention.</p>
<h2 id="how-to-prevent-lost-updates-a-tradeoff">How to prevent lost updates: a tradeoff?</h2>
<p>At first glance these two different techniques for preventing lost updates seem
to embody a tradeoff: on the one hand, PostgreSQL avoids retries, on the other
hand, CockroachDB avoids anomalies. But if we consider things from the
application’s point of view, this is less of a tradeoff than it first appears.
What would an application need to do to be sure that all Gophers were updated
to skill level AA?</p>
<p>When using PostgreSQL, the application itself would need to retry the first
UPDATE statement until all Gophers were at level AA. <strong>Application-level
retries are more expensive than database-level retries</strong>, due to network
latency, so this is strictly worse than retrying within the database. There’s
no way to avoid retrying the first UPDATE statement in this scenario, but
CockroachDB is able to hide the retry from the application.</p>
<h2 id="conclusion">Conclusion</h2>
<p>PostgreSQL adds <em>EvalPlanQual</em> rechecks to UPDATE, DELETE, SELECT FOR UPDATE,
and SELECT FOR SHARE statements under READ COMMITTED isolation to prevent
lost-update anomalies. EvalPlanQual lets PostgreSQL avoid internal retries of
these statements, but it can cause PostgreSQL to <strong>miss rows</strong> which will in
turn require application-level retries to handle.<sup><a href="#footnote3" target="_blank" rel="noopener">3</a></sup></p>
<p>In CockroachDB’s new implementation of READ COMMITTED isolation, instead of
EvalPlanQual we built an internal statement-retry mechanism that <strong>does not
miss rows and thus alleviates the need for application-level retries</strong>.</p>
<p><a name="footnote1">1</a> <a href="https://www.cockroachlabs.com/docs/v23.2/architecture/transaction-layer#write-intents"><em>Intents</em></a> are new versions of a row that also act as exclusive
locks.</p>
<p><a name="footnote2">2</a> Under SERIALIZABLE isolation, CockroachDB retries <em>entire transactions</em>
when a serializable history cannot be achieved through other means. In this
case we’re using a single-statement implicit transaction, so CockroachDB
appears to have the same behavior under both SERIALIZABLE and READ COMMITTED
isolation. But note that if the scenario were slightly different, say a <em>move</em>
of a player to the Gophers rather than a swap, the two isolation levels would
show different behavior.</p>
<p><a name="footnote3">3</a> This article focuses on PostgreSQL, but it’s worth noting that MySQL
(using InnoDB) can also miss rows with some tweaks to the scenario. Of the
databases we’ve studied, so far Oracle has the most similar UPDATE behavior
under READ COMMITTED isolation to CockroachDB.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C++ Insights – See your source code with the eyes of a compiler (152 pts)]]></title>
            <link>https://github.com/andreasfertig/cppinsights</link>
            <guid>39948404</guid>
            <pubDate>Fri, 05 Apr 2024 23:15:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/andreasfertig/cppinsights">https://github.com/andreasfertig/cppinsights</a>, See on <a href="https://news.ycombinator.com/item?id=39948404">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/andreasfertig/cppinsights/blob/main/artwork/logo_cppinsights.png"><img src="https://github.com/andreasfertig/cppinsights/raw/main/artwork/logo_cppinsights.png" alt="cpp insights logo"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">C++ Insights - See your source code with the eyes of a compiler.</h2><a id="user-content-c-insights---see-your-source-code-with-the-eyes-of-a-compiler" aria-label="Permalink: C++ Insights - See your source code with the eyes of a compiler." href="#c-insights---see-your-source-code-with-the-eyes-of-a-compiler"></a></p>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/2bb6ac78e5a9f4f688a6a066cc71b62012101802fcdb478e6e4c6b6ec75dc694/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"></a> <a href="https://github.com/andreasfertig/cppinsights/releases"><img src="https://camo.githubusercontent.com/145be9dc45d710d7cea80bbf55239f700a94082c09e88121e952f902b09ba682/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c61746573742d646f776e6c6f61642d626c75652e737667" alt="download" data-canonical-src="https://img.shields.io/badge/latest-download-blue.svg"></a> <a href="https://github.com/andreasfertig/cppinsights/actions/"><img src="https://github.com/andreasfertig/cppinsights/workflows/ci/badge.svg" alt="Build Status"></a>
<a href="https://codecov.io/gh/andreasfertig/cppinsights" rel="nofollow"><img src="https://camo.githubusercontent.com/1b48b9ce581bdb2b0f05bbb5dd24d271e6652acc4f45f4dad073754114d11ad8/68747470733a2f2f636f6465636f762e696f2f67682f616e64726561736665727469672f637070696e7369676874732f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov" data-canonical-src="https://codecov.io/gh/andreasfertig/cppinsights/branch/master/graph/badge.svg"></a>
<a href="https://cppinsights.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/bce7782b04362d4618e15af221ed0289a1fe26efa0fa75e892f76c3810acd022/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7472792d6f6e6c696e652d626c75652e737667" alt="Try online" data-canonical-src="https://img.shields.io/badge/try-online-blue.svg"></a>
<a href="https://docs.cppinsights.io/" rel="nofollow"><img src="https://camo.githubusercontent.com/c31536ed1f50bf5b0fd3985352a7de9f90e02e0d17dae8cd21bb7affc2ef97c0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f766965772d646f63756d656e746174696f6e2d626c7565" alt="Documentation" data-canonical-src="https://img.shields.io/badge/view-documentation-blue"></a>
<a href="https://www.patreon.com/cppinsights" rel="nofollow"><img src="https://camo.githubusercontent.com/84ca69083b8e354ea225a8d54bd9da803c105ab76c3d09875d16d78aebed6547/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70617472656f6e2d737570706f72742d6f72616e67652e737667" alt="patreon" data-canonical-src="https://img.shields.io/badge/patreon-support-orange.svg"></a></p>
<p dir="auto"><a href="https://gitpod.io/#https://github.com/andreasfertig/cppinsights" rel="nofollow"><img src="https://camo.githubusercontent.com/95fbab4ac41e62a9f66e6d1d78f8249c418b33f8c7739c4f9c593f953f5362de/68747470733a2f2f676974706f642e696f2f627574746f6e2f6f70656e2d696e2d676974706f642e737667" alt="Open in Gitpod" data-canonical-src="https://gitpod.io/button/open-in-gitpod.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ul dir="auto">
<li><a href="#what">What</a></li>
<li><a href="#why">Why</a></li>
<li><a href="#building">Building</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#get-involved">Get Involved</a></li>
<li><a href="#support">Support</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What</h2><a id="user-content-what" aria-label="Permalink: What" href="#what"></a></p>
<p dir="auto"><a href="https://cppinsights.io/" rel="nofollow">C++ Insights</a> is a <a href="https://clang.llvm.org/" rel="nofollow">Clang</a>-based tool that does a source-to-source
transformation. The goal of C++ Insights is to make things visible that normally and intentionally happen behind the scenes.
It's about the magic the compiler does for us to make things work.</p>
<p dir="auto">Take this piece of code for example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class Base {
};

class Derived : public Base {
};

int main() {
  Derived d;

  Derived d2 = d;

  d2 = d;

  Base&amp; b = d;
}"><pre><span>class</span> <span>Base</span> {
};

<span>class</span> <span>Derived</span> : <span>public</span> <span>Base</span> {
};

<span>int</span> <span>main</span>() {
  Derived d;

  Derived d2 = d;

  d2 = d;

  Base&amp; b = d;
}</pre></div>
<p dir="auto">Nothing special, and of course, it compiles. This is the compiler's view on it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class Base
{
  public:
  // inline constexpr Base() noexcept = default;
  // inline constexpr Base(const Base &amp;) noexcept = default;
  // inline constexpr Base &amp; operator=(const Base &amp;) noexcept = default;
};



class Derived : public Base
{
  public:
  // inline constexpr Derived() noexcept = default;
  // inline constexpr Derived(const Derived &amp;) noexcept = default;
  // inline constexpr Derived &amp; operator=(const Derived &amp;) noexcept = default;
};



int main()
{
  Derived d;
  Derived d2 = Derived(d);
  d2.operator=(d);
  Base &amp; b = static_cast<Base&amp;>(d);
  return 0;
}"><pre><span>class</span> <span>Base</span>
{
  <span>public:</span>
  <span><span>//</span> inline constexpr Base() noexcept = default;</span>
  <span><span>//</span> inline constexpr Base(const Base &amp;) noexcept = default;</span>
  <span><span>//</span> inline constexpr Base &amp; operator=(const Base &amp;) noexcept = default;</span>
};



<span>class</span> <span>Derived</span> : <span>public</span> <span>Base</span>
{
  <span>public:</span>
  <span><span>//</span> inline constexpr Derived() noexcept = default;</span>
  <span><span>//</span> inline constexpr Derived(const Derived &amp;) noexcept = default;</span>
  <span><span>//</span> inline constexpr Derived &amp; operator=(const Derived &amp;) noexcept = default;</span>
};



<span>int</span> <span>main</span>()
{
  Derived d;
  Derived d2 = <span>Derived</span>(d);
  d2.<span>operator</span>=(d);
  Base &amp; b = <span>static_cast</span>&lt;Base&amp;&gt;(d);
  <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto">You can see all the compiler-provided special member functions and the downcast from <code>Derived</code> to <code>Base</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why</h2><a id="user-content-why" aria-label="Permalink: Why" href="#why"></a></p>
<p dir="auto"><a href="https://cppinsights.io/" rel="nofollow">C++ Insights</a> is a <a href="https://clang.llvm.org/" rel="nofollow">Clang</a>-based tool that does a source-to-source transformation.
The goal of C++ Insights is to make things visible that normally and intentionally happen behind the scenes. It's about the magic the compiler does
for us to make things work. Or looking through the classes of a compiler.</p>
<p dir="auto">In 2017, I started looking into some new things we got with C++11, C++14, and C++17. Amazing things like lambdas, range-based for-loops,
and structured bindings. I put it together in a talk. You can find the <a href="https://andreasfertig.com/talks/dl/afertig-ndcolo-2017-fast-and-small.pdf" rel="nofollow">slides</a>
and a <a href="https://youtu.be/Bt7KzFxcbgc" rel="nofollow">video</a> online.</p>
<p dir="auto">However, all that research and some of my training and teaching got me to start thinking about how it would be if we could see with the eyes
of the compiler. Sure, there is an AST dump, at least for Clang. We can see what code the compiler generates from a C++ source snippet with
tools like Compiler Explorer. However, what we see is assembler. Neither the AST nor the Compiler Explorer output is in the language I write
code. Hence, I'm not very familiar with this output. Plus, when teaching students C++, showing an AST and explaining that it is all there was
not quite satisfying for me.</p>
<p dir="auto">I started to write a Clang-based tool that can transform a range-based for-loop into the compiler-internal version. Then, I did the same
for structured bindings and lambdas. In the end, I did much more than initially planned. It shows where operators are
invoked and places in which the compiler does some casting. C++ Insights can deduce the type behind <code>auto</code> or <code>decltype</code>. The goal
is to produce compilable code. However, this is not possible in all places.</p>
<p dir="auto">You can see, for example, the transformation of a <a href="https://cppinsights.io/s/e4e19791" rel="nofollow">lambda</a>, <a href="https://cppinsights.io/s/0cddd172" rel="nofollow">range-based for-loop</a>, or <a href="https://cppinsights.io/s/6c61d601" rel="nofollow">auto</a>. Of course, you can transform any other C++ snippet.</p>
<p dir="auto">See yourself. C++ Insights is available online: <a href="https://cppinsights.io/" rel="nofollow">cppinsights.io</a>.</p>
<p dir="auto">Still, there is work to do.</p>
<p dir="auto">I do not claim to get all the things right. I'm also working on supporting features from new standards, like C++20, at the moment.
Please remember that C++ Insights is based on Clang and its understanding of the AST.</p>
<p dir="auto">I did a couple of talks about C++ Insights since I released C++ Insights. For example, at C++ now. Here are the <a href="https://andreasfertig.com/talks/dl/afertig-2021-cppnow-cpp-insights.pdf" rel="nofollow">slides</a> and the <a href="https://youtu.be/p-8wndrTaTs" rel="nofollow">video</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">C++ Insights can be built inside the Clang source tree or outside.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on Windows</h3><a id="user-content-building-on-windows" aria-label="Permalink: Building on Windows" href="#building-on-windows"></a></p>
<p dir="auto">See <a href="https://github.com/andreasfertig/cppinsights/blob/main/Readme_Windows.md">Readme_Windows.md</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on Arch Linux</h3><a id="user-content-building-on-arch-linux" aria-label="Permalink: Building on Arch Linux" href="#building-on-arch-linux"></a></p>
<p dir="auto">To build with <code>extra/clang</code> use the following extra flags: <code>-DINSIGHTS_USE_SYSTEM_INCLUDES=off -DCLANG_LINK_CLANG_DYLIB=on -DLLVM_LINK_LLVM_DYLIB=on</code></p>
<p dir="auto">See <a data-error-text="Failed to load title" data-id="443768833" data-permission-text="Title is private" data-url="https://github.com/andreasfertig/cppinsights/issues/186" data-hovercard-type="issue" data-hovercard-url="/andreasfertig/cppinsights/issues/186/hovercard" href="https://github.com/andreasfertig/cppinsights/issues/186">#186</a> for an explanation of why <code>INSIGHTS_USE_SYSTEM_INCLUDES</code> needs to be turned off.</p>
<p dir="auto"><code>extra/clang</code> and <code>extra/llvm</code> provide <code>/usr/lib/{libclangAST.so,libLLVM*.a,libLLVM.so}</code>. <code>libclangAST.so</code> needs <code>libLLVM.so</code> and there would be a conflict if <code>libLLVM*.a</code> (instead of <code>libLLVM.so</code>) are linked. See <a href="https://bugs.archlinux.org/task/60512" rel="nofollow">https://bugs.archlinux.org/task/60512</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building outside Clang</h3><a id="user-content-building-outside-clang" aria-label="Permalink: Building outside Clang" href="#building-outside-clang"></a></p>
<p dir="auto">You need to have a Clang installation in the search path.</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/andreasfertig/cppinsights.git
mkdir build &amp;&amp; cd build
cmake -G&quot;Ninja&quot; ../cppinsights
ninja"><pre><code>git clone https://github.com/andreasfertig/cppinsights.git
mkdir build &amp;&amp; cd build
cmake -G"Ninja" ../cppinsights
ninja
</code></pre></div>
<p dir="auto">The resulting binary (insights) can be found in the <code>build</code> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building inside Clang</h3><a id="user-content-building-inside-clang" aria-label="Permalink: Building inside Clang" href="#building-inside-clang"></a></p>
<p dir="auto">For building it inside the Clang source tree, assuming you have your source tree already prepared under <code>llvm-project</code>:</p>
<div data-snippet-clipboard-copy-content="cd llvm-project/clang-tools-extra/
git clone https://github.com/andreasfertig/cppinsights.git

echo &quot;add_subdirectory(cppinsights)&quot; >> CMakeLists.txt"><pre><code>cd llvm-project/clang-tools-extra/
git clone https://github.com/andreasfertig/cppinsights.git

echo "add_subdirectory(cppinsights)" &gt;&gt; CMakeLists.txt
</code></pre></div>
<p dir="auto">To activate the C++ Insights build you have to set <code>-DLLVM_ENABLE_PROJECTS="clang;clang-tools-extra"</code> for <code>cmake</code>:</p>
<div data-snippet-clipboard-copy-content="cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS=&quot;clang;clang-tools-extra&quot; -G &quot;Unix Makefiles&quot; ../llvm-project"><pre><code>cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_PROJECTS="clang;clang-tools-extra" -G "Unix Makefiles" ../llvm-project
</code></pre></div>
<p dir="auto">Then, build Clang as you normally do.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">cmake options</h3><a id="user-content-cmake-options" aria-label="Permalink: cmake options" href="#cmake-options"></a></p>
<p dir="auto">There are a couple of options that can be enabled with <a href="https://cmake.org/" rel="nofollow">cmake</a>:</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>INSIGHTS_STRIP</td>
<td>Strip insight after build</td>
<td>ON</td>
</tr>
<tr>
<td>INSIGHTS_STATIC</td>
<td>Use static linking</td>
<td>OFF</td>
</tr>
<tr>
<td>INSIGHTS_COVERAGE</td>
<td>Enable code coverage</td>
<td>OFF</td>
</tr>
<tr>
<td>INSIGHTS_USE_LIBCPP</td>
<td>Use libc++ for tests</td>
<td>OFF</td>
</tr>
<tr>
<td>DEBUG</td>
<td>Enable debug</td>
<td>OFF</td>
</tr>
</tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building for ARM on macOS</h3><a id="user-content-building-for-arm-on-macos" aria-label="Permalink: Building for ARM on macOS" href="#building-for-arm-on-macos"></a></p>
<p dir="auto">It seems best to supply the architecture during configuration:</p>
<div data-snippet-clipboard-copy-content="cmake -DCMAKE_OSX_ARCHITECTURES=arm64 ../cppinsights"><pre><code>cmake -DCMAKE_OSX_ARCHITECTURES=arm64 ../cppinsights
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use it with <a href="https://www.cevelop.com/" rel="nofollow">Cevelop</a></h3><a id="user-content-use-it-with-cevelop" aria-label="Permalink: Use it with Cevelop" href="#use-it-with-cevelop"></a></p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/andreasfertig/cppinsights.git
mkdir build_eclipse
cd build_eclipse
cmake -G&quot;Eclipse CDT4 - Unix Makefiles&quot; ../cppinsights/"><pre><code>git clone https://github.com/andreasfertig/cppinsights.git
mkdir build_eclipse
cd build_eclipse
cmake -G"Eclipse CDT4 - Unix Makefiles" ../cppinsights/
</code></pre></div>
<p dir="auto">Then, in <a href="https://www.cevelop.com/" rel="nofollow">Cevelop</a> Import -&gt; General -&gt; Existing Project into Workspace. Select <code>build_eclipse</code>. Enjoy editing with
<a href="https://www.cevelop.com/" rel="nofollow">Cevelop</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Using C++ Insights is fairly simple:</p>
<div data-snippet-clipboard-copy-content="insights <YOUR_CPP_FILE> -- -std=c++17"><pre><code>insights &lt;YOUR_CPP_FILE&gt; -- -std=c++17
</code></pre></div>
<p dir="auto">Things get complicated when it comes to the system-include paths. These paths are hard-coded in the binary, which seems
to come from the compiler C++ Insights was built with. To help with that, check out <a href="https://github.com/andreasfertig/cppinsights/blob/main/scripts/getinclude.py">scripts/getinclude.py</a>. The script tries to
collect the system-include paths from the compiler. Without an option, <code>getinclude.py</code> uses <code>g++</code>. You can also pass another compiler
as a first argument.</p>
<p dir="auto">Here is an example:</p>
<div data-snippet-clipboard-copy-content="./scripts/getinclude.py
-isystem/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1 -isystem/usr/local/include -isystem/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.3.0/include -isystem/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include -isystem/usr/include"><pre><code>./scripts/getinclude.py
-isystem/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1 -isystem/usr/local/include -isystem/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.3.0/include -isystem/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include -isystem/usr/include
</code></pre></div>
<p dir="auto">The script can be used together with C++ Insights:</p>
<div data-snippet-clipboard-copy-content="insights <YOUR_CPP_FILE> -- -std=c++17 `./scripts/getinclude.py`"><pre><code>insights &lt;YOUR_CPP_FILE&gt; -- -std=c++17 `./scripts/getinclude.py`
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom GCC installation</h3><a id="user-content-custom-gcc-installation" aria-label="Permalink: Custom GCC installation" href="#custom-gcc-installation"></a></p>
<p dir="auto">In case you have a custom build of the GCC compiler, for example, gcc-11.2.0, and <em>NOT</em> installed in the compiler in the default system path, then after building, Clang fails to find the correct <code>libstdc++</code> path (GCC's STL). If you run into this situation, you can use "<code>--gcc-toolchain=/path/GCC-1x.x.x/installed/path</code>" to tell Clang/C++ Insights the location of the STL:</p>
<div data-snippet-clipboard-copy-content="./cppinsights Insights.cpp -- --gcc-toolchain=${GCC_11_2_0_INSTALL_PATH} -std=c++20"><pre><code>./cppinsights Insights.cpp -- --gcc-toolchain=${GCC_11_2_0_INSTALL_PATH} -std=c++20
</code></pre></div>
<p dir="auto">Here "<code>${GCC_11_2_0_INSTALL_PATH}</code>" is the installation directory of your customized-built GCC. The option for Clang is described <a href="https://clang.llvm.org/docs/ClangCommandLineReference.html#cmdoption-clang-gcc-toolchain" rel="nofollow">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ready to use Docker container</h3><a id="user-content-ready-to-use-docker-container" aria-label="Permalink: Ready to use Docker container" href="#ready-to-use-docker-container"></a></p>
<p dir="auto">There is also another GitHub project that sets up a docker container with the latest C++ Insights version in it: <a href="https://github.com/andreasfertig/cppinsights-docker">C++
Insights - Docker</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">C++ Insights @ Vim</h3><a id="user-content-c-insights--vim" aria-label="Permalink: C++ Insights @ Vim" href="#c-insights--vim"></a></p>
<p dir="auto">A plugin for Vim is available at
<a href="https://github.com/Freed-Wu/cppinsights.vim">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">C++ Insights @ VSCode</h3><a id="user-content-c-insights--vscode" aria-label="Permalink: C++ Insights @ VSCode" href="#c-insights--vscode"></a></p>
<p dir="auto">An extension for Visual Studio Code is available at the VS Code marketplace: <a href="https://marketplace.visualstudio.com/items?itemName=devtbi.vscode-cppinsights" rel="nofollow">C++
Insights - VSCode Extension</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">C++ Insights @ brew</h3><a id="user-content-c-insights--brew" aria-label="Permalink: C++ Insights @ brew" href="#c-insights--brew"></a></p>
<p dir="auto">At least for macOS, you can install C++ Insights via Homebrew thanks to <a href="https://formulae.brew.sh/formula/cppinsights" rel="nofollow">this formulae</a>:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Compatibility</h2><a id="user-content-compatibility" aria-label="Permalink: Compatibility" href="#compatibility"></a></p>
<p dir="auto">I aim for the repository to compile with the latest version of Clang and at least the one before. The website tries to
stay close to the latest release of Clang. However, due to certain issues (building Clang for Windows), the website's
version is often delayed by a few months.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">C++ Insights @ YouTube</h2><a id="user-content-c-insights--youtube" aria-label="Permalink: C++ Insights @ YouTube" href="#c-insights--youtube"></a></p>
<p dir="auto">I created a <a href="https://youtube.com/@andreas_fertig" rel="nofollow">YouTube</a> channel where I release a new video each month. In
these videos, I use C++ Insights to show and explain certain C++ constructs, and sometimes I explain C++ Insights as well.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">ToDo's</h2><a id="user-content-todos" aria-label="Permalink: ToDo's" href="#todos"></a></p>
<p dir="auto">See <a href="https://github.com/andreasfertig/cppinsights/blob/main/TODO.md">TODO</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Involved</h2><a id="user-content-get-involved" aria-label="Permalink: Get Involved" href="#get-involved"></a></p>
<ul dir="auto">
<li>Report bugs/issues by submitting a <a href="https://github.com/andreasfertig/cppinsights/issues">GitHub issue</a>.</li>
<li>Submit contributions using <a href="https://github.com/andreasfertig/cppinsights/pulls">pull requests</a>. See <a href="https://github.com/andreasfertig/cppinsights/blob/main/CONTRIBUTING.md">Contributing</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">If you like to support the project, consider <a href="https://github.com/andreasfertig/cppinsights/blob/main/CONTRIBUTING.md">submitting</a> a patch. Another alternative is to become a <a href="https://github.com/sponsors/andreasfertig">GitHub Sponsor</a> or a <a href="https://www.patreon.com/cppinsights" rel="nofollow">Patreon</a> supporter.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SearXNG is a free internet metasearch engine (239 pts)]]></title>
            <link>https://github.com/searxng/searxng</link>
            <guid>39948044</guid>
            <pubDate>Fri, 05 Apr 2024 22:27:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/searxng/searxng">https://github.com/searxng/searxng</a>, See on <a href="https://news.ycombinator.com/item?id=39948044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><hr>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng.svg"><img src="https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng.svg"></a></p>

<hr>
<p dir="auto">Privacy-respecting, hackable <a href="https://en.wikipedia.org/wiki/Metasearch_engine" rel="nofollow">metasearch engine</a></p>
<p dir="auto"><a href="https://searx.space/" rel="nofollow">Searx.space</a> lists ready-to-use running instances.</p>
<p dir="auto">A <a href="https://docs.searxng.org/user" rel="nofollow">user</a>, <a href="https://docs.searxng.org/admin" rel="nofollow">admin</a> and <a href="https://docs.searxng.org/dev" rel="nofollow">developer</a> handbook is available on the <a href="https://docs.searxng.org/" rel="nofollow">homepage</a>.</p>
<p dir="auto"><a href="https://docs.searxng.org/admin/installation.html" rel="nofollow"><img src="https://camo.githubusercontent.com/d03a3c450f1338b2d7b3b13e5df2eee07fc3ebdc2fa1f1fd0d7eaec049717b49/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d696e7374616c6c2d626c7565" alt="SearXNG install" data-canonical-src="https://img.shields.io/badge/-install-blue"></a> <a href="https://docs.searxng.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/98e0355a460efccf423cdd69ced6df4190be41fddd82872a0b83b3dfb102cdf3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d686f6d65706167652d626c7565" alt="SearXNG homepage" data-canonical-src="https://img.shields.io/badge/-homepage-blue"></a> <a href="https://github.com/searxng/searxng/wiki"><img src="https://camo.githubusercontent.com/9164d57b5c4f056547d2535e5cdca025c4cb18ff526539f182bc9dfd0cfc51ae/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d77696b692d626c7565" alt="SearXNG wiki" data-canonical-src="https://img.shields.io/badge/-wiki-blue"></a> <a href="https://github.com/searxng/searxng/blob/master/LICENSE"><img src="https://camo.githubusercontent.com/24368ce71a4355192bbd02cf2fe76a69ff7f18674abaf159b19af37713497c25/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4147504c2d626c75652e737667" alt="AGPL License" data-canonical-src="https://img.shields.io/badge/license-AGPL-blue.svg"></a> <a href="https://github.com/searxng/searxng/issues"><img src="https://camo.githubusercontent.com/504dd793406a904416d3b23fe1c332aa9469aa4beb5b802fdbec146911f6ec3c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f73656172786e672f73656172786e673f636f6c6f723d79656c6c6f77266c6162656c3d697373756573" alt="Issues" data-canonical-src="https://img.shields.io/github/issues/searxng/searxng?color=yellow&amp;label=issues"></a> <a href="https://github.com/searxng/searxng/commits/master"><img src="https://camo.githubusercontent.com/cded9140035497b74511dd5cb02a283fdc96a957affab3ffef893751372faf43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f792f73656172786e672f73656172786e673f636f6c6f723d79656c6c6f77266c6162656c3d636f6d6d697473" alt="commits" data-canonical-src="https://img.shields.io/github/commit-activity/y/searxng/searxng?color=yellow&amp;label=commits"></a> <a href="https://translate.codeberg.org/projects/searxng/" rel="nofollow"><img src="https://camo.githubusercontent.com/bfadff5bc5728aaf3b29a13cdba64f25c27b8e61a3d0a23698c834524dfa21c8/68747470733a2f2f7472616e736c6174652e636f6465626572672e6f72672f776964676574732f73656172786e672f2d2f73656172786e672f7376672d62616467652e737667" alt="weblate" data-canonical-src="https://translate.codeberg.org/widgets/searxng/-/searxng/svg-badge.svg"></a> <a href="https://docs.searxng.org/" rel="nofollow"><img src="https://raw.githubusercontent.com/searxng/searxng/master/src/brand/searxng-wordmark.svg" alt="SearXNG logo"></a></p>
<hr>
<p dir="auto"><h2 id="user-content-contact" tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">Ask questions or just chat about SearXNG on</p>
<dl>
<dt>IRC</dt>
<dd><p dir="auto"><a href="https://web.libera.chat/?channel=#searxng" rel="nofollow">#searxng on libera.chat</a> which is bridged to Matrix.</p>
</dd>
<dt>Matrix</dt>
<dd><p dir="auto"><a href="https://matrix.to/#/#searxng:matrix.org" rel="nofollow">#searxng:matrix.org</a></p>
</dd>
</dl>
<p dir="auto"><h2 id="user-content-setup" tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ul dir="auto">
<li>A well maintained <a href="https://github.com/searxng/searxng-docker">Docker image</a>, also built for ARM64 and ARM/v7 architectures.</li>
<li>Alternatively there are <em>up to date</em> <a href="https://docs.searxng.org/admin/installation-scripts.html" rel="nofollow">installation scripts</a>.</li>
<li>For individual setup consult our detailed <a href="https://docs.searxng.org/admin/installation-searxng.html" rel="nofollow">Step by step</a> instructions.</li>
<li>To fine-tune your instance, take a look at the <a href="https://docs.searxng.org/admin/index.html" rel="nofollow">Administrator documentation</a>.</li>
</ul>
<p dir="auto"><h2 id="user-content-translations" tabindex="-1" dir="auto">Translations</h2><a id="user-content-translations" aria-label="Permalink: Translations" href="#translations"></a></p>
<p dir="auto">Help translate SearXNG at <a href="https://translate.codeberg.org/projects/searxng/searxng/" rel="nofollow">Weblate</a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2d8a8ad3e638c68c348ec0fdf02e96183c769ec4c7aff2f7c613dcb71ea5bba4/68747470733a2f2f7472616e736c6174652e636f6465626572672e6f72672f776964676574732f73656172786e672f2d2f6d756c74692d6175746f2e737667"><img src="https://camo.githubusercontent.com/2d8a8ad3e638c68c348ec0fdf02e96183c769ec4c7aff2f7c613dcb71ea5bba4/68747470733a2f2f7472616e736c6174652e636f6465626572672e6f72672f776964676574732f73656172786e672f2d2f6d756c74692d6175746f2e737667" data-canonical-src="https://translate.codeberg.org/widgets/searxng/-/multi-auto.svg"></a></p>

<p dir="auto"><h2 id="user-content-contributing" tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Are you a developer? Have a look at our <a href="https://docs.searxng.org/dev/quickstart.html" rel="nofollow">development quickstart</a> guide, it's very easy to contribute. Additionally we have a <a href="https://docs.searxng.org/dev/index.html" rel="nofollow">developer documentation</a>.</p>
<p dir="auto"><h2 id="user-content-codespaces" tabindex="-1" dir="auto">Codespaces</h2><a id="user-content-codespaces" aria-label="Permalink: Codespaces" href="#codespaces"></a></p>
<p dir="auto">You can contribute from your browser using <a href="https://docs.github.com/en/codespaces/overview">GitHub Codespaces</a>:</p>
<ul dir="auto">
<li>Fork the repository</li>
<li>Click on the <code>&lt;&gt; Code</code> green button</li>
<li>Click on the <code>Codespaces</code> tab instead of <code>Local</code></li>
<li>Click on <code>Create codespace on master</code></li>
<li>VSCode is going to start in the browser</li>
<li>Wait for <code>git pull &amp;&amp; make install</code> to appear and then disappear</li>
<li>You have <a href="https://github.com/settings/billing">120 hours per month</a> (see also your <a href="https://github.com/codespaces">list of existing Codespaces</a>)</li>
<li>You can start SearXNG using <code>make run</code> in the terminal or by pressing <code>Ctrl+Shift+B</code></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[World_sim: LLM prompted to act as a sentient CLI universe simulator (231 pts)]]></title>
            <link>https://worldsim.nousresearch.com/</link>
            <guid>39947713</guid>
            <pubDate>Fri, 05 Apr 2024 21:55:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worldsim.nousresearch.com/">https://worldsim.nousresearch.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39947713">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Updates App Store Guidelines to Permit Game Emulators, EU Music App Links (198 pts)]]></title>
            <link>https://www.macrumors.com/2024/04/05/app-store-guidelines-emulators-music-app-links/</link>
            <guid>39946694</guid>
            <pubDate>Fri, 05 Apr 2024 20:16:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/04/05/app-store-guidelines-emulators-music-app-links/">https://www.macrumors.com/2024/04/05/app-store-guidelines-emulators-music-app-links/</a>, See on <a href="https://news.ycombinator.com/item?id=39946694">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/04/05/app-store-guidelines-emulators-music-app-links/"><p>Apple today updated its <a href="https://developer.apple.com/app-store/review/guidelines/">App Store guidelines</a> to comply with an <a href="https://www.macrumors.com/2024/03/04/eu-fines-apple-2-billion/">anti-steering mandate</a> levied by the European Commission. Music streaming apps like Spotify are now permitted to include a link or buy button that leads to a website with information about alternative music purchasing options, though this is only permitted <a href="https://developer.apple.com/support/music-streaming-services-entitlement-eea/">in the European Economic Area</a>.</p>
<p><img src="https://images.macrumors.com/t/MuYr2cyAEYdr0CMrrvuycUPK5vs=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy" srcset="https://images.macrumors.com/t/MuYr2cyAEYdr0CMrrvuycUPK5vs=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy 400w,https://images.macrumors.com/t/j81xjhvPhb1xAaD6jc-kW3SoaHc=/800x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy 800w,https://images.macrumors.com/t/6S1CCkPCfv7Bu5OKPv07871bKhY=/1600x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg 1600w,https://images.macrumors.com/t/2D83fOzXH1a-mo51oJaSX0SjmzQ=/2500x0/filters:no_upscale()/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS App Store General Feature JoeBlue" width="2250" height="1266"></p>
<blockquote><p>Music Streaming Services Entitlements: music streaming apps in specific regions can use Music Streaming Services Entitlements to include a link (which may take the form of a buy button) to the developer's website that informs users of other ways to purchase digital music content or services. These entitlements also permit music streaming app developers to invite users to provide their email address for the express purpose of sending them a link to the developer's website to purchase digital music content or services. Learn more about these entitlements.</p>
<p>In accordance with the entitlement agreements, the link may inform users about where and how to purchase those in-app purchase items, and the price of such items. The entitlements are limited to use only in the iOS or iPadOS App Store in specific storefronts. In all other storefronts, streaming music apps and their metadata may not include buttons, external links, or other calls to action that direct customers to purchasing mechanisms other than in-app purchase.</p></blockquote>
<p>The European Commission in March fined <a href="https://www.macrumors.com/2024/03/04/eu-fines-apple-2-billion/">Apple $2 billion</a> for anti-competitive conduct against rival music streaming services. The fine also came with a requirement that Apple "remove the anti-steering provisions" from its <a href="https://www.macrumors.com/guide/app-store/">App Store</a> rules, which Apple has now done. Apple is restricted from repeating the infringement or adopting similar practices in the future, though it is worth noting that Apple plans to appeal the decision.</p>
<p>Apple has accused Spotify of manipulating the European Commission to get the rules of the ‌App Store‌ rewritten in its favor. "They want to use Apple's tools and technologies, distribute on the ‌App Store‌, and benefit from the trust we've built with users - and pay Apple nothing for it," <a href="https://www.macrumors.com/2024/03/04/eu-fines-apple-2-billion/">Apple complained</a> following the ruling.</p>
<p>In addition to updating its streaming music rules, Apple today also added games from retro game console emulator apps to the list of permitted software allowable under guideline 4.7. Guideline 4.7 permits apps to offer HTML5 mini apps and mini games, streaming games, chatbots, game emulators, and plug-ins.</p>
<blockquote><p>Apps may offer certain software that is not embedded in the binary, specifically HTML5 mini apps and mini games, streaming games, chatbots, and plug-ins. Additionally, retro game console emulator apps can offer to download games. You are responsible for all such software offered in your app, including ensuring that such software complies with these Guidelines and all applicable laws.</p></blockquote>
<p>Game emulators have managed to sneak onto the ‌App Store‌ several times over the years by using hidden functionality, but Apple has not explicitly permitted them until now. The rule change that allows for game emulators is worldwide, as is support for apps that offer mini apps and mini games.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/04/02/iphone-16-pro-models-rumored-features/">iPhone 16 Pro Expected Later This Year With These 12 New Features</a></h3><p>While the iPhone 16 Pro and iPhone 16 Pro Max are still months away from launching, there are already over a dozen rumors about the devices. Below, we have recapped new features and changes expected for the devices so far. These are some of the key changes rumored for the iPhone 16 Pro models as of April 2024:Larger displays: The iPhone 16 Pro and iPhone 16 Pro Max will be equipped with large...</p></div><div><h3><a href="https://www.macrumors.com/2024/04/02/ios-18-resource-reveals-visionos-redesign/">Alleged iOS 18 Design Resource Reveals visionOS-Like Redesign [Updated]</a></h3><p>A first look at iOS 18's rumored visionOS-style redesign may have been revealed by a new image of the Camera app. Alleged iOS 18 design resource. MacRumors received the above iPhone frame template from an anonymous source who claims they obtained it from an iOS engineer. It will allegedly be included as part of the Apple Design Resources for iOS 18, which helps developers visually design apps ...</p></div><div><h3><a href="https://www.macrumors.com/2024/04/03/apple-reportedly-investigating-robotics/">Apple Exploring 'Mobile Robot' That 'Follows Users Around Their Homes'</a></h3><p>Wednesday April 3, 2024 12:21 pm PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple is exploring various "personal robotics" projects in an effort to create its "next big thing," according to Bloomberg's Mark Gurman. Amazon's Astro robot One of these projects is described as a "mobile robot" that would "follow users around their homes," while another is said to be an "advanced table-top home device that uses robotics to move a display around":Engineers at Apple have...</p></div><div><h3><a href="https://www.macrumors.com/2024/04/02/apple-reveals-new-ai-system/">Apple Researchers Reveal New AI System That Can Beat GPT-4</a></h3><p>Apple researchers have developed an artificial intelligence system named ReALM (Reference Resolution as Language Modeling) that aims to radically enhance how voice assistants understand and respond to commands. In a research paper (via VentureBeat), Apple outlines a new system for how large language models tackle reference resolution, which involves deciphering ambiguous references to...</p></div><div><h3><a href="https://www.macrumors.com/2024/04/04/apple-suppliers-say-new-ipads-repeatedly-postponed/">Apple Suppliers Say New iPads Have Been 'Repeatedly Postponed'</a></h3><p>It has been nearly 18 months since Apple last updated its iPad lineup, and customers are anxiously waiting for new models to be announced. For months, there have been rumors about new iPad Pro and iPad Air models, but the estimated timeframe for their release has been repeatedly pushed back from March to April to May. In defense of these rumors, it does sound like Apple has experienced...</p></div><div><h3><a href="https://www.macrumors.com/2024/04/01/apple-card-savings-rate-to-decrease/">Apple Card Savings Account to Receive First-Ever Interest Rate Decrease</a></h3><p>Nearly one year after it launched in the U.S., the Apple Card's high-yield savings account will be receiving its first-ever interest rate decrease. Starting on April 3, the Apple Card savings account's annual percentage yield (APY) will be lowered to 4.4%, according to data on Apple's backend discovered by MacRumors contributor Aaron Perris. The account currently has a 4.5% APY. 4.4% will ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI and the Problem of Knowledge Collapse (166 pts)]]></title>
            <link>https://arxiv.org/abs/2404.03502</link>
            <guid>39946169</guid>
            <pubDate>Fri, 05 Apr 2024 19:30:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2404.03502">https://arxiv.org/abs/2404.03502</a>, See on <a href="https://news.ycombinator.com/item?id=39946169">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2404.03502">View PDF</a></p><blockquote>
            <span>Abstract:</span>While artificial intelligence has the potential to process vast amounts of data, generate new insights, and unlock greater productivity, its widespread adoption may entail unforeseen consequences. We identify conditions under which AI, by reducing the cost of access to certain modes of knowledge, can paradoxically harm public understanding. While large language models are trained on vast amounts of diverse data, they naturally generate output towards the 'center' of the distribution. This is generally useful, but widespread reliance on recursive AI systems could lead to a process we define as "knowledge collapse", and argue this could harm innovation and the richness of human understanding and culture. However, unlike AI models that cannot choose what data they are trained on, humans may strategically seek out diverse forms of knowledge if they perceive them to be worthwhile. To investigate this, we provide a simple model in which a community of learners or innovators choose to use traditional methods or to rely on a discounted AI-assisted process and identify conditions under which knowledge collapse occurs. In our default model, a 20% discount on AI-generated content generates public beliefs 2.3 times further from the truth than when there is no discount. Finally, based on the results, we consider further research directions to counteract such outcomes.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Andrew Peterson [<a href="https://arxiv.org/show-email/0d2c7712/2404.03502">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 4 Apr 2024 15:06:23 UTC (57 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breakthrough drug trial saw cancer vanish in every patient (2022) (133 pts)]]></title>
            <link>https://www.euronews.com/health/2022/06/07/this-breakthrough-drug-trial-saw-cancer-vanish-in-every-patient</link>
            <guid>39946137</guid>
            <pubDate>Fri, 05 Apr 2024 19:27:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.euronews.com/health/2022/06/07/this-breakthrough-drug-trial-saw-cancer-vanish-in-every-patient">https://www.euronews.com/health/2022/06/07/this-breakthrough-drug-trial-saw-cancer-vanish-in-every-patient</a>, See on <a href="https://news.ycombinator.com/item?id=39946137">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
        The cancer patients saw their tumours disappear after the treatment, and have been cancer-free for two years.
    </p><div><p>More than a dozen rectal cancer patients in the United States have seen their cancer disappear after undergoing experimental immunotherapy, in what doctors are calling an astonishing result.</p><p>The patients, who were part of a small clinical trial led by researchers from New York’s Memorial Sloan Kettering (MSK) Cancer Center, saw their tumours vanish after being treated with an experimental drug called dostarlimab.</p><p>Details of the trial were <a href="https://www.nejm.org/doi/pdf/10.1056/NEJMoa2201445"><strong>published</strong></a> on Sunday in the New England Journal of Medicine.</p><p>The paper described the results of 12 patients with rectal cancer, all of whom saw their cancer vanish after treatment with dostarlimab.</p><p>Participants received a dose of dostarlimab every three weeks for six months, with the idea being that they would need to undergo standard treatments of chemotherapy, radiation therapy and surgery following treatment.</p><p>However, researchers found that in every case, the cancer was cleared through the experimental treatment alone.</p><h2><strong>A first in cancer treatment</strong></h2><p>The trial has been hailed as a first in cancer treatment, with one of the paper’s authors, Dr Luis Diaz Jr of Memorial Sloan Kettering, telling the <strong><a href="https://www.nytimes.com/2022/06/05/health/rectal-cancer-checkpoint-inhibitor.html?referringSource=articleShare">New York Times</a></strong> that he knew of no other study in which a treatment completely obliterated a cancer in every patient.</p><p>“I believe this is the first time this has happened in the history of cancer,” he said.</p><p>Immunotherapy harnesses the body’s own immune system to identify and destroy cancer cells.</p><p>The trial focussed on a subset of rectal cancer patients whose cancer had a specific mutation, MSK said in a <a href="https://www.mskcc.org/news/rectal-cancer-disappears-after-experimental-use-immunotherapy"><strong>statement</strong></a>.</p><p>This sort of rectal cancer, known as "mismatch repair-deficient" (MMRd) rectal cancer, tends to respond poorly to standard chemotherapy regimens. In the trial, researchers wanted to investigate if immunotherapy alone could beat rectal cancer that had not spread to other tissues, the organisation said.</p><p>The research, which is ongoing, has seen at least 14 patients “and counting” have their tumours disappear, with none of them experiencing significant side effects, it added.</p><p>There was no need for standard treatments of radiation, surgery, or chemotherapy, and the cancer has not returned in any of the patients, who have been cancer-free for up to two years, it said.</p><p>“It’s incredibly rewarding to get these happy tears and happy emails from the patients in this study who finish treatment and realise, ‘Oh my God, I get to keep all my normal body functions that I feared I might lose to radiation or surgery’,” said Dr Andrea Cercek of Memorial Sloan Kettering, who co-led the trial.</p><h2>How does this immunotherapy work?</h2><p>Inspiration for the study came from a previous trial led by Dr Diaz, which saw patients taking a drug called pembrolizumab, the New York Times reported. That trial, which involved patients with advanced cancer that resisted standard treatment, saw participants’ tumours stabilise, shrink and even vanish.</p><p>In the current trial, researchers wanted to see what a similar drug, dostarlimab, would do if used before the cancer cells had a chance to spread.</p><p>This sort of treatment focuses on particular proteins called checkpoints, which are made by some types of immune system cells as well as some cancer cells, according to the US&nbsp;<a href="https://www.cancer.gov/publications/dictionaries/cancer-terms/def/immune-checkpoint-inhibitor"><strong>National Cancer Institute</strong></a>. The checkpoints, which keep immune responses from being too strong, can sometimes prevent immune cells from effectively killing cancer cells.</p><p>Like pembrolizumab, dostarlimab is a “checkpoint inhibitor”: It essentially “releases the brakes” on an immune cell, freeing it to recognise and attack cancer cells, according to MSK.</p><p>“When the brakes are taken off the immune cells, MMRd cells look especially strange because they have so many mutations. So the immune cells attack with much more force,” Dr Cercek said.</p><h2><strong>More research needed</strong></h2><p>The results have provided “what may be an early glimpse of a revolutionary treatment shift”, Dr Hanna Sanoff, an oncologist at the Lineberger Comprehensive Cancer Center at the University of North Carolina, who was not involved in the trial, <a href="https://www.nejm.org/doi/full/10.1056/NEJMe2204282"><strong>wrote in an editorial</strong></a> accompanying the paper.</p><p>However, she added that although the results are “cause for great optimism”, such an approach “cannot yet supplant our current curative treatment approach”.</p><p>“Whether the results of this small study conducted at Memorial Sloan Kettering Cancer Center will be generalisable to a broader population of patients with rectal cancer is also not known,” she said.</p><p>“In order to provide more information regarding which patients might benefit from immunotherapy, subsequent trials should aim for heterogeneity in age, coexisting conditions, and tumour bulk”.</p><p>The clinical trial is continuing to enrol patients and is growing, the MSK researchers said. They are also investigating to see if the same method can beat other cancers, and are looking at patients with gastric (stomach), prostate, and pancreatic cancers.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[For twenty years, PostSecret has broadcast suburban America’s hidden truths (118 pts)]]></title>
            <link>https://hazlitt.net/longreads/dark-matter</link>
            <guid>39945931</guid>
            <pubDate>Fri, 05 Apr 2024 19:09:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazlitt.net/longreads/dark-matter">https://hazlitt.net/longreads/dark-matter</a>, See on <a href="https://news.ycombinator.com/item?id=39945931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-wrap">
<p><strong>In the early aughts</strong>, Frank Warren ran a medical document delivery business in Germantown, Maryland. It was a monotonous job, involving daily trips to government offices to copy thousands of pages of journal articles for pharmaceutical companies, law firms, and non-profits. By his early forties, he had a house in a nice subdivision, a wife, a young daughter, and a dog. His family fostered children for a few weeks or months, and he felt a sense of purpose in helping kids who were suffering acute crises in their own homes. From the outside, things appeared to be going better than well. But inside, something was missing: A sense of adventure, or at least a little fun. An outlet to explore the weirder, darker, and more imaginative parts of his interior world. He’d never been one for small talk, preferring instead to launch into deep discussions, even with people he barely knew. He wondered if he could create a place like that outside of everyday conversation, a place full of awe, anguish, and urgency.</p>
<p>In the fall of 2004, Frank came up with an idea for a project. After he finished delivering documents for the day, he’d drive through the darkened streets of Washington, D.C., with stacks of self-addressed postcards—three thousand in total. At metro stops, he’d approach strangers. “Hi,” he’d say. “I’m Frank. And I collect secrets.” Some people shrugged him off, or told him they didn’t have any secrets. Surely, Frank thought, those people had the best ones. Others were amused, or intrigued. They took cards and, following instructions he’d left next to the address, decorated them, wrote down secrets they’d never told anyone before, and mailed them back to Frank. All the secrets were anonymous.</p>
<p>Initially, Frank received about one hundred postcards back. They told stories of infidelity, longing, abuse. Some were erotic. Some were funny. He displayed them at a local art exhibition and included an anonymous secret of his own. After the exhibition ended, though, the postcards kept coming. By 2024, Frank would have more than a million.</p>
<p>*</p>

<p>After his exhibit closed, the postcards took over Frank’s life. Hundreds poured into his mailbox, week after week. He decided to create a website, PostSecret, where every Sunday he uploaded images of postcards he’d received in the mail.</p>
<p>The website is a simple, ad-free blog with a black background, the 4x6 rectangular confessions emerging from the darkness like faces illuminated around a campfire. Frank is careful to keep himself out of the project—he thinks of the anonymous postcard writers as the project’s authors—so there’s no commentary. Yet curation is what makes PostSecret art. There’s a dream logic to the postcards’ sequence, like walking through a surrealist painting, from light to dark to absurd to profound.</p>

<p><em>I’m afraid that one day, we’ll find out TOMS are made by a bunch of slave kids!</em></p>

<p><em>I am a man. After an injury my hormones got screwed up and my breasts started to grow. I can’t tell anyone this but: I really like having tits.</em></p>

<p><em>I’m in love with a murderer… but I’ve never felt safer in anyone else’s arms.</em></p>

<p><em>I cannot relax in my bathtub because I have an irrational fear that it’s going to fall through the floor.</em></p>

<p>Even if you don’t see him on the website, Frank is always present: selecting postcards, placing them in conversation with one another. Off-screen, he’s a lanky, youthful 60-year-old emanating the healthy glow of those who live near the beach. Last August, we met at his house in Laguna Niguel, in a trim suburban neighbourhood a few miles from the ocean; when I asked about his week, he told me his Oura Ring said he’d slept well the night before. He offered me a seat on his back patio, and the din of children playing sports rang out from a park below. His right arm was in a sling. He’d fractured his scapula after a wave slammed him to the sand while he was bodysurfing.</p>
<p>As we spoke, I gathered that his outlook on most everything is positive—disarmingly so. The first time he had a scapula fracture, after a bike accident a few years ago, “I had this sense of release, I would say, from my everyday concerns and burdens,” he said. Physically exhausting himself through endurance exercise is his relief from the postcards, which skew emotionally dark. “I’ve had to become the kind of person that can do this every day,” he told me.</p>
<p>For years, Frank has been interested in postcards as a medium of narrative. Before PostSecret, he had a project he called “The Reluctant Oracle,” in which he placed postcards with messages like <em>Your question is a misunderstood answer</em> into empty bottles and deposited them in a lake near his house. (A <em>Washington Post</em><a href="https://www.washingtonpost.com/archive/local/2004/10/04/postcards-from-the-waters-edge/316c1548-a0c0-45b4-aaf1-ee023bd8a071/"> article</a> from the time said “The form is cliche: a message in a bottle,” but called the messages themselves “creepy and alluring.”)</p>
<p>What he considers his earliest postcard project, though, dates from his childhood. When he was in fifth grade, just as he was about to board the bus to camp in the mountains near Los Angeles, his mother handed him three postcards. She told him to write down any interesting experiences he had and mail the cards back home.</p>
<p>Frank took the cards. “It’s a Christian sleep-away camp, so of course a lot of crazy stuff happened, and of course I didn’t write my mom about any of it,” he said. But just before camp ended, he remembered the postcards, jotted something down, and mailed them. When he saw them in the mailbox a few days later, he wondered, <em>Am I the same person that wrote this message days ago?</em> The self, he had observed as a grade schooler, was always in a state of flux.</p>
<p>Examining secrets was part of a lifelong inquiry into what it means to speak. Frank’s parents split up when he was twelve—a shocking and destabilizing event that would define his adolescence. Soon after, he moved with his mother and brother from Southern California to Springfield, Illinois. Messed up by his parents’ divorce and his cross-country move, Frank became anxious and depressed.</p>
<p>While he was in high school, Frank went to a Pentecostal church three or four times a week, searching for a sense of connection with others. At the end of every service, churchgoers would pray at the altar to receive the Holy Spirit. Then, they spoke in tongues. All around him, the Spirit took hold, and people flailed their arms, wept, and danced. Frank looked on with envy and shame. No matter how hard he tried, no matter how many people tried to help him, he never spoke in tongues. It was a spiritual failure, this failure of language.</p>
<p>After college, while living in Virginia, he met a guy named Dave on the basketball court. They became close fast. Dave was funny and sensitive, and also athletic: he and Frank played hundreds of pickup games together. But Dave seemed to be struggling. He was living with his parents, couldn’t land a job. He spent a lot of time on computers, and confided in Frank that he was being bullied online. “You’ve got to get out of here,” Frank told him. That was one of the last things he ever said to Dave. Frank moved to Maryland, and not long after, he got a call from Dave’s father. Dave had killed himself. Frank was crushed. He felt like he should have seen more warning signs, and at the same time, felt helpless. He ruminated on how Dave might have interpreted their final conversation. <em>Out of his parents’ house</em>, he’d meant. <em>Not out of this life.</em></p>
<p>In the wake of his loss, Frank wanted “to do something useful with his grief,” so he decided to volunteer on a suicide prevention hotline. In training, his supervisors modelled how to inflect his voice to sound non-judgmental, how to ask open-ended questions and get below the surface of everyday conversation—lessons he would carry into his later life. He felt catharsis in listening to other people’s pain, and, in turn, sensed that they appreciated his presence. Simply by talking about their struggles, he found, they sometimes gained new understanding. Once every week or two, Frank listened for six hours, up until late in the quiet of his house, as people unravelled. He let them talk, and he let them stay silent. Listening to people’s confessions in the wee hours of the morning, Frank realized that people needed a way to talk about the messy topics often off limits in everyday conversation.</p>
<p>PostSecret contains echoes of his time volunteering on the suicide prevention hotline. Like the hotline, the project draws attention to the ways people conceal parts of themselves, and encourages disclosure. But the postcards go even further: They’re public, available for anyone to see. They show us the types of stories people normally keep guarded, creating, in the aggregate, a living inventory of our taboos.</p>

<p>*</p>

<p>What is a secret? Knowledge kept hidden from others, etymologically linked to the words <em>seduction</em> and <em>excrement</em>. To entice someone to look closer; to force them to look away.</p>
<p>Secrecy, writes psychologist Michael Slepian in his 2022 book, <em>The Secret Lives of Secrets</em>, is not an act, but an intention — “I intend,” he writes, “for people not to learn this thing.” “To intend to keep a secret,” he continues, “you need to have a mind capable of reasoning with other minds.” Thus, psychologists believe we start to develop a concept of secrets at around the age of three years old, when we also begin to understand that other people have minds—beliefs, desires, emotions—different from our own. At that point, researchers believe, we also develop the ability to experience self-conscious emotions like guilt, shame, and embarrassment. As our theory of the mind develops, we begin to worry that other people are unable or unwilling to understand us, which, in turn, motivates secrecy. Our teenage years are especially ripe for secret-keeping. As we develop stronger senses of self, we distance ourselves from our parents in a bid to assert control over our lives. Keeping secrets from our parents “allows an escape from [their] criticism, punishment, and anger,” Slepian writes, “but it also precludes the possibility of receiving help when it’s most needed.”</p>
<p>Cultural taboos create secrecy. Systems and structures uphold it. The nature, and content, of secret-keeping varies across cultures, but we have always hidden things from one another. The Greek gods had secret affairs; for centuries, women in central China wrote to each other in a secret language to evade the ire of oppressive husbands. Today, people keep secrets for safety: They conceal medical conditions to receive better insurance coverage, and hide their legal status so they don’t get deported. Even scripture has something to say about secrets, which is, mostly: don’t keep them. Proverbs 28:13 reads, “He who conceals his sins does not prosper, but whoever confesses them and renounces them finds mercy.” God, in other words, wants full disclosure.</p>
<p>We keep secrets because we are ashamed or afraid; we tell them because we want an escape. We want to feel accepted, seen. Naturally, we share some secrets with our friends and partners, but sometimes those relationships are the source of a secret, so instead we seek out neutral interlocutors. A bartender in Las Vegas told me the same client came, week after week, to talk specifically with him about her anxiety and troubled dating life. A hairdresser in Salt Lake City told me that Mormons grappling with their faltering faith came to her, an ex-Mormon, to work through family conflict. A therapist I met in Arkansas observed that many of her clients were leaving Christianity and using therapy as their new religion, which she found “a little spooky.”</p>
<p>When I asked what she meant, she told me that people, ex-Christian or otherwise, often look to therapy to find a source of meaning and release in their life—to fill a spiritual and emotional vacuum. Evangelicalism, she said, values “inappropriate vulnerability,” where people share testimonies and break boundaries in public venues. She’s wary when she hears those same stories within the context of therapy—when clients come in and feel obligated to spill everything up front, then ask for cures to their emotional ailments.</p>
<p>Later, thinking about secrets, I remembered this conversation and the phrase “inappropriate vulnerability.” How much vulnerability with strangers is appropriate? How much is too much?&nbsp;</p>

<p>*</p>

<p>For a while, PostSecret was my secret. The website existed in the internet nest I made for myself during adolescence, along with sites like fmylife.com, where users each posted a few lines about the tediums and mishaps of their days, often involving anxiety, depression, alcohol, and sex. They were websites that revealed glimpses of how other people lived, where I could gather anecdotes about adult life and begin to construct an idea of how my own world might look one day.</p>
<p>I grew up in Temecula, a California suburb not too far from where Frank currently lives. My friends and I wandered around the mall to try on skinny jeans, and sprinted around after dark to toilet-paper our classmates’ yards. Suburban life often felt stifling, so I had a habit of inventing stories to make my world seem more interesting. I recounted to friends, with narrative flourish, an encounter I’d had with a freshwater shark in an alpine lake. I created a mysterious, dark-haired boyfriend who I’d met at a soccer tournament. I’d never actually had a boyfriend.</p>
<p>Temecula had a distinctly conservative atmosphere, and it was impossible to escape the shame that accompanied any stray thought about boys, or my changing body. Ours was a town where, in 2008, neighbours supported a California ban on gay marriage. Residents protested the city’s first mosque with signs reading “no to sharia law” in 2010. Arsonists set fire to a local abortion clinic in 2017, and, just in the past two years, the school board would ban critical race theory and reject an elementary school curriculum that referenced Harvey Milk. My family went to a Methodist church, but I sometimes went to Mormon dances with friends; at one such dance in middle school, my dress was too short, so a chaperone made me staple cloth to the hem to cover my knees. During slow dances, we held on to boys’ shoulders from an arm’s length away.</p>
<p>Most everyone I knew in Temecula went to church on Sundays. But I found church boring. I’d excuse myself to go to the bathroom and linger there during sermons, counting the flowers on the wallpaper. I didn’t understand how God, who I didn’t see or hear, could exist.</p>
<p>But even if I didn’t believe God was real, my family did, and religious ideas subtly permeated our home life, shaping what we did and did not talk about. We talked about doing well in school and sports; we didn’t talk about our feelings, or puberty, or dating. My body was a secret, softening and bleeding, fascinating and repulsive.</p>
<p>I didn’t really speak to anyone about these changes, though I do remember one car ride to school with a friend. Her mom was driving, and my friend slipped me pieces of paper in the backseat. In her scrunched-up handwriting, she asked: <em>Do you wear bras? Do you have hair down there?</em> When I was a freshman, my period bled through my capris, and upperclassmen stared as I waddled across campus to the cross-country teacher’s classroom for gym shorts, sweat slicking down my back. I’d only ever used thin pads, and I was too anxious to ask about buying tampons. I didn’t want to talk about it, and no one ever asked.</p>
<p>I can barely remember sex ed programming in school; for years, I thought just sleeping next to a boy could get me pregnant. When, in high school, I started the drug Accutane to tame my unruly face, my dermatologist listed off options for pregnancy prevention to avoid harm to an unborn fetus. A family member who was in the room interjected: “She’ll choose abstinence.” It was only after I left and my world opened up that I understood where I came from. That my hometown, and even my own family, bred secrecy.</p>
<p>If I wanted answers to questions—Should I be shaving? Why do I sometimes feel sad?—I had to find them elsewhere. So I swivelled for hours on an office chair in front of a wheezing PC. It was here I learned of Frank’s work.</p>
<p>I remember the glow of the monitor in the dark upstairs hallway, the feeling of the mouse under my hand as I scrolled through secrets. I remember the padding of feet on stairs, the quick click of the X. Browser window vanished.</p>

<p>*</p>

<p>Over the years, Frank has developed a process for selecting secrets. He sorts the most promising ones into a few boxes. A good secret involves a particular alchemy of art and content. He likes secrets he’s never heard before—there are fewer and fewer these days, but every once in a while something new will pop up—and secrets he has seen but which are presented in a surprising way. At this point, twenty years after the project began, he mostly relies on intuition to select those he posts to the website. He’s kept every postcard over the years, even during a cross-country move. (The secrets he’s posted in the past decade are stored in his upstairs closet and garage; the rest are mostly on loan to the Museum of Us, in San Diego.) Every postcard, that is, except one. He blames a relative for losing it.</p>
<p>On the website, the scrolling experience is simple enough—scroll, rectangle, scroll, next rectangle—but within the rectangles, something else is happening: a cacophony of colour, scrawl, scribble, cross-outs, stickers, stamps, maps, photographs, sketches. Once, I saw locks of hair taped to a postcard; the writer said they collected the hair of children they babysat. The spectre of tactility, if not tactility itself, reminds the viewer that there are thousands of people behind these postcards, and thousands of hours over the course of twenty years were spent creating them.</p>
<p>Is this sociology? Psychology? Voyeurism? The postcards are shaped like little windows, glimpses into someone’s life, devoid of context. Frank likes to think of them, in the collective, as a cross-section of human nature, and each week he tries to select a range of moods, including a smattering of lighthearted secrets to round out his postcard representation of the psyche, even though most of what he receives is dark. I wondered if reading all these secrets gave him some sort of unique lens into who we are, but he’s not sure. Everyone has different parts of themselves or their lives that they’re afraid to acknowledge. Today, most secrets he receives are about relationships—either feeling dissatisfied with a partner or revolving around loneliness.</p>
<p>“My hope is when people read the secrets each week they have no idea what I think about religion, politics, or feminism. I want to be across the board, so anyone can see themselves in a secret,” he said. “If it’s strong and offensive, guess what, people keep offensive, racist secrets in their heart. That’s part of the project—exposing that.” He doesn’t intentionally seek out racist or sexist secrets, and doesn’t post anything that’s “hardcore racist,” but he thinks there’s value in representing the less-than-savoury aspects of human nature, because that’s a true representation of who we are as a whole.</p>
<p>That said, there are some kinds of secrets he generally doesn’t post. He often doesn’t upload postcards written from the throes of suicidal ideation. He doesn’t want the website to become a toxic cesspool of hopelessness. He also doesn’t generally post the photos included with secrets when doing so might share with someone intimate knowledge that they didn’t know themselves. One postcard, for example, included a family photograph alongside a secret reading, <em>My brother doesn’t realize his father isn’t the same as our father</em>. All the faces were visible. What if the brother saw it and recognized himself? “I don’t feel like I have ownership of that secret,” Frank said. Instead, he posted the text.</p>
<p>There’s no way to fact-check the secrets; Frank takes those sharing them at their word. In 2013, he posted a secret depicting an image from Google Maps and a red arrow. It read: <em>I said she dumped me, but really, I dumped her (body)</em>. After an internet uproar, Reddit users found that the location was in Chicago, someone called the police, and the police found nothing, eventually determining the secret was a hoax. Legally, Frank told me, the postcards are considered hearsay.</p>

<p>*</p>

<p>The secrets come without context, so Frank put me in touch with a handful of their authors so I could&nbsp; understand what inspired them to send him their postcards. (Occasionally, the authors email him and reveal their identities.) One of them, Casey, was possessed by secrets for all of her childhood. (Casey is a pseudonym; some people in this piece asked that their names be changed to maintain their privacy.) Her father discouraged his kids from making friends and conditioned in them a suspicion of other people. Because he didn’t work, and because her mother, who she suspected had undiagnosed schizophrenia, was shuttered inside all day, Casey was forced to support the family financially. At age fourteen, she was collecting soda bottles for money. The roof was falling in. She was afraid to tell her family she was gay.</p>
<p>When she left home for college in the early 2000s, she was finally able to make friends of her own accord. All of them knew about PostSecret—it was, at the time, in its heyday—and they’d scroll through the entries every Sunday to compare favourites.&nbsp;</p>
<p>Casey liked the honesty of PostSecret, how it gave voice to the unspoken. Her father still had a psychic hold over her life, but she started opening up about her family to her new friends. One of them, Ramón, was gay, too, and not out to his family. They soon became close. He was an aspiring actor, extroverted and funny. It seemed like he knew everyone, and in turn, everyone said he was their best friend. Casey and Ramón were the only people in their friend group who didn’t drink. They’d both grown up with unstable families and were afraid that alcohol would make them lose control.</p>
<p>But when, in junior year, she started experimenting with drinking, he cut off their friendship, accusing her of betraying her values. She was baffled and frustrated; she thought his response was extreme. To do something with her frustration, she submitted a secret decorated with a photo of him in a Halloween costume reading: <em>A real friend would have stayed around and helped me</em>. She heard he’d seen the postcard and was furious, but they never really talked about it, and today, decades later, they’re no longer close. Casey doesn’t keep secrets anymore. She doesn’t tolerate them.</p>
<p>Some secret-keepers described their postcard as liberating. One woman, V., sent in a secret acknowledging that her infertility was a relief because she wouldn’t have to go off her bipolar medications while pregnant. She wanted to become a mother, but she felt that, even if fertile, her body wasn’t capable of carrying a baby, and she didn’t know how to tell her husband. When she wrote her secret, she stared at it on her table, and when it was posted, she stared at it on her screen. She was struck by the fact she could reveal her secret to the public but not to her partner, and decided to tell him how she felt. Last September, they adopted a son.</p>
<p>Others didn’t seem to think much about their secrets after the fact, I learned when I talked to Carl, aged sixty-seven, a former federal law enforcement agent who lives in Washington State. His postcard depicted a hand of eight playing cards. With a Sharpie, he’d written in all caps: <em>GAMBLING DESTROYED MY 4TH AND LAST MARRIAGE.</em>&nbsp;</p>
<p>As we talked, he was to the point, answering questions in a sentence or two and never elaborating. I could picture him: a gruff, single, middle-aged man who left the house every once in a while to get a cup of coffee with a buddy. He must be lonely, though he’d never admit it, and gambling must have distracted him from his loneliness. “I don’t have any secrets,” he said. “And if I did, I wouldn’t be telling you.”</p>
<p>In 2007, he found a postcard among the “boxes and boxes of crap” in his dead mother’s house. At the time, the divorce from his fourth wife was fresh and he was feeling bitter, so he grabbed a Sharpie, scrawled his message, and put it in the mailbox. “That was that. I was blowing off steam,” he said. “It wasn’t some contemplative therapeutic thing.” Then, he told me something that upended my assumptions about him. “It wasn’t my gambling,” he said. “It was her gambling.”</p>
<p>Some postcards are impulsive, I realized. And because the postcard hadn’t specified whose gambling was the issue, I’d filled in the gap. Fascinated by my own mental jump, I asked more questions. How long had they been married? How did he learn about the gambling? Four marriages? What about the other three? To that last question, Carl said, “I don’t think that applies.”</p>
<p>I wanted to tell him: <em>Of course it applies! </em>I felt like his whole life was bound up in that postcard. Something led to the breakup with his first wife, and his second, and his third, which then led him to his fourth, and to their breakup, and to this piece of mail that ended up on Frank’s website. I wanted his autobiography. I wanted to know everything.</p>

<p>*</p>

<p>Frank told me, “Most of our lives are secret. I think that in the same way that dark matter makes up ninety percent of the universe—this matter that we cannot see or touch or have any evidence of except for its effect on gravity—our lives are like that too. The majority of what we are and who we are is kept private inside. It might express itself in our behaviours, and our fears, and even in human conflict and celebration, but always in this sublimated way.”</p>
<p>Carl was less philosophical. “This thing happened, I forgot about it, and now I’m talking to you.”</p>

<p>*</p>

<p>In the years after he created the website, Frank wrote several books and held live events, which were often sold-out with more than a thousand people in the audience. The events were usually scripted: Frank shared secrets he’d received and secrets of his own. He was no longer the invisible curator. He was, instead, the very reason people gathered. Today he doesn’t do many events, and he says he’s finished writing books. But at the height of PostSecret a decade ago, the events were central to his work—and underscore how much he values the catharsis that follows disclosure.</p>
<p>In 2013, Frank travelled to Australia for a PostSecret tour. At an event in Melbourne, he seemed comfortable assuming his central role; midway through the evening, he shared his own story. He played a voicemail from his mother, who’d seen a copy of Frank’s first book. “I’m not too happy with it, so forget about mailing me one,” she said. This, Frank told the audience, was not a surprise. “My mom has been like that as long as I’ve known her.” His brother and father were estranged from her, but he and his mother still had a functional relationship. “Even so, my earliest memories being around my mom are memories of having to have my defences up. I couldn’t let my guard down. My earliest memories keeping secrets were from my mom,” he said.</p>
<p>He told the audience about his experience with the Pentecostal church. At the end of every service, he explained, members would share their testimonies, and the congregation would cheer and shout, “Amen!”</p>
<p>Evoking that part of the service, Frank said he wanted to share his own testimony with the audience: If he could go back in time and erase all the moments in his life that had caused him pain and humiliation and suffering, he wouldn’t. Each one of those moments, he said, had brought him to this moment and to the person he is today. He likes who he is today. Suffering in silence led him to make PostSecret; the darkest parts of his past were inseparable from the parts of himself he liked now, and they made him a better father. If you can get through your own struggles, he told the audience, “you’ll have this beautiful story of healing, a story that you can share with others, others who are in that struggle.” Adopting a faint Southern accent, he asked, “Can I get a witness? Can I get an amen, brother?” Someone shouted amen. “Thank you, sister.” The theatre erupted into applause.</p>
<p>Then, Frank invited people to line up in the aisles and share their secrets. When one woman stepped up to the microphone, she said, “About a year ago, my ex-boyfriend raped me.” Her voice broke, and through tears, she continued, “And then told me he was getting engaged the next day. I think it’s about time I ask for help.” The audience applauded, and Frank commented that often the first step to making change in one’s life is sharing a secret. The woman left the microphone. The next person stepped up to share.</p>
<p>There was something both beautiful and garish about this spectacle. I remembered my conversation with the Arkansas therapist and the idea of inappropriate vulnerability. Watching the woman speak, I felt a mix of queasiness and regret and rubbernecking and curiosity. It was the feeling I have when I reveal too much about myself too quickly, without the slow buildup of trust and intimacy. Then the microphone went to another person, as if this were a conveyor belt of secrets, and there was no time to grapple with the weight of what had just been said.&nbsp;</p>
<p>In a 2016 LitHub essay, the writer Erik Anderson accused Frank of profiting, however indirectly, from other people’s traumas with his books and speaker fees. Frank often refers to secrets as the currency of intimacy—we exchange our secrets and become deeper friends or partners—but to Anderson, they’re also Frank’s professional currency, the reason he has a career at all. What happens to the secrets of PostSecret, he asks? Does having a secret posted actually do anything beneficial for the sharer? “Warren’s feel good message about the healing benefits of disclosure, about self-actualization through confession, may elide a painful truth about secrets,” he writes. “Once shared, especially anonymously, they become secrets again, hidden by and in the very excesses of the internet that made them possible.”</p>
<p>Frank told me he’s aware of the delicate role he plays as the keeper of people’s secrets. People trust him to treat their stories with care, so he’s never tried to monetize the website. It’s true, he says, that most of his income from the past decade has come from book advances and speaker fees. But, he told me, “I don’t get too much negative feedback from anyone in the community.” And as for what happens to the secrets, he says he hopes that by sharing them, people might be motivated to take action. Or perhaps, like on the suicide hotline, they can begin to see their secrets differently. We often assign secrets a physical weight; maybe by making them public, we can make them lighter, or smaller. But none of that is guaranteed.&nbsp;</p>
<p>For an hour, the theatre in Melbourne was transformed into a church of secrets. In the church of secrets, pain is pedagogy. Pain must teach us something, must have meaning, or else how could we live through it? We turn pain into a story, and make that story public in the hopes that we might get something in return. Empathy. Action. Friendship. Money. If I could go back, I would never choose pain.&nbsp;</p>

<p>*</p>

<p><em>I have a secret</em>—this is the language we use. We possess secrets, hold them close, though sometimes, perhaps, it’s better said that secrets possess us. And by secrets I mean the things we feel we cannot say, and so no one says them. What I mean by secret is taboo. What I mean by secret is fear.</p>
<p>Around a year ago, when I was in California for the holidays, a high school friend who I’ll call Sam invited me to meet in a park. Another friend, Alex, was in town too. I hadn’t seen either of them in years. We sat underneath a cypress tree and threw a rubber Frisbee to the Australian shepherd Sam was dog-sitting. The air smelled of salt. The grass itched our legs. Sam told us she’d been going to sex therapy with her husband, who she married when she was twenty-three. She was now twenty-seven. She and her husband were both deconstructing from the church, a painful personal reckoning with a culture that preached sexual purity. She’d always felt guilty about sex, and she didn’t know about pleasure.&nbsp;</p>
<p>“I didn’t start going to the gynecologist until after college,” I offered in commiseration. “Until tenth grade I thought having sex was just sleeping next to each other.”</p>
<p>“I always felt so observed,” said Alex, the first of us to have a boyfriend in high school.</p>
<p>“You were observed,” I said. We laughed, and I sat back and marvelled. The three of us had slept together on blow-up mattresses and swum at the beach and splashed in backyard pools but had never really talked—at least, not about the things we considered secrets. We were women now. All these years later, we’d finally found the words.</p>
<p>I wondered if Frank had ever been able to talk openly with his family. PostSecret was, after all, partly inspired by the difficulties of his upbringing. Frank told me his father was initially skeptical of the project, finding it voyeuristic, and maybe unnecessary. But eventually he began to appreciate the project and even told Frank something he’d been holding in for a while.&nbsp;</p>
<p>But Frank’s mother never came around. When we were sitting on his patio in Laguna Niguel, I asked about her a few times, and he told me a story. When he was a teenager, after he’d moved to Illinois, Frank got into a fight with her. He can’t remember what happened, only that he’d probably done something to anger her. He ran to his room and locked the door. His mom pounded on the door with a mop, broke through one of the panels, and reached her hand through to unlock the door. Frank ran into his bathroom and opened the window. It was snowing and dark outside, around 9 p.m. He climbed through the window and ran a block down the street to his friend’s house. He and his friend started talking as though this were a normal hangout, but eventually, his friend looked down at his feet. “Where are your shoes?” Frank was wearing only socks.</p>
<p>I asked another question. Gently, without drawing attention to what he was doing, Frank changed the subject. It was the first time I’d seen him withhold information. He didn’t want to talk about his mother anymore, and I didn’t need to know.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations [video] (158 pts)]]></title>
            <link>https://www.youtube.com/watch?v=C-uekD6VTIQ</link>
            <guid>39945733</guid>
            <pubDate>Fri, 05 Apr 2024 18:50:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=C-uekD6VTIQ">https://www.youtube.com/watch?v=C-uekD6VTIQ</a>, See on <a href="https://news.ycombinator.com/item?id=39945733">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Some colleges will soon charge $100k a year – how did this happen? (121 pts)]]></title>
            <link>https://www.nytimes.com/2024/04/05/your-money/paying-for-college/100k-college-cost-vanderbilt.html</link>
            <guid>39945133</guid>
            <pubDate>Fri, 05 Apr 2024 17:46:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/04/05/your-money/paying-for-college/100k-college-cost-vanderbilt.html">https://www.nytimes.com/2024/04/05/your-money/paying-for-college/100k-college-cost-vanderbilt.html</a>, See on <a href="https://news.ycombinator.com/item?id=39945133">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/04/05/your-money/paying-for-college/100k-college-cost-vanderbilt.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Rise and Fall of Silicon Graphics (302 pts)]]></title>
            <link>https://www.abortretry.fail/p/the-rise-and-fall-of-silicon-graphics</link>
            <guid>39944496</guid>
            <pubDate>Fri, 05 Apr 2024 16:42:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abortretry.fail/p/the-rise-and-fall-of-silicon-graphics">https://www.abortretry.fail/p/the-rise-and-fall-of-silicon-graphics</a>, See on <a href="https://news.ycombinator.com/item?id=39944496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>James Henry Clark was born on the 23rd of March in 1944 in Plainview, Texas. Clark’s family was far from wealthy. His father was fond of drinking and couldn’t keep a job. His mother worked at a local doctor’s office making about $225 per month (around $2605 in 2024). Clark’s parents divorced while Clark was still young, and while that salary may seem fine if low adjusted for inflation, Clark’s mother would only have received $175.50 ($2032) after income tax and social security tax, and it was the sole income for a woman and her three children. For himself, Clark was a bit rowdy. His high school highlights include setting off a smoke bomb on the band bus, smuggling a skunk into a school dance, telling his English teacher to go to Hell, drinking, and drag racing. Given the era, I imagine that the drinking was accompanied by chain smoking.</p><p>That times were different is… inadequate verbiage. For all the unruly behavior, Clark was only suspended from school twice. On his second suspension, young Clark decided he’d not be returning to school. He chose to join the US Navy and convinced his mother to sign the permission forms. Of course, this is Jim Clark, and the initial days of his naval career didn’t exactly go well. Clark had never taken a multiple choice test. He thought that for many questions more than one of the answers were at least partially true and therefore selected them. The officers in charge of test administration thought that Clark was attempting to fool the computer that checked the answers, and he was immediately sent out to sea with other delinquent recruits where he was given poor treatment, and rough and disgusting chores. The experience of Naval life lit a fire in Clark, and he chose to advance his station in life. He began learning about electronics, taking some general educational courses, and offering loans to other sailors at up to forty percent interest.</p><p>His first step was to get his General Education Diploma, which he did. He then enrolled at Tulane. Clark did well at Tulane but transferred to the University of New Orleans from which he received his BS and MA in Physics. He then attended the University of Utah where he earned his Ph.D. in computer science in 1974. From 1974 through 1978, Clark was employed as an assistant professor at UC Santa Cruz, but he left to become an associate professor at Stanford in 1979.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png" width="407" height="584.1046025104603" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:343,&quot;width&quot;:239,&quot;resizeWidth&quot;:407,&quot;bytes&quot;:94503,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F342c42c8-1711-4ced-9743-39ee9643b2f9_239x343.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Jim Clark</figcaption></figure></div><p><span>Early in his time at Stanford, Clark worked on a project with Xerox PARC with support from ARPA to develop three dimensional graphics. This led to the creation of the Geometry Engine. In “</span><em>The Geometry Engine: A VLSI Geometry System for Graphics,</em><span>” Clark also makes specific reference to Marc Hannah and Lynn Conway as being valuable contributors to the effort. What was the Geometry Engine? It was a special purpose microprocessor that handled matrix math along with point mapping. It featured an instruction set suitable both to 2D and 3D graphics, could generate quadratic/cubic curves and conic sections, worked with both vector and raster based systems, and operated in either integer or floating point systems as needed. In fewer words, Jim Clark and his team at Stanford along with the folks of PARC invented the GPU.</span></p><div data-component-name="FileToDOM"><div><p><img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Fattachment_icon.svg"></p><div><p>The Geometry Engine: A VLSI Geometry System for Graphics</p><p>1.01MB ∙ PDF file</p></div><p><a href="https://www.abortretry.fail/api/v1/file/66f72818-db99-4d73-977d-d8aca262e06b.pdf" rel=""><span>Download</span></a></p></div><p>The Geometry Engine: A VLSI Geometry System for Graphics</p><p><a href="https://www.abortretry.fail/api/v1/file/66f72818-db99-4d73-977d-d8aca262e06b.pdf" rel=""><span>Download</span></a></p></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png" width="726" height="958.6267605633802" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:568,&quot;resizeWidth&quot;:726,&quot;bytes&quot;:1079259,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc357f4a4-05b9-4de5-b812-8041185d4d84_568x750.png 1456w" sizes="100vw"></picture></div></a><figcaption>The Geometry Engine, image from ACM via Stanford</figcaption></figure></div><p>Clark founded Silicon Graphics Inc on the 9th of November in 1981, and he left Stanford early in 1982 to pursue building the company full time with just $25000 in funding (around $85000 in 2024) from a friend and the contents of his own accounts. Accompanying Clark in this adventure were Kurt Akeley, Dave Brown, Tom Davis, Mark Grossman, Marc Hannah, Herb Kuta, Rocky Rhodes, and Abbey Silverstone. While SGI knew they would deal in computers outfitted with a powerful GPU, they did not know precisely what else those computers should feature. As a result, Clark asked potential customers what they’d like to see in a workstation. While at least one potential customer was interested in VMS, NASA’s new Advanced Supercomputing division was very interested in UNIX and they were willing to pay. The division’s director at the time spoke with Clark, and (verbally) committed to purchasing at least eighteen workstations in their first order.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png" width="726" height="230.7642857142857" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:178,&quot;width&quot;:560,&quot;resizeWidth&quot;:726,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6feaa947-8be5-4c44-a921-78b74e1f40e9_560x178.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>As things began to come together around a product plan, </span><a href="https://www.mayfield.com/" rel="">Mayfield</a><span> invested in the young company. As the development and production of workstations is rather expensive, Clark and SGI’s other founders were forced to sell more and more of the company’s ownership to keep operating. The first product to ship was the IRIS 1000, where IRIS meant Integrated Raster Imaging System, in November of 1983. This machine was intended for use as a terminal for a VAX-11 and featured a Motorola 68000 clocked at 8 MHz with 768K RAM, a Geometry Engine clocked at 6 MHz capable of over six million geometric floating point operations per second, and a 10 Mbps ethernet NIC. The cabinet of the IRIS 1000 was ten inches wide, twenty one inches tall, twenty seven inches deep, and when fully assembled weighed in at one hundred pounds with a ten slot backplane. This machine was followed by the IRIS 1200 which was the same machine but with a twenty slot backplane. These were followed by workstation models 1400 and 1500 in April of 1984 which upgraded the CPU to the Motorola 68010 clocked at 10 MHz with 1.5M of RAM. These machines were differentiated from one another in the size of HDD they featured with the 1500 having been larger. The 1400 featured a 72MB </span><a href="https://www.abortretry.fail/p/the-floppy-disk" rel="">winchester disk</a><span>, while the 1500 featured 474MB of SMD. Both of these ran a UNIX SVR4 variant with BSD enhancements called GL2, and they featured twenty slot backplanes. The main system boards in these four machines were licensed from Andy Bechtolsheim just before he founded </span><a href="https://www.abortretry.fail/p/the-network-is-the-computer" rel="">Sun Microsystems</a><span>. The 1000 and 1200 used the PM1 and the 1400 and 1500 used the PM2. These were not cheap systems with the IRIS 1000 having a price of $22500 (around $67200 in 2024) and the 1400 having a price of $35700 in 1984 (around $106600 in 2024). These twenty slot machines were eighteen inches wide, twenty nine inches tall, and twenty seven inches deep, and fully assembled weighed in at two hundred pounds. By the time the first of these machines sold to Carnegie-Mellon University’s Electronic Imaging Lab, the founders of SGI owned very little of their company.</span></p><p>From nearly the first day that SGI’s hardware was on the market, software developers began trying to exploit the machines’ graphics capabilities. A rather prominent example of this was Wavefront Technologies in Santa Barbara led by Bill Kovacs, Larry Barels, and Mark Sylvester. Their first product was called Preview and launched in 1984 on SGI’s hardware. Their customer list included Universal Studios, NBC, NASA, and Electronic Arts. Naturally, this also informs us that these companies were using SGI hardware.</p><p>Given the outline of his youth, it isn’t very surprising that Clark was a hands-off kind of manager. He would hire the brightest minds he could, set a general target, and then let people go after it however they saw fit. There are two narratives for what follows. The first and most common that I’ve read was that Mayfield didn’t much care for Clark’s management style and they brought Ed McCracken formerly of HP in as CEO. The second narrative states that Clark didn’t care for running the company and brought McCracken in on his own accord. Whatever the case, McCracken stated of Clark:</p><blockquote><p><em>Jim's not a day-to-day person. He works in his own time frame. He takes complex things and makes it simple. It might take a month, a day, or a year. He gets in these moods for a while where he's almost unavailable. He's most effective when he's in that mood.</em></p></blockquote><p>In August of 1985, the company introduced the IRIS 2000 series of workstations. These machines were all based upon the the PM2 system board featuring the Motorola 68010 clocked at 10 MHz with a floating point coprocessor (SKYFPM-M-03). Naturally, these all featured the graphics engine as well. The IRIS 2000 and 2200 were ten slot backplane, shipped without a disk, and were intended for use as terminals. The 2300 and 2400 were twenty slot backplane and shipped with winchester disks. The IRIS 2500 was rackmount and used SMD disks. The 2000 series used a Geometry Engine clocked at 8 MHz. A few months after the initial launch of these upgraded machines, SGI launched the turbo line. This included the 2300T, 2400T, and 2500T which featured the IP2 system board with a Motorola 68020 clocked at 16 MHz, an FP1 floating point unit, and 2MB to 16MB of RAM. The RAM of the turbo units used a newer, faster, local bus. As a result, the RAM between turbo and non-turbo systems could not be mixed. This was an important bit of information as SGI did offer turbo upgrades for non-turbo systems that would then require the purchase of expensive proprietary memory.</p><p>In January of 1986, SGI made their initial public offering raising $17.2 million (nearly $49 million in 2024) with trading having started at $3 per share and topping $30 on the day. The following month, the company introduced the IRIS 3000 line. These are very similar to the IRIS 2000 turbo machines but with Enhanced IRIS Graphics. These featured either ten or twelve Graphics Engines clocked at 10 MHz with either eight or thirty two bitplanes depending upon configuration. The 3000 line could be ordered with either winchester disk drives, ESDI drives, or SMD.</p><p>Also in 1986, Control Data Corporation and Silicon Graphics signed a deal under which CDC would resell IRIS machines under CDC’s own branding. As far as I know, no complete listing of which models sold under what naming survives today, but it is known that the IRIS 3130 was resold as the CDC Cyber 910. This would make it a machine with twelve GEs at 10 MHz and ESDI drives.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg" width="1456" height="1941" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86698a05-3d98-4ae8-a1d6-332ca658a339_1875x2500.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>IRIS 3130, rebadged by Control Data, image from sgistuff.net</figcaption></figure></div><p>In March of 1987, Silicon Graphics announced a new machine that marked a major transition for the company. The Professional Iris was a RISC machine built around the R2000 from MIPS Computer Systems (another project started at Stanford and spun out as its own company) clocked at 8 MHz. The company’s press release read:</p><blockquote><p><em>The first member of the Iris line is the 4D/60, a RISC superworkstation with a 32-bit 8 MHz CPU from MIPS Computer Systems. It offer performance three times that of the Silicon Graphics Iris 3100 series. The graphics performance has been enhanced with 38 custom and semicustom graphic chips. It performs 140,000 32 bit three dimensional floating point transformations per second and renders over 4,500 100-pixel polygons per second with smooth shading and hidden surface removal. It offers 24 colour bit-planes for more than 16 million colours; four user-accessible system planes for overlay or underlay, menu and windowing functions; a 24-bit Z-buffer enabling hidden surface removal with greater accuracy and realism; high-level primitives such as splines and surfaces for more accurate renderings; and a multi-mode graphics windowing environment. </em></p><p><em>Standard configuration includes 4Mb CPU, eight colour bit-planes for 256 colours); four system planes, a Weitek-based floating point accelerator board; a 170Mb ESDI disk and controller; a 19″ 1,280 by 1,024 60Hz non-interlaced colour monitor; keyboard and mouse; and a floor-standing chassis with 12 VME slots and a 1,000-watt power supply. </em></p><p><em>Software compatible with the previous generation, it runs Unix System V.3 with a base price of $74,000.</em></p></blockquote><p>The Professional Iris line included the 4D/60 mentioned in the press release followed by the 4D/50, 4D/70, 4D/80, and 4D/85. All of these featured the R2000 CPU with a floating point coprocessor. The 50 and 60 had an R2000 clocked at 8 MHz, while the 70 was at 12.5 MHz, and the 80 and 85 were clocked at 16.7 MHz. For comparison to other architectures, the 4D/50 was capable of seven million instructions per second, the 70 was capable of ten million, and the 80 was capable of thirteen million. The 50 and 60 had memory configurations starting at 4MB and upgradeable to 12MB. The rest of the lineup started at 8MB and could be upgraded to a maximum of 144MB. The first of the 4D/60, 50, and 70 systems to ship utilized the Clover 1 graphics system. Later models shipped with Clover 2 branded as IRIS GT. IRIS GT brought hardware support for lighting, smooth shading, antialiasing, pan/zoom of images, arbitrarily shaped windows, and other rather modern capabilities. Importantly, the bus for this system was a proprietary 64 bit bus. The actual chips powering all of the graphics capabilities were still the Graphics Engines, but these were updated some and they were capable of twenty million floating point operations per second. The Professional Iris series brought an end to the disk anarchy of the previous lineup and all systems utilized SCSI hard disks, and QIC-120 tape drives were also available. These systems were resold by both Control Data Corporation and Prime Computers. The UNIX version mentioned in the press release was SGI’s 4D1 which would later be renamed IRIX.</p><p>On the 29th of March in 1988, Control Data Corporation announced that it would be acquiring twenty percent of Silicon Graphics for $68.9 million (nearly $181 million in 2024) and extending its licensing deal for reselling SGI’s machines with an agreement to purchase $150 million (around $393 million in 2024) in hardware over the next three years.</p><p>On the 16th of September in 1988, SGI announced that IBM would be purchasing graphics cards and licensing IRIS GL, the software library for SGI’s graphics, for use in the IBM RS/6000 POWERStation. McCracken commented:</p><blockquote><p><em>We are pleased to establish a relationship with IBM and look forward to working with them. The agreement reinforces our long-time conviction that three-dimensional graphics will become a mainstream technology in the computer industry. As real-time 3D graphics is made more affordable, the rapid growth that the 3D workstation industry is experiencing will continue to escalate.</em></p></blockquote><p><span>The card in question was the IrisVision, and while I refer to it as a card, it was really two cards. The primary card held the Graphics Engine and daughter cards held the framebuffer and z-buffer memories totaling 5MB for the framebuffer and 3.75MB for the z-buffer. The primary card connected to the computer via its MCA bus edge connector, and it provided a DE-15 connector for display attachment. Overall, the IrisVision&nbsp;MCA card’s hardware was extremely similar to the graphics system in the SGI Personal Iris series introduced in 1987. It featured SGI’s fifth generation geometry processing pipeline (referred to as GE5, or Graphics Engine five), either an eight or twenty four bit per pixel frame buffer, and twenty four bits per pixel z-buffer. Also, just as the workstations’ hardware did, the IrisVision implemented the entire IrisGL API in hardware. The primary difference in IrisVision was the presence of a VGA (DE-15) passthrough for 2D graphics. In the course of the IrisVision’s development, an IBM PS/2 running OS/2 was used for testing and development. This resulted not only in a minimal OS/2 driver, but also in an ISA version of the IrisVision being developed. Ultimately, the only major customer SGI had managed to obtain was IBM for the MCA card for the RS/6000 UNIX workstations. Their struggle may have been that the card was priced at $4995 (just over $13000 in 2024). The company ultimately spun off the entire project as a separate company, Pellucid, which didn’t fare well. The former SGI employees who started Pellucid still managed to change the world </span><a href="https://www.abortretry.fail/p/so-powerful-its-kind-of-ridiculous" rel="">when they founded 3dfx</a><span> which used similar technology as well as the passthrough for 2D graphics.</span></p><p>SGI held a rather firm grasp on high-end graphics workstations, but hadn’t yet made a push into the entry level market. This changed with the introduction of the Personal Iris lineup. The line started with the 4D/20 which made use of a R2000 CPU from MIPS clocked at 12.5 MHz achieving ten million instructions per second. The other three machines made use of the R3000. In the 4D/25 the R3000 was clocked at 20 MHz achieving sixteen million instructions per second. In the 4D/30, the clock speed was pushed to 30 MHz and the performance was bumped to twenty seven million instructions per second. The highest performance model was the 4D/35 at 36 MHz and thirty three million instructions per second. Maximum memory supported on these systems was 128MB. Personal Iris systems were sold by both SGI and Control Data as expected, but these systems were also offered rebadged by the somewhat newly reconstituted Groupe Bull. From what I can find, Bull’s sales of rebadged SGI machines weren’t great; they had better luck with NEC hardware. For the naming “Personal Iris” and the thought that SGI would be attacking the “low-end” of the workstation market… the pricing wasn’t all that reflective unless one were to compare to “high-end” SGI machines which could reach lofty prices of about $100000 (about $262000 in 2024). The Personal Iris line started at $20000 (roughly $52000 in 2024).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg" width="709" height="985.6829268292682" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:513,&quot;width&quot;:369,&quot;resizeWidth&quot;:709,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b5e2b31-3d63-4334-b5c4-d9c78fd90694_369x513.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>SGI Personal Iris, press image from SGI</figcaption></figure></div><p>The other, much higher end and far more expensive, SGI lineup introduced at this time was the PowerSeries which were multi-processor systems (up to eight CPUs) and could be deskside or rackmount. These systems could also support higher clocks at up to 40 MHz which in combination with up to eight processors could mean performance over two hundred thirty million instructions per second. The power of these systems was put to use in the movies The Abyss, Terminator 2, and Jurassic Park among many more.</p><p>In March of 1991, Compaq acquired thirteen percent of SGI for $135 million (around $307 million in 2024) along with an agreement to invest another $50 million (about $114 million in 2024) in the development of a new workstation that would be priced at around $7500 (roughly $17100 in 2024). </p><p>The most famous and beloved SGI systems were introduced from 1991 to 1995. These models were the Indigo, Indigo 2, and the Indy. The corresponding high-end systems were the Crimson, and Challenge series. The first Indigo system released in 1991 featured a MIPS R3000 CPU clocked at 30 MHz. The Indigo (and Crimson) moved SGI’s systems to 64 bit MIPS CPUs starting with the R4000 at 100 MHz and the R4400 at 150 MHz in 1992. The 150 MHz part in an Indigo could achieve one hundred twenty million instructions per second. The Indigo 2 was first introduced in 1993 with the MIPS R4400 CPU and “Extreme” graphics. The Indy was lowest end SKU of the three, and it was introduced in July of 1993 with a 100 MHz R4000PC CPU, 24 bit graphics system, 16MB of RAM, the IRIX operating system, a fifteen inch monitor, and a price of $4995 (about $10700 in 2024).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg" width="995" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:995,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9d42849-c42c-4882-8d34-4b5f8e8fa97c_995x512.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>SGI Indigo 2, image from unixhq.com</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2969758,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff762c302-1f27-41f2-88e0-b610553c931f_3264x3264.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>SGI Indigo 2 10000 IMPACT, from my personal collection</figcaption></figure></div><p>On the 13th of March in 1992 announced that it was acquiring MIPS Computer Systems via a stock swap worth about $333 million (around $737 million in 2024). This followed MIPS having had financial problems, high employee turnover, and the exit of the company’s president, Charles Boesenberg, one month earlier. For SGI, the acquisition ensured their part supply. MIPS Computer Systems became MIPS Technologies. The combined company had revenues at around $1 billion (about $2.21 billion in 2024). However, the large acquisition did mean that SGI posted a loss on the year of about $118 million (or $261 million in 2024). This move also briefly brought SGI into the ACE alliance that aimed to build a workstation standard on the MIPS CPU and the UNIX operating system as well as the 80386/486 and NT. This group was built of Compaq, MIPS, Microsoft, DEC, SCO, Acer, CDC, Kubota, Olivetti, NKK, Prime Computer, Pyramid Technology, Siemens, Sony, Sumitomo, Tandem, Zenith, and Wang. SGI and Compaq left the alliance rather promptly. This could be due to their own arrangement not long before, but ACE fell apart completely not much later anyway. I suspect that no strong alliance of fierce competitors would last long in a market that was shrinking due to low-cost commodity hardware and software consistently improving year over year in the PC compatible market. Yet, the SGI Indigo 2, Indy, Challenge and a few more were mildly compliant with the ACE ARC (Advanced RISC Computing) standard.</p><p>On the 30th of June in 1992, Silicon Graphics released OpenGL. This was a cross-platform API for both 2D and 3D graphics allowing hardware acceleration of rendering via one or more GPUs descended directly from IRIS GL. Unlike its predecessor, OpenGL did not have windowing, and it didn’t offer a mouse or keyboard API. IRIS GL had been developed before X and other graphical environments were available, and therefore had needed those features, but OpenGL had no such requirements. Another major change in the transition to OpenGL regarded feature availability. IRIS GL presupposed the use of SGI’s hardware. OpenGL could not make such an assumption, and as a result it allowed features not supported by a GPU to be rendered in software by the CPU. One customer this would positively affect was Microsoft who’d licensed IRIS GL for inclusion in NT in 1991.</p><p>At the end of 1992, Jim Clark met with Nintendo CEO Hiroshi Yamauchi to discuss bringing 3D graphics to Nintendo’s next game console. In many ways, the Nintendo 64 was an SGI workstation in miniature with a MIPS R4300 CPU clocked at 93.75 MHz offering one hundred twenty five million instructions per second, 4MB of Rambus DRAM at 250 MHz (actually 4.5MB but 512K is visible only to the GPU) which could be doubled with a RAM expansion pack, and the Reality coprocessor clocked at 62.5 MHz which offered the SGI GraphicsEngine (though a more modest version). The system supported 16.8 million colors, a maximum resolution of 640x480, and audio sampled at up to 44.1 KHz. Unfortunately, the design of the system meant that the full capabilities would almost never be fully realized. For example, there was no dedicated sound chip, so high sample rates would tax the CPU, and while the R4300 is 64 bit, the Nintendo 64 had a 32 bit data bus. Yet, showing the nature of the hardware packed into the Nintendo 64 is the Nintendo 64DD. This offered 64MB read/write magnetic disks (similar to Zip), a real time clock, internet connectivity via a 28.8 kbps modem, keyboard, mouse, and audio/video capture effectively transforming the Nintendo 64 into a small workstation. The expansion, after significant delays and a one year two and a half month life on the market, was a commercial failure. The Nintendo 64 itself, however, was a huge success following its release in 1996.</p><p>Industrial Light and Magic had been using SGI hardware since 1987, and on the 8th of April in 1993, they announced a partnership with SGI to create the Joint Environment for Digital Imaging, or JEDI. This allowed the two companies to gain insight from each other’s work. SGI got access to much of ILM’s software expertise while ILM got access to the latest and greatest hardware at a discount.</p><p><span>In 1994, Jim Clark left SGI, sold his shares in the company, and went on to </span><a href="https://www.abortretry.fail/p/the-modern-memex" rel="">partner with Marc Andreessen and start Netscape</a><span>.</span></p><p>In 1995, SGI spent about $500 million (or $1 billion in 2024) acquiring Alias Research, Kroyer Films, and Wavefront technologies. At roughly the same time, SGI worked with DreamWorks SKG to form DreamWorks Digital Studio where these newly acquired companies’ products could be put to good use.</p><p><span>On the 26th of February in 1996, Silicon Graphics acquired Cray Research for $740 million (or $1.47 billion in 2024). This gave SGI control of around forty percent of the high performance computing market at the time. While many industry analysts speculated about SGI’s motives, Cray was struggling to survive and they had multiple installations at NASA. While SGI had been successful in entertainment, that sector accounted only for something around ten percent of SGI’s annual revenues. The bulk of SGI’s customers were governmental, so much so that SGI created the wholly owned subsidiary Silicon Graphics Federal Inc to hold those contracts and provide service and support for governmental organizations. In this way, SGI was essentially making sure they couldn’t lose one of their largest and most valuable customers, NASA, as they’d be the provider of not only workstations but also the support and service of NASA’s supercomputers. The supercomputer relationship benefited SGI all the way to 2008 with </span><a href="https://www.nas.nasa.gov/hecc/resources/pleiades.html" rel="">Pleiades</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg" width="800" height="618" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:618,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6616be0-a734-4714-a45d-e53460d6a36b_800x618.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Cray Y-MP Model D installation at NASA’s Glenn Research Center, image from Clive England via cray-history.net</figcaption></figure></div><p>The new SGI workstations of 1996 were the O2 and O2+ series. These systems were very different from both their predecessors and successors in that they utilized a unified memory architecture via the Memory &amp; Rendering ASIC (MRE). The MRE had direct paths to all parts of the O2 such as the CPU, memory, I/O, compression, display, and imaging. Due to this structure, graphics hardware wasn’t optional but rather integral to the system’s design. The O2 could come equipped with an R5000, RM5200SC, RM7000A, R10000, or R12000 CPU. Frequencies ranged from 180 MHz to 400 MHz, all options had on-board floating point support, and could support up to 1GB of unified memory via eight 128MB DIMMs of one hundred thirty nine pin SDRAM.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg" width="1456" height="2174" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2174,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F690e88f8-b329-423e-b05e-adc75b6c7c4f_1674x2500.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The high-end deskside and rackmount options made available at this time were the Origin 2000, Origin 200, and Onyx 2 series. These were multiple CPU systems with distributed, shared-memory architecture called S2MP. The Origin 200 was the entry level system, the Onyx 2 was a step up, and the Origin 2000 was the premium SGI branded system and was rackmount. This series also had Cray Origin at the super-premium level with up to one hundred twenty eight R10000 CPUs. The IRIX operating system shipped with these models supported SMP.</p><p>On the 14th of May in 1997, SGI announced the acquisition of ParaGraph International Inc. ParaGraph was a vendor of VRML and web graphics software, and after the acquisition the company and its assets were moved to Mountain View with the new name of Cosmo Software. McCracken commented:</p><blockquote><p><em>One of the most important long-term growth opportunities for Silicon Graphics is to empower the designers, developers, and service providers of the Second Web. With the acquisition of the leading PC 3D Internet company and the formation of Cosmo Software, we are increasing our investment and reinforcing our leadership in the market for the software and services that will bring about this new interactive medium.</em></p></blockquote><p>Bringing the technologies of Onyx 2 series to the midrange workstation was the Octane, released in January of 1997. This was the a desktop machine instead of deskside, but it supported dual CPUs. This line featured the crossbar switch that debuted in the high-end and server machines of the prior year. The concept was that instead of a traditional shared bus, each subsystem could communicate with any other without interference. The crossbar switch had seven ports: HEART ASIC (CPU and memory), graphics (Impact [first or second generation] or VPro), XIO B, XIO C, XIO D, built-in I/O, PCI bridge. The Octane did have a higher-end version, the Octane 2, which featured more powerful CPUs and GPUs, higher density memory support, and a beefier PSU. CPUs in the Octane ranged from the R10000 at 175 MHz to the R14000A at 600 MHz, and RAM ranged from 64MB to 2GB.</p><p><span>Silicon Graphics didn’t do too well in 1997 overall. For revenues of $3.6 billion (or $7 billion in 2024) the company posted a loss of $78.6 million (roughly $152 million in 2024). On the 29th of October in 1997, Ed McCracken resigned as did the executive vice president of sales and marketing, Gary Lauer. The company then laid off around nine percent of its employees (about seven hundred people). Richard Belluzzo (formerly at HP) took over as CEO and Robert H. Ewald who was already the executive vice president of computer systems (formerly president of Cray) took over Lauer’s job duties. Some sources claim that McCracken was forced out, but this isn’t accurate. At the annual shareholder meeting in Palo Alto, McCracken announced his resignation stating: “</span><em>after a great deal of thought, the time is right for me and the company to make a change</em><span>.” He then proceeded to find and to hire his replacement himself.</span></p><p>Around this time, Silicon Graphics filed a lawsuit against a startup called ArtX. ArtX was founded by Dr. Wei Yen and around nineteen other SGI employees who’d worked on the Nintendo 64. The company’s original goal was to develop a PC graphics chip that would rival 3dfx. Then, in May of 1998, the company gained a contract to develop a graphics processor for Nintendo’s next generation game console, the GameCube. At COMDEX in the autumn of 1999, the company unveiled the Aladdin 7 chipset which shipped as integrated GPUs on K6-2 and K6-3 motherboards made by Acer Labs. ArtX was bought by ATI in February of 2000. ArtX’s technology was incorporated into ATI’s GPUs from 2002 until roughly 2005. SGI’s lawsuit against ArtX was quietly dropped in 1998 without any settlement having been reached.</p><p><span>On the 1st of January in 1998, shortly after taking over as CEO, Belluzzo sold two of SGI’s PCB factories and restructured the company from twenty six groups to just five. SGI then setup MIPS Technologies as its own legal entity (though SGI maintained a majority ownership), terminated the Cosmo software business, and proceeded to make customers hesitant to continue investments into the MIPS architecture by announcing SGI’s intent to migrate to </span><a href="https://www.abortretry.fail/p/the-itanic-saga" rel="">Itanium</a><span> (and collaborating on projects Monterey and Trillian) while simultaneously launching an IA-32 series of machines running NT known as the Visual Workstation. Additionally, the company began outsourcing the manufacturing of their computers, and cut the operating budget by about $200 million (or $381 million in 2024). In Spring of 1998, the company announced a lawsuit against NVIDIA for patent infringement.</span></p><p>None of this helped to change the overall direction of the company. Revenues fell to $3.1 billion and the company posted a loss of $460 million for 1998. On the 20th of July in 1999, without adequate funding to continue the lawsuit against NVIDIA, SGI and NVIDIA agreed to license one another their respective patent portfolios. The company continued to lose money, and Belluzzo left on the 22nd of August in 1999 to lead Microsoft’s MSN division.</p><p>As Bob Bishop took the reigns of SGI, things looked dark. AMD announced their 64 bit architecture in October, PC graphics had made massive strides while remaining significantly less expensive than SGI’s offerings, NT was proving to be a solid and less expensive competitor to UNIX, Linux was eating away at traditional UNIX market segments, and Itanium still hadn’t launched. By this point, the company had no fall back as they’d mostly stopped investment into new MIPS CPUs.</p><p>On the 2nd of March in 2000, SGI sold Cray to Tera Computer for $22 million (or $40 million in 2024). Tera promptly renamed itself to Cray Inc as it took on an installed base of six hundred supercomputers and two hundred customers across thirty different countries.</p><p>SGI’s final MIPS workstations were the Fuel and Tezro lines. Fuel was introduced in 2002 with the R14000 clocked at 500 MHz, up to 4GB of DDR SDRAM, and SGI’s VPro graphics. Models were available with up to an R16000 CPU clocked at 900 MHz. The Tezro was launched in 2003 starting at $20500 (or $34574 in 2024). This model featured only the R16000 and could be configured at clock speeds from 600 MHz to 1 GHz with 512MB to 8GB of DDR SDRAM and SGI’s VPro graphics. Fuel workstations were single CPU, but Tezro was offered with one to four CPUs. While SGI’s IA-32, Itanium, and Xeon workstations and servers sold, they didn’t make much money. On the 10th of July in 2003, SGI vacated and leased their headquarters to Google.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg" width="1024" height="601" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:601,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;undefined&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="undefined" title="undefined" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0506bff0-0947-41de-8184-b978e6120faa_1024x601.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>SGI’s headquarters after becoming the Googleplex, by Coolcaesar at English Wikipedia</figcaption></figure></div><p>As SGI’s fortunes continued to decline, the company sold Alias Systems (formerly Alias|Wavefront) for $58 million on the 16th of April in 2004 to Accel-KKR (roughly $95 million in 2024). Then, in November of 2005, SGI was delisted from the NYSE due its stock price sinking below the minimum required. In January of 2006, Dennis McKenna was hired as president and CEO, and named chairman of the board. Bishop remained on the board of directors and served as vice chairman. On the 8th of May, the company filed for bankruptcy protections. The campus leased to Google was sold to Google in June of 2006 for $319 million (or $491 million in 2024). It’s prior home in Mountain View had been sold to the Computer History Museum in 2002. The company emerged from bankruptcy and was relisted in October. The official end of SGI’s MIPS and IRIX came on the 29th of December in 2006 with the final orders being fulfilled in March of 2007. Bob Ewald replaced McKenna as CEO on the 9th of April in 2007. In December of 2008, SGI was again delisted. On the 1st of April in 2009, the company filed for bankruptcy, and was subsequently purchased by Rackable Systems for around $42 million on the 11th of May in 2009 (roughly $65 million in 2024). Rackable renamed itself Silicon Graphics International following the acquisition, and it was later bought by Hewlett Packard Enterprise.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fortran on WebAssembly (201 pts)]]></title>
            <link>https://gws.phd/posts/fortran_wasm/</link>
            <guid>39944275</guid>
            <pubDate>Fri, 05 Apr 2024 16:21:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gws.phd/posts/fortran_wasm/">https://gws.phd/posts/fortran_wasm/</a>, See on <a href="https://news.ycombinator.com/item?id=39944275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<video controls="" autoplay="" muted="" loop="" width="100%">
<source src="https://gws.phd/posts/fortran_wasm/digits.webm" type="video/webm; codecs=vp9">
<source src="https://gws.phd/posts/fortran_wasm/digits.mp4" type="video/mp4">
</video>
<figcaption>
<a href="#mnist">Digit classification</a> with machine learning, running in the browser using BLAS routines compiled to WebAssembly.
</figcaption>
<section id="introduction">
<h2>Introduction</h2>
<p><span><a href="https://fortran-lang.org/">Fortran</a></span><a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> is one of the oldest programming languages around. It first appeared in 1957, making it older than the C programming language, the Intel 4004 CPU, and even the IBM System/360 series of mainframe computers<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Fortran was created at a time when the byte had just been invented, and computers were still made of vacuum tubes and frustration.</p>
<div><p><sup>1</sup>&nbsp;The name is derived from <em>Formula Translator</em>. <span>Fortran</span> was originally stylised in all-caps, but modern Fortran has dropped it.</p><p><sup>2</sup>&nbsp;The System/360 was released around 1965. The 4004 around 1970. And <a href="https://en.wikipedia.org/wiki/The_C_Programming_Language">K&amp;R C</a> was published in 1978.</p><p><sup>3</sup>&nbsp;The argument goes that Fortran’s lack of aliasing and use of native arrays rather than pointer arithmetic allow the optimiser to generate more efficient output than an equivalent C program. However, there are also counter-arguments against this.</p></div><p>Over the years, Fortran has formed a rich history of use for computationally intensive scientific and engineering applications. It has powered the fluid dynamics of weather prediction and climate models, provided the condensed matter simulations for my <a href="https://gws.phd/docs/thesis_gws.pdf">PhD</a>, and is still considered by some to be more efficient than C for numerically heavy work<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>. The syntax of modern Fortran is also surprisingly easy to get up and running with. This is not your parent’s <span>Fortran&nbsp;77</span> code; most restrictions that make fixed‑form Fortran awful to use are no longer in place in modern Fortran.</p>
<p>In a clash of computational eras, this blog post is about compiling existing Fortran code for WebAssembly so that it can run in a web browser. I’ll describe the method we currently use for the <a href="https://github.com/r-wasm/webr">webR</a> project, compiling Fortran code using a patched version of <a href="https://llvm.org/">LLVM</a>’s <code>flang-new</code> compiler<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>. The post also serves as a request for help. The method I describe unfortunately relies on a hack, and this hack means that I cannot contribute the changes back to LLVM without assistance from a more experienced compiler developer.</p>
<p><sup>4</sup>&nbsp;“LLVM” is not an acronym, it is the full name of the project.</p></section>
<section id="so-whats-the-problem">
<h2>So, what’s the problem?</h2>
<p>There are a surprising number of potential methods and toolchains available to compile Fortran to WebAssembly. Unfortunately, none of the available options are feature complete. Each method has its drawbacks, and none are a simple plug-and-play solution.</p>
<p>Back in 2020, the situation was summarised wonderfully by Christoph Honal’s article <a href="https://chrz.de/2020/04/21/fortran-in-the-browser/">FORTRAN in the Browser</a>. Even now, the article is worth reading and provides a nice background for this post. I personally owe a lot to Christoph’s article, particularly for its description of the <a href="https://dragonegg.llvm.org/">Dragonegg</a> toolchain. Without that article, I would have given up on Fortran for WebAssembly a long time ago.</p>
<p>Our goal by the end of this post is to be able to compile a modern Fortran routine to WebAssembly that takes in some numerical arguments, computes the output of <a href="http://www.netlib.org/blas/">BLAS</a> and <a href="https://netlib.org/lapack/">LAPACK</a> routines<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>, and either returns the result or prints it to console. From what I remember, in 2020 none of the methods outlined in Christoph’s article could do this satisfactorily. Dragonegg and <code>f2c</code> both get close but have some drawbacks, as I’ll describe below.</p>
<div><p><sup>5</sup>&nbsp;LAPACK (Linear Algebra Package) is a popular library of routines used to numerically solve problems in linear algebra. It’s written in Fortran 90 and itself relies on a BLAS (Basic Linear Algebra Subprograms) library.</p><p><sup>6</sup>&nbsp;via <a href="https://pyodide.org/en/stable/">Pyodide</a> and <a href="https://github.com/r-wasm/webr">webR</a>.</p></div><p>Together, BLAS and LAPACK routines provide a powerful numerical platform. Running them in the browser opens the door for several higher-level programming environments that rely on them under the hood, such as SciPy or R<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>. The beauty of this approach is that it allows you to bring existing and extensively battle-tested tools and libraries to the web without having to rewrite them all in Rust or JavaScript.</p>
<p>Later, I’ll show an example of this with a machine learning demo that directly uses BLAS routines compiled from Fortran to WebAssembly. Rather than having to write fiddly linear algebra numerical algorithms in JavaScript, we can use reliable and efficient BLAS routines directly<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p><sup>7</sup>&nbsp;Whilst running machine learning algorithms in a web browser will never be as efficient as using dedicated hardware, such as a GPU, I still think it’s a fun demo.</p><section id="compiler-round-up">
<h2 data-anchor-id="compiler-round-up">Compiler round-up</h2>
<p>Since <a href="https://chrz.de/2020/04/21/fortran-in-the-browser/">FORTRAN in the Browser</a> was published things have changed a little, particularly when it comes to the LLVM-based Fortran compilers. As far as I am aware, here’s a brief round-up of the current situation in 2024.</p>
<section id="the-f2c-utility">
<h3 data-anchor-id="the-f2c-utility">The <code>f2c</code> utility</h3>
<p>The <a href="https://en.wikipedia.org/wiki/F2c"><code>f2c</code></a> program converts <span>Fortran 77</span> to C code, which Emscripten can then compile into WebAssembly. This is the method that the <a href="https://pyodide.org/en/stable/">Pyodide</a> project uses to compile Python packages containing Fortran code. They say that this <a href="https://pyodide.org/en/0.25.0/project/roadmap.html#find-a-better-way-to-compile-fortran">“does not work very well”</a>. The tool doesn’t work with modern Fortran code, and even after conversion the result still throws fatal errors and requires extensive patching.</p>
</section>
<section id="lfortran">
<h3 data-anchor-id="lfortran">LFortran</h3>
<p>The LFortran compiler has made great strides over the last few years. In 2020, it was <a href="https://gitlab.com/lfortran/lfortran/-/issues/121">missing a lot of features</a> and only supported a very small subset of Fortran. Now it now supports a much wider range of language features and can be used to compile a reasonable amount of Fortran code. It can even compile to WebAssembly out of the box!<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><sup>8</sup>&nbsp;Check out the LFortran demo at <a href="https://dev.lfortran.org/">https://dev.lfortran.org</a>. While extremely impressive, note that the first thing I tried was changing <code>x ** 2</code> to <code>x ** 3</code> and saw that such a change is currently not supported by the code generator.</p><p>However, there are still some barriers that make using LFortran a little rough. The project is currently considered to be in alpha phase and the developers state that issues compiling real-world code are expected. While it can successfully compile some projects, such as <a href="https://github.com/fortran-lang/minpack">MINPACK</a>, the full Fortran specification is not yet supported and so many larger projects still cannot be compiled.</p>
<p>The LFortran developers are targeting full support for Fortran 2018, and its standout feature is an interactive Jupyter-like Fortran REPL. With a few more years of development, I expect that LFortran will be an excellent choice for compiling Fortran code for WebAssembly.</p>
</section>
<section id="dragonegg">
<h3 data-anchor-id="dragonegg">Dragonegg</h3>
<p><a href="https://dragonegg.llvm.org/">Dragonegg</a> is a plugin for GCC that uses the GNU compilers as a frontend and emits LLVM IR. With this, LLVM can be used as the backend to produce WebAssembly output. The technique works, and it was the original method that I used to compile Fortran sources for the <a href="https://github.com/r-wasm/webr">webR</a> project.</p>
<p>However, there are some pretty serious drawbacks to this approach. Dragonegg requires a very old version of GCC and LLVM<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>. For most users, this means setting up a virtual machine or Docker container to provide the necessary environment. The LLVM IR emitted by Dragonegg also needs some fairly nasty post-processing before LLVM can produce WebAssembly output. Take a look at the <a href="https://github.com/r-wasm/webr/blob/v0.1.0/tools/dragonegg/emfc.in">script originally used by webR</a> to get an idea of the extra processing required.</p>
<p><sup>9</sup>&nbsp;The latest supported versions are <code>gcc-4.8</code> and <code>llvm-3.3</code></p><p>Nevertheless, in 2020 this was the only real way to compile Fortran code for WebAssembly.</p>
</section>
<section id="classic-flang">
<h3 data-anchor-id="classic-flang">Classic flang</h3>
<p><a href="https://github.com/flang-compiler/flang">“Classic” Flang</a><a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> is another Fortran compiler targeting LLVM, based on an open-sourced PGI/NVIDIA compiler <code>pgfortran</code>. Classic Flang never supported 32-bit output, so it is not an option for us since we’ll be using <code>wasm32</code> for our target architecture. This will likely be the case until browser support for 64-bit Wasm memory has improved<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<div><p><sup>10</sup>&nbsp;Previously, Flang or Flang-7.</p><p><sup>11</sup>&nbsp;At the time of writing Firefox, Chrome and Node supports <code>wasm64</code>, but locked behind a feature flag.</p></div><p>Even so, the project documentation itself suggests that choosing to use Classic Flang for a new project today is probably not a great idea:</p>
<blockquote>
<p>Classic Flang […] continues to be maintained, but the plan is to replace Classic Flang with the new Flang in the future.</p>
</blockquote>
</section>
<section id="llvm-flang">
<h3 data-anchor-id="llvm-flang">LLVM Flang</h3>
<p><a href="https://flang.llvm.org/docs/">“LLVM Flang”</a><a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> is a full ground-up reimplementation of a Fortran frontend for LLVM. It was designed to replace Classic Flang, developed by much of the same team, and was accepted as part of the LLVM project as of LLVM 11. As such, the Flang sources can now be found in <a href="https://github.com/llvm/llvm-project/tree/main/flang">the official LLVM source tree</a>.</p>
<p><sup>12</sup>&nbsp;Also known as Flang, new Flang, or <code>flang-new</code>. Previously, F18.</p><p>Flang is not yet considered to be ready for production use, but its development is extremely active right now and pre-production versions of the <code>flang-new</code> compiler have been made available by the team. In recent years, the compiler has become very usable for compiling real-world Fortran code.</p>
<p>Currently, LLVM Flang cannot generate WebAssembly output out of the box. Despite this, we’ll soon see that with LLVM’s modular design it’s possible to use the Flang frontend with LLVM’s WebAssembly backend. With this, we can take advantage of all the development work put into the Flang frontend by the NVIDIA and PGI teams for our own purposes of compiling Fortran to WebAssembly.</p>
<p>This was also possible back in 2020, though it required larger patches to LLVM, injecting custom maths routines, and a multi-step compilation process. Now, due to the impressive development efforts in the <code>flang-new</code> frontend, creating a Fortran to WebAssembly compiler is possible with just a few small changes to LLVM’s source code.</p>
</section>
</section>
</section>
<section id="building-and-using-llvm-flang-for-webassembly">
<h2>Building and using LLVM Flang for WebAssembly</h2>
<p>If one were interested in trying out LLVM Flang, they might grab an LLVM release using their package manager of choice. However, following that route will disappoint us, as a <code>flang-new</code> binary is not included<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a>.</p>
<p><sup>13</sup>&nbsp;At least, not with LLVM v17.0.6 for macOS using <code>brew</code>.</p><div>
<p><span>[~/fortran]</span>brew install llvm</p>
<p><span>==&gt;</span> Downloading https://formulae.brew.sh/api/formula.jws.json</p>
<p>################################################################################ 100.0%</p>
<p><span>==&gt;</span> Downloading https://ghcr.io/v2/homebrew/core/llvm/manifests/17.0.6_1</p>
<p>################################################################################ 100.0%</p>
<p><span>==&gt;</span> Fetching llvm</p>
<p><span>==&gt;</span> Downloading https://ghcr.io/v2/homebrew/core/llvm/blobs/sha256:8d739bdfa4152</p>
<p>################################################################################ 100.0%</p>
<p><span>==&gt;</span> Installing llvm</p>
<p><span>==&gt;</span> Pouring llvm--17.0.6_1.arm64_sonoma.bottle.tar.gz</p>
<p><span>==&gt;</span> Summary</p>
<p>🍺  /opt/homebrew/Cellar/llvm/17.0.6_1: 7,207 files, 1.7GB</p>
<p><span>==&gt;</span> Checking for dependents of upgraded formulae...</p>
<p><span>==&gt;</span> No broken dependents to reinstall!</p>
<p><span>[~/fortran]</span>flang-new</p>
<p>zsh: command not found: flang-new</p>
</div>
<p>Since we will be modifying the LLVM Flang source in any case, we’ll have to compile from scratch. Let’s grab the LLVM v18.1.1 sources and start there instead. Feel free to follow along at home; I’ll try to provide all the commands and everything you need.<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<p><sup>14</sup>&nbsp;I’m going to assume you’re familiar with <a href="https://emscripten.org/">Emscripten</a> and have a version of that toolchain on your path. If not, and you want to play along, start with <a href="https://github.com/emscripten-core/emsdk">emsdk</a> to setup Emscripten on your machine, get comfortable with compiling C code for WebAssembly, then return here to continue.</p><div>
<p><span>[~/fortran]</span>git clone --depth=1 --branch=llvmorg-18.1.1 https://github.com/llvm/llvm-project.git</p>
<p>Cloning into 'llvm-project'...</p>
<p>remote: Enumerating objects: 138937, done.</p>
<p>Receiving objects: 100% (138937/138937), 199.81 MiB | 11.36 MiB/s, done.</p>
<p>Note: switching to '6009708b4367171ccdbf4b5905cb6a803753fe18'.</p>
<p>Updating files: 100% (132077/132077), done.</p>
<p><span>[~/fortran]</span>cmake -G Ninja -S llvm-project/llvm -B build \</p>
<p>-DCMAKE_INSTALL_PREFIX=llvm-18.1.1 \</p>
<p>-DCMAKE_BUILD_TYPE=MinSizeRel \</p>
<p>-DLLVM_DEFAULT_TARGET_TRIPLE="wasm32-unknown-emscripten" \</p>
<p>-DLLVM_TARGETS_TO_BUILD="WebAssembly" \</p>
<p>-DLLVM_ENABLE_PROJECTS="clang;flang;mlir"</p>
<p>-- The C compiler identification is AppleClang 15.0.0.15000100</p>
<p>-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc</p>
<p>-- Detecting C compiler ABI info - done</p>
<p>-- Performing Test HAVE_POSIX_REGEX -- success</p>
<p>-- Configuring done (29.0s)</p>
<p>-- Generating done (2.9s)</p>
<p>-- Build files have been written to: fortran/build</p>
<p><span>[~/fortran]</span>cmake --build build</p>
<p>...</p>
<p>[6136/6136] Linking CXX executable bin/obj2yaml</p>
</div>
<p>Grab a cuppa<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>, this step uses a lot of resources and can take a <strong>very</strong> long time.</p>
<p><sup>15</sup>&nbsp;<strong>cuppa</strong> (/ˈkʌp.ə/): noun, informal, UK. A hot drink, usually tea or coffee.</p><section id="interlude-calling-fortran-subroutines-from-c">
<h2 data-anchor-id="interlude-calling-fortran-subroutines-from-c">Interlude — Calling Fortran subroutines from C</h2>
<p>While we wait for LLVM to build, start up a new terminal and we’ll remind ourselves how to compile and link a Fortran subroutine as part of a C program. The principles here will help us later when it comes to calling Fortran from JavaScript.</p>
<p>First, let’s write a simple subroutine that takes in three integer arguments: <code>x</code>, <code>y</code>, and <code>z</code>. It will set the value of <code>z</code> to the sum of <code>x</code> and <code>y</code>. Name our new subroutine <code>foo</code> and save the file containing your subroutine as <code>foo.f08</code>.</p>
<div id="cb1" data-filename="foo.f08"><pre><code><span id="cb1-1"><span>SUBROUTINE</span> foo(x, y, z)</span>
<span id="cb1-2">    <span>IMPLICIT</span> <span>NONE</span></span>
<span id="cb1-3">    <span>INTEGER</span>, <span>INTENT(IN)</span>  <span>::</span> x, y</span>
<span id="cb1-4">    <span>INTEGER</span>, <span>INTENT(OUT)</span> <span>::</span> z</span>
<span id="cb1-5">    z <span>=</span> x <span>+</span> y</span>
<span id="cb1-6"><span>END</span></span></code></pre></div>
<p>Notice how, generally, Fortran routines pass arguments by reference and we can declare how an argument will be used in the subroutine using <code>INTENT()</code>. Assuming you already have a traditional Fortran compiler like <code>gfortran</code> installed<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a>, compile the Fortran source into an object file.</p>
<p><sup>16</sup>&nbsp;You don’t <em>need</em> a native fortran compiler to follow along with the rest of the post, but if you’d like one you can get <code>gfortran</code> from your OS’s usual package manager as part of the GCC compiler suite. There’s also <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/fortran-compiler.html"><code>ifort</code></a>, if you’re on an Intel CPU.</p><div>
<p><span>[~/fortran]</span>gfortran -c foo.f08 -o foo.o</p>
<p><span>[~/fortran]</span>file foo.o</p>
<p>foo.o: Mach-O 64-bit object arm64</p>
<p><span>[~/fortran]</span>nm foo.o</p>
<p>0000000000000038 s EH_frame1</p>
<p>0000000000000000 T _foo_</p>
<p>0000000000000000 t ltmp0</p>
<p>0000000000000038 s ltmp1</p>
</div>
<p>I’m on an M1 macOS machine, so my resulting object is a Mach object for ARM. If you’re a Linux user, you should see something like <code>ELF 64-bit LSB shared object, x86-64</code>. I’ve also run <code>nm</code> to take a look at the names of the symbols in the object that the compiler has built. Keep an eye on the symbol for our subroutine — on my machine it’s named <code>_foo_</code>. The leading underscore is fairly standard, but the trailing underscore differs from what is usual for C procedures.</p>
<p>Let’s write a C program that calls our Fortran subroutine<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a>. Notice again how we pass the arguments by reference to the external symbol. Also, if your Fortran compiler added the trailing underscore, we’ll need to include it when we declare the symbol name in C.</p>
<p><sup>17</sup>&nbsp;Modern Fortran standards provide a Fortran module <a href="https://fortranwiki.org/fortran/show/iso_c_binding"><code>iso_c_binding</code></a> and a C header file <code>ISO_Fortran_binding.h</code> to improve C interoperability, but our code is going to be simple enough that we can do without those today.</p><div id="cb2" data-filename="main.c"><pre><code><span id="cb2-1"><span>#include </span><span>&lt;stdio.h&gt;</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span>extern</span> <span>void</span> foo_<span>(</span><span>int</span><span>*,</span> <span>int</span><span>*,</span> <span>int</span><span>*);</span></span>
<span id="cb2-4"></span>
<span id="cb2-5"><span>int</span> main<span>()</span> <span>{</span></span>
<span id="cb2-6">    <span>int</span> x <span>=</span> <span>1</span><span>,</span> y <span>=</span> <span>1</span><span>,</span> z<span>;</span></span>
<span id="cb2-7">    foo_<span>(&amp;</span>x<span>,</span> <span>&amp;</span>y<span>,</span> <span>&amp;</span>z<span>);</span></span>
<span id="cb2-8"></span>
<span id="cb2-9">    printf<span>(</span><span>"</span><span>%d</span><span> + </span><span>%d</span><span> = </span><span>%d\n</span><span>"</span><span>,</span> x<span>,</span> y<span>,</span> z<span>);</span></span>
<span id="cb2-10">    <span>return</span> <span>0</span><span>;</span></span>
<span id="cb2-11"><span>}</span></span></code></pre></div>
<p>Compile the C source using <code>gcc</code> or equivalent, and then run the resulting binary to observe a truly staggering level of numerical computation.</p>
<div>
<p><span>[~/fortran]</span>gcc main.c foo.o -o main</p>
<p><span>[~/fortran]</span>./main</p>
<p>1 + 1 = 2</p>
</div>
</section>
<section id="returning-to-llvm-flang">
<h2 data-anchor-id="returning-to-llvm-flang">Returning to LLVM Flang</h2>
<p>Once LLVM has finished compiling, the <code>flang-new</code> binary should be available in the directory <code>build/bin</code>. We can now run it and confirm that it has been set up to produce binaries for <code>wasm32</code> and Emscripten.</p>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new --version</p>
<p>flang-new version 18.1.1 (https://github.com/llvm/llvm-project.git dba2a75e9c7ef81fe84774ba5eee5e67e01d801a)</p>
<p>Target: wasm32-unknown-emscripten</p>
<p>Thread model: posix</p>
<p>InstalledDir: .../fortran/build/bin</p>
</div>
<p>Great! Let’s try compiling our Fortran subroutine using our freshly built compiler.</p>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new -c foo.f08 -o foo.o</p>
<p>error: fortran/llvm-project/flang/lib/Optimizer/CodeGen/Target.cpp:1162:</p>
<p>not yet implemented: target not implemented</p>
<p>LLVM ERROR: aborting</p>
</div>
<p>Ah, not so great. The <code>wasm32-unknown-emscripten</code> target triple unfortunately hasn’t been implemented yet in the <code>flang-new</code> compiler.</p>
<p>And so here comes our first patch to LLVM. We will implement the target by extending Flang’s list of known target specifics. The required changes, shown below as a diff, can be mostly deduced by looking at the other targets implemented in the file <code>flang/lib/Optimizer/CodeGen/Target.cpp</code>.</p>
<div id="cb3"><pre><code><span id="cb3-1"><span>diff --git a/flang/lib/Optimizer/CodeGen/Target.cpp b/flang/lib/Optimizer/CodeGen/Target.cpp</span></span>
<span id="cb3-2">index 83e7fa9b440b..49e73ec48e0a 100644</span>
<span id="cb3-3"><span>--- a/flang/lib/Optimizer/CodeGen/Target.cpp</span></span>
<span id="cb3-4"><span>+++ b/flang/lib/Optimizer/CodeGen/Target.cpp</span></span>
<span id="cb3-5"><span>@@ -1109,6 +1109,44 @@ struct TargetLoongArch64 : public GenericTarget&lt;TargetLoongArch64&gt; {</span></span>
<span id="cb3-6"> };</span>
<span id="cb3-7"> } // namespace</span>
<span id="cb3-8"> </span>
<span id="cb3-9"><span>+//===----------------------------------------------------------------------===//</span></span>
<span id="cb3-10"><span>+// WebAssembly (wasm32) target specifics.</span></span>
<span id="cb3-11"><span>+//===----------------------------------------------------------------------===//</span></span>
<span id="cb3-12"><span>+</span></span>
<span id="cb3-13"><span>+namespace {</span></span>
<span id="cb3-14"><span>+struct TargetWasm32 : public GenericTarget&lt;TargetWasm32&gt; {</span></span>
<span id="cb3-15"><span>+  using GenericTarget::GenericTarget;</span></span>
<span id="cb3-16"><span>+</span></span>
<span id="cb3-17"><span>+  static constexpr int defaultWidth = 32;</span></span>
<span id="cb3-18"><span>+</span></span>
<span id="cb3-19"><span>+  CodeGenSpecifics::Marshalling</span></span>
<span id="cb3-20"><span>+  complexArgumentType(mlir::Location, mlir::Type eleTy) const override {</span></span>
<span id="cb3-21"><span>+    assert(fir::isa_real(eleTy));</span></span>
<span id="cb3-22"><span>+    CodeGenSpecifics::Marshalling marshal;</span></span>
<span id="cb3-23"><span>+    // Use a type that will be translated into LLVM as:</span></span>
<span id="cb3-24"><span>+    // { t, t }   struct of 2 eleTy, byval, align 4</span></span>
<span id="cb3-25"><span>+    auto structTy =</span></span>
<span id="cb3-26"><span>+        mlir::TupleType::get(eleTy.getContext(), mlir::TypeRange{eleTy, eleTy});</span></span>
<span id="cb3-27"><span>+    marshal.emplace_back(fir::ReferenceType::get(structTy),</span></span>
<span id="cb3-28"><span>+                         AT{/*alignment=*/4, /*byval=*/true});</span></span>
<span id="cb3-29"><span>+    return marshal;</span></span>
<span id="cb3-30"><span>+  }</span></span>
<span id="cb3-31"><span>+</span></span>
<span id="cb3-32"><span>+  CodeGenSpecifics::Marshalling</span></span>
<span id="cb3-33"><span>+  complexReturnType(mlir::Location loc, mlir::Type eleTy) const override {</span></span>
<span id="cb3-34"><span>+    assert(fir::isa_real(eleTy));</span></span>
<span id="cb3-35"><span>+    CodeGenSpecifics::Marshalling marshal;</span></span>
<span id="cb3-36"><span>+    // Use a type that will be translated into LLVM as:</span></span>
<span id="cb3-37"><span>+    // { t, t }   struct of 2 eleTy, sret, align 4</span></span>
<span id="cb3-38"><span>+    auto structTy = mlir::TupleType::get(eleTy.getContext(),</span></span>
<span id="cb3-39"><span>+                                          mlir::TypeRange{eleTy, eleTy});</span></span>
<span id="cb3-40"><span>+    marshal.emplace_back(fir::ReferenceType::get(structTy),</span></span>
<span id="cb3-41"><span>+                          AT{/*alignment=*/4, /*byval=*/false, /*sret=*/true});</span></span>
<span id="cb3-42"><span>+    return marshal;</span></span>
<span id="cb3-43"><span>+  }</span></span>
<span id="cb3-44"><span>+};</span></span>
<span id="cb3-45"><span>+} // namespace</span></span>
<span id="cb3-46"><span>+</span></span>
<span id="cb3-47"> // Instantiate the overloaded target instance based on the triple value.</span>
<span id="cb3-48"> // TODO: Add other targets to this file as needed.</span>
<span id="cb3-49"> std::unique_ptr&lt;fir::CodeGenSpecifics&gt;</span>
<span id="cb3-50"><span>@@ -1158,6 +1196,9 @@ fir::CodeGenSpecifics::get(mlir::MLIRContext *ctx, llvm::Triple &amp;&amp;trp,</span></span>
<span id="cb3-51">   case llvm::Triple::ArchType::loongarch64:</span>
<span id="cb3-52">     return std::make_unique&lt;TargetLoongArch64&gt;(ctx, std::move(trp),</span>
<span id="cb3-53">                                                std::move(kindMap), dl);</span>
<span id="cb3-54"><span>+  case llvm::Triple::ArchType::wasm32:</span></span>
<span id="cb3-55"><span>+    return std::make_unique&lt;TargetWasm32&gt;(ctx, std::move(trp),</span></span>
<span id="cb3-56"><span>+                                               std::move(kindMap), dl);</span></span>
<span id="cb3-57">   }</span>
<span id="cb3-58">   TODO(mlir::UnknownLoc::get(ctx), "target not implemented");</span>
<span id="cb3-59"> }</span></code></pre></div>
<p>Save the contents of the above diff as the file <code>add-wasm32-target.diff</code>, and then apply it to the <code>llvm-project</code> directory using <code>git</code> or the <code>patch</code> utility. Then, rebuild LLVM Flang. It should be quicker to build the second time, as most generated objects are unaffected by the change.</p>
<div>
<p><span>[~/fortran]</span>patch -p1 -d llvm-project &lt; add-wasm32-target.diff</p>
<p><span>[~/fortran]</span>cmake --build build</p>
<p>...</p>
<p>[180/180] Generating ../../../../include/flang/ieee_arithmetic.mod</p>
</div>
<p>Once LLVM has been recompiled, try compiling our Fortran source once again.</p>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new -c foo.f08 -o foo.o</p>
<p><span>[~/fortran]</span>file foo.o</p>
<p>foo.o: WebAssembly (wasm) binary module version 0x1 (MVP)</p>
<p><span>[~/fortran]</span>llvm-nm foo.o</p>
<p>00000001 T foo_</p>
</div>
<p>Success! We can confirm this is a real WebAssembly object using the <code>file</code> utility, and <code>llvm-nm</code><a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a> can see the <code>foo</code> symbol within, corresponding to our Fortran subroutine.</p>
<div><p><sup>18</sup>&nbsp;You might need to use a WebAssembly aware version of this tool from Emscripten. If you’re using <a href="https://github.com/emscripten-core/emsdk">emsdk</a>, ensure that <code>.../emsdk/upstream/bin/</code> is on your <code>$PATH</code>.</p><p><sup>19</sup>&nbsp;Here I’m using Node v18, but I think anything newer than Node v16 should work. Emscripten is bundled with a version of Node, but I like using <a href="https://github.com/nvm-sh/nvm">nvm</a> to manage my Node installations.</p></div><p>Let’s continue compiling our C function for WebAssembly using Emscripten and running it using Node<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a>. We should see the same output as with our native binary.</p>
<div>
<p><span>[~/fortran]</span>emcc main.c foo.o -o main.js</p>
<p><span>[~/fortran]</span>node main.js</p>
<p>1 + 1 = 2</p>
</div>
</section>
<section id="interlude-calling-a-fortran-routine-from-javascript">
<h2 data-anchor-id="interlude-calling-a-fortran-routine-from-javascript">Interlude — Calling a Fortran routine from JavaScript</h2>
<p>In the previous section we used a C program to call Fortran code, but we don’t technically need to do that. If we tell Emscripten about the Fortran subroutine, we can call it directly from JavaScript without writing any C code.</p>
<p>First, let’s link our Fortran object with Emscripten, producing a script that loads our WebAssembly binary into memory but does not execute any routines. In addition to our symbol <code>_foo_</code>, we’ll also export <code>_malloc</code> and <code>_free</code> so that we can use them from JavaScript<a href="#fn20" id="fnref20" role="doc-noteref"><sup>20</sup></a>.</p>
<p><sup>20</sup>&nbsp;See the <a href="https://emscripten.org/docs/tools_reference/settings_reference.html">Emscripten documentation</a> for more details about <code>emcc</code> command line options. By the way, if you’ve not used Emscripten much before you might see extra <code>cache:INFO</code> lines emitted during various steps in this post. They are nothing to worry about and can be ignored.</p><div>
<p><span>[~/fortran]</span>emcc foo.o -sEXPORTED_FUNCTIONS=_foo_,_malloc,_free -o foo.js</p>
<p>cache:INFO: generating system asset: symbol_lists/ae47d07bfa3321ac4dde96bd87821b4aa93f9a19.json... (this will be cached in ".../emsdk/upstream/emscripten/cache/symbol_lists/ae47d07bfa3321ac4dde96bd87821b4aa93f9a19.json" for subsequent builds)</p>
<p>cache:INFO:  - ok</p>
<p><span>[~/fortran]</span>node foo.js</p>
<p><span>[~/fortran]</span></p>
</div>
<p>Notice that when we run the script <code>foo.js</code> directly… nothing happens.</p>
<p>Next, we’ll write a JavaScript file that loads <code>foo.js</code> and then calls our Fortran subroutine. We’ll need to allocate some memory to hold our integers <code>x</code>, <code>y</code> and <code>z</code> using the exported <code>_malloc()</code> function. We’ll also need to set our input arguments <code>x</code> and <code>y</code> to some integer values, and we can do that by setting values in the allocated WebAssembly memory through <code>Module.HEAPU32</code><a href="#fn21" id="fnref21" role="doc-noteref"><sup>21</sup></a>.</p>
<div id="cb4" data-filename="standalone.js"><pre><code><span id="cb4-1"><span>var</span> Module <span>=</span> <span>require</span>(<span>'./foo.js'</span>)<span>;</span></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span>setTimeout</span>(() <span>=&gt;</span> {</span>
<span id="cb4-4">  <span>const</span> x <span>=</span> Module<span>.</span><span>_malloc</span>(<span>4</span>)<span>;</span></span>
<span id="cb4-5">  <span>const</span> y <span>=</span> Module<span>.</span><span>_malloc</span>(<span>4</span>)<span>;</span></span>
<span id="cb4-6">  <span>const</span> z <span>=</span> Module<span>.</span><span>_malloc</span>(<span>4</span>)<span>;</span></span>
<span id="cb4-7">  Module<span>.</span><span>HEAPU32</span>[x <span>/</span> <span>4</span>] <span>=</span> <span>123</span><span>;</span></span>
<span id="cb4-8">  Module<span>.</span><span>HEAPU32</span>[y <span>/</span> <span>4</span>] <span>=</span> <span>456</span><span>;</span></span>
<span id="cb4-9"></span>
<span id="cb4-10">  Module<span>.</span><span>_foo_</span>(x<span>,</span> y<span>,</span> z)<span>;</span></span>
<span id="cb4-11"></span>
<span id="cb4-12">  <span>console</span><span>.</span><span>log</span>(<span>"x = "</span><span>,</span> Module<span>.</span><span>HEAP32</span>[x <span>/</span> <span>4</span>])<span>;</span></span>
<span id="cb4-13">  <span>console</span><span>.</span><span>log</span>(<span>"y = "</span><span>,</span> Module<span>.</span><span>HEAP32</span>[y <span>/</span> <span>4</span>])<span>;</span></span>
<span id="cb4-14">  <span>console</span><span>.</span><span>log</span>(<span>"x + y = "</span><span>,</span> Module<span>.</span><span>HEAP32</span>[z <span>/</span> <span>4</span>])<span>;</span></span>
<span id="cb4-15"></span>
<span id="cb4-16">  Module<span>.</span><span>_free</span>(x)<span>;</span></span>
<span id="cb4-17">  Module<span>.</span><span>_free</span>(y)<span>;</span></span>
<span id="cb4-18">  Module<span>.</span><span>_free</span>(z)<span>;</span></span>
<span id="cb4-19">}<span>,</span> <span>100</span>)<span>;</span></span></code></pre></div>
<div>
<p><span>[~/fortran]</span>node standalone.js</p>
<p>x =  123</p>
<p>y =  456</p>
<p>x + y =  579</p>
</div>
<p>You should also be able to run the resulting WebAssembly binary in a web browser. Remove the line <code><span>var</span> Module <span>=</span> <span>require</span>(<span>'./foo.js'</span>)<span>;</span></code> from <code>standalone.js</code>, and instead load the script <code>foo.js</code> in your HTML.</p>
<div id="cb5" data-filename="index.html"><pre><code><span id="cb5-1"><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span id="cb5-2">  <span>&lt;</span><span>head</span><span>&gt;</span></span>
<span id="cb5-3">    <span>&lt;</span><span>title</span><span>&gt;</span>Fortran Demo<span>&lt;/</span><span>title</span><span>&gt;</span></span>
<span id="cb5-4">  <span>&lt;/</span><span>head</span><span>&gt;</span></span>
<span id="cb5-5">  <span>&lt;</span><span>body</span><span>&gt;</span></span>
<span id="cb5-6">    <span>&lt;</span><span>script</span><span> src</span><span>=</span><span>"foo.js"</span><span>&gt;&lt;/</span><span>script</span><span>&gt;</span></span>
<span id="cb5-7">    <span>&lt;</span><span>script</span><span> src</span><span>=</span><span>"standalone.js"</span><span>&gt;&lt;/</span><span>script</span><span>&gt;</span></span>
<span id="cb5-8">  <span>&lt;/</span><span>body</span><span>&gt;</span></span>
<span id="cb5-9"><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></div>
<p>Spin up a local web server<a href="#fn22" id="fnref22" role="doc-noteref"><sup>22</sup></a>, visit the page, and the same output should be seen in the browser’s JavaScript console.</p>
<p><sup>22</sup>&nbsp;Something like <code>Rscript -e 'httpuv::runStaticServer()'</code> or <code>python3 -m http.server</code> should work well.</p></section>
<section id="the-fortran-runtime-library-a-journey-to-hello-world">
<h2 data-anchor-id="the-fortran-runtime-library-a-journey-to-hello-world">The Fortran runtime library: A journey to “Hello, World!”</h2>
<p>The ubiquitous “Hello, World!” test program is the usual way to introduce a programming language, but I didn’t introduce Fortran using such a program above. As you’ll see, that was for a good reason. Let’s see what happens when we try to build a “Hello, World!” subroutine in Fortran and call it from C. As before, we’ll compile the Fortran object using <code>flang-new</code> and use Emscripten to compile and link the C code.</p>
<div id="cb6" data-filename="hello.f08"><pre><code><span id="cb6-1"><span>SUBROUTINE</span> hello()</span>
<span id="cb6-2">    <span>IMPLICIT</span> <span>NONE</span></span>
<span id="cb6-3">    <span>PRINT</span> <span>*</span>, <span>"Hello, World!"</span></span>
<span id="cb6-4"><span>END</span></span></code></pre></div>
<div id="cb7" data-filename="hello.c"><pre><code><span id="cb7-1"><span>extern</span> <span>void</span> hello_<span>();</span></span>
<span id="cb7-2"></span>
<span id="cb7-3"><span>int</span> main<span>()</span> <span>{</span></span>
<span id="cb7-4">    hello_<span>();</span></span>
<span id="cb7-5">    <span>return</span> <span>0</span><span>;</span></span>
<span id="cb7-6"><span>}</span></span></code></pre></div>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new -c hello.f08 -o hello.o</p>
<p><span>[~/fortran]</span>emcc hello.c hello.o -o hello.js</p>
<p>wasm-ld: error: hello.o: undefined symbol: _FortranAioBeginExternalListOutput</p>
<p>wasm-ld: error: hello.o: undefined symbol: _FortranAioOutputAscii</p>
<p>wasm-ld: error: hello.o: undefined symbol: _FortranAioEndIoStatement</p>
<p>emcc: error: 'wasm-ld -o hello.wasm [...] --no-entry --stack-first --table-base=1' failed (returned 1)</p>
</div>
<p>The build failed due to some missing symbols. This is a consequence of a more general issue in that we have not yet compiled the LLVM Fortran runtime library for WebAssembly. There are a bunch of library symbols that we’re currently missing, including some functions that are required to print output!<a href="#fn23" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<p><sup>23</sup>&nbsp;See <a href="https://flang.llvm.org/docs/IORuntimeInternals.html">LLVM Flang’s documentation</a> for the nuts and bolts of the Fortran IO runtime library implementation.</p><p>Luckily, the runtime library is written in C++ as part of the LLVM source tree at <code>llvm-project/flang/runtime</code>. So, in principle, all we need to do is build the library using Emscripten’s <code>em++</code> compiler and then link to it whenever we’re using Fortran code in our WebAssembly program.</p>
<p>Here is a <code>Makefile</code> designed to make this step easy. Save it<a href="#fn24" id="fnref24" role="doc-noteref"><sup>24</sup></a> in the current directory and then run <code>make</code>. It should go ahead and use the version of Emscripten on your path to build a static Fortran runtime library at <code>build/flang/runtime/libFortranRuntime.a</code>.</p>
<p><sup>24</sup>&nbsp;Be sure to indent the rules in this file using tabs, not spaces.</p><div id="cb8" data-filename="Makefile"><pre><code><span id="cb8-1"><span>ROOT</span> <span>=</span><span> </span><span>$(</span><span>abspath</span><span> .</span><span>)</span></span>
<span id="cb8-2"><span>SOURCE</span> <span>=</span><span> </span><span>$(</span><span>ROOT</span><span>)</span><span>/llvm-project</span></span>
<span id="cb8-3"><span>BUILD</span> <span>=</span><span> </span><span>$(</span><span>ROOT</span><span>)</span><span>/build</span></span>
<span id="cb8-4"></span>
<span id="cb8-5"><span>RUNTIME_SOURCES</span> <span>:=</span><span> </span><span>$(</span><span>wildcard</span><span> </span><span>$(</span><span>SOURCE</span><span>)</span><span>/flang/runtime/*.cpp</span><span>)</span></span>
<span id="cb8-6"><span>RUNTIME_SOURCES</span> <span>+=</span><span> </span><span>$(</span><span>SOURCE</span><span>)</span><span>/flang/lib/Decimal/decimal-to-binary.cpp</span></span>
<span id="cb8-7"><span>RUNTIME_SOURCES</span> <span>+=</span><span> </span><span>$(</span><span>SOURCE</span><span>)</span><span>/flang/lib/Decimal/binary-to-decimal.cpp</span></span>
<span id="cb8-8"><span>RUNTIME_OBJECTS</span> <span>=</span><span> </span><span>$(</span><span>patsubst</span><span> </span><span>$(</span><span>SOURCE</span><span>)</span><span>/%</span><span>,</span><span>$(</span><span>BUILD</span><span>)</span><span>/%</span><span>,</span><span>$(</span><span>RUNTIME_SOURCES</span><span>:</span><span>.cpp</span><span>=</span><span>.o</span><span>))</span></span>
<span id="cb8-9"></span>
<span id="cb8-10"><span>RUNTIME_CXXFLAGS</span> <span>+=</span><span> -I</span><span>$(</span><span>BUILD</span><span>)</span><span>/include -I</span><span>$(</span><span>BUILD</span><span>)</span><span>/tools/flang/runtime</span></span>
<span id="cb8-11"><span>RUNTIME_CXXFLAGS</span> <span>+=</span><span> -I</span><span>$(</span><span>SOURCE</span><span>)</span><span>/flang/include -I</span><span>$(</span><span>SOURCE</span><span>)</span><span>/llvm/include</span></span>
<span id="cb8-12"><span>RUNTIME_CXXFLAGS</span> <span>+=</span><span> -DFLANG_LITTLE_ENDIAN</span></span>
<span id="cb8-13"><span>RUNTIME_CXXFLAGS</span> <span>+=</span><span> -fPIC -Wno-c++11-narrowing -fvisibility=hidden</span></span>
<span id="cb8-14"><span>RUNTIME_CXXFLAGS</span> <span>+=</span><span> -DFE_UNDERFLOW=0 -DFE_OVERFLOW=0 -DFE_INEXACT=0</span></span>
<span id="cb8-15"><span>RUNTIME_CXXFLAGS</span> <span>+=</span><span> -DFE_INVALID=0 -DFE_DIVBYZERO=0 -DFE_ALL_EXCEPT=0</span></span>
<span id="cb8-16"></span>
<span id="cb8-17"><span>$(BUILD)/flang/runtime/libFortranRuntime.a:</span><span> </span><span>$(</span><span>RUNTIME_OBJECTS</span><span>)</span></span>
<span id="cb8-18"><span>    </span><span>@</span><span>rm -f </span><span>$@</span></span>
<span id="cb8-19">    emar -rcs <span>$@</span> <span>$^</span></span>
<span id="cb8-20"></span>
<span id="cb8-21"><span>$(BUILD)%.o :</span><span> </span><span>$(</span><span>SOURCE</span><span>)</span><span>%.cpp</span></span>
<span id="cb8-22"><span>    </span><span>@</span><span>mkdir -p </span><span>$(</span><span>@D</span><span>)</span></span>
<span id="cb8-23">    em++ <span>$(</span><span>RUNTIME_CXXFLAGS</span><span>)</span> -o <span>$@</span> -c <span>$&lt;</span></span>
<span id="cb8-24"></span>
<span id="cb8-25"><span>.PHONY:</span><span> clean</span></span>
<span id="cb8-26"><span>clean:</span></span>
<span id="cb8-27"><span>    </span><span>@</span><span>rm </span><span>$(</span><span>RUNTIME_OBJECTS</span><span>)</span><span> </span><span>$(</span><span>BUILD</span><span>)</span><span>/flang/runtime/libFortranRuntime.a</span></span></code></pre></div>
<div>
<p><span>[~/fortran]</span>make</p>
<p>em++ .../ISO_Fortran_binding.o -c .../ISO_Fortran_binding.cpp</p>
<p>em++ .../allocatable.o -c .../allocatable.cpp</p>
<p>em++ .../array-constructor.o -c .../array-constructor.cpp</p>
<p>...</p>
<p>emar -rcs build/flang/runtime/libFortranRuntime.a .../binary-to-decimal.o</p>
<p><span>[~/fortran]</span>file build/flang/runtime/libFortranRuntime.a</p>
<p>build/flang/runtime/libFortranRuntime.a: current ar archive</p>
</div>
<p>Let’s try again, linking in our shiny new library as part of the Emscripten build step.</p>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new -c hello.f08 -o hello.o</p>
<p><span>[~/fortran]</span>emcc hello.c hello.o build/flang/runtime/libFortranRuntime.a -o hello.js</p>
<p>wasm-ld: <span>warning:</span> function signature mismatch: _FortranAioOutputAscii</p>
<p>&gt;&gt;&gt; defined as (i32, i32, i64) -&gt; i32 in hello.o</p>
<p>&gt;&gt;&gt; defined as (i32, i32, i32) -&gt; i32 in build/flang/runtime/libFortranRuntime.a(io-api.o)</p>
</div>
<p>Success? Not quite. A warning is issued, letting us know about a signature mismatch. Emscripten has compiled the symbol <code>_FortranAioOutputAscii</code> to take three <code>i32</code> arguments<a href="#fn25" id="fnref25" role="doc-noteref"><sup>25</sup></a>. However, <code>flang-new</code> has compiled <code>hello.f08</code> with the expectation that the symbol takes two <code>i32</code> arguments and a single <code>i64</code> argument.</p>
<div><p><sup>25</sup>&nbsp;This is <a href="https://llvm.org/docs/LangRef.html#integer-type">LLVM IR notation</a>, meaning an integer of size 32 bits.</p><p><sup>26</sup>&nbsp;This continues to crop up when compiling R packages for webR. Package authors or vendored libraries may have used tools such as <code>f2c</code> that declare a Fortran <code>SUBROUTINE</code> to return an <code>int</code>, while other libraries might declare a Fortran <code>SUBROUTINE</code> to return <code>void</code>. Who is right? I’m not sure, as I understand it early Fortran did not have a standard interface to C. Personally, I think returning <code>void</code> makes most sense.</p></div><p>This is unfortunate. Despite being emitted as just a warning, if you try running the emitted program using Node you will see that the problem is catastrophic. WebAssembly, unlike a lot of target systems, absolutely requires that symbols defined over multiple compilation units have consistent function signatures, both in argument and return type<a href="#fn26" id="fnref26" role="doc-noteref"><sup>26</sup></a>.</p>
<div>
<p><span>[~/fortran]</span>node hello.js</p>
<p>.../fortran/hello.js:128</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;throw ex;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;^</p>
<p>RuntimeError: unreachable</p>
<p>&nbsp;&nbsp;at wasm://wasm/001a0366:wasm-function[20]:0x15d9</p>
<p>&nbsp;&nbsp;at removeRunDependency (/Users/georgestagg/fortran/hello.js:630:7)</p>

<p>Node.js v18.18.0</p>
</div>
<p>Rather than going over the debugging process that eventually leads us to what is going on here, let me point you directly to the cause of the problem. Take a look at this comment from the LLVM source:</p>
<div>
<div>
<pre><strong>flang/include/flang/Optimizer/Builder/Runtime/RTBuilder.h</strong></pre>
</div>
<div id="cb9" data-filename="flang/include/flang/Optimizer/Builder/Runtime/RTBuilder.h"><pre><code><span id="cb9-1"><span>//===----------------------------------------------------------------------===//</span></span>
<span id="cb9-2"><span>// Type builder models</span></span>
<span id="cb9-3"><span>//===----------------------------------------------------------------------===//</span></span>
<span id="cb9-4"></span>
<span id="cb9-5"><span>// </span><span>TODO</span><span>: all usages of sizeof in this file assume build ==  host == target.</span></span>
<span id="cb9-6"><span>// This will need to be re-visited for cross compilation.</span></span>
<span id="cb9-7"></span>
<span id="cb9-8"><span>/// Return a function that returns the type signature model for the type `T`</span></span>
<span id="cb9-9"><span>/// when provided an MLIRContext*. This allows one to translate C(++) function</span></span>
<span id="cb9-10"><span>/// signatures from runtime header files to MLIR signatures into a static table</span></span>
<span id="cb9-11"><span>/// at compile-time.</span></span>
<span id="cb9-12"><span>///</span></span>
<span id="cb9-13"><span>/// For example, when `T` is `int`, return a function that returns the MLIR</span></span>
<span id="cb9-14"><span>/// standard type `i32` when `sizeof(int)` is 4.</span></span></code></pre></div>
</div>
<p>And therein lies the problem. For us, the host is different to the target, breaking assumptions in the LLVM source code. Surprisingly, this does not cause as much chaos as you might expect. From what I can tell, this machinery is used only to make the Fortran runtime library functions, written in C++, available to Fortran. There is a compile-time calculation using <code>sizeof()</code>, and since most of the sizes match anyway<a href="#fn27" id="fnref27" role="doc-noteref"><sup>27</sup></a> it mostly works fine.</p>
<p><sup>27</sup>&nbsp;The <a href="https://en.wikipedia.org/wiki/64-bit_computing#64-bit_data_models">C data model</a> for the host and target defines how many bits certain fundamental C types are represented with. The specific sizes can differ based on the hardware architecture and your OS.</p><p>Unfortunately for us, assuming you’re following along on a modern 64-bit Unix-like system such as Linux or macOS, the sizes don’t match for the <code>long</code> data type. The result of <code>sizeof(long)</code> on our compiler’s host platform is 8 bytes (<code>i64</code>), but for the target platform of <code>wasm32-unknown-emscripten</code> the returned value should be 4 bytes (<code>i32</code>).</p>
<p>When we compile the Fortran runtime library C++ code using Emscripten, things are fine. The resulting symbols are compiled with signatures such that <code>long</code> arguments are <code>i32</code>. However, when we compile our Fortran code with <code>flang-new</code> the external library symbols are declared such that <code>long</code> arguments are <code>i64</code>. This difference leads to the inconsistent function signature warning and runtime failure.</p>
<p>Why did using <code>PRINT()</code> in our “Hello, World!” program invoke a function that takes an argument of type <code>long</code>? Well, in some implementations of Fortran there are so-called “hidden” arguments that are added whenever you pass a Fortran <code>CHARACTER</code> type to a function or subroutine. These extra arguments pass in the length of the strings. In the Fortran runtime library the hidden arguments are declared with type <code>size_t</code> which, following a chain of <code>typedef</code>s, ends up being the same as <code>unsigned long</code>. This hidden implicit argument is the one with inconsistent size.</p>
</section>
<section id="hacking-around-the-issue">
<h2 data-anchor-id="hacking-around-the-issue">Hacking around the issue</h2>
<p>Unfortunately, I don’t know enough about the LLVM or Flang internals to implement a real solution to this problem. Ideally, <code>flang-new</code> would emit the correct use of <code>i32</code> or <code>i64</code> for the target architecture and data model when cross-compiling, no matter the host architecture the compiler is running on.</p>
<p>Since I can’t solve this today, let’s hack around it for now. We’ll build a version of <code>flang-new</code> with the size of a <code>long</code> hard-coded to what we need for <code>wasm32</code> and Emscripten. We’ll also make some changes so that calls to <code>malloc()</code> from Fortran are emitted with an <code>i32</code> argument<a href="#fn28" id="fnref28" role="doc-noteref"><sup>28</sup></a>.</p>
<p><sup>28</sup>&nbsp;This additionally fixes dynamic allocation with <code>ALLOCATE()</code>, a feature introduced in Fortran 90.</p><p>The required patches are again shown as a diff below. If you’re following along, save it as a file named <code>force-4-byte-values.diff</code> and apply it to the <code>llvm-project</code> directory using <code>git</code> or the <code>patch</code> utility. Finally, recompile <code>flang-new</code> once more.</p>
<div id="cb10" data-filename="force-4-byte-values.diff"><pre><code><span id="cb10-1"><span>diff --git a/flang/include/flang/Optimizer/Builder/Runtime/RTBuilder.h b/flang/include/flang/Optimizer/Builder/Runtime/RTBuilder.h</span></span>
<span id="cb10-2">index b3fe52f4b..c3c7326da 100644</span>
<span id="cb10-3"><span>--- a/flang/include/flang/Optimizer/Builder/Runtime/RTBuilder.h</span></span>
<span id="cb10-4"><span>+++ b/flang/include/flang/Optimizer/Builder/Runtime/RTBuilder.h</span></span>
<span id="cb10-5"><span>@@ -146,7 +146,7 @@ constexpr TypeBuilderFunc getModel&lt;void **&gt;() {</span></span>
<span id="cb10-6"> template &lt;&gt;</span>
<span id="cb10-7"> constexpr TypeBuilderFunc getModel&lt;long&gt;() {</span>
<span id="cb10-8">   return [](mlir::MLIRContext *context) -&gt; mlir::Type {</span>
<span id="cb10-9"><span>-    return mlir::IntegerType::get(context, 8 * sizeof(long));</span></span>
<span id="cb10-10"><span>+    return mlir::IntegerType::get(context, 8 * 4);</span></span>
<span id="cb10-11">   };</span>
<span id="cb10-12"> }</span>
<span id="cb10-13"> template &lt;&gt;</span>
<span id="cb10-14"><span>@@ -187,7 +187,7 @@ constexpr TypeBuilderFunc getModel&lt;long long *&gt;() {</span></span>
<span id="cb10-15"> template &lt;&gt;</span>
<span id="cb10-16"> constexpr TypeBuilderFunc getModel&lt;unsigned long&gt;() {</span>
<span id="cb10-17">   return [](mlir::MLIRContext *context) -&gt; mlir::Type {</span>
<span id="cb10-18"><span>-    return mlir::IntegerType::get(context, 8 * sizeof(unsigned long));</span></span>
<span id="cb10-19"><span>+    return mlir::IntegerType::get(context, 8 * 4);</span></span>
<span id="cb10-20">   };</span>
<span id="cb10-21"> }</span>
<span id="cb10-22"> template &lt;&gt;</span>
<span id="cb10-23"><span>diff --git a/flang/lib/Optimizer/CodeGen/CodeGen.cpp b/flang/lib/Optimizer/CodeGen/CodeGen.cpp</span></span>
<span id="cb10-24">index ba5946415..2931753a8 100644</span>
<span id="cb10-25"><span>--- a/flang/lib/Optimizer/CodeGen/CodeGen.cpp</span></span>
<span id="cb10-26"><span>+++ b/flang/lib/Optimizer/CodeGen/CodeGen.cpp</span></span>
<span id="cb10-27"><span>@@ -1225,7 +1225,7 @@ getMalloc(fir::AllocMemOp op, mlir::ConversionPatternRewriter &amp;rewriter) {</span></span>
<span id="cb10-28">     return mlir::SymbolRefAttr::get(userMalloc);</span>
<span id="cb10-29">   mlir::OpBuilder moduleBuilder(</span>
<span id="cb10-30">       op-&gt;getParentOfType&lt;mlir::ModuleOp&gt;().getBodyRegion());</span>
<span id="cb10-31"><span>-  auto indexType = mlir::IntegerType::get(op.getContext(), 64);</span></span>
<span id="cb10-32"><span>+  auto indexType = mlir::IntegerType::get(op.getContext(), 32);</span></span>
<span id="cb10-33">   auto mallocDecl = moduleBuilder.create&lt;mlir::LLVM::LLVMFuncOp&gt;(</span>
<span id="cb10-34">       op.getLoc(), mallocName,</span>
<span id="cb10-35">       mlir::LLVM::LLVMFunctionType::get(getLlvmPtrType(op.getContext()),</span>
<span id="cb10-36"><span>@@ -1281,6 +1281,7 @@ struct AllocMemOpConversion : public FIROpConversion&lt;fir::AllocMemOp&gt; {</span></span>
<span id="cb10-37">     mlir::Type heapTy = heap.getType();</span>
<span id="cb10-38">     mlir::Location loc = heap.getLoc();</span>
<span id="cb10-39">     auto ity = lowerTy().indexType();</span>
<span id="cb10-40"><span>+    auto i32ty = mlir::IntegerType::get(rewriter.getContext(), 32);</span></span>
<span id="cb10-41">     mlir::Type dataTy = fir::unwrapRefType(heapTy);</span>
<span id="cb10-42">     mlir::Type llvmObjectTy = convertObjectType(dataTy);</span>
<span id="cb10-43">     if (fir::isRecordWithTypeParameters(fir::unwrapSequenceType(dataTy)))</span>
<span id="cb10-44"><span>@@ -1291,9 +1292,10 @@ struct AllocMemOpConversion : public FIROpConversion&lt;fir::AllocMemOp&gt; {</span></span>
<span id="cb10-45">     for (mlir::Value opnd : adaptor.getOperands())</span>
<span id="cb10-46">       size = rewriter.create&lt;mlir::LLVM::MulOp&gt;(</span>
<span id="cb10-47">           loc, ity, size, integerCast(loc, rewriter, ity, opnd));</span>
<span id="cb10-48"><span>+    auto size_i32 = integerCast(loc, rewriter, i32ty, size);</span></span>
<span id="cb10-49">     heap-&gt;setAttr("callee", getMalloc(heap, rewriter));</span>
<span id="cb10-50">     rewriter.replaceOpWithNewOp&lt;mlir::LLVM::CallOp&gt;(</span>
<span id="cb10-51"><span>-        heap, ::getLlvmPtrType(heap.getContext()), size, heap-&gt;getAttrs());</span></span>
<span id="cb10-52"><span>+        heap, ::getLlvmPtrType(heap.getContext()), size_i32, heap-&gt;getAttrs());</span></span>
<span id="cb10-53">     return mlir::success();</span>
<span id="cb10-54">   }</span></code></pre></div>
<div>
<p><span>[~/fortran]</span>patch -p1 -d llvm-project &lt; force-4-byte-values.diff</p>
<p><span>[~/fortran]</span>cmake --build build</p>
<p>[0/60] Building CXX object tools/flang/lib/Lower/Runtime.cpp.o</p>
<p>...</p>
<p>[49/49] Generating ../../../../include/flang/ieee_arithmetic.f18.mod</p>
</div>
<p>Once LLVM has been rebuilt, try compiling our program once again. This time, it should compile without any warnings and successfully run under Node:</p>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new -c hello.f08 -o hello.o</p>
<p><span>[~/fortran]</span>emcc hello.c hello.o build/flang/runtime/libFortranRuntime.a -o hello.js</p><p><span>[~/fortran]</span>node hello.js</p>
<p> Hello, World!</p>
</div>
</section>
</section>
<section id="compiling-blas-and-lapack-for-webassembly">
<h2>Compiling BLAS and LAPACK for WebAssembly</h2>
<p>Now that we have a Fortran compiler that can output WebAssembly objects, let’s build some Fortran projects. <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a> (Basic Linear Algebra Subprograms) is a set of low-level routines that perform many common operations in linear algebra, including tools for matrix and vector multiplication. They are a de facto standard in numerical computation, with several different implementations of the BLAS routines available. Some implementations have been tuned for use on certain hardware<a href="#fn29" id="fnref29" role="doc-noteref"><sup>29</sup></a>, others have been well optimised on account of being around for a long time — the original BLAS routines were released in 1979!</p>
<p>Let’s grab a copy of the latest release of the so-called “reference implementation” of BLAS<a href="#fn30" id="fnref30" role="doc-noteref"><sup>30</sup></a>, written in Fortran 90, and compile it using the patched LLVM we built above.</p>
<div>
<p><span>[~/fortran]</span>curl -L https://www.netlib.org/blas/blas-3.12.0.tgz &gt; blas-3.12.0.tgz</p>
<p>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</p>
<p>                                 Dload  Upload   Total   Spent    Left  Speed</p>
<p>100  323k  100  323k    0     0   317k      0  0:00:01  0:00:01 --:--:--  317k</p>
<p><span>[~/fortran]</span>tar xzf blas-3.12.0.tgz</p>
</div>
<p>We’ll need to modify <code>BLAS-3.12.0/make.inc</code> to tell it about our version of <code>flang-new</code> and the Emscripten tools. Modify the following settings, leaving the other lines in that file as they are, then build BLAS using <code>make</code>.</p>
<div id="cb11" data-filename="BLAS-3.12.0/make.inc"><pre><code><span id="cb11-1"><span>FC</span> <span>=</span><span> ../build/bin/flang-new</span></span>
<span id="cb11-2"><span>FFLAGS</span> <span>=</span><span> -O2</span></span>
<span id="cb11-3"><span>FFLAGS_NOOPT</span> <span>=</span><span> -O0</span></span>
<span id="cb11-4"></span>
<span id="cb11-5"><span>AR</span> <span>=</span><span> emar</span></span>
<span id="cb11-6"><span>RANLIB</span> <span>=</span><span> emranlib</span></span></code></pre></div>
<div>
<p><span>[~/fortran]</span>cd BLAS-3.12.0</p>
<p><span>[~/fortran/BLAS-3.12.0]</span>make</p>
<p>../build/bin/flang-new -O2  -c -o isamax.o isamax.f</p>
<p>../build/bin/flang-new -O2  -c -o sasum.o sasum.f</p>
<p>...</p>
<p>emar cr blas_LINUX.a isamax.o ... xerbla_array.o</p>
<p>emranlib blas_LINUX.a</p>
<p><span>[~/fortran/BLAS-3.12.0]</span>cd ..</p>
<p><span>[~/fortran]</span></p>
</div>
<p>That went pretty well! Let’s try using it in a Fortran subroutine compiled for WebAssembly. For fun, we’ll try working with double precision complex numbers. We’ll use the BLAS level 2 routine <a href="https://netlib.org/lapack/explore-html-3.6.1/dc/dc1/group__complex16__blas__level2_gafaeb2abd9fffa7442b938dc384aeaf47.html#gafaeb2abd9fffa7442b938dc384aeaf47"><code>ZGEMV()</code></a>, which performs the matrix-vector operation</p>
<p><span>\[\begin{equation}
\mathbf{y} \leftarrow \alpha\mathbf{A}\mathbf{x} + \beta\mathbf{y},
\end{equation}\]</span></p>
<p>where <span>\(\alpha\)</span> and <span>\(\beta\)</span> are scalar constants, <span>\(\mathbf{x}\)</span> and <span>\(\mathbf{y}\)</span> are vectors, and <span>\(\mathbf{A}\)</span> is a matrix. Our Fortran routine will take in <code>alpha</code>, <code>beta</code>, <code>A</code>, <code>X</code>, and <code>Y</code>, with a fixed parameter <code>N</code> so that <span>\(\mathbf{A}\)</span> is a square matrix with three rows and columns. The result is written back into <span>\(\mathbf{y}\)</span>, so we declare that <code>Y</code> is of intent <code>INOUT</code>.</p>
<div id="cb12" data-filename="bar.f08"><pre><code><span id="cb12-1"><span>SUBROUTINE</span> bar(alpha, A, X, beta, Y)</span>
<span id="cb12-2">    <span>IMPLICIT</span> <span>NONE</span></span>
<span id="cb12-3">    <span>INTEGER</span>, <span>PARAMETER</span> <span>::</span> N <span>=</span> <span>3</span></span>
<span id="cb12-4">    <span>COMPLEX(KIND=8)</span>, <span>INTENT(IN)</span> <span>::</span> alpha, beta, A(N,N), X(N)</span>
<span id="cb12-5">    <span>COMPLEX(KIND=8)</span>, <span>INTENT(INOUT)</span> <span>::</span> Y(N)</span>
<span id="cb12-6">    <span>EXTERNAL</span> zgemv</span>
<span id="cb12-7"></span>
<span id="cb12-8">    <span>CALL</span> zgemv(<span>'N'</span>, N, N, alpha, A, N, X, <span>1</span>, beta, Y, <span>1</span>)</span>
<span id="cb12-9"><span>END</span></span></code></pre></div>
<p>Notice how with some BLAS routines, <code>CHARACTER</code> strings of length one control configuration settings. Here, we pass <code>'N'</code> as the first argument. It is one of the reasons we spent time and care above building a version of <code>flang-new</code> that can deal with <code>CHARACTER</code> arguments and their hidden implicit length arguments for the <code>wasm32</code> target.</p>
<p>Next, we’ll write a C program to create some complex variables, send them to Fortran and BLAS for processing, and print the result. This will let us know both that passing double precision complex numbers to Fortran and calling BLAS routines works as expected.</p>
<div id="cb13" data-filename="bar.c"><pre><code><span id="cb13-1"><span>#include </span><span>&lt;stdio.h&gt;</span></span>
<span id="cb13-2"><span>#include </span><span>&lt;complex.h&gt;</span></span>
<span id="cb13-3"></span>
<span id="cb13-4"><span>extern</span> <span>void</span> bar_<span>(</span><span>double</span> <span>complex</span><span>*,</span> <span>double</span> <span>complex</span><span>*,</span> <span>double</span> <span>complex</span><span>*,</span></span>
<span id="cb13-5">                 <span>double</span> <span>complex</span><span>*,</span> <span>double</span> <span>complex</span><span>*);</span></span>
<span id="cb13-6"></span>
<span id="cb13-7"><span>int</span> main<span>()</span> <span>{</span></span>
<span id="cb13-8">  <span>double</span> <span>complex</span> alpha <span>=</span> <span>1.</span><span>;</span></span>
<span id="cb13-9">  <span>double</span> <span>complex</span> beta <span>=</span> <span>2.</span><span>*</span>I<span>;</span></span>
<span id="cb13-10">  <span>double</span> <span>complex</span> A<span>[]</span> <span>=</span> <span>{</span></span>
<span id="cb13-11">    <span>1.</span><span>*</span>I<span>,</span> <span>4.</span>  <span>,</span> <span>5.</span><span>,</span></span>
<span id="cb13-12">    <span>7.</span>  <span>,</span> <span>2.</span><span>*</span>I<span>,</span> <span>6.</span><span>,</span></span>
<span id="cb13-13">    <span>8.</span>  <span>,</span> <span>9.</span>  <span>,</span> <span>3.</span><span>*</span>I</span>
<span id="cb13-14">  <span>};</span></span>
<span id="cb13-15">  <span>double</span> <span>complex</span> X<span>[]</span> <span>=</span> <span>{</span> <span>0.</span><span>,</span> <span>1.</span><span>,</span> <span>2.</span> <span>};</span></span>
<span id="cb13-16">  <span>double</span> <span>complex</span> Y<span>[]</span> <span>=</span> <span>{</span> <span>3.</span><span>,</span> <span>4.</span><span>,</span> <span>5.</span> <span>};</span></span>
<span id="cb13-17"></span>
<span id="cb13-18">  bar_<span>(&amp;</span>alpha<span>,</span> A<span>,</span> X<span>,</span> <span>&amp;</span>beta<span>,</span> Y<span>);</span></span>
<span id="cb13-19"></span>
<span id="cb13-20">  printf<span>(</span><span>"Y[0]: </span><span>%f</span><span> + </span><span>%f</span><span>i, Y[1]: </span><span>%f</span><span> + </span><span>%f</span><span>i, Y[2]: </span><span>%f</span><span> + </span><span>%f</span><span>i</span><span>\n</span><span>"</span><span>,</span></span>
<span id="cb13-21">         creal<span>(</span>Y<span>[</span><span>0</span><span>]),</span> cimag<span>(</span>Y<span>[</span><span>0</span><span>]),</span></span>
<span id="cb13-22">         creal<span>(</span>Y<span>[</span><span>1</span><span>]),</span> cimag<span>(</span>Y<span>[</span><span>1</span><span>]),</span></span>
<span id="cb13-23">         creal<span>(</span>Y<span>[</span><span>2</span><span>]),</span> cimag<span>(</span>Y<span>[</span><span>2</span><span>]));</span></span>
<span id="cb13-24">  <span>return</span> <span>0</span><span>;</span></span>
<span id="cb13-25"><span>}</span></span></code></pre></div>
<div>
<p><span>[~/fortran]</span>./build/bin/flang-new -c bar.f08 -o bar.o</p>
<p><span>[~/fortran]</span>emcc bar.c bar.o build/flang/runtime/libFortranRuntime.a BLAS-3.12.0/blas_LINUX.a -o bar.js</p>
<p><span>[~/fortran]</span>node bar.js</p>
<p>Y[0]: 23.000000 + 6.000000i, Y[1]: 18.000000 + 10.000000i, Y[2]: 6.000000 + 16.000000i</p>
</div>
<p>And there we have it: BLAS compiled from Fortran 90 sources and running under WebAssembly! To finish up, let’s confirm for ourselves that this output is correct<a href="#fn31" id="fnref31" role="doc-noteref"><sup>31</sup></a>,</p>
<p><sup>31</sup>&nbsp;Keeping in mind Fortran’s column-major array layout.</p><p><span>\[\begin{equation}
\begin{split}
\alpha\mathbf{A}\mathbf{x} &amp; + \beta\mathbf{y} \\[0.5em]
&amp; =
\begin{pmatrix}
  i &amp; 7 &amp; 8\\
  4 &amp; 2i &amp; 9\\
  5 &amp; 6 &amp; 3i
\end{pmatrix}
\cdot
\begin{pmatrix}
  0\\1\\2
\end{pmatrix}
+ 2i
\begin{pmatrix}
  3\\4\\5
\end{pmatrix}\\[0.5em]
&amp; =
\begin{pmatrix}
  23+6i\\18+10i\\6+16i
\end{pmatrix}.
\end{split}
\end{equation}\]</span></p>
<section id="mnist">
<h2 data-anchor-id="mnist">Example: A handwritten digit classifier</h2>
<p>The following demo uses a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multi-layer perceptron (MLP)</a> artificial neural network to classify hand-drawn digits. Try it out with your mouse or touchscreen! Just draw a digit from 0-9 in the box, and the classifier will try to label what digit you wrote. The relative probabilities according to the network are shown in a plot on the right.</p>

<p>It’s not a perfect model, but it works fairly well for me! The weights powering the model have been pre-trained using Python, but the classification is performed at runtime using JavaScript and WebAssembly, running in your browser right now.</p>
<p>With an MLP network, the classification process is essentially a repeated application of matrix-vector addition and multiplication. In this demo the heavy lifting is done by a single Fortran subroutine making use of the BLAS level 2 routine <a href="https://netlib.org/lapack/explore-html-3.6.1/d7/d15/group__double__blas__level2_gadd421a107a488d524859b4a64c1901a9.html"><code>DGEMV()</code></a>.</p>
</section>
<section id="building-lapack">
<h2 data-anchor-id="building-lapack">Building LAPACK</h2>
<p><a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">LAPACK</a> (Linear Algebra Package) is a software library for solving linear algebra problems numerically. It’s built upon BLAS and has similarly become a standard with many reimplementations designed for specific hardware or systems.</p>
<p>Let’s finish this post by also building the “reference implementation” of LAPACK<a href="#fn32" id="fnref32" role="doc-noteref"><sup>32</sup></a>.</p>
<p><sup>32</sup>&nbsp;Also available from <a href="https://www.netlib.org/lapack/">netlib</a>, released under a modified BSD licence.</p><div>
<p><span>[~/fortran]</span>curl -L https://github.com/Reference-LAPACK/lapack/archive/refs/tags/v3.12.0.tar.gz &gt; lapack-3.12.0.tgz</p>
<p>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</p>
<p>                                 Dload  Upload   Total   Spent    Left  Speed</p>
<p>100 7747k    0 7747k    0     0  4117k      0 --:--:--  0:00:01 --:--:-- 6655k</p>
<p><span>[~/fortran]</span>tar xzf lapack-3.12.0.tgz</p>
</div>
<p>Similar to BLAS, we need to modify some configuration options to let LAPACK know about Emscripten and <code>flang-new</code>. Copy the file <code>lapack-3.12.0/make.inc.example</code> to <code>lapack-3.12.0/make.inc</code>, then make the following modifications. Be sure to replace <code>[...]</code> with the full path to the build directory on your machine<a href="#fn33" id="fnref33" role="doc-noteref"><sup>33</sup></a>, and leave the other options in the file as they are.</p>
<p><sup>33</sup>&nbsp;A relative path doesn’t work here. Alternatively, simply set the option to read <code>flang-new</code> and make it available on your <code>$PATH</code>.</p><div id="cb14" data-filename="lapack-3.12.0/make.inc"><pre><code><span id="cb14-1"><span>FC</span> <span>=</span><span> [...]/build/bin/flang-new</span></span>
<span id="cb14-2"><span>FFLAGS</span> <span>=</span><span> -O2</span></span>
<span id="cb14-3"><span>FFLAGS_DRV</span> <span>=</span><span> </span><span>$(</span><span>FFLAGS</span><span>)</span></span>
<span id="cb14-4"><span>FFLAGS_NOOPT</span> <span>=</span><span> -O0</span></span>
<span id="cb14-5"></span>
<span id="cb14-6"><span>AR</span> <span>=</span><span> emar</span></span>
<span id="cb14-7"><span>RANLIB</span> <span>=</span><span> emranlib</span></span>
<span id="cb14-8"></span>
<span id="cb14-9"><span>TIMER</span> <span>=</span><span> INT_CPU_TIME</span></span></code></pre></div>
<p>Then, build LAPACK using the <code>make lib</code> command to create the WebAssembly static library <code>liblapack.a</code>.</p>
<div>
<p><span>[~/fortran]</span>cd lapack-3.12.0</p>
<p><span>[~/fortran/lapack-3.12.0]</span>make lib</p>
<p>make -C SRC</p>
<p>.../build/bin/flang-new -O2  -c -o sbdsvdx.o sbdsvdx.f</p>
<p>...</p>
<p>emar cr ../../libtmglib.a slatms.o ... dlarnd.o</p>
<p>emranlib ../../libtmglib.a</p>
<p><span>[~/fortran/lapack-3.12.0]</span>cd ..</p>
<p><span>[~/fortran]</span></p>
<p><span>[~/fortran]</span>file lapack-3.12.0/liblapack.a</p>
<p>lapack-3.12.0/liblapack.a: current ar archive</p>
</div>
<p>With this, LAPACK routines can be called in a similar way to the BLAS routine example in the previous section.</p>
</section>
<section id="example-polynomial-interpolation-with-linear-algebra">
<h2 data-anchor-id="example-polynomial-interpolation-with-linear-algebra">Example: Polynomial Interpolation with Linear Algebra</h2>
<p>The following demo finds interpolating polynomials for a set of points, demonstrating LAPACK routines running in your web browser.</p>
<p>Click the plot to add new points. An interpolating polynomial will be found to pass through all the points using <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde’s method</a><a href="#fn34" id="fnref34" role="doc-noteref"><sup>34</sup></a>. The linear algebra equation given by this method is then solved numerically in LAPACK using the <a href="https://netlib.org/lapack/explore-html-3.6.1/d7/d3b/group__double_g_esolve_ga1df516c81d3e902cca1fc79a7220b9cb.html#ga1df516c81d3e902cca1fc79a7220b9cb"><code>DGELS()</code></a> routine.</p>
<p><sup>34</sup>&nbsp; It is always possible to find an <span>\(n-1\)</span> degree polynomial containing <span>\(n\)</span> data points exactly. However, when <span>\(n\)</span> is large the polynomial fluctuates wildly between successive data points. This problem is known as <a href="https://en.wikipedia.org/wiki/Runge%27s_phenomenon">Runge’s phenomenon</a> and can be avoided by using <a href="https://en.wikipedia.org/wiki/Spline_interpolation">spline interpolation</a>.</p>
</section>
</section>
<section id="final-thoughts">
<h2>Final thoughts</h2>
<p>If you followed along at home, you now have a version of LLVM Flang that can compile modern Fortran code into WebAsembly objects. As I mentioned earlier, while it’s always been possible to build numerical algorithms for the web using JavaScript, to me the beauty of this approach is it allows you to use powerful Fortran tools and libraries that already exist, avoiding time and effort rewriting large collections of numerical routines for the web.</p>
<p>It would be wonderful if WebAssembly could be officially supported by the <code>flang-new</code> compiler. It would certainly lessen the burden of maintaining a fork of LLVM for <a href="https://github.com/r-wasm/webr">webR</a> and its R packages. However, without some help and support from a more experienced compiler developer, we will need to continue using a patched version for now.</p>
<p>If you’re familiar with LLVM Flang and know a better route to fixing the issues described above in a way that works for all targets with cross-compilation, feel free to <a href="mailto:george@stagg.phd">email me</a>. I’d be very interested to hear.</p>
<p>If you’d like to experiment with compiling Fortran code for WebAssembly but can’t or don’t want to build LLVM Flang from scratch, we provide a Docker container with <a href="https://github.com/r-wasm/flang-wasm/pkgs/container/flang-wasm">a binary version of our patched LLVM Flang</a> in the GitHub container registry.</p>


</section>


</main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Excellent succinct breakdown of the xz mess, from an OpenBSD developer (101 pts)]]></title>
            <link>https://marc.info/?l=openbsd-misc&amp;m=171227941117852&amp;w=2</link>
            <guid>39943062</guid>
            <pubDate>Fri, 05 Apr 2024 14:49:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marc.info/?l=openbsd-misc&#x26;m=171227941117852&#x26;w=2">https://marc.info/?l=openbsd-misc&#x26;m=171227941117852&#x26;w=2</a>, See on <a href="https://news.ycombinator.com/item?id=39943062">Hacker News</a></p>
<div id="readability-page-1" class="page">
<pre><b>[<a href="https://marc.info/?l=openbsd-misc&amp;m=171227533216409&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=openbsd-misc&amp;m=171228232018853&amp;w=2">next in list</a>] [<a href="https://marc.info/?l=openbsd-misc&amp;m=171227533216409&amp;w=2">prev in thread</a>] [<a href="https://marc.info/?l=openbsd-misc&amp;m=171228232018853&amp;w=2">next in thread</a>] </b>
<b><span size="+1">
List:       <a href="https://marc.info/?l=openbsd-misc&amp;r=1&amp;w=2">openbsd-misc</a>
Subject:    <a href="https://marc.info/?t=171179462900001&amp;r=1&amp;w=2">Re: lcamtuf on the recent xz debacle</a>
From:       <a href="https://marc.info/?a=95737891800004&amp;r=1&amp;w=2">Christian Weisgerber &lt;naddy () mips ! inka ! de&gt;</a>
Date:       <a href="https://marc.info/?l=openbsd-misc&amp;r=1&amp;w=2&amp;b=202404">2024-04-05 1:07:18</a>
Message-ID: <a href="https://marc.info/?i=Zg9Oxp-pPTkvcatH%20()%20lorvorc%20!%20mips%20!%20inka%20!%20de">Zg9Oxp-pPTkvcatH () lorvorc ! mips ! inka ! de</a></span>
[Download RAW <a href="https://marc.info/?l=openbsd-misc&amp;m=171227941117852&amp;q=mbox">message</a> or <a href="https://marc.info/?l=openbsd-misc&amp;m=171227941117852&amp;q=raw">body</a>]</b>

Katherine Mcmillan:

&gt; Just for clarity, does anyone know what "Unix-like operating systems"
&gt; would be affected by this?

None.  TLDR: The build process of the backdoor explicitly aborts
on platforms other than Linux x86-64.

As the maintainer of the archivers/xz port, I took a look at the
build stages of the malicious code, because I had already prepared
an update to 5.6.1 and run the code in question.

Two ostensible test files were committed to the xz repository
immediately before the 5.6.0 release and updated immediately before
5.6.1: bad-3-corrupt_lzma2.xz, which as the name suggests is a
malformed compressed file, and good-large_compressed.lzma, which
is a valid file and extracts to a mixture of easily compressible
repeated characters and uncompressible pseudo-random data.  By
themselves those files are completely harmless.

As is common practice, the xz repository only contains input files
like configure.ac and Makefile.am for the GNU autotools.  For the
release tarball, an autotools run generates the actual configure
script, Makefile.in, etc., so the result can be built with "./configure
&amp;&amp; make".

For the 5.6.0 and 5.6.1 release, the build-to-host.m4 macro package
that ships as part of GNU gettext was replaced by a modified version
that was copied into the release tarball and, importantly, was used
to generate a modified configure script.  Let's call this stage 0.

When you run the configure script, the stage 0 shell snippet is
executed.  The malicious code runs a pipe of commands that reads
the bad-3-corrupt_lzma2.xz file, swaps some byte values to turn it
into a valid file, extracts the file with xz (which must already
be installed), and feeds the content--let's call it stage 1--into
a shell.

In 5.6.1, the stage 1 script will abort right away if the operating
system doesn't identify as "Linux" with uname(1).  The script runs
another pipe of commands that decompresses good-large_compressed.lzma,
picks some chunks of the result, replaces some byte values to turn
it into a valid LZMA data stream, extracts the content--let's call
it stage 2--and feeds it into a shell.  The data manipulation in
stage 1 uses the head(1) command with the "-c" command flag, which
isn't available on OpenBSD.

In 5.6.1 there is another early attempt in the stage 2 script to
verify that the operating system is Linux, however the syntax is
broken so it doesn't actually do anything.  The stage 2 script runs
quite a number of tests to ensure that the environment in which it
executes is the one it expects: details of the directory tree,
details of the files generated by configure, that the platform is
x86-64 Linux, that the compiler is gcc and the linker GNU ld, that
the IFUNC feature is available, that is runs as part of a .deb or
.rpm package build.  If any single one of those tests fails, the
script aborts right away.  If everything checks out, stage 2 again
runs a series of data manipulation commands to extract from
good-large_compressed.lzma two object files and injects them into
the build to generate a manipulated liblzma.  Various checks that
stage 2 performs will fail on OpenBSD and again it relies on
"head -c" and now also on the GNU version of sed(1) to perform the
required data manipulations.

For the actual code inserted into liblzma on Linux x86-64, I have
to refer to the ongoing reverse engineering performed by the Linux
people.  It is my understanding that its code is triggered by an
IFUNC constructor during dynamic linking that checks that it is in
the address space of a /usr/sbin/sshd process and then proceeds to
redirect an RSA signature verification routine to its own malicious
code.  Liblzma ends up dynamically linked to sshd because of a
systemd-related extension added by many Linux packagers that pulls
in liblzma as an unrelated dependency.  The actual backdoor is
triggered by an SSH connection that authenticates with a certificate
that includes an RSA public key, part of which is a payload that
is checked against a fingerprint, then verified for a correct Ed448
signature with a key only the attacker knows, and then this content
is directly executed in a shell spawned by sshd for remote code
execution.

The build stage of the backdoor is well hidden.  The stage 0 shell
snippet looks at first glance like a plausible part of the poorly
readable autoconf/automake tooling.  The test files that hide the
further stages and actual backdoor code are unsuspicious by themselves.
5.6.1 added further tests to abort early on non-Linux platforms,
presumably so that nobody examining build problems would stumble
over anything suspicious.  I think the check for a .deb or .rpm
build is intended to inject the backdoor only during automated
package building, so people developing or debugging xz would not
accidentally discover it in the build directory.

I can identify four commits in the xz Git repository that are related
to the backdoor.  In chronological order:

2024-02-23 cf44e4b Tests: Add a few test files.
2024-03-09 82ecc53 liblzma: Fix false Valgrind error report with GCC.
2024-03-09 8c9b8b2 liblzma: Fix typos in crc32_fast.c and crc64_fast.c.
2024-03-09 6e63681 Tests: Update two test files.

cf44e4b and 6e63681 directly add and update hidden malicious code.
Aside from its documented change, 82ecc53 introduces an unmotivated
whitespace change...

-       return is_arch_extension_supported()   
+       return  is_arch_extension_supported()

... which is then reverted by 8c9b8b2.  The stage 2 script actually
relies on matching "return is_arch_extension_supported", so 82ecc53
breaks the backdoor injection and 8c9b8b2 restores it.  Maybe a
change intended for testing by the malware author accidentally
slipped in.

Another malicious commit, entirely unrelated to the backdoor, is

2024-02-26 328c52d Build: Fix Linux Landlock feature test in Autotools
and CMake builds.

This introduces a syntax error that breaks Landlock detection when
using CMake instead of the autotools build framework, so the Linux
sandboxing is disabled in this case.  The syntax error is a single
period '.' as the first character on an otherwise empty line of C
code.  That is designed so it will be easily missed.  It does not
plausibly pass for a typo because no typical editing glitch will
leave a '.' character there.

I'm not aware of any clearly malicious commit before 2024-02-23.

I'll conclude this brain dump by pointing out that much of the
emerging narrative about this backdoor that you can read all over
the net is based on idle speculation and selective interpretation
of facts.

-- 
Christian "naddy" Weisgerber                          naddy@mips.inka.de

<b>[<a href="https://marc.info/?l=openbsd-misc&amp;m=171227533216409&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=openbsd-misc&amp;m=171228232018853&amp;w=2">next in list</a>] [<a href="https://marc.info/?l=openbsd-misc&amp;m=171227533216409&amp;w=2">prev in thread</a>] [<a href="https://marc.info/?l=openbsd-misc&amp;m=171228232018853&amp;w=2">next in thread</a>] </b>
</pre>
  <br><center>
    <a href="https://marc.info/?q=configure">Configure</a> | 

    <a href="https://marc.info/?q=about">About</a> |
    <a href="https://marc.info/?q=news">News</a> |
    <a href="mailto:webguy@marc.info?subject=Add%20a%20list%20to%20MARC">Add&nbsp;a&nbsp;list</a> |
    Sponsored&nbsp;by&nbsp;<a href="http://www.korelogic.com/">KoreLogic</a>
</center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep sea mining could be worse for the climate than land ores (120 pts)]]></title>
            <link>https://planet-tracker.org/deep-sea-mining-could-be-worse-for-the-climate-than-land-ores/</link>
            <guid>39942902</guid>
            <pubDate>Fri, 05 Apr 2024 14:35:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planet-tracker.org/deep-sea-mining-could-be-worse-for-the-climate-than-land-ores/">https://planet-tracker.org/deep-sea-mining-could-be-worse-for-the-climate-than-land-ores/</a>, See on <a href="https://news.ycombinator.com/item?id=39942902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main" itemscope="itemscope" itemtype="https://schema.org/Blog"><div><div>


<section itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><p>Deep Sea Mining, Greenwashing
</p></section></div>






<div itemprop="blogPost" itemscope="itemscope" itemtype="https://schema.org/BlogPosting"><p><strong>London 5 December: </strong>As decisions over the future viability of deep sea mining are under review, Planet Tracker’s latest report ‘<a href="https://planet-tracker.org/wp-content/uploads/2023/12/The-Climate-Myth-of-Deep-Sea-Mining.pdf" data-wpel-link="internal">The Climate Myth of Deep Sea Mining</a>’ examines deep sea mining companies’ claim that mining polymetallic nodules could be a low carbon alternative to mining on land.</p>
<p>There are only three academic lifecycle assessment studies evaluating the ‘nodule-to-commodity’ climate impact of copper, cobalt, nickel and ferromanganese produced from polymetallic nodules and land ores.</p>
<p>Two of the three studies were funded by, and conducted in collaboration with, deep sea mining companies: one by The Metals Company (formerly DeepGreen Metals) and one by Global Sea Mineral Resources. The third paper, published by Fritz et al. in 2023, had no support from the deep sea mining industry.</p>
<p>Comparing these three studies underlines the huge variance in the estimated climate impacts of nodules and land ores. Depending on the choice of methodology, background data, software and assumptions and parameters used, deep sea nodules could have 28% higher or 76% lower climate impact than land ores. Planet Tracker believes that such a marked variation in results is not a good enough basis for making decisions about the future of deep sea mining.</p>
<p>Over 70% of greenhouse gas emissions for both land ores and deep sea nodules come from metallurgical processing, the energy- and fossil fuel-intensive processes that turn nodules and land ores into metal products. This means that climate impact is less dependent on whether metals come from the deep sea or from land and is more dependent on the properties of processing: mainly the sources of fuel and electricity used, process efficiency and processing technique.</p>
<p>Deep sea mining could have a long-term impact on ocean carbon stocks and sequestration services. Marine sediments are an important part of the global carbon cycle and make up the largest pool of sediment or soil carbon stocks in the world. While the rate of carbon sequestration is relatively slow at the deep seabed, ocean sediments can store carbon for millions of years if left undisturbed.</p>
<p>Planet Tracker estimates that deep sea mining vehicles kicking up sediment plumes could disturb 172.5 tonnes of carbon each year for every km<sup>2</sup> mined. For context, only 13.9 kg of carbon per km<sup>2</sup> is sequestered every year in the Clarion-Clipperton Zone where deep sea mining licenses have been granted.</p>
<p>Sediment plumes from deep sea mining machinery could impact carbon sequestration taking place in ocean waters by blocking sunlight and reducing photosynthesis. Carbon sequestration carried out by plants living on the nodules being mined would also be stopped completely. These impacts haven’t yet been calculated but considering that nodules take tens of millions of years to form, some of this damage would essentially be permanent.</p>
<p>“Through this research we really want to stress that deep sea mining could be a significant greenhouse gas emissions and contribute to climate change, which exposes financial institutions to policy, regulatory and reputational risks”, says Emma Amadi, Investment Analyst at Planet Tracker and one of the report’s authors.</p>
<p>“We want to encourage financial institutions to support a moratorium on deep sea mining&nbsp; and we’d also encourage them to support the decarbonization of terrestrial mining and metals production, for example supporting the electrification of mining with renewable energy sources and supporting the development and scaling of low-carbon metal processing routes like hydrometallurgy which reduce or remove the need for fossil fuels”.</p>
<p><strong>The Climate Myth of Deep Sea Mining can be downloaded &nbsp;</strong><a href="https://planet-tracker.org/wp-content/uploads/2023/12/The-Climate-Myth-of-Deep-Sea-Mining.pdf" data-wpel-link="internal"><strong>here</strong></a><strong>.</strong></p>
<p><strong>ENDS</strong></p>


<p><strong>For more information please contact:</strong></p>
<p>Dominic Lyle, Planet Tracker | t: +44 7484 654 941</p>
<p><a href="mailto:dominic@planet-tracker.org">dominic@planet-tracker.org</a></p>
</div>
</div></main><!-- close content main element --> <!-- section close by builder template -->		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[M 4.7 Earthquake 7 km N of Whitehouse Station, New Jersey – USGS (443 pts)]]></title>
            <link>https://earthquake.usgs.gov/earthquakes/eventpage/at00sbh3yv/executive</link>
            <guid>39942880</guid>
            <pubDate>Fri, 05 Apr 2024 14:34:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthquake.usgs.gov/earthquakes/eventpage/at00sbh3yv/executive">https://earthquake.usgs.gov/earthquakes/eventpage/at00sbh3yv/executive</a>, See on <a href="https://news.ycombinator.com/item?id=39942880">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            The Earthquake Event Page application supports most recent browsers,
            <a href="https://angular.io/guide/browser-support">view supported browsers</a>. Or, try our
            <a href="https://earthquake.usgs.gov/earthquakes/feed/">Real-time Notifications, Feeds, and Web Services</a>.
          </p>
        </div></div>]]></description>
        </item>
    </channel>
</rss>