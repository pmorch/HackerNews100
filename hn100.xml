<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 08 Jan 2024 19:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Today we celebrate by announcing that Elixir is a gradually typed language (141 pts)]]></title>
            <link>https://twitter.com/josevalim/status/1744395345872683471</link>
            <guid>38914407</guid>
            <pubDate>Mon, 08 Jan 2024 16:29:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/josevalim/status/1744395345872683471">https://twitter.com/josevalim/status/1744395345872683471</a>, See on <a href="https://news.ycombinator.com/item?id=38914407">Hacker News</a></p>
Couldn't get https://twitter.com/josevalim/status/1744395345872683471: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Tuna species popular in sashimi and poke bowls in sharp decline (131 pts)]]></title>
            <link>https://phys.org/news/2024-01-tuna-species-popular-sashimi-bowls.html</link>
            <guid>38913649</guid>
            <pubDate>Mon, 08 Jan 2024 15:47:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-01-tuna-species-popular-sashimi-bowls.html">https://phys.org/news/2024-01-tuna-species-popular-sashimi-bowls.html</a>, See on <a href="https://news.ycombinator.com/item?id=38913649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/tuna-species-popular-i.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/tuna-species-popular-i.jpg" data-sub-html="Graphical abstract. Credit: <i>Ocean &amp; Coastal Management</i> (2023). DOI: 10.1016/j.ocecoaman.2023.106902">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/tuna-species-popular-i.jpg" alt="Tuna species popular in sashimi and poke bowls in sharp decline in the Indian Ocean" title="Graphical abstract. Credit: Ocean &amp; Coastal Management (2023). DOI: 10.1016/j.ocecoaman.2023.106902" width="800" height="392">
             <figcaption>
                Graphical abstract. Credit: <i>Ocean &amp; Coastal Management</i> (2023). DOI: 10.1016/j.ocecoaman.2023.106902
            </figcaption>        </figure>
    </div><p>The yellowfin tuna commonly enjoyed in sashimi, poke bowls and salad sandwiches may soon disappear from these dishes if current over-exploitation rates remain unchanged, especially in the Indian Ocean.</p>


										      
																																	<p>A <a href="https://doi.org/10.1016/j.ocecoaman.2023.106902">paper</a>, titled "Multiple lines of evidence highlight the dire straits of yellowfin <a href="https://phys.org/tags/tuna/" rel="tag">tuna</a> in the Indian Ocean," and published in <i>Ocean and Coastal Management</i> shows that since industrial exploitation started in 1950, the global biomass—the weight of a given <a href="https://phys.org/tags/population/" rel="tag">population</a> in the water—of yellowfin tuna has decreased, on average, by 54% across the four populations managed by tuna Regional Fisheries Management Organizations (RFMO). In the Indian Ocean, yellowfin tuna biomass has experienced a 70% decline in the past 70 years.</p>
<p>"If we look at more recent years, we can see that global yellowfin tuna populations continue to struggle. Biomass continues to decline everywhere except for stabilizing trends in the Western Pacific Ocean, prompted by management interventions," said Kristina Heidrich, lead author of the study and Ph.D. candidate with the Sea Around Us—Indian Ocean at the University of Western Australia (UWA).</p>
<p>"In most places, extractions have regularly surpassed the maximum sustainable yield or MSY limit, which is the level that would allow for the highest possible catches to be sustained over time, given that <a href="https://phys.org/tags/environmental+conditions/" rel="tag">environmental conditions</a> don't change much," added Dirk Zeller, co-author of the study and director of the Sea Around Us—Indian Ocean.</p>

																																						
																																			<p>To strengthen their evaluations, Heidrich and her co-authors used multiple methodologies. Since yellowfin tuna is normally assessed by RFMOs, they used the organizations' time series and assessment results of yellowfin tuna biomass to estimate its annual fluctuations from 1950 to 2020.</p>
<p>In parallel, they applied the CMSY++ approach, which relies mainly on time series of fisheries catches to assess the status of fish stocks. Finally, they analyzed 955 records of yellowfin tuna obtained from fisheries-independent sampling with Baited Remote Underwater Video Systems (BRUVS), which record biological and <a href="https://phys.org/tags/ecological+data/" rel="tag">ecological data</a> such as a species' size, biomass and how abundant it is in an area.</p>
<p>"The data collected with BRUVS provided a more holistic and fisheries-independent picture of the pelagic community and the status of the populations, which can complement fisheries-dependent data and analyses," said Jessica Meeuwig, co-author of the paper and director of UWA's Marine Futures Lab.</p>
<p>"These fisheries-independent BRUVS data suggest that, since 2014, yellowfin tuna in the Indian Ocean are the least common, least abundant, have the lowest biomass, and are the smallest yellowfin tuna in the existing dataset."</p>
<p>The CMSY++ approach, on the other hand, showed that even though yellowfin tuna exploitation rates slightly exceeded the MSY limit in the past decade in the eastern and western Pacific Ocean and in the Atlantic Ocean, these populations are not currently undergoing overfishing. That's not the case for the Indian Ocean, where overfishing is ongoing.</p>

																																			<p>"Beyond yellowfin tuna fisheries contributing more than US$16 billion to the <a href="https://phys.org/tags/global+economy/" rel="tag">global economy</a> yearly, the species is an <a href="https://phys.org/tags/apex+predator/" rel="tag">apex predator</a> that plays a critical role in the functioning, productivity and overall health of marine ecosystems," said Daniel Pauly, co-author of the study and principal investigator of the Sea Around Us initiative at the University of British Columbia.</p>
<p>"The risk of population collapse is high if current management does not adapt. Stringent management constraints must be implemented to reduce overall fishing capacity, rebuild overfished populations, and reduce the <a href="https://phys.org/tags/collateral+damage/" rel="tag">collateral damage</a> these fisheries cause to other species such as sharks."</p>
<p>The researchers note that, by using multiple lines of evidence that cross-validate each other, such as the ones they employed, management organizations can improve the confidence, transparency and accuracy of the information that guides their decisions. They also propose stricter management measures such as a reduction of fishing capacity, ensuring that the MSY limit is observed and implementing effective catch limits.</p>
<p>"In the Indian Ocean in particular, a catch reduction of 30% from 2020 levels is urgent to halt and reverse the decline in <a href="https://phys.org/tags/yellowfin+tuna/" rel="tag">yellowfin tuna</a> population," Heidrich said.</p>

																																																					
																				<p><strong>More information:</strong>
												Kristina N. Heidrich et al, Multiple lines of evidence highlight the dire straits of yellowfin tuna in the Indian Ocean., <i>Ocean &amp; Coastal Management</i> (2023). <a data-doi="1" href="https://dx.doi.org/10.1016/j.ocecoaman.2023.106902" target="_blank">DOI: 10.1016/j.ocecoaman.2023.106902</a>
																						
																					</p>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Tuna species popular in sashimi and poke bowls in sharp decline in the Indian Ocean (2024, January 8)
												retrieved 8 January 2024
												from https://phys.org/news/2024-01-tuna-species-popular-sashimi-bowls.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dive: A tool for exploring a Docker image, layer contents and more (187 pts)]]></title>
            <link>https://github.com/wagoodman/dive</link>
            <guid>38913425</guid>
            <pubDate>Mon, 08 Jan 2024 15:35:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wagoodman/dive">https://github.com/wagoodman/dive</a>, See on <a href="https://news.ycombinator.com/item?id=38913425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:wagoodman/dive" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="cxRwDbVdgWeDDH77VkTVoQ6BSPl9Ky3rH886rDLkVuAvWvt4Um26BrrW4zIWGQL0AVHTSnhpV5nrwEU8kXS7Rg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="wagoodman/dive" data-current-org="" data-current-owner="wagoodman" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=yNrgLGbrtexIUJ7xuSyHxN%2BAZK7KpD7UaHzhYYFyuXDOf8MH5J412Vb%2F9pJ3JzFh9%2BV1N%2FFPFBTKWqL564KsthdHalZvbqfAHr31BtIF26o73qbU5lrrZyVhOHuvUwiPzFaUyWteeyLA0%2FfQxbOhZQLpAsZ6VW4ug5vgTXRwqzuP3pl4dqxuy2xLpASvEkBneCoUSuqBRhqpjzSvGpUdEUXIcSEyZgeVKgn%2FUf7AywBzOovZy3%2FSoo1e%2BmGATRq9W2qDucsq1nFDjQd02vaKzGaKy3RMZIy1oy9ArHS1ajdXSN9LL3uekbBWT1CJNNSNL5Kt6DSSSCQqV9wy2qgLRazRmXFiBXrYNMd7AE6wHw96I%2B%2BaaUoQcrOYWhmpMBfxEU7qK5qxA%2FwzRS%2Fg6FYOhzU7VbuQHUVU472UC6TUN43smSufQ5nxl9m0PSMuLYkLXEl6BQGVVAzPKowiTJBJc9mXo2KYcPURpY6s%2F8pvqP%2BpzdjpUJOGt1sOYDB1OcMOYU4xcVJeUrnYDA%3D%3D--npPi04dFXyg8%2BQ3b--PFlS002KpJtsJVg9%2Bt%2Fetg%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=wagoodman%2Fdive" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/wagoodman/dive&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e444211f9c374bb744cf5b1bdd70e4e14ac32368709b0206fd52796af07f44e9" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jeff Lawson Steps Down as CEO of Twilio (260 pts)]]></title>
            <link>https://investors.twilio.com/news/news-details/2024/Twilio-Announces-CEO-Transition/default.aspx</link>
            <guid>38912497</guid>
            <pubDate>Mon, 08 Jan 2024 14:37:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://investors.twilio.com/news/news-details/2024/Twilio-Announces-CEO-Transition/default.aspx">https://investors.twilio.com/news/news-details/2024/Twilio-Announces-CEO-Transition/default.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=38912497">Hacker News</a></p>
Couldn't get https://investors.twilio.com/news/news-details/2024/Twilio-Announces-CEO-Transition/default.aspx: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Vision Pro available in the U.S. on February 2 (260 pts)]]></title>
            <link>https://www.apple.com/newsroom/2024/01/apple-vision-pro-available-in-the-us-on-february-2/</link>
            <guid>38912032</guid>
            <pubDate>Mon, 08 Jan 2024 14:03:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2024/01/apple-vision-pro-available-in-the-us-on-february-2/">https://www.apple.com/newsroom/2024/01/apple-vision-pro-available-in-the-us-on-february-2/</a>, See on <a href="https://news.ycombinator.com/item?id=38912032">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>

	

<section>
<article data-analytics-activitymap-region-id="article">






    
    
    











    <div>
        

        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>January 8, 2024</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple Vision Pro available in the U.S. on February&nbsp;2
    

                    </h2>
                
            </div>

        <div>
                
                
                    The era of spatial computing is here — pre-orders begin Friday, January&nbsp;19
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, A woman wearing Apple Vision Pro.">
        <div>
             
              
              <div>
                Apple Vision Pro will be available beginning Friday, February 2, at all U.S. Apple Store locations and the U.S. Apple Store online.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/01/apple-vision-pro-available-in-the-us-on-february-2/article/Apple-Vision-Pro-availability-hero.zip" download="" data-analytics-title="Download image" aria-label="Download media, A woman wearing Apple Vision Pro."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span></strong>&nbsp;Apple today announced Apple Vision Pro will be available beginning Friday, February 2, at all U.S. Apple Store locations and the U.S. Apple Store online. Vision Pro is a revolutionary spatial computer that transforms how people work, collaborate, connect, relive memories, and enjoy entertainment. Vision Pro seamlessly blends digital content with the physical world and unlocks powerful spatial experiences in visionOS, controlled by the most natural and intuitive inputs possible — a user’s eyes, hands, and voice. An all-new App Store provides users with access to more than 1 million compatible apps across iOS and iPadOS, as well as new experiences that take advantage of the unique capabilities of Vision Pro. Pre-orders for Apple Vision Pro begin Friday, January 19, at 5 a.m. PST.
</div>
                 
             
                 <div>“The era of spatial computing has arrived,” said Tim Cook, Apple’s CEO. “Apple Vision Pro is the most advanced consumer electronics device ever created. Its revolutionary and magical user interface will redefine how we connect, create, and explore.”
</div>
                 
             
                 <h2><strong>A Revolutionary Operating System and User Interface</strong>
</h2>
                 
             
                 <div>Apple Vision Pro is powered by visionOS, which is built on the foundation of decades of engineering innovation in macOS, iOS, and iPadOS. visionOS delivers powerful spatial experiences, unlocking new opportunities at work and at home. Featuring a brand-new three-dimensional user interface and input system controlled entirely by a user’s eyes, hands, and voice, navigation feels magical. Intuitive gestures allow users to interact with apps by simply looking at them, tapping their fingers to select, flicking their wrist to scroll, or using a virtual keyboard or dictation to type. With Siri, users can quickly open or close apps, play media, and more.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>visionOS features a brand-new three-dimensional user interface controlled entirely by a user’s eyes, hands, and voice.</div>
        
            <a aria-label="Download video: visionOS on Apple Vision Pro" data-analytics-title="Download video - visionOS on Apple Vision Pro" download="" href="https://www.apple.com/newsroom/videos/spatial-video-with-apple-vision-pro-available/downloads/Apple-Vision-Pro-availability-visionOS.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>Users can also immerse themselves in Environments — dynamic, beautiful landscapes like Haleakalā, Joshua Tree, and Yosemite national parks, and even the surface of the moon — to help them focus or reduce clutter in busy spaces. With Environments, a user’s world can grow beyond the dimensions of a physical room. With a twist of the Digital Crown, users can control how present or immersed they are in an environment.
</div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>With Environments, a user’s world can grow beyond the dimensions of a physical room, using the Digital Crown to control how present or immersed they are.</div>
        
            <a aria-label="Download video: Apple Vision Pro Digital Crown" data-analytics-title="Download video - Apple Vision Pro Digital Crown" download="" href="https://www.apple.com/newsroom/videos/vision-pro-availability-environments/downloads/Apple-Vision-Pro-availability-Environments.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Extraordinary Experiences</strong>
</h2>
                 
             
                 <div>Apple Vision Pro brings a new dimension to powerful, personal computing by changing the way users interact with their apps. The three-dimensional interface frees apps from the boundaries of a display so they can appear side by side at any scale, providing the ultimate workspace and creating an infinite canvas for multitasking and collaborating.
</div>
                 
             
                 <div>Since visionOS leverages existing developer frameworks, more than 1 million familiar apps across iOS and iPadOS are available on Apple Vision Pro and automatically work with the new input system. Vision Pro also has an all-new App Store where users can find apps that deliver spatial computing experiences unlike any other platform. Apps can be arranged anywhere and scaled to the perfect size, all while allowing the user to stay present in their space.
</div>
                 
             
                 <div><ul>
<li><strong>An infinite canvas for productivity</strong>:<strong> </strong>With key productivity and collaboration apps like Fantastical, Freeform, JigSpace, apps from Microsoft 365, and Slack, Apple Vision Pro is an ideal productivity tool for everyday tasks. Apps can appear side by side at any scale for incredible multitasking, and with support for Magic Keyboard and Magic Trackpad, users can create the perfect workspace. With Mac Virtual Display, users can even bring the powerful capabilities of their Mac into Vision Pro, creating an enormous, private, and portable 4K display, ideal for pro workflows.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>The ultimate entertainment experience</strong>: Apple Vision Pro features ultra-high-resolution displays that deliver more pixels than a 4K TV for each eye, enabling users to watch movies and TV shows from Apple TV+, Disney+, Max, and other services on a screen that feels 100 feet wide with support for HDR content. Within the Apple TV app, users can access more than 150 3D titles with incredible depth wherever they are. Vision Pro also introduces Apple Immersive Video, a remarkable new entertainment format pioneered by Apple that puts users inside the action with&nbsp;180-degree,&nbsp;three-dimensional 8K recordings captured with Spatial&nbsp;Audio. Users can also enjoy new interactive experiences like Encounter Dinosaurs.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>New gaming experiences</strong>: Players can access games on the App Store, including more than 250 titles on Apple Arcade. Hit games like NBA 2K24 Arcade Edition and Sonic Dream Team can be played on a screen as large as they want with incredible audio and support for popular game controllers. New spatial games, including Game Room, What the Golf?, and Super Fruit Ninja, take advantage of the powerful capabilities of Apple Vision Pro to transform the space around players, offering unique and engaging gameplay experiences.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>An all-new App Store on Apple Vision Pro features more than 1 million apps, including new experiences that take advantage of the unique capabilities of Vision Pro.</div>
        
            <a aria-label="Download video: App Store for Apple Vision Pro" data-analytics-title="Download video - App Store for Apple Vision Pro" download="" href="https://www.apple.com/newsroom/videos/vision-pro-visionos/downloads/Apple-Vision-Pro-availability-visionOS-home.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Memories Brought to Life</strong>
</h2>
                 
             
                 <div>Apple Vision Pro enables users to capture and relive their favorite memories in entirely new ways. Spatial photos and videos transport users back to a special moment in time, and Spatial Audio makes the experience incredibly immersive. When users are on the go, they can capture spatial video on their iPhone 15 Pro or iPhone 15 Pro Max and relive them on Vision Pro. Users can also view all their photos and videos at a life-size scale with brilliant color and spectacular detail, including Panoramas that expand and wrap around the user, making them feel like they are right where it was taken.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>Spatial photos and videos transport users back to a special moment in time, and Spatial Audio makes the experience incredibly immersive.</div>
        
            <a aria-label="Download video: Apple Vision Pro Spatial Video" data-analytics-title="Download video - Apple Vision Pro Spatial Video" download="" href="https://www.apple.com/newsroom/videos/vision-pro-spatial-video/downloads/Apple-Vision-Pro-availability-Spatial-video.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>FaceTime Becomes Spatial</strong>
</h2>
                 
             
                 <div>FaceTime on Apple Vision Pro takes advantage of the space around the user so that everyone on a call appears life-size, while Spatial Audio makes it sound like each person’s voice comes from the location of their tile. If a user is wearing Vision Pro while on FaceTime, they appear as their Persona, while others joining from a Mac, iPad, or iPhone will appear in a tile.
</div>
                 
             
                 <div>Persona is an authentic spatial representation of an Apple Vision Pro user that enables others on a call to see their facial expressions and hand movements — all in real time.<sup>1</sup> Using machine learning techniques, a Persona can be created in just minutes using Vision Pro. Personas also work in third-party videoconferencing apps including Zoom, Cisco Webex, and Microsoft Teams.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>Persona is an authentic spatial representation of an Apple Vision Pro user that enables others on a call to see the user’s facial expressions and hand movements in real time.</div>
        
            <a aria-label="Download video: Persona on Apple Vision Pro" data-analytics-title="Download video - Persona on Apple Vision Pro" download="" href="https://www.apple.com/newsroom/videos/vision-pro-persona-creation/downloads/Apple-Vision-Pro-availability-Persona-creation.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Breakthrough Design</strong>
</h2>
                 
             
                 <div>Apple Vision Pro builds on Apple innovation and experience designing high-performance products like Mac, iPhone, and wearables like Apple Watch, culminating in the most advanced personal electronics device ever. An astonishing amount of technology is packed into a beautiful, compact design that utilizes the most advanced materials possible to achieve ambitious goals for performance, mobility, and wearability.
</div>
                 
             
                 <div>Apple Vision Pro is designed as a modular system so users can personalize their fit. A singular piece of three-dimensionally formed, laminated glass gently curves around the user’s face and flows into the custom aluminum alloy frame. The Light Seal is made of a soft textile and comes in a range of shapes and sizes, flexing to conform to a user’s face for a precise fit. Flexible straps ensure audio remains close to the user’s ears, while the included Solo Knit Band and Dual Loop Band allow users to find the optimal fit for them. For those with vision correction needs, ZEISS Optical Inserts are available with a prescription or as readers that magnetically attach to Vision Pro, allowing users to take full advantage of the display’s incredible sharpness and clarity.<sup>2</sup>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="apple-vision-pro-design">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-8f9765a03f02c167f34a35b9be9cbfe2" href="#gallery-8f9765a03f02c167f34a35b9be9cbfe2" data-ac-gallery-trigger="gallery-8f9765a03f02c167f34a35b9be9cbfe2"><span>A side profile showing the modular components of Apple Vision Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-4fae60943d13b7e159832bb455614472" href="#gallery-4fae60943d13b7e159832bb455614472" data-ac-gallery-trigger="gallery-4fae60943d13b7e159832bb455614472"><span>The Dual Loop Band on Apple Vision Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-6d0bad6966f692309c103b01bdf6d067" href="#gallery-6d0bad6966f692309c103b01bdf6d067" data-ac-gallery-trigger="gallery-6d0bad6966f692309c103b01bdf6d067"><span>The Light Seal on Apple Vision Pro.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-8f9765a03f02c167f34a35b9be9cbfe2" aria-labelledby="gallery-dotnav-8f9765a03f02c167f34a35b9be9cbfe2" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:modular-system">
                                
                                <div>
                                    <div>Apple Vision Pro utilizes the most advanced materials possible in a beautiful, compact design.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/01/apple-vision-pro-available-in-the-us-on-february-2/article/Apple-Vision-Pro-availability-profile.zip" download="" data-analytics-title="Download image" aria-label="Download media, A side profile showing the modular components of Apple Vision Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-4fae60943d13b7e159832bb455614472" aria-labelledby="gallery-dotnav-4fae60943d13b7e159832bb455614472" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:dual-loop-band">
                                
                                <div>
                                    <div>Flexible straps ensure audio remains close to the user’s ears, while the included Solo Knit Band and Dual Loop Band allow users to find the optimal fit for them.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/01/apple-vision-pro-available-in-the-us-on-february-2/article/Apple-Vision-Pro-availability-back-angle.zip" download="" data-analytics-title="Download image" aria-label="Download media, The Dual Loop Band on Apple Vision Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-6d0bad6966f692309c103b01bdf6d067" aria-labelledby="gallery-dotnav-6d0bad6966f692309c103b01bdf6d067" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:light-seal">
                                
                                <div>
                                    <div>The Light Seal is made of a soft textile and comes in a range of shapes and sizes, flexing to conform to a user’s face for a precise fit.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/01/apple-vision-pro-available-in-the-us-on-february-2/article/Apple-Vision-Pro-availability-Digital-Crown.zip" download="" data-analytics-title="Download image" aria-label="Download media, The Light Seal on Apple Vision Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Unrivaled Innovation</strong>
</h2>
                 
             
                 <div>Apple Vision Pro is designed to deliver phenomenal compute performance in a compact wearable form factor. Featuring a breakthrough ultra-high-resolution display system built on top of Apple silicon, Vision Pro uses micro-OLED technology to pack 23 million pixels into two displays, each the size of a postage stamp, with wide color and high dynamic range. This technological breakthrough, combined with custom lenses that enable incredible sharpness and clarity, and advanced Spatial Audio, delivers jaw-dropping experiences.&nbsp;
</div>
                 
             
                 <div>Apple Vision Pro also features a high-performance eye tracking system that uses high-speed cameras and a ring of LEDs that project invisible light patterns onto the user’s eyes for responsive, intuitive input. And to help the user stay connected to the people around them, Apple designed a groundbreaking new feature called EyeSight. When a person approaches someone wearing Vision Pro, the device looks transparent — letting the user see them while also displaying the user’s eyes. When a user is immersed in an Environment or using an app, EyeSight gives visual cues to others about what the user is focused on.
</div>
                 
             
                 <div>The breakthrough display, advanced audio experiences, high-performance eye tracking system, and more are powered by Apple silicon in a unique dual-chip design. The M2 chip delivers powerful standalone performance, while the brand-new R1 chip processes input from 12 cameras, five sensors, and six microphones to ensure that content feels like it is appearing right in front of the user’s eyes.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A person wearing Apple Vision Pro while speaking with someone using the EyeSight feature that shows their eyes through the device.">
        <div>
             
              
              <div>
                Apple designed a groundbreaking new feature called EyeSight for Apple Vision Pro to help the user stay connected to the people around them, giving visual cues to others about what the user is focused on.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/01/apple-vision-pro-available-in-the-us-on-february-2/article/Apple-Vision-Pro-availability-lifestyle-EyeSight.zip" download="" data-analytics-title="Download image" aria-label="Download media, A person wearing Apple Vision Pro while speaking with someone using the EyeSight feature that shows their eyes through the device."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Privacy and Security at Its Core</strong>
</h2>
                 
             
                 <div>Apple Vision Pro offers industry-leading privacy and security. Optic ID is a new authentication system that analyzes a user’s iris to unlock Vision Pro, autofill passwords, and complete payments with Apple Pay. Where a user looks stays private while navigating Vision Pro, and eye tracking information is not shared with Apple, third-party apps, or websites. EyeSight also includes a visual indicator that makes it clear to others when a user is capturing a spatial photo or video.
</div>
                 
             
                 <h2><strong>Accessibility in visionOS</strong>
</h2>
                 
             
                 <div>As with all Apple products, powerful accessibility features have been built right into visionOS. Key accessibility features like VoiceOver, Zoom, Switch Control, Guided Access, and more have been reimagined for spatial computing. Users can interact with Apple Vision Pro entirely with their eyes, hands, or voice, or any combination that works best for them. They can select a preferred input method such as their eyes, finger, or wrist using Pointer Control, pause on an element of visionOS for a few seconds to simulate a tap using Dwell Control, or simply use voice commands for activities across Vision Pro using Voice Control. If input from both eyes is not an option, visionOS also allows eye tracking with one dominant eye.
</div>
                 
             
                 <h2><strong>Apple Vision Pro and the Environment</strong>
</h2>
                 
             
                 <div>Apple Vision Pro is designed with the environment in mind, with 100 percent recycled rare earth elements in all magnets and 100 percent recycled tin soldering and gold plating in multiple printed circuit boards. The frame and battery enclosure contain 100 percent recycled aluminum, and the Light Seal and Solo Knit Band are each made with over 70 percent recycled yarn. Vision Pro meets Apple’s high standards for energy efficiency and is free of mercury, brominated flame retardants, PVC, and beryllium. The packaging is 100 percent fiber-based, bringing Apple closer to its goal of eliminating plastics in all packaging by 2025.
</div>
                 
             
                 <div>Today, Apple is carbon neutral for its global corporate operations, and by 2030, plans to be carbon neutral across the entire manufacturing supply chain and life cycle of every product.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>Apple Vision Pro will be available starting at<strong> $3,499</strong> (U.S.) with 256GB of storage. Pre-orders for Apple Vision Pro will begin on Friday, January 19, at 5 a.m. PST, with availability beginning Friday, February 2.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Vision Pro will be available at all U.S. Apple Store locations and the U.S. Apple Store online.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>ZEISS Optical Inserts — Readers will be available for <strong>$99</strong> (U.S.), and ZEISS Optical Inserts — Prescription will be available for <strong>$149</strong> (U.S.).</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Vision Pro comes with a Solo Knit Band and Dual Loop Band — giving users two options for the fit that works best for them. Apple Vision Pro also includes a Light Seal, two Light Seal Cushions, an Apple Vision Pro Cover for the front of the device, Polishing Cloth, Battery, USB-C Charge Cable, and USB-C Power Adapter.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Persona is available in beta.</li>
<li>A valid prescription is required. Not all prescriptions are supported. Vision correction accessories are sold separately. ZEISS Optical Inserts are only available to purchase online.</li>
</ol>

        </div>



    
    
    






    
















	
	
	
		















	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[What PWA Can Do Today (250 pts)]]></title>
            <link>https://whatpwacando.today</link>
            <guid>38911791</guid>
            <pubDate>Mon, 08 Jan 2024 13:44:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://whatpwacando.today">https://whatpwacando.today</a>, See on <a href="https://news.ycombinator.com/item?id=38911791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div slot="body">
        <p>
          When motion sensors are not available on your device, you can follow these steps to enable them.
        </p>

        <h3>iOS</h3>
        <p>
          This only applies to iOS 12. You may need to remove and add the app to the home screen again for the changes
          to take effect.
        </p>
        <p>
          Open settings and scroll down to find Safari:
        </p>
        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-ios-step1.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-ios-step1.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-ios-step1.webp">
          </picture>
        </p>

        <p>
          Scroll down to find "Motion &amp; Orientation Access" and toggle the switch on:
        </p>
        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-ios-step2.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-ios-step2.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-ios-step2.webp">
          </picture>
        </p>

        <h3>Android</h3>
        <p>
          Open this site in Chrome browser for Android and open the main menu by tapping the icon in the top-right
          corner:
        </p>

        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-step1.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-step1.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-step1.webp">
          </picture>
        </p>

        <p>
          In the menu that opens, tap Settings:
        </p>

        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-step2.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-step2.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-step2.webp">
          </picture>
        </p>

        <p>
          In the Settings, tap Site settings:
        </p>

        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-step3.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-step3.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-step3.webp">
          </picture>
        </p>

        <p>
          In the Site settings menu, tap Motion sensors:
        </p>

        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-step4.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-step4.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-step4.webp">
          </picture>
        </p>

        <p>
          You can now enable Motion sensors:
        </p>

        <p>
          <picture>
            <source srcset="https://whatpwacando.today/src/img/sensors-step5.webp" type="image/webp">
            <source srcset="https://whatpwacando.today/src/img/sensors-step5.png" type="image/png">
            <img src="https://whatpwacando.today/src/img/sensors-step5.webp">
          </picture>
        </p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Aren't We Sieve-Ing? (181 pts)]]></title>
            <link>https://brooker.co.za/blog/2023/12/15/sieve.html</link>
            <guid>38911740</guid>
            <pubDate>Mon, 08 Jan 2024 13:39:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2023/12/15/sieve.html">https://brooker.co.za/blog/2023/12/15/sieve.html</a>, See on <a href="https://news.ycombinator.com/item?id=38911740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">
<h2 id="why-arent-we-sieve-ing">Why Aren’t We SIEVE-ing?</h2>

<p>Captain, we are being scanned!</p>

<p>Long-time readers of this blog will know that I have mixed feelings about caches. One on hand, caching is critical to the performance of systems at every layer, from CPUs to storage to whole distributed architectures. On the other hand, caching being this critical means that designers need to carefully consider what happens when the cache is emptied, and they don’t always do that well<sup><a href="#foot1">1</a></sup>.</p>

<p>Because of how important caches are, I follow the literature in the area fairly closely. Even to a casual observer, it’s obvious that there’s one group of researchers who’ve been on a bit of a tear recently, including Juncheng Yang, Yazhuo Zhang, K. V. Rashmi, and Yao Yue in various combinations. Their recent papers include <a href="https://www.usenix.org/system/files/osdi20-yang.pdf">a real-world analysis of cache systems at Twitter</a>, <a href="https://jasony.me/publication/hotos23-qdlp.pdf">an analysis of the dynamics of cache eviction</a>, and <a href="https://dl.acm.org/doi/10.1145/3600006.3613147">a novel FIFO-based cache design with some interesting properties</a>.</p>

<p>The most interesting one to me, which I expect anybody who enjoys a good algorithm will get a kick out of, is the eviction algorithm <a href="https://junchengyang.com/publication/nsdi24-SIEVE.pdf">SIEVE</a> (their paper is coming up at NSDI’24). SIEVE is an <em>eviction algorithm</em>, a way of deciding which cached item to toss out when a new one needs to be put in. There are hundreds of these in the literature. At least. Classics including throwing out the least recently inserted thing (FIFO), least recently accessed thing (LRU), thing that’s been accessed least often (LFU), and even just a random thing. Eviction is interesting because it’s a tradeoff between accuracy, speed (how much work is needed on each eviction and each access), and metadata size. The slower the algorithm, the less latency and efficiency benefit from caching. The larger the metadata, the less space there is to store actual data.</p>

<p>SIEVE performs well. In their words:</p>

<blockquote>
  <p>Moreover, SIEVE has a lower miss ratio than 9 state-of-the-art algorithms on more than 45% of the 1559 traces, while the next best algorithm only has a lower miss ratio on 15%.</p>
</blockquote>

<p>What’s super interesting about SIEVE is that it’s both very effective, and an extremely simple change on top of a basic FIFO queue. Here’s Figure 1 from <a href="https://junchengyang.com/publication/nsdi24-SIEVE.pdf">their paper</a> with the pseudocode:</p>

<p><img src="https://brooker.co.za/blog/images/sieve_figure_1.png" alt=""></p>

<p>The only other change is to set <code>obj.visited</code> on access. Like the classic <a href="https://www.multicians.org/paging-experiment.pdf">CLOCK</a> (from the 1960s!), and unlike the classic implementation of LRU, SIEVE doesn’t require changing the queue order on access, which reduces the synchronization needed in a multi-tenant setting. All it needs on access is to set a <code>bool</code>, which is a simple atomic operation on most processors. That’s something of a big deal, for an algorithm that performs so well.</p>

<h2 id="why-arent-we-all-sieve-ing">Why aren’t we all SIEVE-ing?</h2>

<p>SIEVE raises an interesting question - if it’s so effective, and so simple, and so closely related to an algorithm that’s been around forever, why has nobody discovered it already? It’s possible they have, but I haven’t seen it before, and the authors say they haven’t either. Their hypothesis is an interesting one:</p>

<blockquote>
  <p>In block cache workloads, which frequently feature scans, popular objects often intermingle with objects from scans. Consequently, both types of objects are rapidly evicted after insertion.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>We conjecture that not being scan-resistant is probably the reason why SIEVE remained undiscovered over the decades of caching research, which has been mostly focused on page and block accesses.</p>
</blockquote>

<p>That’s believable. Scan resistance is important, and has been the focus of a lot of caching improvements over the decades<sup><a href="#foot2">2</a></sup>. Still, it’s hard to believe that folks kept finding this, and kept going <em>nah, not scan resistant</em> and tossing it out. Fascinating how these things are discovered.</p>

<p>Scan-resistance is important for block and file workloads because these workloads tend to be a mix of random access (<em>update that database page</em>, <em>move that file</em>) and large sequential access (<em>backup the whole database</em>, <em>do that unindexed query</em>). We don’t want the hot set of the cache that makes the random accesses fast evicted to make room for the sequential<sup><a href="#foot4">4</a></sup> pages that likely will never be accessed again<sup><a href="#foot3">3</a></sup>.</p>

<h2 id="a-scan-resistant-sieve">A Scan-Resistant SIEVE?</h2>

<p>This little historical mystery raises the question of whether there are similarly simple, but more scan-resistant, approaches to cache eviction. One such algorithm, which I’ll call SIEVE-k, involves making a small change to SIEVE.</p>

<ul>
  <li>Each item is given a small counter rather than a single access bit,</li>
  <li>On access the small counter is incremented rather than set, saturating at the value <code>k</code>,</li>
  <li>When the eviction <code>hand</code> goes past, the counter is decremented (saturating at 0), rather than reset.</li>
</ul>

<p>My claim here is that the eviction counter will go up for the most popular objects, causing them to be skipped in the round of evictions kicked off by the scan. This approach has some downsides. One is that eviction goes from worst-case <code>O(N)</code> to worst-case <code>O(kN)</code>, and the average case eviction also seems to go up by <code>k</code> (although I don’t love my analysis there). The other is that this could delay eviction of things that need to be evicted. Balancing these things, the most interesting variant of SIEVE-k is probably SIEVE-2 (along with SIEVE-1, which is the same as Zhang et al’s original algorithm).</p>

<h2 id="does-it-work">Does It Work?</h2>

<p>Yeah. Sort of. First, let’s consider a really trivial case of a Zipf-distributed <em>base</em> workload, and a periodic linear scan workload that turns on and off. In this simple setting SIEVE-2 out-performs SIEVE-1 across the board (lower miss rates are better).</p>

<p><img src="https://brooker.co.za/blog/images/sieve_k_results.png" alt=""></p>

<p>Clearly, with the 16MiB cache size here, SIEVE-2 and SIEVE-3 are doing a better job than SIEVE of keeping the scan from emptying out the cache. Beyond this magic size, it performs pretty much identically to SIEVE-1.</p>

<p>But the real-world is more complicated than that. Using the excellent open source <a href="https://github.com/cacheMon/libCacheSim">libCacheSim</a> I tried SIEVE-2 against SIEVE on a range of real-world traces. It was worse than SIEVE across the board on web-cache style KV workloads, as expected. Performance on block workloads<sup><a href="#foot5">5</a></sup> was a real mixed bag, with some wins and some losses. So it seems like SIEVE-k is potentially interesting, but isn’t a win over SIEVE more generally.</p>

<p>If you’d like to experiment some more, I’ve implemented SIEVE-k in <a href="https://github.com/mbrooker/libCacheSim">a fork of libCacheSim</a>.</p>

<h2 id="updates">Updates</h2>

<p><a name="updates"></a>The inimitable Keegan Carruthers-Smith writes:</p>

<blockquote>
  <p>I believe there is an improvement on your worst case for SIEVE-k eviction from O(kN) to O(N):
When going through the list, keep track of the minimum counter seen.  Then if you do not evict on the first pass, decrement by that minimum value.</p>
</blockquote>

<p>Which is, indeed, correct and equivalent to what my goofy k-pass approach was doing (only <code>k/2</code> times more efficient). He also pointed out that other optimizations are possible, but probably not that interesting for small <code>k</code>.</p>

<p>And, on the fediverse, Tobin Baker pointed out something important about SIEVE compared to FIFO and CLOCK: removing items from the middle of the list (rather than the head or tail only) means that the simple <code>circular array</code> approach doesn’t work. The upshot is needing <code>O(log N)</code> additional state<sup><a href="#foot6">6</a></sup> to keep a linked list. Potentially an interesting line of investigation for implementations that are very memory overhead sensitive or CPU cache locality sensitive (and scanning through entries in a random order rather than sequentially). Tobin then <a href="https://fediscience.org/@tobinbaker@discuss.systems/111660149084030363">pointed out an interesting potential fix</a>:</p>

<blockquote>
  <p>A simple fix to the SIEVE algorithm to accommodate circular arrays would be to move the current tail entry into the evicted entry’s slot (much like CLOCK copies a new entry into the evicted entry’s slot). This is really not very different from the FIFO-reinsertion algorithm, except that its promotion method (moving promoted entries to evicted slots) preserves the SIEVE invariant of keeping new entries to the right of the “hand” and old entries to the left.</p>
</blockquote>

<p>This one is interesting, and I don’t have a good intuition for how it would affect performance (or whether the analogy to FIFO-reinsertion is correct). Implementing it in libCacheSim would likely sort that out.</p>

<p><strong>Footnotes</strong></p>

<ol>
  <li><a name="foot1"></a> Partially because it’s hard to do. <a href="https://brooker.co.za/blog/2022/06/02/formal.html">We need better tools</a> for reasoning about system behavior.</li>
  <li><a name="foot2"></a> Including Betty O’Neil’s <a href="https://dl.acm.org/doi/pdf/10.1145/170036.170081">The LRU-K Page Replacement Algorithm For Database Disk Buffer</a>, a classic approach to scan resistance from the 90s database literature.</li>
  <li><a name="foot3"></a> It’s worth mentioning that some caches solve this by hoping that clients will let them know when data is only going to be accessed once (like with <code>POSIX_FADV_NOREUSE</code> and <code>POSIX_FADV_DONTNEED</code>). This can be super effective with the right discipline, but storage systems <em>in general</em> can’t make these kinds of assumptions (and often don’t have these kinds of interfaces at all).</li>
  <li><a name="foot4"></a> I say <em>sequential</em> here, but it’s really not sequential access that matters. What matters is that scans tend to happen at a high rate, and that they introduce a lot of <em>one hit wonders</em> (pages that are read once and never again, and therefore are not worth caching). Neither of those criteria need sequential access, but it happens to be true that they come along most often during sequential accesses.</li>
  <li><a name="foot5"></a> Block traces are interesting, because they tend to represent a kind of residue of accesses after the <em>easy</em> caching has already been done (by the database engine or OS page cache), and so represent a pretty tough case for cache algorithms in general.</li>
  <li><a name="foot6"></a> Which can be halved by <a href="https://en.wikipedia.org/wiki/XOR_linked_list">committing unspeakable evil</a>.</li>
</ol>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LinguaCafe: Self-hosted software for language learners to read foreign languages (102 pts)]]></title>
            <link>https://simjanos-dev.github.io/LinguaCafeHome/</link>
            <guid>38911088</guid>
            <pubDate>Mon, 08 Jan 2024 12:36:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simjanos-dev.github.io/LinguaCafeHome/">https://simjanos-dev.github.io/LinguaCafeHome/</a>, See on <a href="https://news.ycombinator.com/item?id=38911088">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Big Tech has already made enough money in 2024 to pay all its 2023 fines (192 pts)]]></title>
            <link>https://proton.me/blog/big-tech-2023-fines-vs-revenue</link>
            <guid>38910731</guid>
            <pubDate>Mon, 08 Jan 2024 11:53:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://proton.me/blog/big-tech-2023-fines-vs-revenue">https://proton.me/blog/big-tech-2023-fines-vs-revenue</a>, See on <a href="https://news.ycombinator.com/item?id=38910731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><time datetime="2024-01-08T10:31:15">Published on January 8, 2024</time></p>
<p>Last year, Big Tech companies (Alphabet, Amazon, Apple, Meta, and Microsoft) received about<strong> $3.04 billion in fines for breaking laws on both sides of the Atlantic</strong>. <strong>As of seven days and three hours into 2024</strong><strong>, they had already earned enough revenue to pay it all off</strong>.</p>



<p>A little over a week of operations is all it would take if the companies tackled their fines one after another. If they each paid their fines simultaneously, Meta would be the last to finish after five days and 13 hours.&nbsp;</p>



<p>This shows how insignificant these fines are for companies whose revenues are often larger than countries’ GDPs. In fact, 2023 was the second straight year these five companies were <a rel="" target="_self" href="https://proton.me/blog/big-tech-three-billion-fines">fined over $3 billion<span>(new window)</span></a>, showing that they view fines as nothing more than the cost of doing business.&nbsp;</p>



<p>This also assumes these companies act in good conscience and pay their fines promptly. Unfortunately, there’s no late payment penalty for Big Tech — and they often delay for years. It’s just another instance of Big Tech using its overwhelming revenue to bend or avoid the rules that govern the rest of us. Clearly, the current remedies for antitrust and privacy breaches are not having the desired effect.&nbsp;</p>



<h2>Big Tech annual revenue dwarfs total fines</h2>



<p>We <a rel="" target="_self" href="https://proton.me/blog/big-tech-bigger-fines">published an article<span>(new window)</span></a> in August 2023 showing that the fines Big Tech companies received by that point in the year, some $2.3 billion, were inconsequential compared to their income. We thought it would be good to revisit those figures at the end of the year. We’ve calculated the average revenue earned per hour for each company using the latest figures reported. (We explain the sources we used for calculating this table at the bottom of this article.)</p>



<figure><table><thead><tr><th><strong>Company</strong></th><th><strong>Revenue earned per hour</strong></th><th><strong>How long it takes to earn $1 billion</strong></th></tr></thead><tbody><tr><td><a rel="noopener noreferrer" target="_blank" href="https://www.sec.gov/Archives/edgar/data/1652044/000165204423000094/goog-20230930.htm">Alphabet<span>(new window)</span></a> (for the first three quarters of 2023)</td><td>$33.74 million&nbsp;</td><td>1 day, 5 hours, 35 minutes</td></tr><tr><td><a rel="noopener noreferrer" target="_blank" href="https://s2.q4cdn.com/299287126/files/doc_financials/2023/q3/AMZN-Q3-2023-Earnings-Release.pdf">Amazon<span>(new window)</span></a> (for the first three quarters of 2023)</td><td>$61.79 million</td><td>16 hours, 10 minutes</td></tr><tr><td><a rel="noopener noreferrer" target="_blank" href="https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf">Apple<span>(new window)</span></a> (for the fiscal year ending Sept. 30, 2023)</td><td>$43.75 million</td><td>22 hours, 50 minutes</td></tr><tr><td><a rel="noopener noreferrer" target="_blank" href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001326801/29804645-5034-420a-82b6-54f869ec422b.html#">Meta<span>(new window)</span></a> (for the first three quarters of 2023)</td><td>$12.89 million</td><td>3 days, 3 hours, 35 minutes</td></tr><tr><td><a rel="noopener noreferrer" target="_blank" href="https://view.officeapps.live.com/op/view.aspx?src=https://c.s-microsoft.com/en-us/CMSFiles/MSFT_FY23Q4_10K.docx?version=d86a284d-dfce-35ee-366c-d754d90f9174">Microsoft<span>(new window)</span></a> (for the fiscal year ending June 30, 2023)</td><td>$24.19 million</td><td>1 day, 17 hours, 20 minutes</td></tr></tbody></table></figure>



<p>We also included fines these companies received in 2023 in the months after our article was published, such as a $700 million settlement in a case against Alphabet (Google) for how it <a rel="noopener noreferrer" target="_blank" href="https://finance.yahoo.com/news/google-pay-700-million-us-043053930.html">illegally blocked competition on the Google Play Store<span>(new window)</span></a> that came out in December.&nbsp;</p>



<p>The updated table below shows the 2023 fine totals for each company, along with how long it would take them to pay off those fines with their 2023 average hourly revenue.</p>



<figure><table><thead><tr><th><strong>Company</strong></th><th><strong>2023 total fines</strong></th><th><strong>Company</strong></th></tr></thead><tbody><tr><td>Alphabet</td><td>$941 million</td><td>1 day, 3 hours, 55 minutes</td></tr><tr><td>Amazon</td><td>$111.7 million</td><td>1 hour, 50 minutes</td></tr><tr><td>Apple</td><td>$186.4 million</td><td>4 hours, 15 minutes</td></tr><tr><td>Meta</td><td>$1.72 billion</td><td>5 days, 13 hours, 25 minutes</td></tr><tr><td>Microsoft</td><td>$84 million</td><td>3 hours, 30 minutes</td></tr><tr><td><strong>Total fines</strong></td><td><strong>$3.04 billion</strong></td><td><strong>7 days, 3 hours</strong></td></tr></tbody></table><figcaption>For Apple and Microsoft, we calculated the average revenue per hour using the annual revenue amount in their 10-K report for their 2023 fiscal year (dates are noted in the table). We then divided that amount by 365 and then divided the resulting amount by 24 to reach the hourly average.<p>Alphabet (Google), Amazon, and Meta (Facebook) don’t release their annual financial statements until the beginning of February, so we calculated the average revenue per hour using the revenue they reported for the first nine months of 2023 in their September 10-Q report. We then divided that amount by 273 (the number of days in a year through Sept. 30) and then divided the resulting amount by 24 to reach the hourly average.</p></figcaption></figure>



<h2>Delay, delay, delay</h2>



<p>Big Tech companies can further water down the potential sting of these fines by <a rel="noreferrer noopener" target="_blank" href="https://www.barrons.com/news/how-big-tech-generated-billions-in-fines-then-didn-t-pay-them-ab5d0a77">delaying payment for years<span>(new window)</span></a>. They’ve done this by filing appeals, counter-suing, or simply refusing to pay. Some notable examples include:</p>



<ul>
<li>Amazon continues to contest its <a rel="noreferrer noopener" target="_blank" href="https://www.bbc.com/news/business-58024116">$886 million fine from the Luxembourg DPA in 2021<span>(new window)</span></a>.</li>



<li>Google somehow still arguing against $8 billion in antitrust fines from the EU in <a rel="noreferrer noopener" target="_blank" href="https://ec.europa.eu/commission/presscorner/detail/es/MEMO_17_1785">2017<span>(new window)</span></a>, <a rel="noreferrer noopener" target="_blank" href="https://www.theverge.com/2018/7/18/17580694/google-android-eu-fine-antitrust">2018<span>(new window)</span></a>, and <a rel="noreferrer noopener" target="_blank" href="https://www.theverge.com/2019/3/20/18270891/google-eu-antitrust-fine-adsense-advertising">2019<span>(new window)</span></a>.</li>



<li>Google telling the Indian government that it <a rel="noreferrer noopener" target="_blank" href="https://www.businesstoday.in/technology/news/story/google-refuses-to-pay-fine-in-india-even-though-it-paid-fine-in-same-case-in-europe-ccis-lawyer-360460-2023-01-16">simply refuses to pay their fines<span>(new window)</span></a>.</li>
</ul>



<p>Every company has the right to appeal, and privacy abuses and antitrust cases are notoriously complicated and often governed by laws that were written well before the internet was invented, meaning a great deal of interpretation is necessary. Still, it’s not a sign of a functioning justice system for appeals to take such a long time while people continue to suffer data abuses and unfair high prices. Those with the deepest pockets can often drag out litigation well after it’s clear they have no chance of winning, taking time and resources away from regulators’ efforts to create a level playing field in the market.&nbsp;&nbsp;</p>



<h2>Weak fines are not a deterrent</h2>



<p>These paltry fines aren’t helpful if you think a fine should function as a punishment for poor corporate governance or an incentive for companies to change their behavior. If a company can pay off a fine with less than a week’s revenue or avoid paying it entirely for half a decade, then it won’t cause executives to tread cautiously.&nbsp;</p>



<p>For example, despite Meta racking up nearly $2 billion in fines and Google nearly $1 billion, both Mark Zuckerberg and Sundar Pichai remain secure in their CEO posts. While other CEOs might be ousted immediately if their companies were fined billions of dollars year after year, investors and board members are willing to accept them at Big Tech companies because $1 billion represents less than 1% of these company’s total revenue. They view these fines as nothing more than the cost of doing business under the surveillance capitalism business model. And as their balance sheets show, business remains good.</p>



<p>This is important because the internet has become one of the most (arguably the most) important pieces of global public infrastructure the world has ever seen. It connects billions of people to their friends and family, to gainful employment, to entertainment, to essential information, and more. Despite how essential it is to the global economy and society, governments around the world have allowed Big Tech to bend the internet to their will.</p>



<p>Big Tech believes the internet belongs to them, that they can use their size and wealth to <a rel="" target="_self" href="https://proton.me/blog/how-big-tech-tracks-users">gather people’s private information with impunity<span>(new window)</span></a> and <a rel="" target="_self" href="https://proton.me/blog/search-risk-google">block people from choosing competitors<span>(new window)</span></a> that might offer more privacy.</p>



<h2>We all must fight for a better internet</h2>



<p>Big Tech knows people want privacy. It’s why <a rel="" target="_self" href="https://proton.me/blog/google-privacy-washing">Google<span>(new window)</span></a> and Apple have launched major ad campaigns talking about how they give you control of your data. However, if you look at the features they’re producing, they continue to search for new ways to collect people’s data, from <a rel="" target="_self" href="https://proton.me/blog/privacy-metaverse">launching metaverses<span>(new window)</span></a> to <a rel="" target="_self" href="https://proton.me/blog/google-privacy-washing-android">following your activity in other apps<span>(new window)</span></a>, showing they aren’t taking regulators seriously.</p>



<p>If we want to take back control of the internet, several things must happen.&nbsp;</p>



<ul>
<li>First, governments must combat Big Tech. They must impose fines that get Big Tech’s attention for breaching user privacy and locking competitors out of the market.</li>



<li>But fines can only be part of the solution. Regulators, armed with strong antitrust legislation, must strongly challenge mergers that create massive monopolies. They must encourage competition and empower people to choose the service that is best for them.&nbsp;</li>
</ul>



<p>There are several opportunities for legislators to empower regulators to ensure fair competition and protect people’s rights in the coming year, such as with the Digital Markets, Consumer and Competition Bill in the UK that’s currently going through Parliament. Similar legislative initiatives have started in South Korea and Japan, as well.</p>



<p>But until governments step in, you must protect your information yourself. If you remove your personal data from Big Tech’s reach, not only are you protecting your privacy, you’re also depriving Big Tech of the ability to sell targeted ads.&nbsp;</p>



<p>The best way to protect your information from Big Tech is to use end-to-end encrypted services. You can sign up for Proton today for free and join the over 100 million people who use our encrypted services to protect their <a href="https://proton.me/mail">emails</a>, <a href="https://proton.me/calendar">schedule details</a>, <a href="https://proton.me/drive">photos and files</a>, <a rel="noopener noreferrer" target="_blank" href="https://protonvpn.com/">online browsing<span>(new window)</span></a>, and <a href="https://proton.me/pass">passwords and online identity</a>.&nbsp;</p>



<hr>



<figure><table><thead><tr><th><strong>Company</strong></th><th><strong>Fine</strong></th><th><strong>Time to pay off fine</strong></th></tr></thead><tbody><tr><td>Alphabet (Google)</td><td>$160,000,000 (for <a rel="noreferrer noopener" target="_blank" href="https://www.bbc.com/news/world-asia-india-65120697">Android’s market dominance in India<span>(new window)</span></a>)</td><td>4 hours, 45 minutes</td></tr><tr><td>Apple</td><td>$8,000,000 (by <a rel="noreferrer noopener" target="_blank" href="https://www.politico.eu/article/apple-fined-e8-million-in-privacy-case">the French DPA for privacy violations<span>(new window)</span></a>)</td><td>11 minutes</td></tr><tr><td>Apple</td><td>$17,000,000 (for <a rel="noreferrer noopener" target="_blank" href="https://www.reuters.com/technology/russian-anti-monopoly-agency-fines-apple-17-million-tass-2023-01-17">dominant market activity in Russia<span>(new window)</span></a>)</td><td>23 minutes</td></tr><tr><td>Meta (Facebook)</td><td>$5,950,000 (by the <a rel="noreferrer noopener" target="_blank" href="https://www.dataprotection.ie/en/news-media/data-protection-commission-announces-conclusion-inquiry-whatsapp">Irish DPA for breach of privacy laws<span>(new window)</span></a>)</td><td>28 minutes</td></tr><tr><td>Microsoft</td><td>$64,000,000 (by <a rel="noreferrer noopener" target="_blank" href="https://www.competitionpolicyinternational.com/france-fines-microsoft-e60m-over-cookies">French DPA for Bing’s use of cookies)<span>(new window)</span></a></td><td>2 hours, 40 minutes</td></tr><tr><td>Meta (Facebook)</td><td>$414,000,000 (by Irish DPA <a rel="noreferrer noopener" target="_blank" href="https://www.cnbc.com/2023/01/04/meta-fined-more-than-400-million-in-ireland-over-eu-privacy-breaches.html">for breaches of GDPR by both Meta and Instagram<span>(new window)</span></a>)</td><td>1 day, 8 hours, 7 minutes</td></tr><tr><td>Alphabet (Google)</td><td>$32,000,000 (by South Korea for <a rel="noreferrer noopener" target="_blank" href="https://www.insiderintelligence.com/content/google-dinged-with-32m-fine-south-korea-blocking-mobile-games-on-rival-app-store">blocking mobile games on rival app stores<span>(new window)</span></a>)</td><td>55 minutes</td></tr><tr><td>Amazon</td><td>$30,000,000 (by <a rel="noreferrer noopener" target="_blank" href="https://www.bleepingcomputer.com/news/technology/amazon-faces-30-million-fine-over-ring-alexa-privacy-violations">FTC for privacy and data breaches relating to Ring and Alexa<span>(new window)</span></a>)</td><td>30 minutes</td></tr><tr><td>Meta (Facebook)</td><td>$1,300,000,000 (by <a rel="noreferrer noopener" target="_blank" href="https://www.nytimes.com/2023/05/22/business/meta-facebook-eu-privacy-fine.html">Irish DPA for transfer of personal data to the USA<span>(new window)</span></a>)&nbsp;</td><td>4 days, 4 hours, 50 minutes</td></tr><tr><td>Alphabet (Google)</td><td>$47,000,000 (for <a rel="noreferrer noopener" target="_blank" href="https://www.reuters.com/technology/russian-court-slaps-additional-47-mln-fine-google-owner-alphabet-2023-06-27">anticompetitive activity in video hosting market in Russia<span>(new window)</span></a>)</td><td>1 hour, 23 minutes</td></tr><tr><td>Microsoft</td><td>$20,000,000 (by the <a rel="noreferrer noopener" target="_blank" href="https://sourcepoint.com/blog/ftc-fines-microsoft-over-childrens-privacy">FTC for violations of the Children’s Online Privacy Protection Act<span>(new window)</span></a>)</td><td>50 minutes</td></tr><tr><td>Amazon</td><td>$25,000,000 (by <a rel="noreferrer noopener" target="_blank" href="https://variety.com/2023/digital/news/amazon-fine-alexa-settlement-doj-ftc-childrens-privacy-law-violation-1235675702">FTC for violations of the Children’s Online Privacy Protection Act)<span>(new window)</span></a></td><td>24 minutes</td></tr><tr><td>Alphabet (Google)</td><td>$2,000,000 (by <a rel="noreferrer noopener" target="_blank" href="https://www.euronews.com/2023/07/04/google-fined-2m-in-france-over-search-engine-and-google-play">France for failure to provide search rank criteria on Google search and the Play Store<span>(new window)</span></a>)</td><td>4 minutes</td></tr><tr><td>Apple</td><td>$161,400,000 (by Spain, <a rel="noreferrer noopener" target="_blank" href="https://edition.cnn.com/2023/07/18/tech/spain-antitrust-fine-apple-amazon/index.html">for price fixing Apple products on Amazon<span>(new window)</span></a>)</td><td>3 hours, 41 minutes</td></tr><tr><td>Amazon</td><td>$56,700,000 (by Spain, <a rel="noreferrer noopener" target="_blank" href="https://edition.cnn.com/2023/07/18/tech/spain-antitrust-fine-apple-amazon/index.html">for price fixing Apple products on Amazon)<span>(new window)</span></a></td><td>55 minutes</td></tr><tr><td>Alphabet (Google)</td><td>$700,000,000 (in a settlement with all 50 states in the US <a rel="noreferrer noopener" target="_blank" href="https://finance.yahoo.com/news/google-pay-700-million-us-043053930.html">for abusing its power in the Google Play Store<span>(new window)</span></a>)</td><td>20 hours, 45 minutes</td></tr></tbody><tfoot><tr><td><strong>Time to pay off fine</strong></td><td><strong>$3,043,050,000</strong></td><td><strong>7 days, 2 hours, 51 minutes</strong></td></tr></tfoot></table><figcaption>The above table is not an exhaustive list of all the fines these companies received in the 2023 calendar year. We included all the substantial fines we could find in major news sources, but there might be others. Also, this table excludes several censorship fines from Russia that seem motivated by Big Tech’s refusal to remove news about Russia’s invasion of Ukraine. Also, due to the fluctuation of foreign exchange rates, the dollar amounts might have shifted since this article was published.</figcaption></figure>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Phrase "No Evidence" Is a Red Flag for Bad Science Communication (2021) (246 pts)]]></title>
            <link>https://www.astralcodexten.com/p/the-phrase-no-evidence-is-a-red-flag</link>
            <guid>38909735</guid>
            <pubDate>Mon, 08 Jan 2024 09:14:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.astralcodexten.com/p/the-phrase-no-evidence-is-a-red-flag">https://www.astralcodexten.com/p/the-phrase-no-evidence-is-a-red-flag</a>, See on <a href="https://news.ycombinator.com/item?id=38909735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><strong>Related to:</strong><span> </span><a href="https://www.overcomingbias.com/2008/08/doctor-there-ar.html" rel="">Doctor, There Are Two Types Of No Evidence</a><span>; </span><a href="https://slatestarcodex.com/2020/04/14/a-failure-but-not-of-prediction/" rel="">A Failure, But Not Of Prediction</a><span>.</span></p><p><strong>I.</strong></p><p>Every single one of these statements that had “no evidence” is currently considered true or at least pretty plausible.</p><p>In an extremely nitpicky sense, these headlines are accurate. Officials were simply describing the then-current state of knowledge. In medicine, anecdotes or hunches aren’t considered “real” evidence. So if there hasn’t been a study showing something, then there’s “no evidence”. In early 2020, there hadn’t yet been a study proving that COVID could be airborne, so there was “no evidence” for it.</p><p><span>On the other hand, here is a recent headline: </span><a href="https://www.usatoday.com/story/news/factcheck/2021/09/10/fact-check-no-evidence-vaccine-related-complications-killed-45-000/8256978002/" rel="">No Evidence That 45,000 People Died Of Vaccine-Related Complications</a><span>. Here’s another: </span><a href="https://www.msn.com/en-us/news/us/fact-check-no-evidence-pfizer-moderna-covid-19-vaccines-cause-miscarriage/ar-AARMn2d" rel="">No Evidence Vaccines Cause Miscarriage</a><span>. I don’t think the scientists and journalists involved in these stories meant to shrug and say that no study has ever been done so we can’t be sure either way. I think they meant to express strong confidence these things are false.</span></p><p>You can see the problem. Science communicators are using the same term - “no evidence” - to mean:</p><ol><li><p>This thing is super plausible, and honestly very likely true, but we haven’t checked yet, so we can’t be sure.</p></li><li><p>We have hard-and-fast evidence that this is false, stop repeating this easily debunked lie.</p></li></ol><p><span>This is </span><em>utterly corrosive</em><span> to anybody trusting science journalism. </span></p><p><span>Imagine you are John Q. Public. You read “no evidence of human-to-human transmission of coronavirus”, and then a month later it turns out such transmission is common. You read “no evidence linking COVID to indoor dining”, and a month later your governor has to shut down indoor dining because of all the COVID it causes. You read “no hard evidence new COVID strain is more transmissible”, and a month later everything is in panic mode because it was more transmissible after all. And </span><em>then</em><span> you read “no evidence that 45,000 people died of vaccine-related complications”. Doesn’t sound very reassuring, does it?</span></p><p><strong>II.</strong></p><p>Unfortunately, I don’t think this is just a matter of scientists and journalists using the wrong words sometimes. I think they are fundamentally confused about this. </p><p>In traditional science, you start with a “null hypothesis” along the lines of “this thing doesn’t happen and nothing about it is interesting”. Then you do your study, and if it gets surprising results, you might end up “rejecting the null hypothesis” and concluding that the interesting thing is true; otherwise, you have “no evidence” for anything except the null. </p><p>This is a perfectly fine statistical hack, but it doesn’t work in real life. In real life, there is no such thing as a state of “no evidence” and it’s impossible to even give the phrase a consistent meaning. EG:</p><p><strong>Is there "no evidence" that using a parachute helps prevent injuries when jumping out of planes?</strong><span> This was the conclusion of </span><a href="https://www.bmj.com/content/327/7429/1459?ijkey=c3677213eca83ff6599127794fc58c4e0f6de55a&amp;keytype2=tf_ipsecsha" rel="">a cute paper</a><span> in the </span><em>BMJ</em><span>, which pointed out that as far as they could tell, nobody had ever done a study proving parachutes helped. Their point was that "evidence" isn't the same thing as "peer-reviewed journal articles". So maybe we should stop demanding journal articles, and accept informal evidence as valid?</span></p><p><strong>Is there "no evidence" for alien abductions?</strong><span> There are hundreds of people who say they've been abducted by aliens! By legal standards, hundreds of eyewitnesses is </span><em>great</em><span> evidence! If a hundred people say that Bob stabbed them, Bob is a serial stabber - or, even if you thought all hundred witnesses were lying, you certainly wouldn't say the prosecution had “no evidence”! When we say "no evidence" here, we mean "no really strong evidence from scientists, worthy of a peer-reviewed journal article". But this is the opposite problem as with the parachutes - here we should stop accepting informal evidence, and demand more scientific rigor.</span></p><p><strong>Is there "no evidence" homeopathy works?</strong><span> No, </span><a href="https://pubmed.ncbi.nlm.nih.gov/30202036/" rel="">here's a peer-reviewed study showing that it does</a><span>. Don't like it? I have </span><a href="https://pubmed.ncbi.nlm.nih.gov/9310601" rel="">eighty-nine more peer-reviewed studies showing that right here</a><span>. But a strong theoretical understanding of how water, chemicals, immunology, etc operate suggests homeopathy can't </span><em>possibly</em><span> work, so I assume all those pro-homeopathy studies are methodologically flawed and useless, the same way </span><a href="https://en.wikipedia.org/wiki/Replication_crisis#In_medicine" rel="">somewhere between 16% and 89%</a><span> of other medical studies are flawed and useless. Here we should reject </span><em>journal articles</em><span> because they disagree with </span><em>informal</em><span> evidence!</span></p><p><strong>Is there "no evidence" that King Henry VIII had a spleen?</strong><span> Certainly nobody has published a peer-reviewed article weighing in on the matter. And probably nobody ever dissected him, or gave him an abdominal exam, or collected any informal evidence. Empirically, this issue is just a complete blank, an empty void in our map of the world. Here we should ignore the absence of journal articles </span><em>and</em><span> the absence of informal evidence, and just assume it's true because </span><em>obviously</em><span> it’s true.</span></p><p><span>I challenge anyone to come up with a definition of "no evidence" that wouldn't be misleading in at least one of the above examples. If you can't do it, I think that's because the folk concept of "no evidence" doesn't match how real truth-seeking works. Real truth-seeking is </span><a href="https://www.yudkowsky.net/rational/bayes" rel="">Bayesian</a><span>. You start with a prior for how unlikely something is. Then you update the prior as you gather evidence. If you gather a lot of strong evidence, maybe you update the prior to somewhere very far away from where you started, like that some really implausible thing is nevertheless true. Or that some dogma you held unquestioningly is in fact false. If you gather only a little evidence, you mostly stay where you started.</span></p><p>I'm not saying this process is easy or even that I'm very good at it. I'm just saying that once you understand the process, it no longer makes sense to say "no evidence" as a synonym for “false”. </p><p><strong>III.</strong></p><p>Okay, but then what? “No Evidence That Snake Oil Works” is the bread and butter of science journalism. How do you express that concept without falling into the “no evidence” trap?</p><p>I think you have to go back to the basics of journalism: what story are you trying to cover? </p><p>If the story is that nobody has ever investigated snake oil, and you have no strong opinion on it, and for some reason that’s newsworthy, use the words “either way”: “No Evidence Either Way About Whether Snake Oil Works”. </p><p><span>If the story is that all the world’s top doctors and scientists believe snake oil doesn’t work, then say so. “Scientists: Snake Oil Doesn’t Work”. This doesn’t have the same faux objectivity as “No Evidence Snake Oil Works”. It centers the belief in fallible scientists, as opposed to the much more convincing claim that </span><em>there is literally not a single piece of evidence anywhere in the world</em><span> that anyone could use in favor of snake oil. Maybe it would sound less authoritative. Breaking an addiction to false certainty is as hard as breaking any other addiction. But the first step is admitting you have a problem.</span></p><p><span>But I think the most virtuous way to write this is to actually investigate. If it’s worth writing a story about why there’s no evidence for something, probably it’s because some people believe there </span><em>is</em><span> evidence. What evidence do they believe in? Why is it wrong? How do you know?</span></p><p>Some people thought masks helped slow the spread of COVID. You can type out "no evidence" and hit "send tweet". But what if you try to engage the argument? Why do people believe masks could slow spread? Well, because it seems intuitively obvious that if something is spread by droplets shooting out of your mouth, preventing droplets from shooting out of your mouth would slow the spread. Does that seem like basically sound logic? If so, are you sure your job as a science communicator requires you to tell people not to believe that? How do you know they're not smarter than you are? There's no evidence that they aren't!</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why use strace in 2023? [video] (127 pts)]]></title>
            <link>https://media.ccc.de/v/all-systems-go-2023-228-why-would-you-still-want-to-use-strace-in-2023-</link>
            <guid>38908496</guid>
            <pubDate>Mon, 08 Jan 2024 05:05:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://media.ccc.de/v/all-systems-go-2023-228-why-would-you-still-want-to-use-strace-in-2023-">https://media.ccc.de/v/all-systems-go-2023-228-why-would-you-still-want-to-use-strace-in-2023-</a>, See on <a href="https://news.ycombinator.com/item?id=38908496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<p><a href="https://media.ccc.de/c/asg2023">
<img alt="conference logo" src="https://static.media.ccc.de/media/events/all_systems_go/2023/logo.png">
</a>
</p>

</div>
<p>
<span></span>
<a href="https://media.ccc.de/search?p=Eugene+Syromiatnikov">Eugene Syromiatnikov</a> and
<a href="https://media.ccc.de/search?p=Dmitry+Levin">Dmitry Levin</a>

</p>
<div data-aspect-ratio="16:9">

<!-- Mediaelement Player -->
<video controls="controls" data-id="12622" data-timeline="https://static.media.ccc.de/media/events/all_systems_go/2023/228-34203612-1025-5359-85f9-d42f13739426.timeline.jpg" height="100%" poster="https://static.media.ccc.de/media/events/all_systems_go/2023/228-34203612-1025-5359-85f9-d42f13739426_preview.jpg" preload="metadata" width="100%">
<source data-lang="eng" data-quality="high" src="https://cdn.media.ccc.de/events/all_systems_go/2023/h264-hd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_hd.mp4" title="eng 1080p" type="video/mp4">
<source data-lang="eng" data-quality="high" src="https://cdn.media.ccc.de/events/all_systems_go/2023/webm-hd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_webm-hd.webm" title="eng 1080p" type="video/webm">
<source data-lang="eng" data-quality="low" src="https://cdn.media.ccc.de/events/all_systems_go/2023/h264-sd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_sd.mp4" title="eng 576p" type="video/mp4">
<source data-lang="eng" data-quality="low" src="https://cdn.media.ccc.de/events/all_systems_go/2023/webm-sd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_webm-sd.webm" title="eng 576p" type="video/webm">
<track kind="metadata" label="thumbnails" src="https://static.media.ccc.de/media/events/all_systems_go/2023/228-34203612-1025-5359-85f9-d42f13739426.thumbnails.vtt" srclang="">

</video>


</div><p>
Playlists:
<a href="https://media.ccc.de/v/all-systems-go-2023-228-why-would-you-still-want-to-use-strace-in-2023-/playlist">'asg2023' videos starting here</a>
/
<a data-method="get" href="https://media.ccc.de/v/all-systems-go-2023-228-why-would-you-still-want-to-use-strace-in-2023-/audio">audio</a></p><ul>
<li>
<span></span>
26 min
</li>
<li>
<span title="event date"></span>
2023-09-13
</li>
<li>
<span title="release date"></span>
2023-09-14
</li>
<li>
<span></span>
4781
</li>
<li>
<span></span>
<a href="https://cfp.all-systems-go.io/all-systems-go-2023/talk/GUVYJ7/">Fahrplan</a>
</li>
</ul>
<!-- %h3 About -->
<p>strace is a traditional userspace tracer utility for Linux, implemented using ptrace API. Despite of the abundance of various kernel tracing interfaces nowadays, there are certain classes of tasks that are still better served by strace. In this talk the maintainer of strace will provide examples of such tasks.</p>

<h3>Download</h3>
<div>
<div>
<p>
<h4>Video</h4>
</p>
<div>
<ul role="tablist">
<li role="presentation">
<a aria-controls="mp4" data-toggle="tab" href="#mp4" role="tab">
MP4
</a>
</li>
<li role="presentation">
<a aria-controls="webm" data-toggle="tab" href="#webm" role="tab">
WebM
</a>
</li>
</ul>
<div>
<div id="mp4" role="tabpanel">
<div>
<a href="https://cdn.media.ccc.de/events/all_systems_go/2023/h264-hd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_hd.mp4">
<p>Download 1080p</p>
<span>eng</span>
<span>88 MB</span>
</a>
</div>
<div>
<a href="https://cdn.media.ccc.de/events/all_systems_go/2023/h264-sd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_sd.mp4">
<p>Download 576p</p>
<span>eng</span>
<span>40 MB</span>
</a>
</div>
</div>
<div id="webm" role="tabpanel">
<div>
<a href="https://cdn.media.ccc.de/events/all_systems_go/2023/webm-hd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_webm-hd.webm">
<p>Download 1080p</p>
<span>eng</span>
<span>121 MB</span>
</a>
</div>
<div>
<a href="https://cdn.media.ccc.de/events/all_systems_go/2023/webm-sd/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_webm-sd.webm">
<p>Download 576p</p>
<span>eng</span>
<span>55 MB</span>
</a>
</div>
</div>
</div>
</div>
</div>
<div>
<p>
<h4>Audio</h4>
</p>
<div>
<div>
<a href="https://cdn.media.ccc.de/events/all_systems_go/2023/mp3/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_mp3.mp3">
<p>Download mp3</p>
<span>eng</span>
<span>24 MB</span>
</a>
</div>
<div>
<a href="https://cdn.media.ccc.de/events/all_systems_go/2023/opus/asg2023-228-eng-Why_would_you_still_want_to_use_strace_in_2023_opus.opus">
<p>Download opus</p>
<span>eng</span>
<span>15 MB</span>
</a>
</div>
</div>
</div>
</div>
<!-- %h3 Embed/Share -->

<h3>Tags</h3>
<div>
<p><a href="https://media.ccc.de/c/asg2023/asg2023" rel="tag">asg2023</a>
<a href="https://media.ccc.de/tags/228" rel="tag">228</a>
<a href="https://media.ccc.de/tags/2023" rel="tag">2023</a>
</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How meta built the infrastructure for Threads (123 pts)]]></title>
            <link>https://engineering.fb.com/2023/12/19/core-infra/how-meta-built-the-infrastructure-for-threads/</link>
            <guid>38908427</guid>
            <pubDate>Mon, 08 Jan 2024 04:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2023/12/19/core-infra/how-meta-built-the-infrastructure-for-threads/">https://engineering.fb.com/2023/12/19/core-infra/how-meta-built-the-infrastructure-for-threads/</a>, See on <a href="https://news.ycombinator.com/item?id=38908427">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p><span>On July 5, 2023, Meta launched Threads, the newest product in our family of apps, to an unprecedented success that saw it garner </span><a href="https://www.threads.net/@zuck/post/CuhOXGmr74R"><span>over 100 million sign ups</span></a><span> in its first five days.</span></p>
<p><span>A small,</span><a href="https://engineering.fb.com/2023/09/07/culture/threads-inside-story-metas-newest-social-app/"> <span>nimble team of engineers built Threads</span></a><span> over the course of only five months of technical work. While the app’s production launch had been under consideration for some time, the business finally made the decision and informed the infrastructure teams to prepare for its launch with only two days’ advance notice. The decision was made with full confidence that Meta’s infrastructure teams can deliver based on their past track record and the maturity of the infrastructure. Despite the daunting challenges with minimal lead time, the infrastructure teams supported the app’s rapid growth exceptionally well.</span></p>
<p><span>The seamless scale that people experienced as they signed up by the millions came on the shoulders of over a decade of infrastructure and product development. This was not infrastructure purposely built for Threads, but that had been built over the course of Meta’s lifetime for many products. It had already been built for scale, growth, performance, and reliability, and it managed to exceed our expectations as Threads grew at a pace that no one could have predicted.</span></p>
<p><span>A huge amount of infrastructure goes into serving Threads. But, because of space limitations, we will only give examples of two existing components that played an important role:</span> <a href="https://engineering.fb.com/2021/08/06/core-infra/zippydb/" target="_blank" rel="noopener"><span>ZippyDB</span></a><span>, our distributed key/value datastore, and</span> <a href="https://engineering.fb.com/2020/08/17/production-engineering/async/" target="_blank" rel="noopener"><span>Async</span></a><span>, our aptly named asynchronous serverless function platform.</span></p>
<h2><span>ZippyDB: Scaling keyspaces for Threads</span></h2>
<p><span>Let’s zoom in on part of the storage layer, where we leveraged ZippyDB, a distributed key/value database that is run as a fully managed service for engineers to build on. It’s built from the ground up to leverage Meta’s infrastructure, and keyspaces hosted on it can be scaled up and down with relative ease and flexibly placed across any number of data centers. </span><a href="https://engineering.fb.com/2013/06/25/core-infra/tao-the-power-of-the-graph/" target="_blank" rel="noopener"><span>TAO</span></a><span>, backed by </span><a href="https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/" target="_blank" rel="noopener"><span>MySQL</span></a><span>, is used for our social graph storage – thus you can find Threads posts and replies directly in that stack. ZippyDB is our key/value counterpart to MySQL, the relational part of our online data stack, and is used for counters, feed ranking/state, and search.&nbsp;</span></p>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1024,576" width="1024" height="576"></p>
<p><span>The speed at which we can scale the capacity of a keyspace is made possible by two key features: First, the service runs on a common pool of hardware and is plugged into Meta’s overall capacity management framework. Once new capacity is allocated to the service, the machines are automatically added to the service’s pool and the load balancer kicks in to move data to the new machines. We can absorb thousands of new machines in a matter of a few hours once they are added to the service. While this is great, it is not enough since the end-to-end time in approving capacity, possibly draining it from other services and adding it to ZippyDB, can still be in order of a couple of days. We need to also be able to absorb a surge on shorter notice.</span></p>
<p><span>To enable the immediate absorption, we rely on the service architecture’s multi-tenancy and its strong isolation features. This allows for different keyspaces, potentially with complimentary load demands to share the underlying hosts, without worrying about their service level getting impacted when other workloads run hot. There is also slack in the hosts pool due to unused capacity of individual keyspaces as well as buffers for handling disaster recovery events. We can pull levers that shift unused allocations between keyspaces – dipping into any existing slack and letting the hosts run at a higher utilization level to let a keyspace ramp up almost immediately and sustain it over a short interval (a couple of days). All these are simple config changes with tools and automation built around them as they are fairly routine for day-to-day operations.</span></p>
<p><span>The combined effects of strong multi-tenancy and ability to absorb new hardware makes it possible for the service to scale more or less seamlessly, even in the face of a sudden large new demand.</span></p>
<h2><span>Optimizing ZippyDB for a product launch</span></h2>
<p><span>ZippyDB’s resharding protocol allows us to quickly and transparently increase the sharding factor (i.e., horizontal scaling factor) of a ZippyDB use case with zero downtime for clients, all while maintaining full consistency and correctness guarantees. This allows us to rapidly scale out use cases on the critical path of new product launches with zero interruptions to the launch, even if its load increases by 100x.</span></p>
<p><span>We achieve this by having clients hash their keys to logical shards, which are then mapped to a set of physical shards. When a use case grows and requires resharding, we provision a new set of physical shards and install a new logical-to-physical shard mapping in our clients through live configuration changes without downtime. Using hidden access keys on the server itself, and smart data migration logic in our resharding workers, we are then able to atomically move a logical shard from the original mapping to the new mapping. Once all logical shards have been migrated, resharding is complete and we remove the original mapping.</span></p>
<p><span>Because scaling up use cases is a critical operation for new product launches, we have invested heavily in our resharding stack to ensure ZippyDB scaling does not block product launches. Specifically, we have designed the resharding stack in a coordinator-worker model so it is horizontally scalable, allowing us to increase resharding speeds when needed, such as during the Threads launch. Additionally, we have developed a set of emergency operator tools to effortlessly deal with sudden use case growth.&nbsp;</span></p>
<p><span>The combination of these allowed the ZippyDB team to effectively respond to the rapid growth of Threads. Often, when creating new use cases in ZippyDB, we start small initially and then reshard as growth requires. This approach prevents overprovisioning and promotes efficiency in capacity usage. As the viral growth of Threads began, it became evident that we needed to prepare Threads for a 100x growth by proactively performing resharding. With the help of automation tools developed in the past, we completed the resharding just in time as the Threads team opened up the floodgates to traffic at midnight UK time. This enabled delightful user experiences with Threads, even as its user base soared.</span></p>
<h2><span>Async: Scaling workload execution for Threads</span></h2>
<p><span>Async (also known as </span><a href="https://dl.acm.org/doi/pdf/10.1145/3600006.3613155"><span>XFaaS</span></a><span>) is a serverless function platform capable of deferring computing to off-peak hours, allowing engineers at Meta to reduce their time from solution conception to production deployment. Async currently processes trillions of function calls per day on more than 100,000 servers and can support </span><a href="https://engineering.fb.com/2022/07/27/developer-tools/programming-languages-endorsed-for-server-side-use-at-meta/"><span>multiple programming languages</span></a><span>, including HackLang, Python, Haskell, and Erlang.&nbsp;</span></p>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?w=1024" alt="" width="1024" height="521" srcset="https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?resize=916,466 916w, https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?resize=768,391 768w, https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?resize=1024,521 1024w, https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?resize=1536,782 1536w, https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?resize=96,49 96w, https://engineering.fb.com/wp-content/uploads/2020/08/Async-Server-2.jpg?resize=192,98 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>The platform abstracts the details of deployment, queueing, scheduling, scaling, and disaster recovery and readiness, so that developers can focus on their core business logic and offload the rest of the heavy lifting to Async. By onboarding their code in this platform, their code automatically inherits hyperscale attributes. Scalability is not the only key feature of Async. Code uploaded to the platform also inherits guarantees on execution with configurable retries, time for delivery, rate limits, and capacity accountability.</span></p>
<p><span>The workloads commonly executed on Async are those that do not require blocking an active user’s experience with a product and can be performed anywhere from a few seconds to several hours after a user’s action. Async played a critical role in offering users the ability to build community quickly by choosing to follow people on Threads that they already follow on Instagram. Specifically, when a new user joins Threads and chooses to follow the same set of people they do on Instagram, the computationally expensive operation of executing the user’s request to follow the same social graph in Threads is conducted via Async in a scalable manner, which avoids blocking or negatively impacting the user’s onboarding experience.&nbsp;</span></p>
<p><span>Doing this for 100 million users in five days required significant processing power. Moreover, many celebrities joined Threads, and when that happened millions of people could be queued up to follow them. Both this operation and the corresponding notifications also occurred in Async, enabling scalable operations in the face of a large number of users.</span></p>
<p><span>While the volume of Async jobs generated from the rapid Threads user onboarding was several orders of magnitude higher than our initial expectations, Async gracefully absorbed the increased load and queued them for controlled execution. </span><span>Specifically, the execution was managed within rate limits, which ensured that we were sending notifications and allowing people to make connections in a timely manner without overloading the downstream services that receive traffic from these Async jobs. Async automatically adjusted the flow of execution to match its capacity as well as the capacity of dependent services, such as the social graph database, all without manual intervention from either Threads engineers or infrastructure engineers.</span></p>
<h2><span>Where infrastructure and culture meet</span></h2>
<p><span>Threads’ swift development within a mere five months of technical work underscores the strengths of Meta’s infrastructure and engineering culture. Meta’s products leverage a shared infrastructure that has withstood the test of time, empowering product teams to move fast and rapidly scale successful products. The infrastructure boasts a high level of automation, ensuring that, except for efforts to secure capacity on short notice, the automatic redistribution, load balancing, and scaling up of workloads occurred smoothly and transparently.&nbsp;</span><span>Meta thrives on a move-fast engineering culture, wherein engineers take strong ownership and collaborate seamlessly to accomplish a large shared goal, with efficient processes that would take a typical organization months to coordinate. As an example, our</span><a href="https://atscaleconference.com/videos/metas-sev-culture-how-todays-sevs-create-tomorrows-reliability/"> <span>SEV incident-management culture</span></a><span> has been an important tool in getting the right visibility, focus, and action in places where we all need to coordinate and move fast. Overall, these factors combined to ensure the success of the Threads launch.</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing Ext4 (322 pts)]]></title>
            <link>https://buredoranna.github.io/linux/ext4/2020/01/09/ext4-viz.html</link>
            <guid>38907821</guid>
            <pubDate>Mon, 08 Jan 2024 03:13:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buredoranna.github.io/linux/ext4/2020/01/09/ext4-viz.html">https://buredoranna.github.io/linux/ext4/2020/01/09/ext4-viz.html</a>, See on <a href="https://news.ycombinator.com/item?id=38907821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<center><h2>What does ext4 look like?</h2></center>


<div><p>That is... if I start with a blank drive, a drive made completely of 0x00s, and then do mkfs.ext4, what does the drive, now embossed with ext4, look like?</p></div>

<div><p>I mean, what I wanted to see, is what it takes to transmogrify a bunch 0x00s, from "nothing" into the purposeful assemblage of bytes that is an ext4 filesystem.</p></div>

<div><p>At first I figured I’d try visualizing a live drive, like /dev/sda... but quickly figured 'dd' + 'live drive' could get me into trouble, so opted for adding a small secondary drive to my VM.</p></div>

<div><p>Then I thought, even with a virtual machine, working with 'dd' and /dev/sdX would be more trouble than it was worth. I then remembered I didn’t have to use drives at all, virtual or otherwise, I could just work with a regular file, configured as a loop device.</p></div>

<div><p>And it turns out mount/umount have evolved since I last experimented with loop devices... you don’t have to 'losetup' the loop device anymore just a simple:</p></div>

<pre># mount -o loop &lt;foo_file&gt; &lt;bar_dir&gt;
# umount &lt;bar_dir&gt;
</pre>



<div><p>Using a loop device simplified my efforts, and diminished the likelihood of a<br><a href="https://askubuntu.com/questions/982552/accidentally-did-dd-dev-sda">dd accident</a>.</p></div>

<div><p>So a little about what we’re looking at.</p></div>

<p>I always start with a blank file... that is a file created with 'dd' and a source of '/dev/zero'... the calculated size of the file correspond to a final image with eight blocks, each 64-pixels/bytes high, and 1024-pixels/bytes wide.<br></p>

<pre>$ dd if=/dev/zero of=blockfile.ext4 bs=$((64 * 1024)) count=8
</pre>

<p>The output of this is predictable<br></p>
<pre>$ od -x -A x blockfile.ext4
000000 0000 0000 0000 0000 0000 0000 0000 0000
*
080000
</pre>

<div><p>But I wanted to see the difference between a zero file, and one with whatever structure mkfs.ext4 adds to the drive...</p></div>

<div><p>Of note: the size of drive I’m working with is too small for a journal... but thats okay... Doing a visualization which includes a journal I’m leaving for a future project.</p></div>

<p>So now the output of 'od' of the blockfile which mkfs.ext4 is run against, is a little more interesting... here we begin to see structure:<br></p>

<pre>$ od -x -Ax blockfile.ext4
000000 0000 0000 0000 0000 0000 0000 0000 0000
*
000400 0040 0000 0200 0000 0019 0000 01e2 0000
000410 0035 0000 0001 0000 0000 0000 0000 0000
000420 2000 0000 2000 0000 0040 0000 0000 0000
000430 c737 5e10 0000 ffff ef53 0001 0001 0000
000440 c737 5e10 0000 0000 0000 0000 0001 0000
000450 0000 0000 000b 0000 0080 0000 0038 0000
000460 02c2 0000 046b 0000 927f 9037 d060 5e4c
000470 1b83 287a 7389 0001 0000 0000 0000 0000
000480 0000 0000 0000 0000 0000 0000 0000 0000
*
0004c0 0000 0000 0000 0000 0000 0000 0000 0003
0004d0 0000 0000 0000 0000 0000 0000 0000 0000
0004e0 0000 0000 0000 0000 0000 0000 7da5 5d6c
0004f0 72c1 5b42 719d b2ee 63d5 d142 0001 0040
000500 000c 0000 0000 0000 c737 5e10 0000 0000
000510 0000 0000 0000 0000 0000 0000 0000 0000
*
000560 0001 0000 0000 0000 0000 0000 0000 0000
000570 0000 0000 0104 0000 0015 0000 0000 0000
000580 0000 0000 0000 0000 0000 0000 0000 0000
*
0007f0 0000 0000 0000 0000 0000 0000 7d3f 9ace
000800 0006 0000 0016 0000 0026 0000 01e2 0035
000810 0002 0004 0000 0000 c571 4e0a 0035 4797
000820 0000 0000 0000 0000 0000 0000 0000 0000
000830 0000 0000 0000 0000 96a2 c509 0000 0000
000840 0000 0000 0000 0000 0000 0000 0000 0000
*
001800 ffff 002f 1fe0 0000 0000 0000 0000 0000
001810 0000 0000 0000 0000 0000 0000 0000 0000
*
001830 0000 0000 0000 0000 0000 0000 0000 8000
001840 ffff ffff ffff ffff ffff ffff ffff ffff
*
001c00 0002 0000 000c 0201 002e 0000 0002 0000
001c10 000c 0202 2e2e 0000 000b 0000 03dc 020a
001c20 6f6c 7473 662b 756f 646e 0000 0000 0000
001c30 0000 0000 0000 0000 0000 0000 0000 0000
*
001ff0 0000 0000 0000 0000 000c de00 e669 11f0
002000 000b 0000 000c 0201 002e 0000 0002 0000
002010 03e8 0202 2e2e 0000 0000 0000 0000 0000
002020 0000 0000 0000 0000 0000 0000 0000 0000
*
0023f0 0000 0000 0000 0000 000c de00 0f7a 7b5d
002400 0000 0000 03f4 0000 0000 0000 0000 0000
002410 0000 0000 0000 0000 0000 0000 0000 0000
*
0027f0 0000 0000 0000 0000 000c de00 3e04 8f88
002800 0000 0000 03f4 0000 0000 0000 0000 0000
002810 0000 0000 0000 0000 0000 0000 0000 0000
*
002bf0 0000 0000 0000 0000 000c de00 3e04 8f88
002c00 0000 0000 03f4 0000 0000 0000 0000 0000
002c10 0000 0000 0000 0000 0000 0000 0000 0000
*
002ff0 0000 0000 0000 0000 000c de00 3e04 8f88
003000 0000 0000 03f4 0000 0000 0000 0000 0000
003010 0000 0000 0000 0000 0000 0000 0000 0000
*
0033f0 0000 0000 0000 0000 000c de00 3e04 8f88
003400 0000 0000 03f4 0000 0000 0000 0000 0000
003410 0000 0000 0000 0000 0000 0000 0000 0000
*
[... snip ...]
</pre>

<div><p>But at the byte density provided by the output of 'od', trying to visualize the ext4 structure is like trying to visualize the structure of three deciduous forests by examining the leaves of a single tree... I wanted a picture which would let me "zoom out", giving me a better idea of what I was looking at...</p></div>

<div><p>So I came up with this... each blue block is 1024 pixels wide, and 64 pixels high... each pixel represents a single byte... Nothing much to see here, except a drive made entirely of 0x00s.</p></div>

<p><img src="https://buredoranna.github.io/assets/images/blockfile.nulls.ext4.png" alt="I miss you, image =("></p>

<div><p>It starts to get interesting after creating the ext4 filesystem, and see this...</p></div>

<p><img src="https://buredoranna.github.io/assets/images/blockfile.nouserdata.ext4.png" alt="I miss you, image"></p>

<div><p>With this image we can can see the structure added by mkfs.ext4, and where on the drive the ext4 data is located.</p></div>

<p>Its worth noting this image doesn’t actually differentiate between "ext4 bytes" and "non-ext4 bytes". That is, there could be bytes owned by ext4, but if they are 0x00s they are color coded the same as any other 0x00... But even with this limitation, the image is interesting.
<br></p>

<div><p>But I still wanted an image which differentiated between ext4 data and "user" data. My solution was to create a file 1024 bytes in size from /dev/urandom, and copy that file to the mounted loop device. Then, in my visualization code, when reading the blockfile, I test if "the next 1024 bytes to be read" match "the 1024 bytes of the reference file", and if they match, color code those 1024 pixels accordingly.</p></div>

<div><p>And with user data copied to the drive, we get this:</p></div>

<p><img src="https://buredoranna.github.io/assets/images/blockfile.ext4.120.png" alt="I miss you, image"></p>

<div><p>Which I find very satisfying... But still, I wanted an animation. So I built an animated GIF.</p></div>

<div><p>Between each frame, the "user data" file is copied to the drive three times... so there are three copies written each frame... This makes for a more expressive animation and a smaller GIF than if each frame was a single 'cp' of the file.</p></div>

<div><p>I hope you enjoy this as much as I do.</p></div>



<div><p>And by way of comparison, here is a similar animation, but with ext2</p></div>


<hr>

<p><br>Here begins the ext4 rabbit hole...<br>
<a href="https://en.wikipedia.org/wiki/Ext4">Wikipedia</a>
<a href="https://ext4.wiki.kernel.org/index.php/Main_Page">ext4 wiki</a><a></a>
<a href="https://www.kernel.org/doc/html/latest/admin-guide/ext4.html">Admin Guide</a>
<a href="http://e2fsprogs.sourceforge.net/">e2fsprogs</a>
<a href="https://www.kernel.org/doc/html/latest/filesystems/ext4/index.html">ext4 Data Structures and Algorithms</a></p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database of 16,000 Artists Used to Train Midjourney AI Goes Viral (144 pts)]]></title>
            <link>https://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/</link>
            <guid>38907729</guid>
            <pubDate>Mon, 08 Jan 2024 02:57:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/">https://www.artnews.com/art-news/news/midjourney-ai-artists-database-1234691955/</a>, See on <a href="https://news.ycombinator.com/item?id=38907729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-1234691955">
		<header>
			<div>
	<nav data-dropdown="">

	
	<ul data-dropdown-list="">
					<li data-dropdown-list-item="">
				<span>
	<a href="https://www.artnews.com/">

	Home
	</a>
</span>
			</li>
					<li data-dropdown-list-item="">
				<span>
	<a href="https://www.artnews.com/c/art-news/">

	ARTnews
	</a>
</span>
			</li>
					<li data-dropdown-list-item="">
				<span>
	<a href="https://www.artnews.com/c/art-news/news/">

	News
	</a>
</span>
			</li>
			</ul>
</nav>
</div>

		</header>

		

		

		<div>

			<article>
				<div>
	

<figure>

			
<div>
	
			<p><img src="https://www.artnews.com/wp-content/uploads/2024/01/4f6e27fd85d4a899633e2228d076124f_1920_KR-wide.jpg?w=2000" alt="Metalwork Colossus by Hyan Tran, age 6, for a special drop of Magic the Gathering benefitting the Seattle Children's Hospital in 2021." srcset="https://www.artnews.com/wp-content/uploads/2024/01/4f6e27fd85d4a899633e2228d076124f_1920_KR-wide.jpg 2800w, https://www.artnews.com/wp-content/uploads/2024/01/4f6e27fd85d4a899633e2228d076124f_1920_KR-wide.jpg?resize=400,183 400w" sizes="(min-width: 87.5rem) 1000px, (min-width: 78.75rem) 681px, (min-width: 48rem) 450px, (max-width: 48rem) 250px" height="" width="" decoding="async">
			
			</p>
	
	</div>
	
			
			
<figcaption>
	
					<span>A database shows this image by Hyan Tran, age 6, was also scraped by Midjourney for its AI image generator.</span>
		
									<cite>©2023 Scalefast Inc.
Courtesy of Magic the Gathering Secret Lair</cite>
					
	</figcaption>

			
</figure>

</div>

				<div>
					<p>
	For many, a new year includes resolutions to do better and build better habits. For <a href="https://www.artnews.com/t/midjourney/" id="auto-tag_midjourney" data-tag="midjourney">Midjourney</a>, the start of 2024 meant having to deal with a circulating list of artists whose work the company used to train its generative artificial intelligence program.</p>



<p>
	During the New Year’s weekend, artists linked to a Google Sheet on the social media platforms X (formerly known as Twitter) and Bluesky, alleging that it showed how Midjourney developed a database of time periods, styles, genres, movements, mediums, techniques, and thousands of artists to train its AI text-to-image generator. Jon Lam, a senior storyboard artist at Riot Games, also posted several screenshots of Midjourney software developers discussing the creation of a database of artists to train its AI image generator to emulate.

	</p>
<section>
	

	<h2 id="section-heading">

	
		Related Articles
	
	</h2>


	
</section>




<p>
	https://x.com/JonLamArt/status/1741545927435784424?s=20</p>



<p>
	The 24-page list of artists’ names used by Midjourney as the training foundation for its AI image generator (Exhibit J) includes modern and contemporary blue-chip names,as well as commercially successfully illustrators for companies like Hasbro and Nintendo. Notable artists include Cy Twombly, Andy Warhol, Anish Kapoor, Yayoi Kusama, Gerhard Richter, Frida Kahlo, Andy Warhol, Ellsworth Kelly, Damien Hirst, Amedeo Modigliani, Pablo Picasso, Paul Signac, Norman Rockwell, Paul Cézanne, Banksy, Walt Disney, and Vincent van Gogh.

</p>



<p>
	Midjourney’s dataset also includes artists who contributed art to the popular trading card game Magic the Gathering, including Hyan Tran, a six-year-old child and one-time art contributor who <a rel="nofollow" href="https://secretlair.wizards.com/us/en/product/694985/extra-life-2021" target="_blank">participated in a fundraiser for the Seattle Children’s Hospital in 2021</a>. </p>



<p>
	<a rel="nofollow" href="https://bsky.app/profile/thephilfoglio.bsky.social/post/3khxc4wqgmt2e" target="_blank">Phil Foglio</a> encouraged other artists to search the list to see if their names were included and to seek legal representation if they did not already have a lawyer. </p>



<p>
	Access to the Google file was soon restricted, but <a rel="nofollow" href="https://web.archive.org/web/20231231203837/https://docs.google.com/spreadsheets/d/1MEglfejpqgVcaf-I-cgZ5ngV_MlaOTeGXAoBPJO69FM/htmlview#" target="_blank">a version has been uploaded to the Internet Archive</a>. </p>



<p>
	The list of 16,000 artists was included as part of a lawsuit amendment to a class-action complaint<a href="https://www.artnews.com/art-news/news/artists-class-action-lawsuit-against-ai-image-generator-midjourney-stability-deviantart-1234653892/"> targeted at Stability AI, Midjourney, and DeviantArt </a>and the submission of <a rel="nofollow" href="https://stablediffusionlitigation.com/pdf/00201/1-1-stable-diffusion-complaint-exhibits.pdf" target="_blank">455-pages of supplementary evidence</a> filed on November 29 last year. </p>



<p>
	The amendment was filed after a judge in California federal court <a href="https://www.artnews.com/art-news/news/judge-dismisses-several-copyright-allegations-against-ai-companies-in-artist-class-action-1234685299/">dismissed several claims</a> brought forth by a group of artists against Midjourney and DeviantArt on October 30. 

</p>



<p>
	The class-action copyright lawsuit was<a href="https://www.artnews.com/art-news/news/artists-class-action-lawsuit-against-ai-image-generator-midjourney-stability-deviantart-1234653892/"> first filed almost a year ago</a> in the United States District Court of the Northern District of California.</p>



<p>
	Last September, the US Copyright Review Board decided that an image generated using Midjourney’s software could not be copyright due to how it was produced. Jason M. Allen’s image had garnered the $750 top prize in the digital category for art at the Colorado State Fair in 2022. The win went viral online, but <a href="https://www.artnews.com/art-news/news/colorado-state-fair-ai-generated-artwork-controversy-1234638022/">prompted intense worry and anxiety among artists</a> about the future of their careers. </p>



<p>
	Concern about artworks being scraped without permission and used to train AI image generators also prompted researchers from the University of Chicago to create a digital tool for artists <a href="https://www.artnews.com/art-news/news/new-data-poisoning-tool-enables-artists-to-fight-back-against-image-generating-ai-companies-1234684663/">to help “poison” massive image sets</a> and destabilize text-to-image outputs.</p>



<p>
	At publication time, Midjourney did not respond to requests for comment from <em>ARTnews</em>.</p>










				</div>

				<div>
	<nav data-dropdown="">

						<h4 data-dropdown-list-item=""> Read More About:</h4>
			
	
</nav>
</div>
	

			</article>

			
		</div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone that fell from hole in Alaska 737 MAX flight is found, still open to Mail (499 pts)]]></title>
            <link>https://twitter.com/SeanSafyre/status/1744138937239822685</link>
            <guid>38907620</guid>
            <pubDate>Mon, 08 Jan 2024 02:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SeanSafyre/status/1744138937239822685">https://twitter.com/SeanSafyre/status/1744138937239822685</a>, See on <a href="https://news.ycombinator.com/item?id=38907620">Hacker News</a></p>
Couldn't get https://twitter.com/SeanSafyre/status/1744138937239822685: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[MotorOS: a Rust-first operating system for x64 VMs (303 pts)]]></title>
            <link>https://github.com/moturus/motor-os</link>
            <guid>38907568</guid>
            <pubDate>Mon, 08 Jan 2024 02:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/moturus/motor-os">https://github.com/moturus/motor-os</a>, See on <a href="https://news.ycombinator.com/item?id=38907568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Motūrus OS</h2>
<p dir="auto">Motūrus project builds a simple, fast, and secure operating system (Motūrus OS) for the cloud.</p>
<p dir="auto">In more specific terms, Motūrus OS (sometimes called Motor OS),
is a new operating system targeting virtual machine-based workloads such as web serving, "serverless", edge caching, etc.</p>
<p dir="auto"><a href="https://github.com/moturus/motor-os/blob/main/docs/screenshot.md">Screenshot</a></p>
<h2 tabindex="-1" dir="auto">Why?</h2>
<p dir="auto">At the moment, most virtualized production workloads run Linux.
While Linux has many advanced features that in many
situations mean it is the only reasonable OS choice, there are
several complications that make it not ideal, in theory,
for some virtualized workloads:</p>
<ul dir="auto">
<li>Linux is optimized for baremetal, which leads to inefficiencies
when it is used inside a VM that is running on a Linux host:
<ul dir="auto">
<li>duplicate block caches</li>
<li>duplicate page table walks</li>
<li>the host scheduler can preempt the VCPU holding a spinlock in the VM's kernel</li>
</ul>
</li>
<li>Linux is difficult to use:
<ul dir="auto">
<li>Docker, Nix OS, "serverless", etc. all exist because of Linux's complexity</li>
</ul>
</li>
<li>Linux has, historically, not been very secure</li>
</ul>
<p dir="auto">A new operating system built from ground-up with the focus
on virtualized workloads can be made much simpler and more
secure than Linux, while matching or exceeding its
performance and/or efficiency.</p>
<h2 tabindex="-1" dir="auto">What?</h2>
<p dir="auto">Motūrus OS is a microkernel-based operating system, built in
Rust, that targets virtualized workloads exclusively. It
currently supports x64 KVM-based virtual machines, and can
run in either Qemu or Cloud Hypervisor.</p>
<p dir="auto">Rust is <em>the</em> language of Motūrus OS: not only it is
implemented in Rust, it also exposes its ABI in Rust, not C.</p>
<h3 tabindex="-1" dir="auto">What works</h3>
<p dir="auto">While at the moment most of the subsystems are working in only
POC/MVP mode, they <strong>are</strong> working, and you can run, say, a web
server.</p>
<p dir="auto">More specifically, these things work:</p>
<ul dir="auto">
<li>boots via MBR (Qemu) or PVH (Cloud Hypervisor) in about 200ms</li>
<li>himem micro-kernel</li>
<li>scheduling:
<ul dir="auto">
<li>a simple multi-processor round robin (SMP)</li>
<li>in-kernel scheduling is cooperative
<ul dir="auto">
<li>the kernel is very small and does not block, so does not need to be preemptible</li>
</ul>
</li>
<li>the userspace is preemptible</li>
</ul>
</li>
<li>memory management:
<ul dir="auto">
<li>only 4K pages at the moment</li>
<li>stacks are guarded</li>
<li>page faults in the userspace work and are properly handled (only stack memory allocations are currently lazy)</li>
</ul>
</li>
<li>I/O subsystem (in the userspace)
<ul dir="auto">
<li>VirtIO-BLK and VirtIO-NET <a href="https://github.com/moturus/motor-os/tree/main/src/lib/virtio">drivers</a></li>
<li>two simple filesystems
(<a href="https://crates.io/crates/srfs" rel="nofollow">srfs</a> and
<a href="https://crates.io/crates/flatfs" rel="nofollow">flatfs</a>)</li>
<li><a href="https://crates.io/crates/smoltcp" rel="nofollow">smoltcp</a>-based networking (TCP only at the moment)
<ul dir="auto">
<li>a simple <a href="https://github.com/moturus/motor-os/tree/main/src/bin/httpd">httpd</a> is provided</li>
</ul>
</li>
</ul>
</li>
<li>the userspace:
<ul dir="auto">
<li>multiple processes, with preemption</li>
<li>threads, TLS</li>
<li>Rust's standard library <a href="https://github.com/moturus/rust/tree/moturus-2023-12-16">mostly ported</a>
<ul dir="auto">
<li>Rust programs that use Rust standard library and do not
depend, directly or indirectly, on Unix or Windows FFI,
will cross-compile for Motūrus OS and run, subject to
"what does not work" below</li>
</ul>
</li>
<li>a simple <a href="https://github.com/moturus/rush">unix-like shell</a> in the serial console</li>
</ul>
</li>
</ul>
<h3 tabindex="-1" dir="auto">What does not work</h3>
<p dir="auto">Most pieces are not yet ready for production use. No security
audit has been made. It is very easy to hit a "not implemented"
panic in sys-io (the userspace I/O subsystem).</p>
<p dir="auto">More specifically:</p>
<ul dir="auto">
<li>Filesystem: most Rust std::fs APIs have been implemented as
proof-of-concept, but are slow (synchronous) and will
have to be reimplemented using Motūrus async I/O</li>
<li>Networking:
<ul dir="auto">
<li>std::net::TcpStream is mostly implemented, but there are
todo! panics</li>
<li>other protocols are not implemented yet</li>
<li>performance can (and will) be better</li>
</ul>
</li>
<li>The ecosystem outside Rust std:
<ul dir="auto">
<li>crates like rand or rustls can be compiled and used
with minor tweaks</li>
<li>crates depending on async runtimes (e.g.
<a href="https://tokio.rs/" rel="nofollow">Tokio</a>) will not compile at the moment
<ul dir="auto">
<li><a href="https://github.com/tokio-rs/mio">Tokio Mio</a> should be
not too difficult to port</li>
</ul>
</li>
<li>crates that are wrappers around native Linux or Windows APIs
will not work, obviously</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">How can I build/run it?</h2>
<p dir="auto">See <a href="https://github.com/moturus/motor-os/blob/main/docs/build.md">docs/build.md</a>.</p>
<h2 tabindex="-1" dir="auto">Thanks</h2>
<p dir="auto">Big thanks to Philipp Oppermann for his great <a href="https://os.phil-opp.com/" rel="nofollow">Writing an OS in Rust</a> blog series - it has inspired a lot of people to experiment in this space.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a HTMX Playground 100% in the browser (430 pts)]]></title>
            <link>https://lassebomh.github.io/htmx-playground/</link>
            <guid>38906989</guid>
            <pubDate>Mon, 08 Jan 2024 01:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lassebomh.github.io/htmx-playground/">https://lassebomh.github.io/htmx-playground/</a>, See on <a href="https://news.ycombinator.com/item?id=38906989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="popup-top">
        <main id="popup">
            <article>
                
                
                <p>
                    A simple code sandbox for playing around with HTMX. No setup needed!
                </p>
                <p>
                    It allows you to write code in a backend-like environment, running entirely inside the browser. You can define endpoints within server.js and render your own templates. It will run a mock server that intersepts outgoing requests from HTMX. The request handling and templating engine should be very familiar to people who use Django. In principle, this project isn't specific to HTMX, so you are free to try out other libraries as well.
                </p>
                
                <p>
                    Check out the examples! I've adapted them from the original <a href="https://htmx.org/examples/">htmx.org examples</a>.
                </p>
                <h2>Saving &amp; sharing</h2>
                <ol>
                    <li>Press "Copy as JSON" in the top right.</li>
                    <li>Upload the contents as a Gist, and enter the raw URL in "Load Playground"</li>
                    <li>The URL on this page will update, and can now be shared.</li>
                </ol>
                <p>
                    The code is available <a target="_blank" href="https://github.com/lassebomh/htmx-playground">on GitHub</a>.
                </p>

                <h2>Limitations</h2>
                <ul>
                    <li>No page navigation</li>
                    <li>Limited mobile support</li>
                </ul>

                <h2>Libraries used</h2>
                <ul>
                    <li>Svelte</li>
                    <li>Ace (code editor)</li>
                    <li>PollyJS (mock server)</li>
                    <li>Nunjucks (templating engine)</li>
                </ul>
                
            </article>
        </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I made an app that runs Mistral 7B 0.2 LLM locally on iPhone Pros (278 pts)]]></title>
            <link>https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941</link>
            <guid>38906966</guid>
            <pubDate>Mon, 08 Jan 2024 01:00:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941">https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941</a>, See on <a href="https://news.ycombinator.com/item?id=38906966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
<!---->
<!---->
<!---->
    


<!---->
    


  <div dir="" data-test-bidi=""><p>Introducing Offline Chat, the next-generation AI ChatBot that runs entirely on your device without the Internet.&nbsp;You can use it anywhere, and your data stays private and secure.</p><p>While Offline Chat might not match the prowess of top-tier online models due to inherent memory and processing constraints, it stands out as an engaging and versatile tool. It's perfect for sparking creativity and assisting in various tasks such as writing, though it's advisable to verify facts independently.</p><p>The app requires a Pro iPhone with a minimum of 6GB of RAM. Only the following devices meet the requirement:</p><p>- iPhone 15 Pro, iPhone 14 Pro, iPhone 13 Pro, iPhone 12 Pro.<br>- iPads: Please check. RAM varies based on model and year. </p><p>For the technically oriented, the AI is a fine tuned large language model based on Mistral 7B 0.1, quantized to 3-bit.</p></div>

<!---->
  <section>
    <div>
      <h2>What’s New</h2>
        

    </div>
    <div>
          <p dir="false" data-test-bidi="">Updated model to Mistral 7B 0.2. This new model is truly extraordinary considering its size.</p>


      </div>
  </section>

      <section>
      <p>
        <h2>
          Ratings and Reviews
        </h2>

        <!---->
      </p>

        


      

<!---->    </section>


<!---->
<!---->
<!---->
  <section>
  <div>
    <h2>
      App Privacy
    </h2>

    


  </div>

  <p>
    The developer, <span>Opus Noma LLC</span>, indicated that the app’s privacy practices may include handling of data as described below. For more information, see the <a href="http://opusnoma.com/privacy">developer’s privacy policy</a>.
  </p>

  <div>
        
        <h3>Data Not Collected</h3>
        <p>The developer does not collect any data from this app.</p>
<!---->      </div>

    <p>Privacy practices may vary, for example, based on the features you use or your age. <a href="https://apps.apple.com/story/id1538632801">Learn&nbsp;More</a></p>
</section>


<section>
  <div>
    <h2>Information</h2>
    <dl>
        <p>
          <dt>Seller</dt>
          <dd>
              Opus Noma LLC
          </dd>
        </p>
        <p>
          <dt>Size</dt>
          <dd aria-label="3.3 gigabytes">3.3 GB</dd>
        </p>
        <p>
          <dt>Category</dt>
          <dd>
              <a href="https://itunes.apple.com/us/genre/id6000" data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;actionUrl&quot;:&quot;https://itunes.apple.com/us/genre/id6000&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;GenrePage&quot;}">
                Business
              </a>
          </dd>
        </p>
      <div>
        <dt>Compatibility</dt>
          <dd>
              <dl>
                <dt>
                  iPhone
                </dt>
                <dd>Requires iOS 17.0 or later.
                </dd>
              </dl>
              <dl>
                <dt>
                  iPad
                </dt>
                <dd>Requires iPadOS 17.0 or later.
                </dd>
              </dl>
              <dl>
                <dt>
                  Mac
                </dt>
                <dd>Requires macOS&nbsp;14.0 or later and a Mac with Apple&nbsp;M1&nbsp;chip or later.
                </dd>
              </dl>
          </dd>
      </div>
<!---->      
      <p>
        <dt>Age Rating</dt>
        <dd>
             12+
              <span>Infrequent/Mild Medical/Treatment Information</span>
              <span>Infrequent/Mild Mature/Suggestive Themes</span>
        </dd>
      </p>
<!---->      <p>
        <dt>Copyright</dt>
        <dd>© 2023 Opus Noma LLC</dd>
      </p>
        <p>
          <dt>Price</dt>
          <dd>$1.99</dd>
        </p>
<!---->
    </dl>
  </div>
  <div>
    <ul>
<!---->        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="http://opusnoma.com/">
            App Support
          </a>
        </li>
<!---->        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="http://opusnoma.com/privacy">
            Privacy Policy
          </a>
        </li>
    </ul>
  </div>
</section>

<section>
  <ul>
<!---->      <li>
        <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="http://opusnoma.com/">
          App Support
        </a>
      </li>
<!---->      <li>
        <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="http://opusnoma.com/privacy">
          Privacy Policy
        </a>
      </li>
  </ul>
</section>

  <section>
    <p>
      <h2>Supports</h2>
    </p>
    <ul>
        <li>
          <img src="https://apps.apple.com/assets/images/supports/supports-FamilySharing@2x-f58f31bc78fe9fe7be3565abccbecb34.png" alt="" role="presentation">
          <div>
              <h3 dir="ltr">
    Family Sharing
</h3>


              <h4 dir="">
        

                    <p data-test-bidi="">Up to six family members can use this app with Family&nbsp;Sharing enabled.</p>

    


<!----></h4>


          </div>
        </li>
    </ul>
  </section>

<!---->
    <section>
      <p>
        <h2>
          More By This Developer
        </h2>
        <!---->
      </p>

      
    </section>

    <section>
      <p>
        <h2>
          You Might Also Like
        </h2>
        <!---->
      </p>

      
    </section>


<!---->

<!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How An ASML Lithography Machine Moves a Wafer [video] (170 pts)]]></title>
            <link>https://www.youtube.com/watch?v=1fOA85xtYxs</link>
            <guid>38906881</guid>
            <pubDate>Mon, 08 Jan 2024 00:46:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=1fOA85xtYxs">https://www.youtube.com/watch?v=1fOA85xtYxs</a>, See on <a href="https://news.ycombinator.com/item?id=38906881">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How We Handle Cap Table Information (117 pts)]]></title>
            <link>https://henrysward.medium.com/how-we-handle-captable-information-c98d85d79277</link>
            <guid>38906749</guid>
            <pubDate>Mon, 08 Jan 2024 00:31:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://henrysward.medium.com/how-we-handle-captable-information-c98d85d79277">https://henrysward.medium.com/how-we-handle-captable-information-c98d85d79277</a>, See on <a href="https://news.ycombinator.com/item?id=38906749">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://henrysward.medium.com/?source=post_page-----c98d85d79277--------------------------------"><div aria-hidden="false"><p><img alt="Henry Ward" src="https://miro.medium.com/v2/resize:fill:88:88/1*-fzeFCY7lgBp4RqEXdKdVw.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="9bd8">On Friday we had an internal policy violation that affected three companies. I’ve been in touch with the founders and I’m appalled we made that mistake and it should never have happened. It is unacceptable and we’ve dealt with the violation on Saturday morning and are continuing the investigation to make sure it never happens again.</p><p id="dba1">Let me share our framework on data privacy and access controls to hopefully address concerns from this weekend. For a deeper dive, I will bucket data privacy into four buckets with different rules that I will cover separately.</p><p id="779c">1. <strong>Public Disclosures:</strong> We can only publish <em>aggregate and anonymous data</em>. So we can say things like there are 34K startups on Carta, or the average Series A startup has 25 employees, etc… However, we cannot say Acme Startup has 41 shareholders or the PPS is $13.24. You will see this type of aggregate anonymous information frequently in our data reports.</p><p id="8e1c">2. <strong>Internal Systems Disclosures:</strong> We can use cap table data for onboarding and internal systems development. So for example, we can load cap table data into dashboards for audit, we can write health checks to make sure cap table reports are correct, we can run machine learning algorithms to predict when you need a 409A, etc… We can use cap table data <em>to help us improve the software or customer experience</em>. This also includes things like when support teams access cap tables (through an approval and audit system) or when a customer needs help correcting or updating their cap table. All human access to cap tables is tracked and audited.</p><p id="e651">3. <strong>Sales &amp; Marketing:</strong> Lastly, we can market to our customers and users. For example, we can offer new products to help companies with employee compensation, taxes, and expense reporting. Occasionally we have offered products directly to employee shareholders. For example, in the past we have offered stock based loan products to employees of certain companies where employees can access loans to exercise their stock. But when we offer these products to employees we only do it in collaboration with the company. The company has to approve the program for their employees for us to offer it.</p><p id="5023"><strong>4. CartaX: </strong>CartaX is a separate product that operates as an opt-in marketplace where investors are invited to enter bids and asks on different companies. At any given time we have about one hundred companies that are in the marketplace. Where CartaX and the cap table business converge is if we match a trade in the marketplace, we go to the company and ask if they will allow it. If the company allows it, we use their cap table to execute the trade. If the company doesn’t allow it, we stop the trade. We do not and will never trade without company consent.</p><p id="81c5">In the case of Linear and two other companies, we had an internal breach of protocol and we contacted someone directly on the cap table. That never should have happened and is absolutely a breach of our privacy protocols. And we have addressed it over the weekend.</p><p id="2c11">The second mistake might be whether we are too close to the cap table business to be helping on liquidity. We started CartaX five years ago to help founders and companies with liquidity and it has mostly been a net positive for founders, employees, and shareholders. But even if we do everything perfectly and make zero mistakes, perhaps just the appearance of being in the liquidity business makes us seem compromised. Everything we do must be grounded in trust and if being in the liquidity business compromises that trust, perhaps we need to reevaluate that offering.</p><p id="62dc">I will think about this and come back with more thoughts in the coming months. If you have a perspective on whether Carta should be helping companies with liquidity, please reach out to me. I’d love to hear them.</p><p id="83f6">I’m sorry for scaring everybody about this. After ten years of managing cap tables across 40,000 startups, I promise we aren’t compromising anyone’s data. We won’t be here if you don’t trust us. Trust, transparency, and integrity is our most important currency. If you would like to chat with me more one-on-one, please email me at <a href="mailto:henry.ward@carta.com" rel="noopener ugc nofollow" target="_blank">henry.ward@carta.com</a> and we can set up a zoom.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Buffett once bet $1M that he could beat a group of hedge funds over 10 years (118 pts)]]></title>
            <link>https://finance.yahoo.com/news/warren-buffett-once-bet-1m-113000485.html</link>
            <guid>38906586</guid>
            <pubDate>Mon, 08 Jan 2024 00:07:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finance.yahoo.com/news/warren-buffett-once-bet-1m-113000485.html">https://finance.yahoo.com/news/warren-buffett-once-bet-1m-113000485.html</a>, See on <a href="https://news.ycombinator.com/item?id=38906586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><div><p><img alt="Warren Buffett once bet $1M that he could beat a group of fancy hedge funds over 10 years — and he crushed them with a technique requiring absolutely no investing skill. Here's what he did" src="https://s.yimg.com/ny/api/res/1.2/PAPJyRp1wWXzoalzy.3PIA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTQyNw--/https://media.zenfs.com/en/moneywise_327/2ced5299e5954d8176723c1e63fb13e7" data-src="https://s.yimg.com/ny/api/res/1.2/PAPJyRp1wWXzoalzy.3PIA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTQyNw--/https://media.zenfs.com/en/moneywise_327/2ced5299e5954d8176723c1e63fb13e7"></p></div><p><figcaption>Warren Buffett once bet $1M that he could beat a group of fancy hedge funds over 10 years — and he crushed them with a technique requiring absolutely no investing skill. Here's what he did</figcaption></p></figure><p>Known for their complex investment strategies, hedge funds are typically seen as exclusive options for the ultra-rich. However, you don’t have to be among the elite to achieve comparable, or even superior, returns.</p><p>According to legendary investor Warren Buffett, there’s a very simple strategy that has the potential to outperform these complex hedge funds. Buffett was at one point so confident about this strategy that he was willing to wager a million dollars on its effectiveness.</p><h2>Don’t miss</h2><ul><li><p>Commercial real estate has outperformed the S&amp;P 500 over 25 years. Here's how to diversify your portfolio <a href="https://moneywise.com/investing/alternative-investments/hedge-your-portfolio-with-commercial-real-estate?throw=C1DM1&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_without+the+headache+of+being+a+landlord" rel="nofollow noopener" target="_blank" data-ylk="slk:without the headache of being a landlord;elm:context_link;itc:0">without the headache of being a landlord</a></p></li><li><p>Finish 2023 stronger than you started: <a href="https://moneywise.com/retirement/retirement/alt-5-money-moves-you-should-make-before-the-end-of-2023?throw=C1DM2&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_5+money+moves+you+should+make+before+the+end+of+th" rel="nofollow noopener" target="_blank" data-ylk="slk:5 money moves you should make before the end of the year;elm:context_link;itc:0">5 money moves you should make before the end of the year</a></p></li><li><p>The US dollar has lost 87% of its purchasing power since 1971 — <a href="https://moneywise.com/investing/alternative-investments/gold-ira-secure-financial-future?throw=C1DM3&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_invest+in+this+stable+asset+before+you+lose+your+r" rel="nofollow noopener" target="_blank" data-ylk="slk:invest in this stable asset before you lose your retirement fund;elm:context_link;itc:0">invest in this stable asset before you lose your retirement fund</a></p></li></ul><p>In 2007, <a href="https://www.usatoday.com/story/money/markets/2018/01/02/warren-buffett-bet-against-hedge-funds-girls-charity/996993001/" rel="nofollow noopener" target="_blank" data-ylk="slk:Buffett bet a million dollars;elm:context_link;itc:0">Buffett bet a million dollars</a> that over the course of a decade, a simple S&amp;P 500 index fund would outperform a basket of hand-picked hedge funds. He picked the Vanguard 500 Index Fund Admiral Shares (VFIAX).</p><p>Hedge fund manager Ted Seides from Protégé Partners accepted the bet and picked five funds-of-funds. A fund-of-funds is a portfolio of funds that charges two layers of management fees.</p><p>The outcome? Buffett triumphed decisively.</p><p>Buffett <a href="https://www.berkshirehathaway.com/letters/2017ltr.pdf" rel="nofollow noopener" target="_blank" data-ylk="slk:shared;elm:context_link;itc:0">shared</a> the final scorecard of the bet in his 2017 shareholder letter. The S&amp;P 500 index fund he selected delivered a total gain of 125.8% during the decade, while the five funds-of-funds reported respective gains of 21.7%, 42.3%, 87.7%, 2.8% and 27.0% during the same period.</p><p>Buffett gave all proceeds to charity — and Girls Inc. of Omaha turned out to be the biggest winner of the bet.</p><h2>High returns, low fees</h2><p>This decade-long bet challenged the notion that complex and expensive investment methods always yield the best results.</p><p>After all, anyone can replicate Buffett’ strategy at a very low cost. The Vanguard index fund he picked has an expense ratio of just 0.04%.</p><p>The hedge funds, Buffett pointed out, come at a much higher cost to investors.</p><p>“Even if the funds lost money for their investors during the decade, their managers could grow very rich,” he wrote in the shareholder letter. “That would occur because fixed fees averaging a staggering 2.5% of assets or so were paid every year by the fund-of-funds’ investors, with part of these fees going to the managers at the five funds-of-funds and the balance going to the 200-plus managers of the underlying hedge funds.”</p><p>In the investing world, fees should not be overlooked — they can eat into your returns. In an op-ed for Bloomberg titled “Why I Lost My Bet With Warren Buffett,” Seides agreed with Buffett on the subject of hedge funds’ management fees.</p><p>“He is correct that hedge-fund fees are high, and his reasoning is convincing. Fees matter in investing, no doubt about it,” he <a href="https://www.bloomberg.com/view/articles/2017-05-03/why-i-lost-my-bet-with-warren-buffett" rel="nofollow noopener" target="_blank" data-ylk="slk:wrote;elm:context_link;itc:0">wrote</a>.</p><p><strong>Read more:</strong> 'It's not taxed at all': Warren Buffett shares the <a href="https://moneywise.com/life/lifestyle/hybrid-its-not-taxed-at-all?throw=C1HALF&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_%27best+investment%27+you+can+make+when+battling+infla" rel="nofollow noopener" target="_blank" data-ylk="slk:'best investment' you can make when battling inflation;elm:context_link;itc:0">'best investment' you can make when battling inflation</a></p><p>These days, many ETFs enable investors to track benchmark indices at minimal costs. For instance, the Vanguard S&amp;P 500 ETF (VOO), which follows the S&amp;P 500, has a low expense ratio of 0.03%. Similarly, the SPDR S&amp;P 500 ETF Trust (SPY) tracks the same index and carries an expense ratio of 0.0945%.</p><p>Does that mean every investor should abandon stock-picking and put all their money into index funds?</p><p>The answer varies based on the individual.</p><p>For someone like Buffett, making their own investment decisions could lead to significantly greater success. Consider this: from 1964 to 2022, Buffett’s Berkshire Hathaway <a href="https://www.berkshirehathaway.com/letters/2022ltr.pdf" rel="nofollow noopener" target="_blank" data-ylk="slk:delivered;elm:context_link;itc:0">delivered</a> an astounding overall gain of 3,787,464%, substantially outperforming the S&amp;P 500’s already impressive 24,708% return in the same timeframe.</p><h2>What to read next</h2><ul><li><p>Thanks to Jeff Bezos, you can now <a href="https://moneywise.com/investing/real-estate/invest-vacation-rental-homes?throw=C1WTRN1&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_cash+in+on+prime+real+estate" rel="nofollow noopener" target="_blank" data-ylk="slk:cash in on prime real estate;elm:context_link;itc:0">cash in on prime real estate</a> — without the headache of being a landlord. Here's how</p></li><li><p>Worried about the economy? Here are <a href="https://moneywise.com/top/alternative-investments?throw=C1WTRN2&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_the+best+shock-proof+assets" rel="nofollow noopener" target="_blank" data-ylk="slk:the best shock-proof assets;elm:context_link;itc:0">the best shock-proof assets</a> for your portfolio. (They’re all outside of the stock market.)</p></li><li><p>Rising prices are throwing off Americans' retirement plans — here’s <a href="https://moneywise.com/managing-money/retirement-planning/how-to-retire-early?throw=C1WTRN3&amp;utm_source=syn_oath_mon&amp;utm_medium=Z&amp;utm_campaign=38715&amp;utm_content=oath_mon_38715_how+to+get+your+savings+back+on+track" rel="nofollow noopener" target="_blank" data-ylk="slk:how to get your savings back on track;elm:context_link;itc:0">how to get your savings back on track</a></p></li></ul><p><em>This article provides information only and should not be construed as advice. It is provided without warranty of any kind.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["LibreOffice is better at reading old Word files than Word" (419 pts)]]></title>
            <link>https://eldritch.cafe/@sfwrtr/111716610017454919</link>
            <guid>38906331</guid>
            <pubDate>Sun, 07 Jan 2024 23:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eldritch.cafe/@sfwrtr/111716610017454919">https://eldritch.cafe/@sfwrtr/111716610017454919</a>, See on <a href="https://news.ycombinator.com/item?id=38906331">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MK1 Flywheel Unlocks the Full Potential of AMD Instinct for LLM Inference (122 pts)]]></title>
            <link>https://mkone.ai/blog/mk1-flywheel-amd</link>
            <guid>38906208</guid>
            <pubDate>Sun, 07 Jan 2024 23:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mkone.ai/blog/mk1-flywheel-amd">https://mkone.ai/blog/mk1-flywheel-amd</a>, See on <a href="https://news.ycombinator.com/item?id=38906208">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-yui_3_17_2_1_1703802076508_12216">
  <h3>AMD Enters AI Market with Instinct Series Accelerators</h3><p>The 2023 holiday season marked a significant milestone for the AI community with the launch of AMD’s eagerly anticipated Instinct MI300 series accelerator, showcasing their advanced CDNA 3 architecture. On paper, the MI300 has the potential to challenge NVIDIA’s market dominance for cloud AI workloads, bringing hope for genuine performance competition and leveling the playing field. AMD’s Achilles heel up to this point has been its lagging software ecosystem, however, recently there have been inroads into natively supporting AMD hardware on popular AI frameworks.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1703802231758_5288">
  <p>Once we realized what the AMD Instinct series cards were capable of, we challenged ourselves to port our LLM inference engine MK1 Flywheel to AMD. Having just achieved the best inference performance on NVIDIA hardware (see <a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready">our companion post</a>), we rolled up our sleeves and got to work.</p><p>To jump straight to the conclusion: we believe that with Flywheel, AMD looks to be a force to be reckoned with. For now, our results are on the AMD Instinct MI210, and we are excited to benchmark on MI300 as they become widely available.</p><h3>How does AMD Stack Up to NVIDIA? </h3><p>Before we take you through our journey with AMD Instinct and ROCm, let's start with the results. </p><p>For reasons explained later on, we profiled the AMD Instinct MI100 and MI210 cards, and here focus on the newer MI210 for comparison. On the NVIDIA side, we chose the RTX A6000 since it has a similar hardware specification based on TFLOPS, memory and power. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1704576951001_27949">
  <p><em>Inference performance for MK1 Flywheel and vLLM  (v0.2.6) for Llama-2-13B. The benchmarks measure the throughput (requests/second) against average latency (end-to-end round trip time to service the request) across an increasing number of asynchronous workers (mimicking users) for a given input prompt and output generation distribution. Typical distribution (left) has a max token context of 960 and max token generations of 80; Long prompt distribution (right) has max token context of 1800 and a max token generation of 120. </em></p><p>The results are clear: MK1 Flywheel on an AMD Instinct MI210 now rivals a compute-matched NVIDIA GPU (also running Flywheel). Moreover, Flywheel shows higher throughput across all tested workloads on both AMD and NVIDIA compared to vLLM. The increased performance translates into significant cost savings as the same GPU instance can now service more users. </p><p>For clients considering using AMD Instinct for LLM inference workloads at scale, <strong>please reach out.</strong></p><h3>Recap of MK1 Flywheel</h3><p>In <a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready">our companion post</a>, we introduced MK1 Flywheel, our enterprise LLM inference engine built for real-world enterprise applications. Working with our early adopters, we forged a performant inference solution that is now in commercial use servicing millions of active users for LLM chat applications. As a quick recap, Flywheel has</p><ol data-rte-list="default"><li><p>Unparalleled Throughput vs Latency characteristics.</p></li><li><p>Rapid auto scaling for optimized scaling up of inference on cloud platforms.</p></li><li><p>For enterprise customers: seamless integration into your stack with native PyTorch compatibility, and a drop in replacement for inference backends like vLLM, TensorRT-LLM or Hugging Face TGI. </p></li></ol><p>You can take Flywheel for a spin on <a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-wkonsb76xsee6"><strong>AWS SageMaker</strong></a><strong>. </strong>Currently it runs on NVIDIA backend, and we look forward to offering Flywheel on AMD backends.</p><p>Next up, we want to give you a behind-the-scenes journey of building the hardware and software components that brought Flywheel to life on AMD. Hope you have as much fun reading it as we had doing it!</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1703804286743_15230">
  <h3>Our Journey: Building out the Hardware</h3><p>Our early exploration of the AMD Instinct series began with the harsh reality check that these cards aren’t ubiquitous across cloud platforms (for now). In order to chalk up a quick existential proof, we grabbed an MI100 (CDNA 1 architecture) from the bargain bin on eBay. The exercise was fairly touch-and-go at the start. Since the card does not have active cooling, we jury-rigged a 3D-printed fan hood onto the card to boot the card on our typical workstation desktops. Once everything was plugged in, only then did we realize that the system had no video output! Fortunately, we rummaged a relic in the form of an NVIDIA TitanX, and strapped it in. The workstation was rounded out with an Intel Xeon CPU, for reasons that will require its own blog post. Once we had this chimera of a system up and running, the absurdly loud high-static pressure fan did nothing to curb our enthusiasm when we generated text off a LLama-2-7B. We have come a long way since then.</p><p>A few weeks later we were able to land an MI210 (CDNA 2 architecture). With the lessons learned from the MI100, we had it up and running in no time!</p><h3>Our Journey: Building out the Software</h3><p>AMD has truly stepped up their game on their ROCm software stack over the past year, and it shows. We now have tight integration into bread and butter AI frameworks like PyTorch and TensorFlow. And, for the first time, you can effortlessly run LLMs right off of Hugging Face with a few lines of python. </p><p>As we designed MK1 Flywheel for the NVIDIA platform, we learned priceless lessons along the way and grew the intuition and engineering necessary to distinctly push NVIDIA GPUs to their limits. Energized by the momentum we saw on the AMD platform and community, we challenged ourselves to put our theories and engineering to the test. Once again, staying true to our performance-obsessed nature, we took no shortcuts and built the framework backend from first principles for the AMD stack.</p><p>While the interesting idiosyncrasies of the CDNA architecture flavored our ROCm kernel stack to be understandably different from our CUDA stack, the fundamentals that unlocked the true potential of the GPU hardware remained consistent. To start, we had to grok the CDNA architecture and its history (GCN - Graphics Core Next), alongside the complementary graphics-forward RDNA architecture. In parallel, we had to discover the strengths and limits of the compiler, especially on a compute pipeline that relies on instruction counting. Certain system-level techniques like optimized kernel scheduling worked right away due to being platform agnostic by nature. However, all core device kernels that used platform-specific features (say, NVIDIA Tensor Cores) had to be written from scratch to use the equivalent on the AMD hardware (AMD Matrix Cores).</p><p>In our experience so far, ROCm is in great shape to build <em>functional</em> kernels right off the bat on AMD hardware. Yet, to extract the most out of the CDNA architecture, we've had to go full manual, in comparison to our experience with CUDA. In all fairness, this can be marked up to the fact that NVIDIA has had a significant head start with CUDA for AI workloads. We believe what our work demonstrates, is that after equalizing the playing field on the software front with MK1 Flywheel, the AMD Instinct Series is a serious contender for today’s cloud inference workloads. We are excited to continue developing on ROCm, and extending Flywheel for MI300, as well as further optimizing for finetuning and training workloads. </p><h3>Results: MK1 Flywheel on AMD Instinct MI210 and MI100 </h3><p>We further benchmarked Flywheel on AMD Instinct across Mistral-7B and Llama-2-13B for various workloads. These results confirm that Flywheel has excellent performance across different LLM use cases.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1704576951001_125452">
  <p><em>Inference performance for MK1 Flywheel and vLLM  (v0.2.6) for popular models. Data distribution characteristics are described further in </em><a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready"><em>our companion post</em></a><em> . MI100 measurements are only provided for MK1 Flywheel, since vLLM does not officially support the CDNA 1 architecture.</em></p><h3>Dev Notes for ROCm</h3><p>There are a few things that we do miss from the CUDA stack. For example, the tight-loop performance debugging afforded by NVIDIA Nsight Compute was invaluable as we developed our inference stack for NVIDIA. Omnitrace and Omniperf on the ROCm stack got us some of the way but weren’t as polished as their counterparts. The documentation could use some love as well, as our current experience working on the ROCm stack was like driving a stick shift. On the plus side, the fact that ROCm toolkit and the AMD LLVM project are entirely open source allowed us to parse the true nature of the hardware and build design patterns. Conversely, CUDA is entirely closed.</p><p>The ROCm platform offers <a href="https://github.com/ROCm/HIPIFY">HIPIFY</a>, a convenient utility that converts CUDA code to cross-platform HIP code. However, to extract the most out of the CDNA architecture, we couldn’t “<em>hipify</em>” our CUDA kernels tuned for current NVIDIA architectures. This is where our efforts of building a framework from first principles really paid off, as we applied our learnings and theories onto the new architecture and reaped the results. </p><h3>What's next?</h3><p>The journey has only begun, and there is a lot more work to be done. Despite the VRMs screeching for their dear lives, the GPUs are not on fire… yet. As indicated in <a href="https://mkone.ai/blog/mk1-flywheel-race-tuned-and-track-ready">our companion post</a>, there are a good deal of stack optimizations left on the table in order to maximize inference performance. MK1 Flywheel aims to be platform agnostic, offering performance parity across GPUs with similar hardware specifications, i.e. <em>you will get your TFLOPs worth</em>. This opens up a wider range of cloud hardware platforms and economic mobility to serve your inference applications, while retaining the familiar frontend and user experience. For users planning enterprise deployments, we have different options for integrating MK1 Flywheel natively into your production stack, regardless of platform. <a href="https://mkone.ai/contact"><strong>Don't hesitate to reach out!</strong></a></p><p>We are eager to explore the MI300X accelerator, and in theory, MK1 Flywheel should perform out-of-the-box for CDNA3, as we have prepared for it. The bigger hurdle is access to the new hardware. The MI300A APU has us looking forward to the converged strength of the Zen4 CPU and CDNA3 GPU.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1704517194203_204206">
  <p>A <a href="https://embeddedllm.com/blog/vllm_rocm/">post from EmbeddedLLM dated Oct-27-2023</a> claims that AMD Instinct MI210 achieves LLM inference parity with the NVIDIA A100. There seems to be an unexplained discrepancy, considering that the NVIDIA A100 has 312 TFLOPS compared to the 181 TFLOPS of the Instinct MI210. In contrast, MK1 Flywheel achieves a proportional increase in performance on the A100 which has 1.7x the horsepower of the Instinct MI210, which can be seen in Throughput vs Latency comparisons. </p><p>We believe the EmbeddedLLM results may simply be out-of-date as an older version of vLLM (v0.1.4) was used as the base to develop ROCm support, and may not have included performance improvements from more recent versions. As of <a href="https://docs.vllm.ai/en/latest/getting_started/amd-installation.html">v0.2.4</a>, vLLM natively supports AMD Instinct MI200 series GPUs.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Does the Cerebellum Do? (314 pts)]]></title>
            <link>https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway</link>
            <guid>38905898</guid>
            <pubDate>Sun, 07 Jan 2024 22:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway">https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway</a>, See on <a href="https://news.ycombinator.com/item?id=38905898">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png" width="750" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:750,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:102187,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77e9eefd-34d2-4744-9b19-53981bb68e8f_750x500.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>This is the cerebellum. Its name means “little brain” — it’s a whole other brain under your “big” one.</p><p>If you vaguely remember something about what the cerebellum does, you’re probably thinking something to do with balance. Medical students have to learn the “cerebellar gait” that results from cerebellar injury (it’s the same staggering gait that drunk people have, because alcohol impairs the cerebellum.)</p><div id="youtube2-FFki8FtaByw" data-attrs="{&quot;videoId&quot;:&quot;FFki8FtaByw&quot;,&quot;startTime&quot;:&quot;63s&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/FFki8FtaByw?start=63s&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>A more detailed neurological exam of a patient with cerebellar disease shows a wider variety of motor problems.</p><div id="youtube2-Gn3AcxSn-Dc" data-attrs="{&quot;videoId&quot;:&quot;Gn3AcxSn-Dc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/Gn3AcxSn-Dc?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>Here you’ll notice that the patient can’t bring his finger to his nose or clap his hands without a wobbling back-and-forth motion; that his eyes “wobble” back and forth (which is called </span><a href="https://en.wikipedia.org/wiki/Nystagmus" rel="">nystagmus</a><span>); and that he wobbles backwards and forwards while standing or walking, so that he nearly falls over and needs a broad-based gait to support himself.</span></p><p><span>In cerebellar disease, muscle tone is diminished (people are “floppy”), movements are not fluent (each individual sub-movement is separate), there’s </span><a href="https://en.wikipedia.org/wiki/Dysmetria" rel="">dysmetria </a><span>(failure to “aim” or estimate the distance to move, overshoot or undershoot) and there’s “</span><a href="https://www.ncbi.nlm.nih.gov/books/NBK560642/#:~:text=Intention%20tremor%20is%20defined%20as,worsening%20before%20reaching%20the%20endpoint." rel="">intention tremor</a><span>” (high amplitude, relatively slow wobbles that arise when the patient starts to move, as contrasted with the “resting tremor” characteristic of e.g. Parkinson’s disease.)  </span></p><p><span>Clearly, the cerebellum does something to control movement, and movement is impaired when it is damaged. But </span><em>why do we need a whole other “little brain” to control these aspects of movement</em><span>? </span></p><p>There are already regions of the cerebrum (or “forebrain”) dedicated to movement, like the motor cortex and the basal ganglia. And you can do a lot of movement using just those!</p><p><span>Even in the rare cases known as cerebellar agenesis, where a person is born totally lacking a cerebellum</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-1-140098227" target="_self" rel="">1</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-2-140098227" target="_self" rel="">2</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-3-140098227" target="_self" rel="">3</a></span><span>, movement is still possible, just impaired: slow motor and speech development in childhood, abnormal spoken pronunciation, wobbly limb movements, and mild-to-moderate intellectual disability. But </span><em>not </em><span>paralysis, and not even particularly bad disability overall — a lot of these people were able to live independently and work at jobs.</span></p><p>So…that’s weird. </p><p><span>What is the cerebellum’s job?  It seems weird to have a </span><em>whole separate organ </em><span>for “make motor and cognitive skills work somewhat better.”  </span></p><p>The other weird thing about the cerebellum is anatomical.</p><p>These very large, complex neurons  are the Purkinje cells, which exist only in the cerebellum.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg" width="611" height="715" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:715,&quot;width&quot;:611,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:186474,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3269025e-62c8-4c50-a148-8ee395493397_611x715.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Illustration by Santiago Ramon y Cajal</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png" width="462" height="376" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:376,&quot;width&quot;:462,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:90330,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1cedf1a-27b7-4b5b-a280-4d33673e2e36_462x376.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Notice how the Purkinje cell (e) is much more complex than the other neuron types. </figcaption></figure></div><p>They have hundreds of synapses each, unlike the neurons of the cerebrum which only have a few.</p><p><span>Most of the other cells in the cerebellum are the small granule cells — in fact, they are so numerous that they comprise more than half of all neurons in the whole human brain.  In total, the cerebellum contains 80% of all neurons!</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-4-140098227" target="_self" rel="">4</a></span></p><p>If you were an alien with a microscope who knew nothing about neurology, your first assumption would be “ah yes, the thinking happens in the cerebellum.”</p><p>Why is there so much neuronal complexity dedicated to….making movement a bit smoother and “higher” cognition a bit better?</p><p><span>The third weird fact is that the </span><em>size </em><span>of the cerebellum has been growing throughout primate evolution and human prehistory, </span><em>faster </em><span>than overall brain size.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-5-140098227" target="_self" rel="">5</a></span></p><p><span>Great ape brains are distinguished from monkey brains by their larger frontal </span><em>and cerebellar </em><span>lobes.  The Neanderthals had bigger brains than us but smaller cerebella. And, most strikingly, modern humans have much bigger cerebella than “anatomically modern” Cro-Magnon humans of only 50,000 years ago (but relatively </span><em>smaller </em><span>cerebral hemispheres!) </span></p><p>An alien paleontologist could be forgiven for assuming “ah yes, the cerebellum, the seat of the higher intellect.”</p><p><span>The cerebellum </span><em>looks </em><span>like it should have some crucial unique function. Something key to “what makes us human.”  But what could it be?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg" width="600" height="588" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:116735,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e806a80-1623-4134-91cc-07873b7e920b_600x588.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Some of Ivan Pavlov’s famous dogs</figcaption></figure></div><p>Classical conditioning — the thing that makes a dog salivate when it hears a bell it’s learned to associate with food — is a very low-level process.</p><p>You don’t need a lot of brain for classical conditioning. </p><p><span>You can classically condition the sea slug </span><em>Aplysia, </em><span>which has only 20,000 neurons, to flinch from a neutral sensation it’s learned to associate with a painful one.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-6-140098227" target="_self" rel="">6</a></span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg" width="400" height="289" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:289,&quot;width&quot;:400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:26821,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F680ac9db-3de8-4a5f-bd7c-e692df9f8967_400x289.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Aplysia californica </em><span>releasing a toxic cloud in self-defense</span></figcaption></figure></div><p><span>Even single cells can exhibit learning. The giant slime mold amoeba </span><em>Physarum</em><span> can be “trained” to cross a noxious part of a petri dish to reach food on the other side.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-7-140098227" target="_self" rel="">7</a></span></p><p><span>However, in vertebrates, classical conditioning is highly localized in the nervous system: the cerebellum is </span><em>necessary and sufficient </em><span>for learning conditioned responses.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-8-140098227" target="_self" rel="">8</a></span></p><p>A standard test of classical conditioning is the eyeblink test: can the subject learn to associate a neutral stimulus with an unpleasant one (like a puff of air to the eye) and automatically blink in response to the neutral stimulus.</p><p><span>If you lesion the cerebellum in animals, they no longer exhibit eyeblink conditioning. Humans with cerebellar damage also have no eyeblink conditioning.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-9-140098227" target="_self" rel="">9</a></span></p><p><span>Conversely, if you want classical conditioning, </span><em>all you need </em><span>is a cerebellum — in fact, all you need for classical conditioning is the Purkinje cells!</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-10-140098227" target="_self" rel="">10</a></span><span> </span></p><p>The eyeblink response is governed by the Purkinje cells. When an animal feels a puff of air to the eye, the Purkinje cells stop firing temporarily, resulting in an eyeblink. </p><p>So, in 2007, some Swedish researchers decided to study this response in isolation. Here’s how it works.</p><p><span>You take a “decerebrate” ferret — i.e. a ferret with the whole cerebrum severed from the rest of the brain (consisting of the brainstem and cerebellum). You condition individual Purkinje cells with electrode stimulation of two types of neurons that form their inputs: the climbing fibers (unconditioned stimulus) and the mossy fibers (conditioned stimulus.)  Stimulating the climbing fibers (the same thing that happens naturally when a puff of air hits the eye) causes a temporary suppression of Purkinje cell firing; stimulating the mossy fibers does not. </span><em>But, </em><span>if the mossy fibers are stimulated </span><em>right before </em><span>the climbing fibers, the Purkinje neurons </span><em>learn </em><span>to anticipate the association and suppress their firing in response to the mossy fiber stimulation.  This is a textbook example of classical conditioning.</span></p><p>Are cerebellar Purkinje cells the only individual neurons capable of single-cell learning?</p><p><span>Not in all animals; the sensory neurons of the sea slug </span><em>Aplysia </em><span>can be classically conditioned.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-11-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-11-140098227" target="_self" rel="">11</a></span><span>  But I haven’t been able to find a study in vertebrates of single-cell classical conditioning or associative learning outside the cerebellum. (Other regions of the brain, of course, are involved in learning, but the learning might be occurring  through </span><em>multicellular </em><span>information like the relationships between nearby neurons’ firing patterns.)</span></p><p>Why do we care?</p><div><p><span>First of all, this gives us at least one “unique job” for the cerebellum: it is the place in the brain where conditioned associations are learned.</span></p><p><span>Second of all, it </span><em>disproves </em><span>the </span><a href="https://en.wikipedia.org/wiki/Long-term_potentiation" rel="">long-term potentiation </a><span>theory that learning in the brain happens exclusively through strengthening synaptic connections between neurons. (The old dictum that “neurons that fire together, wire together.”)  The brain is </span><em>not </em><span>like a neural network where the only thing that is “learned” or “updated” is the weights between neurons. At least some learning evidently happens </span><em>within individual neurons. </em></p></div><p><span>That’s bad news for anyone hoping to simulate a brain digitally.  It means there’s a lot more relevant stuff to simulate (like the learning that goes on within cells) than the </span><a href="https://en.wikipedia.org/wiki/Connectionism" rel="">connectionist </a><span>paradigm of treating each biological neuron like a neural-net “neuron” would imply, and thus the computational requirements of simulating a brain are higher — maybe vastly higher — than connectionists hope.</span></p><p><span>On the other hand, intracellular learning is </span><em>great </em><span>news for neuroscientists trying to discover exactly how learning works inside a brain. If you’re studying an example of learning (or classical conditioning) that occurs within a single neuron, then the long-hypothesized “engram”, or physical object corresponding to a piece of newly learned information, has to reside </span><em>inside that neuron</em><span>. Something needs to physically or chemically </span><em>change </em><span>in that neuron, representing the newly learned information, and  causing the corresponding change in the neuron’s firing behavior.</span></p><p><span>Not only can individual Purkinje cells in the cerebellum be classically conditioned, they can also learn information about the </span><em>timing </em><span>of stimuli.</span></p><p><span>If the time interval between the conditioned stimulus and unconditioned stimulus is varied, the Purkinje cells learn to suppress their firing at </span><em>different times </em><span>to match.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-12-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-12-140098227" target="_self" rel="">12</a></span><span> </span></p><p>In other words, if the Purkinje cell has “learned” from experience that an aversive stimulus will typically follow the neutral stimulus in exactly 50 milliseconds, then the Purkinje cell will delay roughly that long after receiving the neutral stimulus before pausing its firing.  If the Purkinje cell “learns” a longer delay, it’ll wait longer before the pause. </p><p><span>This means that something inside the Purkinje cell is capable of representing </span><em>number </em><span>or </span><em>quantity</em><span>. </span></p><p><strong>Cerebellar Purkinje cells, in other words, can count. (Or measure.)</strong></p><p>The fact that individual cerebellar Purkinje neurons contain the ability to measure quantities is highly suggestive about the function of the cerebellum, given the symptoms of cerebellar disease. </p><p><span>Dysmetria, or failure to estimate the right distance and/or speed to move, is a typical symptom of damage to the cerebellum. If </span><em>measurement </em><span>(especially motor timing) is localized to the cerebellum, then dysmetria as a cerebellar deficit makes perfect sense.</span></p><p>Other symptoms of cerebellar damage are easy to understand as consequences of dysmetria. Intention tremor (those big back-and-forth wobbles when the patient tries to move) might simply be what happens when dysmetria causes the movement to initially overshoot, and then an attempt to correct the overshoot itself overshoots and swings in the other direction, and so on. Nystagmus could be the same thing, with the muscles of the eye.  </p><p><span>Even the non-motor symptoms of cerebellar damage, like poor performance on cognitive tests, have sometimes been referred to as a kind of “dysmetria of thought.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-13-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-13-140098227" target="_self" rel="">13</a></span><span> </span></p><p><span>Cerebellar patients have trouble with planning tasks (like the </span><a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi" rel="">Tower of Hanoi</a><span> puzzle), spatial reasoning tasks, and executive function tasks; you might suppose that mentally “gauging” how long to spend doing things, or “relating” subtasks to the whole, as well as understanding how objects fit together in space, are aspects of mental “measurement”.  </span></p><p><span>Cerebellar patients have normal abilities to make grammatical sentences, but make strange errors in generating </span><em>logical </em><span>sentences.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-14-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-14-140098227" target="_self" rel="">14</a></span><span> Things like:</span></p><ul><li><p>“It’s a big job, but it’s not easy.”</p></li><li><p>“If you drive, it will be less crowded”</p></li><li><p>“Although it’s the wrong size for you, I’ll ask to have one that will fit you.”</p></li><li><p>“I never thought I would meet you here; Nor did I, because everything seems so fresh here to buy.”</p></li></ul><p>Conjunctions (like “but”, “although”, “because”, “if”) represent particular logical relationships between parts of sentences. The cerebellar patients’ errors imply a specific impairment in the ability to make those conjunctional relationships.</p><p><span>A sentence that begins “it’s a big job, </span><em>but</em><span>” ought to end with something that would ordinarily seem to conflict with the claim “it’s a big job”; the cerebellar patient instead ended the sentence with something that typically </span><em>reinforces </em><span>the claim (“big jobs” are </span><em>usually </em><span>“not easy”). This error violates the expected logical relationship between clauses.</span></p><p>In a sense this is analogous to a problem in “spatial” or “part-whole” relating — the cerebellar patient has trouble constructing sentences whose clauses relate in the right way to match the conjunction between them. It’s analogous to the way cerebellar patients have the basic sensorimotor ability to do all the same individual movements that healthy people do, but they have trouble sequencing them fluently, “putting the pieces together” in the right way.</p><p>Of course, other areas of the brain are involved in “measurement” or “quantity” activities too. Visual areas of the brain “measure” spatial distances, auditory areas “measure” pitch and rhythm, various parts of the cerebral cortex are active in mental arithmetic, etc. So it’s not that the cerebellum is the sole region that “measures” or “relates” things. But there is something measurement-related going on.</p><p><span>Another broad theory of what the cerebellum is “for” is </span><em>anticipation </em><span>or </span><em>preparation</em><span>.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-15-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-15-140098227" target="_self" rel="">15</a></span></p><p><span>This fits well with the fact that classical conditioning happens in the cerebellum. Classical conditioning consists of learning to </span><em>expect </em><span>one stimulus to follow another, and respond </span><em>in anticipation </em><span>of the expected stimulus.</span></p><p><span>It also fits well with the cerebellum’s role in motor planning and sequencing.  Fluent movement requires unconsciously, rapidly forming intentions to do </span><em>multiple different things in sequence </em><span>without having to stop to think between steps.</span></p><blockquote><p>Gordon Holmes (1939) quotes a cerebellar-lesioned patient as saying, "The movements of my left (unaffected) arm are done subconsciously, but I have to think out each movement of the right (affected) arm. I come to a dead stop in turning and have to think before I start again."</p></blockquote><p><span>This kind of cerebellar damage symptom represents a failure of the </span><em>anticipatory </em><span>or </span><em>preparatory </em><span>function that automatically “gets ready” for the next step before the last one is complete.</span></p><p><span>Patients with cerebellar damage have impaired ability to switch their attentional focus (for instance, between a task that requires watching for visual cues and one that requires listening for auditory cues) but unimpaired ability to perform similar tasks that don’t require shifting focus. Patients with brain lesions </span><em>outside </em><span>the cerebellum didn’t have problems with task-switching.</span></p><p>This also suggests that the cerebellum is involved in the “readiness” or “anticipation” prior to making a mental action.</p><p>Also, experimentally stimulating the cerebellum in animals makes them more sensitive to subsequent sensory stimulus, further supporting the hypothesis that the cerebellum helps organisms “prepare to pay attention”.</p><p>The Purkinje neurons of the cerebellum, in particular, are involved in “preparatory” motor adjustments, such as altering one’s grip on an object in anticipation of the experimenter moving it.</p><p>If the function of the cerebellum is fully general “anticipatory” or “predictive” modeling, this would explain why it’s so important, especially in primate and hominid evolution. Dexterity (tool use, throwing) has obviously been selected for in our primate and hominid ancestors, and so have the general cognitive abilities to predict and make sense of a changing world.</p><p>It also would explain why people without a cerebellum are still capable of most of the same tasks as healthy people, just with worse performance. </p><p><span>People who lack a cerebellum are </span><em>wholly </em><span>incapable of eyeblink classical conditioning, but they’re not wholly incapable of learning or memory; they learn new skills </span><em>slowly</em><span> but they do learn them and they don’t have total amnesia. </span></p><p><span>This would make sense if they are unable to learn to </span><em>instantly anticipate </em><span>context to prepare for upcoming actions, but they are able to learn and form memories through a slower, noisier route using the cerebrum.  They can perform most of the same skills as healthy people, but they have to adjust “by trial and error” instead of using anticipation to get things right the first time, so they’re slower and less accurate.</span></p><p><span>Another way of phrasing this is that the cerebellum is a </span><em>forward model</em><span> of the consequences of action.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-16-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-16-140098227" target="_self" rel="">16</a></span><span> </span></p><p><span>The cerebellum receives an </span><a href="https://en.wikipedia.org/wiki/Efference_copy#:~:text=In%20physiology%2C%20an%20efference%20copy,by%20an%20organism's%20motor%20system." rel="">efference copy </a><span>of motor commands generated by the motor cortex, and uses its model to predict the sensory consequences; it can then compare predicted vs. actual data and error-correct.  A fast, subconscious path for sensorimotor prediction and learning (including classical conditioning) is confined to the cerebellum alone; a slower path includes both the cerebellum and cerebrum. </span></p><p><span>It takes hundreds of milliseconds to consciously perceive sensory information — far too slow a timescale to allow finely tuned and responsive motion that adapts to sensory feedback. Motion in real time has to be shaped and controlled by something faster than sensory processing through the long chains of neurons used for e.g. image recognition in the cortex — but moving totally “blind” without feedback control from </span><em>anything </em><span>would result in unacceptably crude, sloppy, choppy movement. The solution, perhaps, is the “virtual reality” generated by the cerebellum, a predictive model of the world that runs faster than the senses.</span></p><p><span>All vertebrates have a cerebellum.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-17-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-17-140098227" target="_self" rel="">17</a></span></p><p>If you remove the cerebellum from a dogfish, it can still swim, but it has a tendency to “stall” and difficulty judging its turns, so that it often bumps into the sides of the tank.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg" width="800" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:44566,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4d91734-5ee7-4d76-aff4-944952b5c434_800x800.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Pacific spiny dogfish</figcaption></figure></div><p><span>The cerebellum is especially enlarged in fish with electrolocation abilities, like the Peters’s elephant-nose fish </span><em>Gnathonemus petersii, </em><span>an African freshwater fish that uses electricity-sensing receptors all over its body to navigate around obstacles.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg" width="736" height="608" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:608,&quot;width&quot;:736,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:59387,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72eb00bd-0b20-412d-88c4-4bae0bec0b53_736x608.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Peter’s elephant-nose fish</figcaption></figure></div><p>This fish uses its huge cerebellum to monitor the electrical signatures of moving objects in the water around it. Particular Purkinje cells in the elephant-nose fish cerebellum are sensitive to particular target distances and speeds, just as particular neurons in the mammalian visual cortex are sensitive to the shape, location, and angle of visual features.</p><p>This unusual electrosensing function reinforces that the cerebellum is not just a motor control organ, but has a more general function related to spatial and environmental awareness.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif" width="1400" height="809" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:809,&quot;width&quot;:1400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:108045,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/avif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa95a5e5e-981d-467a-b3b2-fe4e0cc03f0a.avif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The platypus also uses its cerebellum to keep track of electrical signals </figcaption></figure></div><p>Monotremes such as platypuses also have an unusually large cerebellum; like the elephant-nose fish, the platypus is electrosensitive (in its beak) which it uses to detect swimming prey in the water.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg" width="1024" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109585,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80d4e5a0-2896-4eda-b36e-1a807cb1785c_1024x629.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The fin whale, whose cerebellum is massively expanded</figcaption></figure></div><p>Aquatic mammals — whales, dolphins, seals, and sea lions — are also notable for their enlarged cerebellums, especially the baleen whales. In marine mammals, the cerebellum is used for echolocation.  Echolocating bats also use the cerebellum for navigation, though their overall cerebellum size relative to brain size is small.</p><p>In many mammals, large areas of the cerebellum are devoted to processing sensory and motor information for parts of the body that are particularly dexterous and used in exploration: the snout in rodents, the hands in primates, the tip of the tail in arboreal monkeys.</p><p>Across animals, the cerebellum seems to be involved in both motion and sensory perception, and intriguingly, seems to be particularly enlarged in animals that use echolocation or electrosensing in the water, for spatial awareness of object locations in all directions.</p><p>This is suggestive of something like “spatial world modeling” going on in the cerebellum, and is consistent with the theory that the cerebellum’s job is anticipation and preparation. </p><p><span>Echolocation and electrosensing are both sensory modalities that involve an organism generating a “field” around itself (of electric or sound waves) and perceiving objects in the patterns of disruption in that field. Unlike vision, hearing, and smell, they are “</span><a href="https://en.wikipedia.org/wiki/Active_sensory_systems" rel="">active sensory systems</a><span>”, in which the organism can control the intensity, direction, and timing of the “probe” signal (the sound or electric signal they emit). </span></p><p>Touch can have an analogous “actively controllable” quality, as the animal reaches out a snout, hand, or tail to explore its nearby surroundings. But truly active sensory systems, unlike touch, allow an animal to explore and probe at a distance, and gain a 3D model of its whole surrounding world. </p><p>While humans don’t have these kinds of sensory systems, it may provide some intuition for what our cerebellum is doing; perhaps building implicit anticipations of where everything in the physical world around us, and how it will respond if “poked”. </p><p><span>If you try to map regions of the cerebellum by function and by functional connectivity</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-18-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-18-140098227" target="_self" rel="">18</a></span><span> you get a close one-to-one match with the cerebrum, even down to the localization of language in one hemisphere (the left hemisphere of the cerebrum and the right hemisphere of the cerebellum; everything’s flipped.)</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-19-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-19-140098227" target="_self" rel="">19</a></span></p><p><span>The only parts of the cerebral cortex </span><em>without </em><span>a corresponding cerebellar region are the auditory and visual cortices. (Suggestive, given that hearing and vision are </span><em>passive </em><span>senses, and my hypothesis in the previous section that the cerebellum has something to do with </span><em>active </em><span>sensing.)</span></p><p><span>The cerebellum has a repeated, almost crystal-like neural structure: it’s divided into multiple identical parallel modules. </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-20-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-20-140098227" target="_self" rel="">20</a></span><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png" width="351" height="585.1986417657046" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:982,&quot;width&quot;:589,&quot;resizeWidth&quot;:351,&quot;bytes&quot;:352847,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14679443-26df-4dcf-8e06-2e24a861c1c9_589x982.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The Purkinje cells (PC, in orange) project down into the core of the cerebellum, where they connect to deep nuclei. Climbing fibers (red) feed back up to the Purkinje cells. Mossy fibers (yellow) also feed into the Purkinje cells indirectly, via the granular cells (beige), and the Golgi, stellate, and basket cells (blue). The blue cells inhibit the Purkinje cells, while the red, yellow, and beige ones are excitatory.  </p><p>Basically, this is a feedforward excitatory chain, plus inhibitory feedback loops. The main chain goes: </p><ul><li><p>Afferent neurons (receiving input from the rest of the brain) → </p></li><li><p>mossy fibers →</p></li><li><p>granular cells → </p></li><li><p>parallel fibers →</p></li><li><p>Purkinje cells →</p></li><li><p>output to the rest of the brain</p></li></ul><p>but then there are lots of additional loops where this pathway can be self-inhibiting.</p><p>The primary feedforward chain, though, is a reasonable candidate for the mechanism behind the super-fast “forward model” that generates predictions to inform action faster than sensory processing can generate conscious perceptions.</p><p>On a slightly larger scale, nearby patches of neurons in the cerebellum form pretty much self-contained modules without much connection to cells in other modules.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png" width="1182" height="853" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f4079330-721b-46fb-93d0-ce825523a261_1182x853.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:853,&quot;width&quot;:1182,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:774398,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4079330-721b-46fb-93d0-ce825523a261_1182x853.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This is in contrast to the cerebral cortex, which varies a lot in cell composition between regions, has lots of recurrent loops, and lots of cross-connections between neurons from different local columns. The cerebellum is “one and done” — information goes </span><em>in</em><span>, </span><em>through </em><span>the Purkinje cells, and </span><em>out. </em></p><p>There are lots of different such modules, looping the cerebellum together with different parts of the cerebrum. </p><ul><li><p>Loops through the parietal lobes are involved in visual-motor coordination (like reaching the hand out to grasp something.) </p></li><li><p>Loops through the oculomotor cortex are involved in controlling eye movements.</p></li><li><p><span>Loops through the prefrontal cortex are involved in control of attention and working memory, fear extinction learning</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-21-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-21-140098227" target="_self" rel="">21</a></span><span>, mental preparation of imminent actions, and procedural learning.  </span></p></li><li><p>There are other loops (including through the basal ganglia and limbic system) but these have less well understood functions.</p></li></ul><p>The independence of cerebellar modules makes sense given the need for speed — you can’t have long chains of interconnected neurons messing around if you want to give near-real-time model responses to control immediate action.</p><p>People often talk as though “higher” intelligence lives in the cortex, especially the frontal lobes. The “hindbrain” is for boring, animal stuff, like controlling heart rate and hormones. Real thinking happens behind your noble brow.</p><p>This picture, it turns out, is wrong.</p><p><span>Modern </span><em>Homo sapiens </em><span>is as much characterized by our big cerebellums as by our big frontal lobes. </span></p><p><span>Even the most iconically “thought-like” thinking — the phonological loop, i.e. imagined or inner speech — passes through both the cerebrum </span><em>and </em><span>cerebellum. (Speech, after all, is motion, and imagined speech is simulated motion.  I can literally feel subtle movements and tension in my tongue and jaw when I think.)</span></p><p><span>Most things we do have a cerebellar component, what some neuroscientists call a cerebellar </span><em>transform</em><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-22-140098227" href="https://sarahconstantin.substack.com/p/what-does-the-cerebellum-do-anyway#footnote-22-140098227" target="_self" rel="">22</a></span><span>, smoothing and tuning and relating and fluently switching between the basic building-block abilities (sensory perception, motion, comprehension) that reside in the cerebrum.  </span></p><p>The cerebellum regulates rate, rhythm, speed, contextual appropriateness; damage it, and the same building block actions are still possible, but all out of whack, clumsy and disproportionate. </p><p><span>This is consistent with the “embodied cognition” worldview where sensorimotor functions are </span><em>on a continuum with </em><span>or </span><em>of the same kind as </em><span>cognitive functions; thought is just “inner” motion and/or “inner” sensation. (And even abstract thought is built up of motions and sensations, via analogy or metaphor.)</span></p><p>The cerebellum may also inspire artificial-intelligence approaches somewhat, especially approaches to robotics or other control, in that it may be be beneficial to include a fast feedforward-only predictive modeling step to control real-time actions, alongside a slower training/updating pathway for model retraining. (I may formalize this more in a later post).</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The optimal amount of fraud is non-zero (2022) (132 pts)]]></title>
            <link>https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/</link>
            <guid>38905889</guid>
            <pubDate>Sun, 07 Jan 2024 22:33:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/">https://www.bitsaboutmoney.com/archive/optimal-amount-of-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=38905889">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>I was recently interviewed by NPR’s Planet Money (<a href="https://www.npr.org/2022/08/26/1119606931/wake-up-and-smell-the-fraud">podcast</a>, <a href="https://www.npr.org/transcripts/1119606931">transcript</a>) regarding a particular form of credit card fraud. One comment which tragically ended on the cutting room floor: "the optimal amount of fraud is greater than zero."</p><p>This is counterintuitive and sounds like it is trying a bit too hard to be clever. You should believe it.</p><h2 id="crime-rates-are-a-policy-choice">Crime rates are a policy choice<br></h2><p>If you enjoy simulation games, you might be familiar with the mechanic where you click a button and some statistic in your civilization moves radically in response. In real life, cause and effect is more subtle, but this relationship exists, and there are (both historically and at this very moment) legal regimes which are radically different than your status quo, and which achieve(d) very different outcomes as a direct consequence of policy decisions.</p><p>A glib way to phrase this is that crime is a policy choice, both definitionally (you could simply agree something was not a crime anymore and bam, crime down) and, more interestingly, because crime responds directly to things which are within your control. Most of the world has taken most of the easy policy choices which have few tradeoffs available! But there are still arbitrarily severe options to control crime from where you are, from “increase the police budget” to “ban alcohol totally” to “implement an Orwellian dystopia.”</p><p>Fraud is a unique subset of crime which occurs, to a major degree, subject to the enforcement efforts of non-state actors. A commanding majority of all fraud which is stopped, detected, adjudicated, and even punished (!) gets those done to it by one or more private sector actors. And the private sector has, in this case, policy decisions to make, which, like the public sector’s decisions, balance the undesirability of fraud against the desirability of social goods such as an open society, easy access to services, and (not least!) making money.</p><h2 id="scoping-down-to-payments-fraud">Scoping down to payments fraud<br></h2><p>To prevent this conversation from being painfully abstract, let’s scope it to one particular type of fraud against one particular type of actor: the bad guy steals a payment credential, like a credit card number, and uses it to extract valuable goods or services from a business. This is an extremely common fraud, costing the world something like $10 to $20 billion a year, and yet it is actually fairly constrained relative to all types of fraud.</p><p>This fraud is possible <em>by design</em>. The very best minds in government, the financial industry, the payments industry, and business have gotten together and decided that they want this fraud to be possible. That probably strikes you as an extraordinary claim, and yet it is true.</p><p>Before we get into the how, let’s get into the why.</p><h2 id="who-pays-for-payments-fraud">Who pays for payments fraud?<br></h2><p>Liability for payments fraud happens in a waterfall, established by a combination of regulation, contracts, and business practice. The specifics get complicated but, for ability to concretely visualize this, consider the case of consumer credit card users in the United States.</p><p>You might assume that, if a credit card is stolen/hacked and used by a bad actor to buy something, the cardholder would be liable. They will suffer the first loss, certainly, but society has decided by regulation (specifically, <a href="https://www.consumerfinance.gov/rules-policy/regulations/1005/">Regulation E</a>) that that loss should flow to their financial institution, less a $50 I-can’t-believe-it’s-not-deductible. As a marketing decision, the U.S. financial industry virtually universally waives that $50.</p><p>The card issuer will, following the credit card brand’s rules (which developed in symbiosis with regulation), automatically seek recovery of the loss from the business’s payments processor. It will, similarly, automatically seek recovery of the loss from the business itself.</p><p>In the overwhelming majority of cases, that is where the waterfall ends. While insurance is available (both specialized chargeback insurance and general business insurance), overwhelmingly businesses simply absorb fraud costs in the same way that they absorb their office rent, staff salaries, and marketing expenses.</p><p>That $10 to $20 billion number we threw around earlier? This is what happens to it, in the ordinary course of business. This allocation of loss is mostly automatic, virtually never involves a court or lawyer, and only sometimes takes human effort at the margin at all.</p><h2 id="fraud-as-a-necessary-business-expense">Fraud as a necessary business expense<br></h2><p>Pretend you are the newly hired Director of Fraud for Business, Inc. You know you are ultimately liable for most fraud that happens in this pattern. What target do you take to the CEO for how much fraud you should suffer?</p><p>Zero?! Do you think the Director of Marketing desires to spend zero on marketing!? That would be an objectively silly goal. They would clearly be fired and replaced with someone who understands marginal returns.</p><p><strong>The marginal return of permitting fraud against you is plausibly greater than zero, and therefore, you should </strong><em><strong>welcome</strong></em><strong> greater than zero fraud.</strong> You can think of it as a necessary expense, just like rent or salary or advertising is. You can even write it off on your taxes. (Ask your accountant; businesses frequently misunderstand the rules here.)</p><p>The reason for this is that Directors of Fraud are aware that the policy choices available to them impact the user experience of <em>fraudsters</em> <em>and legitimate users alike</em>. They want to choose policies which balance the tradeoff of lowering fraud against the ease for legitimate users to transact.</p><h2 id="costs-and-benefits-of-policy-choices-around-trust">Costs and benefits of policy choices around trust</h2><p>Maybe the frame of talking about fraud predisposes people to view the space of choices here negatively. Here’s an equivalent function with different emotional valence: how much do you trust people, and under what circumstances?</p><p>All fraud is a) an abuse of trust causing b) monetary losses for the defrauded and c) monetary gain for the fraudster. You could zero fraud by never trusting anyone in any circumstance.</p><p>Trust, though, is an immensely socially useful technology. Human civilization has a fundamental limitation in that all humans can be trivially killed while sleeping. Huge portions of society’s efforts go toward establishing conditions where this trivial vulnerability virtually never gets exploited. God has, reportedly, closed all bug reports claiming that it is a feature and won’t be patched any time soon.</p><p>Anyhow, trust is also fundamental in commerce, where it’s a layered concept, with different people having different levels of trust in different situations. To increase trust generally tends to frontload the cost to generate that trust, and decrease transactional friction afterwards. You trust your accountant more than most regular employees, you trust your employees more than your customers, you trust your customers more than a person you’ve never met, etc.</p><p>This cost falls on both parties in a trust relationship. To employ an accountant, you (the business) need to identify and interview several prospective accountants and employ one winner for years, and you (the accountant) need to have spent years of your life to get a professional credential and then to have worked your entire career to demonstrate yourself worthy of trust. This is one reason why accountants are routinely trusted with the holiest-of-holies secrets of companies and governments.</p><p>Clearly, e-commerce would cease if, prior to buying a pair of sneakers online, you required someone to go to that degree of effort. You’d almost never lose a pair of sneakers to a fraudster again, but you’d also sell very few sneakers.</p><h2 id="making-a-customer-of-someone-you%E2%80%99ve-never-met">Making a customer of someone you’ve never met<br></h2><p>The payments industry has to solve many foundational problems. One of the core ones is quickly bootstrapping a business over the decision to trust someone they’ve never met, enough to allow them to consume valuable goods and services, based on nothing more than a promise of future payment.</p><p>A promise! Mere words! Billions upon billions of dollars have been spent on marketing to make you think that a payment is more than a promise. It’s a lie, and it’s a lie we all choose to believe in part because it’s a vastly more effective model to run the world under than the truth is.</p><p>Businesses prefer attracting new customers to not attracting new customers, citation hopefully not needed. They have a choice as to how much friction they want that new customer to need to go through prior to being offered goods and services. Many businesses have found that decreasing friction results in getting more new customers, who spend more, and who stick around for more transactions. (These are, incidentally, the “only three goals of marketing.”)</p><p>You could subject first-time customers (or even repeat customers), to an elaborate underwriting process, in part to increase your trust in them / decrease your perception of the risk that they would defraud you. You could, for example, ask them to give you a firm handshake as a condition of doing business.</p><p>The requirement for a firm handshake is, actually, an effective anti-fraud measure. The requirement that it happen face-to-face decreases the number of international professionalized fraud gangs which can target you, because they’re not physically close enough to shake your hand. Unfortunately, for the same reason, it also decreases how many customers you can sell to; most people don’t live within commuting distance of your retail presence.</p><h2 id="anti-fraud-loops-used-in-online-commerce">Anti-fraud loops used in online commerce</h2><p>You’ve probably had a shopping experience impacted by an anti-fraud loop, though you might not have recognized it as such. Ever been asked for billing address in addition to shipping address? That’s for AVS verification. There is an obvious user-experience hit there, and it’s quantifiable; removing fields from checkout forms increases conversion rates nearly as a rule. (Conversion rates are an industry term-of-art describing the percentage of prospects who successfully purchase something.)</p><p>Wonder why everyone under the sun wants you to have an account on their site? One major reason is that it gives customers a history that allows a business to direct more of its anti-fraud attention to (more risky) first-time users than (less risky) multi-year regular customers. Allowing guest checkouts is a business decision to accept more fraud (and less ability to market to the customer) in return for marginal sales.</p><p>Some of the savvier interventions operate in the background or don’t surface for all users. For example, you could imagine asking the purchasers of especially high-risk orders to first confirm possession of a phone number (via typing in a code you text them), or even to talk to a human in your fraud department before completing the transaction. Both of these are aimed at breaking the economics of scaled fraud; phone numbers and voice calls are expensive relative to synthetic identities and tend to leak information about fraud operations, which can further inform defenses.</p><p>We’ll talk about this some other time; risk scoring and marginal interventions is a fascinatingly deep topic.</p><h2 id="different-businesses-have-different-tolerance-for-fraud">Different businesses have different tolerance for fraud</h2><p>Margins create margins. A business with high margins will, all else equal, tend to spend more on marketing and sales than a business with low margins; if they don’t, their competitors will “bid up” the cost of attracting customers out of their own fat margins.</p><p>Businesses with high margins also tend to be more accepting of payments fraud than businesses with low margins. Consider businesses which sell IP, like video game companies, streaming services, or SaaS. Because their margins are often 90%+, if you were to present them with a menu of strategies which traded off conversion rate and fraud rate, they’d maximize for conversion rates until fraud at the margin reached levels not seen in even the most corrupt places imaginable.</p><p>Businesses selling valuable resalable goods with much lower margins, such as Apple hardware or game consoles, have to be <em>much</em> more careful about who they transact with. When they’re offered a conceptual slider for who to do secondary transaction screening on, they screen more marginal orders. They accept painful tradeoffs like, “We’ll have a fraud department review every new order and hold every first-time order for shipping until we can talk to the purchaser.”</p><p>Between these two there exists a spectrum of fraud regimes, and this is broadly a good thing. Society gets to make choices, and here it is choosing through the activities of private agents. It is optimizing for how many resources to let leak to bad actors and much societal effort to burn on policing them versus how much low-friction commerce to enable by good actors. This is often missed in discussions of fraud; one reason it has increased over the past few decades is that legitimate commerce has <em>exploded</em>, as the world becomes richer and as barriers to commerce have come down.</p><h2 id="this-extends-beyond-payments">This extends beyond payments</h2><p>So hopefully you buy that Internet merchants can happily accept non-zero levels of fraud. This argument generalizes, and it has some important ethical considerations. We should, as a society, accept non-zero amounts of benefits fraud. We should accept non-zero amounts of cheating on taxes. You personally have benefited from the financial industry’s decision to not expend the maximum possible effort on defending against so-called identity theft.</p><p>These tradeoffs are often <em>intensely</em> difficult to pursue openly. Who wants to be known as the politician in favor of benefits fraud or the financial CEO who thinks they are not laundering enough money?</p><p>One of the interesting questions here is who gets to resolve tensions like this. Generally speaking, it will be private actors applying their own cost-benefits decisions. There is substantial space for regulations to help with cases, like identity theft, where actors can choose to spend other people’s risk budgets to maximize for their own interests.</p><p>If you have other fraud subtopics you’d love to cover, drop me a line.</p>

        

        <div>
          <h2>Want more essays in your inbox?</h2>
          <p>I write about the intersection of tech and finance, approximately biweekly. It's free.</p>
                  </div>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When random numbers are too random: Low discrepancy sequences (2017) (126 pts)]]></title>
            <link>https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/</link>
            <guid>38905280</guid>
            <pubDate>Sun, 07 Jan 2024 21:24:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/">https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/</a>, See on <a href="https://news.ycombinator.com/item?id=38905280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p>Random numbers can be useful in graphics and game development, but they have a pesky and sometimes undesirable habit of clumping together.</p>
<p>This is a problem in path tracing and monte carlo integration when you take N samples, but the samples aren’t well spread across the sampling range.</p>
<p>This can also be a problem for situations like when you are randomly placing objects in the world or generating treasure for a treasure chest.  You don’t want your randomly placed trees to only be in one part of the forest, and you don’t want a player to get only trash items or only godly items when they open a treasure chest.  Ideally you want to have some randomness, but you don’t want the random number generator to give you all of the same or similar random numbers.</p>
<p>The problem is that random numbers can be TOO random, like in the below where you can see clumps and large gaps between the 100 samples.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniformrandom.png?w=800"></p>
<p>For cases like that, when you want random numbers that are a little bit more well distributed, you might find some use in low discrepancy sequences.</p>
<p>The standalone C++ code (one source file, standard headers, no libraries to link to) I used to generate the data and images are at the bottom of this post, as well as some links to more resources.</p>
<h2>What Is Discrepancy?</h2>
<p>In this context, discrepancy is a measurement of the highest or lowest density of points in a sequence.  High discrepancy means that there is either a large area of empty space, or that there is an area that has a high density of points.  Low discrepancy means that there are neither, and that your points are more or less pretty evenly distributed.</p>
<p>The lowest discrepancy possible has no randomness at all, and in the 1 dimensional case means that the points are evenly distributed on a grid.  For monte carlo integration and the game dev usage cases I mentioned, we do want some randomness, we just want the random points to be spread out a little more evenly.</p>
<p>If more formal math notation is your thing, discrepancy is defined as:<br>
<img src="https://s0.wp.com/latex.php?latex=D_%7BN%7D%28P%29%3D%5Csup+_%7B%7BB%5Cin+J%7D%7D%5Cleft%7C%7B%5Cfrac++%7BA%28B%3BP%29%7D%7BN%7D%7D-%5Clambda+_%7Bs%7D%28B%29%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=D_%7BN%7D%28P%29%3D%5Csup+_%7B%7BB%5Cin+J%7D%7D%5Cleft%7C%7B%5Cfrac++%7BA%28B%3BP%29%7D%7BN%7D%7D-%5Clambda+_%7Bs%7D%28B%29%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=D_%7BN%7D%28P%29%3D%5Csup+_%7B%7BB%5Cin+J%7D%7D%5Cleft%7C%7B%5Cfrac++%7BA%28B%3BP%29%7D%7BN%7D%7D-%5Clambda+_%7Bs%7D%28B%29%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="D_{N}(P)=\sup _{{B\in J}}\left|{\frac  {A(B;P)}{N}}-\lambda _{s}(B)\right|"></p>
<p>You can read more about the formal definition here: <a target="_blank" href="https://en.wikipedia.org/wiki/Equidistributed_sequence#Discrepancy">Wikipedia:<br>
 Equidistributed sequence</a></p>
<p>For monte carlo integration specifically, this is the behavior each thing gives you:</p>
<ul>
<li><b>High Discrepancy:</b> Random Numbers / White Noise aka Uniform Distribution – At lower sample counts, convergance is slower (and have higher variance) due to the possibility of not getting good coverage over the area you integrating. At higher sample counts, this problem disappears. (Hint: real time graphics and preview renderings use a smaller number of samples)</li>
<li><b>Lowest Discrepancy:</b> Regular Grid – This will cause aliasing, unlike the other “random” based sampling, which trade aliasing for noise.  Noise is preferred over aliasing.</li>
<li><b>Low Discrepancy:</b> Low Discrepancy Sequences – In lower numbers of samples, this will have faster convergence by having better coverage of the sampling space, but will use randomness to get rid of aliasing by introducing noise.</li>
</ul>
<p>Also interesting to note, <a target="_blank" href="https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method">Quasi Monte Carlo</a> has provably better asymptotic convergence than regular monte carlo integration.</p>
<h2>1 Dimensional Sequences</h2>
<p>We’ll first look at 1 dimensional sequences.</p>
<h2>Grid</h2>
<p>Here are 100 samples evenly spaced:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniform.png?w=800"></p>
<h2>Random Numbers (White Noise)</h2>
<p>This is actually a high discrepancy sequence. To generate this, you just use a standard random number generator to pick 100 points between 0 and 1.  I used std::mt19937 with a std::uniform_real_distribution from 0 to 1:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniformrandom.png?w=800"></p>
<h2>Subrandom Numbers</h2>
<p>Subrandom numbers are ways to decrease the discrepancy of white noise.</p>
<p>One way to do this is to break the sampling space in half.  You then generate even numbered samples in the first half of the space, and odd numbered samples in the second half of the space.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_2.png?w=800"></p>
<p>There’s no reason you can’t generalize this into more divisions of space though.</p>
<p>This splits the space into 4 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_4.png?w=800"></p>
<p>8 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_8.png?w=800"></p>
<p>16 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_16.png?w=800"></p>
<p>32 regions:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandoma_32.png?w=800"></p>
<p>There are other ways to generate subrandom numbers though.  One way is to generate random numbers between 0 and 0.5, and add them to the last sample, plus 0.5.  This gives you a random walk type setup.</p>
<p>Here is that:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dsubrandomb.png?w=800"></p>
<h2>Uniform Sampling + Jitter</h2>
<p>If you take the first subrandom idea to the logical maximum, you break your sample space up into N sections and place one point within those N sections to make a low discrepancy sequence made up of N points.</p>
<p>Another way to look at this is that you do uniform sampling, but add some random jitter to the samples, between +/- half a uniform sample size, to keep the samples in their own areas.</p>
<p>This is that:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1duniformjitter.png?w=800"></p>
<p>I have heard that Pixar invented this technique interestingly.</p>
<h2>Irrational Numbers</h2>
<p>Rational numbers are numbers which can be described as fractions, such as 0.75 which can be expressed as 3/4.  Irrational numbers are numbers which CANNOT be described as fractions, such as pi, or the golden ratio, or the square root of a prime number.</p>
<p>Interestingly you can use irrational numbers to generate low discrepancy sequences.  You start with some value (could be 0, or could be a random number), add the irrational number, and modulus against 1.0.  To get the next sample you add the irrational value again, and modulus against 1.0 again.  Rinse and repeat until you get as many samples as you want.</p>
<p>Some values work better than others though, and apparently the golden ratio is provably the best choice (1.61803398875…), says <a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence#Additive_recurrence" target="_blank">Wikipedia</a>.</p>
<p>Here is the golden ratio, using 4 different random (white noise) starting values:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_000000.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_287194.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_385180.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_618034_775719.png?w=800"></p>
<p>Here I’ve used the square root of 2, with 4 different starting random numbers again:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_000000.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_287194.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_385180.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_414214_775719.png?w=800"></p>
<p>Lastly, here is pi, with 4 random starting values:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_000000.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_287194.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_385180.png?w=800"><br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dirrational_141593_775719.png?w=800"></p>
<h2>Van der Corput Sequence</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Van_der_Corput_sequence" target="_blank">Van der Corput sequence</a> is the 1d equivelant of the Halton sequence which we’ll talk about later.</p>
<p>How you generate values in the Van der Corput sequence is you convert the index of your sample into some base.</p>
<p>For instance if it was base 2, you would convert your index to binary.  If it was base 16, you would convert your index to hexadecimal.</p>
<p>Now, instead of treating the digits as if they are <img src="https://s0.wp.com/latex.php?latex=B%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^0">, <img src="https://s0.wp.com/latex.php?latex=B%5E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^1">, <img src="https://s0.wp.com/latex.php?latex=B%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^2">, etc (where B is the base), you instead treat them as <img src="https://s0.wp.com/latex.php?latex=B%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^{-1}">, <img src="https://s0.wp.com/latex.php?latex=B%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^{-2}">, <img src="https://s0.wp.com/latex.php?latex=B%5E%7B-3%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=B%5E%7B-3%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=B%5E%7B-3%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="B^{-3}"> and so on.  In other words, you multiply each digit by a fraction and add up the results.</p>
<p>To show a couple quick examples, let’s say we wanted sample 6 in the sequence of base 2.</p>
<p>First we convert 6 to binary which is 110.  From right to left, we have 3 digits: a 0 in the 1’s place, a 1 in the 2’s place, and a 1 in the 4’s place.  <img src="https://s0.wp.com/latex.php?latex=0%2A1+%2B+1%2A2+%2B+1%2A4+%3D+6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0%2A1+%2B+1%2A2+%2B+1%2A4+%3D+6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0%2A1+%2B+1%2A2+%2B+1%2A4+%3D+6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0*1 + 1*2 + 1*4 = 6">, so we can see that 110 is in fact 6 in binary.</p>
<p>To get the Van der Corput value for this, instead of treating it as the 1’s, 2’s and 4’s digit, we treat it as the 1/2, 1/4 and 1/8’s digit.</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%2A+1%2F2+%2B+1+%2A+1%2F4+%2B+1+%2A+1%2F8+%3D+3%2F8&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0+%2A+1%2F2+%2B+1+%2A+1%2F4+%2B+1+%2A+1%2F8+%3D+3%2F8&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+%2A+1%2F2+%2B+1+%2A+1%2F4+%2B+1+%2A+1%2F8+%3D+3%2F8&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0 * 1/2 + 1 * 1/4 + 1 * 1/8 = 3/8">.</p>
<p>So, sample 6 in the Van der Corput sequence using base 2 is 3/8.</p>
<p>Let’s try sample 21 in base 3.</p>
<p>First we convert 21 to base 3 which is 210.  We can verify this is right by seeing that <img src="https://s0.wp.com/latex.php?latex=0+%2A+1+%2B+1+%2A+3+%2B+2+%2A+9+%3D+21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0+%2A+1+%2B+1+%2A+3+%2B+2+%2A+9+%3D+21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+%2A+1+%2B+1+%2A+3+%2B+2+%2A+9+%3D+21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0 * 1 + 1 * 3 + 2 * 9 = 21">.</p>
<p>Instead of a 1’s, 3’s and 9’s digit, we are going to treat it like a 1/3, 1/9 and 1/27 digit.</p>
<p><img src="https://s0.wp.com/latex.php?latex=0+%2A+1%2F3+%2B+1+%2A+1%2F9+%2B+2+%2A+1%2F27+%3D+5%2F27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=0+%2A+1%2F3+%2B+1+%2A+1%2F9+%2B+2+%2A+1%2F27+%3D+5%2F27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=0+%2A+1%2F3+%2B+1+%2A+1%2F9+%2B+2+%2A+1%2F27+%3D+5%2F27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="0 * 1/3 + 1 * 1/9 + 2 * 1/27 = 5/27"></p>
<p>So, sample 21 in the Van der Corput sequence using base 3 is 5/27.</p>
<p>Here is the Van der Corput sequence for base 2:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_2.png?w=800"></p>
<p>Here it is for base 3:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_3.png?w=800"></p>
<p>Base 4:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_4.png?w=800"></p>
<p>Base 5:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dvandercorput_5.png?w=800"></p>
<h2>Sobol</h2>
<p>One dimensional Sobol is actually just the Van der Corput sequence base 2 re-arranged a little bit, but it’s generated differently.</p>
<p>You start with 0 (either using it as sample 0 or sample -1, doesn’t matter which), and for each sample you do this:</p>
<ol>
<li>Calculate the Ruler function value for the current sample’s index(more info in a second)</li>
<li>Make the direction vector by shifting 1 left (in binary) 31 – ruler times.</li>
<li>XOR the last sample by the direction vector to get the new sample</li>
<li>To interpret the sample as a floating point number you divide it by <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="2^{32}"></li>
</ol>
<p>That might sound completely different than the Van der Corput sequence but it actually is the same thing – just re-ordered.</p>
<p>In the final step when dividing by <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="2^{32}">, we are really just interpreting the binary number as a fraction just like before, but it’s the LEFT most digit that is the 1/2 spot, not the RIGHT most digit.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Thomae%27s_function#The_ruler_function" target="_blank">Ruler Function</a> goes like:  0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, …</p>
<p>It’s pretty easy to calculate too.  Calculating the ruler function for an index (starting at 1) is just the zero based index of the right most 1’s digit after converting the number to binary.</p>
<p>1 in binary is 001 so Ruler(1) is 0.<br>
2 in binary is 010 so Ruler(2) is 1.<br>
3 in binary is 011 so Ruler(3) is 0.<br>
4 in binary is 100 so Ruler(4) is 2.<br>
5 in binary is 101 so Ruler(5) is 0.<br>
and so on.</p>
<p>Here is 1D Sobol:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dsobol.png?w=800"></p>
<h2>Hammersley</h2>
<p>In one dimension, the Hammersley sequence is the same as the base 2 Van der Corput sequence, and in the same order.  If that sounds strange that it’s the same, it’s a 2d sequence I broke down into a 1d sequence for comparison.  The one thing Hammersley has that makes it unique in the 1d case is that you can truncate bits.</p>
<p>It doesn’t seem that useful for 1d Hammersley to truncate bits but knowing that is useful info too I guess.  Look at the 2d version of Hammersley to get a fairer look at it, because it’s meant to be a 2d sequence.</p>
<p>Here is Hammersley:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dhammersley_0.png?w=800"></p>
<p>With 1 bit truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dhammersley_1.png?w=800"></p>
<p>With 2 bits truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/1dhammersley_2.png?w=800"></p>
<h2>Poisson Disc</h2>
<p>Poisson disc points are points which are densely packed, but have a minimum distance from each other.</p>
<p>Computer scientists are still working out good algorithms to generate these points efficiently.</p>
<p>I use “Mitchell’s Best-Candidate” which means that when you want to generate a new point in the sequence, you generate N new points, and choose whichever point is farthest away from the other points you’ve generated so far.</p>
<p>Here it is where N is 100:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/1dpoisson.png?w=800"></p>
<h2>2 Dimensional Sequences</h2>
<p>Next up, let’s look at some 2 dimensional sequences.</p>
<h2>Grid</h2>
<p>Below is 2d uniform samples on a grid.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2duniform.png?w=800"></p>
<p>Note that uniform grid is not particularly low discrepancy for the 2d case! More info here: <a href="https://math.stackexchange.com/questions/2283671/is-it-expected-that-uniform-points-would-have-non-zero-discrepancy/2284163#2284163" target="_blank">Is it expected that uniform points would have non zero discrepancy?</a></p>

<p>Here are 100 random points:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2duniformrandom.png?w=800"></p>
<h2>Uniform Grid + Jitter</h2>
<p>Here is a uniform grid that has random jitter applied to the points.  Jittered grid is a pretty commonly used low discrepancy sampling technique that has good success.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2duniformjitter.png?w=800"></p>
<h2>Subrandom</h2>
<p>Just like in 1 dimensions, you can apply the subrandom ideas to 2 dimensions where you divide the X and Y axis into so many sections, and randomly choose points in the sections.</p>
<p>If you divide X and Y into the same number of sections though, you are going to have a problem because some areas are not going to have any points in them.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_2_2.png?w=800"></p>
<p><a target="_blank" href="https://twitter.com/Reedbeta">@Reedbeta</a> pointed out that instead of using i%x and i%y, that you could use i%x and (i/x)%y to make it pick points in all regions.</p>
<p>Picking different numbers for X and Y can be another way to give good results.  Here’s dividing X and Y into 2 and 3 sections respectively:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_2_3.png?w=800"></p>
<p>If you choose co-prime numbers for divisions for each axis you can get maximal period of repeats.  2 and 3 are coprime so the last example is a good example of that, but here is 3 and 11:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_3_11.png?w=800"></p>
<p>Here is 3 and 97.  97 is large enough that with only doing 100 samples, we are almost doing jittered grid on the y axis.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandoma_3_97.png?w=800"></p>
<p>Here is the other subrandom number from 1d, where we start with a random value for X and Y, and then add a random number between 0 and 0.5 to each, also adding 0.5, to make a “random walk” type setup again:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsubrandomb.png?w=800"></p>
<h2>Halton</h2>
<p>The Halton sequence is just the Van der Corput sequence, but using a different base on each axis.</p>
<p>Here is the Halton sequence where X and Y use bases 2 and 3:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dhalton_2_3.png?w=800"></p>
<p>Here it is using bases 5 and 7:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dhalton_5_7.png?w=800"></p>
<p>Here are bases 13 and 9:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dhalton_13_9.png?w=800"></p>
<h2>Irrational Numbers</h2>
<p>The irrational numbers technique can be used for 2d as well but I wasn’t able to find out how to make it give decent looking output that didn’t have an obvious diagonal pattern in them.  <a target="_blank" href="https://twitter.com/BartWronsk">Bart Wronski</a> shared a neat paper that explains how to use the golden ratio in 2d with great success: <a target="_blank" href="https://www.graphics.rwth-aachen.de/publication/2/jgt.pdf">Golden Ratio Sequences For Low-Discrepancy Sampling</a></p>
<p>This uses the golden ratio for the X axis and the square root of 2 for the Y axis.  Below that is the same, with a random starting point, to make it give a different sequence.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_618034_414214_000000_000000.png?w=800"></p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_618034_414214_775719_264045.png?w=800"></p>
<p>Here X axis uses square root of 2 and Y axis uses square root of 3.  Below that is a random starting point, which gives the same discrepancy.</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_414214_732051_000000_000000.png?w=800"></p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dirrational_414214_732051_775719_264045.png?w=800"></p>
<h2>Hammersley</h2>
<p>In 2 dimensions, the Hammersley sequence uses the 1d Hammersley sequence for the X axis: Instead of treating the binary version of the index as binary, you treat it as fractions like you do for Van der Corput and sum up the fractions.</p>
<p>For the Y axis, you just reverse the bits and then do the same!</p>
<p>Here is the Hammersley sequence. Note we would have to take 128 samples (not just the 100 we did) if we wanted it to fill the entire square with samples.<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2dhammersley_0.png?w=800"></p>
<p>Truncating bits in 2d is a bit useful. Here is 1 bit truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2dhammersley_1.png?w=800"></p>
<p>2 bits truncated:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2dhammersley_2.png?w=800"></p>
<h2>Poisson Disc</h2>
<p>Using the same method we did for 1d, we can generate points in 2d space:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dpoisson.png?w=800"></p>
<h2>N Rooks</h2>
<p>There is a sampling pattern called N-Rooks where you put N rooks onto a chess board and arrange them such that no two are in the same row or column.</p>
<p>A way to generate these samples is to realize that there will be only one rook per row, and that none of them will ever be in the same column.  So, you make an array that has numbers 0 to N-1, and then shuffle the array.  The index into the array is the row, and the value in the array is the column.</p>
<p>Here are 100 rooks:<br>
<img src="https://demofox2.files.wordpress.com/2017/05/2drooks.png?w=800"></p>
<h2>Sobol</h2>
<p>Sobol in two dimensions is more complex to explain so I’ll link you to the source I used: <a href="http://papa.bretmulvey.com/post/153648811993/sobol-sequences-made-simple" target="_blank">Sobol Sequences Made Simple</a>.</p>
<p>The 1D sobol already covered is used for the X axis, and then something more complex was used for the Y axis:</p>
<p><img src="https://demofox2.files.wordpress.com/2017/05/2dsobol.png?w=800"></p>
<h2>Links</h2>
<p>Bart Wronski has a really great series on a related topic: <a target="_blank" href="https://bartwronski.com/2016/10/30/dithering-in-games-mini-series/">Dithering in Games</a></p>
<p><a target="_blank" href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">Wikipedia: Low Discrepancy Sequence</a></p>
<p><a target="_blank" href="https://en.m.wikipedia.org/wiki/Halton_sequence">Wikipedia: Halton Sequence</a></p>
<p><a target="_blank" href="https://en.m.wikipedia.org/wiki/Van_der_Corput_sequence">Wikipedia: Van der Corput Sequence</a></p>
<p><a target="_blank" href="http://martin.ankerl.com/2009/12/09/how-to-create-random-colors-programmatically/">Using Fibonacci Sequence To Generate Colors</a></p>
<p><a target="_blank" href="http://gruenschloss.org/">Deeper info and usage cases for low discrepancy sequences</a></p>
<p><a target="_blank" href="https://www.jasondavies.com/poisson-disc/">Poisson-Disc Sampling</a></p>
<p>Low discrepancy sequences are related to blue noise.  Where white noise contains all frequencies evenly, blue noise has more high frequencies and fewer low frequencies.  Blue noise is essentially the ultimate in low discrepancy, but can be expensive to compute.  Here are some pages on blue noise:</p>
<p><a target="_blank" href="http://momentsingraphics.de/?p=127">Free Blue Noise Textures</a></p>
<p><a target="_blank" href="http://momentsingraphics.de/?p=148">The problem with 3D blue noise</a></p>
<p><a target="_blank" href="http://www.joesfer.com/?p=108">Stippling and Blue Noise</a></p>
<p><a target="_blank" href="https://mollyrocket.com/casey/stream_0015.html">Vegetation placement in “The Witness”</a></p>
<p>Here are some links from  <a target="_blank" href="https://twitter.com/marc_b_reynolds">@marc_b_reynolds</a>:<br>
Sobol (low-discrepancy) sequence in 1-3D, stratified in 2-4D.<br>
Classic binary-reflected gray code.<br>
<a target="_blank" href="https://github.com/Marc-B-Reynolds/Stand-alone-junk/blob/master/src/SFH/Sobol.h">Sobol.h</a></p>
<p><a target="_blank" href="http://marc-b-reynolds.github.io/math/2016/02/24/weyl.html">Weyl Sequence</a></p>
<h2>Code</h2>
<pre title="">#define _CRT_SECURE_NO_WARNINGS

#include &lt;windows.h&gt;  // for bitmap headers and performance counter.  Sorry non windows people!
#include &lt;vector&gt;
#include &lt;stdint.h&gt;
#include &lt;random&gt;
#include &lt;array&gt;
#include &lt;algorithm&gt;
#include &lt;stdlib.h&gt;
#include &lt;set&gt;

typedef uint8_t uint8;

#define NUM_SAMPLES 100  // to simplify some 2d code, this must be a square
#define NUM_SAMPLES_FOR_COLORING 100

// Turning this on will slow things down significantly because it's an O(N^5) operation for 2d!
#define CALCULATE_DISCREPANCY 0

#define IMAGE1D_WIDTH 600
#define IMAGE1D_HEIGHT 50
#define IMAGE2D_WIDTH 300
#define IMAGE2D_HEIGHT 300
#define IMAGE_PAD   30

#define IMAGE1D_CENTERX ((IMAGE1D_WIDTH+IMAGE_PAD*2)/2)
#define IMAGE1D_CENTERY ((IMAGE1D_HEIGHT+IMAGE_PAD*2)/2)
#define IMAGE2D_CENTERX ((IMAGE2D_WIDTH+IMAGE_PAD*2)/2)
#define IMAGE2D_CENTERY ((IMAGE2D_HEIGHT+IMAGE_PAD*2)/2)

#define AXIS_HEIGHT 40
#define DATA_HEIGHT 20
#define DATA_WIDTH 2

#define COLOR_FILL SColor(255,255,255)
#define COLOR_AXIS SColor(0, 0, 0)

//======================================================================================
struct SImageData
{
    SImageData ()
        : m_width(0)
        , m_height(0)
    { }
  
    size_t m_width;
    size_t m_height;
    size_t m_pitch;
    std::vector&lt;uint8&gt; m_pixels;
};

struct SColor
{
    SColor (uint8 _R = 0, uint8 _G = 0, uint8 _B = 0)
        : R(_R), G(_G), B(_B)
    { }

    uint8 B, G, R;
};

//======================================================================================
bool SaveImage (const char *fileName, const SImageData &amp;image)
{
    // open the file if we can
    FILE *file;
    file = fopen(fileName, "wb");
    if (!file) {
        printf("Could not save %s\n", fileName);
        return false;
    }
  
    // make the header info
    BITMAPFILEHEADER header;
    BITMAPINFOHEADER infoHeader;
  
    header.bfType = 0x4D42;
    header.bfReserved1 = 0;
    header.bfReserved2 = 0;
    header.bfOffBits = 54;
  
    infoHeader.biSize = 40;
    infoHeader.biWidth = (LONG)image.m_width;
    infoHeader.biHeight = (LONG)image.m_height;
    infoHeader.biPlanes = 1;
    infoHeader.biBitCount = 24;
    infoHeader.biCompression = 0;
    infoHeader.biSizeImage = (DWORD) image.m_pixels.size();
    infoHeader.biXPelsPerMeter = 0;
    infoHeader.biYPelsPerMeter = 0;
    infoHeader.biClrUsed = 0;
    infoHeader.biClrImportant = 0;
  
    header.bfSize = infoHeader.biSizeImage + header.bfOffBits;
  
    // write the data and close the file
    fwrite(&amp;header, sizeof(header), 1, file);
    fwrite(&amp;infoHeader, sizeof(infoHeader), 1, file);
    fwrite(&amp;image.m_pixels[0], infoHeader.biSizeImage, 1, file);
    fclose(file);
 
    return true;
}

//======================================================================================
void ImageInit (SImageData&amp; image, size_t width, size_t height)
{
    image.m_width = width;
    image.m_height = height;
    image.m_pitch = 4 * ((width * 24 + 31) / 32);
    image.m_pixels.resize(image.m_pitch * image.m_width);
    std::fill(image.m_pixels.begin(), image.m_pixels.end(), 0);
}

//======================================================================================
void ImageClear (SImageData&amp; image, const SColor&amp; color)
{
    uint8* row = &amp;image.m_pixels[0];
    for (size_t rowIndex = 0; rowIndex &lt; image.m_height; ++rowIndex)
    {
        SColor* pixels = (SColor*)row;
        std::fill(pixels, pixels + image.m_width, color);

        row += image.m_pitch;
    }
}

//======================================================================================
void ImageBox (SImageData&amp; image, size_t x1, size_t x2, size_t y1, size_t y2, const SColor&amp; color)
{
    for (size_t y = y1; y &lt; y2; ++y)
    {
        uint8* row = &amp;image.m_pixels[y * image.m_pitch];
        SColor* start = &amp;((SColor*)row)[x1];
        std::fill(start, start + x2 - x1, color);
    }
}

//======================================================================================
float Distance (float x1, float y1, float x2, float y2)
{
    float dx = (x2 - x1);
    float dy = (y2 - y1);

    return std::sqrtf(dx*dx + dy*dy);
}

//======================================================================================
SColor DataPointColor (size_t sampleIndex)
{
    SColor ret;
    float percent = (float(sampleIndex) / (float(NUM_SAMPLES_FOR_COLORING) - 1.0f));

    ret.R = uint8((1.0f - percent) * 255.0f);
    ret.G = 0;
    ret.B = uint8(percent * 255.0f);

    float mag = (float)sqrt(ret.R*ret.R + ret.G*ret.G + ret.B*ret.B);
    ret.R = uint8((float(ret.R) / mag)*255.0f);
    ret.G = uint8((float(ret.G) / mag)*255.0f);
    ret.B = uint8((float(ret.B) / mag)*255.0f);

    return ret;
}

//======================================================================================
float RandomFloat (float min, float max)
{
    static std::random_device rd;
    static std::mt19937 mt(rd());
    std::uniform_real_distribution&lt;float&gt; dist(min, max);
    return dist(mt);
}

//======================================================================================
size_t Ruler (size_t n)
{
    size_t ret = 0;
    while (n != 0 &amp;&amp; (n &amp; 1) == 0)
    {
        n /= 2;
        ++ret;
    }
    return ret;
}

//======================================================================================
float CalculateDiscrepancy1D (const std::array&lt;float, NUM_SAMPLES&gt;&amp; samples)
{
    // some info about calculating discrepancy
    // https://math.stackexchange.com/questions/1681562/how-to-calculate-discrepancy-of-a-sequence

    // Calculates the discrepancy of this data.
    // Assumes the data is [0,1) for valid sample range
    std::array&lt;float, NUM_SAMPLES&gt; sortedSamples = samples;
    std::sort(sortedSamples.begin(), sortedSamples.end());

    float maxDifference = 0.0f;
    for (size_t startIndex = 0; startIndex &lt;= NUM_SAMPLES; ++startIndex)
    {
        // startIndex 0 = 0.0f.  startIndex 1 = sortedSamples[0]. etc

        float startValue = 0.0f;
        if (startIndex &gt; 0)
            startValue = sortedSamples[startIndex - 1];

        for (size_t stopIndex = startIndex; stopIndex &lt;= NUM_SAMPLES; ++stopIndex)
        {
            // stopIndex 0 = sortedSamples[0].  startIndex[N] = 1.0f. etc

            float stopValue = 1.0f;
            if (stopIndex &lt; NUM_SAMPLES)
                stopValue = sortedSamples[stopIndex];

            float length = stopValue - startValue;

            // open interval (startValue, stopValue)
            size_t countInside = 0;
            for (float sample : samples)
            {
                if (sample &gt; startValue &amp;&amp;
                    sample &lt; stopValue)
                {
                    ++countInside;
                }
            }
            float density = float(countInside) / float(NUM_SAMPLES);
            float difference = std::abs(density - length);
            if (difference &gt; maxDifference)
                maxDifference = difference;

            // closed interval [startValue, stopValue]
            countInside = 0;
            for (float sample : samples)
            {
                if (sample &gt;= startValue &amp;&amp;
                    sample &lt;= stopValue)
                {
                    ++countInside;
                }
            }
            density = float(countInside) / float(NUM_SAMPLES);
            difference = std::abs(density - length);
            if (difference &gt; maxDifference)
                maxDifference = difference;
        }
    }
    return maxDifference;
}

//======================================================================================
float CalculateDiscrepancy2D (const std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt;&amp; samples)
{
    // some info about calculating discrepancy
    // https://math.stackexchange.com/questions/1681562/how-to-calculate-discrepancy-of-a-sequence

    // Calculates the discrepancy of this data.
    // Assumes the data is [0,1) for valid sample range.

    // Get the sorted list of unique values on each axis
    std::set&lt;float&gt; setSamplesX;
    std::set&lt;float&gt; setSamplesY;
    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
    {
        setSamplesX.insert(sample[0]);
        setSamplesY.insert(sample[1]);
    }
    std::vector&lt;float&gt; sortedXSamples;
    std::vector&lt;float&gt; sortedYSamples;
    sortedXSamples.reserve(setSamplesX.size());
    sortedYSamples.reserve(setSamplesY.size());
    for (float f : setSamplesX)
        sortedXSamples.push_back(f);
    for (float f : setSamplesY)
        sortedYSamples.push_back(f);

    // Get the sorted list of samples on the X axis, for faster interval testing
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; sortedSamplesX = samples;
    std::sort(sortedSamplesX.begin(), sortedSamplesX.end(),
        [] (const std::array&lt;float, 2&gt;&amp; itemA, const std::array&lt;float, 2&gt;&amp; itemB)
        {
            return itemA[0] &lt; itemB[0];
        }
    );

    // calculate discrepancy
    float maxDifference = 0.0f;
    for (size_t startIndexY = 0; startIndexY &lt;= sortedYSamples.size(); ++startIndexY)
    {
        float startValueY = 0.0f;
        if (startIndexY &gt; 0)
            startValueY = *(sortedYSamples.begin() + startIndexY - 1);

        for (size_t startIndexX = 0; startIndexX &lt;= sortedXSamples.size(); ++startIndexX)
        {
            float startValueX = 0.0f;
            if (startIndexX &gt; 0)
                startValueX = *(sortedXSamples.begin() + startIndexX - 1);

            for (size_t stopIndexY = startIndexY; stopIndexY &lt;= sortedYSamples.size(); ++stopIndexY)
            {
                float stopValueY = 1.0f;
                if (stopIndexY &lt; sortedYSamples.size())
                    stopValueY = sortedYSamples[stopIndexY];

                for (size_t stopIndexX = startIndexX; stopIndexX &lt;= sortedXSamples.size(); ++stopIndexX)
                {
                    float stopValueX = 1.0f;
                    if (stopIndexX &lt; sortedXSamples.size())
                        stopValueX = sortedXSamples[stopIndexX];

                    // calculate area
                    float length = stopValueX - startValueX;
                    float height = stopValueY - startValueY;
                    float area = length * height;

                    // open interval (startValue, stopValue)
                    size_t countInside = 0;
                    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
                    {
                        if (sample[0] &gt; startValueX &amp;&amp;
                            sample[1] &gt; startValueY &amp;&amp;
                            sample[0] &lt; stopValueX &amp;&amp;
                            sample[1] &lt; stopValueY)
                        {
                            ++countInside;
                        }
                    }
                    float density = float(countInside) / float(NUM_SAMPLES);
                    float difference = std::abs(density - area);
                    if (difference &gt; maxDifference)
                        maxDifference = difference;

                    // closed interval [startValue, stopValue]
                    countInside = 0;
                    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
                    {
                        if (sample[0] &gt;= startValueX &amp;&amp;
                            sample[1] &gt;= startValueY &amp;&amp;
                            sample[0] &lt;= stopValueX &amp;&amp;
                            sample[1] &lt;= stopValueY)
                        {
                            ++countInside;
                        }
                    }
                    density = float(countInside) / float(NUM_SAMPLES);
                    difference = std::abs(density - area);
                    if (difference &gt; maxDifference)
                        maxDifference = difference;
                }
            }
        }
    }

    return maxDifference;
}

//======================================================================================
void Test1D (const char* fileName, const std::array&lt;float, NUM_SAMPLES&gt;&amp; samples)
{
    // create and clear the image
    SImageData image;
    ImageInit(image, IMAGE1D_WIDTH + IMAGE_PAD * 2, IMAGE1D_HEIGHT + IMAGE_PAD * 2);

    // setup the canvas
    ImageClear(image, COLOR_FILL);

    // calculate the discrepancy
    #if CALCULATE_DISCREPANCY
        float discrepancy = CalculateDiscrepancy1D(samples);
        printf("%s Discrepancy = %0.2f%%\n", fileName, discrepancy*100.0f);
    #endif

    // draw the sample points
    size_t i = 0;
    for (float f: samples)
    {
        size_t pos = size_t(f * float(IMAGE1D_WIDTH)) + IMAGE_PAD;
        ImageBox(image, pos, pos + 1, IMAGE1D_CENTERY - DATA_HEIGHT / 2, IMAGE1D_CENTERY + DATA_HEIGHT / 2, DataPointColor(i));
        ++i;
    }

    // draw the axes lines. horizontal first then the two vertical
    ImageBox(image, IMAGE_PAD, IMAGE1D_WIDTH + IMAGE_PAD, IMAGE1D_CENTERY, IMAGE1D_CENTERY + 1, COLOR_AXIS);
    ImageBox(image, IMAGE_PAD, IMAGE_PAD + 1, IMAGE1D_CENTERY - AXIS_HEIGHT / 2, IMAGE1D_CENTERY + AXIS_HEIGHT / 2, COLOR_AXIS);
    ImageBox(image, IMAGE1D_WIDTH + IMAGE_PAD, IMAGE1D_WIDTH + IMAGE_PAD + 1, IMAGE1D_CENTERY - AXIS_HEIGHT / 2, IMAGE1D_CENTERY + AXIS_HEIGHT / 2, COLOR_AXIS);

    // save the image
    SaveImage(fileName, image);
}

//======================================================================================
void Test2D (const char* fileName, const std::array&lt;std::array&lt;float,2&gt;, NUM_SAMPLES&gt;&amp; samples)
{
    // create and clear the image
    SImageData image;
    ImageInit(image, IMAGE2D_WIDTH + IMAGE_PAD * 2, IMAGE2D_HEIGHT + IMAGE_PAD * 2);
    
    // setup the canvas
    ImageClear(image, COLOR_FILL);

    // calculate the discrepancy
    #if CALCULATE_DISCREPANCY
        float discrepancy = CalculateDiscrepancy2D(samples);
        printf("%s Discrepancy = %0.2f%%\n", fileName, discrepancy*100.0f);
    #endif

    // draw the sample points
    size_t i = 0;
    for (const std::array&lt;float, 2&gt;&amp; sample : samples)
    {
        size_t posx = size_t(sample[0] * float(IMAGE2D_WIDTH)) + IMAGE_PAD;
        size_t posy = size_t(sample[1] * float(IMAGE2D_WIDTH)) + IMAGE_PAD;
        ImageBox(image, posx - 1, posx + 1, posy - 1, posy + 1, DataPointColor(i));
        ++i;
    }

    // horizontal lines
    ImageBox(image, IMAGE_PAD - 1, IMAGE2D_WIDTH + IMAGE_PAD + 1, IMAGE_PAD - 1, IMAGE_PAD, COLOR_AXIS);
    ImageBox(image, IMAGE_PAD - 1, IMAGE2D_WIDTH + IMAGE_PAD + 1, IMAGE2D_HEIGHT + IMAGE_PAD, IMAGE2D_HEIGHT + IMAGE_PAD + 1, COLOR_AXIS);

    // vertical lines
    ImageBox(image, IMAGE_PAD - 1, IMAGE_PAD, IMAGE_PAD - 1, IMAGE2D_HEIGHT + IMAGE_PAD + 1, COLOR_AXIS);
    ImageBox(image, IMAGE_PAD + IMAGE2D_WIDTH, IMAGE_PAD + IMAGE2D_WIDTH + 1, IMAGE_PAD - 1, IMAGE2D_HEIGHT + IMAGE_PAD + 1, COLOR_AXIS);

    // save the image
    SaveImage(fileName, image);
}

//======================================================================================
void TestUniform1D (bool jitter)
{
    // calculate the sample points
    const float c_cellSize = 1.0f / float(NUM_SAMPLES+1);
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i] = float(i+1) / float(NUM_SAMPLES+1);
        if (jitter)
            samples[i] += RandomFloat(-c_cellSize*0.5f, c_cellSize*0.5f);
    }

    // save bitmap etc
    if (jitter)
        Test1D("1DUniformJitter.bmp", samples);
    else
        Test1D("1DUniform.bmp", samples);
}

//======================================================================================
void TestUniformRandom1D ()
{
    // calculate the sample points
    const float c_halfJitter = 1.0f / float((NUM_SAMPLES + 1) * 2);
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
        samples[i] = RandomFloat(0.0f, 1.0f);

    // save bitmap etc
    Test1D("1DUniformRandom.bmp", samples);
}

//======================================================================================
void TestSubRandomA1D (size_t numRegions)
{
    const float c_randomRange = 1.0f / float(numRegions);

    // calculate the sample points
    const float c_halfJitter = 1.0f / float((NUM_SAMPLES + 1) * 2);
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i] = RandomFloat(0.0f, c_randomRange);
        samples[i] += float(i % numRegions) / float(numRegions);
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "1DSubRandomA_%zu.bmp", numRegions);
    Test1D(fileName, samples);
}

//======================================================================================
void TestSubRandomB1D ()
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    float sample = RandomFloat(0.0f, 0.5f);
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        sample = std::fmodf(sample + 0.5f + RandomFloat(0.0f, 0.5f), 1.0f);
        samples[i] = sample;
    }

    // save bitmap etc
    Test1D("1DSubRandomB.bmp", samples);
}

//======================================================================================
void TestVanDerCorput (size_t base)
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i] = 0.0f;
        float denominator = float(base);
        size_t n = i;
        while (n &gt; 0)
        {
            size_t multiplier = n % base;
            samples[i] += float(multiplier) / denominator;
            n = n / base;
            denominator *= base;
        }
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "1DVanDerCorput_%zu.bmp", base);
    Test1D(fileName, samples);
}

//======================================================================================
void TestIrrational1D (float irrational, float seed)
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    float sample = seed;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        sample = std::fmodf(sample + irrational, 1.0f);
        samples[i] = sample;
    }

    // save bitmap etc
    char irrationalStr[256];
    sprintf(irrationalStr, "%f", irrational);
    char seedStr[256];
    sprintf(seedStr, "%f", seed);
    char fileName[256];
    sprintf(fileName, "1DIrrational_%s_%s.bmp", &amp;irrationalStr[2], &amp;seedStr[2]);
    Test1D(fileName, samples);
}

//======================================================================================
void TestSobol1D ()
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        size_t ruler = Ruler(i + 1);
        size_t direction = size_t(size_t(1) &lt;&lt; size_t(31 - ruler));
        sampleInt = sampleInt ^ direction;
        samples[i] = float(sampleInt) / std::pow(2.0f, 32.0f);
    }

    // save bitmap etc
    Test1D("1DSobol.bmp", samples);
}

//======================================================================================
void TestHammersley1D (size_t truncateBits)
{
    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        size_t n = i &gt;&gt; truncateBits;
        float base = 1.0f / 2.0f;
        samples[i] = 0.0f;
        while (n)
        {
            if (n &amp; 1)
                samples[i] += base;
            n /= 2;
            base /= 2.0f;
        }
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "1DHammersley_%zu.bmp", truncateBits);
    Test1D(fileName, samples);
}

//======================================================================================
float MinimumDistance1D (const std::array&lt;float, NUM_SAMPLES&gt;&amp; samples, size_t numSamples, float x)
{
    // Used by poisson.
    // This returns the minimum distance that point (x) is away from the sample points, from [0, numSamples).
    float minimumDistance = 0.0f;
    for (size_t i = 0; i &lt; numSamples; ++i)
    {
        float distance = std::abs(samples[i] - x);
        if (i == 0 || distance &lt; minimumDistance)
            minimumDistance = distance;
    }
    return minimumDistance;
}

//======================================================================================
void TestPoisson1D ()
{
    // every time we want to place a point, we generate this many points and choose the one farthest away from all the other points (largest minimum distance)
    const size_t c_bestOfAttempts = 100;

    // calculate the sample points
    std::array&lt;float, NUM_SAMPLES&gt; samples;
    for (size_t sampleIndex = 0; sampleIndex &lt; NUM_SAMPLES; ++sampleIndex)
    {
        // generate some random points and keep the one that has the largest minimum distance from any of the existing points
        float bestX = 0.0f;
        float bestMinDistance = 0.0f;
        for (size_t attempt = 0; attempt &lt; c_bestOfAttempts; ++attempt)
        {
            float attemptX = RandomFloat(0.0f, 1.0f);
            float minDistance = MinimumDistance1D(samples, sampleIndex, attemptX);

            if (minDistance &gt; bestMinDistance)
            {
                bestX = attemptX;
                bestMinDistance = minDistance;
            }
        }
        samples[sampleIndex] = bestX;
    }

    // save bitmap etc
    Test1D("1DPoisson.bmp", samples);
}

//======================================================================================
void TestUniform2D (bool jitter)
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    const size_t c_oneSide = size_t(std::sqrt(NUM_SAMPLES));
    const float c_cellSize = 1.0f / float(c_oneSide+1);
    for (size_t iy = 0; iy &lt; c_oneSide; ++iy)
    {
        for (size_t ix = 0; ix &lt; c_oneSide; ++ix)
        {
            size_t sampleIndex = iy * c_oneSide + ix;

            samples[sampleIndex][0] = float(ix + 1) / (float(c_oneSide + 1));
            if (jitter)
                samples[sampleIndex][0] += RandomFloat(-c_cellSize*0.5f, c_cellSize*0.5f);

            samples[sampleIndex][1] = float(iy + 1) / (float(c_oneSide) + 1.0f);
            if (jitter)
                samples[sampleIndex][1] += RandomFloat(-c_cellSize*0.5f, c_cellSize*0.5f);
        }
    }

    // save bitmap etc
    if (jitter)
        Test2D("2DUniformJitter.bmp", samples);
    else
        Test2D("2DUniform.bmp", samples);
}

//======================================================================================
void TestUniformRandom2D ()
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    const size_t c_oneSide = size_t(std::sqrt(NUM_SAMPLES));
    const float c_halfJitter = 1.0f / float((c_oneSide + 1) * 2);
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i][0] = RandomFloat(0.0f, 1.0f);
        samples[i][1] = RandomFloat(0.0f, 1.0f);
    }

    // save bitmap etc
    Test2D("2DUniformRandom.bmp", samples);
}

//======================================================================================
void TestSubRandomA2D (size_t regionsX, size_t regionsY)
{
    const float c_randomRangeX = 1.0f / float(regionsX);
    const float c_randomRangeY = 1.0f / float(regionsY);

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samples[i][0] = RandomFloat(0.0f, c_randomRangeX);
        samples[i][0] += float(i % regionsX) / float(regionsX);

        samples[i][1] = RandomFloat(0.0f, c_randomRangeY);
        samples[i][1] += float(i % regionsY) / float(regionsY);
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "2DSubRandomA_%zu_%zu.bmp", regionsX, regionsY);
    Test2D(fileName, samples);
}

//======================================================================================
void TestSubRandomB2D ()
{
    // calculate the sample points
    float samplex = RandomFloat(0.0f, 0.5f);
    float sampley = RandomFloat(0.0f, 0.5f);
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samplex = std::fmodf(samplex + 0.5f + RandomFloat(0.0f, 0.5f), 1.0f);
        sampley = std::fmodf(sampley + 0.5f + RandomFloat(0.0f, 0.5f), 1.0f);
        samples[i][0] = samplex;
        samples[i][1] = sampley;
    }
    
    // save bitmap etc
    Test2D("2DSubRandomB.bmp", samples);
}

//======================================================================================
void TestHalton (size_t basex, size_t basey)
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    const size_t c_oneSide = size_t(std::sqrt(NUM_SAMPLES));
    const float c_halfJitter = 1.0f / float((c_oneSide + 1) * 2);
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        // x axis
        samples[i][0] = 0.0f;
        {
            float denominator = float(basex);
            size_t n = i;
            while (n &gt; 0)
            {
                size_t multiplier = n % basex;
                samples[i][0] += float(multiplier) / denominator;
                n = n / basex;
                denominator *= basex;
            }
        }

        // y axis
        samples[i][1] = 0.0f;
        {
            float denominator = float(basey);
            size_t n = i;
            while (n &gt; 0)
            {
                size_t multiplier = n % basey;
                samples[i][1] += float(multiplier) / denominator;
                n = n / basey;
                denominator *= basey;
            }
        }
    }

    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "2DHalton_%zu_%zu.bmp", basex, basey);
    Test2D(fileName, samples);
}

//======================================================================================
void TestSobol2D ()
{
    // calculate the sample points

    // x axis
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        size_t ruler = Ruler(i + 1);
        size_t direction = size_t(size_t(1) &lt;&lt; size_t(31 - ruler));
        sampleInt = sampleInt ^ direction;
        samples[i][0] = float(sampleInt) / std::pow(2.0f, 32.0f);
    }

    // y axis
    // Code adapted from http://web.maths.unsw.edu.au/~fkuo/sobol/
    // uses numbers: new-joe-kuo-6.21201

    // Direction numbers
    std::vector&lt;size_t&gt; V;
    V.resize((size_t)ceil(log((double)NUM_SAMPLES) / log(2.0)));
    V[0] = size_t(1) &lt;&lt; size_t(31);
    for (size_t i = 1; i &lt; V.size(); ++i)
        V[i] = V[i - 1] ^ (V[i - 1] &gt;&gt; 1);

    // Samples
    sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i) {
        size_t ruler = Ruler(i + 1);
        sampleInt = sampleInt ^ V[ruler];
        samples[i][1] = float(sampleInt) / std::pow(2.0f, 32.0f);
    }

    // save bitmap etc
    Test2D("2DSobol.bmp", samples);
}

//======================================================================================
void TestHammersley2D (size_t truncateBits)
{
    // figure out how many bits we are working in.
    size_t value = 1;
    size_t numBits = 0;
    while (value &lt; NUM_SAMPLES)
    {
        value *= 2;
        ++numBits;
    }

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    size_t sampleInt = 0;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        // x axis
        samples[i][0] = 0.0f;
        {
            size_t n = i &gt;&gt; truncateBits;
            float base = 1.0f / 2.0f;
            while (n)
            {
                if (n &amp; 1)
                    samples[i][0] += base;
                n /= 2;
                base /= 2.0f;
            }
        }

        // y axis
        samples[i][1] = 0.0f;
        {
            size_t n = i &gt;&gt; truncateBits;
            size_t mask = size_t(1) &lt;&lt; (numBits - 1 - truncateBits);

            float base = 1.0f / 2.0f;
            while (mask)
            {
                if (n &amp; mask)
                    samples[i][1] += base;
                mask /= 2;
                base /= 2.0f;
            }
        }
    }


    // save bitmap etc
    char fileName[256];
    sprintf(fileName, "2DHammersley_%zu.bmp", truncateBits);
    Test2D(fileName, samples);
}

//======================================================================================
void TestRooks2D ()
{
    // make and shuffle rook positions
    std::random_device rd;
    std::mt19937 mt(rd());
    std::array&lt;size_t, NUM_SAMPLES&gt; rookPositions;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
        rookPositions[i] = i;
    std::shuffle(rookPositions.begin(), rookPositions.end(), mt);

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        // x axis
        samples[i][0] = float(rookPositions[i]) / float(NUM_SAMPLES-1);

        // y axis
        samples[i][1] = float(i) / float(NUM_SAMPLES - 1);
    }

    // save bitmap etc
    Test2D("2DRooks.bmp", samples);
}

//======================================================================================
void TestIrrational2D (float irrationalx, float irrationaly, float seedx, float seedy)
{
    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    float samplex = seedx;
    float sampley = seedy;
    for (size_t i = 0; i &lt; NUM_SAMPLES; ++i)
    {
        samplex = std::fmodf(samplex + irrationalx, 1.0f);
        sampley = std::fmodf(sampley + irrationaly, 1.0f);

        samples[i][0] = samplex;
        samples[i][1] = sampley;
    }

    // save bitmap etc
    char irrationalxStr[256];
    sprintf(irrationalxStr, "%f", irrationalx);
    char irrationalyStr[256];
    sprintf(irrationalyStr, "%f", irrationaly);
    char seedxStr[256];
    sprintf(seedxStr, "%f", seedx);
    char seedyStr[256];
    sprintf(seedyStr, "%f", seedy);
    char fileName[256];
    sprintf(fileName, "2DIrrational_%s_%s_%s_%s.bmp", &amp;irrationalxStr[2], &amp;irrationalyStr[2], &amp;seedxStr[2], &amp;seedyStr[2]);
    Test2D(fileName, samples);
}

//======================================================================================
float MinimumDistance2D (const std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt;&amp; samples, size_t numSamples, float x, float y)
{
    // Used by poisson.
    // This returns the minimum distance that point (x,y) is away from the sample points, from [0, numSamples).
    float minimumDistance = 0.0f;
    for (size_t i = 0; i &lt; numSamples; ++i)
    {
        float distance = Distance(samples[i][0], samples[i][1], x, y);
        if (i == 0 || distance &lt; minimumDistance)
            minimumDistance = distance;
    }
    return minimumDistance;
}

//======================================================================================
void TestPoisson2D ()
{
    // every time we want to place a point, we generate this many points and choose the one farthest away from all the other points (largest minimum distance)
    const size_t c_bestOfAttempts = 100;

    // calculate the sample points
    std::array&lt;std::array&lt;float, 2&gt;, NUM_SAMPLES&gt; samples;
    for (size_t sampleIndex = 0; sampleIndex &lt; NUM_SAMPLES; ++sampleIndex)
    {
        // generate some random points and keep the one that has the largest minimum distance from any of the existing points
        float bestX = 0.0f;
        float bestY = 0.0f;
        float bestMinDistance = 0.0f;
        for (size_t attempt = 0; attempt &lt; c_bestOfAttempts; ++attempt)
        {
            float attemptX = RandomFloat(0.0f, 1.0f);
            float attemptY = RandomFloat(0.0f, 1.0f);
            float minDistance = MinimumDistance2D(samples, sampleIndex, attemptX, attemptY);

            if (minDistance &gt; bestMinDistance)
            {
                bestX = attemptX;
                bestY = attemptY;
                bestMinDistance = minDistance;
            }
        }
        samples[sampleIndex][0] = bestX;
        samples[sampleIndex][1] = bestY;
    }

    // save bitmap etc
    Test2D("2DPoisson.bmp", samples);
}

//======================================================================================
int main (int argc, char **argv)
{
    // 1D tests
    {
        TestUniform1D(false);
        TestUniform1D(true);

        TestUniformRandom1D();

        TestSubRandomA1D(2);
        TestSubRandomA1D(4);
        TestSubRandomA1D(8);
        TestSubRandomA1D(16);
        TestSubRandomA1D(32);

        TestSubRandomB1D();

        TestVanDerCorput(2);
        TestVanDerCorput(3);
        TestVanDerCorput(4);
        TestVanDerCorput(5);

        // golden ratio mod 1 aka (sqrt(5) - 1)/2
        TestIrrational1D(0.618034f, 0.0f);
        TestIrrational1D(0.618034f, 0.385180f);
        TestIrrational1D(0.618034f, 0.775719f);
        TestIrrational1D(0.618034f, 0.287194f);

        // sqrt(2) - 1
        TestIrrational1D(0.414214f, 0.0f);
        TestIrrational1D(0.414214f, 0.385180f);
        TestIrrational1D(0.414214f, 0.775719f);
        TestIrrational1D(0.414214f, 0.287194f);

        // PI mod 1
        TestIrrational1D(0.141593f, 0.0f);
        TestIrrational1D(0.141593f, 0.385180f);
        TestIrrational1D(0.141593f, 0.775719f);
        TestIrrational1D(0.141593f, 0.287194f);
        
        TestSobol1D();

        TestHammersley1D(0);
        TestHammersley1D(1);
        TestHammersley1D(2);

        TestPoisson1D();
    }

    // 2D tests
    {
        TestUniform2D(false);
        TestUniform2D(true);

        TestUniformRandom2D();

        TestSubRandomA2D(2, 2);
        TestSubRandomA2D(2, 3);
        TestSubRandomA2D(3, 11);
        TestSubRandomA2D(3, 97);

        TestSubRandomB2D();

        TestHalton(2, 3);
        TestHalton(5, 7);
        TestHalton(13, 9);

        TestSobol2D();

        TestHammersley2D(0);
        TestHammersley2D(1);
        TestHammersley2D(2);

        TestRooks2D();

        // X axis = golden ratio mod 1 aka (sqrt(5)-1)/2
        // Y axis = sqrt(2) mod 1
        TestIrrational2D(0.618034f, 0.414214f, 0.0f, 0.0f);
        TestIrrational2D(0.618034f, 0.414214f, 0.775719f, 0.264045f);

        // X axis = sqrt(2) mod 1
        // Y axis = sqrt(3) mod 1
        TestIrrational2D(std::fmodf((float)std::sqrt(2.0f), 1.0f), std::fmodf((float)std::sqrt(3.0f), 1.0f), 0.0f, 0.0f);
        TestIrrational2D(std::fmodf((float)std::sqrt(2.0f), 1.0f), std::fmodf((float)std::sqrt(3.0f), 1.0f), 0.775719f, 0.264045f);

        TestPoisson2D();
    }

    #if CALCULATE_DISCREPANCY
        printf("\n");
        system("pause");
    #endif
}
</pre>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shecc: A self-hosting and educational C optimizing compiler (115 pts)]]></title>
            <link>https://github.com/sysprog21/shecc</link>
            <guid>38905182</guid>
            <pubDate>Sun, 07 Jan 2024 21:12:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sysprog21/shecc">https://github.com/sysprog21/shecc</a>, See on <a href="https://news.ycombinator.com/item?id=38905182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">shecc : self-hosting and educational C optimizing compiler</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/18013815/91671374-b2f0db00-eb58-11ea-8d55-858e9fb160c0.png"><img src="https://user-images.githubusercontent.com/18013815/91671374-b2f0db00-eb58-11ea-8d55-858e9fb160c0.png" alt="logo image" width="40%"></a></p>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto"><code>shecc</code> is built from scratch, targeting both 32-bit Arm and RISC-V architectures,
as a self-compiling compiler for a subset of the C language.
Despite its simplistic nature, it is capable of performing basic optimization strategies as a standalone optimizing compiler.</p>
<h3 tabindex="-1" dir="auto">Features</h3>
<ul dir="auto">
<li>Generate executable Linux ELF binaries for ARMv7-A and RV32IM.</li>
<li>Provide a minimal C standard library for basic I/O on GNU/Linux.</li>
<li>The cross-compiler is written in ANSI C, making it compatible with most platforms.</li>
<li>Include a self-contained C front-end with an integrated machine code generator; no external assembler or linker needed.</li>
<li>Utilize a two-pass compilation process: the first pass checks syntax and breaks down complex statements into basic operations,
while the second pass translates these operations into Arm/RISC-V machine code.</li>
<li>Develop a register allocation system that is compatible with RISC-style architectures.</li>
<li>Implement an architecture-independent, single static assignment (SSA)-based middle-end for enhanced optimizations.</li>
</ul>
<h2 tabindex="-1" dir="auto">Compatibility</h2>
<p dir="auto"><code>shecc</code> is capable of compiling C source files written in the following
syntax:</p>
<ul dir="auto">
<li>data types: char, int, struct, and pointer</li>
<li>condition statements: if, while, for, switch, case, break, return, and
general expressions</li>
<li>compound assignments: <code>+=</code>, <code>-=</code>, <code>*=</code></li>
<li>global/local variable initializations for supported data types
<ul dir="auto">
<li>e.g. <code>int i = [expr]</code></li>
</ul>
</li>
<li>limited support for preprocessor directives: <code>#define</code>, <code>#ifdef</code>, <code>#elif</code>, <code>#endif</code>, <code>#undef</code>, and <code>#error</code></li>
<li>non-nested variadic macros with <code>__VA_ARGS__</code> identifier</li>
</ul>
<p dir="auto">The backend targets armv7hf with Linux ABI, verified on Raspberry Pi 3,
and also supports RISC-V 32-bit architecture, verified with QEMU.</p>
<h2 tabindex="-1" dir="auto">Bootstrapping</h2>
<p dir="auto">The steps to validate <code>shecc</code> bootstrapping:</p>
<ol dir="auto">
<li><code>stage0</code>: <code>shecc</code> source code is initially compiled using an ordinary compiler
which generates a native executable. The generated compiler can be used as a
cross-compiler.</li>
<li><code>stage1</code>: The built binary reads its own source code as input and generates an
ARMv7-A/RV32IM  binary.</li>
<li><code>stage2</code>: The generated ARMv7-A/RV32IM binary is invoked (via QEMU or running on
Arm and RISC-V devices) with its own source code as input and generates another
ARMv7-A/RV32IM binary.</li>
<li><code>bootstrap</code>: Build the <code>stage1</code> and <code>stage2</code> compilers, and verify that they are
byte-wise identical. If so, <code>shecc</code> can compile its own source code and produce
new versions of that same program.</li>
</ol>
<h2 tabindex="-1" dir="auto">Prerequisites</h2>
<p dir="auto">Code generator in <code>shecc</code> does not rely on external utilities. You only need
ordinary C compilers such as <code>gcc</code> and <code>clang</code>. However, <code>shecc</code> would bootstrap
itself, and Arm/RISC-V ISA emulation is required. Install QEMU for Arm/RISC-V user
emulation on GNU/Linux:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo apt-get install qemu-user"><pre>$ sudo apt-get install qemu-user</pre></div>
<p dir="auto">It is still possible to build <code>shecc</code> on macOS or Microsoft Windows. However,
the second stage bootstrapping would fail due to <code>qemu-arm</code> absence.</p>
<p dir="auto">To execute the snapshot test, install the packages below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo apt-get install graphviz jq"><pre>$ sudo apt-get install graphviz jq</pre></div>
<h2 tabindex="-1" dir="auto">Build and Verify</h2>
<p dir="auto">Configure which backend you want, <code>shecc</code> supports ARMv7-A and RV32IM backend:</p>
<div data-snippet-clipboard-copy-content="$ make config ARCH=arm
# Target machine code switch to Arm

$ make config ARCH=riscv
# Target machine code switch to RISC-V"><pre><code>$ make config ARCH=arm
# Target machine code switch to Arm

$ make config ARCH=riscv
# Target machine code switch to RISC-V
</code></pre></div>
<p dir="auto">Run <code>make</code> and you should see this:</p>
<div data-snippet-clipboard-copy-content="  CC+LD	out/inliner
  GEN	out/libc.inc
  CC	out/src/main.o
  LD	out/shecc
  SHECC	out/shecc-stage1.elf
  SHECC	out/shecc-stage2.elf"><pre><code>  CC+LD	out/inliner
  GEN	out/libc.inc
  CC	out/src/main.o
  LD	out/shecc
  SHECC	out/shecc-stage1.elf
  SHECC	out/shecc-stage2.elf
</code></pre></div>
<p dir="auto">File <code>out/shecc</code> is the first stage compiler. Its usage:</p>
<div data-snippet-clipboard-copy-content="shecc [-o output] [--no-libc] [--dump-ir] <infile.c>"><pre><code>shecc [-o output] [--no-libc] [--dump-ir] &lt;infile.c&gt;
</code></pre></div>
<p dir="auto">Compiler options:</p>
<ul dir="auto">
<li><code>-o</code> : output file name (default: out.elf)</li>
<li><code>--no-libc</code> : Exclude embedded C library (default: embedded)</li>
<li><code>--dump-ir</code> : Dump intermediate representation (IR)</li>
</ul>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ out/shecc -o fib tests/fib.c
$ chmod +x fib
$ qemu-arm fib"><pre>$ out/shecc -o fib tests/fib.c
$ chmod +x fib
$ qemu-arm fib</pre></div>
<p dir="auto">Verify that the emitted IRs are identical to the snapshots by specifying <code>check-snapshots</code> target when invoking <code>make</code>:</p>

<p dir="auto"><code>shecc</code> comes with unit tests. To run the tests, give <code>check</code> as an argument:</p>

<p dir="auto">Reference output:</p>
<div data-snippet-clipboard-copy-content="...
int main(int argc, int argv) { exit(sizeof(char)); } => 1
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; case 3: a = 10; break; case 1: return 0; } exit(a); } => 10
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; default: a = 10; break; } exit(a); } => 10
OK"><pre><code>...
int main(int argc, int argv) { exit(sizeof(char)); } =&gt; 1
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; case 3: a = 10; break; case 1: return 0; } exit(a); } =&gt; 10
int main(int argc, int argv) { int a; a = 0; switch (3) { case 0: return 2; default: a = 10; break; } exit(a); } =&gt; 10
OK
</code></pre></div>
<p dir="auto">To clean up the generated compiler files, execute the command <code>make clean</code>.
For resetting architecture configurations, use the command <code>make distclean</code>.</p>
<h2 tabindex="-1" dir="auto">Intermediate Representation</h2>
<p dir="auto">Once the option <code>--dump-ir</code> is passed to <code>shecc</code>, the intermediate representation (IR)
will be generated. Take the file <code>tests/fib.c</code> for example. It consists of a recursive
Fibonacci sequence function.</p>
<div dir="auto" data-snippet-clipboard-copy-content="int fib(int n)
{
    if (n == 0)
        return 0;
    else if (n == 1)
        return 1;
    return fib(n - 1) + fib(n - 2);
}"><pre><span>int</span> <span>fib</span>(<span>int</span> <span>n</span>)
{
    <span>if</span> (<span>n</span> <span>==</span> <span>0</span>)
        <span>return</span> <span>0</span>;
    <span>else</span> <span>if</span> (<span>n</span> <span>==</span> <span>1</span>)
        <span>return</span> <span>1</span>;
    <span>return</span> <span>fib</span>(<span>n</span> <span>-</span> <span>1</span>) <span>+</span> <span>fib</span>(<span>n</span> <span>-</span> <span>2</span>);
}</pre></div>
<p dir="auto">Execute the following to generate IR:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ out/shecc --dump-ir -o fib tests/fib.c"><pre>$ out/shecc --dump-ir -o fib tests/fib.c</pre></div>
<p dir="auto">Line-by-line explanation between C source and IR:</p>
<div dir="auto" data-snippet-clipboard-copy-content=" C Source                IR                                         Explanation
------------------+---------------------------------------+----------------------------------------------------------------------------------

int fib(int n)      def int @fib(int %n)                    Indicate a function definition
{                   {
  if (n == 0)         const %.t1001, $0                     Load constant 0 into a temporary variable &quot;.t1001&quot;
                      %.t1002 = eq %n, %.t1001              Test &quot;n&quot; equals &quot;.t1001&quot; or not, and write the result in temporary variable &quot;.t1002&quot;
                      br %.t1002, .label.1177, .label.1178  If &quot;.t1002&quot; equals zero, goto false label &quot;.label.1178&quot;, otherwise,
                                                            goto true label &quot;.label.1177&quot;
                    .label.1177
    return 0;         const %.t1003, $0                     Load constant 0 into a temporary variable &quot;.t1003&quot;
                      ret %.t1003                           Return &quot;.t1003&quot;
                      j .label.1184                         Jump to endif label &quot;.label.1184&quot;
                    .label.1178
  else if (n == 1)    const %.t1004, $1                     Load constant 1 into a temporary variable &quot;.t1004&quot;
                      %.t1005 = eq %n, %.t1004              Test &quot;n&quot; equals &quot;.t1004&quot; or not, and write the result in temporary variable &quot;.t1005&quot;
                      br %.t1005, .label.1183, .label.1184  If &quot;.t1005&quot; equals zero, goto false label &quot;.label.1184&quot;. Otherwise,
                                                            goto true label &quot;.label.1183&quot;
                    .label.1183
    return 1;         const %.t1006, $1                     Load constant 1 into a temporary variable &quot;.t1006&quot;
                      ret %.t1006                           Return &quot;.t1006&quot;
                    .label.1184
  return
    fib(n - 1)        const %.t1007, $1                     Load constant 1 into a temporary variable &quot;.t1007&quot;
                      %.t1008 = sub %n, %.t1007             Subtract &quot;.t1007&quot; from &quot;n&quot;, and store the result in temporary variable &quot;.t1008&quot;
                      push %.t1008                          Prepare parameter for function call
                      call @fib, 1                          Call function &quot;fib&quot; with one parameter
    +                 retval %.t1009                        Store return value in temporary variable &quot;.t1009&quot;
    fib(n - 2);       const %.t1010, $2                     Load constant 2 into a temporary variable &quot;.t1010&quot;
                      %.t1011 = sub %n, %.t1010             Subtract &quot;.t1010&quot; from &quot;n&quot;, and store the result in temporary variable &quot;.t1011&quot;
                      push %.t1011                          Prepare parameter for function call
                      call @fib, 1                          Call function &quot;fib&quot; with one parameter
                      retval %.t1012                        Store return value in temporary variable &quot;.t1012&quot;
                      %.t1013 = add %.t1009, %.t1012        Add &quot;.t1009&quot; and &quot;.t1012&quot;, and store the result in temporary variable &quot;.t1013&quot;
                      ret %.t1013                           Return &quot;.t1013&quot;
}                   }"><pre> <span>C</span> <span>Source</span>                <span>IR</span>                                         <span>Explanation</span>
<span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>+</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>-</span><span>+</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span><span>--</span>

<span>int</span> <span>fib</span>(<span>int</span> <span>n</span>)      <span>def</span> <span>int</span> @<span>fib</span>(<span>int</span> %<span>n</span>)                    <span>Indicate</span> <span>a</span> <span>function</span> <span>definition</span>
{                   {
  <span>if</span> (<span>n</span> <span>==</span> <span>0</span>)         <span>const</span> %.<span>t1001</span>, $<span>0</span>                     <span>Load</span> <span>constant</span> <span>0</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> <span>".t1001"</span>
                      %.<span>t1002</span> <span>=</span> <span>eq</span> %<span>n</span>, %.<span>t1001</span>              <span>Test</span> <span>"n"</span> <span>equals</span> <span>".t1001"</span> <span>or</span> <span>not</span>, <span>and</span> <span>write</span> <span>the</span> <span>result</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1002"</span>
                      <span>br</span> %.<span>t1002</span>, .<span>label</span>.<span>1177</span>, .<span>label</span>.<span>1178</span>  <span>If</span> <span>".t1002"</span> <span>equals</span> <span>zero</span>, <span>goto</span> <span>false</span> <span>label</span> <span>".label.1178"</span>, <span>otherwise</span>,
                                                            <span>goto</span> <span>true</span> <span>label</span> <span>".label.1177"</span>
                    .<span>label</span>.<span>1177</span>
    <span>return</span> <span>0</span>;         <span>const</span> %.<span>t1003</span>, $<span>0</span>                     <span>Load</span> <span>constant</span> <span>0</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1003</span>"
                      <span>ret</span> %.<span>t1003</span>                           <span>Return</span> ".<span>t1003</span>"
                      <span>j</span> .<span>label</span><span>.1184</span>                         <span>Jump</span> <span>to</span> <span>endif</span> <span>label</span> <span>".label.1184"</span>
                    .<span>label</span>.<span>1178</span>
  <span>else</span> <span>if</span> (<span>n</span> <span>==</span> <span>1</span>)    <span>const</span> %.<span>t1004</span>, $<span>1</span>                     <span>Load</span> <span>constant</span> <span>1</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> <span>".t1004"</span>
                      %.<span>t1005</span> <span>=</span> <span>eq</span> %<span>n</span>, %.<span>t1004</span>              <span>Test</span> <span>"n"</span> <span>equals</span> <span>".t1004"</span> <span>or</span> <span>not</span>, <span>and</span> <span>write</span> <span>the</span> <span>result</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1005"</span>
                      <span>br</span> %.<span>t1005</span>, .<span>label</span>.<span>1183</span>, .<span>label</span>.<span>1184</span>  <span>If</span> <span>".t1005"</span> <span>equals</span> <span>zero</span>, <span>goto</span> <span>false</span> <span>label</span> <span>".label.1184"</span>. <span>Otherwise</span>,
                                                            <span>goto</span> <span>true</span> <span>label</span> <span>".label.1183"</span>
                    .<span>label</span>.<span>1183</span>
    <span>return</span> <span>1</span>;         <span>const</span> %.<span>t1006</span>, $<span>1</span>                     <span>Load</span> <span>constant</span> <span>1</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1006</span>"
                      <span>ret</span> %.<span>t1006</span>                           <span>Return</span> ".<span>t1006</span>"
                    .<span>label</span><span>.1184</span>
  <span>return</span>
    <span>fib</span>(<span>n</span> <span>-</span> <span>1</span>)        <span>const</span> %.<span>t1007</span>, $<span>1</span>                     <span>Load</span> <span>constant</span> <span>1</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1007</span>"
                      %.<span>t1008</span> <span>=</span> <span>sub</span> %<span>n</span>, %.<span>t1007</span>             <span>Subtract</span> ".<span>t1007</span><span>" from "</span><span>n</span><span>", and store the result in temporary variable "</span>.<span>t1008</span>"
                      <span>push</span> %.<span>t1008</span>                          <span>Prepare</span> <span>parameter</span> <span>for</span> <span>function</span> <span>call</span>
                      <span>call</span> @<span>fib</span>, <span>1</span>                          <span>Call</span> <span>function</span> "<span>fib</span><span>" with one parameter</span>
    <span>+</span>                 <span>retval</span> %.<span>t1009</span>                        <span>Store</span> <span>return</span> <span>value</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1009"</span>
    <span>fib</span>(<span>n</span> <span>-</span> <span>2</span>);       <span>const</span> %.<span>t1010</span>, $<span>2</span>                     <span>Load</span> <span>constant</span> <span>2</span> <span>into</span> <span>a</span> <span>temporary</span> <span>variable</span> ".<span>t1010</span>"
                      %.<span>t1011</span> <span>=</span> <span>sub</span> %<span>n</span>, %.<span>t1010</span>             <span>Subtract</span> ".<span>t1010</span><span>" from "</span><span>n</span><span>", and store the result in temporary variable "</span>.<span>t1011</span>"
                      <span>push</span> %.<span>t1011</span>                          <span>Prepare</span> <span>parameter</span> <span>for</span> <span>function</span> <span>call</span>
                      <span>call</span> @<span>fib</span>, <span>1</span>                          <span>Call</span> <span>function</span> "<span>fib</span>" with one parameter
                      <span>retval</span> %.<span>t1012</span>                        <span>Store</span> <span>return</span> <span>value</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1012"</span>
                      %.<span>t1013</span> <span>=</span> <span>add</span> %.<span>t1009</span>, %.<span>t1012</span>        <span>Add</span> <span>".t1009"</span> <span>and</span> <span>".t1012"</span>, <span>and</span> <span>store</span> <span>the</span> <span>result</span> <span>in</span> <span>temporary</span> <span>variable</span> <span>".t1013"</span>
                      <span>ret</span> %.<span>t1013</span>                           <span>Return</span> <span>".t1013"</span>
}                   }</pre></div>
<h2 tabindex="-1" dir="auto">Known Issues</h2>
<ol dir="auto">
<li>The generated ELF lacks of .bss and .rodata section</li>
<li>The support of varying number of function arguments is incomplete. No <code>&lt;stdarg.h&gt;</code> can be used.
Alternatively, check the implementation <code>printf</code> in source <code>lib/c.c</code> for <code>var_arg</code>.</li>
<li>The C front-end is a bit dirty because there is no effective AST.</li>
</ol>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><code>shecc</code> is freely redistributable under the BSD 2 clause license.
Use of this source code is governed by a BSD-style license that can be found in the <code>LICENSE</code> file.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mouse filmed tidying man's shed every night (151 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/jan/07/mouse-secretly-filmed-tidying-mans-shed-every-night</link>
            <guid>38905178</guid>
            <pubDate>Sun, 07 Jan 2024 21:11:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/jan/07/mouse-secretly-filmed-tidying-mans-shed-every-night">https://www.theguardian.com/world/2024/jan/07/mouse-secretly-filmed-tidying-mans-shed-every-night</a>, See on <a href="https://news.ycombinator.com/item?id=38905178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A mouse has been filmed secretly tidying up a man’s shed almost every night for two months.</p><p>Wildlife photographer Rodney Holbrook noticed that objects he left out of place were being mysteriously put back where they belonged overnight.</p><p>Holbrook, from Builth Wells in Powys, <a href="https://www.theguardian.com/uk/wales" data-link-name="in body link" data-component="auto-linked-tag">Wales</a>, set up a night vision camera on his workbench to find out what was happening, and captured footage reminiscent of the 2007 animated movie Ratatouille, where a rodent secretly cooks at a restaurant.</p><p>Holbrook, 75, told the BBC: “It has been going on for months. I call him Welsh Tidy Mouse. At first, I noticed that some food that I was putting out for the birds was ending up in some old shoes I was storing in the shed, so I set up a camera.”</p><figure id="98d56567-bcf1-43ff-940b-562d57d515bd" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Rodney Holbrook in his shed. " src="https://i.guim.co.uk/img/media/3adf1c5bb742820a1b218f6dc34ce892106c2f32/0_202_6016_3611/master/6016.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="267.10355718085106" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Rodney Holbrook was baffled by what was happening, so he set up a camera.</span> Photograph: Animal News Agency</figcaption></figure><p>Night vision footage showed the seemingly conscientious rodent gathering clothes pegs, corks, nuts and bolts, and placing them in a tray on Holbrook’s workbench.</p><p>Holbrook even experimented with leaving out different objects to see if the mouse could lift them, but the creature was undeterred and was even seen carrying cable ties to the pot.</p><p>“I couldn’t believe it when I saw that the mouse was tidying up,” Holbrook said.</p><p>“He moved all sorts of things into the box, bits of plastic, nuts and bolts. I don’t bother to tidy up now, as I know he will see to it. I leave things out of the box and they put it back in its place by the morning. Ninety-nine times out of 100 the mouse will tidy up throughout the night.”</p><p>A similar incident occurred in 2019, when a <a href="https://www.bbc.co.uk/news/uk-england-bristol-47625283" data-link-name="in body link">viral video showed a mouse “stockpiling” items in a man’s shed near Bristol.</a></p><p>Steve Mckears told reporters he thought he “was going mad” when screws and metal objects kept reappearing in a box containing bird feed. He set up a camera and captured footage of the mouse putting screws and other metal objects in the container.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitUI (374 pts)]]></title>
            <link>https://github.com/extrawurst/gitui</link>
            <guid>38905019</guid>
            <pubDate>Sun, 07 Jan 2024 20:53:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/extrawurst/gitui">https://github.com/extrawurst/gitui</a>, See on <a href="https://news.ycombinator.com/item?id=38905019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/extrawurst/gitui/blob/master/assets/logo.png"><img width="300px" src="https://github.com/extrawurst/gitui/raw/master/assets/logo.png"></a>
<p dir="auto"><a href="https://github.com/extrawurst/gitui/actions"><img src="https://github.com/extrawurst/gitui/workflows/CI/badge.svg" alt="CI"></a> <a href="https://crates.io/crates/gitui" rel="nofollow"><img src="https://camo.githubusercontent.com/fa710137de71d3027317b88573a00165cc985cf9e0b1e1c8340b69f0c51314a8/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f67697475692e737667" alt="crates" data-canonical-src="https://img.shields.io/crates/v/gitui.svg"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2bb6ac78e5a9f4f688a6a066cc71b62012101802fcdb478e6e4c6b6ec75dc694/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667"><img src="https://camo.githubusercontent.com/2bb6ac78e5a9f4f688a6a066cc71b62012101802fcdb478e6e4c6b6ec75dc694/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"></a> <a href="https://github.com/rust-secure-code/safety-dance/"><img src="https://camo.githubusercontent.com/4cae2784e68f964e197a4f5792390949288d7335430e3b4da3d0adb0a197bafb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f756e736166652d666f7262696464656e2d737563636573732e737667" alt="UNSAFE" data-canonical-src="https://img.shields.io/badge/unsafe-forbidden-success.svg"></a> <a href="https://extrawurst.itch.io/gitui" rel="nofollow"><img src="https://camo.githubusercontent.com/abe58c6a1baab77730331d1c95aba28b67a44aeccc609b483dbd600ca2ffadb0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f697463682e696f2d6f6b2d677265656e" alt="ITCH" data-canonical-src="https://img.shields.io/badge/itch.io-ok-green"></a> <a href="https://twitter.com/intent/follow?screen_name=extrawurst" rel="nofollow"><img src="https://camo.githubusercontent.com/f6045d21c7c8a8ccb64e31b309e50db34422083e9fd81f982839bd87312fe6b9/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f657874726177757273743f6c6162656c3d666f6c6c6f77267374796c653d736f6369616c" alt="TWEET" data-canonical-src="https://img.shields.io/twitter/follow/extrawurst?label=follow&amp;style=social"></a> <a href="https://deps.rs/repo/github/extrawurst/gitui" rel="nofollow"><img src="https://camo.githubusercontent.com/14d4c73d6a4109c7c11c19f4c3eb19dc3f5daf88325c1bde0c15603c04ac9d84/68747470733a2f2f646570732e72732f7265706f2f6769746875622f657874726177757273742f67697475692f7374617475732e737667" alt="dep_status" data-canonical-src="https://deps.rs/repo/github/extrawurst/gitui/status.svg"></a></p>
</h2>
<h5 tabindex="-1" dir="auto">GitUI provides you with the comfort of a git GUI but right in your terminal</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/extrawurst/gitui/blob/master/demo.gif"><img src="https://github.com/extrawurst/gitui/raw/master/demo.gif" alt="" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto"><a name="user-content-table-of-contents"></a> Table of Contents</h2>
<ol dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#bench">Benchmarks</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#build">Build</a></li>
<li><a href="#faqs">FAQs</a></li>
<li><a href="#diagnostics">Diagnostics</a></li>
<li><a href="#theme">Color Theme</a></li>
<li><a href="#bindings">Key Bindings</a></li>
<li><a href="#sponsoring">Sponsoring</a></li>
<li><a href="#inspiration">Inspiration</a></li>
</ol>
<h2 tabindex="-1" dir="auto">1. <a name="user-content-features"></a> Features <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<ul dir="auto">
<li>Fast and intuitive <strong>keyboard only</strong> control</li>
<li>Context based help (<strong>no need to memorize</strong> tons of hot-keys)</li>
<li>Inspect, commit, and amend changes (incl. hooks: <em>pre-commit</em>,<em>commit-msg</em>,<em>post-commit</em>,<em>prepare-commit-msg</em>)</li>
<li>Stage, unstage, revert and reset files, hunks and lines</li>
<li>Stashing (save, pop, apply, drop, and inspect)</li>
<li>Push / Fetch to / from remote</li>
<li>Branch List (create, rename, delete, checkout, remotes)</li>
<li>Browse / <strong>Search</strong> commit log, diff committed changes</li>
<li>Responsive terminal UI</li>
<li>Async git API for fluid control</li>
<li>Submodule support</li>
</ul>
<h2 tabindex="-1" dir="auto">2. <a name="user-content-motivation"></a> Motivation <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">I do most of my git work in a terminal but I frequently found myself using git GUIs for some use-cases like: index, commit, diff, stash, blame and log.</p>
<p dir="auto">Unfortunately popular git GUIs all fail on giant repositories or become unresponsive and unusable.</p>
<p dir="auto">GitUI provides you with the user experience and comfort of a git GUI but right in your terminal while being portable, fast, free and opensource.</p>
<h2 tabindex="-1" dir="auto">3. <a name="user-content-bench"></a> Benchmarks <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">For a <a href="https://youtu.be/rpilJV-eIVw?t=5334" rel="nofollow">RustBerlin meetup presentation</a> (<a href="https://github.com/extrawurst/gitui-presentation">slides</a>) I compared <code>lazygit</code>,<code>tig</code> and <code>gitui</code> by parsing the entire Linux git repository (which contains over 900k commits):</p>
<table>
<thead>
<tr>
<th></th>
<th>Time</th>
<th>Memory (GB)</th>
<th>Binary (MB)</th>
<th>Freezes</th>
<th>Crashes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>gitui</code></td>
<td><strong>24 s</strong> ✅</td>
<td><strong>0.17</strong> ✅</td>
<td>1.4</td>
<td><strong>No</strong> ✅</td>
<td><strong>No</strong> ✅</td>
</tr>
<tr>
<td><code>lazygit</code></td>
<td>57 s</td>
<td>2.6</td>
<td>16</td>
<td>Yes</td>
<td>Sometimes</td>
</tr>
<tr>
<td><code>tig</code></td>
<td>4 m 20 s</td>
<td>1.3</td>
<td><strong>0.6</strong> ✅</td>
<td>Sometimes</td>
<td><strong>No</strong> ✅</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">4. <a name="user-content-roadmap"></a> Road(map) to 1.0 <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">These are the high level goals before calling out <code>1.0</code>:</p>
<ul dir="auto">
<li>visualize branching structure in log tab (<a href="https://github.com/extrawurst/gitui/issues/81" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/81/hovercard">#81</a>)</li>
<li>interactive rebase (<a href="https://github.com/extrawurst/gitui/issues/32" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/32/hovercard">#32</a>)</li>
</ul>
<h2 tabindex="-1" dir="auto">5. <a name="user-content-limitations"></a> Known Limitations <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<ul dir="auto">
<li>no sparse repo support (see <a href="https://github.com/extrawurst/gitui/issues/1226" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/1226/hovercard">#1226</a>)</li>
<li>no support for GPG signing (see <a href="https://github.com/extrawurst/gitui/issues/97" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/97/hovercard">#97</a>)</li>
<li>no git-lfs support (see <a href="https://github.com/extrawurst/gitui/discussions/1089" data-hovercard-type="discussion" data-hovercard-url="/extrawurst/gitui/discussions/1089/hovercard">#1089</a>)</li>
<li><em>credential.helper</em> for https needs to be <strong>explicitly</strong> configured (see <a href="https://github.com/extrawurst/gitui/issues/800" data-hovercard-type="issue" data-hovercard-url="/extrawurst/gitui/issues/800/hovercard">#800</a>)</li>
</ul>
<p dir="auto">Currently, this tool does not fully substitute the <em>git shell</em>, however both tools work well in tandem.</p>
<p dir="auto">The priorities for <code>gitui</code> are on features that are making me mad when done on the <em>git shell</em>, like stashing, staging lines or hunks. Eventually, I will be able to work on making <code>gitui</code> a one stop solution - but for that I need help - this is just a spare time project for now.</p>
<p dir="auto">All support is welcomed! Sponsors as well! ❤️</p>
<h2 tabindex="-1" dir="auto">6. <a name="user-content-installation"></a> Installation <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">GitUI is in beta and may contain bugs and missing features. However, for personal use it is reasonably stable and is being used while developing itself.</p>
<a href="https://repology.org/project/gitui/versions" rel="nofollow">
    <img src="https://camo.githubusercontent.com/0c4c1bfaf62801fbb97ab015e176e45525a135c2483440964fffd9ad7696189e/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f67697475692e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/gitui.svg">
</a>
<h3 tabindex="-1" dir="auto">Various Package Managers</h3>
<details>
  <summary>Install Instructions</summary>
<h5 tabindex="-1" dir="auto"><a href="https://archlinux.org/packages/extra/x86_64/gitui/" rel="nofollow">Arch Linux</a></h5>

<h5 tabindex="-1" dir="auto">Fedora</h5>

<h5 tabindex="-1" dir="auto">Gentoo</h5>
<p dir="auto">Available in <a href="https://github.com/gentoo-mirror/dm9pZCAq">dm9pZCAq overlay</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo eselect repository enable dm9pZCAq
sudo emerge --sync dm9pZCAq
sudo emerge dev-vcs/gitui::dm9pZCAq"><pre>sudo eselect repository <span>enable</span> dm9pZCAq
sudo emerge --sync dm9pZCAq
sudo emerge dev-vcs/gitui::dm9pZCAq</pre></div>
<h5 tabindex="-1" dir="auto"><a href="https://software.opensuse.org/package/gitui" rel="nofollow">openSUSE</a></h5>
<div dir="auto" data-snippet-clipboard-copy-content="sudo zypper install gitui"><pre>sudo zypper install gitui</pre></div>
<h5 tabindex="-1" dir="auto">Homebrew (macOS)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://ports.macports.org/port/gitui/details/" rel="nofollow">MacPorts (macOS)</a></h5>

<h5 tabindex="-1" dir="auto"><a href="https://github.com/microsoft/winget-pkgs/tree/master/manifests/s/StephanDilly/gitui">Winget</a> (Windows)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://github.com/ScoopInstaller/Main/blob/master/bucket/gitui.json">Scoop</a> (Windows)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://chocolatey.org/packages/gitui" rel="nofollow">Chocolatey</a> (Windows)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://search.nixos.org/packages?channel=unstable&amp;show=gitui&amp;from=0&amp;size=50&amp;sort=relevance&amp;query=gitui" rel="nofollow">Nix</a> (Nix/NixOS)</h5>
<p dir="auto">Nixpkg</p>
<div data-snippet-clipboard-copy-content="nix-env -iA nixpkgs.gitui"><pre><code>nix-env -iA nixpkgs.gitui
</code></pre></div>
<p dir="auto">NixOS</p>

<h5 tabindex="-1" dir="auto"><a href="https://github.com/termux/termux-packages/tree/master/packages/gitui">Termux</a> (Android)</h5>

<h5 tabindex="-1" dir="auto"><a href="https://anaconda.org/conda-forge/gitui" rel="nofollow">Anaconda</a></h5>
<div data-snippet-clipboard-copy-content="conda install -c conda-forge gitui "><pre><code>conda install -c conda-forge gitui 
</code></pre></div>
</details>
<h3 tabindex="-1" dir="auto">Release Binaries</h3>
<p dir="auto"><a href="https://github.com/extrawurst/gitui/releases">Available for download in releases</a></p>
<p dir="auto">Binaries available for:</p>
<h3 tabindex="-1" dir="auto">Linux</h3>
<ul dir="auto">
<li>gitui-linux-musl.tar.gz (linux on x86_64)</li>
<li>gitui-linux-aarch64.tar.gz (linux on 64 bit arm)</li>
<li>gitui-linux-arm.tar.gz</li>
<li>gitui-linux-armv7.tar.gz</li>
</ul>
<p dir="auto">All contain a single binary file</p>
<h3 tabindex="-1" dir="auto">macOS</h3>
<ul dir="auto">
<li>gitui-mac.tar.gz (intel Mac, uses Rosetta on Apple silicon, single binary)</li>
</ul>
<h3 tabindex="-1" dir="auto">Windows</h3>
<ul dir="auto">
<li>gitui-win.tar.gz (single 64bit binary)</li>
<li>gitui.msi (64bit Installer package)</li>
</ul>
<h2 tabindex="-1" dir="auto">7. <a name="user-content-build"></a> Build <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<h3 tabindex="-1" dir="auto">Requirements</h3>
<ul dir="auto">
<li>
<p dir="auto">Minimum supported <code>rust</code>/<code>cargo</code> version: <code>1.65</code></p>
<ul dir="auto">
<li>See <a href="https://www.rust-lang.org/tools/install" rel="nofollow">Install Rust</a></li>
</ul>
</li>
<li>
<p dir="auto">To build openssl dependency (see <a href="https://docs.rs/openssl/latest/openssl/" rel="nofollow">https://docs.rs/openssl/latest/openssl/</a>)</p>
<ul dir="auto">
<li>perl &gt;= 5.12 (strawberry perl works for windows <a href="https://strawberryperl.com/" rel="nofollow">https://strawberryperl.com/</a>)</li>
<li>a c compiler (msvc, gcc or clang, cargo will find it)</li>
</ul>
</li>
<li>
<p dir="auto">To run the complete test suite python is required (and it must be invokable as <code>python</code>)</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Cargo Install</h3>
<p dir="auto">The simplest way to start playing around with <code>gitui</code> is to have <code>cargo</code> build and install it with <code>cargo install gitui</code>. If you are not familiar with rust and cargo: <a href="https://doc.rust-lang.org/book/ch01-00-getting-started.html" rel="nofollow">Getting Started with Rust</a></p>
<h3 tabindex="-1" dir="auto">Cargo Features</h3>
<h4 tabindex="-1" dir="auto">trace-libgit</h4>
<p dir="auto">enable <code>libgit2</code> tracing</p>
<p dir="auto">works if <code>libgit2</code> builded with <code>-DENABLE_TRACE=ON</code></p>
<p dir="auto">this feature enabled by default, to disable: <code>cargo install --no-default-features</code></p>
<h2 tabindex="-1" dir="auto">8. <a name="user-content-faqs"></a> FAQs <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">see <a href="https://github.com/extrawurst/gitui/blob/master/FAQ.md">FAQs page</a></p>
<h2 tabindex="-1" dir="auto">9. <a name="user-content-diagnostics"></a> Diagnostics <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">To run with logging enabled run <code>gitui -l</code>.</p>
<p dir="auto">This will log to:</p>
<ul dir="auto">
<li>macOS: <code>$HOME/Library/Caches/gitui/gitui.log</code></li>
<li>Linux using <code>XDG</code>: <code>$XDG_CACHE_HOME/gitui/gitui.log</code></li>
<li>Linux: <code>$HOME/.cache/gitui/gitui.log</code></li>
<li>Windows: <code>%LOCALAPPDATA%/gitui/gitui.log</code></li>
</ul>
<h2 tabindex="-1" dir="auto">10. <a name="user-content-theme"></a> Color Theme <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/extrawurst/gitui/blob/master/assets/light-theme.png"><img src="https://github.com/extrawurst/gitui/raw/master/assets/light-theme.png" alt=""></a></p>
<p dir="auto"><code>gitui</code> should automatically work on both light and dark terminal themes.</p>
<p dir="auto">However, you can customize everything to your liking: See <a href="https://github.com/extrawurst/gitui/blob/master/THEMES.md">Themes</a>.</p>
<h2 tabindex="-1" dir="auto">11. <a name="user-content-bindings"></a> Key Bindings <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto">The key bindings can be customized: See <a href="https://github.com/extrawurst/gitui/blob/master/KEY_CONFIG.md">Key Config</a> on how to set them to <code>vim</code>-like bindings.</p>
<h2 tabindex="-1" dir="auto">12. <a name="user-content-sponsoring"></a> Sponsoring <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<p dir="auto"><a href="https://github.com/sponsors/extrawurst"><img src="https://camo.githubusercontent.com/a714ba0a49f7b58991cd598e0a5e29f6b5597ad5228ef84bde3b25479fa5a756/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d47697448756225323053706f6e736f72732d6661666266633f6c6f676f3d47697448756225323053706f6e736f7273" alt="github" data-canonical-src="https://img.shields.io/badge/-GitHub%20Sponsors-fafbfc?logo=GitHub%20Sponsors"></a></p>
<p dir="auto"><a href="https://liberapay.com/extrawurst/donate" rel="nofollow"><img alt="Donate using Liberapay" src="https://camo.githubusercontent.com/18cfbc8e266770af00126e8d82b29d1c2f047e6efb5243141f0810496f7c1c66/68747470733a2f2f6c69626572617061792e636f6d2f6173736574732f776964676574732f646f6e6174652e737667" data-canonical-src="https://liberapay.com/assets/widgets/donate.svg"></a></p>
<p dir="auto"><a href="https://ko-fi.com/B0B6GMW1T" rel="nofollow"><img height="36" src="https://camo.githubusercontent.com/1cccbe7e15949c13dca33cfe9142aed92e86101adc8778d3c3f695443a73de06/68747470733a2f2f73746f726167652e6b6f2d66692e636f6d2f63646e2f6b6f6669342e706e673f763d33" alt="Buy Me a Coffee at ko-fi.com" data-canonical-src="https://storage.ko-fi.com/cdn/kofi4.png?v=3"></a></p>
<h2 tabindex="-1" dir="auto">13. <a name="user-content-inspiration"></a> Inspiration <sup><a href="#table-of-contents">Top ▲</a></sup></h2>
<ul dir="auto">
<li><a href="https://github.com/jesseduffield/lazygit">lazygit</a></li>
<li><a href="https://github.com/jonas/tig">tig</a></li>
<li><a href="https://github.com/git-up/GitUp">GitUp</a>
<ul dir="auto">
<li>It would be nice to come up with a way to have the map view available in a terminal tool</li>
</ul>
</li>
<li><a href="https://github.com/andys8/git-brunch">git-brunch</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A concrete example of what goes wrong with Apple's docs (143 pts)]]></title>
            <link>https://www.amimetic.co.uk/blog/a-concrete-example-of-why-apples-docs-are-terrible/</link>
            <guid>38904721</guid>
            <pubDate>Sun, 07 Jan 2024 20:16:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amimetic.co.uk/blog/a-concrete-example-of-why-apples-docs-are-terrible/">https://www.amimetic.co.uk/blog/a-concrete-example-of-why-apples-docs-are-terrible/</a>, See on <a href="https://news.ycombinator.com/item?id=38904721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Apple's documentations is famously dreadful. In particular it is often missing or incomplete. <a href="https://nooverviewavailable.com/">No overview available</a> gives you some numbers on this but I think that actually understates the problem. Even when it is available it is often terrible. Here is a small concrete example I recently encountered while doing some MacOS development (an area I'm less familiar with).</p>
<p>I want to prompt the user to open a directory. How do I do that? Obviously I Google it (throwing in <code>Swift</code> to avoid ancient <code>Obj-C</code> answers).</p>
<p>Seems like I want something called <code>NSOpenPanel</code>.</p>
<p>I find <a href="https://developer.apple.com/documentation/appkit/nsopenpanel">Apple's docs</a> and it seems to be fairly clear. I configure with some straightforward options.</p>
<p>Okay how do I actually use it?</p>
<p>Hmmm. That is far from clear. There is literally nothing on the page that explains even the most basic use case. Sigh. Hit google again finding that I want to call <code>begin</code> and that that this accepts a callback which is given a result.</p>
<p>The example is out of date (a perpertual issue with Swift) but eventually sort that out in the editor (we now have a nice enum with the result).</p>
<p>Okay but how do I get the actual directory selected? The success value is <code>.OK</code>. Is there some kind of data with that? No. Just a result.</p>
<p>Hmmm...</p>
<p>There is a <a href="https://developer.apple.com/documentation/appkit/nsopensavepaneldelegate">delegate</a> that I can set. Maybe that gets the result.</p>
<p>Waste a few minutes on that, basically appears to be for configuration.</p>
<p>Ah, it inherits from <code>NSSavePanel</code>! That was not at all obvious (and I'm sorry OOP devotees, that makes damn all sense in principle). Oh, look, loads of new functions and properties. Ah, so I'm meant to query the NSOpenPanel object for the directory.</p>
<p>Not really clear how. Xcode offers a <code>url</code>, <code>urls</code>, <code>directoryUrl</code>, <code>representedUrl</code> among others (I'm not making any of those up and most didn't seem to be in documentation). Let's be optimistic and chose <code>url</code>. Success! It appears to work.</p>
<p>Oh, also missing (before I continue): in recent versions of MacOS need to add a capability for reading (and writing) to user selected directories. Again completely missing from documentation and yet another frustration for someone not already very familiar with the ecosystem.</p>
<p>This can be so much better. Apple developers sneer at Electron, but look at how <a href="https://electronjs.org/docs/api/dialog#dialogshowopendialogbrowserwindow-options">clear Electron's open dialog docs</a> are.</p>
<h2>What goes wrong with Apple's docs?</h2>
<ol>
<li>Missing and incomplete</li>
<li>Outdated (Obj-C or older Swift versioned docs are common)</li>
<li>Badly written (assumes a level of familiarity that would make reading the docs unnecessary). Lack of explanation.</li>
<li>Lack of examples of even basic usage.</li>
<li>Badly designed, legacy OOP APIs: Swift can sometimes get away without much documentation through good use of types. Sadly a lot of MacOS API's are a blend of old fashioned OOP with surprisingly low level details.</li>
</ol>
<h2>What if you are doing Apple development?</h2>
<p>Avoid Apple's docs (even where they are top Google results). They will likely be pretty bad. Stack Overflow is sometime good (but often an old Swift version). Some third party docs and good (Paul Hudson, Ray Wenderlich).</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Higher fees, more ads: streaming cashes in by using the old tactics of cable TV (133 pts)]]></title>
            <link>https://theconversation.com/with-higher-fees-and-more-ads-streaming-services-like-netflix-disney-and-hulu-are-cashing-in-by-using-the-old-tactics-of-cable-tv-215048</link>
            <guid>38904418</guid>
            <pubDate>Sun, 07 Jan 2024 19:39:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/with-higher-fees-and-more-ads-streaming-services-like-netflix-disney-and-hulu-are-cashing-in-by-using-the-old-tactics-of-cable-tv-215048">https://theconversation.com/with-higher-fees-and-more-ads-streaming-services-like-netflix-disney-and-hulu-are-cashing-in-by-using-the-old-tactics-of-cable-tv-215048</a>, See on <a href="https://news.ycombinator.com/item?id=38904418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>There’s one thing that television viewers can count on in 2024: higher fees and more commercials.</p>

<p>The major streaming services – Amazon, Netflix, Hulu, Disney+ and Max – have <a href="https://www.axios.com/2023/12/28/amazon-prime-netflix-disney-peacock-streaming-subscription">all announced</a> rate hikes and new advertising policies.</p>

<p>As I show in my new book, “<a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">24/7 Politics: Cable Television and the Fragmenting of America from Watergate to Fox News</a>,” the streaming boom that has imperiled cable television is actually built upon the very same business model that made television viewers pay for monthly subscriptions decades ago. </p>

<p>Like their cable predecessors, streaming companies have lured people in with promises of a better and cheaper viewing experience. Now that they have a robust subscriber base, they’re in the process of raising rates while also introducing more commercials and bundling programming to make customers pay more and more.</p>

<p>There is a difference, though. When cable companies tried similar tactics in the late 1980s, there was an uproar from politicians who called such business practices “unfair” to their constituents. Now, there’s nary a peep – a sign of just how inured Americans have become to the whims of corporations trying to squeeze their customers.</p>

<h2>Stemming the tide of ‘toll television’</h2>

<p>Like streaming companies, cable TV’s entrepreneurs in the 1960s saw the business potential of framing cable television as a path for more choice with fewer commercials.</p>

<p>At the time, <a href="https://www.worldradiohistory.com/BOOKSHELF-ARH/Regulatory/Television's-Guardians-The-FCC-1958-1967-Baughman-1985.pdf">federal regulations</a> squashed competition by allowing the “Big Three” broadcast networks — CBS, NBC and ABC — to dominate the airwaves as long as they also served a vaguely defined “<a href="https://www.rutgersuniversitypress.org/public-interests/9780813572291/">public interest</a>.” Advertisers underwrote the cost of programs, which meant that while viewers didn’t have to pay a monthly TV bill, they did have to endure commercials.</p>

<p>This business structure also encouraged programming with mass appeal in order to deliver the broadest possible audiences to advertisers. But not all TV viewers were happy with the formulaic quiz shows and sitcoms that dominated the airwaves. Sensing an untapped opportunity, TV entrepreneurs tried to concoct ways to circumvent the dominance of the Big Three. </p>

<p>Cable television actually dates back to the late 1940s. It was initially known as “<a href="https://tupress.temple.edu/books/blue-skies">community antenna television</a>,” or CATV, because it was used to bring broadcast signals to smaller communities that couldn’t get signals from the big cities.</p>

<p>At first, this technology simply expanded the reach of CBS, NBC and ABC rather than providing a competing service.  </p>

<figure>
            <p><img alt="Elderly man with mustache wearing suit smiles as he stands next to a young woman in black dress, who gazes his way." data-src="https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=772&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=772&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=772&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=970&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=970&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=970&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/567713/original/file-20240103-19-dfysy4.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>Former NBC executive Pat Weaver – here with his daughter, actress Sigourney Weaver – was an early proponent of subscription TV.</span>
              <span><a href="https://www.gettyimages.com/detail/news-photo/sigourney-weaver-and-sylvester-pat-weaver-during-the-year-news-photo/105909150?adppopup=true">Ron Galella Collection/Getty Images</a></span>
            </figcaption>
          </figure>

<p>But in 1963, a former NBC executive named Pat Weaver <a href="https://www.theatlantic.com/magazine/archive/1964/10/why-suppress-pay-tv-the-fight-in-california/658253/">proposed subscription television</a>, in which people would pay a monthly fee for access to specialized channels through a wired connection. </p>

<p>His company, STV, offered a way to sidestep the “vast land of advertising trivia” that beamed into living rooms across the nation, Weaver explained during one public forum. Weaver dreamed of how giving individual subscribers more choices could forge a business model that could break through the programming limitations of broadcast.</p>

<p>In the end, STV didn’t last. Broadcasters and theater owners mobilized to convince the public that such experiments would turn all television into pay TV, dividing Americans into those with television access and those without it. </p>

<p><a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">Broadcasting lobbyists warned</a> that “toll television” would “have an undemocratic and divisive effect” by depriving viewers of their right to consume television for free. <a href="https://go.gale.com/ps/i.do?p=AONE&amp;u=googlescholar&amp;id=GALE%7CA70451817&amp;v=2.1&amp;it=r&amp;asid=23eb4633">One flyer featured</a> a devastated young boy with a football helmet who didn’t have enough coins to insert in the television. </p>

<p>“Pop says he don’t have any more Dollar and a halfs for me to watch each ball game,” the caption read.</p>

<p>The dire warnings about the end of free TV worked, and voters supported a <a href="https://ballotpedia.org/California_Proposition_15,_Prohibition_of_Paid_Television_Programming_Initiative_(1964)">state ballot initiative</a> in 1964 that outlawed subscription television. While the courts <a href="https://caselaw.findlaw.com/court/ca-supreme-court/1820874.html">overturned the new law</a> for violating the First Amendment, STV didn’t survive. </p>

<h2>Cable catches on</h2>

<p>But the idea of wired television delivering more choices to viewers persisted.  </p>

<p>As frustrations with the limits of broadcast television intensified <a href="https://www.cambridge.org/core/journals/modern-american-history/article/watergate-the-bipartisan-struggle-for-media-access-and-the-growth-of-cable-television/64F2A0E3B8D3EAD28F8E6449DEF11BDE">across the political spectrum during the 1970s</a>, consumers, elected officials and regulators all embraced the potential of cable television to offer an alternative.</p>

<p>By the mid-1970s, experiments with programming disseminated via satellite on cable systems tested new types of niche channels and shows – like nonstop movies, sports, music or the weather – to see if audiences might be interested. In 1975, HBO gambled that a live international boxing match between Muhammad Ali and Joe Frazier, “<a href="https://www.cnn.com/2016/06/04/sport/thrilla-in-manila-remembered/index.html">Thrilla in Manila</a>,” would boost its struggling pay-TV operation. </p>

<p>It did: Income from pay television services like HBO, which first launched in 1972, <a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">soared</a> from US$29 million in 1975 to $769 million in 1980. </p>

<p>Like STV before them, cable companies tapped into frustrations with broadcasting and its advertising model. They sold subscriptions by promising that premium channels like HBO could provide movies with “no cuts, no commercials.” </p>

<p>Millions of people eagerly signed up for cable subscriptions and premium channels like HBO that cost even extra.</p>

<h2>Deregulation nation</h2>

<p>Niche cable channels soon emerged that appealed to specific demographic groups. Black Entertainment Television created new opportunities for programming geared toward Black audiences. The Daytime Channel offered entertainment and news directed at women, while MTV connected a younger generation through music videos.</p>

<p>Then there was C-SPAN, a cable industry-funded initiative that put the cameras on the House of Representatives starting in 1979. In a 1984 letter to the network, an enthusiastic viewer praised the public affairs channel for providing “over-the-back-fence discussion with your neighbors on matters of common interest, but with the scope that the neighborhood extends to encompass all areas of the United States.”</p>

<p>Cable’s popularity buoyed the lobbying efforts of the industry, which was pushing Congress to deregulate key aspects of their business operations. In 1984, they succeeded: <a href="https://www.congress.gov/bill/98th-congress/senate-bill/66">The Cable Communications Policy Act of 1984</a> notably removed local government caps on what companies could charge for subscription services. </p>

<p>The consequences quickly became clear: price hikes and poor customer service. In the next few years, basic cable rates skyrocketed, <a href="https://press.princeton.edu/books/hardcover/9780691246666/247-politics">increasing by an average of 90%</a>.</p>

<h2>Playing political football</h2>

<p>Al Gore, then an ambitious senator representing Tennessee, saw an opportunity. He pounced on the issue, decrying how cable companies and lobbyists had leveraged consumer demand in ways that amounted to <a href="https://www.c-span.org/video/?9959-1/cable-telecommunications-act-day-1-part-1">what he described as</a> “total domination of the marketplace.”</p>

<figure>
            <p><img alt="Man with graying hair wearing suit being interviewed." data-src="https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=738&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=738&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=738&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=927&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=927&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=927&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/567738/original/file-20240103-23-mpnhrg.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>John Malone served as CEO of TCI for over 20 years.</span>
              <span><a href="https://www.gettyimages.com/detail/news-photo/john-malone-news-photo/1039963656?adppopup=true">Rick Maiman/Sygma via Getty Images</a></span>
            </figcaption>
          </figure>

<p><a href="https://www.c-span.org/video/?9959-1/cable-telecommunications-act-day-1-part-1">He condemned the industry</a> as an American “Cosa Nostra,” and having likened Tele-Communications Inc. (TCI) executive <a href="https://www.thegentlemansjournal.com/article/john-malone-everything-need-know-americas-single-largest-land-owner/">John Malone</a> to “Darth Vader,” Gore then lashed out at him during a 1989 congressional hearing for “shaking down” average Americans.</p>

<p>Malone pushed back, highlighting the unprecedented choice that people now had on cable. Rate increases allowed for experimentation with niche programming that never stood a chance on network broadcast television, he added. And they also helped pay the costs of laying – and then later upgrading – wires across the country to deliver such services.</p>

<h2>Everything old is new again</h2>

<p>Cable-bashing was effective on the campaign trail for Gore and his top-of-the-ticket running mate, Arkansas Gov. Bill Clinton. But, once in office, they changed tack. They wanted private industry to build the information highway they saw as <a href="https://www.dissentmagazine.org/article/let-them-eat-tech/">central to their governing agenda</a>, and cable companies were the ones who owned the coaxial wires going into millions of homes. </p>

<p>Four years later, Gore and Clinton celebrated the <a href="https://www.fcc.gov/general/telecommunications-act-1996">1996 Telecommunications Act</a>, which slashed many price regulatory measures Gore had championed while on the campaign trail in 1992.</p>

<p>The rationale? That the marketplace competition and programming choice alone could deliver for the public interest. </p>

<p>The result? The expansion of a media landscape forged on the terrain of private businesses and their profit margins.</p>

<p>Despite today’s frustrations with changes designed to boost bottom lines – rate hikes, <a href="https://www.washingtonpost.com/technology/2023/05/27/netflix-password-sharing-why-users-mad/">limits on password sharing</a>, <a href="https://www.theatlantic.com/technology/archive/2022/08/sports-streaming-makes-losers-us-all/671231/">exclusive streaming contracts for sporting events</a> – people no longer look to politicians to help them navigate and address these concerns as they once did. The bipartisan belief in deregulation has seemingly closed down these conversations about policy alternatives. </p>

<p>That’s why cable didn’t just blaze a path for a new business model. It also convinced elected officials and constituents to embrace a different understanding of the public interest, one where the market reigns supreme.</p>
  </div></div>]]></description>
        </item>
    </channel>
</rss>