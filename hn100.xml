<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 31 May 2025 20:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A Lean companion to Analysis I (111 pts)]]></title>
            <link>https://terrytao.wordpress.com/2025/05/31/a-lean-companion-to-analysis-i/</link>
            <guid>44145517</guid>
            <pubDate>Sat, 31 May 2025 16:55:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terrytao.wordpress.com/2025/05/31/a-lean-companion-to-analysis-i/">https://terrytao.wordpress.com/2025/05/31/a-lean-companion-to-analysis-i/</a>, See on <a href="https://news.ycombinator.com/item?id=44145517">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>
 Almost 20 years ago, I wrote a textbook in real analysis called “<a href="https://terrytao.wordpress.com/books/analysis-i/">Analysis I</a>“. It was intended to complement the many good available analysis textbooks out there by focusing more on foundational issues, such as the construction of the natural numbers, integers, rational numbers, and reals, as well as providing enough set theory and logic to allow students to develop proofs at high levels of rigor.
</p><p>
While some proof assistants such as Coq or Agda were well established when the book was written, formal verification was not on my radar at the time. However, now that I have had some experience with this subject, I realize that the content of this book is in fact very compatible with such proof assistants; in particular, the ‘naive type theory’ that I was implicitly using to do things like construct the standard number systems, dovetails well with the dependent type theory of Lean (which, among other things, has excellent support for quotient types).
</p><p>
I have therefore decided to launch a <a href="https://github.com/teorth/analysis">Lean companion to “Analysis I”</a>, which is a “translation” of many of the definitions, theorems, and exercises of the text into Lean. In particular, this gives an alternate way to perform the exercises in the book, by instead filling in the corresponding “sorries” in the Lean code. (I do not however plan on hosting “official” solutions to the exercises in this companion; instead, feel free to create forks of the repository in which these sorries are filled in.)
</p><p>
Currently, the following sections of the text have been translated into Lean: 

</p><ul> <li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_1.html">Section 2.1: The natural numbers</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_2.html">Section 2.2: Addition</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_3.html">Section 2.3: Multiplication</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_2_epilogue.html">Chapter 2 epilogue: Isomorphism with the Mathlib natural numbers</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_3_1.html">Section 3.1: Basic set theory</a> </li><li> <a href="https://teorth.github.io/estimate_tools/docs/Analysis/Section_4_1.html">Section 4.1: The integers</a> 
</li></ul>


<p>
The formalization has been deliberately designed to be separate from the standard Lean math library <a href="https://leanprover-community.github.io/mathlib4_docs/">Mathlib</a> at some places, but reliant on it at others. For instance, Mathlib already has a standard notion of the natural numbers <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf N}}">. In the Lean formalization, I first develop “by hand” an alternate construction <code>Chapter2.Nat</code> of the natural numbers (or just <code>Nat</code>, if one is working in the <code>Chapter2</code> namespace), setting up many of the basic results about these alternate natural numbers which parallel similar lemmas about <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{{\bf N}}"> that are already in Mathlib (but with many of these lemmas set as exercises to the reader, with the proofs currently replaced with “sorries”). Then, in an epilogue section, isomorphisms between these alternate natural numbers and the Mathlib natural numbers are established (or more precisely, set as exercises). From that point on, the Chapter 2 natural numbers are deprecated, and the Mathlib natural numbers are used instead. I intend to continue this general pattern throughout the book, so that as one advances into later chapters, one increasingly relies on Mathlib’s definitions and functions, rather than directly referring to any counterparts from earlier chapters. As such, this companion could also be used as an introduction to Lean and Mathlib as well as to real analysis (somewhat in the spirit of the “<a href="https://adam.math.hhu.de/">Natural number game</a>“, which in fact has significant thematic overlap with Chapter 2 of my text).
</p><p>
The code in this repository compiles in Lean, but I have not tested whether all of the (numerous) “sorries” in the code can actually be filled (i.e., if all the exercises can actually be solved in Lean). I would be interested in having volunteers “playtest” the companion to see if this can actually be done (and if the helper lemmas or “API” provided in the Lean files are sufficient to fill in the sorries in a conceptually straightforward manner without having to rely on more esoteric Lean programming techniques). Any other feedback will of course also be welcome.
</p><p>
[UPDATE, May 31: moved the companion to a standalone repository.]


</p>	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Precision Clock Mk IV (265 pts)]]></title>
            <link>https://mitxela.com/projects/precision_clock_mk_iv</link>
            <guid>44144750</guid>
            <pubDate>Sat, 31 May 2025 15:06:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitxela.com/projects/precision_clock_mk_iv">https://mitxela.com/projects/precision_clock_mk_iv</a>, See on <a href="https://news.ycombinator.com/item?id=44144750">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mxmain"><p><a href="https://mitxela.com/projects/hardware"><img onload="this.style.opacity=1;" src="https://mitxela.com/img/titles/mitxela_dot_com-65.png" title="Back to Hardware" alt="Back to Hardware"></a></p><p>31 May 2025<br><b>Progress: Complete</b></p><p>
This page is about the development of the Precision Clock Mk IV.</p><ul>
<li>If you would like to buy a precision clock, head to <a href="https://mitxela.com/shop/clock4">the shop page</a>.
</li><li>For the kit, see the <a href="https://mitxela.com/projects/precision_clock_mk_iv/assembly_instructions">assembly instructions</a>
</li><li>There is also a <a href="https://mitxela.com/projects/precision_clock_mk_iv/docs">user manual</a>
</li></ul><p>

<iframe width="704" height="396" src="https://www.youtube.com/embed/XL2cZjO5IUY" allowfullscreen=""></iframe></p><p>

I designed this clock years ago, with the intention to incorporate every feature request I ever received for the <a href="https://mitxela.com/projects/precision_clock_mk_iii">previous precision clock</a>. However, during the pandemic there was a chip shortage, where these STM32 parts became impossible to acquire. Given the amount of work I put into the clock, I didn't have the heart to redesign it using different parts, so instead I put it aside, and never got around to releasing it – until now.</p><h3 id="contents">Contents</h3>
<ul>
<li><a href="#summary">Summary</a>
</li><li><a href="#architecture">Architecture</a>
</li><li><a href="#interface">Interface</a>
</li><li><a href="#the-double-buffered-display">The double-buffered display</a>
</li><li><a href="#gps-discipline">GPS discipline</a>
</li><li><a href="#oscillator-oddness">Oscillator Oddness</a>
</li><li><a href="#interrupt-vector-hot-swap">Interrupt vector hot-swap</a>
</li><li><a href="#the-chain-loading-bootloader">The chain-loading bootloader</a>
</li><li><a href="#auto-timezone">Auto timezone</a>
</li><li><a href="#interrupt-priorities">Interrupt priorities</a>
</li><li><a href="#display-design">Display design</a>
</li><li><a href="#brightness-details">Brightness details</a>
</li><li><a href="#colon-control">Colon control</a>
</li><li><a href="#hinge-development">Hinge development</a>
</li><li><a href="#the-two-layer-challenge">The Two Layer Challenge</a>
</li><li><a href="#plastic-parts">Plastic Parts</a>
</li><li><a href="#logo">Logo</a>
</li><li><a href="#countdown-mode">Countdown mode</a>
</li><li><a href="#leap-seconds">Leap seconds</a>
</li><li><a href="#high-speed-testing">High speed testing</a>
</li><li><a href="#travel-testing">Travel testing</a>
</li><li><a href="#the-production-batch">The production batch</a>
</li><li><a href="#conclusion">Conclusion</a>
</li></ul><h3 id="summary">Summary</h3><p>
The specifications for the clock were as follows:</p><ul>
<li>Millisecond precision, with no perceptible jitter
</li><li>Display clearly, without flicker, when filmed at very high framerates (20,000fps or more). The brightness should still automatically adjust, of course, and without the use of PWM
</li><li>Timezone and offset should be determined automatically from the GPS coordinates, no customisation or user interaction needed
</li><li>Keep good time while turned off (a quartz crystal and watch battery)
</li><li>Easy antenna upgrades (an SMA connector instead of a fiddly u.FL)
</li><li>Easy repairs (all parts on the back are surface mount, so every component is accessible without having to unsolder digits)
</li><li>The precision should reflect accuracy. So if the GPS fix is lost and the time starts to drift, digits should progressively disappear from the end
</li><li>Multiple display modes, such as ISO-ordinal. More modes should be easy to enable and easy to expand in future (this means writing in C, not assembly!)
</li><li>Easy firmware updates. It's paramount that updating the timezone database, and the clock firmware, should be as simple as copying and pasting a file.
</li></ul><p>

Another request I often had was to split the display onto two lines, as it's very wide otherwise. But some people love the very wide design, for the ISO timestamp aesthetic. There was only one way to please everyone: add an articulated joint, so the clock can transform from one to the other.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/mk4-folded-front.jpg" width="1200" height="516" loading="lazy" alt="Front view of folded clock"></p><p>

This <b>substantially</b> complicates the design of the clock, as we've now got multiple displays to keep in sync, and multiple processors to update. The connection across the hinge has to convey power, bidirectional data and ground, but also provide the analog signal for brightness level, and a latch signal to accurately control the display refresh. All of this is done over four wires. It also does firmware updates over the same four wires.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/mk4-unfolded-rear.jpg" width="1200" height="200" loading="lazy" alt="Rear view of clock in unfolded position"></p><p>

The hinge is made from laser-cut Delrin for maximum strength. The intention with this design was, in the wide configuration, that the shape of the hinge would subtly imply the letter T. It's not illuminated, but it goes some way towards our ISO8601 compliance.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/mk4-unfolded-front.jpg" width="1200" height="200" loading="lazy" alt="Front view of clock in unfolded position"></p><p>

One of the feature requests I had was to produce the precision clock as a clapperboard. This partly inspired the design of the Mark IV, although in the end I don't really recommend using it in this way, as the mechanical shock probably isn't good for it. It did mean that I chose a very secure coin-cell holder though. Clapperboard at your own risk!</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/mk4-folded-rear.jpg" width="1200" height="611" loading="lazy" alt="Rear view of folded clock"></p><h3 id="architecture">Architecture</h3><p>
The previous clocks used off-the-shelf display drivers, which aren't an option as they have a fixed refresh rate of about 330Hz. Ideally we would drive every segment directly from a GPIO pin of a microcontroller, but with 19 digits (of 7 segments plus DP) that's 152 segments to control. We could use shift registers, it's certainly <i>possible</i> to latch them with high accuracy, but this was unappealing both because of the increased BOM cost (potentially one shift register per digit) and my aversion to high-speed signals in the design. A lot of clock customers are ham radio enthusiasts and care a lot about electrical interference. Multi-megahertz signals aren't necessarily bad, and certainly low-frequency signals are just as capable of causing interference if slew rates are not controlled, but I like to be able to tell people that there are no megahertz signals on board.</p><p>

After a very long development period, I came up with the following system design, which uses two processors and six buffer chips.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/architecture.png" width="1681" height="719" loading="lazy" alt="Architecture diagram for Precision Clock Mk IV"></p><p>

The display is split into four separate matrices, each arranged as 5 by 8. This lets us refresh the whole display in just five steps. It also means each matrix can be handled by a single 16-bit GPIO port. The idea was to set up the processors' DMA to drive the matrices in circular mode, so once configured, the display hardware has zero overhead. It means we can just place whatever data we want into an array, and it will get displayed for us. We can change the data in the array directly at any time. It even continues to work if we breakpoint the processor!</p><p>

I spoke a bit more about the development of the display on <a href="https://mitxela.com/projects/automatic_led">this page about an automatic LED</a>.</p><p>

Display brightness is a huge problem, as it was solved so beautifully in the previous clock. With our DMA setup, we could also change brightness by PWM, just by making the array bigger and inserting dead time. But I didn't want to do that. It's essential that we maximise the duty cycle, or visibility under high-speed camera will suffer.</p><p>

The buffer chips in the design solve the problem. The LEDs are <i>voltage controlled</i>. The main microcontroller is 3.3V logic. The matrix signals from the microcontroller are passed through the buffer chips, which are capable of operating at down to 1.8V, and we vary the voltage to the buffer chips to set the level of the matrix signals. This has a non-linear relationship with brightness, but that's fine, we can apply a non-linear curve in software.</p><p>

In order to supply this varying power, I used an adjustable LDO and use the DAC output of the microcontroller to bend the feedback voltage. The display at full brightness will draw about 600mA, so this is the easiest way to provide that.</p><p>

Only four wires pass across the hinge: power, ground, TX and RX. To save adding another wire, the brightness signal is transmitted by varying the power voltage. I specifically chose an STM32 chip that supported both 1.8V and 3.3V operation. In this way, we dim the voltage to the whole date side of the display, and it all works perfectly.</p><p>

There's more cleverness to the hinge signals, as they need to show the display data at an exact moment in time. The conventional way of doing this is to provide a latch signal. Instead, I transmit UART data of what to display, and then a specific byte (a line ending) signals the receiving processor to disable its UART, and wait for the next falling edge on the wire (UART is idle high). At the instant we want to update the display, we send a single byte of 0xFE. The start bit of this byte triggers the update with sub-microsecond accuracy.</p><p>

So we have power, bidirectional data, analog brightness control, and a high precision latch signal, all over four wires with the highest speed signal just 115200 baud.</p><h3 id="interface">Interface</h3><p>
Something I really enjoyed about the previous versions is that there was no interface. Nothing to configure, just power it on and it finds the time automatically. I definitely didn't want to compromise on that, in fact the auto-timezone means it's even better than before, but multiple display modes means we do need to have something.</p><p>

Given that the update system mandates a USB port with mass storage device, we can easily add a config.txt file with as many parameters as we need. That's great for occasional tweaks to the clock behaviour, but really we want more than that. Eventually I settled on the following:</p><ul>
<li>There are a large number of clock display modes available, but most of them are turned off in the config file
</li><li>Buttons on the clock just cycle through the enabled modes.
</li></ul><p>

In addition, later on I added a USB serial device, which can be used to send commands directly to the clock. I also implemented a hook, so that when the config.txt file is saved, the config is reloaded.</p><h3 id="the-double-buffered-display">The double-buffered display</h3><p>
For the previous clock, which has its origins in <a href="https://mitxela.com/projects/precision_clock_mk_ii">the Mark II</a>, I didn't really think about what I was doing. We get a PPS interrupt at the start of the second, and in the interrupt handling routine, I calculate what the display should show next.</p><p>

This started off fairly reasonable, but it's a tricky situation. The serial data from the GPS module tells us what the time was at the preceding pulse. So when the next pulse comes in, we need to add one second to the time data we have. As the data had arrived in ASCII, and needed to be sent out in BCD, I ended up implementing the entire clock logic in BCD, in assembly. At one second to midnight on the 28th of February, there's a fair bit of calculation to do, and we need to do all of it in that few microseconds of the interrupt routine.</p><p>

It gets worse as we added daylight saving time, and different timezones, and daylight saving time in different timezones. If you want to get a feel for how mad the final thing ended up, take a look at the <a href="https://github.com/mitxela/PrecisionClockMkII/blob/master/GPSClock.asm">source code for the mark III clock</a>. All in assembly, all in BCD, all within the interrupt routine.</p><p>

One of the "improvements" to the Mark IV clock is that the code is written in C, to make modification more accessible. I also wanted to manage the time sensibly. This means, when serial data arrives, we decode it into a unix timestamp. In preparation of the next pulse, we add one to it, and then we decode it into a string of local time using the standard C functions. Even with our faster processor, this does mean the whole thing takes a while, long enough to cause display jitter if we aren't careful.</p><p>

The correct way to implement this is a double-buffer. If we imagine each second is deconstructed into a timeline, at about 0.5s we have received enough data from the GPS module to know what the time was at the last pulse. At 0.9s, we can start preparing the next display, which means we leisurely work out local time using the built-in C functions, and we put that data in a buffer (and send the date across the hinge). When the next pulse happens, all we need to do in the interrupt is swap the buffers (and send the latch signal to the date side). Even the most complex date calculations, like the dreaded ISO-week display, are now trivial to display with sub-microsecond jitter.</p><p>

There is a complication to consider, with multiple display modes and a button to cycle through them. We might need to spontaneously change mode at any time. My solution is to check when the button press comes in whether we've already prepared the buffer, and if so, delay the mode switch until the next pulse. The effect is almost invisible, just that if you press a button when the display reads 0.9, there's a 100ms delay before it activates. Otherwise, the button presses are instant.</p><h3 id="gps-discipline">GPS discipline</h3><p>
One sore point, which has always been a pain, is that PPS is not guaranteed to happen each second. If the GPS fix is lost, the PPS signal stops. That means the "pulse" to swap the buffers could either be PPS, or our own timer rolling over. It gets especially confusing if we roll over, thinking that PPS is gone, but then moments later it arrives as we'd just happened to be running fast.</p><p>

This was a big deal on the previous clocks, and the solution was to discipline the internal oscillator to the PPS signal. If PPS happened when our display was at 0.99, we make our oscillator a little faster. If it happened at 0.00, we slow it down and make sure not to accidentally roll over twice. There's also a chance of PPS appearing at some random offset, if we've lost it for a long time and it's suddenly come back, so that needs some extra care.</p><p>

What I realised with the Mark IV is that disciplining our primary oscillator is completely pointless.</p><p>

I want the clock to keep good time when it's lost signal, ideally within 1ppm, so that it takes 1000 seconds to drift 1 millisecond. To that end I fitted a reasonable quality temperature-compensated oscillator. There is no upper limit to how good an oscillator we could fit, but I chose something that seemed like a good compromise between cost and performance here. It's specifically 10MHz so that if the user feels like it, they can provide a better clock source. But even the one I fitted is vastly better than a normal AT-cut crystal (which themselves are better than the 32kHz quartz crystal you'd find in most clocks).</p><p>

The TCXO is not driving a timer or interrupt, it's used directly as the external oscillator for the processor. The sinewave output of the part needed to be coupled to the oscillator-in pin via a small capacitor. I prototyped this by wiring it up to the L476-nucleo dev board. The LGA footprint needs to be mounted upside down and soldered with very fine wires.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/tcxo1.jpg" width="1200" height="802" loading="lazy" alt="TCXO reverse-mounted with fine wire onto the STM32 dev board"></p><p>

In the background is another model of TCXO which was also prototyped and rejected for some reason.</p><p>

With the TCXO, I realised there is no point in disciplining it. A few PPM is such a tiny difference over one second, well beyond the capability of our display, that it's not worth the trouble. In the context of this clock, our primary time source is GPS, which gives us the long term stability. If we lose it, I'd rather we just explicitly show the degraded tolerance with the digits disappearing, than try to eek out a little bit more stability. Again, if the user has another time source available, such as a commercially made GPS-disciplined 10MHz oscillator, that's easy to connect up.</p><p>

The main reason most clocks use tuning-fork style 32.768kHz oscillators is that they are substantially lower power than AT-cut crystals. One of the biggest complaints with the previous clock was the wait after turning it on: it can take a while to get a GPS fix. So this time, we fitted a coin cell battery so that immediately after turning it on the approximate time can be shown. The TXCO would drain power far too quickly, so we also added a (high quality) tuning fork oscillator.</p><p>

You can get temperature compensated tuning fork crystals, but they're very expensive for what they are. The best type use a capacitor network to switch the load capacitance on the crystal as the temperature varies. Some of them can hold the time to within a couple of seconds a year. But given what we want it for, to keep approximate time while the clock is powered off, before reverting as quick as possible to the much better GPS time source, it didn't seem worth the added cost.</p><p>

Additionally, I wasn't able to find a temperature compensated RTC module that allowed for subsecond precision. In contrast the onboard RTC of the STM32 does allow us to set the time to fractions of a second.</p><p>

This time, it genuinely is worth disciplining the oscillator. We have PPS, so I set up a counter for the RTC crystal which monitors it over a few minutes and calculates the error. We're never going to get this perfect, as it will obviously drift with temperature, but we can get it much better than it would have been out of the box.</p><p>

When the clock is first powered on, it takes note of how long it's been off. If it's less than a day, it sets the precision to 100ms. Otherwise, it turns off the fractional display entirely. It never drops the precision worse than one second, as at that point it's not particularly helpful, but if the clock shows no fractional second, it's clear that it's not synchronised in a long time.</p><p>

The coin cell also gives the GPS module the ability to do a warm start, and it provides the processor with a small amount of battery-backed ram. This is used to hold the current timezone offset, and the daylight saving rules for the next decade, calculated from the database I'll explain in a moment.</p><p>

The effect is that if you turn off the clock, and a daylight saving change happens while it's turned off, when you next switch it on it will show the correct time (to within the tolerance). In other words, the RTC "understands" daylight saving rules, even though it's actually just storing UTC.</p><p>

(The one exception is if you turn off the clock, then carry it to a different timezone, and turn it on. Then it will need to get a full GPS fix before correcting itself, which I think is a fair requirement!)</p><h3 id="oscillator-oddness">Oscillator Oddness</h3><p>
Possibly the most interesting problem I encountered related to a subtle mistake on the PCB.</p><p>

The PLL arrangements on STM32 are notoriously complex, and they provide an interactive flow diagram to help you set them up. I wanted the primary oscillator to be 10MHz, to maximise choices for people. But there's no specific PLL/prescaler arrangement that can get from 10MHz to 48MHz, needed to run the USB hardware. However, there is a way to jump from the low frequency 32.768kHz oscillator up to 48MHz. Perfect, we have it fitted, so let's use it, I thought.</p><p>

The USB system worked fine, and the RTC worked fine. The DMA driven matrix displays worked fine. I had deliberately made the display matrix frequency adjustable: from 1000Hz to 100,000Hz, in case a specific frequency is needed, or some frequency caused interference problems, either with filming equipment or electrically. During testing, I used the USB port to set the frequency and it worked fine, except when I set the frequency to 30kHz.</p><p>

At this point, the USB disconnected!</p><p>

The astonishing problem was that one of the display signals was routed too close to the 32.768kHz quartz crystal, and when the display frequency was similar, it caused the crystal frequency to bend. Since the USB clock was derived from this crystal, it disconnected.</p><p>

The fix, once I'd figured it out, was dead simple, just keep those display signals well away from the sensitive crystal. But if I hadn't fed the USB clock from here, I may never have noticed the problem. This little bodge wire fixed the prototype, but on subsequent PCBs I simply used a different pin for that signal.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/bodge1.jpg" width="1200" height="800" loading="lazy" alt="Magnet wire used to re-route a signal around the extended crystal tracks"></p><p>

The advice for routing tracks to crystals is to keep them as short as possible, and in the design above you can see how embarrassingly far away I'd located it (just out of sight to the left). The processor has an amplifier that boosts the tiny signal from the crystal. In this specific arrangement, because of the physical size of the processor and the position of the through-hole pins for the digits, placing it directly adjacent wasn't an option. I was able to shuffle things around and get it a tiny bit closer on the next revision, but the main repair was just to not route anything else near it.</p><h3 id="interrupt-vector-hot-swap">Interrupt vector hot-swap</h3><p>
The interrupt vectors are like a list of function pointers, pointing to functions that will run when the interrupts happen. The "Nested Vectored Interrupt Controller" (NVIC) of ARM chips is a bit of a step up from the simple AVR interrupt system, with the possibility of interrupting your interrupts, but fundamentally it's the same idea. We want our PPS (external interrupt) function to run with the lowest possible latency, and the SysTick interrupt, configured for 1kHz, will update the milliseconds of the display. Although this is double-buffered, the actual task to be performed is going to depend on which mode the clock is in, and what the current precision should be. With count-up, count-down, "other", and three levels of precision, we're potentially facing a stack of conditional branches within the interrupt, and branches on STM32 mean stall cycles.</p><p>

A much more elegant way to do it is to move the interrupt vectors to RAM, and dynamically swap the function pointers depending on mode and precision level. This is a common and expected technique, but doing it in C in a way that's not horrifically ugly takes a fair bit of thought. Moving the vectors is as simple as copying them to the new location and writing to the relevant register, like this:</p><pre>memcpy((void*)0x20000000, (void const*)0x08000000, 0x188);
SCB-&gt;VTOR = 0x20000000;
</pre><p>

and then re-assigning the SysTick interrupt could, I suppose, be done like this:</p><pre>#define SetSysTick(x) *((uint32_t *)0x2000003C) = (uint32_t)x
</pre><p>

Preprocessor macros can always be used to make it explicit what we're doing, but in my attempt to keep the code somewhat sane, I tried to do this "properly". I first altered the linker script to declare the first 392 bytes of RAM as a separate section:</p><pre>MEMORY
{
  VECT    (xrw)    : ORIGIN = 0x20000000,   LENGTH = 392
  RAM     (xrw)    : ORIGIN = 0x20000188,   LENGTH = 96K - 392
...
</pre><p>

then within the sections, we declare a couple of symbols, which we'll later reference:</p><pre>  .isr_vector :
  {
    . = ALIGN(4);
    __VECTORS_FLASH = .;
    KEEP(*(.isr_vector)) /* Startup code */
    . = ALIGN(4);
  } &gt;FLASH

  .isr_vectors_ram :
  {
    __VECTORS_RAM = .;
  } &gt;VECT
</pre><p>

Now within our C code, we can declare these as external uint32 arrays. </p><pre>extern uint32_t __VECTORS_FLASH[];
extern uint32_t __VECTORS_RAM[];
</pre><p>

Our startup code now actually makes sense:</p><pre>memcpy(__VECTORS_RAM, __VECTORS_FLASH, 0x188);
SCB-&gt;VTOR = (uint32_t)&amp;__VECTORS_RAM;
</pre><p>

and reassigning SysTick now almost doesn't even need the macro:</p><pre>#define SetSysTick(x) __VECTORS_RAM[15] = (uint32_t)x
</pre><p>

The arcane syntax of linker scripts is something I've always struggled with, so coming up with something that's <i>almost</i> elegant still feels like a win. It possibly wasn't necessary, but forcing ourselves to figure this out is the only way to learn. And the linker scripts will need plenty of further wrangling when it comes to the bootloader...</p><h3 id="the-chain-loading-bootloader">The chain-loading bootloader</h3><p>
One of the most important things with this clock is that updating the firmware is easy. I have made a big deal about this in the past, of how shameful it is to ask your users to install a driver. I wanted to do it right.</p><p>

I designed this clock before the Raspberry Pi Pico came out. The RP2040 wouldn't have been sufficient to run the clock as it has no hardware FPU (the successor, the RP2350 would be fine). But these chips allow updates through a mechanism known as UF2. If I had known about it, I would have used it. Instead I created something very similar, and functionally the same from the user perspective.</p><p>

In addition to updating the firmware, we want to be able to update the timezone database and the shapefiles for the world maps. This adds up to about 12MB of data, so I kitted out the clock with a 16MB flash memory chip. I considered fitting an SD card, which at least has a fool-proof method of updating, but it's increasingly difficult to find small size SD cards and dealing with exFAT adds some overhead I'm not interested in.</p><p>

So the design was to have the USB port present a small mass storage device. We can just copy and paste the necessary files onto the drive, and no drivers are needed. My dev board at this point looked like this:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/nucleo1.jpg" width="1200" height="800" loading="lazy" alt="QSPI Flash and USB port wired up to nucleo dev board"></p><p>

The foremost consideration is that we never end up in an unrecoverable state. An ST-link SWD header is the ultimate backup but we can't expect the user to have the programming cable. STM32 chips come with a system bootloader, a piece of ROM that runs a few basic protocols, and one of them is DFU over USB. We can trigger this bootloader using the dedicated pin for it. But USB DFU requires a driver on the host side; I'm not interested in using it. I took the boot pin out to a small pad, as a secondary line of defence, if <i>absolutely</i> necessary the user could short this pad, trigger the DFU update using a driver.</p><p>

For the main bootloader, we present ourselves as a mass storage device. The first thing we do on power-up is verify the currently loaded firmware by checking the CRC. If it's wrong (either because of a failed update, or it's blank) then we definitely don't want to launch the main application. Next we check the flash memory, and if it finds a firmware image on there, and the CRC is correct, and it's a different firmware to what's currently loaded, then we can perform an update. The USB stack is not triggered until it has decided what to do. Only in the case of no valid firmware being available does the <i>bootloader's</i> mass storage device enumerate, where it then hangs waiting for you to copy a valid image across.</p><p>

Otherwise, it launches the main application. This has its own USB stack which I eventually configured as a composite device, both a mass storage device and a serial port for controlling the clock.</p><p>

The first thing the application does is query the second microcontroller over the hinge connection. That microcontroller responds with its own firmware CRC. If there's a new firmware image on the flash memory, we can now chain-load the update across the hinge. Instead of using a custom bootloader on the second microcontroller, I made use of the more reliable system bootloader. I specifically used the same UART pins for the hinge connection as the system loader uses, and on the application side of the main processor I re-implemented the entirety of the STM32 bootloader protocol. The UART sends a command to trigger the system loader, the update is performed (while showing a cool progress animation) and then the secondary microcontroller launches its application.</p><p>

One concern I always have with this stuff is whether a sudden power cut could cause problems. We have a full backup of the firmware image on the flash memory, so it should be possible to recover from a power cut gracefully. The one situation I had to make a call on was whether to add a dedicated signal for the second microcontroller's BOOT0 pin. If we cut the power at exactly the right moment, it may end up with a partial firmware image that runs but doesn't respond to the command to re-enter the system bootloader. If we had hardware control of the power and the BOOT0 pin from the main microcontroller, we could recover, but that means adding more wires to the hinge, wires which are redundant except for this exact scenario. I decided to risk it. If ever a power cut does happen at that exact moment, the user can short the BOOT pad manually on the back of the clock to recover.</p><p>

I did consider having the secondary microcontroller <i>always</i> boot into the system loader, but there are a few annoyances with doing so, and powering up the clock would be much more clunky.</p><p>

I probably spent more time on the chain-loading bootloader than on any other aspect of the clock. The end result is a work of art, even if it's one that nobody else will appreciate. I spent a long time monitoring the SCSI commands sent by the host OS during different operations and on different systems. Windows, linux and OSX all have slightly different behaviour when, for instance, you "eject" a mass storage device (which is different to just unmounting it). Unlike the Pico, which immediately disconnects and performs the update when the file is copied, I wait before performing the update, as there may be several files you want to copy across, two firmware images, the database and the map file. Once copied, we could just power-cycle the clock, but it's more swish to "eject" the device, which then performs the updates with its cool animations, and reconnects when it's ready.</p><p>

The external flash memory has pages of 4kB. If you want to write to it, you need to erase an entire page, and write it back. I chose FAT12 or FAT16 as the filesystem, as they're widely supported and quite simple. FAT12 may seem like an odd choice but it's so old that it makes efficient use of the storage available, if that's a concern. The USB mass storage class is really a wrapper for SCSI commands, which was a bit of a surprise to me. While we have FATFS running on the clock, for interpreting the files, that's totally independent to the mass storage USB device, which essentially just provides "read" and "write" access to the host.</p><p>

This presents a bit of a problem as to do this properly we would need to accept arbitrary writes, store them in RAM, then erase a whole page and write it again. This feels inherently dumb as the flash memory has a limited number of write cycles, so if we copied 4kB in small chunks, it would erase and re-write the page repeatedly. I suppose we should have made a proper cache system, with several pages in RAM that then get written asynchronously, but the write process is slow. My simple solution to all this was to set the block size of the file system to 4K. This is a bit of a gamble as it's extremely unusual to have a block size other than 512, but I've tested it with Windows, Linux and OSX without problems. By setting the block size to 4K, we essentially force the changes to the filesystem to be a page at a time.</p><p>

I don't expect the memory to be written to that often, it's only every few months that the database changes. There are wear-levelling filesystems available but they're quite a bit more complex. I chose a QSPI flash memory chip that's rated for at least 100,000 writes, but my ultimate cop-out is that it's in a SOIC-8 footprint, so it's one of the easiest surface mount parts to replace.</p><h3 id="auto-timezone">Auto timezone</h3><p>
Possibly the most requested feature, automatically setting the timezone based on the GPS coordinates was essential. However, most people don't realise how complex this process is, at least within the restrictions I'd given myself: no internet or cellular modem, nothing beyond the GPS data, and a USB port for occasional updates.</p><p>

If we had an internet connection, sure, we could just query an online service. But I don't want my clock to require wifi.</p><p>

If we had a cellular modem, we could get the time from the cell towers, which is broadcast through a protocol called NITZ. This is how phones auto-update when you enter a different timezone, but it's somewhat unreliable depending on the carrier, and overall worse than using GPS. And I really don't want my clock to have a sim card.</p><p>

I'd also like, just for the sake of it, for the clock to work anywhere in the world, even at sea.</p><p>

This means we need to keep a copy of the map data for the country outlines for the whole world. It also means we need a full copy of the timezone database, and have the processing power onboard to make use of all this.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/timezone-boundary-builder-2022f.jpg" width="1200" height="601" loading="lazy" alt="Timezone map for 2022f from Timezone Boundary Builder"></p><p>

We can break it down into two parts, first of all is determining the IANA timezone name from GPS coordinates. This <i>is</i> a solved problem, at least on desktop computers. A project called <a href="https://github.com/evansiroky/timezone-boundary-builder">Timezone Boundary Builder</a> has collated this data, extracted from OpenStreetMap. There are moderately frequent updates to it, which is fine.</p><p>

Another project, <a href="https://github.com/BertoldVdb/ZoneDetect">ZoneDetect</a>, is an example of how to use this data, written in C. In theory, all we need to do is port ZoneDetect to a microcontroller.</p><p>

It ended up being a fair bit of work, because ZoneDetect was written for desktop computers with infinite RAM. The shapefile is about 11MB, which doesn't sound like much, but the first thing ZoneDetect does is <code>mmap()</code> the whole thing into memory. The STM32L476 running the show only has 128kB of RAM.</p><p>

The shapefile on the external flash memory is accessed repeatedly as it searches through the file. We're using FATFS, by our hero ELM ChaN, to read the filesystem. The QSPI reads have to be carefully locked in case of any USB activity (in fact, if any USB read or writes to the flash memory are detected, we abort any ongoing zone detection for the next few seconds). When I finally got the program running on the chip, it was a great relief, but it took <b>several seconds</b> to figure out my coordinates mapped to Europe/London. This is on the 80MHz ARM Cortex M4 with FPU.</p><p>

It's possible we could reconfigure the shapefile to minimise the number of random accesses. I ended up plotting some graphs of which addresses are accessed, and some clear patterns emerged. The two main activities were searching forwards, and searching backwards, so I split these into separate calls. Each one loads data from the file, and also caches the page ahead (or behind). Experimentally, caching about half a kilobyte ahead seemed to give the best performance. I got the behaviour down to about 500ms in most cases, which I considered good enough.</p><p>

Once we've managed that, we need to load the database rules for that location. I'm repeating myself here, but I'll say it again, there is no central authority on timezones. Every government makes up whatever it likes. "The Timezone Database", the Olsen tzdb, is just a guide, an approximation assembled as a best guess. It's an incredible project, but it comes with disclaimers about accuracy, and it's immensely complex. There are over 500 timezone entries, and the database is updated multiple times a year.</p><p>

The database is surprisingly readable, and is intended primarily for human consumption. You tend to have several paragraphs of prose, followed by one line of data. I heartily recommend giving it a read, it's fascinating. For a while I was subscribed to the timezone database mailing list. A lot of people have very strong opinions about timezones, particularly how the historical data is presented (which is very important in some situations, thankfully not for us).</p><p>

If we take a look at the <code>europe</code> file, and search for <code>Europe/London</code>, after pages of comments we get to the actual data:</p><pre># Zone	NAME		STDOFF	RULES	FORMAT	[UNTIL]
Zone	Europe/London	-0:01:15 -	LMT	1847 Dec  1
			 0:00	GB-Eire	%s	1968 Oct 27
			 1:00	-	BST	1971 Oct 31  2:00u
			 0:00	GB-Eire	%s	1996
			 0:00	EU	GMT/BST
</pre><p>

Only the last line matters, which says since 1996 we've had the standard offset of 0:00 from UTC, and follow the daylight saving rules known as EU.</p><p>

Searching for Rules EU, we find the following:</p><pre># Rule	NAME	FROM	TO	-	IN	ON	AT	SAVE	LETTER/S
Rule	EU	1977	1980	-	Apr	Sun&gt;=1	 1:00u	1:00	S
Rule	EU	1977	only	-	Sep	lastSun	 1:00u	0	-
Rule	EU	1978	only	-	Oct	 1	 1:00u	0	-
Rule	EU	1979	1995	-	Sep	lastSun	 1:00u	0	-
Rule	EU	1981	max	-	Mar	lastSun	 1:00u	1:00	S
Rule	EU	1996	max	-	Oct	lastSun	 1:00u	0	-
</pre><p>

To "max" means the last two lines are still valid, and that we start DST from 1AM UTC on the last Sunday of March, and end DST at 1AM UTC on the last Sunday of October. This is one of the simpler rules in the database, after all, Europe switches DST all at the same time, whereas in the US the switchover is at 2AM local time.</p><p>

Attempting to parse the database on the fly, that is, asking if today is the last Sunday of the month every time the seconds interrupt fires, is not viable. Even for desktop computers the database is preprocessed into a more easily parsed format. The tz database also includes some reference implementations of how to use it, but once again these are aimed at desktop computers and servers.</p><p>

For the previous clock, I managed to solve this problem by pre-calculating all of the dates where changes occur, for the next hundred years. A simple javascript utility spat them out and a compile-time definition included the relevant lines. In that case, I just checked whether the month was March/October and the day was the recorded number. It got more complex for some of the zones, like New Zealand, where the switch happens on the first Sunday of April, because that can occasionally land on the first of the month. In that scenario, the UTC month will be different to the local month, because they're 12 hours ahead.</p><p>

But fundamentally, I became convinced that precalculating the switchovers was the most sensible option. The tz db includes a utility called <code>zdump</code>. If passed the right arguments, we can get it to tell us the switchover times for any zone, for any range of years.</p><pre>$ zdump -V Europe/London -c 2020,2030

Europe/London  Sun Mar 29 00:59:59 2020 UT = Sun Mar 29 00:59:59 2020 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 29 01:00:00 2020 UT = Sun Mar 29 02:00:00 2020 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 25 00:59:59 2020 UT = Sun Oct 25 01:59:59 2020 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 25 01:00:00 2020 UT = Sun Oct 25 01:00:00 2020 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 28 00:59:59 2021 UT = Sun Mar 28 00:59:59 2021 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 28 01:00:00 2021 UT = Sun Mar 28 02:00:00 2021 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 31 00:59:59 2021 UT = Sun Oct 31 01:59:59 2021 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 31 01:00:00 2021 UT = Sun Oct 31 01:00:00 2021 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 27 00:59:59 2022 UT = Sun Mar 27 00:59:59 2022 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 27 01:00:00 2022 UT = Sun Mar 27 02:00:00 2022 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 30 00:59:59 2022 UT = Sun Oct 30 01:59:59 2022 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 30 01:00:00 2022 UT = Sun Oct 30 01:00:00 2022 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 26 00:59:59 2023 UT = Sun Mar 26 00:59:59 2023 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 26 01:00:00 2023 UT = Sun Mar 26 02:00:00 2023 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 29 00:59:59 2023 UT = Sun Oct 29 01:59:59 2023 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 29 01:00:00 2023 UT = Sun Oct 29 01:00:00 2023 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 31 00:59:59 2024 UT = Sun Mar 31 00:59:59 2024 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 31 01:00:00 2024 UT = Sun Mar 31 02:00:00 2024 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 27 00:59:59 2024 UT = Sun Oct 27 01:59:59 2024 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 27 01:00:00 2024 UT = Sun Oct 27 01:00:00 2024 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 30 00:59:59 2025 UT = Sun Mar 30 00:59:59 2025 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 30 01:00:00 2025 UT = Sun Mar 30 02:00:00 2025 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 26 00:59:59 2025 UT = Sun Oct 26 01:59:59 2025 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 26 01:00:00 2025 UT = Sun Oct 26 01:00:00 2025 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 29 00:59:59 2026 UT = Sun Mar 29 00:59:59 2026 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 29 01:00:00 2026 UT = Sun Mar 29 02:00:00 2026 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 25 00:59:59 2026 UT = Sun Oct 25 01:59:59 2026 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 25 01:00:00 2026 UT = Sun Oct 25 01:00:00 2026 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 28 00:59:59 2027 UT = Sun Mar 28 00:59:59 2027 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 28 01:00:00 2027 UT = Sun Mar 28 02:00:00 2027 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 31 00:59:59 2027 UT = Sun Oct 31 01:59:59 2027 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 31 01:00:00 2027 UT = Sun Oct 31 01:00:00 2027 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 26 00:59:59 2028 UT = Sun Mar 26 00:59:59 2028 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 26 01:00:00 2028 UT = Sun Mar 26 02:00:00 2028 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 29 00:59:59 2028 UT = Sun Oct 29 01:59:59 2028 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 29 01:00:00 2028 UT = Sun Oct 29 01:00:00 2028 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 25 00:59:59 2029 UT = Sun Mar 25 00:59:59 2029 GMT isdst=0 gmtoff=0
Europe/London  Sun Mar 25 01:00:00 2029 UT = Sun Mar 25 02:00:00 2029 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 28 00:59:59 2029 UT = Sun Oct 28 01:59:59 2029 BST isdst=1 gmtoff=3600
Europe/London  Sun Oct 28 01:00:00 2029 UT = Sun Oct 28 01:00:00 2029 GMT isdst=0 gmtoff=0
</pre><p>

It extrapolates out into the future as far as you'd care to go, and obviously it's questionable how much we can say about daylight saving rules hundreds of years in the future, but this is an output I can work with. A python script was used to run through every zone, and inhale the DST rules until the year 2106, interpreting each transition time as a Unix timestamp and an offset (the archaic gmtoff in the lines above).</p><p>

I chose 2106 as a cutoff because that's the 32bit rollover of the Unix timestamp. Everyone's heard of the 2038 problem, but that's an overflow of a signed 32 bit number. Unsigned, we can delay the problem until 2106! But seriously, the GPS data only has a 2-digit year, so in the event the clock is still going by the year 2100, we'll need a firmware update anyway.</p><p>

The compressed data, along with a lookup mechanism that made some amount of sense, ended up at about 175kB. With that, we finally reached the point where the clock can figure out the exact local time, from nothing but GPS.</p><p>

I've glossed over a lot of stuff in this summary, but implementing all of this took forever, maybe several months of solid work. Phew!</p><h3 id="interrupt-priorities">Interrupt priorities</h3><p>
We heard you like interrupts, so we stuck pre-emption priorities in your NVIC so you can interrupt while you interrupt, or something.</p><p>

The possibility of using an RTOS to run the clock was dismissed fairly quickly, because I didn't believe it would be sufficiently real-time. The prime directive is to make the clock display infallible and the only way to achieve that is by going bare metal. Given the number of tasks that need completing at specific moments in the timeline I ended up writing something that approaches the complexity of an RTOS anyway, but it's all handled through the interrupt controller.</p><p>

This is something of a minefield because as soon as we start fiddling with the priorities the HAL functions can have weird problems and edge cases. Most functions expect to be pre-empted by SysTick, and rely on it for timeouts when problems occur. If you call a HAL function from within SysTick, or something higher priority than it, then it will still work, right up until it doesn't work, never times out and effectively hangs the processor.</p><p>

At the very top, our two most important tasks are servicing PPS, and the SysTick interrupt that updates the milliseconds of the display. Early versions of the clock would have the display flicker slightly when files were copied to the USB drive, which I considered unacceptable. With the lower priority for USB interrupts, transactions take longer but the display remains stable.</p><p>

At the other end, the lowest priority and slowest task is doing our ZoneDetect. Since this can take a good fraction of a second, we stick this in the main loop. It is basically the only thing that happens there, along with a few "delayed" tasks that get requested by certain flags, everything else is done in an interrupt context.</p><p>

A really neat feature of the UART hardware on the L476 is the ability to do a character-match interrupt. We can receive NMEA data from the GPS module into a buffer via DMA, and just get an interrupt when a line-ending is detected. This is super efficient and I wish all microcontrollers had something like this. Incidentally, unlike the previous versions of the clock, we do actually check the checksums of the NMEA strings and discard them if necessary.</p><p>

So we have high, medium and low priorities, plus the base level of the main loop outside of the interrupt context. But beyond that, we ideally need to jump to different levels of priority at certain tasks. When PPS happens, we immediately want to activate the timing-critical code to get the best possible latency regardless of what else is happening. But we also want to do a bunch of other stuff that is not quite so important and may rely on HAL timeouts and so on. For this, we can trigger a software interrupt (in this case, PendSV) that runs at a lower priority, triggered at the end of PPS or seconds rollover.</p><p>

There's a lot of asynchronous stuff that happens and in some ways it's a mess, but it's a mess that works with extremely low latency. The thing to watch out for is that a lot of C code is not "thread safe" meaning that interrupted operations can potentially have data corrupted. A classic example is bitfields in structs, which are accessed through multiple opcodes that depend on register state that is not preserved by the interrupt controller. This feels like a complete failing of the compiler, as it results in code that works <i>most</i> of the time but occasionally doesn't. My only solution is to avoid bitfields entirely.</p><p>

That's a complaint about GCC, but I have many more complaints about STM32CubeIDE. I've ranted in the past about how awful the UI is, the unbelievable failures in its crushed java interface. They recently somehow made the IDE even worse, by forcing you to "log in" to use it, and yet they still haven't fixed the most basic glitches that make it torturous to use, glitches that I complained about five years ago when I started this project and that still exist today.</p><p>

Perhaps the most offensive UI problem is that if you change the build mode from "Debug" to "Release" and then click "run code", it will compile the Release version, and then flash the Debug version. This can be almost impossible to notice unless you're doing something weird enough that the optimisations break it. An example is jumping to the main application from the bootloader. All I want to do is generate a jump instruction, but it's incredibly painful to do that in plain C. With linker script wrangling much like the vector table shifting I described above, I was able to reduce the jump to something like this:</p><pre>  __set_MSP(_app_start[0]);
  ((void (*)(void)) _app_start[1])();
</pre><p>

The first entry in the vector table is stack pointer, the second entry in the vector table is the application entry point.</p><p>

This worked nicely, and I considered the bootloader finished. When I started flashing boards via the ST-link utility, however, they didn't work! Outside of the IDE, which I thought had been running the "release" build, the code hadn't actually been tested with optimisations enabled. A failure at this point in the bootloader is catastrophic. The fix is trivial (just disable optimisations for those lines of code) but I was incredulous that the problem had gone unseen at all.</p><h3 id="display-design">Display design</h3><p>
The big question was how the display was going to work, and I've lost track of the number of different options I considered. I really wanted to reuse the analog current-mirror display brightness idea, and for a while I was yearning for a parallel-input LED driver chip, the kind of thing that used to exist in the 90s but no one makes them now. It would have allowed me to use the reference current to set the brightness, but kept all the options open for how to drive things. The idea of driving the display with discrete transistors was on the table for some time. I considered LED driver shift registers, which are shift registers with a reference current input, which would have worked but have their own downsides. For one, we'd need a lot of them, to drive our ~150 segments, which would make the PCB layout difficult and the BOM cost increase significantly (regular shift registers are very cheap, but the LED driver chips with settable current are pricey). As current sinks, they'd need to be driving common-anode displays, whereas the previous clock used the more widely available common-cathode. Finally, mirroring the current to 10 or 20 shift registers is nontrivial, it would either require a lot of extra transistors or something hacky.</p><p>

I'm not saying the final design is <i>not</i> hacky, but it meets the spec and only requires six reasonably cheap buffer chips. We split the display into four separate matrices, as a compromise between direct drive and a full matrix. We control each matrix directly from microcontroller GPIO, via DMA. The LEDs are voltage-controlled, and we use buffer chips on the output signals as a means to control the brightness. They each function as a kind of analog, continuously variable level shifter, that also boosts the maximum current we can deliver.</p><p>

As an illustration, imagine driving a four-by-four LED matrix this way:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/buffer-matrix.png" width="1200" height="584" loading="lazy" alt="Illustrative schematic of 4x4 matrix using the voltage-controlled buffer chip"></p><p>

The buffer chip needs to sink and source current. We also need some amount of current limiting, although how much is not clear. It provides some protection against burning out if the matrix stalls, and it goes some way to evening out the current between LEDs, which may not have perfectly matched bandgaps. Too large a resistance will limit our maximum brightness, however.</p><p>

There's a whole range of 74 series octal buffer chips that end with 244, where the prefix and the letters in the middle tell us about their performance and characteristics. Of concern are the supply voltage range, the output current capability, the switching speed, and I suppose the cost. Happily, they mostly have the same footprints, so we can defer the decision until later.</p><p>

Our matrix needs to refresh at around 100kHz, which means a switching speed of 500kHz, if there are five digits per matrix. We then set the display brightness by dipping the power supply to the buffer chip. I wasn't sure this was going to work until I prototyped it, which I began by tediously wiring up the SSOP20 buffer chips with magnet wire.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/buffers4.jpg" width="1200" height="802" loading="lazy" alt="Closeup of soldering on buffer chip without breakout board"></p><p>

This quickly grew tiresome so I ordered some breakout boards. I took one of the old clock PCBs, cut the lines to the cathodes, and wired our new display circuit idea up to it.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/buffers1.jpg" width="1200" height="802" loading="lazy" alt="Buffer chips tediously wired up to F072 dev board"></p><p>

The blue devboard is for an STM32F072, which I originally planned to use for the date side of the clock.</p><p>

The results were promising, so to wire up the time-side, I tried to make things a little tidier.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/buffers2.jpg" width="1200" height="802" loading="lazy" alt="Buffer chips wired in a more tidy fashion to the display"></p><p>

Visible in the picture above is the knife-cut I made to the existing matrix wiring on the PCB, to split it into two matrices. The final four digits make up the last matrix, with the last digit super-glued in place and the matrix extended manually. Thus the time-side prototype came together, and on this I developed the bulk of the firmware.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/buffers3.jpg" width="1200" height="727" loading="lazy" alt="Display wired up, via buffer chips, to L476 dev board"></p><p>

I don't remember all of the part numbers used, but the final choice of buffer chip was SN74LVTH244A, and the limiting resistors were 10ohm. Technically this buffer chip has a minimum voltage of 2.7V, but it functions correctly down to 1.8V, and has good current and switching characteristics otherwise. A backup plan, had I not been able to reach the desired output current, was to stack the buffer chips. This is an old technique that probably doesn't translate too well to surface mount, but you just solder another chip directly on top, and it doubles the output current.</p><p>

In the end I probably could have eliminated the current limiting resistors entirely, as the DMA matrices continue to cycle even when the processor hangs, but it's best to have them on the PCB, and if not needed we could replace them with 0-ohm resistors.</p><h3 id="brightness-details">Brightness details</h3><p>
Until the first PCB, I was dipping the buffer chip voltage manually from the bench power supply. Perhaps there's a suitable way to wire up a light sensor directly to a variable power supply, but given the response of the LEDs, I fully expected to have to control this digitally. The light sensor goes into the main processor's ADC, a non-linear curve is applied, and a DAC output then controls the brightness.</p><p>

One could simply send the DAC value through a beefy op-amp with enough current output, but that'd be expensive and we don't exactly need the bandwidth. A simpler option is to take an adjustable voltage regulator, and control it by bending the feedback voltage. These adjustable regulators simply expose the feedback pin, with the intention that you feed it from a potential divider on the output. There'll be some internal voltage reference, maybe 1.2V, and the regulator will do what it can so that the feedback pin meets that voltage. The datasheet for the TPS784 adjustable LDO has this diagram:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/tps784.png" width="1046" height="462" loading="lazy" alt="Screenshot of figure 8-1 from TPS784 datasheet"></p><p>

What we want to do is add a third resistor into that network, driven from the DAC, so that the full range of DAC output (0.0 to 3.3V) steers the regulator to between 1.8V and 3.6V. It's essential we don't let it go too high or it might burn the LEDs, and letting it go too low will just waste our resolution. The three resistor values should also ideally be part of the E12, or at the very least E24 series.</p><p>

On paper I figured out the ratios, which involves going back to basics with our Kirchoff's laws, and finally in a spreadsheet figured out what values best matched the E12 series. To minimise the number of unique components, I also wanted to use the same LDO to give us a 3.3V for the rest of the circuit, and again we'd like it to share the same resistors. I came up with this, which uses 180K, 270K and 470K (all in E12).</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/tps784-2.png" width="890" height="670" loading="lazy" alt="Use of two adjustable LDOs to give one variable and one fixed voltage"></p><p>

By my calculations, the fixed arrangement gives us 3.29V, and the variable ranges from 1.79V to 3.69V.</p><p>

Confusingly, although not once you've thought about it a bit, the output voltage is highest when the DAC line goes lowest, and vice versa. This is inconsequential as we're applying a non-linear gain curve anyway.</p><p>

One last point to consider is what the resting voltage will be. When the processor is in reset, the DAC is turned off and we revert to just the potential divider, which with 270K/180K will settle on 3.00V. That's fine, a medium brightness. In future, we could potentially add another resistor, a pull-up or pull-down on the DAC line, which is drowned out in normal use but holds our brightness to a known value when the DAC is turned off.</p><p>

A note: this feedback-bending technique can equally be applied to switched-mode supplies. I'm not the first person to do this. Several times in the past, when I've wanted to add remote control to an off-the-shelf buck module, I've just added a resistor into the feedback network to gain voltage control over it. The reason I went with linear regulators here is that they're electrically quieter, and cheaper.</p><p>

On the processor side, we set up a timer and double-buffered DMA transfers to the DAC. In addition to reading the ADC and non-linear mapping it, we apply a healthy amount of filtering, essentially a lowpass filter with a cutoff of about 2Hz, so it takes about half a second to react to light changes instead of bouncing around instantly when the sensor is covered.</p><h3 id="colon-control">Colon control</h3><p>
Since the Mark II, I've used the blinking colon separators to indicate the presence of a GPS fix. In fact, for the previous clock they also functioned debug data about the oscillator discipline. The new dynamic precision shows us if we've drifted more than a millisecond, but nothing beats the assurance of a signal to indicate PPS within the last second.</p><p>

But not everyone likes the harshness of the blinking colons. It was certain that they should be configurable this time, but better than that, we want them to do a smooth animation, perhaps a fade in and out. They still need to dim with the overall brightness of the display.</p><p>

I made the executive decision that stability at high frame rates is not critical for the colons, and that we can use PWM to dim them. Combining that with the voltage control of the main display, the PWM at max duty should be as bright as the other segments. This is surprisingly easy to achieve, now that everything's voltage controlled. I connect the anode of each colon LED to the variable voltage, and the cathodes, via limiting resistors, to GPIO of the microcontroller, with hardware timers configured for PWM output. Even though they're CMOS, at logic high level no current will flow (and at anything other than max brightness, the LEDs will be reverse biased).</p><p>

As with, well, everything, we set up a DMA stream to clock data to the duty cycle. A two-second 100Hz buffer was used, meaning we can configure any arrangement of colon brightnesses we like for up to two seconds. That might be a simple on/off with each alternating second, or fading in and out, or what I ended up leaving as the default, a staggered fade in/out that suggests a heartbeat. More modes might be added later.</p><p>

Since they're not matrixed and only low-side switching, the colon LEDs are not driven via the buffer chips. It's fine to drive a few LEDs from the GPIO, it's only an issue when the sum total current through the microchip approaches the limit, which it would if we drove everything from it. Similarly, on the date side, I didn't have enough pins on our three buffer chips to drive everything through them, as it's ten digits with decimal places. I chose to drive the decimal places directly from the microcontroller there. As usually only one decimal place is illuminated (if any, on the date side), there really won't be much current here.</p><h3 id="hinge-development">Hinge development</h3><p>
One of the most irritating things about the earlier clock designs was that the PCB was so long. If anyone wanted to buy a bare PCB, the cost of shipping just one was prohibitive. Some shipping rules jump from "small" to "medium" if any one dimension is longer than 50cm (which the clock was, just). It always amused me that I could potentially save on shipping by placing the PCB diagonally in a "small" box.</p><p>

Splitting the circuit into two pieces was something I had in mind from the start, but the idea of an articulated joint, once it had occurred to me, did not go away. Realistically, nobody needs it to dynamically transform, but oh, what a swell feature to have. I think it was the request for a Precision Clock Clapperboard that made it stick. With the plan for automatic timezones, that middle portion of the clock is now dead weight anyway, and if we're millisecond precision, the widths will match perfectly too... it had to happen!</p><p>

I took one of the old clock PCBs, sheared it in half with the tin snips, drilled some holes, filed the edges, and concocted this laser-cut prototype.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/hingedev1.jpg" width="1200" height="800" loading="lazy" alt="Early hinge prototype in plywood"></p><p>

The vague idea was to paint a white line along the edge of the three sections. In the folded position, it would simply be a vertical line, but in the open configuration, it would spell out a capital T. I forgot about that when we switched to Delrin as a material, which is basically impossible to paint, but it has the benefit of being much stronger. Possibly excessively so. With the hinge made of Delrin, I'm pretty sure the PCB is going to snap well before the hinge gives way.</p><p>

The electrical connection, of how many wires we can get across the hinge and what their impedance and current capability is, dictates quite a lot of the system architecture. There are a bunch of wire-to-board connectors available and I've had mixed results with several of them. I eventually settled on the Molex Pico-EZMate system. The hinge prototype was again used to try and find the best placement for it, with my initial plan something like this:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/hingedev2.jpg" width="1200" height="800" loading="lazy" alt="Early hinge prototype board-to-board cable placement"></p><p>

This worked but the cable was free to flop about, so the later design added the acetate cable support, and moved the connectors up appropriately.</p><p>

This same concoction was used to prototype the placement of the hall sensor and magnet, to detect folded status, and then even, much later, to prototype the 3D printed colon holders with their plastic snap-tabs.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/hingedev3.jpg" width="1200" height="800" loading="lazy" alt="Full view of hinge prototype in folded position"></p><p>

I've now confirmed the board-to-board cable to be quite reliable, but the paranoia at the time was that the hinge would fail in some way and make the clock unusable. I really dislike wire-to-board connectors that can't be hand soldered easily. The Pico-EZMate connectors are not too bad, but still tricky for a beginner, so for a foolproof method, I made sure to add big test pads behind each connector on the PCB, so in the worst scenario we can just solder wires across the hinge.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/hingedev4.jpg" width="1200" height="727" loading="lazy" alt="The board-to-board cable across the hinge on the Rev C clock"></p><p>

An unfortunate consequence of using the digits themselves as the end-stop of the hinge, that only became apparent much later, is that different suppliers of digits put the pins in slightly different places. My first prototype fit the digits I had to hand perfectly, since it was made to measure, but much later, as I built clocks with other digits, I had to bend the legs to get the plastic parts flush.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/digits-comparison.jpg" width="1200" height="800" loading="lazy" alt="Comparison of three clocks with nominally the same, but slightly different 7-segment displays"></p><p>

There is at least one producer of digits with a design that fits perfectly, but sellers make no distinction between these subtle varieties so it's very hard to get the right ones.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/digits-comparison2.jpg" width="1200" height="800" loading="lazy" alt="Another comparison of three clocks with slightly different 7-segment displays"></p><h3 id="the-two-layer-challenge">The Two Layer Challenge</h3><p>
In the old days, having more than two layers on a PCB meant a substantial jump up in cost. Nowadays the difference, though still present, is not so significant and the idea of trying to be frugal with the number of layers might seem bizarre. Four layer boards have a multitude of benefits, even ignoring how much easier it becomes to route things. So I fully concede that my behaviour is irrational when I say that multilayer boards are a cop-out.</p><p>

Artificially confining ourselves to two layers is exactly the kind of challenge I am wont to be engulfed by. The trick, if you want the board to work well, is to work on only one layer, and keep almost the entirety of the other layer as ground. It is almost always possible to do this, if you're willing to put in enough thought. I rather find that designing circuit boards is a lot like Tetris, and once I'm in the swing of it I can route things for hours on end. It can become a multi-day trance, with dreams of signal integrity and current loop area.</p><p>

Not only did I want the circuit to fit within the physical bounds, I wanted the circuit to be beautiful. To this end I embarked on a <a href="https://mitxela.com/projects/melting_kicad">multipart journey</a> about <a href="https://mitxela.com/projects/melting_kicad_2">melting KiCad</a>. If you've perused mitxela.com at all in the last five years, you've probably been exposed to closeups and screenshots of the Mk IV clock PCB, which was the instigator of the whole thing.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/meltingthumbnail.jpg" width="1200" height="675" loading="lazy" alt="Thumbnail image from the melt your circuit boards video"></p><p>

The rounded tracks, the cubic bezier teardrops, melting the circuit as if it's dipped in honey – all of this was just for the aesthetics. The irony being that the tracks are too small to really be visible, but at least under a microscope, the layout is gorgeous.</p><p>

The PCB was designed in KiCad 5, so all of the rounded tracks were done by subdivision, but here's a screenshot of the board loaded up in the more aesthetically pleasing default theme of newer KiCad versions:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/kicad-screenshot.png" width="1612" height="844" loading="lazy" alt="Screenshot of KiCad circuit, focused on the main processor"></p><p>

I mentioned the desire to route everything on one layer, which obviously isn't possible when it gets this dense, but a lot of thought was put into the ground plane continuity under and around the main processor. Ideally, the bottom layer would have been just tiny jumps amidst an otherwise unbroken ground plane. If signals really need to cross, they should do so at right angles, which I sadly wasn't able to pull off everywhere, but in these cases we at least think about what the signals are doing. The display signals are reasonably fast switching, the USB diff pair is fast but only used intermittently and reasonably robust anyway, the QSPI flash is fast but the traces are very short and direct.</p><p>

Some of the ground tracks within zones are to prevent me from accidentally moving other tracks and breaking the continuity there. Also visible in the screenshot above are the little pads I designed to act as switches for the reset and boot pins, which may need activating, but not often enough to justify fitting real switches. They can be shorted with a metal object if needed. It's <i>very</i> easy to make these too small without realising it: it looks big in the screenshot but the silkscreen around the pad has a diameter of just 3mm.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/boot-reset-pads.jpg" width="1200" height="800" loading="lazy" alt="Boot and reset PCB switches on date side"></p><p>

For a development board, it's possible to solder tiny tact switches onto these pads to make them easier to use. To trigger the system loader, you need to hold the boot pin while tapping reset, which can be fiddly, but it's not something that normal users of the clock should ever need to do.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/kicad-screenshot3.png" width="1200" height="700" loading="lazy" alt="KiCad screenshot of date side PCB"></p><p>

I have a lot of self doubt when it comes to PCB nuances. A lot of people have very strong opinions about the "right" way to do a layout and I'm not really qualified to argue with them. The only thing we have to back ourselves up is the fact that the circuit boards work. But are they optimal? Do they benefit from the extra effort we're putting in? Would they, in fact, work, regardless?</p><p>

Upon researching, I found some interesting opinions. One senior PCB designer challenged the common knowledge that differential pairs should be routed together. The total path length matters, but as for the individual lines, he claimed, it makes no difference. To demonstrate, he modified a board so that the USB diff pair split, with one line going up and around the top of the board, and the other going right around the bottom. The USB connection still worked. I'm not sure that's a completely fair test, but it at least proves that a lot of the worry is probably unwarranted. Another point made was about track thickness. It's conventional to make power supply tracks wider, for lower resistance and better current capacity. But unless you're pulling tens of amps, the heating effect of a thin PCB track is negligible. In contrast, said this one PCB designer, a thin track has a higher inductance, so if you make your power supply lines thin, you'll get a free bit of extra filtering.</p><p>

I'm not sure what to make of it all.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/kicad-screenshot2.png" width="1744" height="600" loading="lazy" alt="KiCad screenshot of full Rev C PCB design"></p><p>

Before finalising the interface, I was unsure how many buttons to add. I originally chucked three on there, but by carefully spacing the footprints, fewer switches would work too. Ultimately I went with the two buttons to cycle back and forth through modes, but even with two buttons, they're still spaced so that a single central button could be fitted instead.</p><p>

If we wanted to do anything more than switching modes, such as capturing accurate timestamp signals, we'd want to patch that directly onto the time side PCB, so it doesn't have the latency of crossing the hinge. I put a couple of test pads near the main processor exposing the last of the GPIO pins.</p><p>

The first revision of the Mk IV PCB kept the simple header-pin mounting for the GPS module. This is pretty open-ended as there are a huge number of modules available with similar (but not exactly the same!) pinouts. The older clock kit was still shipping with the uBlox 6 modules, which are now quite old. At the time of development, the latest parts were the 8th generation uBlox modules, that have better performance, but the breakout boards for these had TX and RX swapped compared to the earlier pinout. This breakout board uncertainty, coupled with the fact that I wanted easy antenna changes, led me to solder the castellated module directly to the board with an SMA connector next to it.</p><p>

The problem we then encounter is that the track between the SMA connector and the castellated module needs a controlled impedance. There are calculations for track width based on substrate thickness to get a given impedance, but if you run the numbers on a 2 layer 1.6mm board you'll find the track needs to be some crazy width, maybe a few millimetres. In addition, because no one attempts this usually, the dielectric constant of the FR4 is not well controlled for 2 layer boards.</p><p>

Having said that, the breakout boards for GPS modules are usually 2-layer with little to no consideration given to track impedance. Often the antenna turns a few corners on the PCB with no apparent loss in performance. The fact is, at 1.5GHz, so long as the trace is fairly short it's not going to be a problem. Still, to be on the safe side, I made the track as short as physically possible, by mounting the GPS module in almost direct contact with the SMA footprint, as we can see on the prototype revision B:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pcb-revB.jpg" width="1200" height="747" loading="lazy" alt="GPS module mounted to revision B PCB"></p><p>

The tiny 0402 component is an inductor, that forms part of the RLC filter providing the bias voltage to the antenna.</p><p>

Also visible in the picture above are two holes drilled to make a footprint for the LDR. I spent a long time worrying about the light sensor, even after we'd decided on how the digits were driven. The first revision of the PCB relied on an <a href="https://mitxela.com/projects/automatic_led">op-amp circuit</a>. After dismissing that method, I mounted an SMD ambient light sensor (SFH5711-2/3-Z) but mechanically, topographically, this posed a problem. The light sensor obviously needs to point forwards, but part of my specification was that surface-mount parts are only on the back, and through-mount parts only on the front. Ideally, we could come up with some reverse-mount photodiode and a light pipe arrangement, but after fiddling around with this for ages I just reverted to using an LDR.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/LDR-holes.jpg" width="1200" height="802" loading="lazy" alt="Closeup of drilled holes in PCB before I fitted the LDR"></p><p>

The one downside to this is that technically, CdS LDRs are not RoHS compliant as they contain cadmium. I kept the ambient light sensor footprint on the board as DNP in case anyone would prefer to fit that, but I think the long term solution would be to find a through-mount, photodiode-style ambient light sensor. This may happen on a later revision of the PCB.</p><h3 id="plastic-parts">Plastic Parts</h3><p>
I am a huge fan of laser-cutting, and when it came to finalising the clock instinctively I turned to laser-cut parts. Instead of plywood that needs painting, or expensive delrin, for the non-structural parts of the clock I tried out matt black acrylic.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/lasercut-switches.jpg" width="1200" height="802" loading="lazy" alt="Switches sandwiched between laser-cut acrylic"></p><p>

The hole at the top is for the magnet. Also apparent, on this prototype clock I applied red window tint film over the digits, to see if it was worth it (I decided it wasn't).</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/lasercut-ldr.jpg" width="1200" height="802" loading="lazy" alt="LDR supported using laser-cut acrylic pieces"></p><p>

The matt acrylic looks great, but has the same problem as the parts in the old kit. To build up the shape, the layers need to be glued together, which I know from the previous kit is enough of a hassle that many people don't bother. Reluctantly I turned to 3D printing.</p><p>

The 3D printed designs, with the addition of snap-tabs, mean we can avoid glue entirely. My only complaint is that printing them is very slow. On the other hand, unlike the laser, it can be set printing and left unattended, so in reality it's not so bad.</p><p>

It took a few iterations before I was content. The magnets alone do an OK job of holding the clock together when it's folded, but they don't prevent the PCBs from flexing slightly, meaning the digits farthest from the hinge could slide forwards and backwards a few millimetres. To ensure they snap together and don't slide about, the joint was augmented with a recess, and a corresponding protrusion on the other end, so when the magnets snap together it grips tightly.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/3d-printed1.jpg" width="1200" height="800" loading="lazy" alt="3D-printed LDR holder"></p><p>

The texture on the face is from the printer bed, which I think is a nicer effect than without it. Unfortunately that means we need to print in the plane that gives us the weakest possible snap-tabs. That shouldn't be an issue, as they're not going to be subjected to much in the way of forces normally, and printing in PETG gives us a bit of extra strength anyway. We can potentially alter the design later if it turns out to be a problem.</p><h3 id="logo">Logo</h3><p>
Design optimisations mean that a fair amount of the date side PCB is empty, and represents prime real estate for a logo. But the date side is inverted when the clock folds in half, so our logo needs to look good in both orientations.</p><p>

This predates the full-colour PCB printing options that some board houses now offer, and I wouldn't have gone for that anyway given the variety of visual interest we can create from the layers we already have. The silkscreen is white, the soldermask is red, we also have a darker red if we remove the copper from behind the soldermask. Exposed copper will end up either gold, or silver if we go for HASL finish. Finally there's the option of bare substrate material if we need it.</p><p>

The idea of a "mitxela" ambigram didn't really appeal, I've always felt they're something of a gimmick. Instead, as you may have already noticed from the images above, I laid out the name twice with "Precision Clock Mk. IV" in silkscreen through the middle.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/clock4-logo.png" width="965" height="261" loading="lazy" alt="Raster of mitxela Precision Clock Mk. IV logo"></p><p>

White is silkscreen, yellow is exposed copper. The shadows (which are intentionally not symmetric, as merely flipping it would look weird) are soldermask over bare substrate, i.e. keepout regions for the copper pour. I did the logo layout in inkscape, exported it as SVG and then attempted to import into KiCad using a tool called <a href="https://github.com/svg2mod/svg2mod">svg2mod</a>. That tool has been rewritten substantially since I did this logo, but at the time it struggled to process it and I had to alter the plugin code slightly.</p><p>

The task is to convert the arbitrary shapes of an SVG into distinct, closed polygons for KiCad. The main copper layer is essentially a rectangle with holes in it. Initially I tried exporting directly as keepout polygons, but this gave a subpar result and besides, having a rectangular border was desireable. But exporting to the copper layer directly means breaking up the holes in the shape into a single contour. Svg2mod attempts to do this by finding the nearest point, and slicing an invisible line, so that, say, a donut becomes a closed C shape. Lots of holes and only four corners to branch from led to some errors. I eventually submitted a pull request with my modifications for a recursive approach, but it turned out they were partway through rewriting the algorithm anyway.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/logo-render.jpg" width="1200" height="622" loading="lazy" alt="3D render of PCB, focused on logo"></p><p>

A more pragmatic solution turned out to be quite easy, we just overlap the shape with a rectangle in inkscape and do a boolean modifier. That lets us manually split complex shapes into smaller, closed pieces and the conversion process doesn't need to worry.</p><p>

The stylised logo is in a font called Cyberdyne, by <a href="https://www.iconian.com/">Dan Zadorozny</a>. We do enjoy their 2007 webpage layout.</p><p>

The logo was something of an uncertainty until the first PCBs arrived. I was reasonably pleased with the result. The HASL finish has a mirror-like quality, which I expected to dull quickly but these boards seem to have held up pretty well.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/logo-real.jpg" width="1200" height="800" loading="lazy" alt="Closeup of logo on assembled clock"></p><h3 id="countdown-mode">Countdown mode</h3><p>
With the ease of firmware updates, we could have delayed most of the software development until later, especially for secondary features like Countdown Mode. The logical thing to do would be to release the clock as soon as possible, and add these bonus features later. It's apparent that I'm not really a logical person though.</p><p>

Countdown mode is something of a mind-bender, as simple as it may seem. Just display the number of days, hours, minutes, seconds until an epoch. But the precision, and the dynamic precision, make this a far more subtle problem.</p><p>

To be clear, a normal wall clock does not "round" the time display, it "floors" it (round-down). The last digits of precision are truncated. This has to be true: if the time is 3:59, we might colloquially say it's 4 o'clock, but a digital clock would not display the number four until the exact moment of 4 o'clock is in the past. Further precision, of seconds and subseconds, doesn't have any bearing on when the hours digit ticks over.</p><p>

Think about New Year's Eve where people count down to midnight. They might say "3... 2... 1... Happy new year!" In other words, the event happens when the seconds remaining reaches zero. But consider a countdown with fractional seconds. At 3.x seconds remaining, we'd say 3. But when the whole seconds reaches zero, we still have a 0.9... fraction remaining. The exact moment that the whole seconds ticks down to zero, there is one second left until the event.</p><p>

I don't think we'll be able to convince people to do their New Year's countdown as "3... 2... 1... 0... Happy new year!" but that would make an awful lot more sense. By truncating digits off a count-down, we are "ceiling" it (round-up), and it can never precisely display the moment of the epoch. Our precision clock displays to the millisecond, so you would think the event happens when it ticks to 0.000, but in reality, it should be one millisecond after that.</p><p>

This was essential to get right, as the dynamic precision of the clock otherwise wrecks havoc on when the moment is signified. If we've lost GPS fix, or maybe the clock's been powered off for a while and only showing to whole seconds, should it display the whole number of seconds rounded down? That'd satisfy our New Year crowd but not the scientists in the audience. And what if it then gets its GPS fix and can display to the millisecond, do we then alter that number of whole seconds remaining? I think not. The only thing that makes sense is to truncate, rounded up, and spread the word that everyone's doing their countdowns wrong.</p><h3 id="leap-seconds">Leap seconds</h3><p>
Officially a digital clock should display the number 60 during a leap second.</p><p>

After a bit of research, it seems that GPS modules are wildly inconsistent in their behaviour during a leap second. I eventually concluded that the safest thing to do is to prepare, in advance, a list of upcoming leap seconds and during that second, ignore the GPS module.</p><p>

The list of upcoming leap seconds is available as part of the timezone database, and since we already need to keep that up to date, there's not <i>too</i> much overhead in doing it this way. The only thing is that some people may not ever update the database, especially if they live in a country that never changes its timezone rules.</p><p>

When I started writing the firmware, this was my plan. But the drift between UTC and UT1 suggested there wouldn't be a leap second for a long time, so I put this at a lower priority. Eventually, in the last few years, the difference became such that it was suggested to have a negative leap second, which has always been possible but has never happened before.</p><p>

If you'd asked me, we should have redefined the second orginally so that <i>all</i> leap seconds were negative. Skipping 59 is so much easier than adding a 60. And the Unix timestamp would skip a value, instead of repeating a value and being ambiguous. I think everyone knows what adding a negative leap second would do to the world though, and an announcement was made recently that leap seconds have been semi-retired. Instead, we'll have a leap minute when that becomes necessary. Thus, the problem has been thrust upon our great grandchildren, and thankfully when that happens we'll all be dead.</p><p>

I could say a lot more, but with regard to the clock, if another leap second is ever announced I will implement the feature and release a firmware update for it.</p><h3 id="high-speed-testing">High speed testing</h3><p>
While developing the previous clock, I invested in a consumer camera that could shoot at 1000FPS. It was exactly what I needed to debug and confirm the operation of the seconds interpolation.</p><p>

I got the Mk IV display working without much issue, but 1000FPS is not actually sufficient to confirm its operation. I felt confident, partly because of the interference effects I was getting through the viewfinder of the camera. The preview decimates the framerate, but apparently it's exact, showing only every tenth frame. As such, the milliseconds digit of the clock appeared to be completely solid, unchanging.</p><p>

The really cool thing about this is that there's still a rolling shutter effect, so as you tilt the camera up and down, with the clock being in different parts of the frame, the number shown on that static last digit changes.</p><p>

But I wanted to be sure, and started asking around. It turns out that companies that own high speed cameras are surprisingly amenable and enthusiastic when people turn up and say they have something cool to film at high speed. Huge thanks to Pirate for giving me some time on a Phantom Flex 4K.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/highspeed1.jpg" width="1200" height="550" loading="lazy" alt="Freeze frame from 3000FPS footage"></p><p>

Even the memory cards for this camera cost as much as a house. In the freeze frame above, the real frame rate was 2932FPS at 1080p. The last decimal point was being used the debug the interrupt timing. The red dot above the clock face is the PPS LED. To as good as I could tell, the clock was ticking perfectly.</p><p>

I later found out how to activate the higher framerates of the camera, so in a later session, we filmed the clock first at 14,000FPS:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/highspeed2.jpg" width="1500" height="274" loading="lazy" alt="Freeze frame from 14,000FPS footage"></p><p>

and finally at 25,000FPS, the highest it will go. At that speed the frame is just 16 pixels high, which was enough to see that the time was displayed perfectly. Note that the camera used a shutter angle of 180°, so the effective shutter speed was 1/50,000th.</p><p>

We'd already confirmed the accuracy of the PPS LED with respect to the UTC second, and now the display was confirmed to be aligned to the PPS LED. Proper job.</p><p>

I also, for fun, turned down the matrix update speed. We were able to slow it right down, and film the matrix progressing, and also set the matrix speed to match the camera framerate, which leads to a beat frequency as their relative oscillators slowly drift apart. In that situation, the beat frequency of the time and date sides differ, as the date matrix frequency is derived from the internal oscillator of the date side chip, while the time matrix frequency is based on its highly accurate TCXO. I briefly looked into disciplining the date side's oscillator to the highly accurate latch signals being sent across the hinge, but the internal oscillator calibration is quite low resolution so it's probably not worth it.</p><h3 id="travel-testing">Travel testing</h3><p>
For the entire development period, the clock existed only on my desk. The automatic timezone detection worked in theory, and it's possible to feed it fake GPS coordinates, but that's not exactly the same as proving it works. When the opportunity arose to take the clock abroad, I had to take it.</p><p>

I first took it to the Dutch hacker camp, MCH2022. I had very much hoped to be able to watch the timezone update while riding the Eurostar. Unfortunately, the metal structure of the train completely blocked the GPS signal, and it wasn't able to get a fix until we departed. Aside from that, it all appeared to work.</p><p>

Later I was given the chance to take the clock to Greece. The idea of taking the clock on a plane, and watching it update as we flew across the timezones, was particularly appealing, but ultimately I decided to keep it in my luggage, as the large red ticking digits do have a resemblance to a cartoon bomb.</p><p>

One morning, in the natural harbour of a Greek island, I unfolded the clock and powered it up.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/travel1.jpg" width="1200" height="800" loading="lazy" alt="The clock on the transom of a boat with the sea and the island in the background"></p><p>

This was the Rev B PCB, with the "stubby" antenna. Clear skies, so it updated to the correct UTC offset of +03:00 almost immediately.</p><p>

My ambition was to sail out to sea, and find somewhere with no cellular phone reception – somewhere that only our GPS Precision Clock could serve the time reliably. Unfortunately, there's fantastic phone signal between all the islands so we couldn't actually find anywhere that it mattered.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/travel2.jpg" width="1200" height="800" loading="lazy" alt="The clock, sailing through the Ionian sea."></p><p>

Still, we proved the clock worked at sea!</p><p>

The digits were a little difficult to make out in the Mediterranean sun. For Rev C, I slightly boosted the max brightness, although direct sunlight is always hard to compete with.</p><h3 id="the-production-batch">The production batch</h3><p>
As I geared up to sell the new clock, a global chip shortage happened which meant producing it was, for the moment, impossible. There are horror stories about multi-year lead times, paying ten times the normal price, and still not getting the delivery because someone else outbid at the last minute. Everyone sensible redesigned their circuits to make use of parts that could actually be bought. I didn't have the heart.</p><p>

When the STM32 chips finally came into stock, at a significantly inflated price, I tentatively ordered 100 of them. I wasn't really thinking about how this would work. For a kit, it wouldn't really be fair or practical to send out unpopulated circuit boards with dozens of fine pitch surface mount components to fit. The only viable clock kit would be if the SMD parts were fitted, and just the through mount and mechanical parts need assembly. I had originally planned to order the populated PCBs from China, but sending my own parts out there is completely impractical. Aside from anything else, I'd end up paying import/export duty on everything repeatedly.</p><p>

When a pick and place machine was donated to the hackspace, however, an idea occurred. Possibly the only benefit of the hackspace being evicted from their building is that everything had to go into storage for a few years. I helpfully offered to "store" the pick and place machine at my house.</p><p>

The machine is a Charmhigh CHM-T36VA.</p><p>

I've since featured this machine in a few projects. It directly led to the <a href="https://mitxela.com/projects/candle">volumetric display</a> and the <a href="https://mitxela.com/projects/badge">badge</a>, and I pushed the limits of it with the <a href="https://mitxela.com/projects/ledstud">matrix earrings</a>. Anything with a large number of similar components is very easy to set up and produce. But if you have the patience to load all of the reels, the machine can be used to produce complex circuit boards too.</p><p>

There are 26 unique surface-mount components on the clock, which means 26 reels. The machine has a number of fixed-width feeders: the first twenty-two are 8mm, then four at 12mm, two at 16mm and one 24mm. That's enough to hold almost (but not quite) all of our parts.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pick-and-place1.jpg" width="1200" height="798" loading="lazy" alt="Loading reels into the pick and place machine"></p><p>

Loading a reel is a tedious affair. The film needs to be peeled back enough to reach the take-up spool, which usually means joining onto the previous piece of film. There are tiny stickers available that can be used for this purpose. The sprocket holes need to align to where the machine expects them, and the waste tape needs to be guided down into the slot, when enough of it has been exposed.</p><p>

I ordered most of the parts from digikey. Like several other distributors, they offer a re-reeling service, so if you buy less than a full reel, and are willing to pay a $5 surcharge, they'll attach a leader to it and put it on a reel. I went with this as I thought it would save time. It turned out to be completely and inexplicably useless.</p><p>

They obviously have the technology to join two pieces of cut tape together. That is how the leaders and trailers are attached. And yet, what they sent me was a total mishmash of reels with random amounts on them. For one part number, they sent three separate reels, one of which had <b>only four components</b> on it! Given what a palaver it is to load a reel, this is actively unhelpful.</p><p>

Additionally, the brass shimstock they used to join the leaders was pretty poor. It was both weak enough to easily break, but thick enough that it tended to jam in the feeder. It's perfectly possible to load cut tape into a feeder, you just need to advance it enough to be able to attach the film to the take-up reel.</p><p>

Another pain in loading the reels for the first time is that you need to carefully note down in a spreadsheet what's been loaded, the part orientations, the feed spacing and so on. The machine will accept quite a few parameters for each feeder, such as which head to use, whether to use the vision system, whether to check the vacuum pressure, centroid position correction and so on.</p><p>

The big ICs were shipped in a tray. There's provisions to clamp a tray into place and instruct the machine on the array of parts. I was reasonably impressed with the ease of setting this up.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pick-and-place3.jpg" width="1200" height="798" loading="lazy" alt="Closeup of pick and place heads over IC tray"></p><p>

The machine software is intended to run on a dedicated tablet, and has a pseudo-touchscreen interface.  Some idiosyncrasies arise from this, such as the software being unable to talk to anything except "COM1" on the windows machine. I had to tediously redefine the COM number of the serial port in Device Manager. Another, somewhat comical effect is that it assumes the first two video devices are the vision system. The laptop's built-in camera had to be completely disabled to avoid this. Powerup is also sensitive to the order in which the USB cables are plugged in, get it wrong and the cameras don't work.</p><p>

It's a bit clunky but defining an IC tray amounts to x,y coordinates of the first and last component, and how many there are. The vision system lets you line up the parts through the GUI. The software keeps track of which parts have been picked during use and is surprisingly robust. For the parts I couldn't fit into feeders, I laser-cut some holders for them out of plywood, effective turning cut tape into single-row IC trays.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pick-and-place2.jpg" width="1200" height="800" loading="lazy" alt="Pick and place machine with most parts ready"></p><p>

For the two most awkward parts, the coin cell holder and the SMA connector, I just placed these by hand at the end.</p><p>

A nice feature is that you can step through a job one motion at a time, and check everything's working.</p><p>

When we're ready to go, it's time to stencil some solder-paste onto a board, and start placing parts.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/paste1.jpg" width="1200" height="800" loading="lazy" alt="Closeup of stencilled solder paste"></p><p>

There's a knack to stencilling paste well. Something I quickly figured out, when it came to making the batch, is to frequently wipe the underneath of the stencil. If the tiniest bit of paste pulls astray as you lift the stencil, it will get jammed on the next iteration and stop the stencil from sitting perfectly flat. This leads to more stray paste under the stencil, and it quickly spreads to being a total mess. A simple wipe with a paper towel on the underside keeps this from happening.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/paste2.jpg" width="1200" height="800" loading="lazy" alt="Parts placed on stencilled paste, closeup of microcontroller pins"></p><p>

The smallest part is our 0402 inductor, and the largest part is one of these integrated circuits. There's no automatic toolchanger on the machine, but the two heads are just about enough to cover everything. I pretty much grouped it into two categories, small things and big things. Using too-small a vacuum nozzle means parts are likely to twist as they get grabbed. After picking up a part, the machine holds it over the up-facing camera and determines the outline.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pnp-sw1.jpg" width="1200" height="675" loading="lazy" alt="Screenshot from pick and place machine, with LQFP-48 part outlined"></p><p>

There's no information in the feeders list about part sizes. It has no idea if what it's holding is a tiny resistor or a big IC. It just draws a box around what it thinks are the pins, finds the centre and angle, and corrects for it as it places the part.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pnp-sw2.jpg" width="1200" height="675" loading="lazy" alt="Screenshot from pick and place machine, with LQFP-48 part outlined, at an angle"></p><p>

A problem I had early on is that stray light interfered with this, sunlight from the window and the big fluorescent light of the desk caused reflections. Sometimes it would think a small resistor was a big chip with a pin over in the corner. The addition of some black tape on the carriage solved this entirely, so that seems like a design flaw.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/pnp-sw3.jpg" width="1200" height="675" loading="lazy" alt="Screenshot from pick and place machine, with SOIC-8 part outlined"></p><p>

For the most part, once I got this working, it worked great.</p><p>

After placement, the boards need to be reflowed (cooked). The pick and place machine, both its donation to the hackspace and my temporary procurement of it, came with a reflow oven. This oven is a monster. It weighs a ton, draws 3.5kW, has a conveyor belt and six PID-controlled temperature stages (three top, three bottom). If anything, I think I would have preferred something smaller and better matched to the speed at which I could stencil boards and place the parts.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/reflow1.jpg" width="1200" height="800" loading="lazy" alt="PCB with logo exits the reflow oven"></p><p>

One tiny disadvantage of reflowing is that the HASL finish of the beautiful logo gets a little tarnished by the process.</p><p>

I didn't experiment too much with the settings for the oven. I realised midway through that one of the fans was stuck, and after dismantling the machine I was able to repair it. I think this was the reason that the previous user had set the temperature a lot higher than it needed to be. Speed of the conveyor is quite important. The hardest part to get to reflow was the GPS module, with its shielding can.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/reflow2.jpg" width="1200" height="800" loading="lazy" alt="A batch of PCBs cooling down after reflow"></p><p>

The cardboard is my cooling surface. The conveyor by default just dumps the boards on the floor. At least twice, I wasn't watching the output and a board got caught in the conveyor and pulled into the machine, in one case breaking the SMA connector. The need to supervise it is a huge detriment to the throughput. What I ended up doing, once I'd got the speed right, is setting a timer every time I put a board in, which would go off a few seconds before it came out the other end.</p><p>

Honestly, for these 100 sets of clock PCBs, I think doing them in batches of 10 or 20 in a normal oven would have been much more sensible.</p><p>

The thing I regret most about this batch is that I used up the last of a pot of solder paste I already had. I only planned to do a test run with it but since it worked fine, I continued through the whole batch. Towards the end, the paste had definitely expired. For the most part this doesn't matter, it just means the joints are not as beautiful as they could be. It was lead-free solder, as I was trying to be RoHS compliant, so they would never be <i>that</i> pretty, but since the clock PCB sort-of puts the parts on display I would have preferred them to be as nice as possible. Given how much I invested into this, it's silly that I didn't buy a fresh pot of solder paste for the batch.</p><p>

Moreso, I learned that the fumes from lead-free solder are horrible thanks to the much more aggressive flux it requires. The oven has provision for proper extraction, but on the first day I didn't pay much attention to this and ended up with a splitting headache which knocked me out for most of the next day. I probably hypersensitised myself to it, as even after installing the proper ducting, getting a whiff of the flux fumes would cause my brain to swell.</p><p>

There's a lot I've skipped over here. For instance sourcing the GPS module was a nightmare. It was out of stock everywhere, but a seller on aliexpress listed them. I bought two initially to check they were OK. The two parts sent to me appeared to be genuine and worked perfectly. I then ordered a hundred, and what arrived were one hundred fake GPS modules that didn't work. They had the same sticker on top but the PCB design was subtly different, and, well, they just couldn't get a GPS fix. It was a colossal waste of time and money (which I never managed to claw back from aliexpress – they wanted video proof of all of them not working simultaneously).</p><p>

I eventually found someone in the UK with an unopened reel of genuine parts, which they were willing to sell to me. What's the lesson in all this? I don't think there is one, sourcing parts is a nightmare at the best of times and sourcing them during a global pandemic was excruciating. As a final slap in the face, after fighting through all this I then lost momentum and waited long enough before releasing the clock that the supply chain problems were resolved, and these parts are now available for a lot less than what I paid for them.</p><h3 id="conclusion">Conclusion</h3><p>
Congratulations if you've read all of this, it must be one of the longest project pages on the site. Writing it up took only a few days. Developing the clock took years!</p><p>

I've had a Mk IV clock in every room of my house for the last few years, and one thing that has really grown on me is the dynamic precision. Being able to immediately see the tolerance is invaluable.</p><p>

There's an old adage that a man with one clock knows the time, but a man with two clocks is never sure. Unless, of course, those clocks display a tolerance.</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/twoclocks-tolerance.jpg" width="1200" height="400" loading="lazy" alt="Two clocks, with one displaying time to the millisecond, and another to the decisecond"></p><p>

I showed off the clock at EMF 2022, and then, two years later at EMF 2024, I did a talk on it and the development that went into it. That talk is available on <a href="https://media.ccc.de/v/emf2024-337-gps-time-leap-seconds-and-a-clock-thats-always-right">media.ccc.de</a> and <a href="https://www.youtube.com/watch?v=u2wyY0CTiXo">youtube</a>.</p><p>

It is weird to think that there are almost a thousand Mk III precision clocks out there. Throughout the development of the Mk IV, I tried to incorporated every feature request I'd ever had, and there are only three things that I couldn't squeeze into the Mk IV.</p><ul>
<li>One was to make the clock waterproof. Both for outdoor usage, and in one request, to be fully submerged. The hinge design complicates this greatly, and I decided the best option for waterproofing is to put the whole clock inside a sealed clear box.</li><li>Two was to give it a lithium-ion battery. I did consider adding a charge controller and the footprint for an 18650 holder, which would have turned the clock into a self-contained thing with no cables, but for the vast majority of use cases it's totally fine to connect up a USB battery bank. </li><li>Three was to give the clock even more precision. There aren't many people that have access to cameras that can go above 100,000FPS, but one rather prominent person asked me to build a clock that would read out to the microsecond. We did some tests, and it's absolutely doable, but I promised I would release the millisecond clock first.
</li></ul><p>

For the Mk V, or whatever I call the microsecond version, it'll not only need direct drive of all the digits but the ability to go a lot brighter. At those frame rates we need a lot of light, so we might need a boost button to temporarily up the current. I may as well share some screenshots of the single-digit prototype I built, first ticking glacially slowly at 100000FPS:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/microsecond1.jpg" width="1024" height="320" loading="lazy" alt="Microsecond prototype filmed at 100000FPS"></p><p>

and finally ticking at one microsecond, filmed at an incredible 1.75 million FPS:</p><p>

<img src="https://mitxela.com/img/uploads/clock/mk4/microsecond2.jpg" width="640" height="129" loading="lazy" alt="Microsecond prototype filmed at 1750000FPS"></p><p>

Naturally if we did build this full clock, we'd need to put a bit more thought into the time source, as the basic GPS modules have a sawtooth error on the PPS that would start to become significant. There are timing-specific modules with more stable outputs, or we could go back to disciplining a local oscillator. It's important that the microsecond digit has a jitter of <i>much less</i> than one microsecond.</p><p>

Anyway, that's in the future. To conclude this conclusion, I'll just say what a relief it is to finally publish this. The source code to the clock is in the <a href="https://github.com/mitxela/clock4">github repo</a> where many of the commit messages contain even deeper technical info. The PCB files will also be published soon.</p><nav>
<a href="https://mitxela.com/projects/random" title="random project">~</a>
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/">mitxela.com</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects">Projects</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/hardware">Hardware</a></span> »
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/precision_clock_mk_iv">Precision Clock Mk IV</a></span>
<p>Questions? Comments? Check out the <a href="https://mitxela.com/forum">Forum</a>
</p><p><a href="https://mitxela.com/support">Support mitxela.com</a>
</p></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using lots of little tools to aggressively reject the bots (124 pts)]]></title>
            <link>https://lambdacreate.com/posts/68</link>
            <guid>44142761</guid>
            <pubDate>Sat, 31 May 2025 08:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lambdacreate.com/posts/68">https://lambdacreate.com/posts/68</a>, See on <a href="https://news.ycombinator.com/item?id=44142761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    <h2>403: You are not authorized to access this material</h2>
<h2>Or using lots of little tools to aggressively reject the bots · October 18th, 2024</h2>
<p>U G H.</p>
<p>For some reason my quaint little piece of the internet has suddenly been inundated with unwanted guests. Now normally speaking I would be over joyed with having more guests to this tiny part of the internet. Come inside the cozy little server room, we have podcasts to drown out the noise of the fans, and plenty to read. Probably at some point in the future there will be photography things too, and certainly plenty of company. But no, nobody can have such nice things. Instead the door to the server room was kicked down and in came a horde of robots hell bent on scraping every bit of data they possibly could from the site.</p>
<p>Now, for the longest time, I've had no real issue with this. Archive.org is welcome to swing by any time, index the entire site, and stash it away for posterity. Keep up the good work folks! But instead of respectful netizens like that, I have the likes of Amazon, Facebook, and OpenAI, along with a gaggle of random friends, knocking on my doors. These big corporations 1) do not need my content and 2) are only accessing it for entirely self serving means.</p>
<p>Lets not even pretend it's anything else, because we know it isn't. These large companies scrape data broadly and with little regard to the effect it has on the infrastructure servicing whatever it is they're pulling from. With the brain slug that is "AI" now openly encouraging the mass consumption of data from the internet at large to train their models on, it was really only a matter of time before the scraping became more severe. This is the hype cycle at work. OpenAI needs to scrape to train, Facebook does too because they have a competing model. Amazon and Google and Microsoft all have their own reasons related to search and advertising, bending the traffic to flow through their platforms. The point is, these are not "consumers" of Lambdacreate. You, the human reading this, are! Thanks for reading.</p>
<p>To the bots. Roboti ite domum!</p>
<h3>Hyperbole aside, what's our problem exactly?</h3>
<p>Fortunately, I am well versed in systems administration, and have a whole toolkit at my disposal to analyze the issue. Let's put some numbers against all of the above hyperbole.</p>
<p>My initial sign that something was up came in from my Zabbix instance. I call the little server that runs my Zabbix &amp; Loki instances Vignere after the creator of the Vignere Cipher, hence the funky photo in Discord. Anyways, Vignere complained about my server using up its entire disc for all of my containers. Frustrating, but not a big deal since I'm using LXD under the hood.</p>
<p><img src="https://lambdacreate.com/static/images/68/zabbix-disc-alerts.png" alt="A Zabbix alert in a Discord channel displaying disc exhaustion for multiple containers."></p>
<p>Fine, I'll take my lumps. Took down all of my sites briefly, expanding the underlying ZFS sparse file, and brought the world back up. No harm no foul, just growing pains. But of course, that really wasn't the issue. I was inundated with more alerts. Suddenly I was seeing my Gitea instance grow to consume the entire disc every single day, easily generating 20-30G of data each day. Super frustrating, and enough information on the internet says that Gitea just does this and doesn't enable repo archive cleanup by default, so that must be it. I happily go and setup some aggressive cleanup tasks thinking my problems are over. Maybe I shouldn't have setup a self-hosted git forge and just stuck with Gitlab or Github.</p>
<p>But no, not at all, this thin veneer of a fix rapidly crumbled under the sudden and aggressive uptick in web traffic I started seeing. Suddenly it wasn't just disc usage, I was getting inundated with CPU and Memory alerts from my poor server. I couldn't git pull or push to my Gitea. Hell my weechat client couldn't even stay connected. Everything ground to a halt for a bit. But by the time I could get away from work, or the kids, and pull out my computer to dig into it the problem had stopped. I could access everything. Sysstat and Zabbix told me that the resource utilization issues were real, but I couldn't exactly tell why from just that.</p>
<p><img src="https://lambdacreate.com/static/images/68/zabbix-cpu-alerts.png" alt="A Zabbix alert in a Discord channel displaying extremely high cpu utilization."></p>
<p>This is however, why I keep an out of band monitoring system in the first place. I need to be able to look at historic metrics to see what "normal" looks like. Otherwise it's all just guesswork. And boy did Zabbix have a story to tell me. To get a clear understanding of what I mean, lets take a quick look at the full dashboard from when I redid my Zabbix server after it failed earlier this year. Pew pew flashy graphs right? The important one here is the nginx requests and network throughput chart in the bottom left hand corner of the dashboard. Note that that's what "normal" traffic looks like for my tiny part of the internet.</p>
<p><img src="https://lambdacreate.com/static/images/68/zabbix-dashboard.png" alt="An aggregate Zabbix graph that shows Nginx requests per second overlaid with in/out bound network traffic data."></p>
<p>And this, dear reader, is what the same graph looks like after LC was laid siege to. Massive difference right? And not a fun one either. On average I was seeing 8 requests per second come into nginx across a one month period. It's not a lot, but once again, this is just a tiny server hosting a tiny part of the internet. I'm not trying to dump hyper scale resources into my personal blog, it just isn't necessary.</p>
<p><img src="https://lambdacreate.com/static/images/68/zabbix-nginx-requests.png" alt="The same graph, only scary."></p>
<p>At its worst Zabbix shows that for a period I was getting hit with 20+ requests per second. Once again, not a lot of traffic, but it is 10x what my site usually gets, and that makes a big difference!</p>
<p>So why the spike in traffic? Why specifically from my gitea instance? Why are there CPU and Disc alerts mixed into all of this, it's not like 20+ requests a second is a lot for nginx to handle by any means. To understand that, we need to dig into the logs on the server.</p>
<h3>Looking under the hood</h3>
<p>But before I could even start to do that I needed a way to get keep the server online long enough to actually review the logs. This is where out of band logging like a syslog or loki server would be extremely helpful. But instead I had the join of simply turning off all of my containers and disabling the nginx server for a little bit. After that I dug two great tools out of my toolkit to perform the analysis, lnav &amp; goaccess.</p>
<p>lnav is this really great log analysis tool, it provides you with a little TUI that color codes your log files and skim through them like any other pager. That in and of itself is cool, but it also provides an abstraction layer on top of common logging formats and lets you query the data inside of the log using SQL queries. That, for me, is a killer feature. I'm certainly not scared to grep, sed, and awk my way through a complex log file, but SQL queries are way simpler to grasp.</p>
<p>Here's the default view, it's the equivalent of a select * from access_log.</p>
<p><img src="https://lambdacreate.com/static/images/68/lnav_default.png" alt="The default rendering for an nginx access log, there's a ton of colors, it's log files made pretty!"></p>
<p>Digging through this log ended up being incredibly easy and immediately informative. I won't bore anyone with random data, but these are the various queries I ran against my access.log to try and understand what was happening.</p>
<pre><code># How many different visitors are there total?
select count(distinct(c_ip)) from access_log;

# Okay that's a big number, what do these IPs look like, is there a pattern?
select distinct(c_ip) from access_log;

# Are these addresses coming from somewhere specific (ie: has LC been posted to Reddit/Hackernews and hugged to death?)
select distinct(cs_referer) from access_log;

# Are these IPs identified by a specific agent?
select distinct(cs_user_agent) from access_log;

# Theres a lot of agents and IPs, what IPs are associated with what address?
select c_ip, cs_user_agent from access_log;
</code></pre>
<p>After a quick review of the log it was obvious that the traffic wasn't originating from the same referrer, ie: no hug of death. Would've been neat though right? Instead there was entire blocks of IP addresses hitting www.lambdacreate.com and krei.lambdacreate.com and scraping every single url. Some of these IPs were kind enough to use actual agent names like Amazonbot, OpenAI, Applebot, and Facebook, but there was plenty of obviously spoofed user agents in the mix. Since this influx of traffic was denying my own access to the services I host (specifically my Gitea instance) I figured the easiest and most effective solution was just to slam the door in everyone's face. Sorry, this is MY corner of the internet, if you can't play nice you aren't welcome.</p>
<p><img src="https://lambdacreate.com/static/images/68/so_anyways.png" alt="So anyways, I just started banning."></p>
<h3>Roboti ite infernum</h3>
<p>Nginx is frankly an excellent web server. I've managed lots of Apache in my time, but Nginx is just slick. Really it's probably the fact that Openresty + Lapis brings you this wonderful blog that I really have a preference at all because I'm positive what I'm about to describe is entirely doable in Apache as well. Right, anyways, the easiest way to immediately change the situation is to outright reject anyone who reports their user agent and is causing any sort of disruption.</p>
<p>My hamfisted solution to that is to just build up a list of all of the offensive agents. Sort of like this, only way longer.</p>
<pre><code>map $http_user_agent $badagent {
        default         0;
        ~*AdsBot-Google 1;
        ~*Amazonbot     1;
        ~*Amazonbot/0.1 1;
}
</code></pre>
<p>Then in the primary nginx configuration, source the user agent list, and additional setup a rate limit. Layering the defenses here allows me to outright block what I know is a problem, and slow down anything that I haven't accounted for while I make adjustments.</p>
<pre><code># Filter bots to return a 403 instead of content.
include /etc/nginx/snippets/useragent.rules;

# Define a rate limit of 1 request per second every 1m
limit_req_zone $binary_remote_addr zone=krei:10m rate=5r/s;
</code></pre>
<p>Then in the virtual host configuration we configure both the rate limit and a 403 rejection statement.</p>
<pre><code>limit_req zone=krei burst=20 nodelay;

if ($badagent) {
         return 403;
}
</code></pre>
<p>It really is that hamfisted and easy. If you're on the list, 403. If you're not and you start to scrape, you get the door slammed in your face! But of course this only half helps, while issuing 403s prevents access to the content of the site, my server still needs to process that http request and reject it. That's less resource intense then processing something on the backend, but it's still enough where if the server is getting tons of simultaneous scraping requests that it bogs it down.</p>
<p>Now with 403 rejections in place we can start to prod the nginx access log with lnav. How about checking to see all of the unique IPs that our problems originate from?</p>
<pre><code>select distinct(c_ip) from access_log where sc_status = 403;
</code></pre>
<p><img src="https://lambdacreate.com/static/images/68/lnav_distinct.png" alt="126 distinct IPs displayed in lnav"></p>
<p>Or better yet, we can use goaccess to analyze in detail all of our logs, historic and current, and see how many requests have hit the server, and what endpoint they're targeting the most.</p>
<pre><code>zcat -f access.log-*.gz | goaccess --log-format=COMBINED access.log -o scrapers.html
</code></pre>
<p><img src="https://lambdacreate.com/static/images/68/goaccess_dash.png" alt="The Goaccess dashboard displaying the broad total statistics.">
<img src="https://lambdacreate.com/static/images/68/goaccess_ips.png" alt="The Goaccess graphs displaying the total IP and agent type graphs."></p>
<p>Either of these is enough to indicate that there are hundreds of unique IPs, and to fetch lists of user agents to block. But to actually protect the server we need to go deeper, we need firewall rules, and some kind of automation. What we need is Fail2Ban.</p>
<p>Since we're 403 rejecting traffic based off of known bad agents, our fail2ban rule can be wicked simple. And because I just don't care anymore we're handing out 24 hour bans for anyone breaking the rules. That means adding this little snippet to our fail2ban configuration.</p>
<pre><code>[nginx-forbidden]
enabled = true
port     = http,https
logpath = /var/log/nginx/access.log
bantime = 86400
</code></pre>
<p>And then creating a custom regex to watch for excessively 403 requests.</p>
<pre><code>[INCLUDES]

before = nginx-error-common.conf

[Definition]
failregex = ^&lt;HOST&gt; .* "(GET|POST) [^"]+" 403
ignoreregex =

datepattern = {^LN-BEG}

journalmatch = _SYSTEMD_UNIT=nginx.service + _COMM=nginx
</code></pre>
<p>And our result is! Boom! A massive ban list! 735 bans at the time of writing this. Freaking ridiculous.</p>
<pre><code>~|&gt;&gt; fail2ban-client status nginx-forbidden
Status for the jail: nginx-forbidden
|- Filter
|  |- Currently failed: 13
|  |- Total failed:     57135
|  `- File list:        /var/log/nginx/access.log
`- Actions
   |- Currently banned: 38
   |- Total banned:     735
   `- Banned IP list:   85.208.96.210 66.249.64.70 136.243.220.209 85.208.96.207 185.191.171.18 85.208.96.204 185.191.171.15 85.208.96.205 85.208.96.201 185.191.171.8 85.208.96.200 185.191.171.4 185.191.171.11 185.191.171.1 85.208.96.202 185.191.171.5 185.191.171.6 85.208.96.209 185.191.171.10 85.208.96.203 85.208.96.195 85.208.96.206 185.191.171.16 185.191.171.7 85.208.96.208 185.191.171.17 185.191.171.2 85.208.96.199 85.208.96.212 185.191.171.13 66.249.64.71 66.249.64.72 185.191.171.3 85.208.96.197 85.208.96.193 85.208.96.196 185.191.171.12 85.208.96.194
</code></pre>
<h2>So what?</h2>
<p>The end result is that you're able to enjoy this blog post, and have had access to all the other great lambdacreate things for several months now. Because it is incredibly difficult to write blog posts when you have to fend off the robotic horde. None of this was even scraping my blog, it was all targeting at generating tarballs of every single commit of every single publicly listed git repo in my gitea instance. Disgusting and unnecessary. But I'm leaving the rule set, take a quick glance at the resource charts from Zabbix and you'll readily understand why.</p>
<p><img src="https://lambdacreate.com/static/images/68/zabbix_fixed.png" alt="The change displayed as a Zabbix graph, everything is looking way better now."></p>
<p>Long term, I'll probably want to figure out a way to extend this list, or make exceptions for legitimate services such as archive.org. And I don't want the content here to be delisted from search engines necessarily, but at the same time this isn't here to fuel the AI enshitification of the internet either. So allez vous faire foutre scrapers.</p>

  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beware of Fast-Math (259 pts)]]></title>
            <link>https://simonbyrne.github.io/notes/fastmath/</link>
            <guid>44142472</guid>
            <pubDate>Sat, 31 May 2025 07:05:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonbyrne.github.io/notes/fastmath/">https://simonbyrne.github.io/notes/fastmath/</a>, See on <a href="https://news.ycombinator.com/item?id=44142472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>One of my more frequent rants, both online and in person, is the danger posed by the "fast-math" compiler flag. While these rants may elicit resigned acknowledgment from those who already understand the dangers involved, they do little to help those who don't. So given the remarkable paucity of writing on the topic (including the documentation of the compilers themselves), I decided it would make a good inaugural topic for this blog.</p> <h2 id="so_what_is_fast-math"><a href="#so_what_is_fast-math">So what is fast-math?</a></h2> <p>It's a compiler flag or option that exists in many languages and compilers, including:</p> <ul> <li><p><code>-ffast-math</code> (and included by <code>-Ofast</code>) in <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a> and <a href="https://clang.llvm.org/docs/UsersManual.html#cmdoption-ffast-math">Clang</a></p> </li><li><p><code>-fp-model=fast</code> (the default) in <a href="https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/compiler-option-details/floating-point-options/fp-model-fp.html">ICC</a></p> </li><li><p><code>/fp:fast</code> in <a href="https://docs.microsoft.com/en-us/cpp/build/reference/fp-specify-floating-point-behavior?view=msvc-170">MSVC</a></p> </li><li><p><a href="https://docs.julialang.org/en/v1/manual/command-line-options/#command-line-options"><code>--math-mode=fast</code> command line option</a> or <a href="https://docs.julialang.org/en/v1/base/math/#Base.FastMath.@fastmath"><code>@fastmath</code> macro</a> in Julia.</p> </li></ul> <p>So what does it actually do? Well, as the name said, it makes your math faster. That sounds great, we should definitely do that!</p> <blockquote> <p>I mean, the whole point of fast-math is trading off speed with correctness. If fast-math was to give always the correct results, it wouldn’t be fast-math, it would be the standard way of doing math.</p> </blockquote> <p>— <a href="https://discourse.julialang.org/t/whats-going-on-with-exp-and-math-mode-fast/64619/7?u=simonbyrne">Mosè Giordano</a></p> <p>The rules of floating point operations are specified in <a href="https://en.wikipedia.org/wiki/IEEE_754">the IEEE 754 standard</a>, which all popular programming languages (mostly) adhere to; compilers are only allowed to perform optimizations which obey these rules. Fast-math allows the compiler to break some of these rules: these breakages may seem pretty innocuous at first glance, but can have significant and occasionally unfortunate downstream effects.</p> <p>In <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a>, <code>-ffast-math</code> (or <code>-Ofast</code>) enables the following options: <code>-fno-math-errno</code>, <code>-funsafe-math-optimizations</code>, <code>-ffinite-math-only</code>, <code>-fno-rounding-math</code>, <code>-fno-signaling-nans</code>, <code>-fcx-limited-range</code> and <code>-fexcess-precision=fast</code>. Note that <code>-funsafe-math-optimizations</code> is itself a collection of options <code>-fno-signed-zeros</code>, <code>-fno-trapping-math</code>, <code>-fassociative-math</code> and <code>-freciprocal-math</code>, plus some extra ones, which we will discuss further below.</p> <p>Now some of these are unlikely to cause problems in most cases: <code>-fno-math-errno</code><sup id="fnref:1"><a href="#fndef:1">[1]</a></sup>, <code>-fno-signaling-nans</code>, <code>-fno-trapping-math</code> disable rarely-used (and poorly supported) features. Others, such as <code>-freciprocal-math</code> can reduce accuracy slightly, but are unlikely to cause problems in most cases.</p> <p><a href="https://kristerw.github.io/2021/10/19/fast-math/">Krister Walfridsson</a> gives a very nice (and somewhat more objective) description of some of these, but I want to focus on three in particular.</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-ffinite-math-only"><code>-ffinite-math-only</code></a></h2> <blockquote> <p>Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or +-Infs.</p> </blockquote> <p>The intention here is to allow the compiler to perform some <a href="https://stackoverflow.com/a/10145714/392585">extra optimizations</a> that would not be correct if NaNs or Infs were present: for example the condition <code>x == x</code> can be assumed to always be true (it evaluates false if <code>x</code> is a NaN).</p> <p>This sounds great! My code doesn't generate any NaNs or Infs, so this shouldn't cause any problems.</p> <p>But what if your code doesn't generate any intermediate NaNs only because it internally calls <code>isnan</code> to ensure that they are correctly handled?</p> <p>  — based on <a href="https://twitter.com/johnregehr/status/1440024236257542147">an example from John Regehr</a></p> <p>(to explain what this is showing: the function is setting the return register <code>eax</code> to zero, by <code>xor</code>-ing it with itself, which means the function will always return <code>false</code>)</p> <p>That's right, your compiler has just removed all those checks.</p> <p>Depending on who you ask, this is either obvious ("you told the compiler there were no NaNs, so why does it need to check?") or ridiculous ("how can we safely optimize away NaNs if we can't check for them?"). Even compiler developers <a href="https://twitter.com/johnregehr/status/1440021297103134720">can't agree</a>.</p> <p>This is perhaps the single most frequent cause of fast-math-related <a href="https://stackoverflow.com/a/22931368/392585">StackOverflow</a> <a href="https://stackoverflow.com/q/7263404/392585">questions</a> and <a href="https://github.com/numba/numba/issues/2919">GitHub</a> <a href="https://github.com/google/jax/issues/276">bug</a> <a href="https://github.com/pytorch/glow/issues/2073">reports</a>, and so if your fast-math-compiled code is giving wrong results, the very first thing you should do is disable this option (<code>-fno-finite-math-only</code>).</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-fassociative-math"><code>-fassociative-math</code></a></h2> <blockquote> <p>Allow re-association of operands in series of floating-point operations.</p> </blockquote> <p>This allows the compiler to change the order of evaluation in a sequence of floating point operations. For example if you have an expression <code>(a + b) + c</code>, it can evaluate it instead as <code>a + (b + c)</code>. While these are mathematically equivalent with real numbers, they aren't equivalent in floating point arithmetic: the errors they incur can be different, in some cases quite significantly so:</p> <pre><code>julia&gt; a = <span>1e9</span>+<span>1</span>; b = -<span>1e9</span>; c = <span>0.1</span>;

julia&gt; (a+b)+c
<span>1.1</span>

julia&gt; a+(b+c)
<span>1.100000023841858</span></code></pre> <h3 id="vectorization"><a href="#vectorization">Vectorization </a></h3> <p>So why would you want to do this? One primary reason is that it can enable use of vector/SIMD instructions:</p>  <p>For those who aren't familiar with SIMD operations (or reading assembly), I'll try to explain briefly what is going on here (others can skip this part). Since raw clock speeds haven't been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a "vector" (basically, a short sequence of values contiguous in memory).</p> <p>In this case, instead of performing a sequence of floating point additions (<code>addss</code>), it is able to make use of a SIMD instruction (<code>addps</code>) which takes vector <code>float</code>s (4 in this case, but it can be up to 16 with AVX 512 instructions), and adds them element-wise to another vector in one operation. It does this for the whole array, followed by a final reduction step where to sum the vector to a single value. This means that instead of evaluating</p> <pre><code>s = arr[<span>0</span>] + arr[<span>1</span>];
s = s + arr[<span>2</span>];
s = s + arr[<span>3</span>];
...
s = s + arr[<span>255</span>];</code></pre> <p>it is actually doing</p> <pre><code>s0 = arr[<span>0</span>] + arr[<span>4</span>]; s1 = arr[<span>1</span>] + arr[<span>5</span>]; s2 = arr[<span>2</span>] + arr[<span>6</span>];  s3 = arr[<span>3</span>] + arr[<span>7</span>];
s0 = s0 + arr[<span>8</span>];     s1 = s1 + arr[<span>9</span>];     s2 = s2 + arr[<span>10</span>];     s3 = s3 + arr[<span>11</span>]);
...
s0 = s0 + arr[<span>252</span>];   s1 = s1 + arr[<span>253</span>];   s2 = s2 + arr[<span>254</span>];    s3 = s3 + arr[<span>255</span>]);
sa = s0 + s1;
sb = s2 + s3;
s = sa + sb;</code></pre> <p>where each line corresponds to one floating point instruction.</p> <p>The problem here is that the compiler generally isn't allowed to make this optimization: it requires evaluating the sum in a different association grouping than was specified in the code, and so can give different results<sup id="fnref:4"><a href="#fndef:4">[2]</a></sup>. Though in this case it is likely harmless (or may even improve accuracy<sup id="fnref:2"><a href="#fndef:2">[3]</a></sup>), this is not always the case.</p> <h3 id="compensated_arithmetic"><a href="#compensated_arithmetic">Compensated arithmetic</a></h3> <p>Certain algorithms however depend very strictly on the order in which floating point operations are performed. In particular <em>compensated arithmetic</em> operations make use of it to compute the error that is incurred in intermediate calculations, and correct for that in later computations.</p> <p>The most well-known algorithm which makes use of this is <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan summation</a>, which corrects for the round off error incurred at addition step in the summation loop. We can compile an implementation of Kahan summation with <code>-ffast-math</code>, and compare the result to the simple loop summation above:</p>  <p>It gives <em>exactly</em> the same assembly as the original summation code above. Why?</p> <p>If you substitute the expression for <code>t</code> into <code>c</code>, you get</p> <pre><code>c = ((s + y) - s) - y);</code></pre>
<p>and by applying reassociation, the compiler will then determine that <code>c</code> is in fact always zero, and so may be completely removed. Following this logic further, <code>y = arr[i]</code> and so the inside of the loop is simply</p>
<pre><code>s = s + arr[i];</code></pre>
<p>and hence it "optimizes" identically to the simple summation loop above.</p>
<p>This might seem like a minor tradeoff, but compensated arithmetic is often used to implement core math functions, such as trigonometric and exponential functions. Allowing the compiler to reassociate inside these can give <a href="https://github.com/JuliaLang/julia/issues/30073#issuecomment-439707503">catastrophically wrong answers</a>.</p>
<h2 id="flushing_subnormals_to_zero"><a href="#flushing_subnormals_to_zero">Flushing subnormals to zero</a></h2>
<p>This one is the most subtle, but by far the most insidious, as it can affect code compiled <em>without</em> fast-math, and is only cryptically documented under <code>-funsafe-math-optimizations</code>:</p>
<blockquote>
<p>When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.</p>
</blockquote>
<p>So what does that mean? Well this is referring to one of those slightly annoying edge cases of floating point numbers, <em>subnormals</em> (sometimes called <em>denormals</em>). <a href="https://en.wikipedia.org/wiki/Subnormal_number">Wikipedia gives a decent overview</a>, but for our purposes the main thing you need to know is (a) they're <em>very</em> close to zero, and (b) when encountered, they can incur a significant performance penalty on many processors<sup id="fnref:6"><a href="#fndef:6">[4]</a></sup>.</p>
<p>A simple solution to this problem is "flush to zero" (FTZ): that is, if a result would return a subnormal value, return zero instead. This is actually fine for a lot of use cases, and this setting is commonly used in audio and graphics applications. But there are plenty of use cases where it isn't fine: FTZ breaks some important floating point error analysis results, such as <a href="https://en.wikipedia.org/wiki/Sterbenz_lemma">Sterbenz' Lemma</a>, and so unexpected results (such as iterative algorithms failing to converge) may occur.</p>
<p>The problem is how FTZ actually implemented on most hardware: it is not set per-instruction, but instead <a href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/floating-point-operations/understanding-floating-point-operations/setting-the-ftz-and-daz-flags.html">controlled by the floating point environment</a>: more specifically, it is controlled by the floating point control register, which on most systems is set at the thread level: enabling FTZ will affect all other operations in the same thread.</p>
<p>GCC with <code>-funsafe-math-optimizations</code> enables FTZ (and its close relation, denormals-are-zero, or DAZ), even when building shared libraries. That means simply loading a shared library can change the results in completely unrelated code, which is <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/issues/253#issuecomment-573589022">a fun debugging experience</a>.</p>
<h2 id="what_can_programmers_do"><a href="#what_can_programmers_do">What can programmers do?</a></h2>
<p>I've joked on Twitter that "friends don't let friends use fast-math", but with the luxury of a longer format, I will concede that it has valid use cases, and can actually give valuable performance improvements; as SIMD lanes get wider and instructions get fancier, the value of these optimizations will only increase. At the very least, it can provide a useful reference for what performance is left on the table. So when and how can it be safely used?</p>
<p>One reason is if you don't care about the accuracy of the results: I come from a scientific computing background where the primary output of a program is a bunch of numbers. But floating point arithmetic is used in many domains where that is not the case, such as audio, graphics, games, and machine learning. I'm not particularly familiar with requirements in these domains, but there is an interesting rant by <a href="https://gcc.gnu.org/legacy-ml/gcc/2001-07/msg02150.html">Linus Torvalds from 20 years ago</a>, arguing that overly strict floating point semantics are of little importance outside scientific domains. Nevertheless, <a href="https://twitter.com/supahvee1234/status/1382907921848221698">some anecdotes</a> suggest fast-math can cause problems, so it is probably still useful understand what it does and why. If you work in these areas, I would love to hear about your experiences, especially if you identified which of these optimizations had a positive or negative impact.</p>
<blockquote>
<p>I hold that in general it’s simply intractable to “defensively” code against the transformations that <code>-ffast-math</code> may or may not perform. If a sufficiently advanced compiler is indistinguishable from an adversary, then giving the compiler access to <code>-ffast-math</code> is gifting that enemy nukes. That doesn’t mean you can’t use it! You just have to test enough to gain confidence that no bombs go off with your compiler on your system.</p>
</blockquote>
<p>— <a href="https://discourse.julialang.org/t/when-if-a-b-x-1-a-b-divides-by-zero/7154/5?u=simonbyrne">Matt Bauman</a></p>
<p>If you do care about the accuracy of the results, then you need to approach fast-math much more carefully and warily. A common approach is to enable fast-math everywhere, observe erroneous results, and then attempt to isolate and fix the cause as one would usually approach a bug. Unfortunately this task is not so simple: you can't insert branches to check for NaNs or Infs (the compiler will just remove them), you can't rely on a debugger because <a href="https://gitlab.com/libeigen/eigen/-/issues/1674#note_709679831">the bug may disappear in debug builds</a>, and it can even <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1127544">break printing</a>.</p>
<p>So you have to approach fast-math much more carefully. A typical process might be:</p>
<ol>
<li><p>Develop reliable validation tests</p>

</li><li><p>Develop useful benchmarks</p>

</li><li><p>Enable fast-math and compare benchmark results</p>

</li><li><p>Selectively enable/disable fast-math optimizations<sup id="fnref:5"><a href="#fndef:5">[5]</a></sup> to identify</p>
<p>a. which optimizations have a performance impact,</p>
<p>b. which cause problems, and</p>
<p>c. where in the code those changes arise.</p>

</li><li><p>Validate the final numeric results</p>

</li></ol>
<p>The aim of this process should be to use the absolute minimum number of fast-math options, in the minimum number of places, while testing to ensure that the places where the optimizations are used remain correct.</p>
<p>Alternatively, you can look into other approaches to achieve the same performance benefits: in some cases it is possible to rewrite the code to achieve the same results: for example, it is not uncommon to see expressions like <code>x * (1/y)</code> in many scientific codebases.</p>
<p>For SIMD operations, tools such as <a href="https://www.openmp.org/spec-html/5.0/openmpsu42.html">OpenMP</a> or <a href="https://ispc.github.io/">ISPC</a> provide constructions to write code that is amenable to automatic SIMD optimizations. Julia provides the <a href="https://docs.julialang.org/en/v1/base/base/#Base.SimdLoop.@simd"><code>@simd</code> macro</a>, though this also has some important caveats on its use. At the more extreme end, you can use <a href="https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/">SIMD intrinsics</a>: these are commonly used in libraries, often with the help of code generation (<a href="http://fftw.org/">FFTW</a> uses this appraoch), but requires considerably more effort and expertise, and can be difficult to port to new platforms.</p>
<p>Finally, if you're writing an open source library, please don't <a href="https://github.com/tesseract-ocr/tesseract/blob/5884036ecdb2807419cbd21b7ca44b630f547d80/Makefile.am#L140">hardcode fast-math into your Makefile</a>.</p>
<h2 id="what_can_language_and_compilers_developers_do"><a href="#what_can_language_and_compilers_developers_do">What can language and compilers developers do?</a></h2>
<p>I think the widespread use of fast-math should be considered a fundamental design failure: by failing to provide programmers with features they need to make the best use of modern hardware, programmers instead resort to enabling an option that is known to be blatantly unsafe.</p>
<p>Firstly, GCC should address the FTZ library issue: the bug has been <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522">open for 9 years, but is still marked NEW</a>. At the very least, this behavior should be more clearly documented, and have a specific option to disable it.</p>
<p>Beyond that, there are 2 primary approaches: educate users, and provide finer control over the optimizations.</p>
<p>The easiest way to educate users is to give it a better name. Rather than "fast-math", something like "unsafe-math". Documentation could also be improved to educate users on the consequences of these choices (consider this post to be my contribution to toward that goal). Linters and compiler warnings could, for example, warn users that their <code>isnan</code> checks are now useless, or even just highlight which regions of code have been impacted by the optimizations.</p>
<p>Secondly, languages and compilers need to provide better tools to get the job done. Ideally these behaviors shouldn't be enabled or disabled via a compiler flag, which is a very blunt tool, but specified locally in the code itself, for example</p>
<ul>
<li><p>Both GCC and Clang let you <a href="https://stackoverflow.com/a/40702790/392585">enable/disable optimizations on a per-function basis</a>: these should be standardized to work with all compilers.</p>

</li><li><p>There should be options for even finer control, such as a pragma or macro so that users can assert that "under no circumstances should this <code>isnan</code> check be removed/this arithmetic expression be reassociated".</p>

</li><li><p>Conversely, a mechanism to flag certain addition or subtraction operations which the compiler is allowed to reassociate (or contract into a fused-multiply-add operation) regardless of compiler flags.<sup id="fnref:3"><a href="#fndef:3">[6]</a></sup></p>

</li></ul>
<p>This still leaves open the exact question of what the semantics should be: if you combine a regular <code>+</code> and a fast-math <code>+</code>, can they reassociate? What should the scoping rules be, and how should it interact with things like inter-procedural optimization? These are hard yet very important questions, but they need to be answered for programmers to be able to make use of these features safely.</p>
<p>For more discussion, see <a href="https://news.ycombinator.com/item?id=29201473">HN</a>.</p>
<h2 id="updates"><a href="#updates">Updates</a></h2>
<p>A few updates since I wrote this note:</p>
<ul>
<li><p>Brendan Dolan-Gavitt wrote a fantastic piece about <a href="https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html">FTZ-enabling libraries in Python packages</a>: it also has some nice tips on how to find out if your library was compiled with fast-math.</p>
<ul>
<li><p>He also has a nice proof-of-concept <a href="https://github.com/moyix/2_ffast_2_furious">buffer overflow vulnerability</a>.</p>

</li></ul>

</li><li><p>It turns out Clang also enables FTZ when building shared libraries with fast-math: but only if you have a system GCC installation. I've <a href="https://github.com/llvm/llvm-project/issues/57589">opened an issue</a>.</p>

</li><li><p>MSVC doesn't remove <code>isnan</code> checks, but instead <a href="https://twitter.com/dotstdy/status/1567748577962741760">generates what looks like worse code</a> when compiling with fast-math.</p>

</li><li><p>The FTZ library issue will be <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522#c45">fixed in GCC 13</a>!</p>

</li></ul>

 
 
 
 <table id="fndef:5">
    <tbody><tr>
        <td><a href="#fnref:5">[5]</a>
        </td><td>As mentioned above, <code>-fno-finite-math-only</code> should be the first thing you try.
    
</td></tr></tbody></table>
 <table id="fndef:3">
    <tbody><tr>
        <td><a href="#fnref:3">[6]</a>
        </td><td>Rust provides something like this via <a href="https://stackoverflow.com/a/40707111/392585">experimental intrinsics</a>, though I'm not 100% clear on what optimzations are allowed.
    
</td></tr></tbody></table>

<div>
  <p>
    © Simon Byrne. Last modified: April 06, 2024.
  </p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gradients Are the New Intervals (117 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2025-05-14-gradients/</link>
            <guid>44142266</guid>
            <pubDate>Sat, 31 May 2025 06:25:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2025-05-14-gradients/">https://www.mattkeeter.com/blog/2025-05-14-gradients/</a>, See on <a href="https://news.ycombinator.com/item?id=44142266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->









<p>At the <a href="https://nesg.graphics/">New England Symposium on Graphics</a>,
<a href="https://jamestompkin.com/">James Tompkin</a> compared graphics researchers to
magpies: they're easily distracted by shiny objects and pretty renderings.</p>
<p>While this is true, the analogy also holds from a different angle: when I'm
reading graphics papers, I'm constantly looking for ideas to <del>steal</del>
bring back to my nest.</p>
<p>Researchers at <a href="https://www.irit.fr/en/home/">IRIT</a> and
<a href="https://research.adobe.com/">Adobe Research</a> recently published a
<a href="https://research.adobe.com/publication/lipschitz-pruning-hierarchical-simplification-of-primitive-based-sdfs/">paper</a>
that's <em>full</em> of interesting ideas, and I'd like to talk about it.</p>
<p>
<a href="https://wbrbr.org/publications/LipschitzPruning/"><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/paper.png" alt="Picture of the paper, showing the abstract and title"></a>
</p>
<p>This blog post assumes a vague understanding of implicit surface rasterization,
and how interval arithmetic is used to both skip regions of space and simplify
complex expressions.
See <a href="https://www.mattkeeter.com/projects/fidget#intervals">this section of the Fidget writeup</a> for a short
introduction, or
<a href="https://www.youtube.com/watch?v=UxGxsGnbyJ4">my colloquium talk</a>
for a longer explanation.</p>
<p>Here's the key sentence from the paper's abstract:</p>
<blockquote>
<p>We introduce an efficient hierarchical tree pruning method based on the
Lipschitz property of SDFs, which is compatible with hard and smooth CSG
operators.</p>
</blockquote>
<p>In this case, "the Lipschitz property" means that the gradient of the distance
value is bounded, e.g. $||\nabla f(x, y, z)|| \lt 1$.  You'll also see it
referred to as
<a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a>.</p>
<p>Lipschitz continuity provides a <strong>bound</strong> on the maximum change in the distance
value between two points: if you have points $\vec{p}$ and $\vec{q}$ in 3D
space, then</p>
<p>$$|f(\vec{p}) - f(\vec{q})| \le ||\vec{p} - \vec{q}||$$</p>
<p>Given this requirement on the distance fields, the authors present two core
tricks that can be applied to CSG-tree-flavored models:</p>
<ul>
<li><strong>Pruning</strong> identifies primitives which are inactive within a particular
region of space, and builds a simplified shape with only active primitives</li>
<li><strong>Far-field culling</strong> finds primitives which are far from a particular
region (and therefore not relevant), and replaces them with a simpler
expression.</li>
</ul>
<p>These optimizations make rendering dramatically cheaper, allowing for
interactive visualizations of complex models.</p>
<p>The first optimization also sounds familiar – I presented something
similar <a href="https://www.mattkeeter.com/research/mpr/">back in 2020</a> (which
itself builds on <a href="https://fab.cba.mit.edu/classes/S62.12/docs/Duff_interval_CSG.pdf">work from 1992</a>):</p>
<blockquote>
<p>To render a model, its underlying expression is evaluated in a shallow
hierarchy of spatial regions, using a high branching factor for efficient
parallelization. Interval arithmetic is used to both skip empty regions and
construct reduced versions of the expression. The latter is the optimization
that makes our algorithm practical: in one benchmark, expression complexity
decreases by two orders of magnitude between the original and reduced
expressions.</p>
</blockquote>
<p>Here's the big difference: my implementation used
<a href="https://en.wikipedia.org/wiki/Interval_arithmetic">interval arithmetic</a>
on arbitrary math expressions, while this new paper uses single-point evaluation
on Lipschitz-continuous primitives.</p>
<p>Single-point evaluation is cheaper, and also doesn't suffer from the
conservative nature of interval arithmetic (where intervals tend to grow over
time).  However, their shapes are limited to a set of well-behaved primitives
and transforms; something as simple as scaling a model can break Lipschitz
continuity if not handled carefully.</p>
<p>After thinking about it for a few days, I ended up taking home a slightly
different conclusion than the paper presented:</p>
<p><em>If your distance field is Lipschitz-continuous, then you can use single-point
evaluation to get interval-ish results, then apply all the usual tricks which
normally require interval arithmetic.</em></p>
<p>I've now gone far too long without showing a pretty picture (the magpies in the
audience are growing rowdy), so let's dig into some examples.  I'm going to
take this idea — of using single-point evaluations to get pseudo-intervals — and
see how it applies to my usual box of tools.</p>
<p>Note that I'm not hewing to the implementation from the paper (which you should
go read, it's great!); I'm taking this idea and running in a slightly different
direction based on my background knowledge.</p>
<hr>
<h3>Basic rendering</h3>
<p>Here's a circle of radius 1, $f(x, y) = \sqrt{x^2 + y^2} - 1$:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle.png" alt="Picture of a circle with field lines shown"></p>
<p>It's rendered with orange / blue indicating whether we're inside or outside the
shape, and field lines showing the distance field values.  I'm not sure who
developed the original coloring strategy, but it's pervasive on Shadertoy; I
borrowed it from <a href="https://www.shadertoy.com/view/t3X3z4">Inigo Quilez</a>.</p>
<p>My typical strategy for rendering is to do several rounds of evaluation using
interval arithmetic, then pixel-by-pixel evaluation of the remaining ambiguous
regions.  The goal is to produce a binary image: each pixel is either inside or
outside, depending on sign.</p>
<p>Interval evaluation takes intervals for $x$ and $y$, and returns an interval
output.<br>
For example, we can evaluate a region in the top-left quadrant:</p>
<p>$$f([0.5, 1.5], [0.5, 1.5]) = [-0.3, 1.12]$$</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_rect.png" alt="Picture of a circle with field lines and a rectangle region"></p>
<p>Because the interval result is ambiguous (contains 0), we can't prove this
region inside or outside the shape, so we subdivide and recurse.  If it was
unambiguously less than or greater than zero, we could stop processing right
away!</p>
<p>The full algorithm produces a set of colored regions (proven inside / outside),
and pixel-level results for ambiguous regions below a certain size:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_interval.png" alt="Circle with interval regions shown"></p>
<p>Intervals are shown as large uniformly-shaded squares; pixel-by-pixel evaluation
is concentrated at the edge of the circle.</p>
<h3>Using gradients instead</h3>
<p>Our circle model has a gradient of 1 everywhere (proof is left as an exercise to
the reader).
If we want to evaluate an interval region
$[x_\text{min}, x_\text{max}], [y_\text{min}, y_\text{max}]$,
we can sample a single point in the center:</p>
<p>$$v = f\left((x_\text{min} + x_\text{max}) / 2, (y_\text{min} + y_\text{max}) / 2\right)$$</p>
<p>From this point, the maximum distance to a corner is given by
$$d = \frac{\sqrt{(x_\text{max} - x_\text{min})^2 + (y_\text{max} - y_\text{min})^2}}{2}$$</p>
<p>Because our gradient is bounded, we know that the value anywhere in the region
must be in the range $[v - d, v + d]$.</p>
<p>In other words, we can evaluate a single point in the center of the region,
then construct a <em>pseudo-interval</em> result by adding the distance to a corner.</p>
<p>Visually, this is like drawing a circle over the rectangular region:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_region.png" alt="Region with a circle around it"></p>
<p>The red box is our target region (i.e. intervals in X and Y); the green elements
shows our center point, radius, and effective circle.</p>
<p>This is a drop-in replacement for our previous interval evaluator; we'll call it
a <strong>"pseudo-interval evaluator"</strong>.  We can run the exact same algorithm to
produce an image, swapping in this new evaluator.  For the original circle, it
produces the same picture:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/circle_pseudo.png" alt="Pseudo-interval evaluation of the circle SDF"></p>
<p>By carefully positioning the circle, we can see a case where pseudo-interval
evaluation requires more subdivision (interval evaluation on the left,
pseudo-interval evaluation on the right):</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/comparison.png" alt="Comparison"></p>
<p>Because the pseudo-interval considers the green circle (rather than the red
square of the interval evaluator), it can't prove this entire region outside of
the shape, and therefore has to subdivide it.</p>
<h3>Downsides to interval evaluation</h3>
<p>There are also cases where the pseudo-interval evaluator has the upper hand!</p>
<p>Consider rotating a model by 45°: this is equivalent to a new evaluation</p>
<p>$$ f_\text{rot}(x, y) = f\left(\frac{x + y}{\sqrt{2}}, \frac{y - x}{\sqrt{2}}\right) $$</p>
<p>The "natural" axes of the model are now at 45° angles from our image axes.
Unfortunately, our intervals are in terms of the image's $x$ and $y$
coordinates, which remain horizontal and vertical.   During evaluation, this
transform enlarges the sampled region by a factor of $\sqrt{2}$:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/rectangle_rot.png" alt="Rotated rectangle"></p>
<p>You can now imagine situations where pseudo-interval evaluation is more precise,
because its circle circumscribes the <em>original</em> region:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/rectangle_rot_circle.png" alt="Rotated rectangle with a circle"></p>
<p>Sure enough, there are cases where interval arithmetic recurses down to
individual pixels, while pseudo-interval evaluation succeeds in proving
regions entirely inside the shape:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/rot_compared.png" alt="Comparison of rotated renderings"></p>
<p>More generally, interval evalution gets worse and worse as you stack up more
transforms.  One way to think about it is that interval arithmetic "forgets" any
correlations between its inputs.  For example, if you evaluate $x^2$ as $x
\times x$ (instead of with a dedicated power operation), interval arithmetic
produces a much wider result:</p>
<p>$$[-1, 1]^2 = [0, 1]$$
$$[-1, 1] \times [-1, 1] = [-1, 1]$$</p>
<p>We know that the value must be the same on each side of the multiplication, so
it's impossible to get -1 as a result; however, the implementation of
multiplication assumes that the two sides are totally uncorrelated.</p>
<p>You can work around this failure mode (to some extent) by automatically
<a href="https://www.mattkeeter.com/blog/2016-03-20-affine/">detecting and collapsing affine transforms</a>, but
you certainly don't get it for free.</p>
<p>Using point samples on a Lipschitz-continuous function means that this problem
disappears: we know that the stack of transforms hasn't changed the bounds on
the gradient, so any transforms of the model don't affect sampling!</p>
<h3>Normalization</h3>
<p>Here's a more complex example, built from about 400 math expressions:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello.png" alt="SDF reading 'hello world'"></p>
<p>Looking at the field lines, it's clear that this is a less well-behaved distance
field!  Specifically, the gradient at the <code>w</code> is very low; the field lines are
<em>extremely</em> stretched out.</p>
<p>Interval evaluation shows the effects of the badly-behaved field: there's a
bunch of regions around the <code>w</code> where the renderer recurses farther than
necessary.</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_interval.png" alt="Interval evaluation of the above SDF"></p>
<p>Having a badly-behaved field means that we can't assume Lipschitz continuity.
In this model, most of the field has a magnitude $\ge 1$:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/grad_mag.png" alt="Gradient magnitude"></p>
<p>Our evaluation is using <a href="https://www.mattkeeter.com/projects/fidget">Fidget</a>, which provides a
forward-mode automatic differentiation evaluator.  This suggests a dumb
potential solution: find the <code>(value, dx, dy, dz)</code> tuple using automatic
differentiation, then normalize by dividing the value by the magnitude of the
partial derivatives:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_norm_bad.png" alt="Normalized distance field (which is subtly wrong)"></p>
<p>The gradient looks <em>vaguely</em> correct – field lines advance at the same rate
everywhere – but there's a subtle issue here.  Looking above the <code>e</code>, you can
spot a dramatic shift in brightness, indicating that the distance field believes
it's suddenly much further away from the model.  This clearly won't work: a
sample taken above the <code>e</code> could falsely report that it's very far from the
model.</p>
<p>So, how did this go wrong?</p>
<p>Above the <code>e</code>, two fields are combined with <code>min</code>.  We'll call these two fields
the $e$ and $w$ fields, based on the letter than generated the field.  At the
point where the fields are exactly equal, they have the same value but
<strong>different gradient magnitudes</strong>.</p>
<p>$$
\begin{aligned}
f(x, y) &amp;= \min(e(x, y), w(e, y)) \\
e(x, y) &amp;= 0.1,\hspace{0.1in}||\nabla e(x, y)|| = 1.2 \\
w(x, y) &amp;= 0.1,\hspace{0.1in}||\nabla w(x, y)|| = 0.5 \\
\end{aligned}
$$</p>
<p>Now consider the modified $f_\text{norm}(x, y) = f(x, y) / ||\nabla f(x, y)||$,
slightly above and below the $\min$ transition:</p>
<p>$$
\begin{aligned}
f_\text{norm}(x, y - \epsilon) &amp;= 0.1 / 1.2 = 0.083 \\
f_\text{norm}(x, y + \epsilon) &amp;= 0.1 / 0.5 = 0.2
\end{aligned}
$$</p>
<p>Normalization instroduces a discontinuity in the <strong>value</strong> here!  Even though
our gradient is guaranteed to be 1 everywhere (because of normalization), there
are instantaneous transitions in the value itself.  This means that the
Lipschitz property doesn't hold, so we can't use pseudo-interval evaluation.</p>
<h3>Normalization, once more</h3>
<p>What's to be done?</p>
<p>Non-rigorously, it seems like discontinuities in the gradient magnitude lead to
discontinuities in the value after normalization.  The only expresions which
introduce discontinuities in the gradient magnitude are <code>min</code> and <code>max</code>, which
suggests a simple solution: normalize <em>before</em> those operations.</p>
<p>Of course, this changes the values, but it doesn't change their sign.  We're
using <code>min</code> and <code>max</code> to perform <code>union</code> and <code>intersection</code> operations, so the
sign is the only thing that matters.</p>
<pre><code>min(a, b) =&gt;     union(a, b) = min(normalize(a), normalize(b))
max(a, b) =&gt; intersect(a, b) = max(normalize(a), normalize(b))
</code></pre>
<p>Here's the new normalization:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_norm_good.png" alt="Normalized distance field (which is fine)"></p>
<p>Everything looks good: the field lines are evenly spaced, and there are no
discontinuities.</p>
<p>Plugging this into our psuedo-interval evaluator, we can get a perfectly
reasonable result.  It ends up evaluating slightly more tiles than the interval
evaluator, but still produces a correct image:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/hello_pseudo.png" alt="Hello, world"></p>
<h3>Expression simplification</h3>
<p>Interval evaluation has a second benefit: at each <code>min</code> and <code>max</code> node, we may
be able to prove that one branch is <em>always taken</em> within a particular spatial
region.  If this happens, then we can simplify the shape, generating a
shorter expression which is valid within that spatial region (and less expensive
to evaluate).</p>
<p>With intervals, we check that they are not overlapping: <code>min([0, 1], [2, 3])</code>
will always pick the left-hand (<code>[0, 1]</code>) argument.</p>
<p>A similar strategy works for Lipschitz-bounded point samples: within a region of
radius $r$, the value can only differ by $\pm r$ from a point sample in the
center.  The equivalent condition for selecting the left-hand argument of a
<code>min</code> operation is</p>
<p>$$ \min(a, b) \text{ where } a + r \lt b - r$$</p>
<p>Similar rules apply for the right-hand argument and <code>max</code> expressions.</p>
<p>I find it helpful to visualize how much the expression is simplified at each
tile.  For our <code>hello, world</code> model, here's a comparison of tape lengths at the
final tile, with the original tape being 390 clauses:</p>
<p><img src="https://www.mattkeeter.com/blog/2025-05-14-gradients/tape_lens.png" alt="Tape lengths in each tile"></p>
<p>It's definitely working in both cases!  There are a few tiles where they differ,
but there's not a clear winner; average tape lengths are 120.2 (for interval
evaluation) versus 121.5 (for pseudo-intervals).</p>
<h3>Performance impacts</h3>
<p>We've presented three different evaluation strategies, which can be used to both
prove entire regions inside / outside and provide data for expression
simplification.</p>
<p><strong>Interval arithmetic</strong> is conservative and works on any kind of distance field.
The evaluator has to do extra work for each arithmetic operation to track
intervals, compared to floating-point (value-only) evaluation.  If many
transforms are stacked up, it tends to provide results that are overly broad.</p>
<p>For Lipschitz-continuous distance fields, you can sample at the center of the
region, then add the radius to calculate a <strong>pseudo-interval</strong>.  This is a
single floating-point evaluation, which is cheaper than interval arithmetic.  In
addition, it doesn't have issues with stacked transforms.</p>
<p>Finally, for distance fields that are not Lipschitz-continuous, you can evaluate
them using forward-mode automatic differentiation, then <strong>normalize</strong> by
dividing by the magnitude of the gradient before <code>min</code> and <code>max</code> nodes, creating
a new Lipschitz-continuous field.  Like single-point sampling, this doesn't have
issues with stacked transforms; however, automatic differentiation is more
expensive than floating-point evaluation.  Normalization also produces a
different numerical result, so this strategy is only valid if you just care
about the sign.</p>
<p>It's pretty obvious that single-point sampling will beat interval arithmetic:
it's doing less work, and doesn't have issues with overly conservative
estimates.  (Indeed, the paper shows this in Figure 13)</p>
<p>However, it's not immediately obvious whether interval arithmetic or
<em>normalized</em> single-point samples (using forward-mode automatic differentiation)
would be more expensive!  Obviously, the answer is going to be
implementation-dependent, but I can provide one data point.</p>
<p><a href="https://www.mattkeeter.com/projects/prospero">The Prospero Challenge</a> presents a large expression (7668
clauses), and challenges readers to write an efficient renderer.  The underlying
expression is <em>not</em> Lipschitz-continuous, so we have to use the normalized
evaluator.</p>
<p>Testing these two evaluators on this model, I see the following results:</p>
<table>
    <tbody><tr><th></th><th>Interval</th><th>Normalized<br>pseudo-interval
    </th></tr><tr><td>512×512</td><td>12 ms</td><td>18 ms
    </td></tr><tr><td>1024×1024</td><td>20 ms</td><td>28 ms
    </td></tr><tr><td>1536×1536</td><td>29 ms</td><td>38 ms
    </td></tr><tr><td>2048×2048</td><td>37 ms</td><td>48 ms
</td></tr></tbody></table>
<p>It looks like interval arithmetic is slightly faster <em>in this case</em>, but they're
relatively close to each other – there's certainly no order-of-magnitude
differences here.</p>
<h3>Wrapping up</h3>
<p>This has been a fun exercise, and I'd encourage you to go read
<a href="https://wbrbr.org/publications/LipschitzPruning/"><em>Lipschitz Pruning: Hierarchical Simplification of Primitive-Based SDFs</em></a>
yourself and see their full system laid out on paper.</p>
<p>Suggested further reading:</p>
<ul>
<li>Blake Courter is building up a theory of
<a href="https://www.blakecourter.com/ugfs/">unit gradient fields</a>,
which would be amenable to these techniques (since by definition they have a
gradient of 1 everywhere)</li>
<li><a href="https://dl.acm.org/doi/10.1145/3386569.3392374"><em>Monte Carlo geometry processing</em></a>
and subsequent papers discuss using <a href="https://en.wikipedia.org/wiki/Walk-on-spheres_method">walk-on-spheres</a> to
solve geometry problems without discretization, using the same strategy of
point samples on a Lipschitz-continuous distance field</li>
</ul>
<p>I've published the code used to generate figures in this blog post at
<a href="https://github.com/mkeeter/gradients-and-intervals"><code>mkeeter/gradients-and-intervals</code></a>,
along with <a href="https://www.mattkeeter.com/blog/2025-05-14-gradients/run.py"><code>run.py</code></a> for post-processing.  This is Research Quality
Code™, so take it with a grain of salt!</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Responses May Include Mistakes (172 pts)]]></title>
            <link>https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</link>
            <guid>44142113</guid>
            <pubDate>Sat, 31 May 2025 05:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/">https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</a>, See on <a href="https://news.ycombinator.com/item?id=44142113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>The other day I wanted to look up a specific IBM PS/2 model, a circa 1992 PS/2 Server system. So I punched the model into Google, and got this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png"><img decoding="async" width="640" height="487" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-300x228.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-768x584.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png 1147w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That did not look quite right, since the machine I was looking for had 486 processors (yes, plural). And it most certainly <em>did</em> use Microchannel (MCA).</p>



<p>Alright, let’s try again:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png"><img decoding="async" width="640" height="557" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-300x261.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-768x669.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png 1174w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Simply re-running the identical query produces a different summary. Although the AI still claims that PS/2 Model 280 is an ISA-based 286 system. Maybe the third time is the charm?</p>



<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png"><img loading="lazy" decoding="async" width="640" height="465" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-300x218.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-768x557.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>The AI is really quite certain that PS/2 Model 280 was a 286-based system released in 1987, and I was really looking for a newer machine. Interestingly, the first time the AI claimed Model 280 had 1MB RAM expandable to 6MB, and now it supposedly only has 640 KB RAM. But the AI seems sure that Model 280 had a 1.44 MB drive and VGA graphics.</p>



<p>What if we try again? After a couple of attempts, yet different answer pops up:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png"><img loading="lazy" decoding="async" width="640" height="431" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-300x202.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-768x518.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png 1138w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Oh look, now the PS/2 Model 280 is a 286 expandable to 128 MB RAM. Amazing! Never mind that the 286 was architecturally limited to 16 MB.</p>



<p>Even better, the AI now tells us that “PS/2 Model 280 was a significant step forward in IBM’s personal computer line, and it helped to establish the PS/2 as a popular and reliable platform.”</p>



<p>The only problem with all that? <em>There is no PS/2 Model 280, and never was.</em> I simply had the model number wrong. The Google AI just “helpfully” hallucinates something that at first glance seems quite plausible, but is in fact utter nonsense.</p>



<p>But wait, that’s not the end of the story. If you try repeating the query often enough, you might get this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png"><img loading="lazy" decoding="async" width="640" height="409" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-300x192.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-768x491.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That answer is <em>actually correct</em>! “Model 280 was not a specific model in the PS/2 series”, and there was in fact an error in the query.</p>



<p>Here’s another example of a correct answer:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png"><img loading="lazy" decoding="async" width="640" height="415" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-300x194.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-768x498.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png 1174w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Unfortunately the correct answer comes up maybe 10% of the time when repeating the query, if at all. In the vast majority of attempts, the AI simply makes stuff up. I do not consider made up, hallucinated answers useful, in fact they are worse than useless. </p>



<p>This minor misadventure might provide a good window into AI-powered Internet search. To a non-expert, the made up answers will seem highly convincing, because there is a lot of detail and overall the answer does not look like junk.</p>



<p>An expert will immediately notice discrepancies in the hallucinated answers, and will follow for example the <a href="https://en.wikipedia.org/wiki/List_of_IBM_PS/2_models">List of IBM PS/2 Models</a> article on Wikipedia. Which will very quickly establish that there is no Model 280.</p>



<p>The (non-expert) users who would most benefit from an AI search summary will be the ones most likely misled by it.</p>



<p>How much would you value a research assistant who gives you a different answer every time you ask, and although sometimes the answer may be correct, the incorrect answers look, if anything, more “real” than the correct ones?</p>



<p>When Google says “AI responses may include mistakes”, do not take it lightly. The AI generated summary could be utter nonsense, and just because it sounds convincing doesn’t mean it has anything to do with reality. Caveat emptor!</p>
											</div><div><p>
							This entry was posted in <a href="https://www.os2museum.com/wp/category/ibm/" rel="category tag">IBM</a>, <a href="https://www.os2museum.com/wp/category/ps2/" rel="category tag">PS/2</a>. Bookmark the <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/" title="Permalink to AI Responses May Include Mistakes" rel="bookmark">permalink</a>.													</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simpler Backoff (111 pts)]]></title>
            <link>https://commaok.xyz/post/simple-backoff/</link>
            <guid>44141887</guid>
            <pubDate>Sat, 31 May 2025 04:43:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://commaok.xyz/post/simple-backoff/">https://commaok.xyz/post/simple-backoff/</a>, See on <a href="https://news.ycombinator.com/item?id=44141887">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
                <header>
                    
                    <h2>
                    May 30, 2025 
                    <br>
                    
                    </h2>
                </header>
                <section id="post-body">
                    <p>Exponential backoff with jitter is de rigeur for making service calls. This code, or something like it, probably looks really familiar:</p>
<pre tabindex="0"><code>func do(ctx context.Context) error {
	const (
		maxAttempts = 10
		baseDelay   = 1 * time.Second
		maxDelay    = 60 * time.Second
	)

	delay := baseDelay
	for attempt := range maxAttempts {
		err := request(ctx)
		if err == nil {
			return nil
		}

		delay *= 2
		delay = min(delay, maxDelay)

		jitter := multiplyDuration(delay, rand.Float64()*0.5-0.25) // ±25%
		sleepTime := delay + jitter

		select {
		case &lt;-ctx.Done():
			return ctx.Err()
		case &lt;-time.After(sleepTime):
		}
	}

	return fmt.Errorf("failed after %d attempts", maxAttempts)
}

func multiplyDuration(d time.Duration, mul float64) time.Duration {
	return time.Duration(float64(d) * mul)
}
</code></pre><p>But we can make this much nicer with a simple <a href="https://commaok.xyz/post/lookup_tables/">lookup table</a>.</p>
<pre tabindex="0"><code>func do(ctx context.Context) error {
	delays := []time.Duration{
		1 * time.Second, 2 * time.Second,
		4 * time.Second, 8 * time.Second,
		16 * time.Second, 32 * time.Second,
		60 * time.Second, 60 * time.Second,
		60 * time.Second, 60 * time.Second,
	}

	for _, delay := range delays {
		err := request(ctx)
		if err == nil {
			return nil
		}

		delay = multiplyDuration(delay, 0.75 + rand.Float64()*0.5) // ±25%
		select {
		case &lt;-ctx.Done():
			return ctx.Err()
		case &lt;-time.After(delay):
		}
	}

	return fmt.Errorf("failed after %d attempts", len(delays))
}
</code></pre><p>This is much simpler. There are fewer variables, with smaller scope. There’s no need to reasoning about behavior across loops, and if there’s a bug in the calculations, it won’t affect subsequent iterations.</p>
<p>It is more readable. It is obvious how it will behave. It is also more editable. Changing the backoff schedule and number of attempts now feels safe and trivial.</p>
<p>Don’t write code that generates a small, fixed set of values. Use a lookup table instead.</p>

                </section>
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Every 5x5 Nonogram (104 pts)]]></title>
            <link>https://pixelogic.app/every-5x5-nonogram</link>
            <guid>44140918</guid>
            <pubDate>Sat, 31 May 2025 00:14:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pixelogic.app/every-5x5-nonogram">https://pixelogic.app/every-5x5-nonogram</a>, See on <a href="https://news.ycombinator.com/item?id=44140918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      <b>Every 5x5 Nonogram</b> is a realtime, collaborative web game by <a href="https://bsky.app/profile/joelriley.com" target="_blank">Joel</a>, the creator of <a href="https://pixelogic.app/">Pixelogic</a>.
    </p>
    <p>
      Can you help solve all <b>24,976,511</b> possible 5x5 nonogram puzzles? Every puzzle has a unique solution and requires no guessing.
    </p>
    <p>
      Solving a puzzle solves it for everyone else!
    </p>
    <p>
      <a href="https://pixelogic.app/every-5x5-nonogram-help">How to play</a>
    </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valkey Turns One: Community fork of Redis (235 pts)]]></title>
            <link>https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</link>
            <guid>44140379</guid>
            <pubDate>Fri, 30 May 2025 22:24:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/">https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</a>, See on <a href="https://news.ycombinator.com/item?id=44140379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  
  <article>
    <div>
        
<p>A year ago, Redis Inc (formerly Garantia Data) made a controversial move that disrupted the open source ecosystem: it closed the source of Redis.&nbsp;<a href="https://www.gomomento.com/blog/rip-redis-how-garantia-data-pulled-off-the-biggest-heist-in-open-source-history/" target="_blank" rel="noreferrer noopener">As I wrote at the time</a>, it was a trust-breaking decision that could have shattered the community.</p>



<p>But instead of splintering, the community responded with purpose. Out of that disruption came&nbsp;<a href="https://valkey.io/" target="_blank" rel="noreferrer noopener">Valkey</a>,&nbsp;a fork that took a shot at keeping the community alive.</p>



<h2>A Return, A Reversal</h2>



<p>As part of efforts to rebuild trust with the community, Redis Inc&nbsp;<a href="https://redis.io/blog/welcome-back-to-redis-antirez/" target="_blank" rel="noreferrer noopener">brought back</a>&nbsp;Salvatore Sanfilippo (aka Antirez), the original creator of Redis. I am genuinely excited about his return because it is already impactful. He’s following through on his promise of contributing new features and performance optimizations to Redis. More profoundly,&nbsp;<a href="https://redis.io/blog/agplv3/" target="_blank" rel="noreferrer noopener">Redis 8.0 has been open-sourced again</a>.</p>



<p>Redis acknowledged that adopting <a href="https://en.wikipedia.org/wiki/Server_Side_Public_License" target="_blank" rel="noreferrer noopener">SSPL</a> strained their bond with the community, questioning contributions from others in the same breath.</p>



<blockquote>
<figure><blockquote><p>How do you keep innovating and investing in OSS projects when cloud providers reap the profits and control the infrastructure without proportional contributions back to the projects that they exploit?</p></blockquote></figure>
</blockquote>



<p>The disheartening move from Redis Inc catalyzed an unprecedented wave of collaboration and contributions. Valkey became the test of the community resolve to keep itself together. One year later, Valkey hasn’t just survived –&nbsp;<a href="https://www.linkedin.com/posts/kshams_valkey-rocks-the-most-remarkable-thing-about-activity-7318683448506793985-_qJE" target="_blank" rel="noreferrer noopener">it’s thriving</a>! The Async I/O Threading model <a href="https://github.com/valkey-io/valkey/pull/758" target="_blank" rel="noreferrer noopener">contribution</a> from AWS unlocked 3x+ throughput by fundamentally changing how I/O threads work inside Redis.</p>



<p>But how do these and other contributions compare to Redis 8? Can we hit 1M RPS out of an 8 VCPU instance (c8g.2xl) on either Valkey 8.1 or Redis 8.0 (with 1KB items, 3M items in the key space, and ~500 connections)? It’s time for a bake off!</p>



<h2>Valkey 8.1 vs Redis 8.0: Can the Fork Outrun the Source?</h2>



<p>The punchline: we could not sustain 1M RPS on an 8 VCPU instance with either Valkey or Redis 8.0, but we got really close!!</p>



<p>On a full-tuned c8g.2xl (8 VCPU), Valkey 8.1.1 pushed to 999.8K RPS on SETs with .8ms p99 latency. Redis 8.0 got as high as 729.4 RPS on SETs with .99ms p99 latencies. On each iteration, we tested 50M SETs followed by 50M GETs. We varied connection counts to optimize the maximum throughput for each system.</p>



<figure><blockquote><p><mark>Valkey achieved higher throughput and lower latency across both reads and writes (37% higher on SET and 16% higher on GET), alongside 30% faster p99 latencies for SET and 60%+ faster on GET.</mark></p></blockquote></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png"></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="341" src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png" alt="Valkey vs Redis table with SET and GET command" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20341'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png"></figure>



<h2>Threading the multi-threading needle</h2>



<p>If I had a penny for every time heard, “but Redis /Valkey is single threaded….”</p>



<p>Antirez’s emphasis on a shared nothing architecture has been foundational for Redis. Nevertheless, as early as 2020, Redis added support for I/O threads. Unfortunately, they did not offer drastic improvement until recently. If you have previously tried and discarded I/O threads, it is time to evaluate again!</p>



<p>On Valkey, we see SET throughput going from 239K RPS without I/O threads to 678K with 6 threads. Meanwhile, p99 latencies dropped from 1.68ms to 0.93ms <strong>despite doing nearly 3x the throughput</strong>! Similarly, Redis went from 235K RPS without I/O threads to 563K RPS with 6 I/O threads. P99s for Redis also dropped around 40% from 1.35ms to 0.84ms.</p>



<p>Two key takeaways emerged:</p>



<ol>
<li>With two threads, gains were modest (~20%). The impact only really surfaced at three threads and beyond.</li>



<li>Redis and Valkey were neck-and-neck until the fourth thread. After that, Valkey pulled away sharply.</li>
</ol>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png"></figure>



<h3>SET Performance on Valkey with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png"></figure>



<h3>SET Performance on Redis with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png"></figure>



<h2>Pushing Valkey Throughput Further</h2>



<p>In the previous section, we saw that Valkey could hit 678K RPS on SETs with 6 threads and 256 connections. If we up the connections to 400, the throughput goes up to 832K RPS. How did we get the additional 167K RPS?</p>



<p>We used <a href="https://github.com/iopsystems/rezolus" target="_blank" rel="noreferrer noopener">Rezolus</a>, our favorite Linux performance telemetry agent, to get deep insights into the system under stress. You can see in the charts below that overall CPU utilization is around 80% and unevenly distributed across the 8 cores.</p>



<p>Diving deeper, this is driven by hardware interrupts from network queues across all 8 cores. Interrupts are bad because they disrupt a hard working Valkey thread to yield to handle network packets.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="835" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png" alt="CPU Usage chart" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20835'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png"></figure>



<p>What if we could avoid the context switching on our <code><sup>c8g.2xl</sup></code> with 8 cores? Running close to a million RPS requires considerable packet processing horsepower. Luckily, since a lot of work happens at the Nitro level on EC2 instances, two allocated cores to IRQs is all you need (if you let them focus). Pinning the IRQs to two cores is pretty straightforward.</p>



<pre><code>sudo ethtool -L ens34 combined 2 # reduce to 2 IRQs
grep ens34 /proc/interrupts # ours were on 99 and 100
echo 1 | sudo tee /proc/irq/99/smp_affinity # pin 99 to core 1
echo 2 | sudo tee /proc/irq/100/smp_affinity # pin 100 to core 2</code></pre>



<p>But how do we let these threads focus? and how do we avoid Redis / Valkey threads contending for the same cores? We pin Redis/Valkey to cores 3-8, giving their IO-Threads better isolation while also allowing the IRQs to focus. We used the <code><sup>--cpuset-cpus</sup></code> Docker flag to set these CPU assignments, making sure that Redis and Valkey process stayed pinned to the intended cores throughout the test. This reduces cross-core contention and improves cache locality, <strong>both of which are critical for minimizing tail latencies at high throughput</strong>. Ideal core allocation can vary in multi-tenant environments or mixed workloads, but in this benchmark it provided clean isolation between system and application workloads.</p>



<p><strong>Redis:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" redis:8.0 \
  --save "" --appendonly no \
  --io-threads 6  \
  --protected-mode no --maxmemory 10gb</code></pre>



<p><strong>Valkey:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" valkey/valkey:8.1.1 \
  --save "" --appendonly no --io-threads 6 \
  --protected-mode no --maxmemory 10gb</code></pre>



<p>Let’s see what Rezolus has to say about the new setup with IRQs pinned to the first 2 cores and Valkey pinned to the remaining 6 cores. First, we observed meaningfully higher CPU Utilization. Second, looking at the bottom chart (SoftIRQ), we see that it is now limited to only the first two cores. Third, the Valkey cores are running red hot, whereas we previously saw a much more scattered distribution one usage across cores. <strong>While this setup is ideal for this benchmark, optimal IRQ tuning depends heavily on NIC architecture and the concurrency model of your application.</strong></p>



<figure><img loading="lazy" decoding="async" width="720" height="603" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png" alt="CPU Chart 2" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" sizes="(max-width: 720px) 100vw, 720px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20720%20603'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png"></figure>



<p>The extra 20% CPU utilization is what buys us the extra 167K RPS (from 832K RPS to 999.8K RPS).</p>



<h2>Try it Yourself (And Know Before You Go)</h2>



<p>These benchmarks are hopefully a beginning and not the end. Our hope is that this sets up both Valkey and Redis communities to continue the performance improvement journey. We also recognize that many folks may want to reproduce the benchmark in their own environments, incorporating their workflow specifics. Below, we outline some key instructions that you can use to reproduce this in your own AWS account within an hour.</p>



<p><strong>Instance Types:</strong>&nbsp;We used AWS Graviton4-based&nbsp;<code><sup>c8g</sup></code>&nbsp;instances, launched in September 2024. The&nbsp;<sup><code>c8g.2xlarge</code></sup> server node provides 8 vCPUs (costing roughly $250/month in us-east-1), while the&nbsp;<code><sup>c8g.8xlarge</sup></code>&nbsp;load generator offers 32 vCPUs. This provided enough CPU headroom to cleanly isolate the benchmark workload, IRQ handling, and Redis/Valkey processing. The same c8g.2xl instance was used to run valkey and redis (one at a time). The same load gen node was run each time. Valkey and Redis were restarted right before each test to ensure fairness.</p>



<p><strong>Placement Groups:</strong>&nbsp;We used EC2 placement groups (cluster mode) to ensure minimal network jitter and low-latency communication between the client and server nodes. Placement groups offer extremely tight latencies by reducing the number of hops between your EC2 instances. This has the upside on higher throughput, fewer interruptions, and lower latencies – but it has some shared fate / blast radius implications that are worth considering before deploying them in your production environment.</p>



<p><strong>Core Pinning.</strong> To see the highest throughput and lowest latencies, consider core pinning and reducing the IRQs. See section above for specific instructions we used on our 8 core instance. It is also important to apply similar techniques on your test nodes.</p>



<p><strong>Vary the connections.</strong> Connection count is a surprisingly crucial variable for both Redis and Valkey. In fact, latencies rise steeply as you approach 1024 connections on both of them. For example, going from 400 to 1024 connections, Valkey’s SET throughput dropped from 999.9K RPS with to 969K RPS and p99 latencies doubled from .8ms to 1.6ms (at 2048 conns, p99 latencies triple). Going from 384 connections to 1024, Redis throughput drops from 729.4K RPS to 668K RPS, and p99 latencies more than double from .99ms to 2.5ms. Lower throughput with higher latencies? You get why connection count tuning is so crucial here.</p>



<p><strong>Key Space.</strong> If you want the best numbers, <a href="https://www.linkedin.com/posts/yaoyue-thinkingfish_my-performance-rant-of-the-day-if-you-are-activity-7326350824261980161-qB6h/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAANW9wBFDc3gQ3Jwp6YswZ_BARGfJvyJQQ" target="_blank" rel="noreferrer noopener">use smaller values and a really small key space</a> (<sup><code>-r 10000</code></sup>). This will help you get everything from L3 cache. To make this test slightly more real world, we used 1KB items (<code><sub><sup>-d 1024</sup></sub></code>) and a key space of 3Million (<code><sup>-r 3000000</sup></code>).</p>



<p><strong>Multi-Thread the Benchmark App.</strong> To get the maximum throughput out of valkey-benchmark, make sure to turn on multi-threading on the benchmark tool as well. The <code><sup>--threads 6</sup></code> flag tells valkey-benchmark to run in multi-threaded mode.</p>



<p><strong>Benchmark command:</strong></p>



<pre><code>docker run --network="host" --rm --cpuset-cpus="2-7" \
valkey/valkey:8.0.1 valkey-benchmark \
-h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \
-r 3000000 --threads 6 -d 1024</code></pre>



<h2>A Final Caveat: Benchmarking is imprecise in nature</h2>



<p>We made every effort to make this benchmark resemble more real world workflows, but you can always do better. Valkey-bench is not perfect (nothing is). We have a wishlist of improvements (and so <a href="https://github.com/valkey-io/valkey/issues/900" target="_blank" rel="noreferrer noopener">does</a> the Valkey project).</p>



<p>First, today, it simply pushes as much load as the server is able to handle instead of targeting a particular TPS. The real world rarely modulates its throughput based on your latency. Second, once it can target specific load, it would become closer to real world if it could modulate the load to show spikes and troughs as opposed to running a consistent throughput profile. Lastly, it’s rare to see 100% GETs or 100% SETs in a Key Value cache workflow. We’d love to provide a SET:GET ratio to see how the system reacts.</p>



<p>At Momento, we typically do our testing using <a href="https://github.com/iopsystems/rpc-perf" target="_blank" rel="noreferrer noopener">rpc-perf</a>. It is written entirely in rust, handles more real world scenarios (like the three feature requests above), and pairs incredibly well with Rezolus. Regardless, even rpc-perf is a synthetic benchmark and even though it gives you more degrees of freedom to simulate production workflows, the results should not be interpreted as generally applicable to every workflow. Small variables make huge differences – and simulations are no match for production.</p>



<h2>Final Thoughts: Performance Is a Practice</h2>



<p>Valkey has not only kept pace – it’s setting it. Meanwhile, the performance ceiling keeps rising. But getting there isn’t automatic. It requires expertise across systems, infrastructure, and workload behavior.</p>



<p>At Momento, we help teams achieve this kind of performance every day. Whether you’re running Valkey, Redis, or evaluating your options – we’re here to help you scale with confidence.</p>



<p><strong>Want help tuning your real-time infrastructure? <a href="https://gomomento.com/contact-us" target="_blank" rel="noreferrer noopener">Let’s talk.</a></strong></p>







<hr>



<p><strong>Special thanks to Yao and Brian from&nbsp;<a href="https://iop.systems/" target="_blank" rel="noreferrer noopener">IOP Systems</a>&nbsp;for providing the tools, including rpc-perf and rezolus, as well as insights for this benchmark.</strong></p>




        
      </div>

  </article>
  


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Silicon Valley finally has a big electronics retailer again: Micro Center opens (271 pts)]]></title>
            <link>https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</link>
            <guid>44140378</guid>
            <pubDate>Fri, 30 May 2025 22:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx">https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=44140378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p dir="ltr">After years of waiting, the ribbon has been cut and Micro Center Silicon Valley is officially open.&nbsp;</p>
<p dir="ltr">On a sunny Friday morning in Santa Clara, with hundreds of fans queued in a line wrapping down the block and around the corner, we welcomed the Silicon Valley community to our newest store, at <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">5201 Stevens Creek Blvd</a>.&nbsp;</p>
<p dir="ltr">If you're a DIY PC builder, a serious gamer, a creator, a maker, or just someone who gets excited over the latest CPUs, GPUs, and 3D printers, then you already know what Micro Center is about</p>
<p dir="ltr">The Bay Area sets a high bar for all things tech, and here you'll find aisles stacked high with components, knowledgeable staff who actually know what they're talking about, and a hands-on experience you just can't replicate online.&nbsp;</p>
<p dir="ltr">The grand opening celebration also features special promotions, including 20% off Windows desktops and laptops and 20% off monitors. Additionally, the store has over 4,000 graphics cards in stock, including exclusive models, that will available for our grand opening event. For more information, please visit the <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">Micro Center Santa Clara page</a>.</p>
<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-1.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>

<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-2.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C++ to Rust Phrasebook (178 pts)]]></title>
            <link>https://cel.cs.brown.edu/crp/</link>
            <guid>44140349</guid>
            <pubDate>Fri, 30 May 2025 22:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cel.cs.brown.edu/crp/">https://cel.cs.brown.edu/crp/</a>, See on <a href="https://news.ycombinator.com/item?id=44140349">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox></mdbook-sidebar-scrollbox>
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                <div id="menu-bar">
                    

                    <h2>C++ to Rust Phrasebook</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="c-to-rust-phrasebook"><a href="#c-to-rust-phrasebook">C++ to Rust Phrasebook</a></h2>
<p>This book is designed to help C++ programmers learn Rust. It provides translations of common C++ patterns into idiomatic Rust. Each pattern is described through concrete code examples along with high-level discussion of engineering trade-offs.</p>
<p>The book can be read front-to-back, but it is designed to be used random-access.
When you are writing Rust code and think, "I know how to do this in C++ but not Rust," then
look for the corresponding chapter in this book.</p>
<p>This book was hand-written by expert C++ and Rust programmers at Brown University's <a href="https://cel.cs.brown.edu/">Cognitive Engineering Lab</a>. Our goal is provide accurate information with a tasteful degree of detail. No text in this book was written by AI.</p>
<p>If you would like updates on when we add new chapters to this book, you can <a href="https://forms.gle/rcrdZihmT81LWy6F6">drop your email here</a>.</p>
<h2 id="other-resources"><a href="#other-resources">Other resources</a></h2>
<p>If you have zero Rust experience, you might consider first reading <a href="https://rust-book.cs.brown.edu/">The Rust Programming
Language</a> or getting a quick overview at <a href="https://learnxinyminutes.com/rust/">Learn X in Y Minutes</a>.</p>
<p>If you are primarily an embedded systems programmer using C or C++, this book is
a complement to <a href="https://docs.rust-embedded.org/book/">The Embedded Rust Book</a>.</p>
<p>Compared to resources like the <a href="https://doc.rust-lang.org/nomicon/">Rustonomicon</a> and <a href="https://rust-unofficial.github.io/too-many-lists/">Learn Rust With Entirely Too Many Linked Lists</a>, this book is less about "Rust behind the scenes" and more about explicitly describing how Rust works in terms of C++.</p>
<h2 id="feedback-on-this-book"><a href="#feedback-on-this-book">Feedback on this book</a></h2>
<p>At the bottom of every page there is a link
to a form where you can submit feedback: typos, factual errors, or any other issues you spot.</p>
<p>If you answer the quizzes at the end of each chapter, we will save your
responses anonymously for research purposes.</p>


                        <a href="https://docs.google.com/forms/d/e/1FAIpQLScoygeNlygODY2owQ-HvU8VGx3hi50aic7ZlKCyhJ0VktjiCg/viewform?usp=pp_url&amp;entry.1450251950=C++%20to%20Rust%20Phrasebook">Click here to leave us feedback about this page.</a>
                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://cel.cs.brown.edu/crp/idioms/constructors.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://cel.cs.brown.edu/crp/idioms/constructors.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>


        <!-- Google tag (gtag.js) -->
        
        


        


        
        
        

        
        
        

        <!-- Custom JS scripts -->
        


    </div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Photos taken inside musical instruments (911 pts)]]></title>
            <link>https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</link>
            <guid>44139626</guid>
            <pubDate>Fri, 30 May 2025 20:32:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments">https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</a>, See on <a href="https://news.ycombinator.com/item?id=44139626">Hacker News</a></p>
Couldn't get https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>