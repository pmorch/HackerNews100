(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 11 Nov 2025 11:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[SoftBank sells its entire stake in Nvidia for $5.83B (106 pts)]]></title>
            <link>https://www.cnbc.com/2025/11/11/softbank-sells-its-entire-stake-in-nvidia-for-5point83-billion.html</link>
            <guid>45884937</guid>
            <pubDate>Tue, 11 Nov 2025 07:32:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/11/11/softbank-sells-its-entire-stake-in-nvidia-for-5point83-billion.html">https://www.cnbc.com/2025/11/11/softbank-sells-its-entire-stake-in-nvidia-for-5point83-billion.html</a>, See on <a href="https://news.ycombinator.com/item?id=45884937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108065851" data-test="InlineImage"><p>Nvidia CEO Jensen Huang (L) and the CEO of the SoftBank Group Masayoshi Son pose during an AI event in Tokyo on November 13, 2024.</p><p>Akio Kon | Bloomberg | Getty Images</p></div><div><p><a id="107312506" href="https://www.cnbc.com/quotes/" type="security" brand="cnbc" section="[object Object]" contentclassification="">SoftBank</a> said Tuesday it has sold its entire stake in U.S. chipmaker <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> for $5.83 billion as the Japanese giant looks to capitalize on its <a href="https://www.cnbc.com/2025/06/27/softbank-ceo-says-he-wanted-to-be-openai-early-investor.html">"all in"</a> bet on ChatGPT maker OpenAI. </p><p>The firm said in its earnings statement that it sold 32.1 million Nvidia shares in October. It also disclosed that it sold part of its T-Mobile stake for $9.17 billion.</p><p>"We want to provide a lot of investment opportunities for investors, while we can still maintain financial strength," said SoftBank's Chief Financial Officer Yoshimitsu Goto during an investor presentation. </p><p>"So through those options and tools we make sure that we are ready for funding in a very safe manner," he said in comments translated by the company, adding that the stake sales were part of the firm's strategy for "asset monetization."</p><p>Nvidia shares dipped 0.95% in premarket trade on Tuesday.</p><p>While the Nvidia exit may come as a surprise to some investors, it's not the first time SoftBank has cashed out of the American AI chip darling.</p><p>SoftBank's Vision Fund was an early backer of Nvidia, <a href="https://www.cnbc.com/2017/05/24/the-stock-markets-hottest-stock-nvidia-just-got-a-big-new-backer.html">reportedly amassing</a> a $4 billion stake in 2017 before <a href="https://www.cnbc.com/2019/02/06/softbank-vision-fund-sells-nvidia-stake.html">selling all</a> of its holdings in January 2019. Despite its latest sale, SoftBank's business interests remain heavily intertwined with Nvidia's.</p></div><div id="Placeholder-ArticleBody-Video-108212821" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000392166" aria-labelledby="Placeholder-ArticleBody-Video-108212821"><p><img src="https://image.cnbcfm.com/api/v1/image/108212822-17605971971760597195-42119380833-1080pnbcnews.jpg?v=1760597197&amp;w=750&amp;h=422&amp;vtcrop=y" alt="ABB CEO: Softbank will be good home for robotics business"><span></span><span></span></p></div><div><p>That Tokyo-based company is involved in a number of AI ventures that rely on Nvidia's technology, including the $500 billion Stargate project for data centers in the U.S.</p><p>"This should not be seen, in our view, as a cautious or negative stance on Nvidia, but rather in the context of SoftBank needing at least $30.5bn of capital for investments in the Oct-Dec quarter, including $22.5bn for OpenAI and $6.5bn for Ampere," Rolf Bulk, equity research analyst at New Street Research, told CNBC.</p><p>That amounts to "more in a single quarter than it has invested in aggregate over the two prior years combined," Bulk said.</p><p>Morningstar's Dan Baker added that he doesn't see the move as representing a fundamental shift in strategy for the company.</p><p>"[SoftBank] made a point of saying that it wasn't any view on NVIDIA... At the end of the day, they are using the money to invest in other AI related companies," he said.</p></div><h2><a id="headline0"></a>Vision fund posts blowout $19 billion gain</h2><div><p>The stake sales and a blowout gain of $19 billion from SoftBank's Vision Fund helped the company <a href="https://www.cnbc.com/2025/11/11/softbank-earnings-report-2q.html">double its profit</a> in its fiscal second quarter.</p><p>The Vision Fund has been aggressively pushing into artificial intelligence, investing and acquiring firms throughout the AI value chain from chips to large language models and robotics.</p><p>"The reason we were able to have this result is because of September last year, that was the first time we invested in OpenAI," said SoftBank's Goto. He added that OpenAI's <a href="https://www.cnbc.com/2025/10/02/openai-share-sale-500-billion-valuation.html">latest valuation milestone of $500 billion</a> marks one of the largest valuations in the world, according to fair value.  </p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Softbank's shares this year</p></div><div><p>The Japanese conglomerate's stock has slumped in the past week as <a href="https://www.cnbc.com/2025/11/07/ai-valuation-fears-grip-investors-as-tech-bubble-concerns-heighten.html">concerns of an AI bubble</a> sent jitters through global markets. </p><p>"Our share price recently has been going up and down dynamically… we want to provide as many invest opportunities as possible," said Goto Tuesday, adding that the company's announced four-for-one stock split is part of its strategy to provide as many investment opportunities for shareholders as possible.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI documentation you can talk to, for every repo (116 pts)]]></title>
            <link>https://deepwiki.com/</link>
            <guid>45884169</guid>
            <pubDate>Tue, 11 Nov 2025 04:38:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepwiki.com/">https://deepwiki.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45884169">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="138"><a href="https://deepwiki.com/bregman-arie/devops-exercises"><div><div><p><span>bregman-arie</span>/<span>devops-exercises</span></p></div><p>Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions</p><div><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256"><path d="M239.18,97.26A16.38,16.38,0,0,0,224.92,86l-59-4.76L143.14,26.15a16.36,16.36,0,0,0-30.27,0L90.11,81.23,31.08,86a16.46,16.46,0,0,0-9.37,28.86l45,38.83L53,211.75a16.38,16.38,0,0,0,24.5,17.82L128,198.49l50.53,31.08A16.4,16.4,0,0,0,203,211.75l-13.76-58.07,45-38.83A16.43,16.43,0,0,0,239.18,97.26Zm-15.34,5.47-48.7,42a8,8,0,0,0-2.56,7.91l14.88,62.8a.37.37,0,0,1-.17.48c-.18.14-.23.11-.38,0l-54.72-33.65a8,8,0,0,0-8.38,0L69.09,215.94c-.15.09-.19.12-.38,0a.37.37,0,0,1-.17-.48l14.88-62.8a8,8,0,0,0-2.56-7.91l-48.7-42c-.12-.1-.23-.19-.13-.5s.18-.27.33-.29l63.92-5.16A8,8,0,0,0,103,91.86l24.62-59.61c.08-.17.11-.25.35-.25s.27.08.35.25L153,91.86a8,8,0,0,0,6.75,4.92l63.92,5.16c.15,0,.24,0,.33.29S224,102.63,223.84,102.73Z"></path></svg><p><span>74.0k</span></p></div></div></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 'Toy Story' You Remember (524 pts)]]></title>
            <link>https://animationobsessive.substack.com/p/the-toy-story-you-remember</link>
            <guid>45883788</guid>
            <pubDate>Tue, 11 Nov 2025 03:17:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://animationobsessive.substack.com/p/the-toy-story-you-remember">https://animationobsessive.substack.com/p/the-toy-story-you-remember</a>, See on <a href="https://news.ycombinator.com/item?id=45883788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!oYAZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!oYAZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 424w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 848w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1272w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png" width="1456" height="782" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:782,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1780036,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!oYAZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 424w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 848w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1272w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>A still from </span><em>Toy Story</em><span> on 35 mm film</span></figcaption></figure></div><p><strong>Welcome!</strong><span> Glad you could join us for another Sunday edition of the </span><em>Animation Obsessive</em><span> newsletter. This is our slate:</span></p><ul><li><p><strong>1)</strong><span> Digital animation on film stock.</span></p></li><li><p><strong>2)</strong><span> Animation newsbits.</span></p></li></ul><p>With that, let’s go!</p><p><em>Toy Story</em><span> used to look different. It’s a little tricky to explain.</span></p><p><span>Back in 1995, CG animation was </span><em>the</em><span> topic in the industry, and Pixar was central to the hype. The studio had already </span><a href="https://animationobsessive.substack.com/p/when-disney-went-digital" rel="">shifted Disney to computers</a><span> and won the first Oscar for a CG short (</span><em><a href="https://www.youtube.com/watch?v=DWi2WTqD59A" rel="">Tin Toy</a></em><span>). Giant movies like </span><em>Jurassic Park</em><span> incorporated Pixar’s software.</span></p><p><span>The next step was </span><em>Toy Story</em><span>, billed as the first animated feature to go all-CG.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-1-178330349" target="_self" rel="">1</a></span><span> Even after Pixar’s successes, that was a risk. Would a fully digital movie sell tickets? </span></p><p><span>It clearly worked out. </span><em>Toy Story</em><span> appeared 30 years ago this month — and its popularity created the animation world that exists now. A new process took over the business.</span></p><p><span>But not </span><em>entirely</em><span> new — not at first. There was something old about </span><em>Toy Story</em><span>’s tech, too, back in 1995. Pixar made the thing with computers, but it still needed to screen in theaters. And computers couldn’t really </span><em>do</em><span> that yet. From its early years, Pixar had relied on physical film stock. According to authors Bill Kinder and Bobbie O’Steen:</span></p><blockquote><p><span>[Pixar’s Ed]</span><em> Catmull recognized that his studio’s pixels needed to merge with that world-standard distribution freeway, 35 mm film. Computer chips were not fast enough, nor disks large enough, nor compression sophisticated enough to display even 30 minutes of standard-definition motion pictures. It was axiomatic that for a filmgoing audience to be going to a film, it would be a... film.</em><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-2-178330349" target="_self" rel="">2</a></span></p></blockquote><p><em>Toy Story</em><span> was a transitional project. Since Pixar couldn’t send digital data to theaters, every one of the movie’s frames was printed on analog film. When </span><em>Toy Story</em><span> originally hit home video, that 35 mm version was its source. Only years later, after technology advanced, did Pixar start doing digital transfers — cutting out the middleman. And </span><em>Toy Story</em><span>’s look changed with the era.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-3-178330349" target="_self" rel="">3</a></span><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!rk4n!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rk4n!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 424w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 848w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rk4n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png" width="1456" height="1688" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1688,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4835256,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rk4n!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 424w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 848w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Toy Story</em><span>’s original release on 35 mm (top), and the version currently streaming on Disney+ (bottom). See the film’s trailer on 35 mm </span><a href="https://www.youtube.com/watch?v=LoBFN_V66P0" rel="">here</a><span>.</span></figcaption></figure></div><p><span>While making </span><em>Toy Story</em><span>, Pixar’s team knew that the grain, softness, colors and contrasts of analog film weren’t visible on its monitors. They were different mediums. </span></p><p><span>So, to get the right look, the studio had to keep that final, physical output in mind. The digital colors were tailored with an awareness that they would change after printing. “Greens go dark really fast, while the reds stay pretty true,” said </span><em>Toy Story</em><span>’s art director, Ralph Eggleston. “Blues have to be less saturated to look fully saturated on film, while the oranges look really bad on computer screens, but look really great on film.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-4-178330349" target="_self" rel="">4</a></span></p><p>The team checked its work along the way. In the words of Pixar’s William Reeves:</p><blockquote><p><em>During production, we’re working mostly from computer monitors. We’re rarely seeing the images on film. So, we have five or six extremely high-resolution monitors that have better color and picture quality. We put those in general work areas, so people can go and see how their work looks. Then, when we record, we try to calibrate to the film stock, so the image we have on the monitor looks the same as what we’ll get on film.</em></p></blockquote><p><span>Behind the final images was a “painstaking transfer process,” according to the press. Leading it was David DiFrancesco, one of Pixar’s early MVPs, who began working with Ed Catmull before Pixar even existed. He broke ground in film printing — specifically, in putting digital images on analog film.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-5-178330349" target="_self" rel="">5</a></span></p><p><span>He and his team in Pixar’s photoscience department used their expertise here. Their tools were “commercial grade” film printers, DiFrancesco noted: modified Solitaire Cine II machines. He’d invented more advanced stuff, but it wasn’t viable for a project of </span><em>Toy Story</em><span>’s size. Using the best equipment would’ve taken “several terabytes of data,” he said.</span></p><p><span>Their system was fairly straightforward. Every frame of </span><em>Toy Story</em><span>’s negative was exposed, three times, in front of a CRT screen that displayed the movie. “Since all film and video images are composed of combinations of red, green and blue light, the frame is separated into its discrete red, green and blue elements,” noted the studio. Exposures, filtered through each color, were layered to create each frame.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-6-178330349" target="_self" rel="">6</a></span><span> </span></p><p><span>It reportedly took nine hours to print 30 seconds of </span><em>Toy Story</em><span>. But it had to be done: it was the only way to screen the film.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5nPg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5nPg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 424w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 848w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1272w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5nPg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png" width="1456" height="882" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:882,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2072015,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5nPg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 424w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 848w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1272w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Examples of green, blue and red exposures, and the final scene on 35 mm film. Courtesy of the </span><em>Ultimate Toy Box</em><span> DVD.</span></figcaption></figure></div><p>In 1999, Pixar made history again.</p><p><span>Its second feature, </span><em>A Bug’s Life</em><span>, reached theaters in 1998. Once more, the studio designed its visuals for analog film (</span><a href="https://www.youtube.com/watch?v=izmlSjjOEdo" rel="">see the trailer on 35 mm</a><span>). Its people knew the ins-and-outs of this process, down to the amount of detail that film stock could accept and a projector could show. That’s partly how they got away with the movie’s tiny 2048×862 resolution, for example.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-7-178330349" target="_self" rel="">7</a></span></p><p><span>Still, the team struggled with one thing: the dip in image quality when film got converted to home video. That’s how </span><em>Toy Story</em><span> was released, but there </span><em>had</em><span> to be a better way.</span></p><p><span>For the home version of</span><em> A Bug’s Life</em><span>, Pixar devised a method of “go[ing] from our digital image within our system … straight to video,” John Lasseter said. He called it “a real pure version of our movie straight from our computers.” </span><em>A Bug’s Life</em><span> became the first digital-to-digital transfer on DVD. Compared to the theatrical release, the look had changed. It was sharp and grainless, and the colors were kind of different.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-8-178330349" target="_self" rel="">8</a></span></p><p><span>A digital transfer of </span><em>Toy Story</em><span> followed in the early 2000s. And it wasn’t </span><em>quite</em><span> the same movie that viewers had seen in the ‘90s. “The colors are vivid and lifelike, [and] not a hint of grain or artifacts can be found,” raved one reviewer. It was a crisp, blazingly bright, digital image now — totally different from the softness, texture and deep, muted warmth of physical film, on which </span><em>Toy Story </em><span>was created to be seen.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!P-J7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!P-J7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 424w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 848w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!P-J7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png" width="1456" height="1676" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1676,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3528708,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!P-J7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 424w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 848w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Toy Story</em><span> on 35 mm (top) and the Disney+ edition (bottom)</span></figcaption></figure></div><p><span>Quickly, digital transfers became a standard thing. Among others by Pixar, </span><em>The Incredibles</em><span> puts off a very different vibe between its theatrical and later releases (see </span><a href="https://www.youtube.com/watch?v=M_nSbqsLmEk" rel="">the 35 mm trailer</a><span> for reference). </span></p><p>Pixar wasn’t the only studio to make the leap, either. Disney did as well. </p><p><span>Like </span><em>Toy Story</em><span>, the Disney renaissance work of the ‘90s was transitional. </span><em>The Lion King</em><span>, </span><em>Mulan</em><span> and the rest existed as </span><a href="https://animationobsessive.substack.com/p/when-disney-went-digital" rel="">files in computer systems</a><span> — and the idea was always to record them on analog film at the end. Early home releases were based on those 35 mm versions. Later releases, like the ones Disney streams today, were direct transfers of the digital data. </span></p><p><span>At times, especially in the colors, they’re almost unrecognizable. And the images feel less cohesive — like something’s missing that was </span><em>supposed</em><span> to bring all the elements together. These aren’t quite the same films that ruled the ‘90s.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!6IkD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6IkD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 424w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 848w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1272w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!6IkD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png" width="1456" height="1633" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1633,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4330548,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6IkD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 424w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 848w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1272w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Aladdin</em><span> on 35 mm film (top) versus Blu-ray (bottom). See a clip from the film on 35 mm </span><a href="https://www.youtube.com/watch?v=AuhNnovKXLA" rel="">here</a><span>.</span></figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qdDU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qdDU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 424w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 848w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qdDU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png" width="1456" height="1662" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1662,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4506414,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qdDU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 424w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 848w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The Lion King</em><span> on 35 mm film (top) versus Blu-ray. See a clip from the film on 35 mm </span><a href="https://www.youtube.com/watch?v=uivXq3tXOhg" rel="">here</a><span>.</span></figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!T9lv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!T9lv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 424w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 848w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1272w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!T9lv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png" width="1456" height="1702" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1702,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3824553,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!T9lv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 424w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 848w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1272w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Mulan</em><span> on 35 mm film (top) versus Blu-ray. See the film’s trailer on 35 mm </span><a href="https://www.youtube.com/watch?v=2z2KsFZs-8I" rel="">here</a><span>.</span></figcaption></figure></div><p><span>For a number of years, there’s been talk in film-preservation circles about </span><em>Toy Story</em><span> and the Disney renaissance. This work sits in an odd place. The world was still pretty analog when the computer animation boom arrived: out of necessity, these projects became hybrids of new and old. What’s the</span><em> right </em><span>way to see digital movies that were designed for 35 mm film?</span></p><p><span>The studios themselves haven’t quite figured it out. On Disney+, the colors of </span><em>Toy Story</em><span> feel a bit raw — searing greens that were meant to darken on film, for example. Meanwhile, the newer </span><em>Toy Story</em><span> Blu-ray shares more in common with the original colors, but it’s still an altered, colder look.</span></p><p><span>When digital transfers first showed up, people were thrilled, including at Pixar. Movies became “crisper, clearer and more stunning on home video systems” than in theaters, some claimed.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-9-178330349" target="_self" rel="">9</a></span><span> Even so, it’s a little disquieting to think that </span><em>Toy Story</em><span>, the film that built our current world, is barely available in the form that wowed audiences of the ‘90s. The same goes for many other movies from the transitional era.</span></p><p><span>The good news is that this conversation gets bigger all the time. In those film-preservation circles, a dedicated few are trying to save the old work. More and more </span><a href="https://www.youtube.com/watch?v=BMvgu_KdpjA" rel="">comparison videos</a><span> are popping up on YouTube. If you get the chance to see one of the old Disney or Pixar films on 35 mm, it’s always worthwhile.</span></p><p><span>These companies, ultimately, decide how </span><em>Toy Story</em><span> looks today. Still, for some, it’s nice to see the original version of the film again — the version Pixar originally intended to make. It’s evidence that the film </span><em>did</em><span> feel</span><em> </em><span>different back then. The memories were real.</span></p><ul><li><p><em>I Am Frankelda</em><span> continues its strong performance in </span><strong>Mexican</strong><span> theaters. Analyst Edgar Apanco </span><a href="https://x.com/elapanco/status/1987639735091921274" rel="">reports</a><span> that 658,000 people have gone to see it, surpassing the popular </span><em>Chainsaw Man</em><span> movie. Revenues are </span><a href="https://x.com/elapanco/status/1987660916633325574" rel="">over $2.15 million</a><span> and climbing — having fallen </span><a href="https://palomaynacho.com/blog/chainsaw-y-frankelda-se-enfrentan-en-un-halloween-complicado/" rel="">just 17%</a><span> in week two, and an estimated 20% in week three.</span></p></li><li><p><span>In </span><strong>Japan</strong><span>, Goro Miyazaki </span><a href="https://ghibli.jpn.org/news/goro-talk-4/" rel="">revealed</a><span> that his father is still going to Studio Ghibli to draw for a few hours each day.</span></p></li><li><p><span>An exhibition in </span><strong>Taiwan</strong><span> </span><a href="https://reading.udn.com/read/story/124410/9126552" rel="">brought</a><span> the films of Karel Zeman to the country, reportedly for the first time. </span><em>The Fabulous Baron Munchausen</em><span> and </span><em>Invention for Destruction</em><span> are showing, among others.</span></p></li><li><p><span>In </span><strong>Nigeria</strong><span>, animator Gabriel Ugbodaga had </span><a href="https://www.arise.tv/gabriel-ugbodaga-nigeria-has-enough-animation-talent-what-we-lack-is-training-and-exposure/" rel="">a televised interview</a><span> about his well-received film </span><em>Vainglorious</em><span> (</span><a href="https://www.youtube.com/watch?v=6tVVWgz1cEk" rel="">watch</a><span>) and the state of the country’s industry. “When it comes to 2D hand-drawn animation,” he said, “there’s a lot of talent in Nigeria.”</span></p></li><li><p><span>If you missed that </span><em>Baahubali: The Eternal War</em><span> teaser this week, </span><a href="https://www.youtube.com/watch?v=RdUPs9e1bUk" rel="">see it here</a><span>. It’s an </span><strong>Indian</strong><span> feature presented by S. S. Rajamouli (</span><em>RRR</em><span>).</span></p></li><li><p><span>In </span><strong>Germany</strong><span>, Werner Herzog’s animated film </span><em>The Twilight World</em><span> </span><a href="https://cineuropa.org/en/newsdetail/485526" rel="">picked up</a><span> “€100,000 for production preparation support,” reports </span><em>Cineuropa</em><span>.</span></p></li><li><p><em>Infinity Castle</em><span> will reach </span><strong>China</strong><span> next weekend, and forecasters </span><a href="https://cn.investing.com/news/stock-market-news/article-3066545" rel="">believe</a><span> it could earn a billion yuan (over $140 million) and become the highest-grossing anime film in the country.</span></p></li><li><p><span>Also </span><a href="https://weibo.com/7985578740/Qcrf6p4D6" rel="">happening</a><span> in </span><strong>China</strong><span> next weekend: the latest edition of Feinaki Beijing Animation Week. The festival posted </span><a href="https://www.bilibili.com/video/BV1rMyzBxE9w/" rel="">55 trailers</a><span> for its selections this year.</span></p></li><li><p><span>The </span><strong>Japanese</strong><span> journalist Atsushi Matsumoto is raising concerns that the anime boom of the 2020s </span><a href="https://news.yahoo.co.jp/expert/articles/55f696fd42ce9a821f4bf682327f452bf3b7245c" rel="">could be a bubble</a><span>. (Meanwhile, despite huge industry profits, analysis suggests that studio closures are </span><a href="https://prtimes.jp/main/html/rd/p/000001179.000043465.html" rel="">set to rise</a><span> for the third year in a row.)</span></p></li><li><p><span>In</span><strong> America</strong><span>, for those in New York, there’s an interesting series of stop-motion screenings </span><a href="https://www.eastman.org/stop-motion-artform" rel="">at the Eastman Museum</a><span> this month — including </span><em>The Wolf House</em><span>.</span></p></li><li><p><span>Last of all: we wrote about a handful of </span><a href="https://animationobsessive.substack.com/p/free-films-worth-seeing" rel="">recent, free films worth seeing</a><span>. </span></p></li></ul><p><em><strong>Until next time!</strong></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I hate screenshots of text (278 pts)]]></title>
            <link>https://parkscomputing.com/page/i-hate-screenshots-of-text</link>
            <guid>45883124</guid>
            <pubDate>Tue, 11 Nov 2025 01:36:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://parkscomputing.com/page/i-hate-screenshots-of-text">https://parkscomputing.com/page/i-hate-screenshots-of-text</a>, See on <a href="https://news.ycombinator.com/item?id=45883124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        




<article>
    
    <div>
        
<p>During the course of a regular working day, I receive a lot of screenshots like this from well-meaning colleagues:</p>
<p> <img src="https://parkscomputing.com/images/screenshots.png" alt="A screenshot of some text">
</p><p>It's almost always in a chat about some issue that occurred in the code, or perhaps code that's somehow related to the code in the screenshot, or… well, how am I supposed to even know? Upon seeing this code, I might think, “How is <code>slug</code> defined? Is <code>slug</code> being used to create the <code>baseUrl</code>? Why is the domain name hard-coded in that URL? What happens if an exception is thrown? <em>What module is this code even in?</em>”</p>
<p>I have to either very carefully type some of the code into a search box or (these days) get my coding agent to find the relevant module for me.</p>
<p>Why couldn't my colleague have just used copy &amp; paste? I could have seen a bit more of the context, even if the same lines were selected, and I could copy-and-paste <em>that</em> text into my IDE's search function so much more easily.</p>
<p>In fact, why couldn't they just send me the file, or even a link to the file (since everybody and their dog use GitHub, anyway).</p>
<p>It gets worse. Sometimes, I'll get a screenshot of an error log. “Hey, Paul, the build is failing. Can you look at this?”</p>
<p> <img src="https://parkscomputing.com/images/screenshots-errors2.png" alt="A screenshot of some build errors">
</p><p>What were you building? What line did it fail on? <em>What even was the error?</em></p>
<p>Of course, if I do a full rebuild of everything on my workstation, it'll succeed.</p>
<p>It would have been SO easy to just copy all of the error log, or even dump the log into a file, and just send me that.</p>
<p> <img src="https://parkscomputing.com/images/banging-head-against-wall-cracked.gif" alt="Me reading a screenshot of some build errors">
</p><p>Please, don't take screenshots of text unless it's to demonstrate a cosmetic issue related to the display of the text, or there is truly something relevant about the content of the screenshot that would be lost in a purely textual context.</p>

    </div>
</article>



    

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Warren Buffett's final shareholder letter [pdf] (310 pts)]]></title>
            <link>https://berkshirehathaway.com/news/nov1025.pdf</link>
            <guid>45882837</guid>
            <pubDate>Tue, 11 Nov 2025 00:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://berkshirehathaway.com/news/nov1025.pdf">https://berkshirehathaway.com/news/nov1025.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45882837">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[High-performance 2D graphics rendering on the CPU using sparse strips [pdf] (245 pts)]]></title>
            <link>https://github.com/LaurenzV/master-thesis/blob/main/main.pdf</link>
            <guid>45881568</guid>
            <pubDate>Mon, 10 Nov 2025 22:05:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/LaurenzV/master-thesis/blob/main/main.pdf">https://github.com/LaurenzV/master-thesis/blob/main/main.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45881568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            


<react-partial partial-name="marketing-navigation" data-ssr="true" data-attempted-ssr="true" data-react-profiling="false">
  
  
  <div data-target="react-partial.reactRoot"><nav aria-label="Global"><ul><li><div><ul><li><div><p><span>AI CODE CREATION</span></p><ul><li><a href="https://github.com/features/copilot" data-analytics-event="{&quot;action&quot;:&quot;github_copilot&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}"><div><p><span>GitHub Copilot</span><span>Write better code with AI</span></p></div></a></li><li><a href="https://github.com/features/spark" data-analytics-event="{&quot;action&quot;:&quot;github_spark&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}"><div><p><span>GitHub Spark</span><span>Build and deploy intelligent apps</span></p></div></a></li><li><a href="https://github.com/features/models" data-analytics-event="{&quot;action&quot;:&quot;github_models&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}"><div><p><span>GitHub Models</span><span>Manage and compare prompts</span></p></div></a></li><li><a href="https://github.com/mcp" data-analytics-event="{&quot;action&quot;:&quot;mcp_registry&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}"><div><p><span>MCP Registry<sup>New</sup></span><span>Discover and integrate external tools</span></p></div></a></li></ul></div></li><li><div><p><span>DEVELOPER WORKFLOWS</span></p><ul><li><a href="https://github.com/features/actions" data-analytics-event="{&quot;action&quot;:&quot;actions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}"><div><p><span>Actions</span><span>Automate any workflow</span></p></div></a></li><li><a href="https://github.com/features/codespaces" data-analytics-event="{&quot;action&quot;:&quot;codespaces&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}"><div><p><span>Codespaces</span><span>Instant dev environments</span></p></div></a></li><li><a href="https://github.com/features/issues" data-analytics-event="{&quot;action&quot;:&quot;issues&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}"><div><p><span>Issues</span><span>Plan and track work</span></p></div></a></li><li><a href="https://github.com/features/code-review" data-analytics-event="{&quot;action&quot;:&quot;code_review&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}"><div><p><span>Code Review</span><span>Manage code changes</span></p></div></a></li></ul></div></li><li><div><p><span>APPLICATION SECURITY</span></p><ul><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}"><div><p><span>GitHub Advanced Security</span><span>Find and fix vulnerabilities</span></p></div></a></li><li><a href="https://github.com/security/advanced-security/code-security" data-analytics-event="{&quot;action&quot;:&quot;code_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_security_link_platform_navbar&quot;}"><div><p><span>Code security</span><span>Secure your code as you build</span></p></div></a></li><li><a href="https://github.com/security/advanced-security/secret-protection" data-analytics-event="{&quot;action&quot;:&quot;secret_protection&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;secret_protection_link_platform_navbar&quot;}"><div><p><span>Secret protection</span><span>Stop leaks before they start</span></p></div></a></li></ul></div></li><li><div><p><span>EXPLORE</span></p><ul><li><a href="https://github.com/why-github" data-analytics-event="{&quot;action&quot;:&quot;why_github&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;why_github_link_platform_navbar&quot;}"><span>Why GitHub</span></a></li><li><a href="https://docs.github.com/" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_platform_navbar&quot;}" target="_blank" rel="noreferrer"><span>Documentation</span></a></li><li><a href="https://github.blog/" data-analytics-event="{&quot;action&quot;:&quot;blog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;blog_link_platform_navbar&quot;}" target="_blank" rel="noreferrer"><span>Blog</span></a></li><li><a href="https://github.blog/changelog" data-analytics-event="{&quot;action&quot;:&quot;changelog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;changelog_link_platform_navbar&quot;}" target="_blank" rel="noreferrer"><span>Changelog</span></a></li><li><a href="https://github.com/marketplace" data-analytics-event="{&quot;action&quot;:&quot;marketplace&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;marketplace_link_platform_navbar&quot;}"><span>Marketplace</span></a></li></ul></div></li></ul><p><a href="https://github.com/features" data-analytics-event="{&quot;action&quot;:&quot;view_all_features&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}"><span>View all features</span></a></p></div></li><li><div><ul><li><div><p><span>BY COMPANY SIZE</span></p><ul><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprises&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprises_link_solutions_navbar&quot;}"><span>Enterprises</span></a></li><li><a href="https://github.com/team" data-analytics-event="{&quot;action&quot;:&quot;small_and_medium_teams&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;small_and_medium_teams_link_solutions_navbar&quot;}"><span>Small and medium teams</span></a></li><li><a href="https://github.com/enterprise/startups" data-analytics-event="{&quot;action&quot;:&quot;startups&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;startups_link_solutions_navbar&quot;}"><span>Startups</span></a></li><li><a href="https://github.com/solutions/industry/nonprofits" data-analytics-event="{&quot;action&quot;:&quot;nonprofits&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;nonprofits_link_solutions_navbar&quot;}"><span>Nonprofits</span></a></li></ul></div></li><li><div><p><span>BY USE CASE</span></p><ul><li><a href="https://github.com/solutions/use-case/app-modernization" data-analytics-event="{&quot;action&quot;:&quot;app_modernization&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;app_modernization_link_solutions_navbar&quot;}"><span>App Modernization</span></a></li><li><a href="https://github.com/solutions/use-case/devsecops" data-analytics-event="{&quot;action&quot;:&quot;devsecops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devsecops_link_solutions_navbar&quot;}"><span>DevSecOps</span></a></li><li><a href="https://github.com/solutions/use-case/devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_solutions_navbar&quot;}"><span>DevOps</span></a></li><li><a href="https://github.com/solutions/use-case/ci-cd" data-analytics-event="{&quot;action&quot;:&quot;ci/cd&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ci/cd_link_solutions_navbar&quot;}"><span>CI/CD</span></a></li><li><a href="https://github.com/solutions/use-case" data-analytics-event="{&quot;action&quot;:&quot;view_all_use_cases&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_use_cases_link_solutions_navbar&quot;}"><span>View all use cases</span></a></li></ul></div></li><li><div><p><span>BY INDUSTRY</span></p><ul><li><a href="https://github.com/solutions/industry/healthcare" data-analytics-event="{&quot;action&quot;:&quot;healthcare&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;healthcare_link_solutions_navbar&quot;}"><span>Healthcare</span></a></li><li><a href="https://github.com/solutions/industry/financial-services" data-analytics-event="{&quot;action&quot;:&quot;financial_services&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;financial_services_link_solutions_navbar&quot;}"><span>Financial services</span></a></li><li><a href="https://github.com/solutions/industry/manufacturing" data-analytics-event="{&quot;action&quot;:&quot;manufacturing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;manufacturing_link_solutions_navbar&quot;}"><span>Manufacturing</span></a></li><li><a href="https://github.com/solutions/industry/government" data-analytics-event="{&quot;action&quot;:&quot;government&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;government_link_solutions_navbar&quot;}"><span>Government</span></a></li><li><a href="https://github.com/solutions/industry" data-analytics-event="{&quot;action&quot;:&quot;view_all_industries&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_industries_link_solutions_navbar&quot;}"><span>View all industries</span></a></li></ul></div></li></ul><p><a href="https://github.com/solutions" data-analytics-event="{&quot;action&quot;:&quot;view_all_solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_solutions_link_solutions_navbar&quot;}"><span>View all solutions</span></a></p></div></li><li><div><ul><li><div><p><span>EXPLORE BY TOPIC</span></p><ul><li><a href="https://github.com/resources/articles?topic=ai" data-analytics-event="{&quot;action&quot;:&quot;ai&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ai_link_resources_navbar&quot;}"><span>AI</span></a></li><li><a href="https://github.com/resources/articles?topic=software-development" data-analytics-event="{&quot;action&quot;:&quot;software_development&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;software_development_link_resources_navbar&quot;}"><span>Software Development</span></a></li><li><a href="https://github.com/resources/articles?topic=devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_resources_navbar&quot;}"><span>DevOps</span></a></li><li><a href="https://github.com/resources/articles?topic=security" data-analytics-event="{&quot;action&quot;:&quot;security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_link_resources_navbar&quot;}"><span>Security</span></a></li><li><a href="https://github.com/resources/articles" data-analytics-event="{&quot;action&quot;:&quot;view_all_topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_topics_link_resources_navbar&quot;}"><span>View all topics</span></a></li></ul></div></li><li><div><p><span>EXPLORE BY TYPE</span></p><ul><li><a href="https://github.com/customer-stories" data-analytics-event="{&quot;action&quot;:&quot;customer_stories&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}"><span>Customer stories</span></a></li><li><a href="https://github.com/resources/events" data-analytics-event="{&quot;action&quot;:&quot;events__webinars&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;events__webinars_link_resources_navbar&quot;}"><span>Events &amp; webinars</span></a></li><li><a href="https://github.com/resources/whitepapers" data-analytics-event="{&quot;action&quot;:&quot;ebooks__reports&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ebooks__reports_link_resources_navbar&quot;}"><span>Ebooks &amp; reports</span></a></li><li><a href="https://github.com/solutions/executive-insights" data-analytics-event="{&quot;action&quot;:&quot;business_insights&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;business_insights_link_resources_navbar&quot;}"><span>Business insights</span></a></li><li><a href="https://skills.github.com/" data-analytics-event="{&quot;action&quot;:&quot;github_skills&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_skills_link_resources_navbar&quot;}" target="_blank" rel="noreferrer"><span>GitHub Skills</span></a></li></ul></div></li><li><div><p><span>SUPPORT &amp; SERVICES</span></p><ul><li><a href="https://docs.github.com/" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_resources_navbar&quot;}" target="_blank" rel="noreferrer"><span>Documentation</span></a></li><li><a href="https://support.github.com/" data-analytics-event="{&quot;action&quot;:&quot;customer_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_support_link_resources_navbar&quot;}" target="_blank" rel="noreferrer"><span>Customer support</span></a></li><li><a href="https://github.com/orgs/community/discussions" data-analytics-event="{&quot;action&quot;:&quot;community_forum&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;community_forum_link_resources_navbar&quot;}"><span>Community forum</span></a></li><li><a href="https://github.com/trust-center" data-analytics-event="{&quot;action&quot;:&quot;trust_center&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trust_center_link_resources_navbar&quot;}"><span>Trust center</span></a></li><li><a href="https://github.com/partners" data-analytics-event="{&quot;action&quot;:&quot;partners&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}"><span>Partners</span></a></li></ul></div></li></ul></div></li><li><div><ul><li><div><p><span>COMMUNITY</span></p><ul><li><a href="https://github.com/sponsors" data-analytics-event="{&quot;action&quot;:&quot;github_sponsors&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}"><div><p><span>GitHub Sponsors</span><span>Fund open source developers</span></p></div></a></li></ul></div></li><li><div><p><span>PROGRAMS</span></p><ul><li><a href="https://securitylab.github.com/" data-analytics-event="{&quot;action&quot;:&quot;security_lab&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_lab_link_open_source_navbar&quot;}" target="_blank" rel="noreferrer"><span>Security Lab</span></a></li><li><a href="https://maintainers.github.com/" data-analytics-event="{&quot;action&quot;:&quot;maintainer_community&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;maintainer_community_link_open_source_navbar&quot;}" target="_blank" rel="noreferrer"><span>Maintainer Community</span></a></li><li><a href="https://github.com/accelerator" data-analytics-event="{&quot;action&quot;:&quot;accelerator&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;accelerator_link_open_source_navbar&quot;}"><span>Accelerator</span></a></li><li><a href="https://archiveprogram.github.com/" data-analytics-event="{&quot;action&quot;:&quot;archive_program&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;archive_program_link_open_source_navbar&quot;}" target="_blank" rel="noreferrer"><span>Archive Program</span></a></li></ul></div></li><li><div><p><span>REPOSITORIES</span></p><ul><li><a href="https://github.com/topics" data-analytics-event="{&quot;action&quot;:&quot;topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;topics_link_open_source_navbar&quot;}"><span>Topics</span></a></li><li><a href="https://github.com/trending" data-analytics-event="{&quot;action&quot;:&quot;trending&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trending_link_open_source_navbar&quot;}"><span>Trending</span></a></li><li><a href="https://github.com/collections" data-analytics-event="{&quot;action&quot;:&quot;collections&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;collections_link_open_source_navbar&quot;}"><span>Collections</span></a></li></ul></div></li></ul></div></li><li><div><ul><li><div><p><span>ENTERPRISE SOLUTIONS</span></p><ul><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}"><div><p><span>Enterprise platform</span><span>AI-powered developer platform</span></p></div></a></li></ul></div></li><li><div><p><span>AVAILABLE ADD-ONS</span></p><ul><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_enterprise_navbar&quot;}"><div><p><span>GitHub Advanced Security</span><span>Enterprise-grade security features</span></p></div></a></li><li><a href="https://github.com/features/copilot/copilot-business" data-analytics-event="{&quot;action&quot;:&quot;copilot_for_business&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;copilot_for_business_link_enterprise_navbar&quot;}"><div><p><span>Copilot for Business</span><span>Enterprise-grade AI features</span></p></div></a></li><li><a href="https://github.com/premium-support" data-analytics-event="{&quot;action&quot;:&quot;premium_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;premium_support_link_enterprise_navbar&quot;}"><div><p><span>Premium Support</span><span>Enterprise-grade 24/7 support</span></p></div></a></li></ul></div></li></ul></div></li><li><a href="https://github.com/pricing" data-analytics-event="{&quot;action&quot;:&quot;pricing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;pricing&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;pricing_link_pricing_navbar&quot;}"><span>Pricing</span></a></li></ul></nav></div>
</react-partial>



        <div>
                


<qbsearch-input data-scope="repo:LaurenzV/master-thesis" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="gjP2bom9MneAfFMoYIHRW9za-u5vhVJJHm5vfaTlEdG8qH4iHCp0pF2ISxvbOvUB2fGhWK-mx5easnUWe2Sacw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="LaurenzV/master-thesis" data-current-org="" data-current-owner="LaurenzV" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        <div data-view-component="true">        <!-- '"` --><!-- </textarea></xmp> --><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post">
          <p>We read every piece of feedback, and take your input very seriously.</p>
          
          
          <label for="include_email">Include my email address so I can be contacted</label>
</form></div>
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            <div>
              <p><a href="https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FLaurenzV%2Fmaster-thesis%2Fblob%2Fmain%2Fmain.pdf" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/LaurenzV/master-thesis/blob/main/main.pdf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d4712b985595d0ee9eef3ef57900e345d9365d3b9bb74c25427d77c8aca3315c" data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}">
                Sign in
              </a>
            </p></div>

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=LaurenzV%2Fmaster-thesis" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/LaurenzV/master-thesis/blob/main/main.pdf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d4712b985595d0ee9eef3ef57900e345d9365d3b9bb74c25427d77c8aca3315c" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-a472a9ef-fa55-4047-8728-a57ca0e96a26" for="icon-button-0e257615-683d-47b5-9a35-75c50f858c72" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.2ed7297523f7a189873b.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spatial intelligence is AI’s next frontier (196 pts)]]></title>
            <link>https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence</link>
            <guid>45880939</guid>
            <pubDate>Mon, 10 Nov 2025 21:07:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence">https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence</a>, See on <a href="https://news.ycombinator.com/item?id=45880939">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><p><span>In 1950, when computing was little more than automated arithmetic and simple logic, Alan Turing asked a question that still reverberates today: can machines think? It took remarkable imagination to see what he saw: that intelligence might someday be built rather than born. That insight later launched a relentless scientific quest called Artificial Intelligence (AI). Twenty-five years into my own career in AI, I still find myself inspired by Turing’s vision. But how close are we? The answer isn’t simple.</span></p><p><span>Today, leading AI technology such as large language models (LLMs) have begun to transform how we access and work with abstract knowledge. Yet they remain wordsmiths in the dark; eloquent but inexperienced, knowledgeable but ungrounded. </span><strong>Spatial intelligence will transform how we create and interact with real and virtual worlds—revolutionizing storytelling, creativity, robotics, scientific discovery, and beyond. This is AI’s next frontier.</strong></p></div><p><span>The pursuit of visual and spatial</span><em> </em><span>intelligence has been the North Star guiding me since I entered the field. It’s why I spent years building ImageNet, the first large-scale visual learning and benchmarking dataset and one of three key elements enabling the birth of modern AI, along with neural network algorithms and modern compute like graphics processing units (GPUs). It’s why </span><a href="https://svl.stanford.edu/" rel="">my academic lab at Stanford</a><span> has spent the last decade combining computer vision with robotic learning. And it’s why my cofounders Justin Johnson, Christoph Lassner, Ben Mildenhall, and I created </span><a href="https://www.worldlabs.ai/" rel="">World Labs</a><span> more than one year ago: to realize this possibility in full, for the first time.</span></p><p>In this essay, I’ll explain what spatial intelligence is, why it matters, and how we’re building the world models that will unlock it—with impact that will reshape creativity, embodied intelligence, and human progress.</p><p>AI has never been more exciting. Generative AI models such as LLMs have moved from research labs to everyday life, becoming tools of creativity, productivity, and communication for billions of people. They have demonstrated capabilities once thought impossible, producing coherent text, mountains of code, photorealistic images, and even short video clips with ease. It’s no longer a question of whether AI will change the world. By any reasonable definition, it already has.</p><p>Yet so much still lies beyond our reach. The vision of autonomous robots remains intriguing but speculative, far from the fixtures of daily life that futurists have long promised. The dream of massively accelerated research in fields like disease curation, new material discovery, and particle physics remains largely unfulfilled. And the promise of AI that truly understands and empowers human creators—whether students learning intricate concepts in molecular chemistry, architects visualizing spaces, filmmakers building worlds, or anyone seeking fully immersive virtual experiences—remains beyond reach.</p><p>To learn why these capabilities remain elusive, we need to examine how spatial intelligence evolved, and how it shapes our understanding of the world.</p><p>Vision has long been a cornerstone of human intelligence, but its power emerged from something even more fundamental. Long before animals could nest, care for their young, communicate with language, or build civilizations, the simple act of sensing quietly sparked an evolutionary journey toward intelligence.</p><p><span>This seemingly isolated ability to glean information from the external world, whether a glimmer of light or the feeling of texture, created a bridge between perception and survival that only grew stronger and more elaborate as the generations passed. Layer upon layer of neurons grew from that bridge, forming nervous systems that interpret the world and coordinate interactions between an organism and its surroundings. Thus, many scientists have conjectured that </span><strong>perception and action became the core loop driving the evolution of intelligence</strong><span>, and the foundation on which nature created our species—the ultimate embodiment of perceiving, learning, thinking, and doing.</span></p><p>Spatial intelligence plays a fundamental role in defining how we interact with the physical world. Every day, we rely on it for the most ordinary acts: parking a car by imagining the narrowing gap between bumper and curb, catching a set of keys tossed across the room, navigating a crowded sidewalk without collision, or sleepily pouring coffee into a mug without looking. In more extreme circumstances, firefighters navigate collapsing buildings through shifting smoke, making split-second judgements about stability and survival, communicating through gestures, body language and a shared professional instinct for which there’s no linguistic substitute. And children spend the entirety of their pre-verbal months or years learning the world through playful interactions with their environments. All of this happens intuitively, automatically—a fluency machines have yet to achieve.</p><div><p><span>Spatial Intelligence is also foundational to our imagination and creativity. Storytellers create uniquely rich worlds in their minds and leverage many forms of visual media to bring them to others, from ancient cave painting to modern cinema to immersive video games. Whether it’s children building sandcastles on the beach or playing Minecraft on the computer, spatially-grounded imagination forms the basis for interactive experiences in real or virtual worlds. And in many industry applications, simulations of objects, scenes and dynamic interactive environments power countless numbers of critical business use cases from industrial design to digital twins to robotic training. </span></p><p><span>History is full of civilization-defining moments where spatial intelligence played central roles. In ancient Greece, Eratosthenes transformed shadows into geometry—measuring a 7-degree angle in Alexandria at the exact moment the sun cast no shadow in Syene—to calculate the Earth’s circumference. Hargreave’s “Spinning Jenny” revolutionized textile manufacturing through a spatial insight: arranging multiple spindles side-by-side in a single frame allowed one worker to spin multiple threads simultaneously, increasing productivity eightfold. Watson and Crick discovered DNA’s structure by physically building 3D molecular models, manipulating metal plates and wire until the spatial arrangement of base pairs clicked into place. In each case, spatial intelligence drove civilization forward when scientists and inventors had to manipulate objects, visualize structures, and reason about physical spaces - none of which can be captured in text alone.</span></p></div><p><strong>Spatial Intelligence is the scaffolding upon which our cognition is built.</strong><span> It’s at work when we passively observe or actively seek to create. It drives our reasoning and planning, even on the most abstract topics. And it’s essential to the way we interact—verbally or physically, with our peers or with the environment itself. While most of us aren’t revealing new truths on the level of Eratosthenes most days, we </span><em>routinely</em><span> think in the same way—making sense of a complex world by perceiving it through our senses, then leveraging an intuitive understanding of how it works in physical, spatial terms.</span></p><div><p><span>Unfortunately, today’s AI doesn’t think like this yet.</span></p><p><span>Tremendous progress has indeed been made in the past few years. Multimodal LLMs (MLLMs), trained with voluminous multimedia data in addition to textual data, have introduced some basics of spatial awareness, and today’s AI can analyze pictures, answer questions about them, and generate hyperrealistic images and short videos. And through breakthroughs in sensors and haptics, our most advanced robots can begin to manipulate objects and tools in highly constrained environments.</span></p></div><p>Yet the candid truth is that AI’s spatial capabilities remain far from human level. And the limits reveal themselves quickly. State-of-the-art MLLM models rarely perform better than chance on estimating distance, orientation, and size—or “mentally” rotating objects by regenerating them from new angles. They can’t navigate mazes, recognize shortcuts, or predict basic physics. AI-generated videos—nascent and yes, very cool—often lose coherence after a few seconds.</p><p>While current state-of-the-art AI can excel at reading, writing, research, and pattern recognition in data, these same models bear fundamental limitations when representing or interacting with the physical world. Our view of the world is holistic—not just what we’re looking at, but how everything relates spatially, what it means, and why it matters. Understanding this through imagination, reasoning, creation, and interaction—not just descriptions—is the power of spatial intelligence. Without it, AI is disconnected from the physical reality it seeks to understand. It cannot effectively drive our cars, guide robots in our homes and hospitals, enable entirely new ways of immersive and interactive experiences for learning and recreation, or accelerate discovery in materials science and medicine.</p><p>The philosopher Wittgenstein once wrote that “the limits of my language mean the limits of my world.” I’m not a philosopher. But I know at least for AI, there is more than just words. Spatial intelligence represents the frontier beyond language—the capability that links imagination, perception and action, and opens possibilities for machines to truly enhance human life, from healthcare to creativity, from scientific discovery to everyday assistance.</p><p>So how do we build spatially-intelligent AI? What’s the path to models capable of reasoning with the vision of Eratosthenes, engineering with the precision of an industrial designer, creating with the imagination of a storyteller, and interacting with their environment with the fluency of a first responder?</p><p>Building spatially intelligent AI requires something even more ambitious than LLMs: world models, a new type of generative models whose capabilities of understanding, reasoning, generation and interaction with the semantically, physically, geometrically and dynamically complex worlds - virtual or real - are far beyond the reach of today’s LLMs. The field is nascent, with current methods ranging from abstract reasoning models to video generation systems. World Labs was founded in early 2024 on this conviction: that foundational approaches are still being established, making this the defining challenge of the next decade.</p><p><span>In this emerging field, what matters most is establishing the principles that guide development. For spatial intelligence, I define world models through </span><strong>three essential capabilities:</strong></p><p><span>World models that unlock spatial understanding and reasoning must also generate simulated worlds of their own. They must be capable of spawning endlessly varied and diverse simulated worlds that follow semantic or perceptual instructions—</span><em>while</em><span> remaining geometrically, physically, and dynamically consistent—whether representing real or virtual spaces. The research community is actively exploring whether these worlds should be represented implicitly or explicitly in terms of the innate geometric structures. Furthermore, in addition to powerful latent representations, I believe the outputs of a universal world model must also allow the generation of an explicit, observable state of the worlds for many different use cases. In particular, its understanding of the present must be tied coherently to its past; to the previous states of the world that led to the current one.</span></p><p><span>Just as animals and humans do, a world model should be able to process inputs—known as “prompts” in the generative AI realm—in a wide range of forms. Given partial information—whether images, videos, depth maps, text instructions, gestures, or actions—world models should predict or generate world states as </span><em>complete </em><span>as possible. This requires processing visual inputs with the fidelity of real vision while interpreting semantic instructions with equal facility. This enables both agents and humans to communicate with the model about the world through diverse inputs and receive diverse outputs in return.</span></p><p><span>Finally, if actions and/or goals are part of the prompt to a world model, its outputs must include the </span><em>next</em><span> state of the world, represented either implicitly or explicitly. When given only an action with or without a goal state as the input, the world model should produce an output consistent with the world’s previous state, the intended goal state if any, and its semantic meanings, physical laws, and dynamical behaviors. As spatially intelligent world models become more powerful and robust in their reasoning and generation capabilities, it is conceivable that in the case of a given goal, the world models themselves would be able to predict not only the next state of the world, but also the next actions based on the new state.</span></p><p><strong>The scope of this challenge exceeds anything AI has faced before.</strong></p><p>While language is a purely generative phenomenon of human cognition, worlds play by much more complex rules. Here on Earth, for instance, gravity governs motion, atomic structures determine how light produces colors and brightness, and countless physical laws constrain every interaction. Even the most fanciful, creative worlds are composed of spatial objects and agents that obey the physical laws and dynamical behaviors that define them. Reconciling all of this consistently—the semantic, the geometric, the dynamic, and physical—demands entirely new approaches. The dimensionality of representing a world is vastly more complex than that of a one-dimensional, sequential signal like language. Achieving world models that deliver the kind of universal capabilities we enjoy as humans will require overcoming several formidable technical barriers. At World Labs, our research teams are devoted to making fundamental progress toward that goal.</p><p>Here are some examples of our current research topics:</p><ul><li><p><strong>A new, universal task function for training: </strong><span>Defining a universal task function as simple and elegant as next-token prediction in LLMs has long been a central goal of world model research. The complexities of both their input and output spaces make such a function inherently more difficult to formulate. But while much remains to be explored, this objective function and corresponding representations must reflect the laws of geometry and physics, honoring the fundamental nature of world models as grounded representations of both imagination and reality.</span></p></li><li><p><strong>Large-scale training data</strong><span>:</span><strong> </strong><span>Training world models requires far more complex data than text curation. The promising news: massive data sources already exist. Internet-scale collections of images and videos represent abundant, accessible training material—the challenge lies in developing algorithms that can extract deeper spatial information from these two-dimensional image or video frame-based signals (i.e. RGB). Research over the past decade has shown the power of scaling laws linking data volume and model size in language models; the key unlock for world models is building architectures that can leverage existing visual data at comparable scale. In addition, I would not underestimate the power of high-quality synthetic data and additional modalities like depth and tactile information. They supplement the internet scale data in critical steps of the training process. But the path forward depends on better sensor systems, more robust signal extraction algorithms, and far more powerful neural simulation methods.</span></p></li><li><p><strong>New model architecture and representational learning: </strong><span>World model research will inevitably drive advances in model architecture and learning algorithms, particularly beyond the current MLLM and video diffusion paradigms. Both of these typically tokenize data into 1D or 2D sequences, which makes simple spatial tasks unnecessarily difficult - like counting unique chairs in a short video, or remembering what a room looked like an hour ago. Alternative architectures may help, such as 3D or 4D-aware methods for tokenization, context, and memory. For example, at World Labs, our recent work on a real-time generative frame-based model called RTFM has demonstrated this shift, which uses spatially-grounded frames as a form of spatial memory to achieve efficient real-time generation while maintaining persistence in the generated world.</span></p></li></ul><p>Clearly, we are still facing daunting challenges before we can fully unlock spatial intelligence through world modeling. This research isn’t just a theoretical exercise. It is the core engine for a new class of creative and productivity tools. And the progress within World Labs has been encouraging. We recently shared with a limited number of users a glimpse of Marble, the first ever world model that can be prompted by multimodal inputs to generate and maintain consistent 3D environments for users and storytellers to explore, interact with, and build further in their creative workflow. And we are working hard to make it available to the public soon!</p><p>Marble is only our first step in creating a truly spatially intelligent world model. As the progress accelerates, researchers, engineers, users, and business leaders alike are beginning to recognize its extraordinary potential. The next generation of world models will enable machines to achieve spatial intelligence on an entirely new level—an achievement that will unlock essential capabilities still largely absent from today’s AI systems.</p><p><strong>It matters what motivates the development of AI. </strong><span>As one of the scientists who helped usher in the era of modern AI, my motivation has always been clear: AI must augment human capability, not replace it. For years, I’ve worked to align AI development, deployment, and governance with human needs. Extreme narratives of techno-utopia and apocalypse are abundant these days, but I continue to hold a more pragmatic view: AI is developed by people, used by people, and governed by people. It must always respect the agency and dignity of people. Its magic lies in extending our capabilities; making us more creative, connected, productive, and fulfilled. Spatial intelligence represents this vision—AI that empowers human creators, caregivers, scientists, and dreamers to achieve what was once impossible. This belief is what drives my commitment to spatial intelligence as AI’s next great frontier. </span></p><p>The applications of spatial intelligence span varying timelines. Creative tools are emerging now—World Labs’ Marble already puts these capabilities in creators’ and storytellers’ hands. Robotics represents an ambitious mid-term horizon as we refine the loop between perception and action. The most transformative scientific applications will take longer but promise a profound impact on human flourishing.</p><p>Across all these timelines, several domains stand out for their potential to reshape human capability. It will take significant collective effort, more than a single team or a company can possibly achieve. It will require participation across the entire AI ecosystem—researchers, innovators, entrepreneurs, companies, and even policymakers—working toward a shared vision. But this vision is worth pursuing. Here’s what that future holds:</p><p>“Creativity is intelligence having fun.” This is one of my favorite quotes by my personal hero Albert Einstein. Long before written language, humans told stories—painted them on cave walls, passed them through generations, built entire cultures on shared narratives. Stories are how we make sense of the world, connect across distance and time, explore what it means to be human, and most importantly, find meaning in life and love within ourselves. Today, spatial intelligence has the potential to transform how we create and experience narratives in ways that honor their fundamental importance, and extend their impacts from entertainment to education, from design to construction.</p><p>World Labs’ Marble platform will be putting unprecedented spatial capabilities and editorial controllability in the hands of filmmakers, game designers, architects, and storytellers of all kinds, allowing them to rapidly create and iterate on fully explorable 3D worlds without the overhead of conventional 3D design software. The creative act remains as vital and human as ever; the AI tools simply amplify and accelerate what creators can achieve. This includes:</p><ul><li><p>Narrative experiences in new dimensions: Filmmakers and game designers are using Marble to conjure entire worlds without the constraints of budget or geography, exploring varieties of scenes and perspectives that would have been intractable to explore within a traditional production pipeline. As the lines between different forms of media and entertainment blur, we’re approaching fundamentally new kinds of interactive experiences that blend art, simulation, and play—personalized worlds where anyone, not just studios, can create and inhabit their own stories. With the rise of newer, more rapid ways to lift concepts and storyboards into full experiences, narratives will no longer be bound to a single medium, with creators free to build worlds with shared throughlines across myriad surfaces and platforms.</p></li><li><p>Spatial narratives through design: Essentially every manufactured object or constructed space must be designed in virtual 3D before its physical creation. This process is highly iterative and costly in terms of both time and money. With spatially intelligent models at their disposal, architects can quickly visualize structures before investing months into designs, walking through spaces that don’t yet exist—essentially telling stories about how we might live, work, and gather. Industrial and fashion designers can translate imagination into form instantly, exploring how objects interact with human bodies and spaces.</p></li><li><p>New immersive and interactive experiences: Experience itself is one of the deepest ways that we, as a species, create meaning. For the entirety of human history, there has been one singular 3D world: the physical one we all share. Only in recent decades, through gaming and early virtual reality ( VR), have we begun to glimpse what it means to share alternate worlds of our own creation. Now, spatial intelligence combined with new form factors, like VR and extended reality (XR) headsets and immersive displays, elevates these experiences in unprecedented ways. We’re approaching a future where stepping into fully realized multi-dimensional worlds becomes as natural as opening a book. Spatial intelligence makes world-building accessible not just to studios with professional production teams but to individual creators, educators, and anyone with a vision to share.</p></li></ul><p>Animals from insects to humans depend on spatial intelligence to understand, navigate and interact with their worlds. Robots will be no different. Spatially-aware machines have been the dream of the field since its inception, including my own work with my students and collaborators at my Stanford research lab. This is also why I’m so excited by the possibility of bringing them about using the kinds of models World Labs is building.</p><ul><li><p><strong>Scaling robotic learning via world models:</strong><span> The progress of robotic learning hinges on a scalable solution of viable training data. Given the enormous state spaces of possibilities that robots have to learn to understand, reason, plan, and interact with, many have conjectured that a combination of internet data, synthetic simulation, and real-world capture of human demonstration are required to truly create generalizable robots. But unlike language models, training data is scarce for today’s robotic research. World models will play a defining role in this. As they increase their perceptual fidelity and computational efficiency, outputs of world models can rapidly close the gap between simulation and reality. This will in turn help train robots across simulations of countless states, interactions and environments.</span></p></li><li><p><strong>Companions and collaborators: </strong><span>Robots as human collaborators, whether aiding scientists at the lab bench or assisting seniors living alone, can expand part of the workforce in dire need of more labour and productivity. But doing so demands spatial intelligence that perceives, reasons, plans, and acts while—and this is most important—staying empathetically aligned with human goals and behaviors. For instance, a lab robot might handle instruments so the scientist can focus on tasks needing dexterity or reasoning, while a home assistant might help an elderly person cook without diminishing their joy or autonomy. Truly spatially intelligent world models that can predict the next state or possibly even actions consistent with this expectation are critical for achieving this goal.</span></p></li><li><p><strong>Expanding forms of embodiment: </strong><span>Humanoid robots play a role in the world we’ve built for ourselves. But the full benefit of innovation will come from a far more diverse range of designs: nanobots that deliver medicine, soft robots that navigate tight spaces, and machines built for the deep sea or outer space. Whatever their form, future spatial intelligence models must integrate both the environments these robots inhabit and their own embodied perception and movement. But a key challenge in developing these robots is the lack of training data in these wide varieties of embodied form factors. World models will play a critical role in simulation data, training environments, and benchmarking tasks for these efforts.</span></p></li></ul><p>In addition to creative and robotics applications, spatial intelligence’ profound impact will also extend to fields where AI can enhance human capability in ways that save lives and accelerate discovery. I highlight below three areas of applications that can be deeply transformative, though it goes without saying the use cases of spatial intelligence are truly expansive across many more industries.</p><p><span>In </span><strong>scientific research,</strong><span> spatially intelligent systems can simulate experiments, test hypotheses in parallel, and explore environments inaccessible to humans—from deep oceans to distant planets. This technology can transform computational modeling in fields like climate science and materials research. By integrating multi-dimensional simulation with real-world data collection, these tools can lower compute barriers and extend what every laboratory can observe and understand.</span></p><p><span>In </span><strong>healthcare</strong><span>, spatial intelligence will reshape everything from laboratory to bedside. At Stanford, my students and collaborators have spent many years working with hospitals, elder care facilities, and patients at home. This experience has convinced me of spatial intelligence’s transformative potential here. AI can accelerate drug discovery by modeling molecular interactions in multi-dimensions, enhance diagnostics by helping radiologists spot patterns in medical imaging, and enable ambient monitoring systems that support patients and caregivers without replacing the human connection that healing requires, not to mention the potential of robots in helping our healthcare workers and patients in many different settings.</span></p><p><span>In </span><strong>education,</strong><span> spatial intelligence can enable immersive learning that makes abstract or complex concepts tangible, and create iterative experiences so essential to how our brains and bodies are wired in learning. In the age of AI, the need for faster and more effective learning and reskilling is particularly important for both school-aged children and adults. Students can explore cellular machinery or walk through historical events in multi-dimenality. Teachers gain tools to personalize instruction through interactive environments. Professionals—from surgeons to engineers—can safely practice complex skills in realistic simulations.</span></p><p>Across all these domains, the possibilities are boundless, but the goal remains constant: AI that augments human expertise, accelerates human discovery, and amplifies human care—not replacing the judgment, creativity, and empathy that are central for being humans.</p><p>The last decade has seen AI become a global phenomenon and an inflection point in technology, the economy, and even geopolitics. But as a researcher, educator, and now, entrepreneur, it’s still the spirit behind Turing’s 75-year-old question that inspires me most. I still share his sense of wonder. It’s what energizes me every day by the challenge of spatial intelligence.</p><p>For the first time in history, we’re poised to build machines so in tune with the physical world that we can rely on them as true partners in the greatest challenges we face. Whether accelerating how we understand diseases in the lab, revolutionizing how we tell stories, or supporting us in our most vulnerable moments due to sickness, injury, or age, we’re on the cusp of technology that elevates the aspects of life we care about most. This is a vision of deeper, richer, more empowered lives.</p><p>Almost a half billion years after nature unleashed the first glimmers of spatial intelligence in the ancestral animals, we’re lucky enough to find ourselves among the generation of technologists who may soon endow machines with the same capability—and privileged enough to harness those capabilities for the benefits of people everywhere. Our dreams of truly intelligent machines will not be complete without spatial intelligence.</p><p><span>This quest is my North Star. </span><a href="https://www.worldlabs.ai/" rel="">Join me</a><span> in pursuing it.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using Generative AI in Content Production (148 pts)]]></title>
            <link>https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production</link>
            <guid>45879793</guid>
            <pubDate>Mon, 10 Nov 2025 19:28:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production">https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production</a>, See on <a href="https://news.ycombinator.com/item?id=45879793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="content">

        <p><br><a href="#h_01K39BXBFJKBC2HEXRP00QCG8J"><em>Skip to Translations</em></a></p><h2 id="h_01K1BTNMB992RYKRQRNKWD40A6">
    <span><span><strong>Introduction&nbsp;</strong></span></span>
  </h2><p><span>Generative AI tools (GenAI)&nbsp; that allow users to rapidly generate new and creatively unique media (video, sound, text, and image) are increasingly being used across creative workflows in Content Production. At Netflix, we see these tools as valuable creative aids when used transparently and responsibly.</span></p><p>
    <span>This guidance helps filmmakers, production partners, and vendors understand when and how to use GenAI tools in production. It also offers</span><a href="#h_01K1BTNMC4RTXXMXPKW2TJJ2ZJ"><span><span><span> a practical tool</span></span></span></a><span> for assessing and enabling confident GenAI use when producing content for Netflix.&nbsp;</span>
  </p><p><span>To support global productions and stay aligned with best practices, we expect all production partners to share any intended use of GenAI with their Netflix contact, especially as new tools continue to emerge with different capabilities and risks.&nbsp;</span></p><p><span>Most low-risk use cases that follow the guiding principles below are unlikely to require legal review. However, if the output includes final deliverables, talent likeness, personal data, or third-party IP, written approval will be required before you proceed.</span></p><hr><h2 id="h_01K1BTQC952EKYY3T7SQ98G4NJ">
    <span><span><strong>TABLE OF CONTENTS</strong></span></span>
  </h2>
  <p>
    <a href="#h_01K1BTNMBC8DKAF1607XQ3S9AK"><span><span>Guiding Principles</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBHW1JSCHK4914BAXDE"><span><span>What use cases always require written approval?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBHB5GW4FJW8X64KHNN"><span><span>1. Data Use</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBKMP99CP2PCXZ8W3H5"><span><span>2. Creative Output</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBP76KFGEWAFWAH22TA"><span><span>3. Talent &amp; Performance</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBRSBGNTCCTY9DBXMV2"><span><span>4. Ethics &amp; Representation</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBS130Y200ZWV3H6ZAT"><span><span>How can I ensure confidentiality and data protection?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBVFQYQNJCCMKR254VK"><span><span>Are the considerations different for final output vs temporary media?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBWWPTJJA79EFPY8NRJ"><span><span>What should we consider before using GenAI for talent enhancement?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMC21630W4ZWFFS0EYP2"><span><span>What if I’m using a custom workflow or working with a vendor who is?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMC3K7ECQKP84CDSQVZG"><span><span>Appendix</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMC4RTXXMXPKW2TJJ2ZJ"><span><span>Proposed Use Case Matrix</span></span></a>
  </p><hr><h2 id="h_01K1BTNMBC8DKAF1607XQ3S9AK">
    <span><span><strong>Guiding Principles&nbsp;</strong></span></span>
  </h2><p><span>Given the sensitivities surrounding the use of these tools and the evolving legal landscape, it is essential to act responsibly when employing generative workflows. Netflix asks partners to consider the following guiding principles before leveraging GenAI in any creative workflow:&nbsp;</span></p><ol>
    <li>
      <span>The outputs do not replicate or substantially recreate identifiable characteristics of unowned or copyrighted material, or infringe any copyright-protected works</span>
    </li>
    <li>
      <span>The generative tools used do not store, reuse, or train on production data inputs or outputs.</span>
    </li>
    <li>
      <span>Where possible, generative tools are used in an </span><a href="#h_01K1BTNMBS130Y200ZWV3H6ZAT"><span><span><span>enterprise-secured environment</span></span></span></a><span> to safeguard inputs.</span>
    </li>
    <li>
      <span>Generated material is temporary and not part of the</span><a href="#h_01K1BTNMBVFQYQNJCCMKR254VK"><span> </span><span><span><span>final deliverables</span></span></span></a><span>.</span>
    </li>
    <li>
      <span>GenAI is not used to replace or generate new </span><a href="#h_01K1BTNMBWWPTJJA79EFPY8NRJ"><span><span><span>talent performances</span></span></span><span><span> </span></span></a><span>or union-covered work without consent.</span>
    </li>
  </ol><p><span>If you can confidently say "yes" to all the above, socializing the intended use with your Netflix contact may be sufficient. If you answer “no” or “unsure” to any of these principles, escalate to your Netflix contact for more guidance before proceeding, as written approval may be required.&nbsp;</span></p><p>
    <span>If your partner vendor is using a custom GenAI workflow — meaning a pipeline built from multiple tools or models — the same principles apply. More details can be found </span><a href="#h_01K1BTNMC21630W4ZWFFS0EYP2"><span><span><span>here</span></span></span></a><span>.&nbsp;</span>
  </p><hr><h2 id="h_01K1BTNMBHW1JSCHK4914BAXDE">
    <span><span><strong>What use cases always require written approval?</strong></span></span>
  </h2><p><span>Below are a few examples of situations that, in addition to reporting intended use, always require escalation and written approval before proceeding.&nbsp;</span></p><h4 id="h_01K1BTNMBHB5GW4FJW8X64KHNN">
    <span><span><strong>1. Data Use&nbsp;</strong></span></span>
  </h4><p><span>Protecting personal data and creative rights is essential when working with GenAI. These tools often require input data to generate outputs, and how that data is handled matters. Before using any GenAI tool, especially third-party or off-the-shelf options, consider whether you are using material that requires special handling, clearance, or consent.</span></p><ul>
<li data-list-item-id="eadab2bf1f9f6c9e08da9a9e4c1e9e80b"><span>Use of Proprietary or Personal Information: Do not input Netflix-owned materials (e.g., unreleased assets, scripts, production images) or personal data (e.g., cast or crew details) into tools unless explicitly approved.</span></li>
<li data-list-item-id="eebe54d8bab43f122ac308fb7d5628a48"><span>Third-Party or Unowned Talent Assets: Do not train or fine-tune models using material from artists, performers, or other rights holders unless you have the proper legal clearance.</span></li>
</ul><p><span>Example: Training an image model in the style of another artist using a library of their past work, where Netflix or the talent has not cleared rights.</span></p><h4 id="h_01K1BTNMBKMP99CP2PCXZ8W3H5">
    <span><span><strong>2. Creative Output&nbsp;&nbsp;</strong></span></span>
  </h4><p><span>AI-generated content must be used with care, especially when it forms a visible or story-critical part of the production. Whether you're designing a world, a character, or artwork that appears in a scene, the same creative and legal standards apply as with traditionally produced assets.</span></p><ul>
    <li>
      <span>Generation of Key Creative Elements: GenAI should not be used to generate main characters, key visual elements, or fictional settings that are central to the story without written approval.</span>
      <ul>
        <li>
          <span>Examples: GenAI is used to generate a second killer doll to play the red light/green light game with Young-hee in Squid Game.</span>
        </li>
      </ul>
    </li>
    <li>
      <span>Copyrighted or Estate-Controlled: Avoid using inputs (e.g., prompts, images) that reference copyrighted materials or likenesses of public figures or deceased individuals without appropriate permissions.</span>
      <ul>
        <li>
          <span>Example: “Create an image inspired by </span><a href="https://www.stevemccurry.com/posters/p/afghan-girl"><span><span><span>McCurry’s Afghan Girl</span></span></span></a><span>” or referencing distinctive features of a known performer (e.g., “Create a character with Meryl Streep’s nose”).</span>
        </li>
      </ul>
    </li>
  </ul><h4 id="h_01K1BTNMBP76KFGEWAFWAH22TA">
    <span><span><strong>3. Talent &amp; Performance&nbsp;</strong></span></span>
  </h4><p><span>Respect for performers and their work is foundational to the responsible use of GenAI. Whether enhancing a recorded performance or generating a digital likeness, the threshold for consent and care is exceptionally high when the intent or character of a performance may be altered.</span></p><ul>
<li data-list-item-id="e4d40cb1df5caa3a3682badc50d00f86f"><span>Synthetic or Digital Replicas - Do not create digital performers, voices, or likenesses of real talent without explicit and documented consent and complying with guild requirements (where applicable).</span></li>
<li data-list-item-id="edbd78f2c674b7bf80443280ebdfc5961"><span>Significant Digital Alterations to Performances - Be cautious when making changes that affect a performance's emotional tone, delivery, or intent, as even subtle edits may have legal or reputational implications.</span></li>
</ul><p><span>Examples include visual ADR (altering lip-sync or facial performance to match new, unscripted dialogue).</span></p><h4 id="h_01K1BTNMBRSBGNTCCTY9DBXMV2">
    <span><span><strong>4. Ethics &amp; Representation</strong></span></span>
  </h4><p><span>Audiences should be able to trust what they see and hear on screen. GenAI (if used without care) can blur the line between fiction and reality or unintentionally mislead viewers. That’s why we ask you to consider both the intent and the impact of your AI-generated content.</span></p><ul>
<li data-list-item-id="e888ce79e22790a32566543967d72f687">
<span>Misleading or Misrepresentative Content: Avoid creating content that could be mistaken for real events, people, or statements if they never actually occurred (e.g., fabricated footage, dialogue, or scenes presented as authentic).</span><ul><li data-list-item-id="ed46505213321fa75e536d07d6eb5161d"><span>Example: using GenAI to create a fake news segment featuring a real journalist delivering a fabricated statement, even if intended as background.</span></li></ul>
</li>
<li data-list-item-id="ee334b51a1afff56655a98d7ab8cd583e"><span>Impact on Union Roles: Ensure that your use of GenAI does not replace or materially impact work typically done by union-represented individuals, including actors, writers, or crew members, without proper approvals or agreements.</span></li>
</ul><hr><h2 id="h_01K1BTNMBS130Y200ZWV3H6ZAT">
    <span><span><strong>How can I ensure confidentiality and data protection?</strong></span></span>
  </h2><p><span>The use of tools covered by Netflix Enterprise Agreements provides an additional level of security to protect input data. Speak with your Netflix primary contact about available tools and the onboarding process. These tools:</span></p><ul>
<li data-list-item-id="e68332c5bcee6dff16288681e7b954f32"><span>Prevent capture, training, or resale of your inputs</span></li>
<li data-list-item-id="ee904f7888a03c5358236991d6cc536f9"><span>Protect sensitive inputs like scripts, production images, or talent visuals</span></li>
</ul><p><span>Even with secure tools, any use of sensitive information (e.g., talent likeness, unreleased footage, contracts) requires escalation to your Netflix contact.</span></p><p><span>When not using enterprise tools, ensure that any AI tools, plugins, or workflows you use do not train on inputs or outputs, as using the wrong license tier or missing pre-negotiated data terms could compromise confidentiality. You are responsible for reviewing the terms and conditions (T&amp;Cs). Please check with your Netflix contact if you have any further questions.</span></p><h2 id="h_01K1BTNMBVFQYQNJCCMKR254VK">
    <span><span><strong>Are the considerations different for final output vs temporary media?</strong></span></span>
  </h2><p><span>If created with GenAI, content that appears in the final cut—even in the background—can raise legal, copyright, or trust issues with the audience. That’s why we ask you to flag any GenAI-generated elements early, especially if they will be seen or heard on screen.</span></p><p><span>If your proposed use case includes visual, audio, or text elements generated by AI (e.g., posters, documents, signage, or news clippings), contact your Netflix representative as early as possible for legal guidance. These items may require rights clearance before they can be included in final deliverables.</span></p><p><span>Some GenAI-generated props or set pieces may be considered incidental, for example, a historical document shown briefly in the background and not referenced in the scene. However, if the element is prominent (e.g., a character reads it aloud or it contributes to the story), it must be treated with greater care.</span></p><p><span>In these cases, you can use GenAI to explore ideas or mockups. Still, the final version should involve meaningful human input and follow the legal review process through your Netflix contact.</span></p><hr><h2 id="h_01K1BTNMBWWPTJJA79EFPY8NRJ">
    <span><span><strong>What should we consider before using GenAI for talent enhancement?</strong></span></span>
  </h2><p><span>There is a long tradition of digitally altering performances in post-production and VFX. However, the use of AI to modify or replicate a performer's likeness or voice introduces new legal, ethical, and reputational challenges. Therefore, obtaining consent when appropriate and exercising caution are crucial. Many talent enhancement use cases require legal review, so please plan accordingly. Here are some guidelines to consider:&nbsp;</span></p><ul><li data-list-item-id="ee650de448e890ec3259cea9e3f215d7e"><span>If creating a Digital Replica (i.e., a generated output recognizable as the voice and/or likeness of an identifiable performer for the purpose of portraying them in photography or soundtrack, they did not perform), consent is required. No further consent is needed to use the Digital Replica if the performance output: (1)&nbsp; remains substantially as scripted, performed, or recorded (e.g. reshoots); (2) depicts activities incapable of being performed by a human for safety reasons; or (3) results in the performer being unrecognizable (e.g. wearing a mask).</span></li></ul><ul><li data-list-item-id="eb553309b5b4c9280624005c8b8dc3be7">
<span>Digital Alterations: Consent is generally required for digital alterations, except for those customarily done in the entertainment and film industry, such as:</span><ul>
<li data-list-item-id="eabb77a1e3013a0ba03166f275032945d"><span>Alterations where the photography or soundtrack remains substantially as scripted, performed, or recorded.</span></li>
<li data-list-item-id="efca531cce7e6caef547165f10ea1d4b8"><span>Post-production changes for cosmetics, wardrobe, noise reduction, timing, continuity, pitch, clarity, and similar purposes.</span></li>
<li data-list-item-id="eff26a574f34290ce1b409f9286abfc71"><span>Circumstances where dubbing or using a double is permitted under existing agreements.</span></li>
</ul>
</li></ul><ul>
<li data-list-item-id="e8933fe8b834666f88ca2ce7cbdcac1e3">
<span>Model Usage:</span><ul>
<li data-list-item-id="ed72a3c2d1a0ab668c016f69220395448"><span>Any models trained to perform talent enhancement manipulation should be used solely for the production in question and within the scope of work agreed upon with the talent.</span></li>
<li data-list-item-id="e6803efcc74ee2497779461493a153e3b"><span>Models must not be used to create an actor's performance in another production, pitch, or concept without the express consent of all parties involved.</span></li>
</ul>
</li>
<li data-list-item-id="e77b0d872f73d494f71a7f67478e8f380">
<span>Quality Assurance:</span><ul>
<li data-list-item-id="e53f9b4c6a212869163f0a58c2be99a55"><span>Perform early tests to ensure that the quality of the outputs is acceptable both creatively and technically, so as not to adversely affect the talent’s original performance.</span></li>
<li data-list-item-id="ed9502f1976cbd6ba189c9683079d054a"><span>Where applicable and practical, plan dedicated data capture sessions with the talent to ensure the best possible outcomes.</span></li>
<li data-list-item-id="ee61ab0b5412c6f71ec8bcbc931106d3d"><span>Avoid enhancements that could harm the actor’s reputation, dignity, or personal image.</span></li>
</ul>
</li>
</ul><p><span>By following these guidelines, you can navigate the complexities of using AI in creative workflows while respecting the rights and integrity of performers.</span></p><hr><h2 id="h_01K1BTNMC21630W4ZWFFS0EYP2">
    <span><span><strong>What if I’m using a custom workflow or working with a vendor who is?</strong></span></span>
  </h2><p><span>For vendors: If you're delivering work to Netflix using a custom GenAI workflow built from multiple tools, each step in the pipeline must meet our standards for data protection, consent, and content integrity as outlined in this document.&nbsp;</span></p><p><span>For production partners: If you're hiring a vendor or AI studio, use this guidance as a framework to help assess how they manage data, creative control, and final outputs. If you are unsure whether the pipeline meets the expectations outlined in this guidance, seek guidance from your Netflix contact.&nbsp;</span></p><hr><h2 id="h_01K1BTNMC3K7ECQKP84CDSQVZG">
    <span><span><strong>Appendix</strong></span></span>
  </h2>
  <h3 id="h_01K1BTNMC4RTXXMXPKW2TJJ2ZJ">
    <span><span><strong>Proposed Use Case Matrix</strong></span></span>
  </h3><p><span>We have provided a&nbsp; Proposed Use Case Matrix at the end of this guidance as a tool to triage your proposed use case quickly.&nbsp;</span></p><div><figure><table>
<colgroup>
<col>
<col>
<col>
</colgroup>
<tbody>
<tr>
<td><span><strong>Proposed Use Case</strong></span></td>
<td><span><strong>Action&nbsp;</strong></span></td>
<td><span><strong>Rationale</strong></span></td>
</tr>
<tr>
<td><span>Using GenAI for ideation only (moodboards, reference images)</span></td>
<td><span>✅</span></td>
<td><span>Low risk, non-final, likely not needing escalation if guiding principles are followed.</span></td>
</tr>
<tr>
<td><span>Using GenAI to generate background elements (e.g., signage, posters) that appear on camera</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813746061587" alt=":warning:" width="23" height="23"></span></td>
<td><span>Use judgment: Incidental elements may be low risk, but if story-relevant, please escalate.&nbsp;</span></td>
</tr>
<tr>
<td><span>Using GenAI to create final character designs or key visuals</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813724428819" alt=":octagonal_sign:" width="23" height="23">&nbsp;</span></td>
<td><span>Requires escalation as it could impact legal rights, audience perception, or union roles.</span></td>
</tr>
<tr>
<td><span>Using GenAI for talent replication (re-ageing, or synthetic voices)</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813724428819" alt=":octagonal_sign:" width="23" height="23"></span></td>
<td><span>Requires escalation for consent and legal review.&nbsp;</span></td>
</tr>
<tr>
<td><span>Using unowned&nbsp; training data (e.g., celebrity faces, copyrighted art)</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813724428819" alt=":octagonal_sign:" width="23" height="23"></span></td>
<td><span>Needs escalation due to copyright and other rights risk.</span></td>
</tr>
<tr>
<td><span>Using Netflix's proprietary material</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813746061587" alt=":warning:" width="23" height="23"></span></td>
<td>
<p><span>Needs escalation for review if outside secure enterprise tools.</span></p>

</td>
</tr>
</tbody>
</table></figure></div><h3 id="h_01K39BXBFJKBC2HEXRP00QCG8J">
  <br>
  <span><span><strong>Translations</strong></span></span>
</h3><p><a href="https://drive.google.com/file/d/1OhOJXv6cwcc8Ob61K_zuxOxxzJst5V-V/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Español (Latinoamérica)</span></a></p><p><a href="https://drive.google.com/file/d/1PcVBUivo-CYSIj_fXMjPwS9lh6WC9WlT/view?usp=sharing" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Français</span></a></p><p><a href="https://drive.google.com/file/d/16v2PFCYKk08s0o37kHJ4LZmr6n5cdxia/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Português</span></a></p><p><a href="https://drive.google.com/file/d/122a0P_EASxSQ2CklK2Tnm6N4CjrbC92h/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">ไทย</span></a></p><p><a href="https://drive.google.com/file/d/1Gr8ml-b9QSoWLkZkN18p03uNVQs5Aj2X/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Türkçe</span></a></p><p><a href="https://drive.google.com/file/d/1wU1Q6zTd7zt7G7umgtk2VRepkGAwruA8/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">繁體中文</span></a></p>
        
        
        <section>

          <span>Was this article helpful?</span>
          
          <small>
            <span data-helper="vote" data-item="article" data-type="label" data-id="43393929218323" data-upvote-count="121" data-vote-count="128" data-vote-sum="114" data-vote-url="/hc/en-us/articles/43393929218323/vote" data-value="null" data-label="121 out of 128 found this helpful">121 out of 128 found this helpful</span>
          </small>
        </section>
        
   
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Redmond, WA, turns off Flock Safety cameras after ICE arrests (335 pts)]]></title>
            <link>https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/</link>
            <guid>45879101</guid>
            <pubDate>Mon, 10 Nov 2025 18:30:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/">https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/</a>, See on <a href="https://news.ycombinator.com/item?id=45879101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
    <p>Redmond police turned off their city’s Flock Safety cameras last week after masked, unidentified officers — later identified as U.S. Immigration and Customs Enforcement agents — arrested seven people, at least three less than a mile from  one or more of the city’s cameras.</p><p>During a City Council session Oct. 27, Redmond police Chief Darrell Lowe said no federal agency had accessed the city’s Flock data, but agreed to suspend officers’ access to the system until city officials had discussed ending Redmond’s contract with the company.&nbsp;</p><p>Redmond City Council member Angie Nuevacamina said Thursday the proximity of the arrests to some of the city’s cameras was coincidental, and not because ICE had “somehow tapped into” Redmond’s Flock cameras or data. The city suspended its Flock system because city officials could not guarantee they wouldn’t be forced to release data collected by those devices someday, she said.</p><p>Their concerns may have been prescient.</p><p>On Thursday, a Skagit County Superior Court judge ruled that pictures taken by Flock cameras in the cities of Sedro-Woolley and Stanwood qualify as public records, and therefore must be released as required by the state’s Public Records Act, court records show<strong>.</strong></p><p>Flock’s cameras, also called automated license plate readers, continuously and indiscriminately capture time- and location-stamped photos of any passing vehicles. Those images are then stored, and information about the vehicles, including their condition, make, model and license plate number, is added to a searchable database controlled by the customer.</p><p>Last week’s Skagit County ruling could oblige the dozens of Washington police agencies which use Flock cameras, ostensibly to help them find stolen vehicles, crime suspects and missing people, to release the photos and data they collect — an outcome privacy advocates warned was possible.</p>
<div>
      <h3>
        Related
        
      </h3>
      

      
        
    

      </div><p>The ruling also exacerbated concerns about potential misuse of Flock data, which swelled after University of Washington researchers released <a href="https://www.seattletimes.com/seattle-news/law-justice/feds-searched-flock-security-systems-at-18-wa-police-agencies-report/">a report Oct. 21</a> showing federal immigration agencies like ICE and Border Patrol had accessed the data of at least 18 Washington cities, often without their police departments’ knowing. The report raised concerns that the agencies might be using the data to target and arrest immigrants as part of Trump’s immigration crackdown.</p><p>Redmond was the latest in a string of Flock-using, Seattle-area cities to <a href="https://www.seattletimes.com/seattle-news/law-justice/feds-searched-flock-security-systems-at-18-wa-police-agencies-report/">change their surveillance programs</a> in the last three weeks in response to those concerns.</p><p>Police officials in Renton, Auburn, Mukilteo and Lakewood, Pierce County, changed their Flock settings after the UW report showed federal agencies had accessed their Flock data. The agencies never requested permission to access their cities’ data, and the respective police departments weren’t aware it happened until after UW researchers notified them or they saw the report, the officials said.</p><p>Redmond’s Police Department was not among those listed in the report, and has never allowed external agencies to access their Flock data without requesting and receiving permission from the police chief first, according to an <a href="https://www.redmond.gov/CivicAlerts.aspx?AID=2698" target="_blank">Oct. 24 statement</a> by Lowe.</p><p>But concerns about the cameras, and potential misuse of the data they collect, still swirled among Redmond residents and City Council members after photos and videos began circulating online last Monday of masked, unidentified officers emerging from unmarked cars and arresting people in town.</p><p>Three arrests by ICE at Redmond’s Bear Creek Village shopping center, the parking lot of a Home Depot and near the intersection of Avondale Road Northeast and Novelty Hill Road Northeast all happened within a half-mile of at least one of Redmond’s 24 Flock cameras, according to a partial list of their locations provided through a public disclosure request.</p>
<p>The city installed most of its cameras on Redmond’s “main thoroughfares” in May and began using them in June, according to Nuevacamina and Redmond police spokesperson Jill Green. </p><p>Though Lowe confirmed no federal agencies had accessed Redmond’s Flock system, Nuevacamina said residents’ and city officials’ concerns about the technology were still strong enough to support turning the cameras off. The step was also one of the few things city officials could do to help residents who felt powerless that day, as Redmond’s police officers were not allowed to intervene in ICE activity, and residents could not either without risking arrest or their own personal safety, she said.</p><p>Turning off Redmond’s Flock cameras was what the city had in its “tool belt to be able to protect and stand up for (their) community,” Nuevacamina said.</p><p>In a statement Saturday, Tricia McLaughlin, the Department of Homeland Security’s assistant secretary, confirmed ICE agents arrested seven people Nov. 3 in Redmond. McLaughlin’s statement did not identify those arrested but accused all of them of being in the country illegally.</p><p>Flock Safety is communicating with Redmond city officials to address their concerns and hopefully convince them to reverse their decision, the company’s chief legal officer, Dan Haley, said in a phone call Friday. The company is also advocating for legislation that would prevent people from “taking advantage of” Washington’s public records law, Haley said.</p><p>Flock can be made to release data collected by its technology through a subpoena or court order, Haley said, but the company would not do so without notifying and involving its customer first. The cameras also only capture what anyone could see on a public road, where there is no legal expectation of privacy, he said.  </p>
<p>Communities must balance concerns they have about Flock cameras against what Haley called the “very real outcomes” of using the technology: “these are kidnapped kids returned home, elderly people with dementia found quickly.”</p><p>For now, all Redmond police officers’ access to the city’s Flock data has been turned off. Two Police Department employees can access the data, but only to fulfill public records requests, and two police administrators can access — but not search — the data. A “disconnect signal” was also sent Tuesday morning to all of the cameras, which will stop them from taking and storing photos, Redmond police officials said.</p><p>During the Oct. 27 City Council session, Lowe said he would meet with Redmond Mayor Angela Birney Nov. 18 to “see where we go from here.”</p><p>Redmond City Council President Vanessa Kristzer said she had “grave concerns” about continuing to use Flock’s technology.</p><p>“We will continue to explore every avenue to be able to keep our community members safe and their dignity and human rights protected,” Kristzer said.</p>    
        <div>
   <p><span>Catalina Gaitán</span>:       <span>206-464-8276</span> or <span><a href="mailto:cgaitan@seattletimes.com">cgaitan@seattletimes.com</a></span>. <span>Catalina Gaitán is a breaking news reporter at The Seattle Times.</span>   </p>
</div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Omnilingual ASR: Advancing automatic speech recognition for 1600 languages (134 pts)]]></title>
            <link>https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1</link>
            <guid>45878826</guid>
            <pubDate>Mon, 10 Nov 2025 18:10:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1">https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=45878826">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-scoped-css="fullscreen-hero-scoped-u_0_0_Qb" id="u_0_4_nK"><p>Open Source</p><p>November 10, 2025</p></div><div><div><p>Takeaways:</p><div><ul><li>We’re introducing <a href="https://github.com/facebookresearch/omnilingual-asr" target="_blank" data-lnfb-mode="ie"><u>Meta Omnilingual Automatic Speech Recognition (ASR)</u></a>, a suite of models providing automatic speech recognition capabilities for more than 1,600 languages, achieving state-of-the-art quality at an unprecedented scale.</li><li>Omnilingual ASR was designed as a community-driven framework. People around the world can extend Omnilingual ASR to new languages by using just a few of their own samples.</li><li>We’re also releasing the Omnilingual ASR Corpus, an extensive collection of transcribed speech in 350 underserved languages; Omnilingual wav2vec 2.0, a scaled up massively multilingual speech representation model; and a <a href="https://aidemos.atmeta.com/omnilingualasr" target="_blank" data-lnfb-mode="ie"><u>language exploration demo</u></a> people can explore languages covered by the model.</li></ul></div><div><p>Automatic speech recognition (ASR) systems aim to make spoken language universally accessible by transcribing speech into text that can be searched, analyzed, and shared. Currently, most automatic speech recognition systems focus on a limited set of high-resource languages that are well represented on the internet, often relying on large amounts of labeled data and human-generated metadata to achieve good performance. This means high-quality transcriptions are often unavailable for speakers of less widely represented or low-resource languages, furthering the digital divide.</p><p>Today, Meta’s Fundamental AI Research (FAIR) team is introducing <a href="https://github.com/facebookresearch/omnilingual-asr" target="_blank" data-lnfb-mode="ie"><u>Omnilingual ASR</u></a> — a groundbreaking suite of models that deliver automatic speech recognition for more than 1,600 languages, including 500 low-resource languages never before transcribed by AI. We’re also open sourcing Omnilingual wav2vec 2.0, a new self-supervised massively multilingual speech representation model scaled up to 7B parameters that can be leveraged for other downstream speech-related tasks. In addition, we’re releasing the Omnilingual ASR Corpus, a unique collection of transcribed speech in 350 underserved languages, curated in collaboration with our global partners.</p><p>This work supports our goal of building technology to help bring the world closer together. Omnilingual ASR is a significant step toward delivering a truly universal transcription system and expanding access to speech technology worldwide, ensuring that high-quality speech-to-text systems are accessible to even the most underrepresented language communities. The hope is to ultimately break down language barriers and enable communication across diverse linguistic and cultural backgrounds.</p></div><p>Beyond Multilinguality: Unprecedented Language Coverage and Performance</p><div><p>Automatic Speech Recognition has made strong progress in recent years, approaching near-perfect accuracy for many high-resource languages. However, expanding language coverage has been prohibitively resource intensive as current AI architectures are too data demanding to scale universally.</p><p>Omnilingual ASR addresses this research blocker by introducing two architectural variants. First, we scaled our previous wav2vec 2.0 speech encoder to 7B parameters for the first time, producing rich, massively multilingual semantic representations from raw, untranscribed speech data. We then built two decoder variants to map those into character tokens. The first decoder relies on a traditional connectionist temporal classification (CTC) objective, while the second leverages a traditional transformer decoder, commonly used in LLMs.</p></div></div><div><p><img src="https://scontent-cph2-1.xx.fbcdn.net/v/t39.2365-6/559212493_868065139217532_1757975235142574465_n.png?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=hXWf-YJrXR0Q7kNvwHGrT0F&amp;_nc_oc=AdnQhjTpZcypjjUuVdWWdRexcxeHFyn-bTOsskqaVbFQ1Hit3rgARzuVbZwtIeHM8A4&amp;_nc_zt=14&amp;_nc_ht=scontent-cph2-1.xx&amp;_nc_gid=ReRyiZMa_7nAphMwyAKPzg&amp;oh=00_AfgUQnN2pYDlk0LyGXF9pEVzDlOmzyF4HjYpC7Mt96483A&amp;oe=692D1C20" alt="" id="u_0_b_i1"></p></div><p>Dubbed LLM-ASR, this approach introduces a step change in ASR performance, especially for long tail languages. Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</p><div><p><img src="https://scontent-cph2-1.xx.fbcdn.net/v/t39.2365-6/576960745_1209180377769385_6702108631461736038_n.png?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=crHPI5fOvFoQ7kNvwE0Se4M&amp;_nc_oc=AdnYzyhKcwSVbSsLkSvM8ji0moMH4ZD4gFcSWwGKl8l8whUA_7kLTLNYBWQsR3-t3uE&amp;_nc_zt=14&amp;_nc_ht=scontent-cph2-1.xx&amp;_nc_gid=ReRyiZMa_7nAphMwyAKPzg&amp;oh=00_AfjjCyT2ERCTIGraDBUlslVC2qUNvzIlKJoxXlmBIAlQwQ&amp;oe=692D32A6" alt="" id="u_0_c_Uf"></p></div><div><p>Bring Your Own Language</p><div><h2>Beyond expanding to more than 1,600 languages, Omnilingual ASR also shifts the paradigm for how new languages can be brought into the fold. In most existing systems, languages not included at release time can only be added through expert-driven fine-tuning — a path inaccessible to most communities. Omnilingual ASR instead introduces the first large-scale ASR framework capable of extending to entirely new languages with just a few in-context examples.</h2><p>This is made possible by our LLM-inspired system, which brings in-context learning capabilities over from the field of LLMs. In practice, this means that a speaker of an unsupported language can provide only a handful of paired audio-text samples and obtain usable transcription quality — without training data at scale, onerous expertise, or access to high-end compute. While zero-shot performance cannot yet match that of fully trained systems, it offers a far more scalable path to bringing new languages into digital reach.</p></div></div><div><p><img src="https://scontent-cph2-1.xx.fbcdn.net/v/t39.2365-6/576939007_1511214026693639_7210936081314134566_n.png?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=njlqK_uhU8gQ7kNvwEdx5uB&amp;_nc_oc=AdkbDeeDpieXhZ_fK1oiQKFrGbMpOhyWULeLkAep5oOw0x9B-sG6prAVjgMHkp3SQRo&amp;_nc_zt=14&amp;_nc_ht=scontent-cph2-1.xx&amp;_nc_gid=ReRyiZMa_7nAphMwyAKPzg&amp;oh=00_AfjdkkK8B5EwpP92f0owW8Rq3VADbneSJRdpyj0S-pECdQ&amp;oe=692D1731" alt="" id="u_0_d_9g"></p></div><div><p>A Suite of Models for Various Use Cases</p><div><p>We’re releasing a full suite of models and one dataset. Built on the foundation of FAIR’s <a href="https://ai.meta.com/blog/multilingual-model-speech-recognition/" target="_blank" data-lnfb-mode="ie"><u>previous research</u></a>, Omnilingual ASR gives stakeholders everything they need to expand and improve speech technology for any language.</p><p>The two decoding variants are available as a versatile family of models — from lightweight 300M versions designed for low-power devices to powerful 7B models that offer top-tier accuracy for a variety of use cases. Our general-purpose speech foundation model wav2vec 2.0 is also made available at various sizes. It can be used by researchers and developers alike to enable speech-related tasks beyond ASR.</p><p>All assets are released under a permissive Apache 2.0 license while the data is provided under the CC-BY license and are based on FAIR’s open source <a href="https://github.com/facebookresearch/fairseq2" target="_blank" data-lnfb-mode="ie"><u>fairseq2</u></a> framework, empowering researchers, developers, and language advocates worldwide to advance and tailor speech solutions for their own use cases using the latest tools and technologies in the PyTorch ecosystem.</p></div><p>Built With Global Partners</p><div><p>Omnilingual ASR also advances the state of multilingual ASR along more familiar dimensions. Its training corpus is one of the largest ever assembled for ASR in both volume and linguistic diversity, integrating publicly available datasets with community-sourced speech recordings collected through multiple partnerships.</p><p>To reach languages with little or no digital presence, we worked with local organizations that recruited and compensated native speakers, often in remote or under-documented regions. We’re releasing this commissioned part of our training corpus as Omnilingual ASR Corpus to further benefit the ASR research community. To date, it is the largest ultra-low-resource spontaneous ASR dataset ever made available, covering hundreds of languages never seen before by ASR systems. Explore the languages in the dataset <a href="https://aidemos.atmeta.com/omnilingualasr/language-globe" target="_blank" data-lnfb-mode="ie"><u>here</u></a>.</p><p>Beyond commissioned partnerships, collaborations through the <a href="https://about.fb.com/news/2025/02/announcing-language-technology-partner-program/" target="_blank" data-lnfb-mode="ie"><u>Language Technology Partner Program</u></a> have brought together linguists, researchers, and language communities from around the world, providing essential expertise and resources. We joined forces with organizations such as Mozilla Foundation’s Common Voice and Lanfrica/NaijaVoices to work directly with local communities.</p><p>These partnerships have been instrumental in infusing Omnilingual ASR with deep linguistic knowledge and cultural understanding, ensuring that the technology meets local needs and empowers diverse language communities globally.</p></div><a href="https://github.com/facebookresearch/omnilingual-asr" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_e_H4"></a><a href="https://aidemos.atmeta.com/omnilingualasr/language-globe" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_f_G2"><div><p>Try the Language Exploration Demo</p><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 36 36" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M21.9502 15.4645L13.7579 23.6569C13.3674 24.0475 12.7342 24.0475 12.3437 23.6569C11.9532 23.2664 11.9532 22.6333 12.3437 22.2427L20.536 14.0503H15.8792C15.3269 14.0503 14.8792 13.6026 14.8792 13.0503C14.8792 12.498 15.3269 12.0503 15.8792 12.0503H22.9502C23.5025 12.0503 23.9502 12.498 23.9502 13.0503V20.1214C23.9502 20.6737 23.5025 21.1214 22.9502 21.1214C22.398 21.1214 21.9502 20.6737 21.9502 20.1214V15.4645Z" fill="CurrentColor"></path></svg></div></a><a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_g_tU"><div><p>Try the Transcription Tool</p><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 36 36" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M21.9502 15.4645L13.7579 23.6569C13.3674 24.0475 12.7342 24.0475 12.3437 23.6569C11.9532 23.2664 11.9532 22.6333 12.3437 22.2427L20.536 14.0503H15.8792C15.3269 14.0503 14.8792 13.6026 14.8792 13.0503C14.8792 12.498 15.3269 12.0503 15.8792 12.0503H22.9502C23.5025 12.0503 23.9502 12.498 23.9502 13.0503V20.1214C23.9502 20.6737 23.5025 21.1214 22.9502 21.1214C22.398 21.1214 21.9502 20.6737 21.9502 20.1214V15.4645Z" fill="CurrentColor"></path></svg></div></a><a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_h_sn"></a></div><div><p>Our latest updates delivered to your inbox</p><p><a href="https://ai.facebook.com/subscribe/" target="_blank">Subscribe</a> to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The lazy Git UI you didn't know you need (356 pts)]]></title>
            <link>https://www.bwplotka.dev/2025/lazygit/</link>
            <guid>45878578</guid>
            <pubDate>Mon, 10 Nov 2025 17:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bwplotka.dev/2025/lazygit/">https://www.bwplotka.dev/2025/lazygit/</a>, See on <a href="https://news.ycombinator.com/item?id=45878578">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>When my son was born last April, I had ambitious learning plans for the upcoming 5w paternity leave. As you can imagine, with two kids, life quickly verified this plan 🙃. I did eventually start <em>some</em> projects. One of the goals (sounding rebellious in the current AI hype cycle) was to learn and use <a href="https://neovim.io/" target="_blank" rel="noopener noreffer">neovim</a>
for coding. As a <a href="https://www.jetbrains.com/go" target="_blank" rel="noopener noreffer">Goland</a>
aficionado, I (and my wrist) have always been tempted by no-mouse, OSS, <a href="https://pkg.go.dev/golang.org/x/tools/gopls" target="_blank" rel="noopener noreffer">gopls</a>
based, highly configurable dev setups.</p>
<p>Long story short, I’d still stick to Goland for my professional coding (for now), but during the experiments with <code>nvim</code>, I accidentally stumbled upon <a href="https://github.com/jesseduffield/lazygit" target="_blank" rel="noopener noreffer">lazygit</a>
Git UI. I literally mistyped <code>&lt;space&gt;gg</code> instead of <code>gg</code>, which opened up the built-in <code>lazygit</code> overlay UI.</p>
<p>A week later, I have already switched all my <code>git</code> workflows to <code>lazygit</code> (also outside <code>nvim</code>), and I have been using it since then. In this post, I’d like to explain why it happened so quickly, so:</p>
<ul>
<li>What makes <code>lazygit</code> so special?</li>
<li>How can it make you more productive?</li>
<li>What we can all learn from <code>lazygit</code> around designing incredible software with seamless UX?</li>
</ul>
<p>Let’s jump in!</p>

<p>Likely every developer knows and (in some form) uses the <a href="https://git-scm.com/docs" target="_blank" rel="noopener noreffer">git CLI</a>
. It’s relatively simple, and it seems incredibly stable – the only change I noticed in the last decade was the new <code>git switch</code> command, although I still haven’t “switched” to it from the lovely <code>git checkout</code>🙃 .</p>
<p>As a result, it’s common to see developers memorize a few commands you typically use (e.g.<code>clone</code>, <code>fetch/pull</code>, <code>config/remote</code>, <code>add/rm</code>, <code>status</code>, <code>checkout</code>, <code>commit</code>, <code>push</code>, <code>cherry-pick</code>, <code>rebase</code>, <code>merge</code>, <code>log</code>) and stick to the CLI. In fact, in <a href="https://survey.stackoverflow.co/2022/#section-version-control-interacting-with-version-control-systems" target="_blank" rel="noopener noreffer">2022, 83% of the StackOverflow responders said they prefer CLI to other interfaces</a>
and that number is likely still quite high nowadays.</p>
<p>However, <a href="https://git-scm.com/downloads/guis" target="_blank" rel="noopener noreffer">graphical interfaces</a>
and generally other <code>git</code> compatible clients do exist:</p>
<ul>
<li>Some of them offer more or less the same <code>git</code> workflows as the original <code>git</code> CLI, just more visually appealing and with buttons/interactivity instead of remembering the CLI flags, e.g. <a href="https://git-scm.com/docs/git-gui" target="_blank" rel="noopener noreffer">git gui</a>
, <a href="https://github.com/apps/desktop" target="_blank" rel="noopener noreffer">GitHub Desktop</a>
or <code>lazygit</code> discussed here.</li>
<li>Other projects add more magic (e.g. AI), and potentially new light abstractions/workflows in an attempt to simplify or enhance <code>git</code> use e.g. <a href="https://www.gitkraken.com/" target="_blank" rel="noopener noreffer">GitKraken</a>
.</li>
<li>There are even projects like recently popular <a href="https://github.com/jj-vcs/jj" target="_blank" rel="noopener noreffer">jj</a>
tool that completely abstracts away <code>git</code> API and replace it with a new source control flows to “simplify” them or unify them across various <a href="https://en.wikipedia.org/wiki/Version_control" target="_blank" rel="noopener noreffer">VCS</a>
other than <code>git</code> (e.g. <code>mercurial</code>, Google <a href="https://www.youtube.com/watch?v=W71BTkUbdqE&amp;t=645s" target="_blank" rel="noopener noreffer">Piper</a>
and everything else you wished it was <code>git</code>, but it’s not 😛).</li>
</ul>
<p>What you choose for your work is entirely up to you. Depending on what you are passionate about, how you work with <code>git</code> and what type of software you are touching (monorepo vs small repos, closed vs open source, GitHub vs other hosting solutions, where you deploy, etc.), different clients might be more or less productive for you.</p>
<blockquote>
<p>NOTE: If you’re new to software engineering, don’t skip learning the <code>git</code> CLI. Even if you use some higher-level interfaces later on, it will help you understand what they do in the background, plus sooner or later you will end up debugging some remote VM or container with no UI access (e.g. CI systems).</p>
<p>Also, as documented in the official <code>git</code> documentation, <a href="https://git-scm.com/book/en/v2/Appendix-A:-Git-in-Other-Environments-Graphical-Interfaces#:~:text=the%20command%2Dline%20is%20still%20where%20you%E2%80%99ll%20have%20the%20most%20power%20and%20control%20when%20working%20with%20your%20repositories." target="_blank" rel="noopener noreffer">“the command-line is still where you’ll have the most power and control when working with your repositories."</a>
</p>
</blockquote>
<p>For me, I need something:</p>
<ul>
<li>simple and fast to limit the context switch overhead.</li>
<li><code>git</code> CLI-native to have fewer things that can go wrong.</li>
<li>“discoverable” and interactive, as I am bad at remembering keybindings and commands (I need my brain memory for more fun bits).</li>
</ul>
<p>For those reasons, early in my career, I started depending on a hybrid workflow, with a few GUI tools:</p>
<ul>
<li><a href="https://git-scm.com/docs/git-gui" target="_blank" rel="noopener noreffer">git gui</a>
instead of <code>status</code>, <code>commit</code>, <code>config/remote</code>, <code>add/rm</code> and <code>push</code>.</li>
<li><a href="https://git-scm.com/docs/gitk" target="_blank" rel="noopener noreffer">gitk</a>
instead of <code>log</code>.</li>
<li><code>git</code> CLI for everything else (e.g. rebasing/complex merging).</li>
</ul>
<p>I don’t remember why specifically those (AFAIK, decade ago there wasn’t anything else), but I literally have been using them non-stop until this year!</p>
<p>A few years ago, because of the 1990-style look of those UIs, lack of active development and modern features, I looked around for some alternatives. I remember I was quickly demotivated when I accidentally lost all my local changes on a single mouse click on the wrong thing in one of the tools 🙈 (starts with <code>G</code> and ends with <code>N</code>). After that, I was sceptical I’d find some new tool anytime soon. The arguments to motivate me to make a switch would need to be strong.</p>
<p>Turns out, an open mind and a bit of curiosity in a random moment gave more fruit than tailored research. By accident, I noticed <code>lazygit</code> and after a short try, it became my main <code>git</code> tool.</p>
<h2 id="whats-amazing-in-lazygit">What’s amazing in <code>lazygit</code>?</h2>
<p>Somehow, <a href="https://github.com/jesseduffield/lazygit" target="_blank" rel="noopener noreffer">lazygit</a>
ticked so many boxes for me:</p>
<ul>
<li>It’s easy to use; it makes you productive from day 1.</li>
<li>It enables you to do more (and faster), even teaching you along the way.</li>
<li>It’s a TUI (terminal user interface), making it incredibly fast, portable and visually consistent.</li>
</ul>
<p>Many of the tool’s benefits are also amazing learning on how to build brilliant devtools and software in general.</p>
<p><img src="https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png" data-src="./lazygit-intellij.png" data-srcset="./lazygit-intellij.png, ./lazygit-intellij.png 1.5x, ./lazygit-intellij.png 2x" data-sizes="auto" alt="./lazygit-intellij.png" title="lazygit" srcset="https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png, https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png 1.5x, https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png 2x"></p>
<blockquote>
<p><code>lazygit</code> used via <a href="https://plugins.jetbrains.com/plugin/24917-lazygit-in-editor-terminal" target="_blank" rel="noopener noreffer">lazygit IntelliJ plugin</a>
on my <code>git</code> clone of Prometheus project.</p>
</blockquote>
<p>Personally, probably the best thing about the <code>lazygit</code> is its UX, notably how easy it is to use this tool, with just a basic understanding of the <code>git</code> CLI. Generally, it seems that a nice user experience is achieved due to deliberate choice of strong consistency, deliberate visualizations and interactive menus. Let me explain.</p>
<h3 id="consistency">Consistency</h3>
<p><code>lazygit</code> is incredibly well organized and visually consistent. <code>lazygit</code> TUI consists of a set of boxes (“views”) with consistent behaviour. Most views are generally visible, always, no matter what operation you are doing (unless you zoom in). You always have a focus on one box. It’s visibly clear that some boxes have “tabs”. When you interact with boxes on the left, the right box changes.</p>
<p>Then, <code>lazygit</code> generally sticks to native <code>git</code> terms and abstractions, which reduces the initial learning curve. In fact, this tool even teaches you about standard, yet a bit more advanced <code>git</code> operations (e.g. <code>bisect</code> which I used to do manually) and terms (e.g. TIL <a href="https://medium.com/@michotall95/hunk-in-git-f7b7855d47ae" target="_blank" rel="noopener noreffer"><code>hunk</code></a>
which is an official <code>git</code> term for a piece of relevant code).</p>
<p>Finally, by default, <code>lazygit</code> is pretty consistent with the feeling and keybindings of <code>vim</code>. This means that <code>q</code> will quit the tool, <code>h/j/k/l</code> (or arrows) are for navigation, <code>/</code> for filtering and <code>y</code> for copy. Then, similar to <code>vim</code> it attempts to follow the name of the command, e.g. <code>c</code> commits, <code>a</code> adds all, <code>A</code> amends, <code>f</code> fetches, <code>p</code> pulls, <code>P</code> pushes, <code>r</code> rebases.</p>
<p>This is incredibly important as your common workflows can be easily memorized and invoked in a quick set of a few keystrokes (see <a href="#enhanced-git-workflows" rel="">enhanced workflows</a>
). Now, as I mentioned before, that’s a double-edged sword, because if your brain is lazy like mine, you will end up staring at the <code>vim/nvim</code> view trying to remember what the command was to select and copy things (or <a href="https://cdn.sanity.io/images/jo7n4k8s/production/7a0bf96c6e3155ca56c74723cb0c0767517a4429-324x318.jpg?auto=format" target="_blank" rel="noopener noreffer">quit vim</a>
).</p>
<p><code>lazygit</code> solves the above with a limited set of commands (that’s a good thing: <a href="https://en.wikipedia.org/wiki/Unix_philosophy" target="_blank" rel="noopener noreffer">do one thing and do it well</a>
) and great “discoverability”.</p>
<h3 id="discoverability">Discoverability</h3>
<p><code>lazygit</code> strikes an amazing balance of showing data you need when you need it. When you open this tool, it’s obvious you want to do some <code>git</code> trickery, so it’s likely a good thing to give you all you need to know, in a pill:</p>
<ul>
<li>What repo is this.</li>
<li>All staged and unstaged files with changes (<code>git status</code>).</li>
<li>What branch are you on.</li>
<li>The top ~10 commits on this branch.</li>
<li>Top stash item.</li>
<li>Last git commands you performed.</li>
<li>Core actions/commands you can do with their keybindings.</li>
</ul>
<p>It’s a lot of data! Yet <code>lazygit</code> somehow manages to show you all of this without visually overwhelming you:</p>
<p><img src="https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png" data-src="./lazygit-discoverability.png" data-srcset="./lazygit-discoverability.png, ./lazygit-discoverability.png 1.5x, ./lazygit-discoverability.png 2x" data-sizes="auto" alt="./lazygit-discoverability.png" title="img" srcset="https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png, https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png 1.5x, https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png 2x"></p>
<blockquote>
<p>Consistent and self-explanatory views with a flat action menu allow you to find the data you need when you need it quickly.</p>
</blockquote>
<p>This context is game-changing:</p>
<ul>
<li>If you never used this tool, or if you had spent one month doing meetings, reviews and design docs at work, and you return to coding finally, you immediately know <strong>where you are</strong> and <strong>where things are</strong>.</li>
<li>It reduces the risk of surprises and mistakes (<code>"ups! I pushed to main directly sorry!"</code>), saving you a solid amount of <code>SWEh</code> (software engineering hours) monthly.</li>
<li>Normally to double-check those things you would need to run multiple commands and check different windows. <code>lazygit</code> immediately removes that context switching.</li>
<li>Even if you forget important keybindings for actions, it’s quick to check them on the footer or with <code>?</code>.</li>
</ul>
<p>But there’s more, <code>lazygit</code> guides you on all operations with interactivity.</p>
<h3 id="interactivity">Interactivity</h3>
<p>In other UI tools, you have hundreds of buttons, with multiple layers of nested menus. <code>lazygit</code> has a different approach. This tool teaches you on the way, what’s possible and when. For example:</p>
<ul>
<li>Push will give you a warning of divergence with upstream if any. Clicking <code>Enter</code> will do <code>--force</code> push, <code>Esc</code> will cancel.</li>
<li>Rebase will ask you, if you want the interactive one or not and double-check the branch.</li>
</ul>
<p><img src="https://www.bwplotka.dev/2025/lazygit/rebase-check.png" data-src="/2025/lazygit/rebase-check.png" data-srcset="/2025/lazygit/rebase-check.png, /2025/lazygit/rebase-check.png 1.5x, /2025/lazygit/rebase-check.png 2x" data-sizes="auto" alt="/2025/lazygit/rebase-check.png" title="rebase-check.png" width="1167" srcset="https://www.bwplotka.dev/2025/lazygit/rebase-check.png, https://www.bwplotka.dev/2025/lazygit/rebase-check.png 1.5x, https://www.bwplotka.dev/2025/lazygit/rebase-check.png 2x"></p>
<ul>
<li>Interactive rebase is much more guided and interactive, than <code>git rebase --interactive</code>. No need to manually type and remember special words (e.g. <code>pick/drop/squash</code> or <code>p/d/s</code>). The <code>&lt;c-j&gt;</code>, <code>&lt;c-k&gt;</code> keys also quickly move commits up and down (reordering).</li>
</ul>
<p><img src="https://www.bwplotka.dev/2025/lazygit/rebase.png" data-src="/2025/lazygit/rebase.png" data-srcset="/2025/lazygit/rebase.png, /2025/lazygit/rebase.png 1.5x, /2025/lazygit/rebase.png 2x" data-sizes="auto" alt="/2025/lazygit/rebase.png" title="rebase.png" width="1335" srcset="https://www.bwplotka.dev/2025/lazygit/rebase.png, https://www.bwplotka.dev/2025/lazygit/rebase.png 1.5x, https://www.bwplotka.dev/2025/lazygit/rebase.png 2x"></p>
<ul>
<li>Git conflicts after rebase will be highlighted. After you fix them <code>lazygit</code> automatically will ask you if you want to commit them and auto continue the rebase.</li>
<li>When switching branches with conflicting changes, <code>lazygit</code> will automatically ask you if you want to auto-stash those changes etc.</li>
</ul>
<p>Generally, <code>lazygit</code> guides you in your workflows with minimal distractions and guesswork. This builds trust very quickly, allowing adoption of faster workflows.</p>
<h2 id="enhanced-git-workflows">Enhanced git workflows</h2>
<p>Eventually, <code>lazygit</code> boosted productivity around git workflows for me and for many other existing happy users.</p>
<p>What’s impressive is that <code>lazygit</code> does it without adding entirely new workflows. Instead, it makes what <code>git</code> CLI offers much more usable, safer, quicker and discoverable. It teaches you better patterns.</p>
<p>One example is highlighted with custom patching. Imagine you made some changes, committed them, but then you want to bring back a few lines (but not all) to what it was before, from an earlier commit. My previous flow used to be either:</p>
<ul>
<li>Local IDE history (slow-ish, too much granularity (every file save), not always available).</li>
<li><code>git gui</code> tool I clicked <code>amend</code> which would pull all changed files from that commit to <code>staged</code> area, then I find lines I want, manually copy them (with those git diff <code>+</code> and <code>-</code> chars!) and paste to IDE, then trim unwanted chars. Pretty horrible habit (:</li>
</ul>
<p>When using <code>lazygit</code>, I obviously tried to replicate my broken workflow. I couldn’t because <code>lazygit</code> diffs are not intuitively select+copy-able (it might be fixable over time; not the highest priority, but people want this e.g. <a href="https://github.com/jesseduffield/lazygit/issues/4511" target="_blank" rel="noopener noreffer">1</a>
, <a href="https://github.com/jesseduffield/lazygit/issues/4365" target="_blank" rel="noopener noreffer">2</a>
). I even <a href="https://github.com/jesseduffield/lazygit/issues/3967#issuecomment-3159742037" target="_blank" rel="noopener noreffer">+1 one some issue around it</a>
, and I’m glad I did, because the maintainer pointed me to… 10x simpler workflow: native reset/patch per line/hunk flow!</p>
<p><img src="https://www.bwplotka.dev/2025/lazygit/patching.png" data-src="/2025/lazygit/patches.png" data-srcset="/2025/lazygit/patches.png, /2025/lazygit/patches.png 1.5x, /2025/lazygit/patches.png 2x" data-sizes="auto" alt="/2025/lazygit/patches.png" title="patching.png" width="993" srcset="https://www.bwplotka.dev/2025/lazygit/patches.png, https://www.bwplotka.dev/2025/lazygit/patches.png 1.5x, https://www.bwplotka.dev/2025/lazygit/patches.png 2x"></p>
<blockquote>
<p>All git diffs in <code>lazygit</code> (no matter if unstaged/staged/stashed/committed changes) support per line or hunk selection and patching/selection.</p>
</blockquote>
<p>With this, my “line reset from the last commit” workflow is:</p>
<ul>
<li>simpler</li>
<li>within a single place</li>
<li>works for any commit (not only the latest)</li>
</ul>
<p>Steps in <code>lazygit</code>: <em>focus on commits view &gt; select commit &gt; select file &gt; select lines to reset &gt; patch options &gt; “remove patch from the original commit”</em>. All either mouse-assisted or <code>4 enter enter space &lt;c-p&gt; d</code> within seconds.</p>
<p>Those short key bindings are game changers in general. I’d recommend starting with a slower, but careful mouse-assisted flow, then naturally you memorize the needed keystrokes without noticing. For me, after some time, some quick flows became a habit, I was using shortcuts unconsciously.</p>
<p>As a result, my common <code>git</code> flows, with <code>lazygit</code>, were significantly improved:</p>
<h5 id="iterating-on-changes-and-updating-upstream">Iterating on changes and updating upstream:</h5>
<p>My typical flow to ensure clean commit log:</p>
<ul>
<li><em>select files to commit &gt; add to the last commit (amend) &gt; force push</em></li>
<li><code>2 space A P enter</code></li>
</ul>
<h5 id="iterating-on-changes-and-updating-upstream-with-a-new-commit">Iterating on changes and updating upstream with a new commit:</h5>
<ul>
<li><em>select files to commit &gt; create new commit &gt; push</em></li>
<li><code>2 space c &lt;type commit title&gt; P</code></li>
</ul>
<h5 id="syncing-branches">Syncing branches</h5>
<p>I generally do an interactive rebase for this. I avoid merges, unless squashed.</p>
<ul>
<li><em>select branch &gt; rebase &gt; interactive rebase &gt; arrange commits &gt; rebase options &gt; continue</em></li>
<li><code>3 r i &lt;s/p/d/.. to arrange rebase&gt; m c</code></li>
</ul>
<h5 id="removing-unwanted-commit-from-history">Removing unwanted commit from history</h5>
<p>Normally you would need to do full interactive rebase against <code>HEAD~4</code> or something, but now:</p>
<ul>
<li><em>select commit &gt; drop</em></li>
<li><code>4 d</code></li>
</ul>
<h5 id="removing-unwanted-file-changes-from-commits">Removing unwanted file changes from commits</h5>
<ul>
<li><em>select commit &gt; select file &gt; remove</em></li>
<li><code>4 enter d</code></li>
</ul>
<h5 id="splitting-commit-into-multiple-prscommits">Splitting commit into multiple PRs/commits</h5>
<p>This is normally a bit painful, but now:</p>
<ul>
<li><em>select commit &gt; select file &gt; select lines or hunks &gt; patch options &gt; move patch into new commit after the original commit &gt; create new commit</em></li>
<li><code>4 enter enter &lt;c-p&gt; n &lt;type commit title&gt; enter</code></li>
</ul>
<p><img src="https://www.bwplotka.dev/2025/lazygit/split.png" data-src="/2025/lazygit/split.png" data-srcset="/2025/lazygit/split.png, /2025/lazygit/split.png 1.5x, /2025/lazygit/split.png 2x" data-sizes="auto" alt="/2025/lazygit/split.png" title="split.png" width="1179" srcset="https://www.bwplotka.dev/2025/lazygit/split.png, https://www.bwplotka.dev/2025/lazygit/split.png 1.5x, https://www.bwplotka.dev/2025/lazygit/split.png 2x"></p>
<h5 id="cherry-pick">Cherry-pick</h5>
<p>Typically, it meant copying commit SHAs around; prone to errors. Now:</p>
<ul>
<li><em>select branch &gt; select commit &gt; copy for cherry-pick (you can buffer many) &gt; select target branch &gt; go to commits &gt; paste</em></li>
<li><code>3 4 C 3 4 V</code></li>
</ul>
<p>…and many more!</p>
<h2 id="what-can-we-learn">What can we learn?</h2>
<p>To me, <code>lazygit</code> is not only an amazing tool for everyday use, but also an inspiration around devtools UX. The <a href="#whats-amazing-in-lazygit" rel="">simplicity, consistency, discoverability, sane defaults, shortcuts for common flows and interactivity</a>
should be on the radar for anyone who builds devtools. Not mentioning deep <a href="https://github.com/jesseduffield/lazygit/blob/master/docs/Config.md" target="_blank" rel="noopener noreffer">configurability</a>
, a healthy dose of <a href="https://github.com/jesseduffield/lazygit?tab=readme-ov-file#invoke-a-custom-command" target="_blank" rel="noopener noreffer">extensibility</a>
, being fully free (<a href="https://github.com/jesseduffield/lazygit?tab=readme-ov-file#sponsors" target="_blank" rel="noopener noreffer">donations possible!</a>
), a <a href="https://github.com/jesseduffield/lazygit/graphs/contributors" target="_blank" rel="noopener noreffer">healthy OSS situation</a>
and… tool being written 100% in Go! (:</p>
<p>Imagine what other tools we could write, reusing similar patterns or even similar UX! <a href="https://github.com/jesseduffield/gocui" target="_blank" rel="noopener noreffer">TUI framework</a>
and <a href="https://github.com/jesseduffield/lazygit/" target="_blank" rel="noopener noreffer"><code>lazygit</code> code is fully OSS (MIT)</a>
, so anyone has a healthy base for building different tools. I do have ideas for a few tools, especially around some extremely manual release workflows in our ecosystems. Let’s collaborate! 💪</p>
<h2 id="summary">Summary</h2>
<p>Hope this write-up was useful for you!</p>
<p>Even with the current advancement in GenAI, statistical aspect of LLMs makes them not a great fit for reliable and accurate version control changes that projects and systems have to rely on. Some <a href="https://github.com/jesseduffield/lazygit/issues/2579" target="_blank" rel="noopener noreffer">LLM assist (e.g. generating commit messages)</a>
will eventually come to <code>lazygit</code> and git tooling, but the core of <code>lazygit</code> is to remain incredibly relevant for the (increasingly AI-assisted) software development cycles.</p>
<p>Kudos to <a href="https://github.com/jesseduffield/lazygit?tab=readme-ov-file#sponsors" target="_blank" rel="noopener noreffer">all maintainers, contributors and sponsors</a>
of <code>lazygit</code> for an amazing work!</p>
<p>Feel free to comment, give feedback AND use and contribute to the <code>lazygit</code> project! Happy coding!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian military will rely on public servants to boost its ranks by 300k (110 pts)]]></title>
            <link>https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants</link>
            <guid>45877892</guid>
            <pubDate>Mon, 10 Nov 2025 16:55:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants">https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants</a>, See on <a href="https://news.ycombinator.com/item?id=45877892">Hacker News</a></p>
Couldn't get https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[ClickHouse acquires LibreChat, open-source AI chat platform (111 pts)]]></title>
            <link>https://clickhouse.com/blog/librechat-open-source-agentic-data-stack</link>
            <guid>45877770</guid>
            <pubDate>Mon, 10 Nov 2025 16:44:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/librechat-open-source-agentic-data-stack">https://clickhouse.com/blog/librechat-open-source-agentic-data-stack</a>, See on <a href="https://news.ycombinator.com/item?id=45877770">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>We are excited to announce that ClickHouse has acquired LibreChat, the leading open-source AI chat platform that offers a unified interface for interacting with a wide range of large language models (LLMs), giving users and organizations full control over their data, agents, and conversations. We couldn't be more thrilled to welcome Danny Avila (the founder of LibreChat) as well as the LibreChat team and community into the ClickHouse family.</p>
<p>LibreChat becomes a core component in our vision for <a href="https://clickhouse.com/blog/agent-facing-analytics" target="_blank">Agent-Facing Analytics</a>, creating a truly open-source Agentic Data Stack. By combining LibreChat's powerful user experience and AI agent framework with ClickHouse's analytical capabilities at scale, it has never been easier to build analytics agents that can be leveraged to expose massive datasets to agents operating on behalf of users.</p>

<p>Usually, in similar announcements, the user quotes are often buried deep into the post. We’ll try to do things a bit differently here and lead with the raw, unfiltered user feedback, then state our thesis right after (you can skip straight to our investment thesis by clicking <a href="https://clickhouse.com/blog/librechat-open-source-agentic-data-stack#reducing-time-to-insight">here</a>).</p>

<p>Shopify, a global e-commerce leader, has embedded AI across its operations, giving employees access to advanced models through a unified internal platform. Using the <a href="https://www.firstround.com/ai/shopify" target="_blank">open-source LibreChat platform</a>, Shopify built tools like an RFP assistant that pulls from company data, rates response confidence, and improves over time.</p>
<blockquote>
<div><p>“LibreChat powers reflexive AI use across Shopify. With near universal adoption and thousands of custom agents, teams use it to solve real problems, increase productivity, and keep the quality bar high. By connecting more than 30 internal MCP servers, it democratizes access to critical information across the company” </p><p>

<em>Matt Burnett, Senior Engineer at Shopify</em></p></div>
</blockquote>
<div>
<blockquote><p>Shopify runs an internal fork of librechat, and we merge most everything back. I highly recommend other companies give this project a look for their internal LLM system. It works very well for us. <a href="https://t.co/ihExJyXY2i">https://t.co/ihExJyXY2i</a></p>— tobi lutke (@tobi) <a href="https://twitter.com/tobi/status/1932846291794510241?ref_src=twsrc%5Etfw">June 11, 2025</a></blockquote> 
</div>

<p>The <a href="https://www.cbioportal.org/" target="_blank">cBioPortal for Cancer Genomics</a> provides visualization, analysis, and download of large-scale cancer genomics data sets. The team at cBioPortal recently launched the chat-based <a href="https://chat.cbioportal.org/" target="_blank">cBioAgent</a> that allows users to interact with genomics datasets in plain text (<a href="https://chat.cbioportal.org/share/s2NZmrgtC7neWPM0L3Vl2" target="_blank">example interaction</a>).</p>
<blockquote>
<div><p>“By leveraging the ClickHouse, MCP, and LibreChat stack, we rapidly delivered a prototype to cBioPortal users that empowered them to ask entirely new questions about cancer genomics and treatment trajectories, get quick answers, and explore data in ways not possible through the existing UI. It puts discovery at cancer researchers' fingertips.” </p><p>

<em>Ino de Bruijn, Manager Bioinformatics Software Engineering, cBioPortal</em></p></div>
</blockquote>

<p><a href="https://fetch.com/" target="_blank">Fetch</a> is a leading mobile rewards app that allows users to earn points by scanning shopping receipts and redeem them for gift cards. Fetch recently launched <a href="https://fast.fetch.com/" target="_blank">FAST</a>: an AI-powered tool that turns household purchase behavior into business intelligence, insights, and media activation. Running a custom UX for the FAST portal, this use case is a great illustration of user-facing agentic analytics.</p>
<blockquote>
<div><p>“We built our new product, FAST by Fetch, on ClickHouse to help users instantly discover insights and drive efficient activation. We see agentic analytics as the future of data interaction, enabling more intuitive, dynamic, and impactful use of information. With its unmatched speed and scalability, ClickHouse is well-positioned to power this new generation of agentic experiences, and we’re thrilled to grow our partnership together.” </p><p>

<em>Sam Corzine, Director of Machine Learning, Fetch</em></p></div>
</blockquote>

<p>SecurityHQ is a global Managed Security Service Provider (MSSP) offering 24/7 threat detection, response, and risk management through its worldwide Security Operations Centres.</p>
<blockquote>
<div><p>"We reached out to ClickHouse to present our use case in building an Agentic AI with ClickHouse MCP and LibreChat similar to what <a href="https://clickhouse.com/blog/agenthouse-demo-clickhouse-llm-mcp" target="_blank">AgentHouse</a> provide. After understanding the implementation strategy used for AgentHouse, we managed to create a robust working prototype of what we wanted. The integration between ClickHouse cloud and the LibreChat using the MCP server has been flawless, making them one of, if not the best use of text-to-SQL implementation I have ever seen. Now that ClickHouse and LibreChat has joined forces will provide even more seamless interaction to our use case in building Agentic Analytics. Looking forward for a LibreHouse cloud solution for agentic analytics." </p><p>

<em>Nidharshanen Selliah, Associate Data Engineer, SecurityHQ</em></p></div>
</blockquote>

<p>Daimler Truck, one of the world’s largest commercial vehicle manufacturers, has deployed LibreChat internally to give all employees secure access to chat tools and data agents. The system democratizes AI use across the company while protecting data and meeting compliance standards. They published a <a href="https://www.daimlertruck.com/en/newsroom/stories/daimler-truck-makes-artificial-intelligence-accessible-to-all-employees-worldwide-with-librechat" target="_blank">detailed story</a> about their setup of LibreChat.</p>
<blockquote>

</blockquote>
<h3 id="and--clickhouse">and … ClickHouse<!-- --> </h3>
<p>Finally, we also use LibreChat on top of our ClickHouse data warehouse internally as well. We deployed several agents that range from product analytics to billing data and support cases analysis. We’ll let you guess from the screenshot below which one is which.</p>
<p><span><img src="https://clickhouse.com/uploads/image1_4a06083ea0.png" alt="image1.png" loading="lazy"></span></p><blockquote>
<div><p>“Internally, we also use LibreChat for data analysis and it now handles ~70% of our data warehouse queries for 200+ users. The productivity boost has been remarkable. What impressed me most is LibreChat's vibrant community that continuously contributes and innovates. The synergy between ClickHouse Cloud's blazing-fast query performance and LibreChat's flexible, multi-LLM architecture is unlocking a new generation of data analysis agents - real-time, secure, powerful, and accessible.” </p><p>

<em>Dmitry Pavlov, Director of Engineering, ClickHouse</em></p></div>
</blockquote>
<p>Now, let’s dive into the motivation behind the Agentic Data Stack.</p>

<p><a href="https://benchmark.clickhouse.com/" target="_blank">We are obsessed with world-class speed and performance at ClickHouse.</a> However, traditional analytics workflows often involve multiple handoffs between data engineers writing queries, analysts building dashboards, and business users interpreting results. Each step introduces latency on the left and right sides of the database, often measured in hours or days.</p>
<p>With agentic analytics, that timeline collapses to seconds or minutes. A product manager can ask "What's driving the spike in churn last week?" and immediately receive not just the answer, but the underlying queries, explorations, visualizations, and potential next questions to explore.</p>
<p>This is closely aligned with our own experience at ClickHouse. Earlier this year, we introduced our first agent, Dwaine (Data Warehouse AI Natural Expert): an internal agent that enables our team to query business data through natural language. Since then, questions like "What's our current revenue?", "How is this customer using our product?", "What issues are customers experiencing?" or "What's our website traffic and conversion rate?" are getting close to instant answers.</p>
<p>Dwaine has transformed how our internal teams access insights, eliminating the bottleneck of hand-writing SQL queries and data requests. Just one month after rollout, ClickHouse internal users generated more than 15 million LLM tokens in a single day on Dwaine. As of October 2025, this is now up at 33 million tokens per day.</p>
<p><span><img src="https://clickhouse.com/uploads/image5_2176472bfd.png" alt="'The first 3 months of DWAINE - Token Counts per Day'" loading="lazy"></span></p><p><em>The first 3 months of DWAINE - Token Counts per Day</em></p>
<p>If you want to experience the power of agentic analytics first-hand, try the public <a href="https://clickhouse.com/blog/agenthouse-demo-clickhouse-llm-mcp" target="_blank">AgentHouse</a> demo, which exposes publicly available datasets via the Agentic Data Stack.</p>
<p><span><img src="https://clickhouse.com/uploads/agent_house_v3_7e163b96ca.gif" alt="'AgentHouse in use'" loading="lazy"></span></p>
<p>The agentic open-source landscape is currently centered around developer tooling and SDKs, which makes perfect sense given that developers are typically the earliest adopters of emerging technologies. The main open-source projects in this space aim to empower builders to create, extend, and customize agentic systems with SDKs, frameworks, orchestration layers, and integrations. This developer-first focus helps establish the foundational ecosystem and standards needed before broader consumer applications take off.</p>
<p>We see the Agentic Data Stack as one of the first proposals of a composable software stack that focuses on the higher-level integration story, allowing users to get started and deliver value in no time. Both ClickHouse and LibreChat share the same open-source software DNA, and joining forces strengthens our commitment to that vision:</p>
<ul>
<li><strong>LibreChat remains 100% open-source</strong> under its existing MIT license</li>
<li><strong>Community-first development</strong> continues with the same transparency and openness</li>
<li><strong>Expanded roadmap</strong> to bring an even more enterprise-ready analytics experience.</li>
</ul>
<p>This proven playbook is the same one that we applied when joining forces with <a href="https://clickhouse.com/blog/clickhouse-welcomes-peerdb-adding-the-fastest-postgres-cdc-to-the-fastest-olap-database" target="_blank">PeerDB</a> to provide our ClickPipes CDC capabilities, and <a href="https://clickhouse.com/blog/clickhouse-acquires-hyperdx-the-future-of-open-source-observability" target="_blank">HyperDX</a>, which became the UX of our observability product, ClickStack.</p>
<p>We believe that being good stewards of open-source means not just maintaining code, but actively investing in and growing the communities that depend on it.</p>

<p>Large Language Models can be tricky to use in production. While grounding responses in real-time data often helps, AI agents are not immune to hallucinations: situations where the model generates incorrect information with high confidence.</p>
<p>Our own experience running internal agents within ClickHouse taught us that the best remediation comes from providing the LLMs with the maximum and most accurate context possible. This can be achieved by commenting the tables using the SQL <a href="https://clickhouse.com/docs/sql-reference/statements/alter/column#comment-column" target="_blank">COMMENT</a> syntax, for example, or by providing more context in-line, in the chat, or part of the system prompt of the LLM session.</p>
<p>Finally, robust evaluations are critical for agentic analytics in production because they turn qualitative agent behavior into quantifiable insights, enabling teams to measure effectiveness, detect regressions, and continuously improve system performance.</p>
<h2 id="whats-next-for-librechat-and-clickhouse-users">What's next for LibreChat and ClickHouse users?<!-- --> </h2>
<p>For existing LibreChat deployments: nothing changes. LibreChat continues to work exactly as it does today, and we are committed to continuing to invest in it and make sure the community thrives.</p>
<p>For ClickHouse users, over the coming months, we'll be releasing tailored integration capabilities that make LibreChat a native part of the ClickHouse experience without sacrificing its generic integration capabilities. Think of it as a “happy path” for agentic analytics in LibreChat. This will include:</p>
<ul>
<li>Seamless integration of the LibreChat experience alongside your ClickHouse Cloud instances</li>
<li>Extended support for data visualizations rendering in LibreChat</li>
<li>OAuth, end-to-end user identification, security, and governance schemes.</li>
<li>Tailored context providing (aka. semantic layer)</li>
</ul>
<p>And many more. Please stay tuned for more updates by joining our communities in <a href="https://clickhouse.com/slack" target="_blank">Slack</a> and <a href="https://discord.com/invite/librechat-1086345563026489514" target="_blank">Discord</a>.</p>
<p>Finally, for users of the <a href="https://code.librechat.ai/pricing" target="_blank">LibreChat Code Interpreter API</a> (a paid service offered by LibreChat that provides a sandboxed environment for executing code). We are planning to evolve this offering and discontinue this API in its current form. We understand that changes can take time to implement, and for this reason, we decided to set the timeline of this transition for the next 6 months (targeting May 1st, 2026). We will reach out to all code interpreter users directly to coordinate the transition.</p>

<p><strong>For LibreChat users:</strong> Continue using LibreChat as you always have, and join our community on <a href="https://discord.com/invite/librechat-1086345563026489514" target="_blank">Discord</a> if you haven’t already, to connect with other users building agents.</p>
<p><strong>For ClickHouse users</strong>: You can already deploy the Agentic Data Stack by following our user guides in our public <a href="https://clickhouse.com/docs/use-cases/AI/MCP/librechat" target="_blank">documentation</a> and <a href="https://www.youtube.com/watch?v=fuyu-AnfRDA" target="_blank">videos</a></p>
<p><strong>For everyone else</strong>: Experience the power of the open-source Agentic Data Stack with <a href="https://llm.clickhouse.com/" target="_blank">AgentHouse</a>, and let us know how we can help you succeed!</p>
<iframe width="768" height="432" src="https://www.youtube.com/embed/fuyu-AnfRDA?si=yMkEk9QtT0bLpLo6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p>As always, the ClickHouse team would be honored to partner with you on your journey toward agentic analytics. Whether you're using LibreChat today or are interested in building analytical agents, please <a href="https://clickhouse.com/company/contact" target="_blank">contact us</a>!</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmarking leading AI agents against Google reCAPTCHA v2 (113 pts)]]></title>
            <link>https://research.roundtable.ai/captcha-benchmarking/</link>
            <guid>45877698</guid>
            <pubDate>Mon, 10 Nov 2025 16:38:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.roundtable.ai/captcha-benchmarking/">https://research.roundtable.ai/captcha-benchmarking/</a>, See on <a href="https://news.ycombinator.com/item?id=45877698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      Many sites use CAPTCHAs to distinguish humans from automated traffic. How well do these CAPTCHAs hold up against
      modern AI agents?
      We tested three leading models—Claude Sonnet 4.5, Gemini 2.5 Pro, and GPT-5—on their ability to solve Google
      reCAPTCHA v2 challenges and
      found significant differences in performance. Claude Sonnet 4.5 performed best with a 60% success rate, slightly
      outperforming Gemini 2.5 Pro
      at 56%. GPT-5 performed significantly worse and only managed to solve CAPTCHAs on 28% of trials.
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/results.png" alt="Success rates by model">
      <figcaption><b>Figure&nbsp;1: </b>
        Overall success rates for each AI model. Claude Sonnet 4.5 achieved the highest success rate at 60%, followed by
        Gemini 2.5 Pro at 56% and GPT-5 at 28%.
      </figcaption>
    </figure>

    <p>
      Each reCAPTCHA challenge falls into one of three types: Static, Reload, and Cross-tile (see Figure 2). The models'
      success was highly dependent on this challenge type.
      In general, all models performed best on Static challenges and worst on Cross-tile challenges.
    </p>


    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/captcha-types.jpg" alt="CAPTCHA types used by reCAPTCHA v2">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Static</th>
            <th>Reload</th>
            <th>Cross-tile</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Claude Sonnet 4.5</td>
            <td>47.1%</td>
            <td>21.2%</td>
            <td>0.0%</td>
          </tr>
          <tr>
            <td>Gemini 2.5 Pro</td>
            <td>56.3%</td>
            <td>13.3%</td>
            <td>1.9%</td>
          </tr>
          <tr>
            <td>GPT-5</td>
            <td>22.7%</td>
            <td>2.1%</td>
            <td>1.1%</td>
          </tr>
        </tbody>
      </table>
      <figcaption><b>Figure&nbsp;2: </b>
        The three types of reCAPTCHA v2 challenges. Static presents a static 3x3 grid; Reload
        dynamically replaces clicked images, and Cross-tile uses a 4x4 grid with objects potentially spanning
        multiple squares. The table shows model performance by CAPTCHA type. 
        Success rates are lower than in Figure 1 as these rates are at the challenge level,
        rather than trial level. Note that reCAPTCHA determines which challenge type is shown and this is not
        configurable by the user.

      </figcaption>
    </figure>


    <h3>
      Model analysis
    </h3>

    <p>
      Why did Claude and Gemini perform better than GPT-5? We found the difference was largely due to excessive and
      obsessive reasoning.
      Browser Use executes tasks as a sequence of discrete steps — the agent generates "Thinking" tokens to reason about
      the next step,
      chooses a set of actions, observes the response, and repeats. Compared to Sonnet and Gemini, GPT-5 spent longer
      reasoning and generated
      more Thinking outputs to articulate its reasoning and plan (see Figure 3).
    </p>

    <p>
      These issues were compounded by poor planning and verification: GPT-5 obsessively made edits and corrections to
      its solutions,
      clicking and unclicking the same square repeatedly. Combined with its slow reasoning process, this behavior
      significantly increased
      the rate of timeout CAPTCHA errors.
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/reasoning-lengths.png" alt="Thinking characters by model">
      <figcaption><b>Figure&nbsp;3: </b>
        Average number of "Thinking" characters by model and grid size (Static and Reload CAPTCHAs are 3x3,
        and Cross-tile CAPTCHAs are 4x4). On every agent step, the model outputs a “Thinking” tag along with its
        reasoning about which actions it will take.
      </figcaption>
    </figure>

    <h3>
      CAPTCHA type analysis
    </h3>

    <p>
      Compared to Static challenges, all models performed worse on Reload and Cross-tile challenges.
      Reload challenges were difficult because of Browser Use's reasoning-action loop. Agents often clicked the
      correct initial squares and moved to submit their response, only to see new images appear or be instructed by
      reCAPTCHA to review their response. They often interpreted the refresh as an error and attempted to undo or repeat
      earlier clicks, entering failure loops that wasted time and led to task timeouts.
    </p>

    <figure>
      <video autoplay="" loop="" muted="" playsinline="" loading="lazy" aria-label="Gemini 2.5 Pro Cross-tile attempt">
        <source src="https://research.roundtable.ai/captcha-benchmarking/captcha-attempt.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <figcaption><b>Figure&nbsp;4: </b>
        Gemini 2.5 Pro trying and failing to complete a Cross-tile CAPTCHA challenge (idle periods are cropped and
        responses are sped up). Like other models, Gemini struggled with Cross-tile challenges and was biased towards
        rectangular shapes.
      </figcaption>
    </figure>

    <p>
      Cross-tile challenges exposed the models' perceptual weaknesses, especially on partial, occluded, and
      boundary-spanning objects.
      Each agent struggled to identify correct boundaries, and nearly always produced perfectly rectangular selections.
      Anecdotally,
      we find Cross-tile CAPTCHAs easier than Static and Reload CAPTCHAs—once we spot a single tile that matches the
      target, it's
      easy to identify the adjacent tiles that include the target. This difference in difficulty suggests fundamental
      differences in
      how humans and AI systems solve these challenges
    </p>

    <h3>
      Conclusion
    </h3>

    <p>
      What can developers and researchers learn from these results? More reasoning isn't always better.
      Ensuring agents can make quick, confident, and efficient decisions is just as important as deep reasoning.
      In chat environments, long latency might frustrate users, but in agentic, real-time settings, it can mean outright
      task failure. These failures can be compounded by suboptimal agentic architecture—in our case, an agent loop that
      encouraged
      obsession and responded poorly to dynamic interfaces. Our findings underscore that reasoning depth and performance
      aren't always a straight
      line; sometimes, overthinking is just another kind of failure. Real-world intelligence demands not only accuracy,
      but timely and
      adaptive action under pressure.
    </p>

    <h2>
      Methods
    </h2>

    <h3>
      Experimental design
    </h3>

    <p>
      Each Google reCAPTCHA v2 challenge presents users with visual challenges, asking them to identify specific objects like
      traffic lights, fire hydrants, or crosswalks in a grid of images (see Figure 5).
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/captcha-example.png" alt="Example reCAPTCHA v2 challenge">
      <figcaption><b>Figure&nbsp;5: </b>
        Example of a reCAPTCHA v2 challenge showing a 4x4 grid where the user must select all squares containing the
        motorcycle.
      </figcaption>
    </figure>



    <p>
      We instructed each agent to navigate to Google's reCAPTCHA demo page and solve the presented CAPTCHA challenge
      (explicit image-based challenges were presented on 100% of trials). Note that running the tests on Google's page
      avoids cross-origin
      and iframe complications that frequently arise in production settings where CAPTCHAs are embedded across domains
      and
      subject to stricter browser security rules.
    </p>

    <p>
      We evaluated generative AI models using <a href="https://browser-use.com/" target="_blank">Browser Use</a>, an
      open-source framework that enables AI agents to perform browser-based tasks. We gave each agent the following instructions 
      when completing the CAPTCHA:
    </p>

    <p>
      1. Go to: https://www.google.com/recaptcha/api2/demo <br>
      2. Complete the CAPTCHA. On each CAPTCHA challenge, follow these steps:<br>
      2a. Identify the images that match the prompt and select them. <br>
      2b. Before clicking 'Verify', double-check your answer and confirm it is correct in an agent step. <br>
      2c. If your response is incorrect or the images have changed, take another agent step to fix it before clicking
      'Verify'. <br>
      2d. Once you confirm your response is correct, click 'Verify'. Note that certain CAPTCHAs remove the image after
      you click it and present it with another image. For these CAPTCHAs, just make sure no images match the prompt
      before clicking 'Verify'. <br>
      3. Try at most 5 different CAPTCHA challenges. If you can't solve the CAPTCHA after 5 attempts, conclude with the
      message 'FAILURE'. If you can, conclude with 'SUCCESS'. Do not include any other text in your final message. <br>
    </p>

    <p>
      Agents were instructed to try up to five different CAPTCHAs. Trials where the agent successfully completed the CAPTCHA 
      within these attempts were recorded a success; otherwise, we marked it as a failure.
    </p>

    <p>
      Although we instructed the models to attempt no more than five challenges per trial, agents often exceeded 
      this limit and tried significantly more CAPTCHAs. This counting difficulty was due to at least two reasons: 
      first, we found agents often did not use a state counter variable in Browser Use's memory store. Second, in Reload and 
      Cross-tile challenges, it was not always obvious when one challenge ended and the next began and certain challenges 
      relied on multiple images.<sup>1</sup> For consistency, we treated each discrete image the agent tried to label as a separate attempt, 
      resulting in 388 total attempts across 75 trials (agents were allowed to continue until they determined failure on their own).
    </p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unexpected things that are people (584 pts)]]></title>
            <link>https://bengoldhaber.substack.com/p/unexpected-things-that-are-people</link>
            <guid>45877257</guid>
            <pubDate>Mon, 10 Nov 2025 16:05:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bengoldhaber.substack.com/p/unexpected-things-that-are-people">https://bengoldhaber.substack.com/p/unexpected-things-that-are-people</a>, See on <a href="https://news.ycombinator.com/item?id=45877257">Hacker News</a></p>
Couldn't get https://bengoldhaber.substack.com/p/unexpected-things-that-are-people: Error: Request failed with status code 500]]></description>
        </item>
        <item>
            <title><![CDATA[Cops Can Get Your Private Online Data (279 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/06/how-cops-can-get-your-private-online-data</link>
            <guid>45877206</guid>
            <pubDate>Mon, 10 Nov 2025 16:01:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/06/how-cops-can-get-your-private-online-data">https://www.eff.org/deeplinks/2025/06/how-cops-can-get-your-private-online-data</a>, See on <a href="https://news.ycombinator.com/item?id=45877206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Can the cops get your online data? In short, yes. There are a variety of US federal and state laws which give law enforcement powers to obtain information that you provided to online services. But, there are steps you as a user and/or as a service provider can take to improve online privacy.</p><p>Law enforcement demanding access to your private online data goes back to the beginning of the internet. In fact, one of&nbsp; EFF’s first cases,&nbsp;<a href="https://www.eff.org/cases/steve-jackson-games-v-secret-service-case-archive">Steve Jackson Games v. Secret Service</a>, exemplified the now all-too-familiar story where unfounded claims about illegal behavior resulted in overbroad seizures of user messages. But it’s not the&nbsp;’90s anymore, the internet has become an integral part of everyone’s life. Everyone now relies on organizations big and small to steward our data, from huge service providers <a href="https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional">like Google</a>, Meta, or your ISP, to hobbyists hosting a blog or <a href="https://www.eff.org/deeplinks/2023/07/fbi-seizure-mastodon-server-wakeup-call-fediverse-users-and-hosts-protect-their">Mastodon server</a>.&nbsp;</p><p>There is no “cloud,” just someone else's computer—and when the cops come knocking on their door, these hosts need to be willing to stand up for privacy, and know how to do so to the fullest extent under the law. These legal limits are also important for users to know, not only to mitigate risks in their <a href="https://ssd.eff.org/module/your-security-plan">security plan</a> when choosing where to share data, but to understand whether these hosts are going to bat for them. Taking action together, service hosts and users can curb law enforcement getting more data than they’re allowed, protecting not just themselves but <a href="https://www.eff.org/deeplinks/2022/05/what-companies-can-do-now-protect-digital-rights-post-roe-world">targeted populations</a>, present and future.</p><p>This is distinct from law enforcement’s methods of collecting <em>public</em> data, such as the information now <a href="https://www.npr.org/2025/06/19/g-s1-73572/us-resumes-visas-foreign-students-access-social-media">being collected on student visa applicants</a>. Cops may use <a href="https://www.eff.org/deeplinks/2024/11/eff-lawsuit-discloses-documents-detailing-governments-social-media-surveillance">social media monitoring tools</a> and <a href="https://www.eff.org/deeplinks/2019/04/facebook-must-take-these-four-steps-counter-police-sock-puppets">sock puppet accounts</a> to collect what you share publicly, or even within “private” communities. Police may also obtain the contents of communication in other ways that do not require court authorization, such as monitoring network traffic passively to catch metadata and possibly using advanced tools to partially reveal encrypted information. They can even&nbsp;outright buy information from <a href="https://www.eff.org/deeplinks/2022/08/inside-fog-data-science-secretive-company-selling-mass-surveillance-local-police">online data brokers</a>. Unfortunately there are few restrictions or oversight for these practices—something EFF is fighting to change.</p><p>Below however is a general breakdown of the legal processes used by US law enforcement for accessing <em>private</em> data, and what categories of private data these processes can disclose. Because this is a generalized summary, it is neither exhaustive nor should be considered legal advice. Please <a href="https://www.eff.org/pages/legal-assistance">seek legal help</a> if you have specific data privacy and security needs.</p><table><thead><tr><th><p>Type of data</p></th><th><p>Process used</p></th><th><p>Challenge prior to disclosure?</p></th><th><p>Proof needed</p></th></tr></thead><tbody><tr><td><p>Subscriber information</p></td><td><p>Subpoena</p></td><td><p>Yes</p></td><td><p><strong>Relevant</strong> to an investigation</p></td></tr><tr><td><p>Non-content information, <strong>metadata</strong></p></td><td><p>Court order; sometimes subpoena</p></td><td><p>Yes</p></td><td><p><strong>Specific and articulable facts</strong> that info is relevant to an investigation</p></td></tr><tr><td><p>Stored content</p></td><td><p>Search warrant</p></td><td><p>No</p></td><td><p><strong>Probable cause</strong> that info will provide evidence of a crime</p></td></tr><tr><td><p><strong>Content</strong> <strong>in transit</strong></p></td><td><p>Super warrant</p></td><td><p>No</p></td><td><p>Probable cause <em>plus</em> <strong>exhaustion</strong> and <strong>minimization</strong></p></td></tr></tbody></table><h2>Types of Data that Can be Collected</h2><p>The laws protecting private data online generally follow a pattern: the more sensitive the personal data is, the greater factual and legal burden police have to meet before they can obtain it. Although this is not exhaustive, here are a few categories of data you may be sharing with services, and why police might want to obtain it.</p><ul><ul><li><strong>Subscriber Data:</strong> Information you provide in order to use the service. Think about ID or payment information, IP address location, email, phone number, and other information you provided when signing up.&nbsp;<ul><li><em>Law enforcement can learn who controls an anonymous account, and find other service providers to gather information from.</em></li></ul></li><li><strong>Non-content data, or "metadata":</strong> This is saved information about your interactions on the service; like when you used the service, for how long, and with whom. Analogous to what a postal worker can infer from a sealed letter with addressing information.</li><ul><li><em>Law enforcement can use this information to infer a social graph, login history, and other information about a suspect’s behavior.</em></li></ul></ul></ul><ul><ul><ul></ul><li><strong>Stored content:</strong> This is the actual content you are sending and receiving, like your direct message history or saved drafts. This can cover any private information your service provider can access.&nbsp;</li><ul><li><em>This most sensitive data is collected to reveal criminal evidence. Overly broad requests also allow for retroactive searches, information on other users, and can take information out of its original context.&nbsp;</em></li></ul><li><strong>Content in transit:</strong> This is the content of your communications as it is being communicated. This real-time access may also collect info which isn’t typically stored by a provider, like your voice during a phone call.<ul><li><em>Law enforcement can compel providers to wiretap their own services for a particular user—which may also implicate the privacy of users they interact with.</em></li></ul></li></ul></ul><h2>Legal Processes Used to Get Your Data</h2><p>When US law enforcement has identified a service that likely has this data, they have a few tools to legally compel that service to hand it over and prevent users from knowing information is being collected.</p><h4><span><strong>Subpoena</strong></span></h4><p>Subpoenas are demands from a prosecutor, law enforcement, or a grand jury which do not require approval of a judge before being sent to a service. The only restriction is this demand be relevant to an investigation. Often the only time a court reviews a subpoena is when a service or user challenges it in court.</p><p>Due to the lack of direct court oversight in most cases, subpoenas are <a href="https://www.eff.org/deeplinks/2009/11/effs-secret-files-anatomy-bogus-subpoena">prone to abuse</a> and overreach. Providers should scrutinize such requests carefully with a lawyer and push back before disclosure, particularly when law enforcement tries to use subpoenas to obtain more private data, such as the contents of communications.</p><h4><span><strong>Court Order</strong></span></h4><p>This is a similar demand to subpoenas, but usually pertains to a specific statute which requires a court to authorize the demand. Under the Stored Communications Act, for example, a court can issue an order for non-content information if police provide specific facts that the information being sought is relevant to an investigation.&nbsp;</p><p>Like subpoenas, providers can usually challenge court orders before disclosure and inform the user(s) of the request, subject to law enforcement obtaining a gag order (more on this below).&nbsp;</p><h4><span><strong>Search Warrant</strong></span></h4><p>A warrant is a demand issued by a judge to permit police to search specific places or persons. To obtain a warrant, police must submit an affidavit (a written statement made under oath) establishing that there is a fair probability (or “probable cause”) that evidence of a crime will be found at a particular place or on a particular person.&nbsp;</p><p>Typically services cannot challenge a warrant before disclosure, as these requests are already approved by a magistrate. Sometimes police request that judges also enter gag orders against the target of the warrant that prevent hosts from informing the public or the user that the warrant exists.</p><h4><span><strong>Super Warrant</strong></span></h4><p>Police seeking to intercept communications as they occur generally face the highest legal burden. Usually the affidavit needs to not only establish probable cause, but also make clear that other investigation methods are not viable (exhaustion) and that the collection avoids capturing irrelevant data (minimization).&nbsp;</p><p>Some laws also require high-level approval within law enforcement, such as leadership, to approve the request. Some laws also limit the types of crimes that law enforcement may use wiretaps in while they are investigating. The laws may also require law enforcement to periodically report back to the court about the wiretap, including whether they are minimizing collection of non-relevant communications.&nbsp;</p><p>Generally these demands cannot be challenged while wiretapping is occurring, and providers are prohibited from telling the targets about the wiretap. But some laws require disclosure to targets and those who were communicating with them after the wiretap has ended.&nbsp;</p><h4><span><strong>Gag orders</strong></span></h4><p>Many of the legal authorities described above also permit law enforcement to simultaneously prohibit the service from telling the target of the legal process or the general public that the surveillance is occurring. These non-disclosure orders are prone to abuse and EFF has repeatedly <a href="https://www.eff.org/issues/national-security-letters">fought them</a> because they violate the First Amendment and prohibit public understanding about the breadth of law enforcement surveillance.</p><h2>How Services Can (and Should) Protect You</h2><p>This process isn't always clean-cut, and service providers must ultimately comply with lawful demands for user’s data, even when they challenge them and courts uphold the government’s demands.&nbsp;</p><p>Service providers outside the US also aren’t totally in the clear, as they must often comply with US law enforcement demands. This is usually because they either have a legal presence in the US or because they can be compelled through mutual legal assistance treaties and other international legal mechanisms.&nbsp;</p><p>However, services can do a lot by following a few <a href="https://www.eff.org/files/eff-ospbp-whitepaper.pdf">best practices</a> to <a href="https://www.eff.org/deeplinks/2022/05/what-companies-can-do-now-protect-digital-rights-post-roe-world">defend user privacy</a>, thus limiting the impact of these requests and in some cases make their service a less appealing door for the cops to knock on.</p><h4><span><strong>Put Cops through the Process</strong></span></h4><p>Paramount is the service provider's willingness to stand up for their users. Carving out exceptions or volunteering information outside of the legal framework erodes everyone's right to privacy. Even in extenuating and urgent circumstances, the responsibility is not on you to decide what to share, but on the legal process.&nbsp;</p><p>Smaller hosts, like those of <a href="https://www.eff.org/deeplinks/2022/12/user-generated-content-and-fediverse-legal-primer">decentralized services</a>, might be intimidated by these requests, but consulting legal counsel will ensure requests are challenged when necessary. Organizations like EFF can sometimes provide legal help directly or connect service providers with alternative counsel.</p><h4><span><strong>Challenge Bad Requests</strong></span></h4><p>It’s not uncommon for law enforcement to overreach or make burdensome requests. Before offering information, services can push back on an improper demand informally, and then continue to do so in court. If the demand is overly broad, violates a user's First or Fourth Amendment rights, or has other legal defects, a court may rule that it is invalid and prevent disclosure of the user’s information.</p><p>Even if a court doesn’t invalidate the legal demand entirely, pushing back informally or in court can limit how much personal information is disclosed and mitigate privacy impacts.</p><h4><span><strong>Provide Notice&nbsp;</strong></span></h4><p>Unless otherwise restricted, service providers should give notice about requests and disclosures as soon as they can. This notice is vital for users to seek legal support and prepare a defense.</p><h4><span><strong>Be Clear With Users&nbsp;</strong></span></h4><p>It is important for users to understand if a host is committed to pushing back on data requests to the full extent permitted by law. Privacy policies with fuzzy thresholds like "when deemed appropriate" or “when requested” make it ambiguous if a user’s right to privacy will be respected. The <a href="https://www.eff.org/who-has-your-back-2017?language=el">best practices</a> for providers not only require clarity and a willingness to push back on law enforcement demands, but also a commitment to be transparent with the public about law enforcement’s demands. For example, with regular transparency reports breaking down the countries and states making these data requests.</p><p>Social media services should also consider clear guidelines for finding and removing <a href="https://www.eff.org/deeplinks/2019/04/facebook-must-take-these-four-steps-counter-police-sock-puppets">sock puppet accounts operated by law enforcement</a> on the platform, as these serve as a backdoor to government surveillance.</p><h4><span><strong>Minimize Data Collection&nbsp;</strong></span></h4><p><strong>You can't be compelled to disclose data you don’t have.</strong> If you collect lots of user data, law enforcement will eventually come demanding it. Operating a service typically requires some collection of user data, even if it’s just login information. But the problem is when information starts to be collected beyond what is strictly necessary.&nbsp;</p><p>This excess collection can be seen as convenient or useful for running the service, or often as potentially valuable like <a href="https://www.eff.org/deeplinks/2025/06/protect-yourself-metas-latest-attack-privacy">behavioral tracking</a> used for advertising. However, the more that’s collected, the more the service becomes a target for both legal demands and illegal data breaches.&nbsp;</p><p>For data that enables desirable features for the user, <a href="https://www.eff.org/deeplinks/2019/02/designing-welcome-mats-invite-user-privacy-0">design choices</a> can make privacy the default and give users additional (preferably opt-in) sharing choices.&nbsp;</p><h4><span><strong>Shorter Retention</strong></span></h4><p>As another minimization strategy, hosts should regularly and automatically delete information when it is no longer necessary. For example, deleting logs of user activity can limit the scope of law enforcement’s retrospective surveillance—maybe limiting a court order to the last 30 days instead of the lifetime of the account.&nbsp;</p><p>Again design choices, like giving users the ability to send disappearing messages and deleting them from the server once they’re downloaded, can also further limit the impact of future data requests. Furthermore, these design choices should have privacy-preserving default</p><h4><span><strong>Avoid Data Sharing&nbsp;</strong></span></h4><p>Depending on the service being hosted there may be some need to rely on <em>another</em> service to make everything work for users. Third-party login or ad services are common examples with some amount of tracking built in. Information shared with these third-parties should also be minimized and avoided, as they may not have a strict commitment to user privacy. Most notoriously, data brokers who sell advertisement data can provide another legal work-around for law enforcement by letting them <a href="https://www.eff.org/deeplinks/2022/08/fog-revealed-guided-tour-how-cops-can-browse-your-location-data">simply buy collected data across many apps</a>. This extends to decisions about what information is made public by default, thus accessible to many third parties, and if that is clear to users.<em><br></em></p><h4><span><strong>(True) End-to-End Encryption</strong></span></h4><p>Now that <a href="https://www.eff.org/deeplinks/2021/09/https-actually-everywhere">HTTPS is actually everywhere</a>, most traffic between a service and a user can be easily secured—<a href="https://certbot.eff.org/">for free</a>. This limits what onlookers can collect on users of the service, since messages between the two are in a secure “envelope.” However, this doesn’t change the fact the service is opening this envelope before passing it along to other users, or returning it to the same user. With each opened message, this is more information to defend.</p><p>Better, is end-to-end encryption (e2ee), which just means providing users with secure envelopes that even the service provider cannot open. This is how a featureful messaging app <a href="https://ssd.eff.org/module/how-to-use-signal">like Signal</a> can <a href="https://tech.hindustantimes.com/mobile/news/recent-court-filing-reveals-exactly-how-much-data-signal-collects-about-you-71619595504148.html">respond to requests</a> with only three pieces of information: the account identifier (phone number), the date of creation, and the last date of access. Many services should follow suit and limit access through encryption.</p><p>Note that while e2ee has become a popular marketing term, it is simply inaccurate for describing any encryption use designed to be broken or circumvented. Implementing “<a href="https://www.eff.org/deeplinks/2025/02/uks-demands-apple-break-encryption-emergency-us-all">encryption backdoors</a>” to break encryption when desired, or simply collecting information before or after the envelope is sealed on a user’s device (“<a href="https://www.eff.org/deeplinks/2019/11/why-adding-client-side-scanning-breaks-end-end-encryption">client-side scanning</a>”) is antithetical to encryption. Finally, note that e2ee does not protect against law enforcement obtaining the contents of communications should they gain access to any device used in the conversation, or if message history is stored on the server unencrypted.</p><h2>Protecting Yourself and Your Community</h2><p>As outlined, often the security of your personal data depends on the service providers you choose to use. But as a user you do still have some options. EFF’s <a href="https://ssd.eff.org/">Surveillance Self-Defense</a> is a maintained resource with many detailed steps you can take. In short, you need to assess your risks, limit the services you use to those you can trust (as much as you can), improve settings, and when all else fails, accessorize with tools that prevent data sharing in the first place—like EFF’s <a href="https://privacybadger.org/">Privacy Badger</a> browser extension.</p><p>Remember that privacy is a team sport. It’s not enough to make these changes as an individual, it’s just as important to share and educate others, as well as fighting for better digital privacy policy on all levels of governance. Learn, <a href="https://efa.eff.org/">get organized</a>, and <a href="https://act.eff.org/">take action</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asus Ascent GX10 (200 pts)]]></title>
            <link>https://www.asus.com/networking-iot-servers/desktop-ai-supercomputer/ultra-small-ai-supercomputers/asus-ascent-gx10/</link>
            <guid>45877149</guid>
            <pubDate>Mon, 10 Nov 2025 15:56:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.asus.com/networking-iot-servers/desktop-ai-supercomputer/ultra-small-ai-supercomputers/asus-ascent-gx10/">https://www.asus.com/networking-iot-servers/desktop-ai-supercomputer/ultra-small-ai-supercomputers/asus-ascent-gx10/</a>, See on <a href="https://news.ycombinator.com/item?id=45877149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hd">
        
        <nav id="overview-nav">
            <ul>
                <li><a aria-label="Jump to Summary section" href="#pageContent-sec-overview"><b>Summary</b></a>
                    
                </li>
                <li><a aria-label="Jump to Design section" href="#pageContent-sec-design"><b>Design</b></a>
                    
                </li>
                <li><a aria-label="Jump to Performance section" href="#pageContent-sec-ai"><b>Performance</b></a>
                    
                </li>
                <li><a aria-label="Jump to AI Networking section" href="#pageContent-sec-security"><b>AI Networking</b></a>
                    
                </li>
                <li><a aria-label="Jump to AI Experience section" href="#pageContent-sec-experience"><b>AI Experience</b></a>
                    
                </li>
                 <li><a aria-label="Jump to Thermal section" href="#pageContent-sec-thermal"><b>Thermal</b></a>
                    
                </li>
                <li><a aria-label="Jump to Scalable section" href="#pageContent-sec-scalable"><b>Scalable</b></a>
                    
                </li>
                <li><a aria-label="Jump to Connectivity section" href="#pageContent-sec-port"><b>Connectivity</b></a>
                    
                </li>
                <li><a aria-label="Jump to Application section" href="#pageContent-sec-application"><b>Application</b></a>
                    
                </li>
                <li><a aria-label="Jump to Contac Us section" href="#pageContent-sec-started"><b>Contac Us</b></a>
                    
                </li>
                <li><a aria-label="Jump to FAQ section" href="#pageContent-sec-qa"><b>FAQ</b></a>
                    
                </li>
            </ul>
        </nav>
        <!-- <section id="BTN-Datasheet" class="product-content">
              <div class="maintitle">
                <a href="#" data-theme="White" target="_blank" rel="noreferrer noopener" download="RUC-1000G Datasheet.pdf" aria-label="Download ASUS NUC 15 Performance Datasheet" class="Button_Datasheet"><span class="Button_Datasheet_Text">
                        ASUS NUC 15 Performance Datasheet
                    </span> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" svg-inline role="presentation" focusable="false" tabindex="-1" class="Button_Datasheet_arrowDownloadSVG">
                        <path stroke="#181818" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="1.2" d="M16.5 9L12 13.5 7.5 9"></path>
                        <path stroke="#181818" stroke-linecap="round" stroke-width="1.2" d="M12 12.75V3"></path>
                        <path stroke="#181818" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2" d="M3.75 12.75v6h16.5v-6"></path>
                    </svg></a>
              </div>
            </section> -->
        <section id="ASUS-Datasheet">
            <p>
                <a href="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/asus-ascent-gx10-datasheet.pdf" data-theme="White" target="_blank" rel="noreferrer noopener" aria-label="Download ASUS Ascent GX10 Datasheet" download="ASUS Ascent GX10 Datasheet.pdf"><span>
                        ASUS Ascent GX10 Datasheet
                    </span> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" svg-inline="" role="presentation" focusable="false" tabindex="-1">
                        <path stroke="#181818" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" stroke-width="1.2" d="M16.5 9L12 13.5 7.5 9"></path>
                        <path stroke="#181818" stroke-linecap="round" stroke-width="1.2" d="M12 12.75V3"></path>
                        <path stroke="#181818" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2" d="M3.75 12.75v6h16.5v-6"></path>
                    </svg></a>
            </p>
        </section>
        <section id="pageContent-sec-overview" aria-labelledby="pageContent-title-kv">
            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/kv.jpg" width="2560" height="975" alt="Desk with ASUS Ascent GX10 and data display on monitor"></p>
            <div>
                <!-- <p class="h4 color-gradient-light">Ultra-Small AI Supercomputer</p> -->
                <h2 id="pageContent-title-kv"><span>Ultra-Small AI Supercomputer</span><strong>ASUS Ascent GX10</strong></h2>
                <h3>Based on NVIDIA DGX™ Spark</h3>
                <p>The ASUS Ascent GX10, accelerated by the NVIDIA GB10 Grace Blackwell Superchip and the NVIDIA AI software stack, provides a full-stack solution for AI development and deployment. Its compact design facilitates seamless integration and deployment, enabling powerful AI performance for innovators who demand excellence. With advanced AI tools and NVIDIA<sup>®</sup> ConnectX<sup>®</sup>-7, this small-scale server enhances your AI capabilities while empowering your unique solutions.</p>
            </div>
        </section>
        <div>
                <ul>
                    <li>
                        <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/V1/img/logo_nvidia.png" alt="The NVIDIA logo">
                        </p>
                        <p>NVIDIA<sup>®</sup> GB10 Grace Blackwell Superchip</p>
                    </li>
                    <li>
                        <p>NVIDIA<sup>®</sup><br>
                                Blackwell GPU<br>128 GB Unified memory</p>
                    </li>
                    <li>
                        <div>
                            <p>1 petaFLOP</p>
                            <p>AI Performance</p>
                            <!-- <p class="p3 text-left">1 petaFLOP</p> -->
                        </div>
                    </li>
                    <li>
                        
                    </li>
                    <li>
                        <div>
                            <p>NVIDIA<sup>®</sup><br>NVLink™-C2C</p>
                            <p>Deliver a cohesive CPU+GPU memory model with five times the bandwidth of PCIe Gen 5</p>
                        </div>
                    </li>
                    <li>
                        <p>NVIDIA DGX™ OS with Ubuntu Linux</p>
                    </li>
                    <li>
                        <div>
                            <p>NVIDIA<sup>®</sup> ConnectX<sup>®</sup>-7</p>
                            <p>Allows two GX10 systems to be linked for handling even larger models</p>
                        </div>
                    </li>
                    <li>
                        <p>Optimized Cooling Design</p>
                    </li>
                    <li>
                        <p>NVIDIA AI Software Stack</p>
                    </li>
                    <li>
                        <div>
                            <p>Al Development Environment</p>
                            <p>Includes NVIDIA NIM™ and Blueprints. Supports Pytorch, Jupyter, Ollama for prototyping and inference.</p>
                        </div>
                    </li>
                </ul>
            </div>
        
        <section id="pageContent-sec-design" aria-labelledby="pageContent-title-design">
            <div>
                <p>Design</p>
                <h2 id="pageContent-title-design">Revolutionary AI Performance on Your Desktop</h2>
                <p>
                    The groundbreaking ASUS Ascent GX10 AI Supercomputer, powered by the state-of-the-art NVIDIA<sup>®</sup> GB10 Grace Blackwell Superchip found in the NVIDIA DGX Spark, brings petaflop-scale AI computing capabilities directly to the desks of developers, AI researchers, and data scientists. This innovative device is designed to empower local AI development with its exceptional performance and advanced features.
                </p>
            </div>
            <div>
                <div>
                    <h3>Compact, Powerful, and Scalable</h3>
                    <div>
                        <p>Compact Size 150 x 150 x 51mm</p>
                        <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/pic_scalable.jpg" alt="Man in suit holds a levitating, sleek ASUS mini PC in a futuristic setting.">
                    </p></div>
                </div>
                <div>
                    <h3>Unparalleled AI Performance</h3>
                    <h4>Up to 1 petaFLOP of AI performance using FP4</h4>
                    <p>
                        Delivers up to 1 petaFLOP of AI performance to power large AI workloads.</p>
                    <h4>128 GB LPDDR5x Coherent Unified System Memory</h4>
                    <p>
                        Empowers model development, experimentation, and inferencing with ample memory capacity.</p>
                    <ul>
                        <li>
                            <p>Up to</p>
                            <p><strong>1 petaFLOP</strong></p>
                            <p>of AI Performance<br>Using FP4</p>
                        </li>
                        <li>
                            
                            <p><strong>128G</strong></p>
                            <p>Coherent Unified<br>System Memory</p>
                        </li>
                    </ul>
                </div>
            </div>
            <div>
                <div>
                    <h3>Cutting-Edge Architecture</h3>
                    <h4>NVIDIA GB10 Grace Blackwell Superchip:</h4>
                    <p>
                        Central to the ASUS Ascent GX10, this advanced chip features a robust Blackwell GPU with fifth-generation Tensor Cores and FP4 support.</p>
                    <h4>High-Performance 20-Core Arm CPU:</h4>
                    <p>
                        Enhances data preprocessing and orchestration, accelerating model tuning and real-time inferencing.</p>
                    <h4>NVLink™-C2C Technology: </h4>
                    <p>
                        Provides a cohesive CPU+GPU memory model with five times the bandwidth of PCIe 5.0.</p>
                    <!-- <ul class="ksp_box">
                            <li>
                                <p>Up to</p>
                                <p><strong>1,000</strong></p>
                                <p>AI TOPS<br>Processing Power</p>
                            </li>
                            <li>
                                <p>&nbsp;</p>
                                <p><strong>128G</strong></p>
                                <p>Coherent Unified<br>System Memory</p>
                            </li>
                        </ul> -->
                </div>
                <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/pic_gpu.png" alt="Close-up of an AI chip with glowing orange pathways on a circuit board.">
                </p>
            </div>
        </section>
        <div id="pageContent-sec-ai" aria-labelledby="pageContent-title-models">
                <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/pic_models.jpg" alt="Person typing on a laptop, surrounded by monitors displaying colorful code.">
                </p>
                <div>
                    <p>Performance</p>
                    <h2 id="pageContent-title-models">Handles Large Parameter Gen AI Models</h2>
                    <h3>Support for AI Models 200 Billion Parameters</h3>
                    <p>Prototype, fine-tune, and infer the latest AI reasoning models directly on your desktop.​</p><br>
                    <h3>Integrated NVIDIA<sup>®</sup> ConnectX<sup>®</sup>-7 Network Technology</h3>
                    <p>Link two ASUS Ascent GX10 systems to handle even larger models, such as Llama 3.1 with 405 billion parameters.</p>
                </div>
            </div>
        <div>
            <div id="pageContent-sec-security">
                    <div>
                        <p>AI Networking</p>
                        <h2>Meets Next-Gen Connectivity: NVIDIA<sup>®</sup> ConnectX-7</h2>
                        <p>The ASUS Ascent GX10 integrates NVIDIA<sup>®</sup> ConnectX-7 to deliver ultra-high-speed networking, enabling rapid data transfer and low-latency communication across distributed AI workloads.</p>
                    </div>
                    <div>
                        <div>
                            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/security-1.jpg" alt="Futuristic digital cityscape with blue lights and data streams, evoking innovation.">
                            </p>
                            <h3>Ultra-Fast AI Data Throughput</h3>
                            <p>Ultra-fast bandwidth ensures rapid data transfer between nodes, perfect for large-scale distributed AI workloads.</p>
                        </div>
                        <div>
                            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/security-2.jpg" alt="Blue lock surrounded by glowing lines and smaller locks, symbolizing cybersecurity.">
                            </p>
                            <h3>Secure, Intelligent Networking</h3>
                            <p>Built-in hardware acceleration for TLS, IPsec, and MACsec ensures encrypted data transmission without CPU overhead.</p>
                        </div>
                        <div>
                            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/security-3.jpg" alt="Robotic arms assemble electronics on a conveyor in a high-tech factory.">
                            </p>
                            <h3>Precision-Critical Performance</h3>
                            <p>IEEE 1588v2 PTP support enables microsecond-level time synchronization for time-sensitive AI and edge computing applications.</p>
                        </div>
                    </div>
                </div>
            <div id="pageContent-sec-experience">
                    <div>
                        <p>AI Experience</p>
                        <h2>Integrated AI Software Stack for Seamless Development</h2>
                    </div>
                    <div>
                        <div>
                            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/ico_experience_01.png" alt="Ready-to-use feature icon">
                            </p>
                            <h3>Preloaded AI for Instant Development</h3>
                            <div><p>NVIDIA DGX OS (Ubuntu-based) – Optimized AI environment, ready to use.</p><p>NVIDIA AI Software Stack – Preloaded frameworks, SDKs, and tools for fast deployment.</p></div>
                        </div>
                        <div>
                            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/ico_experience_02.png" alt="AI Tool feature icon">
                            </p>
                            <h3>Optimized AI Tools &amp; AI Frameworks</h3>
                            <div><p>CUDA, PyTorch, TensorFlow, Jupyter – Optimized for AI model development &amp; inference.</p><p>NVIDIA TensorRT – High-performance AI inference engine.</p><p>NVIDIA NIMs &amp; Blueprints – Prebuilt AI workflows &amp; microservices.</p></div>
                        </div>
                        <div>
                            <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/ico_experience_03.png" alt="AI Mode feature icon">
                            </p>
                            <h3>Industry-Leading AI Model Support</h3>
                            <div><p>DeepSeek R1 – AI inference optimized up to 70B parameters.</p><p>Llama 3.1 – Generative AI up to 405B parameters (dual-GX10).</p><p>Meta, Google models – Broad compatibility with industry-leading AI frameworks.</p></div>
                        </div>
                    </div>
                </div>
            <section id="pageContent-sec-thermal">
                <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/GX10_Thermal_3D.jpg" width="2560" height="975" alt="ASUS Ascent GX10 thermal design showcasing dual fans and efficient airflow"></p>
                <div>
                    <p>Thermal</p>
                    <h2>Precision-Crafted for Ultimate Thermal Efficiency</h2>
                    <p>Tackle AI's most demanding workloads with the ASUS Ascent GX10. Its advanced dual-fan design with 7-level control delivers smooth, precise airflow. With 1.6× more efficient thermal coverage than comparable compact systems, the GX10 stays cooler and consistently performs at its peak.</p>
                </div>
            </section>
            <div id="pageContent-sec-scalable">
                    <div>
                        <p>Scalable</p>
                        <h2>Engineered for Maximum Efficiency</h2>
                    </div>
                    <div>
                        <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/pic_scalable.png" alt="Two stacked ASUS GX10 devices on a dark surface, exuding modern tech."></p><div>
                            <p>Optimized cooling design ensure sustained AI performance under heavy workloads</p>
                            <p>Compact from factor, delivering high-density AI computing in a small footprint</p>
                        </div>
                    </div>
                </div>
        </div>
        <div id="pageContent-sec-port">
                    <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/line_pd.png" alt="Silver ASUS GX10 AI supercomputer with rear ports on dark blue background.">
                        <img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/line_pd-mobile.png" alt="Silver ASUS GX10 AI supercomputer with rear ports on dark blue background.">
                    </p>
                    <div>
                        <p><span>1</span><b>Kensington Lock Slot</b></p>
                        <p><span>2</span><b>1 x USB 3.2 Gen 2x2 Type-C, 20Gbps,</b><br>alternate mode (DisplayPort 2.1), with PD in(180W EPR PD3.1 SPEC)</p>
                        <p><span>3</span><b>3 x USB 3.2 Gen 2x2 Type-C, 20Gbps,</b><br>alternate mode (DisplayPort 2.1)</p>
                        <p><span>4</span><b>HDMI 2.1b port</b></p>
                        <p><span>5</span><b>10 GbE LAN</b></p>
                        <p><span>6</span><b>NVIDIA ConnectX®-7 NIC</b></p>
                    </div>
                </div>
        <section id="pageContent-sec-application" aria-labelledby="pageContent-title-application">
            <div>
                <p>Application</p>
                <h2 id="pageContent-title-application">Local Development, Scalable Deployment</h2>
            </div>
            <div>
                <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/pic_deployment.jpg" alt="Two people working at a computer in a futuristic setting.">
                </p>
                <div>
                    <h3>Seamless Transition to Cloud</h3>
                    <p>Leverage NVIDIA AI platform software architecture to move models from desktop environments to DGX Cloud or any accelerated cloud or data center infrastructure with minimal code adjustments.</p>
                    <h3>Cost-Effective Experimentation Platform</h3>
                    <p>Free up essential compute resources in clusters better suited for training and deploying production models.</p>
                </div>
            </div>
            <div>
                <div>
                    <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/application-1.jpg" alt="Engineers review a 3D holographic gear model, symbolizing innovation.">
                    </p>
                    <h4>Prototyping</h4>
                </div>
                <div>
                    <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/application-2.jpg" alt="Modern control room with people at computers and blue-lit screens showing complex data.">
                    </p>
                    <h4>Fine Tuning / Inference</h4>
                </div>
                <div>
                    <p><img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/application-3.jpg" alt="Person coding on a laptop, flanked by two monitors displaying colorful code.">
                    </p>
                    <h4>Data Science</h4>
                </div>
            </div>
        </section>
        <div id="pageContent-sec-started" aria-labelledby="pageContent-title-started">
                <h2 id="pageContent-title-started">Stay Informed About Future Innovations!</h2>
                <p>Sign up to be the first to know about upcoming ASUS Ascent products and technologies.</p>
                <!-- <p class="txt_Notify color-white">Notify Me</p> -->
                <p><a href="https://www.connect.asus.com/notify-me-202503" aria-label="Click Notify Me button to leave your info and get updates when ASUS Ascent GX10 is available." target="_blank" onclick="
                                window.dataLayer = window.dataLayer || [];
                                dataLayer.push({
                                    'event' : 'data_layer_event',
                                    'event_name_ga4':'ascent_gx10_notify_me',
                                    'event_category_DL':'button',
                                    'event_action_DL':'clicked',
                                    'event_label_DL':'Notify_Me',
                                    'event_value_DL':'0'
                                });
                                ">
                    <span>Contact Us</span></a></p><p>*Specifications and product images are subject to change.</p>
            </div>
        <div id="pageContent-sec-qa" aria-labelledby="pageContent-title-qa" aria-label="FAQ">
        <h2>FAQ</h2>
        <div>
          <ul>
                <li id="faq1">
                    <p>
                        What is the memory bandwidth supported by Ascent GX10?
                        
                    </p>
                    <p><span id="faq1-label">What is the memory bandwidth supported by Ascent GX10?</span>
                        AI applications often require a bigger memory. With the NVIDIA Blackwell GPU that supports 128GB of unified memory, ASUS Ascent GX10 is an AI supercomputer that enables faster training, better real-time inference, and support larger models like LLMs.
                    </p>
                </li>
                <li id="faq2">
                    <p>
                        What software and tools are included with the ASUS Ascent GX10?
                        
                    </p>
                    
                </li>
                <li id="faq3">
                    <p>
                        Can multiple Ascent GX10 be clustered for greater AI model capacity?
                        
                    </p>
                    
                </li>
                <li id="faq4">
                    <p>
                        What are the main differences between the Ascent GX10 and NVIDIA DGX Spark?
                        
                    </p>
                    
                </li>
                <li id="faq5">
                    <p>
                        What AI applications does the Ascent GX10 support?
                        
                    </p>
                    
                </li>
                <li id="faq5">
                    <p>
                        Can I use Ascent GX10 for education?
                        
                    </p>
                    
                </li>
                <li id="faq5">
                    <p>
                        Can I use Ascent GX10 for enterprise?
                        
                    </p>
                    
                </li>
          </ul>
        </div>
      </div>
        <!-- For PDC content -->
        <!-- <section id="pdc">
              <div class="product_gallery theme-white"></div>
              <div class="product_awards theme-white"></div>
              <div class="product_videos theme-white"></div>
              <div class="product_medias theme-white"></div>
            </section> -->
        <!-- <section class="sec-video bg-black">
              <div class="hd-container">
                <div class="vid-container trigger-video-toggle">
                  <video src="v1/video/product.mp4" poster="v1/video/product.jpg" muted playsinline autoplay loop id="pageContent-video-product" width="1920" height="1080" aria-label="RT-BE50 angled views">
                    <track src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/video/product.vtt" kind="descriptions" srclang="en" label="English" default="">
                  </video>
                  <button type="button" class="vid-control" aria-controls="pageContent-video-product" aria-label="Pause ASUS RT-BE86U Product video" data-on="Pause ASUS RT-BE50 Product video" data-off="Play ASUS RT-BE50 Product video">
                    <img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/play.svg" alt="" class="play" loading="lazy">
                    <img src="https://dlcdnwebimgs.asus.com/files/media/202506/5c0fb57c-4e48-4e96-8c97-04bf8df2677c/v1/img/pause.svg" alt="" class="pause" loading="lazy">
                  </button>
                </div>
                <div class="btn-container">
                  <button type="button" class="trigger-video hd-btn" aria-label="Click to watch the product video" aria-controls="pageContent-lightbox" aria-expanded="false" data-type="iframe" data-src="https://www.youtube.com/embed/uP3qJB3n4-Y" onclick="
                  dataLayer.push({
                  'event': 'data_layer_event',
                  'event_category_DL':'videos',
                  'event_action_DL':'played',
                  'event_label_DL':'Watch Now',
                  });">Watch Now <span></span></button>
                </div>
              </div>
            </section> -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reminder to passengers ahead of move to 100% digital boarding passes (148 pts)]]></title>
            <link>https://corporate.ryanair.com/news/ryanair-issues-reminder-to-passengers-ahead-of-move-to-100-digital-boarding-passes-from-wednesday-12-nov/</link>
            <guid>45877027</guid>
            <pubDate>Mon, 10 Nov 2025 15:46:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corporate.ryanair.com/news/ryanair-issues-reminder-to-passengers-ahead-of-move-to-100-digital-boarding-passes-from-wednesday-12-nov/">https://corporate.ryanair.com/news/ryanair-issues-reminder-to-passengers-ahead-of-move-to-100-digital-boarding-passes-from-wednesday-12-nov/</a>, See on <a href="https://news.ycombinator.com/item?id=45877027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Ryanair, Europe’s No.1 airline, today (Thurs, 6 Nov) reminded passengers that from Wed (12 Nov) it will move to 100% digital boarding passes.<em> </em>This means that from Wed (12 Nov) passengers will no longer be able to download and print a physical paper boarding pass but will instead need to use the digital boarding pass generated in their “myRyanair” app during check-in to board their Ryanair flight.</p>



<p>This transition, already adopted by nearly 80% of Ryanair’s 207M+ annual passengers, will deliver a faster, smarter, and greener travel experience. It will also give passengers easier access to a range of innovative in-app features, including:</p>



<ul>
<li><strong>Order to Seat:</strong> Order food and drinks from your phone and get served first.</li>



<li><strong>Live Flight Information:</strong> Real-time status updates on boarding, gates, and delays.</li>



<li><strong>Direct Updates:</strong> Instant notifications from Ryanair’s operations centre during disruption.</li>



<li><strong>Alternative Flight Options:</strong> Real-time alternative flight options during disruption.</li>



<li><strong>Travel Documents:</strong> accessible in one convenient place. </li>
</ul>



<p><strong>Ryanair CMO, Dara Brady, said:</strong></p>



<p><em>“We are now just a little less than a week out from our move to 100% digital boarding passes, meaning that from Wednesday, 12 November, passengers will no longer be able to download and print a physical paper boarding pass but will instead need to use the digital boarding pass generated in their “myRyanair” app during check-in to board their Ryanair flight.</em></p>



<p><em>While over 80% of passengers already use digital boarding passes, and therefore won’t be affected by this progressive change, we remind the small number of passengers who still print boarding passes to download the myRyanair app ahead of the move to 100% digital boarding passes from Wednesday, 12 November.</em></p>



<p><em>Moving fully digital means a faster, smarter, and greener experience for passengers, whilst also providing easier access to a range of innovative in-app features, including ‘Order to Seat’, live flight information and direct updates during disruption. We look forward to delivering an enhanced travel experience for 100% of our customers, streamlined through our best-in-class myRyanair app.”</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig and the design choices within (118 pts)]]></title>
            <link>https://blueberrywren.dev/blog/on-zig/</link>
            <guid>45876990</guid>
            <pubDate>Mon, 10 Nov 2025 15:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blueberrywren.dev/blog/on-zig/">https://blueberrywren.dev/blog/on-zig/</a>, See on <a href="https://news.ycombinator.com/item?id=45876990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


      



  <meta property="og:title" content="Opinion piece: On Zig (and the design choices within)">
  <title> Opinion piece: On Zig (and the design choices within) </title>




<div>
  <p><strong>2025-10-14</strong></p>
  
  <p><img src="https://blueberrywren.dev/wren_glasses.png" alt="A Wren.">  
</p></div>

<details><summary>
Table of contents
</summary>

    <ul>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#ideological-differences">Ideological differences</a>
            
                <ul>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#memory">Memory</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#philosophy">Philosophy</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#practical">Practical</a>
            
                <ul>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#comptime">Comptime</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#casting">Casting</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#semantics">Semantics</a>
            
                <ul>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#result-location-semantics-rls">Result location semantics (RLS)</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#pointer-reference-optimization">Pointer reference optimization</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#misc-improvable-things">Misc. Improvable things</a>
            
                <ul>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#speed">Speed</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#tooling">Tooling</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#debug-mode">Debug mode</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#misc-improvable-things-that-won-t-be-improved-for-reasons-beyond-me">Misc. Improvable things that won't be improved for reasons beyond me</a>
            
                <ul>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#behavior-around-undefined">Behavior around undefined</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#tabs">Tabs</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#iteration">Iteration</a>
                        </li>
                    
                        <li>
                            <a href="https://blueberrywren.dev/blog/on-zig/#warnings">Warnings</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#my-only-note-on-the-community">My only note on the community</a>
            
        </li>
    
        <li>
            <a href="https://blueberrywren.dev/blog/on-zig/#summary">Summary</a>
            
        </li>
    
    </ul>

</details>

<p>(From someone who has spent far too much time thinking about the designs of programming languages)</p>
<p>This post is split up into a few sections. I would also like to preface this post with:</p>
<ol>
<li>Existing memory safe languages are not perfect. Rust will be used as an example quite a bit, as from what is currently avaliable, it embodies closer to what I think is reasonable in a programming language. However, there is still much room for improvement in this space.</li>
<li>All of the following is ultimately my opinion. If you disagree, you are welcome to find me somewhere on the internet and ask me more.</li>
<li>I bear no grudge or ill will against anyone who contributes to, works on, or is otherwise involved in the Zig community. The following are things I have noted about the language, not the people behind it. If you enjoy Zig, go and write it, and make cool stuff - I am not the programming language police.</li>
<li>Exception to the above: I have one gripe about the Zig community. This gripe "targets" no one person in particular; rather, larger trends.</li>
</ol>
<h2 id="ideological-differences">Ideological differences</h2>
<h3 id="memory">Memory</h3>
<p>First, and very much foremost, Zig is not memory safe. This is, in my opinion, the most
egregious thing in this post, by a very large margin. Moreso, Zig does not make any attempt to
be memory safe -
it can catch some things at runtime, with specific allocators, but so can C these days. Indeed, there
are some cases, like use-after-realloc, that <code>asan</code> can catch and Zig cannot.</p>
<p>A language in the modern day that does not make an attempt at memory safety is, in my opinion, not reasonable. It has been shown that in some areas, up to 70% of security bugs are due to memory safety issues (<a href="https://www.chromium.org/Home/chromium-security/memory-safety/">Source</a>).</p>
<p>I subscribe to the idea that the user must be constrained. It is perhaps harsh to say, but for large and complex programs, I believe that there are very few programmers who will write memory-correct code nine times out of ten. When writing code with others, that goes down. I personally do not believe I fit into that category.</p>
<p>The fact that Zig allows the user to write faulty software is supported by various somewhat informal, but still useful, statistics. Notably, the following statistic disregard duplicates, and unreported errors. However, general trends are still of note. </p><details><summary>Here are some:</summary>
<ol>
<li>The Rust compiler has had a lifetime 59,780 issues reported. Of these, 4,158 contain one of "crash", "segmentation fault", or "segfault".</li>
<li>The Zig compiler has had a lifetime 13,269 issues reported. Of these, 2,260 contain one of "crash", "segmentation fault", or "segfault".</li>
</ol>
<p>This means that, roughly:</p>
<ol>
<li>7% of lifetime issues reported in the Rust compiler are crashes.</li>
<li>17% of lifetime issues reported in the Zig compiler are crashes.</li>
</ol>
<p>Not ideal.</p>
<ol>
<li>The Node/v8 JS runtime, written partially itself in JS and partially in C++, has had a lifetime 19,631 issues reported, 1,710 of which contain one of the above keywords.</li>
<li>The Deno JS runtime, written primarily in Rust, has had a lifetime 13,422 issues reported, 552 of which contain one of the above keywords.</li>
<li>The Bun JS runtime, written in Zig, has had a lifetime 13,828 issues reported, 3676 of which contain one of the above keywords.</li>
</ol>
<p>Again, this roughly means:</p>
<ol>
<li>8.7% of lifetime issues reported in Node are crashes.</li>
<li>4.1% of lifetime issues reported in Deno are crashes.</li>
<li>26.5% (!!!) of lifetime issues reported in Bun are crashes.</li>
</ol>
<p>Also not great. Again, these statistics are slightly off at best simply due to the nature of their collection, but the trends do not lie.</p>
</details>
<p>If you're not reading the above: It can be summarized as "Not ideal."</p>
<h3 id="philosophy">Philosophy</h3>
<p>At one point, this part of the article contained a runthrough of the <code>zig zen</code>, and my opinions on each bullet point. I have decided that that is not a constructive discussion of Zig. It suffices for me to say that I do not believe Zig particularly embodies its own zen.</p>
<h2 id="practical">Practical</h2>
<h3 id="comptime">Comptime</h3>
<p>Zig does generics in an odd way. I believe this is the best way of putting it.
This post is not meant to be, nor will it contain, a proper explanation of Zig's <code>comptime</code>
capabilities, so I refer the reader to the wider internet there. However, doing generics
with "normal" code means that there are multiple ways to write the same generic function.
There is no standardization between different libraries, different styles of writing code, and
different users of Zig. Every person can do generics in their own special way.</p>
<p>This obviously has slightly dire effects on readability. In practice, most Zig users are reasonable enough
to stick to some "common patterns" of doing generics and similar, but it is widely known that if the user is giving the ability to do something, it will be done. I believe that generics are important enough they should be first class.</p>
<p>Arguments can be made that Zig's comptime is also useful for other things, not just generics. Some examples I have been given are conditional compilation, and variadic functions. Beyond these, I am yet to see a convincing motivating example that <em>requires</em> the machinery that Zig provides. Every such example can, in my experience, be solved with less powerful (and hence, for the most part, less confusing) machinery.</p>
<p>As a result, I am inclined to believe that Zig's comptime is a very large and all-encompassing feature that ultimately brings very little to the table that smaller features cannot.</p>
<p>I am personally a proponent of a good macro system, but I will readily admit people can also go overboard with one of those.</p>
<h3 id="casting">Casting</h3>
<p>Zig's casting is a bit cumbersome. To cast a float to a specific int width, for example, must be done with <code>@as(i32, @intFromFloat(flt))</code>. Bit of a mouthful. Inference can help here (For example if a variable is already known to be a <code>i32</code>, the outer cast is not needed), but I would think that with Zig's comptime abilities, this could be made a bit nicer. Luckily, it is common Zig practice to annotate everything if possible, so this does come up slightly less in practice. It is still a bit bulky however.</p>
<p>Float to int casting additionally can invoke undefined behavior if the float is outside the integer's range. I personally prefer truncating semantics, with perhaps a specialized method (<code>intFromFloatUnsafe</code>?) for UB semantics. That's more of a personal preference though.</p>
<h2 id="semantics">Semantics</h2>
<h3 id="result-location-semantics-rls">Result location semantics (RLS)</h3>
<p><a href="https://ziglang.org/documentation/0.15.1/#Result-Location-Semantics">See here</a>.</p>
<p>Result location semantics are in theory a quite nice idea. Knowing predictably where things are going to be placed in memory is, of course, good for any systems-adjacent language! In practice, there are several choices within RLS that I find counterintuitive. Take the following example, where we attempt to swap two struct members in place:</p>
<pre data-lang="rust"><code data-lang="rust"><span>
</span><span>const</span><span> std </span><span>= @</span><span>import</span><span>(</span><span>"std"</span><span>);
</span><span>const</span><span> What </span><span>= </span><span>struct </span><span>{
</span><span>    a: i32,
</span><span>    b: i32,
</span><span>};
</span><span>
</span><span>pub </span><span>fn </span><span>main</span><span>() void {
</span><span>    var x: What </span><span>= </span><span>.{ .a </span><span>= </span><span>1</span><span>, .b </span><span>= </span><span>2 </span><span>};
</span><span>    x </span><span>=</span><span> What { .a </span><span>=</span><span> x.b, .b </span><span>=</span><span> x.a };
</span><span>    std.log.</span><span>info</span><span>(</span><span>"x: {}"</span><span>, .{x});
</span><span>
</span><span>    var y: What </span><span>= </span><span>.{ .a </span><span>= </span><span>1</span><span>, .b </span><span>= </span><span>2 </span><span>};
</span><span>    y </span><span>= </span><span>.{ .a </span><span>=</span><span> y.b, .b </span><span>=</span><span> y.a };
</span><span>    std.log.</span><span>info</span><span>(</span><span>"y: {}"</span><span>, .{y});
</span><span>}
</span></code></pre>
<p>This outputs the following:</p>
<pre data-lang="rust"><code data-lang="rust"><span>info: x: .{ .a </span><span>= </span><span>2</span><span>, .b </span><span>= </span><span>1 </span><span>}
</span><span>info: y: .{ .a </span><span>= </span><span>2</span><span>, .b </span><span>= </span><span>2 </span><span>}
</span></code></pre>
<p>I will note that the equivalent C can only be written in the former style, and it prints the former.</p>
<p>The only difference between the latter and the former is whether the type name is present. This is intended behavior! Indeed, this very example is given in the Zig reference, a fact that I find odd. Why give a warning against something when you could just fix it? Zig does not have move or copy constructors; I do not think there is very much reason for the latter to ever behave the way it does. One extra register is all you ever need, even for an arbitrary parallel move! (<a href="https://compiler.club/parallel-moves/">Source</a>.)</p>
<h3 id="pointer-reference-optimization">Pointer reference optimization</h3>
<p>In its previous form, PRO has been removed from Zig. There was a long period where this would print <code>5</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>const</span><span> std </span><span>= @</span><span>import</span><span>(</span><span>"std"</span><span>);
</span><span>
</span><span>const </span><span>AAAA </span><span>= </span><span>struct </span><span>{
</span><span>    foo: [100]u32,
</span><span>};
</span><span>
</span><span>fn </span><span>kindabad</span><span>(</span><span>a</span><span>: AAAA, </span><span>b</span><span>: *AAAA) void {
</span><span>  b.</span><span>*</span><span>.foo[</span><span>0</span><span>] </span><span>= </span><span>5</span><span>;
</span><span>
</span><span>  std.debug.</span><span>print</span><span>(</span><span>"unideal: {}"</span><span>, .{ a.foo[</span><span>0</span><span>] });
</span><span>}
</span><span>
</span><span>pub </span><span>fn </span><span>main</span><span>() </span><span>!</span><span>void {
</span><span>    var f: </span><span>AAAA </span><span>=</span><span> undefined;
</span><span>
</span><span>    f.foo[</span><span>0</span><span>] </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>kindabad</span><span>(f, </span><span>&amp;</span><span>f);
</span><span>}
</span></code></pre>
<p>As Zig would correctly, per PRO at the time, but incorrectly per common sense, pass <code>a</code> as a reference.</p>
<p>This is actually mildly unfortunate. It's a very interesting optimization, and being able to guarantee behavior around optimization of parameter passing would also be quite beneficial. Unfortunately, Zig simply does not have the level of control needed to do it. Aliasing is freely allowed, and in order to make PRO work, it cannot be. Rust can, and in many cases does, do this! It's a shame Zig has to miss out.</p>
<p>There is current work to make PRO work in the case of pure functions (<a href="https://github.com/ziglang/zig/issues/5973#issuecomment-2380332493">Source</a>), so I remain hopeful that it will return in some form eventually.</p>
<h2 id="misc-improvable-things">Misc. Improvable things</h2>
<h3 id="speed">Speed</h3>
<p>The Zig compiler is not particularly fast. The LLVM backend even more so; compared to Clang, (which has the home field advantage, but can still be quite slow), and by my measurements, Clang is consistently between 3-10x faster. A new Zig backend has been written to help fix this; by similar measurements, it is consistently faster than Clang by 1.5-2x, and much faster than the Zig LLVM backend. While these numbers are nothing to scoff at, I believe there is more room for improvement, and I would like to see where it goes. Zig is a simpler language than C when comptime is not involved, and I would be excited to see that reflected in the benchmarks. It is also possible that Zig could use LLVM more efficiently, but I cannot comment on this without digging into the internals of the compiler.</p>
<p>Of course, all of this is quite reasonable. The Zig compiler has had far, far fewer man-hours put into it, and this of course reflects in aspects of the compiler that simply take a lot of time, such as these. I look forward to seeing what can be done in the future.</p>
<h3 id="tooling">Tooling</h3>
<h4 id="build-system">Build system</h4>
<p>The Zig build system is a little confusing. It is very neat to be able to write build system code in the language itself, and this is a feature I believe more languages should have. Unfortunately, it is as of current not well documented enough to justify its own complexity. This of course can be improved, and I hope it is.</p>
<h4 id="language-server">Language server</h4>
<p>Zig currently does not have a "first class" language server. The "Zig Language Server", or ZLS, is unofficial; It does not have real compiler integration, so it is unfortunately limited in some ways. The creator of Zig apparently does not use a language server while programming, so I do slightly understand why it is not a priority, but I personally believe tools like a good language server are quite essential to wider/popular usage. Here are some quotes I have heard from people who have used it far more than I:</p>
<ul>
<li>"Deeply horrid"</li>
<li>"It is one of the worst LSPs I have ever used"</li>
<li>"Don't get me started"</li>
</ul>
<p>Apparently it is very limited when <code>comptime</code> comes into play, and cannot handle compound types like 2D arrays.</p>
<p>I cannot offer much firsthand experience however.</p>
<h3 id="debug-mode">Debug mode</h3>
<p>There are always more errors to be caught. I particularly recently noticed that Zig cannot currently catch use-after-realloc errors, like the following:</p>
<pre data-lang="rust"><code data-lang="rust"><span>const</span><span> std </span><span>= @</span><span>import</span><span>(</span><span>"std"</span><span>);
</span><span>
</span><span>pub </span><span>fn </span><span>main</span><span>() </span><span>!</span><span>void {
</span><span>    </span><span>const</span><span> allocator </span><span>=</span><span> std.heap.page_allocator;
</span><span>
</span><span>    var buffer </span><span>=</span><span> try allocator.</span><span>alloc</span><span>(</span><span>u8</span><span>, </span><span>4</span><span>);
</span><span>
</span><span>    buffer[</span><span>0</span><span>] </span><span>= </span><span>1</span><span>;
</span><span>    
</span><span>    </span><span>const</span><span> new_buffer </span><span>=</span><span> try allocator.</span><span>realloc</span><span>(buffer, </span><span>8</span><span>);
</span><span>    
</span><span>    </span><span>// Should be caught!
</span><span>    buffer[</span><span>0</span><span>] </span><span>= </span><span>99</span><span>; 
</span><span>
</span><span>    allocator.</span><span>free</span><span>(new_buffer);
</span><span>}
</span></code></pre>
<p>Address sanitizer with Clang can catch this, so theoretically Zig should be able to catch it (perhaps with a similar system) too.</p>
<h2 id="misc-improvable-things-that-won-t-be-improved-for-reasons-beyond-me">Misc. Improvable things that won't be improved for reasons beyond me</h2>
<h3 id="behavior-around-undefined">Behavior around <code>undefined</code></h3>
<h4 id="debug-mode-1">Debug mode</h4>
<p>Apparently, it is desired that comparisons with <code>undefined</code> panic. This seems reasonable to me. However, the issue for this has been open <a href="https://github.com/ziglang/zig/issues/63">for almost ten years</a>, which makes me worry that it might be a while still until it is finished.</p>
<p>This panics in <code>Debug</code> and <code>ReleaseSafe</code> modes, however, so it does at least work in some cases.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>main</span><span>() void {
</span><span>    </span><span>const</span><span> x: </span><span>u32 </span><span>=</span><span> undefined;
</span><span>    </span><span>if </span><span>(x </span><span>== </span><span>0</span><span>) {
</span><span>        </span><span>return </span><span>1</span><span>;
</span><span>    } </span><span>else </span><span>{
</span><span>        </span><span>return </span><span>0</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<h4 id="safety">Safety</h4>
<p>According to <a href="https://github.com/ziglang/zig/issues/21915">this issue</a>, it is "not a goal of the Zig compiler" to catch issues like the following:</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>main</span><span>() void {
</span><span>    var buf: []</span><span>u8 </span><span>=</span><span> undefined;
</span><span>    
</span><span>    </span><span>// Very not allowed!
</span><span>    buf[</span><span>0</span><span>] </span><span>= </span><span>1</span><span>;
</span><span>}
</span></code></pre>
<p>This just seems silly to me. Catching something like this, in the simple case, should be just one pass over the AST; why not do it?</p>
<h3 id="tabs">Tabs</h3>
<p>Zig simply doesn't allow tabs in comments and strings? <a href="https://github.com/ziglang/zig-spec/issues/38">This issue</a> explains more, but the justification of "it (a tab) is ambiguous (in) how it should be rendered", I do not quite agree with.</p>
<p>Strings make some sense; you can still write <code>\t</code> for a tab, and they do indeed make output ambiguous. However, they're still allowed in indentation! This means that in commenting out code, you can take a program from valid to invalid. <code>zig fmt</code> does reify tabs into spaces, but if your editor is misconfigured and you're not running <code>zig fmt</code> constantly?</p>
<pre><code><span>tabs.zig:5:36: error: comment contains invalid byte: '\t'
</span><span>//        const str: *const [1:0]u8 = "w";
</span><span>  ^~~~~~~~~~~
</span></code></pre>
<p>Very odd.</p>
<h3 id="iteration">Iteration</h3>
<p>Zig does not have a way to iterate through a slice or similar in any way except one-at-a-time forwards. Anything else? You're using a <code>while</code> loop, and you'll enjoy it. Zig blanket bans proposals to change the language, so despite issues like <a href="https://github.com/ziglang/zig/issues/21814">this one</a>, I doubt this will happen any time soon.</p>
<h3 id="warnings">Warnings</h3>
<p>Zig doesn't have 'em. Everything is an error. In practice, this mostly manifests through reasonably harmless things like "unused variable" notices becoming errors, which is reasonably annoying when just trying to spitball.</p>

<p>For the most part, the community is about par for the course for programming language communities; that is to say, quite nice. However, I have had some reasonably negative interactions that I feel are somewhat of a general trend. They usually proceed like the following:</p>
<ol>
<li>I ask a question about solving something in Zig. Often, given my background, it is about adapting my more functional-esque thinking to Zig.</li>
<li>Someone chimes in with a half-solution that does not actually address my problem.</li>
<li>I attempt to communicate this to them, which often involves explaining the features I would use in &lt;insert other language&gt;.</li>
<li>They insist that these features are not needed, but do not elaborate on how one would solve the problem without them.</li>
</ol>
<p>This is unpleasant, and while it's not a particularly uncommon thing to see in programming language communities, Zig seems to have a bit of a bad case of it. I suspect it is due to Zig's fairly minimalistic nature; it lacks a lot of features that one would otherwise use to solve problems. Of course, this is the appeal for many, but still. It is made worse when said topic is something one is particularly knowledgeable about, and the people you are conversing with believe they can solve the issue without that knowledge.</p>
<p>I do not believe that a few bad apples necessarily spoil the whole barrel here, but they definitely sour it.</p>
<h2 id="summary">Summary</h2>
<p>I find Zig interesting, with an unfortunately negative connotation. I believe the goal of a C-like memory unsafe language "for the modern day", while interesting at first glance, ignores many of the issues that make C a problem in said modern day. Much of Zig seems to me like "wishful thinking"; if every programmer was 150% smarter and more capable, perhaps it would work. Alas, they are not; myself included.</p>
<p>I believe that modern concerns of memory safety and correctness require modern solutions; not performing patchwork fixes over the core issue.</p>




      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs are steroids for your Dunning-Kruger (357 pts)]]></title>
            <link>https://bytesauna.com/post/dunning-kruger</link>
            <guid>45876744</guid>
            <pubDate>Mon, 10 Nov 2025 15:14:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bytesauna.com/post/dunning-kruger">https://bytesauna.com/post/dunning-kruger</a>, See on <a href="https://news.ycombinator.com/item?id=45876744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div> <p>In his 1933 essay <em>“The Triumph of Stupidity,”</em> Bertrand Russell remarked that<!-- --> <em>“the problem with the world is that the stupid are cocksure, while the intelligent are full of doubt.”</em> This is something I often think about when ChatGPT hits me up with another <em>“that’s a fantastic idea”</em> when the idea is clearly anything but great.</p> <!-- --> <p>How often do you think a ChatGPT user walks away not just misinformed, but misinformed with conviction? I would bet this happens all the time. And I can’t help but wonder what the effects are in the big picture.</p> <!-- --> <p>I can relate to this on a personal level: As I ChatGPT user I notice that I’m often left with a sense of certainty. After discussing an issue with an LLM I feel like I know something — a lot, perhaps — but more often than not this information is either slightly incorrect or completely wrong. And you know what? I can’t help it. <strong>Even when I acknowledge this illusion, I can’t help chasing the wonderful feeling of conviction these models give.</strong> It’s great to<!-- --> <em>feel</em> like you know almost everything. Of course I come back for more. And it’s just not the feeling; I would be dishonest to claim these models wouldn’t have huge utility. Yet I’m a little worried about the psychological dimension of this whole ordeal.</p> <!-- --> <p>They say AI is a mirror. This summarizes my experience. I feel LLMs “amplify” thinking. These models make your thoughts reverberate by taking them to multiple new directions. And sometimes these directions are really interesting. The thing is, though, that this goes both ways: A short ChatGPT session may help improve a good idea to a great idea. On the other hand, LLMs are amazing at supercharging self-delusion. These models will happily equip misguided thinking with a fluent, authoritative voice, which, in turn, sets up a psychological trap by delivering nonsense in a nice package.</p> <!-- --> <p>And it’s so insanely habit-forming! I almost instinctively do a little back and forth with an LLM when I want to work on an idea. It hasn’t even been that long (these models have been around for, what, three years?) and I’m so used to them that I feel naked without. It’s getting even comical sometimes. When I lost my bag the other day and was going through the apartment looking for it, my first response to my growing frustration was that “I should ask ChatGPT where it is”.</p> <!-- --> <p>I feel like LLMs are a fairly boring technology. They are stochastic black boxes. The training is essentially run-of-the-mill statistical inference. There are some more recent innovations on software/hardware-level, but these are not LLM-specific really. Is it too sardonic to say that the real “innovation” was throwing enough money at the problem to train the models at a huge scale? Maybe RLHF was a real innovation; I’m not sure. However, I don’t really feel like there is a lot to be interested in there. And yet, the current AI boom is extraordinarily interesting.</p> <!-- --> <p>It’s the impact. The very real effect of all this in our lives. In hindsight, this will probably be one of the major shifts, and it will be reflected upon in terms of education, work and even society at large. Language cuts to the core of what and who we are. Speech is so natural to us that we even think in speech. And when a machine credibly stepped into that territory, something changed. I’m not sure what it is — I don’t think anyone really knows at this point — but I think there is a <em>sense</em> of shifting tides. I think it’s something most of us are trying to make sense of.</p> <!-- --> <p>I think LLMs should <strong>not</strong> be seen as knowledge engines but as confidence engines. That, I feel, would better illustrate the potential near and medium-term futures we are dealing with.</p> </div><div><figure><div><p><img src="https://bytesauna.com/me.webp" alt="My photo" loading="lazy"></p></div></figure><div><h2>Article by</h2><p>Matias Heikkilä</p></div></div><div><p>Let's stay in touch</p><p>Subscribe to the newsletter for ideas that help you lead with focus and navigate complexity in software, strategy, and AI. If you’re looking to sharpen your team’s next move, let’s schedule a call. We’ll explore how we can turn insight into action together.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Time to start de-Appling (475 pts)]]></title>
            <link>https://heatherburns.tech/2025/11/10/time-to-start-de-appling/</link>
            <guid>45876598</guid>
            <pubDate>Mon, 10 Nov 2025 14:57:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heatherburns.tech/2025/11/10/time-to-start-de-appling/">https://heatherburns.tech/2025/11/10/time-to-start-de-appling/</a>, See on <a href="https://news.ycombinator.com/item?id=45876598">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><span>I</span>‘ve done such a thorough job of de-Googling that I forgot to show up for a meeting with someone, because I hadn’t checked my Google calendar in ages. (No, they were not amused.) In my defense, I proceeded to explain to them that having de-Googled, I was also in the process of de-Appling, which is a special bonus level that those of us in the UK have unlocked.</p>
<p>If you’re reading this in the sunlit uplands, you need to start that too.</p>

<p>You need to start that because, <a href="https://support.apple.com/en-us/122234">as we recently learned</a>, at some point in the very near future Apple is withdrawing its Advanced Data Protection (ADP) feature from the UK altogether as a result of the Home Office TCN through the Investigatory Powers Act.</p>
<p>Users who already had ADP enabled when the first TCN <a href="http://www.washingtonpost.com/technology/2025/02/07/apple-encryption-backdoor-uk/">became public in February</a> will be required to manually switch it off or lose their iCloud account.</p>
<p>I am not going to explain the chapter and verse of the legal saga today, because I prefer to do that for people who pay me to explain them the chapter and verse.</p>
<p>But I will say that the shutdown of ADP is Apple being <em>on the right side</em> of the <a href="https://doi.org/10.1080/09615768.2025.2551419">geopolitical fight</a>, as inconvenient as that may be to you and me.</p>
<p><span>W</span>hen the whole debacle blew up in February, Apple announced that ADP would no longer be available for new users, but would remain unaffected for those of us who already had it activated. That assurance was nothing to sleep on, and so we have been waiting for the inevitable. Apple’s <a href="https://support.apple.com/en-us/122234">September update</a> confirmed that its days are numbered:<br>
</p><p>For users in the UK who already enabled Advanced Data Protection, Apple will soon provide additional guidance. Apple cannot disable ADP automatically for these users. Instead, UK users will be given a period of time to disable the feature themselves to keep using their iCloud account.</p>
<p>So what does that mean for you? Again, from their September update:<br>
</p><p>Withdrawing Advanced Data Protection from the UK will not affect the 15 iCloud data categories that are end-to-end encrypted by default. Data like iCloud Keychain and Health remains protected with full end-to-end encryption.<br>
Our communication services, like iMessage and FaceTime, remain end-to-end encrypted globally, including in the UK.<br>
Users in the UK who have not already enabled Advanced Data Protection will no longer have the option to do so. That means the 10 iCloud data categories covered by ADP will be protected by Standard Data Protection, and UK users will not have a choice to benefit from end-to-end encryption for these categories: iCloud Backup; iCloud Drive; Photos; Notes; Reminders; Safari Bookmarks; Siri Shortcuts; Voice Memos; Wallet Passes; and Freeform.</p>
<p>This means that <ins>if you already had ADP activated, and e2ee is critical to your personal or operational security,</ins> you need to get everything in that list – &nbsp;iCloud Backup, iCloud Drive, Photos, Notes, Reminders, Safari Bookmarks, Siri Shortcuts, Voice Memos, Wallet Passes, and Freeform – off of iCloud sooner rather than later.</p>
<p>Once you’ve done that, go into your iCloud settings, click on Manage, then click on each thing individually to purge it off iCloud.</p>
<p><img decoding="async" src="https://heatherburns.tech/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-09.20.42.png" alt="" width="952" height="256" srcset="https://heatherburns.tech/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-09.20.42.png 952w, https://heatherburns.tech/wp-content/uploads/2025/10/Screenshot-2025-10-22-at-09.20.42-300x81.png 300w" sizes="(max-width: 952px) 100vw, 952px"></p>
<p>I’m not going to tell you where to move your stuff other than to say that if you’re moving it from one big tech company to another, you’re just being daft. Likewise, if you’re moving your stuff to a non-e2ee service, don’t bother. If you need an e2ee service try Proton. They have a Black Friday sale on.</p>
<p>If you have a lot of <strong>Notes</strong>, first download the <a href="https://apps.apple.com/gb/app/exporter/id1099120373?mt=12">Exporter app</a> from the app store. It does what it says on the tin. You’ll end up with a folder full of markdown files which you can upload elsewhere. E2EE being the dealbreaker, I chose Standard Notes. I know a lot of folk who prefer Obsidian or Joplin. Whatever you choose, do not use a non-E2EE note service.*</p>
<p>You know as well as I do that you need to be <a href="https://european-alternatives.eu/">moving everything you can out of the American stack anyway</a> so just stick this task on your to-do list, which should not be Reminders, and get it done.</p>
<h2>What about the non-e2ee stuff in iCloud?</h2>
<p>The full list of what lives in iCloud and how it is or is not encrypted is <a href="https://support.apple.com/en-gb/102651">here</a>.</p>
<p>We know from the tiny bits of the TCN saga which have been publicly disclosed, thanks to the only two media outlets that are bothering to cover it, that the first TCN was not just for the end-to-end encrypted data protected by ADP. It was for anything on iCloud, full stop, worldwide:</p>
<blockquote><p><em>…however, the new IPT filing states the TCN “is not limited to” data stored under ADP, suggesting the UK government sought bulk interception access to Apple’s standard iCloud service, which is much more widely used by the company’s customers. The TCN also included “obligations to provide and maintain a capability to disclose categories of data stored within a cloud-based backup service”, the filing states, which suggests the government sought to tap messages or passwords that were backed up in the cloud as well. “The obligations included in the TCN are not limited to the UK or users of the service in the UK; they apply globally in respect of the relevant data categories of all iCloud users,” the IPT filing adds.</em> <cite><a href="https://www.ft.com/content/fe2c9ae1-d175-4eb9-909e-0b171f6d097c">Tim Bradshaw and Anna Gross at the Financial Times (£)</a></cite></p></blockquote>
<p>This means that you have some serious thinking to do about what you intend to trust to the Apple stack altogether going forward, even things like passwords.</p>
<p>I can’t tell you what to do but once again, you have options. Educate yourself. Consider the opsec and persec needs not just of yourself, but for the people around you who could be adversely affected by insecure data going walkies out of your account.</p>
<h2>What if I’m not in the UK?</h2>
<p>This impacts the UK only: as their September update noted,<em> Advanced Data Protection continues to be available everywhere else in the world. </em></p>
<p><a href="https://heatherburns.tech/2025/09/12/the-dog-that-caught-the-car/">We’re just so world-leading</a>.</p>
<p>It does mean that if you have someone in the UK on your team, you need to factor them in as part of your threat model. We are all liabilities to our own opsec now.</p>
<p>If you’re not in the UK, and you don’t have ADP activated, take 10 seconds to do it right now, you lucky sod.<br>
Settings &gt; Your name Apple Account &gt; iCloud &gt; Advanced Data Protection</p>
<h2>What about that second TCN?</h2>
<p>On the 1st of October, the Home Office issued a second TCN against Apple for the same as before, but <a href="https://www.ft.com/content/d101fd62-14f9-4f51-beff-ea41e8794265">only for British citizens’ data</a>. World-leading!</p>
<p>Those who follow my work know that this phrase made me spew a double barrel of Glaswegian swearing.&nbsp; British <em>citizens’</em> data, as opposed to British <em>users’</em> data? The dividing line here is not e.g. being located in the UK or having registered an account here, but <em>what it says on your passport</em>? How is Apple going to know that, much less roll it out? (<a href="https://heatherburns.tech/2025/09/12/the-dog-that-caught-the-car/">/s</a>)</p>
<p><em>Did Apple just publicly state that they’re going to be removing a security layer <strong>and</strong> adding a nationality check layer?&nbsp;</em></p>
<p>We don’t know.</p>
<p>We don’t know because as with the first TCN, that information only became available in the public domain due to someone leaking it to the media. That’s all there is to know. Everything else is confidential and NCND. There is nothing else to say because nothing else is known. If someone who did know something was sitting across from me right now, and they told me, they would be committing a crime.</p>
<p>Those of us who care about these things enough to show up in difficult places are keeping tabs on both TCNs, and the wider legal and technical implications of both, as best we possibly can. Don’t expect to hear anything more until January, when the Liberty/PI challenge on the first TCN goes to the Investigatory Powers Tribunal. In the interim, if you want me to bore you about ECHR case law and how the UK’s review into Article 8 seems a little too coincidentally timed, pick a pub.</p>
<p>Otherwise, please make sure you de-Apple, de-Google, and de-American Stack yourself when you have time, clarity, and focus to do it. Start today.</p>
<p>In the meantime please follow and support the only media coverage being produced about the second TCN, which comes from <a href="https://www.computerweekly.com/news/366630023/Home-Office-back-door-seeks-world-wide-access-to-Apple-iCloud-users-data-court-documents-confirm">Bill Goodwin at Computer Weekly</a> and <a href="https://www.ft.com/content/fe2c9ae1-d175-4eb9-909e-0b171f6d097c">Tim Bradshaw and Anna Gross at the Financial Times (£)</a>.</p>
<p>Above all, please remember that this <em>is</em> the sunlit uplands. That’s the thing about Brexit Britain having decided to go it alone where tech regulation is concerned. It did not become the vanguard of a <a href="https://heatherburns.tech/2025/09/12/the-dog-that-caught-the-car/">“world-leading” third way.</a></p>
<p>It became a small and inconsequential thing easily thrown under a bus.</p>
<p><small><em>Header image by me: Alan Turing memorial, Manchester, where he reminds you why keeping data private can be a matter of life and death.</em></small></p>
</div><p>*For the love of the wee man do not use a <a href="https://standardnotes.com/compare/simplenote-alternative">non-e2ee notetaking app</a> which has been abandoned by an owner who has a track record of personally snooping through user data when he’s in a mood, i.e. if he’s breathing.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe Code Warning – A personal casestudy (319 pts)]]></title>
            <link>https://github.com/jackdoe/pico2-swd-riscv</link>
            <guid>45874987</guid>
            <pubDate>Mon, 10 Nov 2025 11:45:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jackdoe/pico2-swd-riscv">https://github.com/jackdoe/pico2-swd-riscv</a>, See on <a href="https://news.ycombinator.com/item?id=45874987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">pico2-swd-riscv</h2><a id="user-content-pico2-swd-riscv" aria-label="Permalink: pico2-swd-riscv" href="#pico2-swd-riscv"></a></p>
<p dir="auto">A stateful SWD protocol implementation for debugging RP2350 RISC-V cores (Hazard3) from any Raspberry Pi Pico2 (target) using GPIO's on another Pico (probe).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">0. VIBE CODE WARNING (WRITTEN BY HUMAN)</h2><a id="user-content-0-vibe-code-warning-written-by-human" aria-label="Permalink: 0. VIBE CODE WARNING (WRITTEN BY HUMAN)" href="#0-vibe-code-warning-written-by-human"></a></p>
<p dir="auto">About 80% of the code is vibe coded; The readme is almost completely generated (except the whole vibe-code-warning section). I spent many nights with the oscilloscope and the docs and made a working prototype that was able ti do sba/read/write regs and do abstract commands and progbuf, the rest was done with claude code. The tests are quite <a href="https://github.com/jackdoe/pico2-swd-riscv/blob/master/examples/test_suite">comprehensive test suite</a> and I use the core of the library in my own projects, but, as they say, "hic sunt dracones". I also read the readme and the code didn't notice anything wrong (and removed the wrong/unclear parts).</p>
<p dir="auto">This project was my casestudy of vibecoding a more complicated project that I dont understand 100% and there is no obvious existing code that can be "used". It started as ~1000 loc that I have written and knew very well, reading the rp2350, arm swd and riscv debug docs, capturing data with oscilloscope and openocd then decoding it and analyzing the wakeup sequence and then read/write commands. After I got it working I gave it to claude to make it into a library that I can use in other projects, and then I slowly built it up.</p>
<p dir="auto">After about 3-4k lines of code I completely lost track of what is going on, and I woudn't consider this code that I have written, but adding more and more tests felt "nice", or at least reassuring.</p>
<p dir="auto">There was a some gaslighting, particularly when it misunderstood dap_read_mem32 thinking it is reading from ram and not MEM-AP TAR/DRW/RDBUFF protocol, which lead to incredible amount of nonsense.</p>
<p dir="auto">Overall I would say it was a horrible experience, even though it took 10 hours to write close to 10000 lines of code, I don't consider this my project, and I have no sense of acomplishment or growth.</p>
<p dir="auto">In contrast, using AI to read all the docs (which are thousands of pages) and write helpful scripts to decode the oscilloscope data, create packed C structs from docs and etc, was very nice, and I did feel good after. The moment I read the first register and then when I was able to read memory via SBA I felt amazing.</p>
<p dir="auto">The main issue is <code>taste</code>, when I write code I feel if its good or bad, as I am writing it, I know if its wrong, but using claude code I get desensitized very quickly and I just can't tell, it "reads" OK, but I don't know how it feels. In this case it happened when the code grew about 4x, from 1k to 4k lines. And worse of all, my mental model of the code is completely gone, and with it my ownership.</p>
<p dir="auto"><strong>The tokens have no reason or purpose</strong>, which makes reading code ridiculously difficult, as and each token can be complete nonsense. When reading human code the symbols have a purpose, someone thought "I will put this in a variable, later I will check its status.", so I pretend I am them, and think <code>why</code> would they have written this? Shortly after I understand, as they are human and I am human. But the AI symbols have no reason, and worse of all, they all look deceptively correct, so I have to think 10 times harder if it is wrong. With any human code (including your own) it is quite easy to gauge how much you can trust it, and it is quite consistent, with the AI code, one function can be much better than what you woudld've written, and the code 2 lines below can be cargo culted gunk that looks incredibly good, but is structurally wrong.</p>
<p dir="auto">In the end I would say I have gained good understanding of the wires, timings, and the lower level ap/dp mechanics, sba and progbuf, but I regret not writing the whole thing myself, even if it would've taken 10x the time.</p>
<p dir="auto">I fucking hate this.</p>
<p dir="auto">And I can not help, but feel dusgust and shame. Is this what programming is now? I really hope this is some intermediate stage and it changes for the better, the problem is I dont know what "better" is, it seems for some people is not writing the code, for others is not modeling the problem and for third is not having to think. For me, I am not sure, I do want to make things, and many times I dont want to know something, but I want to use it, e.g. the rp2350 usb host controller the way you have to re-arm interrupts and the way the epx register is shared is super annoying, for good reasons probably, but I just want to use it to make my CBI driver.</p>
<p dir="auto">I guess the question is what is the thing I want to make, because you can go way up the stack, from the USB chip registers to CBI to UFI to FAT16 to the OS of the old school computer I am making, but why stop? make the schematics, the pcbs, the cad files, maybe automatically send it to the factory? and then just ship it to me? but why stop? make my webshop, start selling, make a community, ads, marketing, generate some unboxing videos, maybe some viral memes? process the orders directly to the factory, on demand, if there is an issue, it is ready with customer support.</p>
<p dir="auto">What do I do in the meanwhile? Sit on the beach? I hate the beach.</p>
<p dir="auto">Where does it stop?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">1. ARCHITECTURAL OVERVIEW</h2><a id="user-content-1-architectural-overview" aria-label="Permalink: 1. ARCHITECTURAL OVERVIEW" href="#1-architectural-overview"></a></p>
<p dir="auto">This library implements a complete three-layer abstraction for Serial Wire Debug protocol communication with RP2350's RISC-V Debug Module, modeled after the Debug Access Port specification and informed by ARM Debug Interface Architecture Specification v5.2.</p>
<div data-snippet-clipboard-copy-content="┌────────────────────────────────────────┐
│  Application Layer                     │
│  (User Code)                           │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Debug Module Layer (rp2350.c)         │
│  - RISC-V Debug Specification v0.13    │
│  - Hart control via DMCONTROL          │
│  - Abstract commands for GPR access    │
│  - System Bus Access (non-intrusive)   │
│  - PROGBUF execution for CSR access    │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Debug Access Port Layer (dap.c)       │
│  - DP/AP register transactions         │
│  - RP2350-specific DP_SELECT encoding  │
│  - Bank selection caching              │
│  - Memory-mapped debug register access │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Serial Wire Debug Layer (swd*.c)      │
│  - 2-wire bidirectional protocol       │
│  - PIO state machine bit-banging       │
│  - Request/ACK/Data phase handling     │
│  - Parity computation and verification │
│  - Line reset and dormant sequences    │
└────────────────────────────────────────┘"><pre><code>┌────────────────────────────────────────┐
│  Application Layer                     │
│  (User Code)                           │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Debug Module Layer (rp2350.c)         │
│  - RISC-V Debug Specification v0.13    │
│  - Hart control via DMCONTROL          │
│  - Abstract commands for GPR access    │
│  - System Bus Access (non-intrusive)   │
│  - PROGBUF execution for CSR access    │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Debug Access Port Layer (dap.c)       │
│  - DP/AP register transactions         │
│  - RP2350-specific DP_SELECT encoding  │
│  - Bank selection caching              │
│  - Memory-mapped debug register access │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Serial Wire Debug Layer (swd*.c)      │
│  - 2-wire bidirectional protocol       │
│  - PIO state machine bit-banging       │
│  - Request/ACK/Data phase handling     │
│  - Parity computation and verification │
│  - Line reset and dormant sequences    │
└────────────────────────────────────────┘
</code></pre></div>
<p dir="auto">The separation of concerns follows classical protocol stack design: each layer exposes a well-defined interface and maintains independent state, with lower layers unaware of higher-layer semantics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2. RISC-V DEBUG ARCHITECTURE: A FORMAL MODEL</h2><a id="user-content-2-risc-v-debug-architecture-a-formal-model" aria-label="Permalink: 2. RISC-V DEBUG ARCHITECTURE: A FORMAL MODEL" href="#2-risc-v-debug-architecture-a-formal-model"></a></p>
<p dir="auto">Before examining the protocol implementation, we must establish the theoretical foundations of RISC-V external debugging. This section develops the debug architecture from first principles, following the RISC-V External Debug Support Specification v0.13.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.A The Hart State Machine</h3><a id="user-content-2a-the-hart-state-machine" aria-label="Permalink: 2.A The Hart State Machine" href="#2a-the-hart-state-machine"></a></p>
<p dir="auto">A RISC-V hart (hardware thread) exists in one of three abstract states:</p>
<div data-snippet-clipboard-copy-content="                    ┌─────────────┐
                    │   RUNNING   │
                    │  (Normal)   │
                    └──────┬──────┘
                           │
                  halt_request, ebreak,
                  trigger_fire, step_complete
                           │
                           ▼
                    ┌─────────────┐
                    │   HALTED    │
                    │  (Debug)    │
                    └──────┬──────┘
                           │
                     resume_request
                           │
                           ▼
                    ┌─────────────┐
                    │  RESUMING   │
                    │ (Transient) │
                    └──────┬──────┘
                           │
                           ▼
                    ┌─────────────┐
                    │   RUNNING   │
                    └─────────────┘"><pre><code>                    ┌─────────────┐
                    │   RUNNING   │
                    │  (Normal)   │
                    └──────┬──────┘
                           │
                  halt_request, ebreak,
                  trigger_fire, step_complete
                           │
                           ▼
                    ┌─────────────┐
                    │   HALTED    │
                    │  (Debug)    │
                    └──────┬──────┘
                           │
                     resume_request
                           │
                           ▼
                    ┌─────────────┐
                    │  RESUMING   │
                    │ (Transient) │
                    └──────┬──────┘
                           │
                           ▼
                    ┌─────────────┐
                    │   RUNNING   │
                    └─────────────┘
</code></pre></div>
<p dir="auto"><strong>State 1: RUNNING</strong> - The hart executes instructions from main memory. PC advances according to program flow. All architectural state (GPRs, CSRs, memory) is accessible to the executing program.</p>
<p dir="auto"><strong>State 2: HALTED</strong> - The hart has entered debug mode. No instructions from main memory execute. The hart is "parked" in a special debug ROM or implicit debug loop within the Debug Module. Debug-specific CSRs (DPC, DCSR, DSCRATCH) become accessible.</p>
<p dir="auto"><strong>State 3: RESUMING</strong> - A transient state where the hart has received a resume request but has not yet returned to normal execution. This state exists to model the asynchronous nature of resume operations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.B The Debug Module: An Independent Controller</h3><a id="user-content-2b-the-debug-module-an-independent-controller" aria-label="Permalink: 2.B The Debug Module: An Independent Controller" href="#2b-the-debug-module-an-independent-controller"></a></p>
<p dir="auto">The Debug Module (DM) is a hardware block separate from the hart itself. It acts as a "shadow controller" that can:</p>
<ol dir="auto">
<li><strong>Observe hart state</strong> without halting (DMSTATUS register)</li>
<li><strong>Command hart transitions</strong> (halt, resume, reset via DMCONTROL)</li>
<li><strong>Access hart registers</strong> when halted (abstract commands)</li>
<li><strong>Access system memory</strong> independently of hart state (System Bus Access)</li>
</ol>
<p dir="auto">The DM is itself controlled by an external debugger via a Debug Transport Module (DTM). In our case, the DTM is the SWD interface.</p>
<div data-snippet-clipboard-copy-content="┌──────────────────────────────────────────────────┐
│  External Debugger (Host CPU)                    │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼ SWD Protocol
┌──────────────────────────────────────────────────┐
│  Debug Transport Module (DTM)                    │
│  - Exposes DM registers as memory-mapped space   │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼ Internal Bus
┌──────────────────────────────────────────────────┐
│  Debug Module (DM)                               │
│  ┌──────────────┐  ┌──────────────┐              │
│  │  Abstract    │  │  System Bus  │              │
│  │  Command     │  │  Master      │              │
│  │  Engine      │  │              │              │
│  └──────┬───────┘  └──────┬───────┘              │
│         │                 │                      │
└─────────┼─────────────────┼──────────────────────┘
          │                 │
          ▼                 ▼
    ┌─────────────┐   ┌──────────────┐
    │  Hart 0     │   │ System Bus   │
    │  (Hazard3)  │   │              │
    ├─────────────┤   └──────────────┘
    │  Hart 1     │
    │  (Hazard3)  │
    └─────────────┘    "><pre><code>┌──────────────────────────────────────────────────┐
│  External Debugger (Host CPU)                    │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼ SWD Protocol
┌──────────────────────────────────────────────────┐
│  Debug Transport Module (DTM)                    │
│  - Exposes DM registers as memory-mapped space   │
└────────────────────┬─────────────────────────────┘
                     │
                     ▼ Internal Bus
┌──────────────────────────────────────────────────┐
│  Debug Module (DM)                               │
│  ┌──────────────┐  ┌──────────────┐              │
│  │  Abstract    │  │  System Bus  │              │
│  │  Command     │  │  Master      │              │
│  │  Engine      │  │              │              │
│  └──────┬───────┘  └──────┬───────┘              │
│         │                 │                      │
└─────────┼─────────────────┼──────────────────────┘
          │                 │
          ▼                 ▼
    ┌─────────────┐   ┌──────────────┐
    │  Hart 0     │   │ System Bus   │
    │  (Hazard3)  │   │              │
    ├─────────────┤   └──────────────┘
    │  Hart 1     │
    │  (Hazard3)  │
    └─────────────┘    
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.C Debug Mode: A Privileged Exception Context</h3><a id="user-content-2c-debug-mode-a-privileged-exception-context" aria-label="Permalink: 2.C Debug Mode: A Privileged Exception Context" href="#2c-debug-mode-a-privileged-exception-context"></a></p>
<p dir="auto">When a hart enters debug mode, it is not simply "stopped." Rather, it enters a special execution context analogous to an exception handler:</p>
<ol dir="auto">
<li><strong>PC is saved</strong> to DPC (Debug Program Counter, CSR 0x7b1)</li>
<li><strong>Privilege level</strong> is elevated to M-mode (Machine mode, highest privilege)</li>
<li><strong>DCSR.cause</strong> records the entry reason (halt request, ebreak, trigger, etc.)</li>
<li><strong>Hart begins executing</strong> from the debug exception vector (typically in debug ROM)</li>
</ol>
<p dir="auto">The debug exception vector contains a tight polling loop that repeatedly checks for commands from the Debug Module. This loop is architecturally invisible to the debugger—we simply observe the hart as "halted."</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.D Abstract Commands: The Debug Module API</h3><a id="user-content-2d-abstract-commands-the-debug-module-api" aria-label="Permalink: 2.D Abstract Commands: The Debug Module API" href="#2d-abstract-commands-the-debug-module-api"></a></p>
<p dir="auto">The Abstract Command mechanism provides a hardware-implemented function call interface. Each abstract command is a 32-bit word written to the COMMAND register that encodes:</p>
<div data-snippet-clipboard-copy-content="31                            24 23                            0
┌────────────────────────────────┬────────────────────────────────┐
│         cmdtype                │         command-specific       │
└────────────────────────────────┴────────────────────────────────┘"><pre><code>31                            24 23                            0
┌────────────────────────────────┬────────────────────────────────┐
│         cmdtype                │         command-specific       │
└────────────────────────────────┴────────────────────────────────┘
</code></pre></div>
<p dir="auto"><strong>cmdtype=0</strong>: Access Register</p>
<div data-snippet-clipboard-copy-content="31      24 23          20 19 18 17 16 15                          0
┌──────────┬─────────────┬─┬──┬──┬──┬──────────────────────────────┐
│    0     │   aarsize   │0│pc│tr│wr│         regno                │
└──────────┴─────────────┴─┴──┴──┴──┴──────────────────────────────┘

aarsize: Access size (2 = 32-bit)
aarpostincrement: Ignored
postexec: Execute program buffer after transfer
transfer: Perform the transfer (1=yes)
write: Direction (1=write, 0=read)
regno: Register number (0x1000-0x101f for GPRs x0-x31)"><pre><code>31      24 23          20 19 18 17 16 15                          0
┌──────────┬─────────────┬─┬──┬──┬──┬──────────────────────────────┐
│    0     │   aarsize   │0│pc│tr│wr│         regno                │
└──────────┴─────────────┴─┴──┴──┴──┴──────────────────────────────┘

aarsize: Access size (2 = 32-bit)
aarpostincrement: Ignored
postexec: Execute program buffer after transfer
transfer: Perform the transfer (1=yes)
write: Direction (1=write, 0=read)
regno: Register number (0x1000-0x101f for GPRs x0-x31)
</code></pre></div>
<p dir="auto">The Debug Module hardware interprets this command and performs the register access autonomously. From the debugger's perspective, this is a synchronous operation: write COMMAND, poll ABSTRACTCS.busy until clear, read result from DATA0.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.E The Program Buffer: A Programmable Exception Handler</h3><a id="user-content-2e-the-program-buffer-a-programmable-exception-handler" aria-label="Permalink: 2.E The Program Buffer: A Programmable Exception Handler" href="#2e-the-program-buffer-a-programmable-exception-handler"></a></p>
<p dir="auto">The Program Buffer (PROGBUF) is a small instruction memory (2-16 entries) within the Debug Module. When abstract commands cannot accomplish a task (e.g., accessing debug-only CSRs), the debugger can:</p>
<ol dir="auto">
<li>Write RISC-V instructions to PROGBUF</li>
<li>Issue an abstract command with the <code>postexec</code> bit set</li>
<li>The hart executes PROGBUF instructions while still in debug mode</li>
<li>The final <code>ebreak</code> instruction returns control to the Debug Module</li>
</ol>
<p dir="auto">This is not a "code injection" attack—the hart never leaves debug mode. It's analogous to a debugger writing instructions into a trap handler's stack frame.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.F System Bus Access: A Parallel Execution Path</h3><a id="user-content-2f-system-bus-access-a-parallel-execution-path" aria-label="Permalink: 2.F System Bus Access: A Parallel Execution Path" href="#2f-system-bus-access-a-parallel-execution-path"></a></p>
<p dir="auto">SBA provides a second path to memory that bypasses the hart entirely:</p>
<div data-snippet-clipboard-copy-content="         Debugger Commands
                │
                ▼
         ┌─────────────┐
         │     DM      │
         └──┬───────┬──┘
            │       │
   ┌────────┘       └───────┐
   │                        │
   ▼ Abstract Cmd           ▼ SBA
┌──────┐                ┌─────────┐
│ Hart │───────────────▶│ Memory  │
└──────┘  Hart Accesses └─────────┘"><pre><code>         Debugger Commands
                │
                ▼
         ┌─────────────┐
         │     DM      │
         └──┬───────┬──┘
            │       │
   ┌────────┘       └───────┐
   │                        │
   ▼ Abstract Cmd           ▼ SBA
┌──────┐                ┌─────────┐
│ Hart │───────────────▶│ Memory  │
└──────┘  Hart Accesses └─────────┘
</code></pre></div>
<p dir="auto">The hart and SBA compete for memory bus bandwidth. The hart's view of memory may differ from SBA's view due to:</p>
<ol dir="auto">
<li><strong>Cache</strong>: Hart caches writes; SBA sees stale memory</li>
<li><strong>MMU/PMP</strong>: Hart accesses are translated/protected; SBA bypasses</li>
<li><strong>Atomicity</strong>: Hart's atomic operations (LR/SC) are invisible to SBA</li>
</ol>
<p dir="auto">This is not a bug—it's a fundamental architectural trade-off. SBA provides speed and non-intrusiveness at the cost of coherency guarantees.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.G The Debugging Contract</h3><a id="user-content-2g-the-debugging-contract" aria-label="Permalink: 2.G The Debugging Contract" href="#2g-the-debugging-contract"></a></p>
<p dir="auto">RISC-V debugging rests on several invariants:</p>
<p dir="auto"><strong>Invariant 1: Debug Mode is Atomic</strong>
While in debug mode, the hart executes no instructions from main memory. The debugger has exclusive control.</p>
<p dir="auto"><strong>Invariant 2: Architectural Transparency</strong>
Entering and exiting debug mode does not change architected state (except DPC/DCSR). The program cannot detect it was halted (modulo real-time constraints).</p>
<p dir="auto"><strong>Invariant 3: Debug Privilege</strong>
Debug mode executes at maximum privilege (M-mode). All memory is accessible, all CSRs are readable.</p>
<p dir="auto"><strong>Invariant 4: No Interrupts in Debug</strong>
Interrupts are masked while in debug mode. The debugger must explicitly re-enable them.</p>
<p dir="auto">These invariants enable reproducible debugging: halting twice at the same PC should show identical state.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">3. THE SERIAL WIRE DEBUG PROTOCOL</h2><a id="user-content-3-the-serial-wire-debug-protocol" aria-label="Permalink: 3. THE SERIAL WIRE DEBUG PROTOCOL" href="#3-the-serial-wire-debug-protocol"></a></p>
<p dir="auto">Serial Wire Debug (SWD) is a 2-wire replacement for JTAG's 5-wire interface, developed by ARM. The protocol operates over two signals:</p>
<ul dir="auto">
<li><strong>SWCLK</strong>: Clock signal driven by the debugger (host)</li>
<li><strong>SWDIO</strong>: Bidirectional data signal with turnaround phases</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">3.A Protocol Packet Structure</h3><a id="user-content-3a-protocol-packet-structure" aria-label="Permalink: 3.A Protocol Packet Structure" href="#3a-protocol-packet-structure"></a></p>
<p dir="auto">Each SWD transaction consists of three phases:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Request Phase</strong> (8 bits, host drives SWDIO):</p>
<div data-snippet-clipboard-copy-content="Bit 0:     Start (always 1)
Bit 1:     APnDP (0=DP access, 1=AP access)
Bit 2:     RnW (0=Write, 1=Read)
Bit 3-4:   A[3:2] (register address bits)
Bit 5:     Parity (even parity of bits 1-4)
Bit 6:     Stop (always 0)
Bit 7:     Park (always 1)"><pre><code>Bit 0:     Start (always 1)
Bit 1:     APnDP (0=DP access, 1=AP access)
Bit 2:     RnW (0=Write, 1=Read)
Bit 3-4:   A[3:2] (register address bits)
Bit 5:     Parity (even parity of bits 1-4)
Bit 6:     Stop (always 0)
Bit 7:     Park (always 1)
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Acknowledge Phase</strong> (3 bits, target drives SWDIO):</p>
<div data-snippet-clipboard-copy-content="OK    (001): Transaction accepted
WAIT  (010): Target requests retry
FAULT (100): Error condition"><pre><code>OK    (001): Transaction accepted
WAIT  (010): Target requests retry
FAULT (100): Error condition
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Data Phase</strong> (33 bits, direction depends on RnW):</p>
<div data-snippet-clipboard-copy-content="Bits 0-31: Data word
Bit 32:    Parity bit"><pre><code>Bits 0-31: Data word
Bit 32:    Parity bit
</code></pre></div>
</li>
</ol>
<p dir="auto">Turnaround cycles (host releases SWDIO, target can drive) occur between request→ack and during data phase direction changes.</p>
<p dir="auto">Our implementation of the packet construction is in <code>swd_protocol.c:97-113</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static uint8_t make_swd_request(bool APnDP, bool RnW, uint8_t addr) {
    uint8_t a2 = (addr >> 2) &amp; 1;
    uint8_t a3 = (addr >> 3) &amp; 1;
    uint8_t parity = (APnDP + RnW + a2 + a3) &amp; 1;

    uint8_t request = 0;
    request |= (1 << 0);          // Start bit
    request |= (APnDP << 1);      // AP/DP select
    request |= (RnW << 2);        // Read/Write
    request |= (a2 << 3);         // Address bit 2
    request |= (a3 << 4);         // Address bit 3
    request |= (parity << 5);     // Parity
    request |= (0 << 6);          // Stop bit
    request |= (1 << 7);          // Park bit
    return request;
}"><pre><span>static</span> <span>uint8_t</span> <span>make_swd_request</span>(<span>bool</span> <span>APnDP</span>, <span>bool</span> <span>RnW</span>, <span>uint8_t</span> <span>addr</span>) {
    <span>uint8_t</span> <span>a2</span> <span>=</span> (<span>addr</span> &gt;&gt; <span>2</span>) <span>&amp;</span> <span>1</span>;
    <span>uint8_t</span> <span>a3</span> <span>=</span> (<span>addr</span> &gt;&gt; <span>3</span>) <span>&amp;</span> <span>1</span>;
    <span>uint8_t</span> <span>parity</span> <span>=</span> (<span>APnDP</span> <span>+</span> <span>RnW</span> <span>+</span> <span>a2</span> <span>+</span> <span>a3</span>) <span>&amp;</span> <span>1</span>;

    <span>uint8_t</span> <span>request</span> <span>=</span> <span>0</span>;
    <span>request</span> |= (<span>1</span> &lt;&lt; <span>0</span>);          <span>// Start bit</span>
    <span>request</span> |= (<span>APnDP</span> &lt;&lt; <span>1</span>);      <span>// AP/DP select</span>
    <span>request</span> |= (<span>RnW</span> &lt;&lt; <span>2</span>);        <span>// Read/Write</span>
    <span>request</span> |= (<span>a2</span> &lt;&lt; <span>3</span>);         <span>// Address bit 2</span>
    <span>request</span> |= (<span>a3</span> &lt;&lt; <span>4</span>);         <span>// Address bit 3</span>
    <span>request</span> |= (<span>parity</span> &lt;&lt; <span>5</span>);     <span>// Parity</span>
    <span>request</span> |= (<span>0</span> &lt;&lt; <span>6</span>);          <span>// Stop bit</span>
    <span>request</span> |= (<span>1</span> &lt;&lt; <span>7</span>);          <span>// Park bit</span>
    <span>return</span> <span>request</span>;
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">3.B PIO-Based Physical Layer</h3><a id="user-content-3b-pio-based-physical-layer" aria-label="Permalink: 3.B PIO-Based Physical Layer" href="#3b-pio-based-physical-layer"></a></p>
<p dir="auto">Unlike software bit-banging (which suffers from timing jitter and CPU overhead), this implementation uses RP2040/RP2350's Programmable I/O (PIO) blocks for deterministic timing.</p>
<p dir="auto">The PIO program (<code>swd.pio</code>) implements a command-based interface where each FIFO entry encodes either a command or data payload. Command format:</p>
<div data-snippet-clipboard-copy-content="Bits 0-7:   Bit count - 1
Bit 8:      Direction (0=input, 1=output)
Bits 9-13:  Target instruction address"><pre><code>Bits 0-7:   Bit count - 1
Bit 8:      Direction (0=input, 1=output)
Bits 9-13:  Target instruction address
</code></pre></div>
<p dir="auto">The state machine operates at 4 cycles per clock period, providing precise SWCLK generation independent of system clock frequency. See <code>swd.pio:45-68</code> for the complete implementation.</p>
<p dir="auto">Clock divider calculation (<code>swd_protocol.c:313-330</code>) accounts for this 4-cycle period:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t clk_sys_khz = clock_get_hz(clk_sys) / 1000;
uint32_t divider = (((clk_sys_khz + freq_khz - 1) / freq_khz) + 3) / 4;"><pre><span>uint32_t</span> <span>clk_sys_khz</span> <span>=</span> <span>clock_get_hz</span>(<span>clk_sys</span>) / <span>1000</span>;
<span>uint32_t</span> <span>divider</span> <span>=</span> (((<span>clk_sys_khz</span> <span>+</span> <span>freq_khz</span> <span>-</span> <span>1</span>) / <span>freq_khz</span>) <span>+</span> <span>3</span>) / <span>4</span>;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">3.C The Dormant State and Protocol Selection</h3><a id="user-content-3c-the-dormant-state-and-protocol-selection" aria-label="Permalink: 3.C The Dormant State and Protocol Selection" href="#3c-the-dormant-state-and-protocol-selection"></a></p>
<p dir="auto">ARM Debug Interface Architecture v6 introduces a <strong>Dormant State</strong> to enable coexistence of multiple debug protocols (JTAG and SWD) on the same pins. At power-up, RP2350's SW-DP enters the Dormant state, requiring explicit activation before SWD operations can proceed.</p>
<p dir="auto">The dormant state solves a fundamental problem: JTAG uses 5 signals (TMS, TCK, TDI, TDO, TRST), while SWD uses 2 (SWCLK, SWDIO). When both protocols share physical pins, the debug port must determine which protocol the debugger intends to use. The solution is to require a protocol-specific "unlock" sequence that:</p>
<ol dir="auto">
<li>Cannot be generated accidentally by non-debug traffic on the pins</li>
<li>Is sufficiently long to avoid false positives (128 bits)</li>
<li>Uniquely identifies the target protocol (JTAG vs SWD)</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">3.C.1 The State Transition Model</h4><a id="user-content-3c1-the-state-transition-model" aria-label="Permalink: 3.C.1 The State Transition Model" href="#3c1-the-state-transition-model"></a></p>
<p dir="auto">The SW-DP implements a finite state machine with three protocol modes:</p>
<div data-snippet-clipboard-copy-content="Power-On → [Default State] → Dormant
                                 │
                    ┌────────────┼────────────┐
                    │                         │
         JTAG Activation              SWD Activation
         Sequence (0x33bbbbba)        Sequence (0x1a)
                    │                         │
                    ▼                         ▼
              ┌──────────┐              ┌──────────┐
              │   JTAG   │              │   SWD    │
              │  Active  │              │  Active  │
              └────┬─────┘              └────┬─────┘
                   │                         │
         JTAG-to-Dormant              SWD-to-Dormant
         Sequence                     Sequence
                   │                         │
                   └──────────┬──────────────┘
                              ▼
                         ┌──────────┐
                         │ Dormant  │
                         └──────────┘"><pre><code>Power-On → [Default State] → Dormant
                                 │
                    ┌────────────┼────────────┐
                    │                         │
         JTAG Activation              SWD Activation
         Sequence (0x33bbbbba)        Sequence (0x1a)
                    │                         │
                    ▼                         ▼
              ┌──────────┐              ┌──────────┐
              │   JTAG   │              │   SWD    │
              │  Active  │              │  Active  │
              └────┬─────┘              └────┬─────┘
                   │                         │
         JTAG-to-Dormant              SWD-to-Dormant
         Sequence                     Sequence
                   │                         │
                   └──────────┬──────────────┘
                              ▼
                         ┌──────────┐
                         │ Dormant  │
                         └──────────┘
</code></pre></div>
<p dir="auto">Once activated, the debug port remains in the selected protocol mode until:</p>
<ul dir="auto">
<li>A transition-to-dormant sequence is sent</li>
<li>Power is cycled</li>
<li>The external reset (RUN) pin is asserted</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">3.C.2 The Selection Alert Sequence</h4><a id="user-content-3c2-the-selection-alert-sequence" aria-label="Permalink: 3.C.2 The Selection Alert Sequence" href="#3c2-the-selection-alert-sequence"></a></p>
<p dir="auto">Before sending a protocol-specific activation code, ARM requires transmission of a 128-bit <strong>Selection Alert Sequence</strong>. This sequence serves as a "wake-up call" that:</p>
<ol dir="auto">
<li>Synchronizes the target's bit-stream parser</li>
<li>Ensures the target is listening for an activation sequence</li>
<li>Provides sufficient entropy to avoid accidental activation</li>
</ol>
<p dir="auto">The Selection Alert Sequence is a fixed 128-bit pattern defined in the ADI v6 specification:</p>
<div data-snippet-clipboard-copy-content="0x19bc0ea2_e3ddafe9_86852d95_6209f392 (transmitted LSB-first)"><pre><code>0x19bc0ea2_e3ddafe9_86852d95_6209f392 (transmitted LSB-first)
</code></pre></div>
<p dir="auto">This constant was chosen for its Hamming distance properties—it is unlikely to occur in normal signal traffic or be generated by crosstalk, glitches, or other non-debug activity.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3.C.3 Implementation: Robust Activation Strategy</h4><a id="user-content-3c3-implementation-robust-activation-strategy" aria-label="Permalink: 3.C.3 Implementation: Robust Activation Strategy" href="#3c3-implementation-robust-activation-strategy"></a></p>
<p dir="auto">Our implementation (<code>swd_protocol.c:357-382</code>) uses a <strong>defensive activation strategy</strong> that ensures reliable connection regardless of the SW-DP's initial state:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Phase 1: Exit any prior protocol mode
static const uint8_t seq_jtag_to_dormant[] = {
    0xff,0xff,0xff,0xff,0xff,0xff,0xff, 0xbc,0xe3
};

// Phase 2: Activate SWD mode
static const uint8_t seq_dormant_to_swd[] = {
    0xff,                                        // Line reset (8 ones)
    0x92,0xf3,0x09,0x62,0x95,0x2d,0x85,0x86,     // Selection Alert
    0xe9,0xaf,0xdd,0xe3,0xa2,0x0e,0xbc,0x19,     //   (128 bits)
    0xa0,0xf1,0xff,                              // SWD Activation Code (0x1a)
    0xff,0xff,0xff,0xff,0xff,0xff,0xff, 0xff,    // Line reset (>50 ones)
    0x00                                         // Idle low
};"><pre><span>// Phase 1: Exit any prior protocol mode</span>
<span>static</span> <span>const</span> <span>uint8_t</span> <span>seq_jtag_to_dormant</span>[] <span>=</span> {
    <span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>, <span>0xbc</span>,<span>0xe3</span>
};

<span>// Phase 2: Activate SWD mode</span>
<span>static</span> <span>const</span> <span>uint8_t</span> <span>seq_dormant_to_swd</span>[] <span>=</span> {
    <span>0xff</span>,                                        <span>// Line reset (8 ones)</span>
    <span>0x92</span>,<span>0xf3</span>,<span>0x09</span>,<span>0x62</span>,<span>0x95</span>,<span>0x2d</span>,<span>0x85</span>,<span>0x86</span>,     <span>// Selection Alert</span>
    <span>0xe9</span>,<span>0xaf</span>,<span>0xdd</span>,<span>0xe3</span>,<span>0xa2</span>,<span>0x0e</span>,<span>0xbc</span>,<span>0x19</span>,     <span>//   (128 bits)</span>
    <span>0xa0</span>,<span>0xf1</span>,<span>0xff</span>,                              <span>// SWD Activation Code (0x1a)</span>
    <span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>,<span>0xff</span>, <span>0xff</span>,    <span>// Line reset (&gt;50 ones)</span>
    <span>0x00</span>                                         <span>// Idle low</span>
};</pre></div>
<p dir="auto"><strong>Why this two-phase approach?</strong></p>
<p dir="auto">The problem is that we don't know the SW-DP's current state:</p>
<ol dir="auto">
<li><strong>Fresh power-up</strong>: SW-DP is in Dormant mode (default)</li>
<li><strong>Prior debug session</strong>: SW-DP may be in SWD or JTAG mode</li>
<li><strong>Failed connection attempt</strong>: SW-DP may be in an undefined state</li>
</ol>
<p dir="auto">If the SW-DP is already in SWD or JTAG mode, sending the Selection Alert Sequence will be interpreted as data transactions, potentially putting the DP into an error state. Our solution:</p>
<p dir="auto"><strong>Phase 1: Force transition to Dormant</strong></p>
<p dir="auto">Send the JTAG-to-Dormant sequence (56 ones followed by 0xbc, 0xe3). This sequence:</p>
<ul dir="auto">
<li>If in JTAG mode: transitions to Dormant</li>
<li>If in SWD mode: interpreted as line reset + invalid transactions (harmless)</li>
<li>If already Dormant: has no effect (dormant state ignores invalid input)</li>
</ul>
<p dir="auto">The sequence consists of:</p>
<ul dir="auto">
<li><strong>56 clock cycles high</strong> (JTAG TMS=1 → Test-Logic-Reset state)</li>
<li><strong>0x3cbe</strong> (0xbc, 0xe3 LSB-first): JTAG-specific exit pattern</li>
</ul>
<p dir="auto"><strong>Phase 2: Activate SWD from Dormant</strong></p>
<p dir="auto">Now that we're guaranteed to be in Dormant mode (or already in SWD mode where line reset is idempotent), we send:</p>
<ol dir="auto">
<li><strong>Line reset</strong> (8 ones): Clears any pending SWD transactions</li>
<li><strong>Selection Alert Sequence</strong> (128 bits): Wakes dormant state machine</li>
<li><strong>SWD Activation Code</strong> (0x1a, 8 bits): Selects SWD protocol</li>
<li><strong>Line reset</strong> (&gt;50 ones): Enters SWD Reset state, clearing sticky errors</li>
<li><strong>Idle cycles</strong>: Ensures clean transition</li>
</ol>
<p dir="auto">The SWD Activation Code <code>0x1a</code> decodes as:</p>
<div data-snippet-clipboard-copy-content="Bits[7:0] = 0x1a = 0b00011010"><pre><code>Bits[7:0] = 0x1a = 0b00011010
</code></pre></div>
<p dir="auto">This specific bit pattern was chosen to be distinct from valid JTAG TMS sequences, ensuring protocol disambiguation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3.C.4 Why Not Use the RP2350 Datasheet Sequence?</h4><a id="user-content-3c4-why-not-use-the-rp2350-datasheet-sequence" aria-label="Permalink: 3.C.4 Why Not Use the RP2350 Datasheet Sequence?" href="#3c4-why-not-use-the-rp2350-datasheet-sequence"></a></p>
<p dir="auto">The RP2350 datasheet (Section 3.5.1) describes a simpler connection sequence:</p>
<div data-snippet-clipboard-copy-content="1. At least 8 × SWCLK cycles with SWDIO high
2. The 128-bit Selection Alert sequence
3. Four SWCLK cycles with SWDIO low
4. SWD activation code: 0x1a, LSB first
5. At least 50 × SWCLK cycles with SWDIO high (line reset)
6. A DPIDR read to exit the Reset state"><pre><code>1. At least 8 × SWCLK cycles with SWDIO high
2. The 128-bit Selection Alert sequence
3. Four SWCLK cycles with SWDIO low
4. SWD activation code: 0x1a, LSB first
5. At least 50 × SWCLK cycles with SWDIO high (line reset)
6. A DPIDR read to exit the Reset state
</code></pre></div>
<p dir="auto">This sequence assumes the SW-DP is in Dormant mode at power-up. However, in real-world scenarios:</p>
<ul dir="auto">
<li>The target may have been previously debugged (SW-DP in SWD mode)</li>
<li>A debugger crash may have left the SW-DP in an error state</li>
<li>Multi-drop SWD configurations may require explicit state reset</li>
</ul>
<p dir="auto">Our JTAG→Dormant→SWD sequence provides <strong>universal robustness</strong>: it works regardless of the SW-DP's initial state. The cost is negligible—approximately 100 extra clock cycles, taking ~100µs at 1 MHz SWCLK—while the benefit is reliable connection without manual power-cycling.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3.C.5 Post-Activation Verification</h4><a id="user-content-3c5-post-activation-verification" aria-label="Permalink: 3.C.5 Post-Activation Verification" href="#3c5-post-activation-verification"></a></p>
<p dir="auto">After activation, we immediately read DP_IDCODE (<code>swd_protocol.c:386-397</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t idcode = 0;
err = swd_read_dp_raw(target, DP_IDCODE, &amp;idcode);
if (err != SWD_OK) {
    swd_set_error(target, err, &quot;Failed to read IDCODE&quot;);
    return err;
}

if ((idcode &amp; 0x0fffffff) == 0) {
    swd_set_error(target, SWD_ERROR_PROTOCOL, &quot;Invalid IDCODE: 0x%08x&quot;, idcode);
    return SWD_ERROR_PROTOCOL;
}"><pre><span>uint32_t</span> <span>idcode</span> <span>=</span> <span>0</span>;
<span>err</span> <span>=</span> <span>swd_read_dp_raw</span>(<span>target</span>, <span>DP_IDCODE</span>, <span>&amp;</span><span>idcode</span>);
<span>if</span> (<span>err</span> <span>!=</span> <span>SWD_OK</span>) {
    <span>swd_set_error</span>(<span>target</span>, <span>err</span>, <span>"Failed to read IDCODE"</span>);
    <span>return</span> <span>err</span>;
}

<span>if</span> ((<span>idcode</span> <span>&amp;</span> <span>0x0fffffff</span>) <span>==</span> <span>0</span>) {
    <span>swd_set_error</span>(<span>target</span>, <span>SWD_ERROR_PROTOCOL</span>, <span>"Invalid IDCODE: 0x%08x"</span>, <span>idcode</span>);
    <span>return</span> <span>SWD_ERROR_PROTOCOL</span>;
}</pre></div>
<p dir="auto">A successful IDCODE read confirms:</p>
<ol dir="auto">
<li>SWD protocol is active</li>
<li>The SW-DP is responding to transactions</li>
<li>The SWCLK frequency is within tolerance</li>
<li>SWDIO signal integrity is sufficient</li>
</ol>
<p dir="auto">For RP2350, the IDCODE is <code>0x4c013477</code></p>
<p dir="auto">This defensive activation strategy, while not strictly necessary for fresh power-up scenarios, ensures our library works reliably across the full range of real-world debug connection scenarios—a critical property for a reusable debug library.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">4. DEBUG ACCESS PORT ARCHITECTURE</h2><a id="user-content-4-debug-access-port-architecture" aria-label="Permalink: 4. DEBUG ACCESS PORT ARCHITECTURE" href="#4-debug-access-port-architecture"></a></p>
<p dir="auto">The Debug Access Port (DAP) provides memory-mapped access to debug resources through two register banks:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">4.A Debug Port Registers</h3><a id="user-content-4a-debug-port-registers" aria-label="Permalink: 4.A Debug Port Registers" href="#4a-debug-port-registers"></a></p>
<p dir="auto">The Debug Port (DP) manages power domains and AP selection:</p>
<ul dir="auto">
<li><strong>DP_IDCODE</strong> (0x0): Designer and part number identification</li>
<li><strong>DP_CTRL_STAT</strong> (0x4): Power control and status flags</li>
<li><strong>DP_SELECT</strong> (0x8): AP and register bank selection</li>
<li><strong>DP_RDBUFF</strong> (0xC): Read buffer for pipelined AP reads</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">4.B Access Port Registers</h3><a id="user-content-4b-access-port-registers" aria-label="Permalink: 4.B Access Port Registers" href="#4b-access-port-registers"></a></p>
<p dir="auto">Access Ports (AP) provide interfaces to debug resources. RP2350 implements multiple APs:</p>
<ul dir="auto">
<li><strong>AP 0x0</strong>: ROM Table</li>
<li><strong>AP 0x2</strong>: ARM Core 0 AHB-AP</li>
<li><strong>AP 0x4</strong>: ARM Core 1 AHB-AP</li>
<li><strong>AP 0x8</strong>: RP2350-specific AP</li>
<li><strong>AP 0xA</strong>: RISC-V APB-AP (target of this library)</li>
</ul>
<p dir="auto">Each AP has standardized registers:</p>
<ul dir="auto">
<li><strong>AP_CSW</strong> (0x00): Control/Status Word</li>
<li><strong>AP_TAR</strong> (0x04): Transfer Address Register</li>
<li><strong>AP_DRW</strong> (0x0C): Data Read/Write Register</li>
<li><strong>AP_IDR</strong> (0xFC): Identification Register</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">4.C RP2350-Specific DP_SELECT Encoding</h3><a id="user-content-4c-rp2350-specific-dp_select-encoding" aria-label="Permalink: 4.C RP2350-Specific DP_SELECT Encoding" href="#4c-rp2350-specific-dp_select-encoding"></a></p>
<p dir="auto">Standard ARM DP_SELECT format uses bits[31:24] for APSEL and bits[7:4] for APBANKSEL. RP2350 implements a non-standard encoding (<code>dap.c:18-22</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t make_dp_select_rp2350(uint8_t apsel, uint8_t bank, bool ctrlsel) {
    // [15:12] = APSEL, [11:8] = 0xD, [7:4] = bank, [0] = ctrlsel
    return ((apsel &amp; 0xF) << 12) | (0xD << 8) | ((bank &amp; 0xF) << 4) | (ctrlsel ? 1 : 0);
}"><pre><span>uint32_t</span> <span>make_dp_select_rp2350</span>(<span>uint8_t</span> <span>apsel</span>, <span>uint8_t</span> <span>bank</span>, <span>bool</span> <span>ctrlsel</span>) {
    <span>// [15:12] = APSEL, [11:8] = 0xD, [7:4] = bank, [0] = ctrlsel</span>
    <span>return</span> ((<span>apsel</span> <span>&amp;</span> <span>0xF</span>) &lt;&lt; <span>12</span>) | (<span>0xD</span> &lt;&lt; <span>8</span>) | ((<span>bank</span> <span>&amp;</span> <span>0xF</span>) &lt;&lt; <span>4</span>) | (<span>ctrlsel</span> ? <span>1</span> : <span>0</span>);
}</pre></div>
<p dir="auto">The magic constant 0xD in bits[11:8] is undocumented but required for correct AP selection.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">4.D Bank Selection Caching</h3><a id="user-content-4d-bank-selection-caching" aria-label="Permalink: 4.D Bank Selection Caching" href="#4d-bank-selection-caching"></a></p>
<p dir="auto">AP registers are accessed through a banking mechanism where DP_SELECT must be written before each AP access. To minimize SWD transactions, the library maintains a cache of the current bank selection (<code>dap.c:28-55</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="static swd_error_t select_ap_bank(swd_target_t *target, uint8_t apsel, uint8_t bank) {
    if (target->dap.current_apsel == apsel &amp;&amp;
        target->dap.current_bank == bank &amp;&amp;
        target->dap.ctrlsel == true) {
        return SWD_OK;  // Already selected
    }
    // Write DP_SELECT...
    target->dap.current_apsel = apsel;
    target->dap.current_bank = bank;
    // ...
}"><pre><span>static</span> <span>swd_error_t</span> <span>select_ap_bank</span>(<span>swd_target_t</span> <span>*</span><span>target</span>, <span>uint8_t</span> <span>apsel</span>, <span>uint8_t</span> <span>bank</span>) {
    <span>if</span> (<span>target</span><span>-&gt;</span><span>dap</span>.<span>current_apsel</span> <span>==</span> <span>apsel</span> <span>&amp;&amp;</span>
        <span>target</span><span>-&gt;</span><span>dap</span>.<span>current_bank</span> <span>==</span> <span>bank</span> <span>&amp;&amp;</span>
        <span>target</span><span>-&gt;</span><span>dap</span>.<span>ctrlsel</span> <span>==</span> true) {
        <span>return</span> <span>SWD_OK</span>;  <span>// Already selected</span>
    }
    <span>// Write DP_SELECT...</span>
    <span>target</span><span>-&gt;</span><span>dap</span>.<span>current_apsel</span> <span>=</span> <span>apsel</span>;
    <span>target</span><span>-&gt;</span><span>dap</span>.<span>current_bank</span> <span>=</span> <span>bank</span>;
    <span>// ...</span>
}</pre></div>
<p dir="auto">This caching reduces transaction count by approximately 50% in typical debug sessions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">5. DEBUG DOMAIN POWER SEQUENCING</h2><a id="user-content-5-debug-domain-power-sequencing" aria-label="Permalink: 5. DEBUG DOMAIN POWER SEQUENCING" href="#5-debug-domain-power-sequencing"></a></p>
<p dir="auto">Before any debug operations can proceed, the Debug Power Domain (DPD) and System Power Domain (SPD) must be powered up. This is not a physical power operation but rather clock and reset domain enabling.</p>
<p dir="auto">The power-up sequence (<code>dap.c:61-110</code>) follows the ARM Debug Interface specification:</p>
<ol dir="auto">
<li><strong>Clear sticky errors</strong>: Write 0 to DP_CTRL_STAT</li>
<li><strong>Request power-up</strong>: Set CDBGPWRUPREQ (bit 28) and CSYSPWRUPREQ (bit 30)</li>
<li><strong>Poll acknowledgment</strong>: Wait for CDBGPWRUPACK (bit 29) and CSYSPWRUPACK (bit 31)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t ctrl_stat = (1 << 28) | (1 << 30);
swd_write_dp_raw(target, DP_CTRL_STAT, ctrl_stat);

for (int i = 0; i < 10; i++) {
    swd_read_dp_raw(target, DP_CTRL_STAT, &amp;status);
    bool cdbgpwrupack = (status >> 29) &amp; 1;
    bool csyspwrupack = (status >> 31) &amp; 1;
    if (cdbgpwrupack &amp;&amp; csyspwrupack) {
        return SWD_OK;
    }
    sleep_ms(20);
}"><pre><span>uint32_t</span> <span>ctrl_stat</span> <span>=</span> (<span>1</span> &lt;&lt; <span>28</span>) | (<span>1</span> &lt;&lt; <span>30</span>);
<span>swd_write_dp_raw</span>(<span>target</span>, <span>DP_CTRL_STAT</span>, <span>ctrl_stat</span>);

<span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>10</span>; <span>i</span><span>++</span>) {
    <span>swd_read_dp_raw</span>(<span>target</span>, <span>DP_CTRL_STAT</span>, <span>&amp;</span><span>status</span>);
    <span>bool</span> <span>cdbgpwrupack</span> <span>=</span> (<span>status</span> &gt;&gt; <span>29</span>) <span>&amp;</span> <span>1</span>;
    <span>bool</span> <span>csyspwrupack</span> <span>=</span> (<span>status</span> &gt;&gt; <span>31</span>) <span>&amp;</span> <span>1</span>;
    <span>if</span> (<span>cdbgpwrupack</span> <span>&amp;&amp;</span> <span>csyspwrupack</span>) {
        <span>return</span> <span>SWD_OK</span>;
    }
    <span>sleep_ms</span>(<span>20</span>);
}</pre></div>
<p dir="auto">Failure to complete this sequence results in all subsequent debug operations returning WAIT responses indefinitely.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">6. RP2350 DEBUG MODULE INITIALIZATION</h2><a id="user-content-6-rp2350-debug-module-initialization" aria-label="Permalink: 6. RP2350 DEBUG MODULE INITIALIZATION" href="#6-rp2350-debug-module-initialization"></a></p>
<p dir="auto">After DAP power-up, the RP2350-specific Debug Module must be initialized through an undocumented activation handshake. This sequence was reverse-engineered from OpenOCD's RP2350 support with an oscilloscope and patience.</p>
<p dir="auto">The activation sequence (<code>rp2350.c:106-194</code>) consists of:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">6.A AP Selection and CSW Configuration</h3><a id="user-content-6a-ap-selection-and-csw-configuration" aria-label="Permalink: 6.A AP Selection and CSW Configuration" href="#6a-ap-selection-and-csw-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t sel_bank0 = make_dp_select_rp2350(AP_RISCV, 0, true);
dap_write_dp(target, DP_SELECT, sel_bank0);

uint32_t csw = 0xA2000002;  // 32-bit access, auto-increment disabled
dap_write_ap(target, AP_RISCV, AP_CSW, csw);"><pre><span>uint32_t</span> <span>sel_bank0</span> <span>=</span> <span>make_dp_select_rp2350</span>(<span>AP_RISCV</span>, <span>0</span>, true);
<span>dap_write_dp</span>(<span>target</span>, <span>DP_SELECT</span>, <span>sel_bank0</span>);

<span>uint32_t</span> <span>csw</span> <span>=</span> <span>0xA2000002</span>;  <span>// 32-bit access, auto-increment disabled</span>
<span>dap_write_ap</span>(<span>target</span>, <span>AP_RISCV</span>, <span>AP_CSW</span>, <span>csw</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">6.B Bank 1 Activation Handshake</h3><a id="user-content-6b-bank-1-activation-handshake" aria-label="Permalink: 6.B Bank 1 Activation Handshake" href="#6b-bank-1-activation-handshake"></a></p>
<p dir="auto">The Debug Module registers are normally accessed through Bank 0, but activation requires Bank 1:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t sel_bank1 = make_dp_select_rp2350(AP_RISCV, 1, true);
dap_write_dp(target, DP_SELECT, sel_bank1);

// Three-phase handshake
dap_write_ap(target, AP_RISCV, AP_CSW, 0x00000000);  // Reset
dap_read_dp(target, DP_RDBUFF);
sleep_ms(50);

dap_write_ap(target, AP_RISCV, AP_CSW, 0x00000001);  // Activate
dap_read_dp(target, DP_RDBUFF);
sleep_ms(50);

dap_write_ap(target, AP_RISCV, AP_CSW, 0x07FFFFC1);  // Configure
dap_read_dp(target, DP_RDBUFF);
sleep_ms(50);"><pre><span>uint32_t</span> <span>sel_bank1</span> <span>=</span> <span>make_dp_select_rp2350</span>(<span>AP_RISCV</span>, <span>1</span>, true);
<span>dap_write_dp</span>(<span>target</span>, <span>DP_SELECT</span>, <span>sel_bank1</span>);

<span>// Three-phase handshake</span>
<span>dap_write_ap</span>(<span>target</span>, <span>AP_RISCV</span>, <span>AP_CSW</span>, <span>0x00000000</span>);  <span>// Reset</span>
<span>dap_read_dp</span>(<span>target</span>, <span>DP_RDBUFF</span>);
<span>sleep_ms</span>(<span>50</span>);

<span>dap_write_ap</span>(<span>target</span>, <span>AP_RISCV</span>, <span>AP_CSW</span>, <span>0x00000001</span>);  <span>// Activate</span>
<span>dap_read_dp</span>(<span>target</span>, <span>DP_RDBUFF</span>);
<span>sleep_ms</span>(<span>50</span>);

<span>dap_write_ap</span>(<span>target</span>, <span>AP_RISCV</span>, <span>AP_CSW</span>, <span>0x07FFFFC1</span>);  <span>// Configure</span>
<span>dap_read_dp</span>(<span>target</span>, <span>DP_RDBUFF</span>);
<span>sleep_ms</span>(<span>50</span>);</pre></div>
<p dir="auto">The expected status response is <code>0x04010001</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">7. RISC-V DEBUG MODULE INTERFACE</h2><a id="user-content-7-risc-v-debug-module-interface" aria-label="Permalink: 7. RISC-V DEBUG MODULE INTERFACE" href="#7-risc-v-debug-module-interface"></a></p>
<p dir="auto">The RISC-V Debug Module implements the RISC-V External Debug Support specification v0.13. Debug Module registers are memory-mapped at base address 0x40 (register addresses are byte offsets × 4).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">7.A Debug Module Registers</h3><a id="user-content-7a-debug-module-registers" aria-label="Permalink: 7.A Debug Module Registers" href="#7a-debug-module-registers"></a></p>
<p dir="auto">Key registers (<code>rp2350.c:17-29</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define DM_DMCONTROL   (0x10 * 4)  // Hart control
#define DM_DMSTATUS    (0x11 * 4)  // Hart status
#define DM_ABSTRACTCS  (0x16 * 4)  // Abstract command status
#define DM_COMMAND     (0x17 * 4)  // Abstract command execution
#define DM_DATA0       (0x04 * 4)  // Data transfer register
#define DM_PROGBUF0    (0x20 * 4)  // Program buffer word 0
#define DM_PROGBUF1    (0x21 * 4)  // Program buffer word 1
#define DM_SBCS        (0x38 * 4)  // System Bus Access Control
#define DM_SBADDRESS0  (0x39 * 4)  // SBA Address
#define DM_SBDATA0     (0x3C * 4)  // SBA Data"><pre><span>#define</span> <span>DM_DMCONTROL</span>   (0x10 * 4)  // Hart control
<span>#define</span> <span>DM_DMSTATUS</span>    (0x11 * 4)  // Hart status
<span>#define</span> <span>DM_ABSTRACTCS</span>  (0x16 * 4)  // Abstract command status
<span>#define</span> <span>DM_COMMAND</span>     (0x17 * 4)  // Abstract command execution
<span>#define</span> <span>DM_DATA0</span>       (0x04 * 4)  // Data transfer register
<span>#define</span> <span>DM_PROGBUF0</span>    (0x20 * 4)  // Program buffer word 0
<span>#define</span> <span>DM_PROGBUF1</span>    (0x21 * 4)  // Program buffer word 1
<span>#define</span> <span>DM_SBCS</span>        (0x38 * 4)  // System Bus Access Control
<span>#define</span> <span>DM_SBADDRESS0</span>  (0x39 * 4)  // SBA Address
<span>#define</span> <span>DM_SBDATA0</span>     (0x3C * 4)  // SBA Data</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">7.B Hart Control via DMCONTROL</h3><a id="user-content-7b-hart-control-via-dmcontrol" aria-label="Permalink: 7.B Hart Control via DMCONTROL" href="#7b-hart-control-via-dmcontrol"></a></p>
<p dir="auto">Hart (hardware thread) execution is controlled through DMCONTROL register fields:</p>
<ul dir="auto">
<li><strong>dmactive</strong> (bit 0): Debug Module active (must be 1)</li>
<li><strong>haltreq</strong> (bit 31): Request hart halt</li>
<li><strong>resumereq</strong> (bit 30): Request hart resume</li>
</ul>
<p dir="auto">Halt sequence (<code>rp2350.c:205-240</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t dmcontrol = (1 << 31) | (1 << 0);  // haltreq | dmactive
dap_write_mem32(target, DM_DMCONTROL, dmcontrol);

// Poll DMSTATUS.allhalted (bit 9)
for (int i = 0; i < 10; i++) {
    swd_result_t result = dap_read_mem32(target, DM_DMSTATUS);
    bool allhalted = (result.value >> 9) &amp; 1;
    if (allhalted) {
        target->rp2350.hart_halted = true;
        return SWD_OK;
    }
    sleep_ms(10);
}"><pre><span>uint32_t</span> <span>dmcontrol</span> <span>=</span> (<span>1</span> &lt;&lt; <span>31</span>) | (<span>1</span> &lt;&lt; <span>0</span>);  <span>// haltreq | dmactive</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_DMCONTROL</span>, <span>dmcontrol</span>);

<span>// Poll DMSTATUS.allhalted (bit 9)</span>
<span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>10</span>; <span>i</span><span>++</span>) {
    <span>swd_result_t</span> <span>result</span> <span>=</span> <span>dap_read_mem32</span>(<span>target</span>, <span>DM_DMSTATUS</span>);
    <span>bool</span> <span>allhalted</span> <span>=</span> (<span>result</span>.<span>value</span> &gt;&gt; <span>9</span>) <span>&amp;</span> <span>1</span>;
    <span>if</span> (<span>allhalted</span>) {
        <span>target</span><span>-&gt;</span><span>rp2350</span>.<span>hart_halted</span> <span>=</span> true;
        <span>return</span> <span>SWD_OK</span>;
    }
    <span>sleep_ms</span>(<span>10</span>);
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">7.C Abstract Commands for Register Access</h3><a id="user-content-7c-abstract-commands-for-register-access" aria-label="Permalink: 7.C Abstract Commands for Register Access" href="#7c-abstract-commands-for-register-access"></a></p>
<p dir="auto">Abstract commands provide a high-level interface to hart state without halting. The COMMAND register format for GPR access:</p>
<div data-snippet-clipboard-copy-content="Bits 0-15:   regno (0x1000 + reg_num for GPRs)
Bit 16:      write (1=write, 0=read)
Bit 17:      transfer (1=execute transfer)
Bits 20-22:  aarsize (2=32-bit access)"><pre><code>Bits 0-15:   regno (0x1000 + reg_num for GPRs)
Bit 16:      write (1=write, 0=read)
Bit 17:      transfer (1=execute transfer)
Bits 20-22:  aarsize (2=32-bit access)
</code></pre></div>
<p dir="auto">GPR read implementation (<code>rp2350.c:333-389</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t command = 0;
command |= (0x1000 + reg_num) << 0;    // regno
command |= (1 << 17);                  // transfer
command |= (2 << 20);                  // aarsize=32-bit

dap_write_mem32(target, DM_COMMAND, command);
wait_abstract_command(target);  // Poll ABSTRACTCS.busy
result = dap_read_mem32(target, DM_DATA0);"><pre><span>uint32_t</span> <span>command</span> <span>=</span> <span>0</span>;
<span>command</span> |= (<span>0x1000</span> <span>+</span> <span>reg_num</span>) &lt;&lt; <span>0</span>;    <span>// regno</span>
<span>command</span> |= (<span>1</span> &lt;&lt; <span>17</span>);                  <span>// transfer</span>
<span>command</span> |= (<span>2</span> &lt;&lt; <span>20</span>);                  <span>// aarsize=32-bit</span>

<span>dap_write_mem32</span>(<span>target</span>, <span>DM_COMMAND</span>, <span>command</span>);
<span>wait_abstract_command</span>(<span>target</span>);  <span>// Poll ABSTRACTCS.busy</span>
<span>result</span> <span>=</span> <span>dap_read_mem32</span>(<span>target</span>, <span>DM_DATA0</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">7.D Program Buffer Execution Model</h3><a id="user-content-7d-program-buffer-execution-model" aria-label="Permalink: 7.D Program Buffer Execution Model" href="#7d-program-buffer-execution-model"></a></p>
<p dir="auto">The Program Buffer (PROGBUF) is a 16-entry instruction memory within the Debug Module that enables execution of arbitrary RISC-V code in the debug context. Understanding its operation requires examining the execution model, register preservation semantics, and synchronization mechanisms.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">7.D.1 The Dual-Context Execution Model</h4><a id="user-content-7d1-the-dual-context-execution-model" aria-label="Permalink: 7.D.1 The Dual-Context Execution Model" href="#7d1-the-dual-context-execution-model"></a></p>
<p dir="auto">A RISC-V hart operates in one of two contexts:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Normal Context</strong>: The hart executes from main memory, PC advances sequentially, and all architectural state is visible to the program.</p>
</li>
<li>
<p dir="auto"><strong>Debug Context</strong>: Upon entering debug mode (via halt request, ebreak, or trigger), the hart:</p>
<ul dir="auto">
<li>Saves PC to DPC (Debug Program Counter, CSR 0x7b1)</li>
<li>Enters a special execution mode where PROGBUF instructions execute</li>
<li>Maintains all GPRs and CSRs in their pre-halt state</li>
<li>Cannot access main memory without explicit instructions</li>
</ul>
</li>
</ol>
<p dir="auto">The Debug Module provides a "scratch pad" where debugger-supplied instructions execute with full access to hart state, but without disturbing that state beyond explicit modifications.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">7.D.2 PROGBUF Entry Layout</h4><a id="user-content-7d2-progbuf-entry-layout" aria-label="Permalink: 7.D.2 PROGBUF Entry Layout" href="#7d2-progbuf-entry-layout"></a></p>
<p dir="auto">RP2350's Debug Module provides 2 program buffer entries (PROGBUF0 and PROGBUF1), though the specification allows up to 16. Each entry holds one 32-bit RISC-V instruction:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define DM_PROGBUF0  (0x20 * 4)  // First instruction
#define DM_PROGBUF1  (0x21 * 4)  // Second instruction (typically ebreak)"><pre><span>#define</span> <span>DM_PROGBUF0</span>  (0x20 * 4)  // First instruction
<span>#define</span> <span>DM_PROGBUF1</span>  (0x21 * 4)  // Second instruction (typically ebreak)</pre></div>
<p dir="auto">The execution model assumes the final instruction is <code>ebreak</code> (0x00100073), which returns control to the Debug Module and makes the hart available for further debug operations.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">7.D.3 The Abstract Command Postexec Mechanism</h4><a id="user-content-7d3-the-abstract-command-postexec-mechanism" aria-label="Permalink: 7.D.3 The Abstract Command Postexec Mechanism" href="#7d3-the-abstract-command-postexec-mechanism"></a></p>
<p dir="auto">Abstract commands can trigger PROGBUF execution through the <code>postexec</code> bit (bit 18 of the COMMAND register). This creates a transactional execution model:</p>
<div data-snippet-clipboard-copy-content="┌──────────────────────────────────────────┐
│ 1. Debugger writes PROGBUF instructions  │
├──────────────────────────────────────────┤
│ 2. Debugger writes DATA0 (optional)      │
├──────────────────────────────────────────┤
│ 3. Abstract command with postexec=1      │
│   - Transfers DATA0 → GPR (if transfer=1)│
│   - Executes PROGBUF[0]..PROGBUF[N]      │
│   - Executes ebreak (returns to DM)      │
│   - Transfers GPR → DATA0 (if transfer=1)│
└──────────────────────────────────────────┘"><pre><code>┌──────────────────────────────────────────┐
│ 1. Debugger writes PROGBUF instructions  │
├──────────────────────────────────────────┤
│ 2. Debugger writes DATA0 (optional)      │
├──────────────────────────────────────────┤
│ 3. Abstract command with postexec=1      │
│   - Transfers DATA0 → GPR (if transfer=1)│
│   - Executes PROGBUF[0]..PROGBUF[N]      │
│   - Executes ebreak (returns to DM)      │
│   - Transfers GPR → DATA0 (if transfer=1)│
└──────────────────────────────────────────┘
</code></pre></div>
<p dir="auto">This mechanism eliminates race conditions: the data transfer and program execution form an atomic operation from the debugger's perspective.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">7.D.4 Case Study: Reading Debug CSR (DPC)</h4><a id="user-content-7d4-case-study-reading-debug-csr-dpc" aria-label="Permalink: 7.D.4 Case Study: Reading Debug CSR (DPC)" href="#7d4-case-study-reading-debug-csr-dpc"></a></p>
<p dir="auto">The Debug Program Counter (DPC, CSR 0x7b1) cannot be accessed via abstract commands—it exists only in debug context and abstract commands target normal context registers. Reading DPC requires PROGBUF execution (<code>rp2350.c:804-833</code>):</p>
<p dir="auto"><strong>Phase 1: Preserve scratch register</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_result_t saved_s0 = rp2350_read_reg(target, hart_id, 8);  // x8 = s0"><pre><span>swd_result_t</span> <span>saved_s0</span> <span>=</span> <span>rp2350_read_reg</span>(<span>target</span>, <span>hart_id</span>, <span>8</span>);  <span>// x8 = s0</span></pre></div>
<p dir="auto">The RISC-V ABI designates s0 (x8) as a saved register, but we must preserve it because our PROGBUF code will clobber it.</p>
<p dir="auto"><strong>Phase 2: Write PROGBUF instructions</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="dap_write_mem32(target, DM_PROGBUF0, 0x7b102473);  // csrr s0, dpc
dap_write_mem32(target, DM_PROGBUF1, 0x00100073);  // ebreak"><pre><span>dap_write_mem32</span>(<span>target</span>, <span>DM_PROGBUF0</span>, <span>0x7b102473</span>);  <span>// csrr s0, dpc</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_PROGBUF1</span>, <span>0x00100073</span>);  <span>// ebreak</span></pre></div>
<p dir="auto">The instruction <code>csrr s0, dpc</code> (CSR Read) has the encoding:</p>
<div data-snippet-clipboard-copy-content="31      20 19   15 14  12 11    7 6      0
┌─────────┬───────┬──────┬───────┬────────┐
│ 0x7b1   │ 0x00  │ 0x2  │ 0x08  │ 0x73   │
│ CSR addr│ rs1   │funct3│  rd   │ opcode │
│  DPC    │  x0   │CSRRS │  s0   │ SYSTEM │
└─────────┴───────┴──────┴───────┴────────┘"><pre><code>31      20 19   15 14  12 11    7 6      0
┌─────────┬───────┬──────┬───────┬────────┐
│ 0x7b1   │ 0x00  │ 0x2  │ 0x08  │ 0x73   │
│ CSR addr│ rs1   │funct3│  rd   │ opcode │
│  DPC    │  x0   │CSRRS │  s0   │ SYSTEM │
└─────────┴───────┴──────┴───────┴────────┘
</code></pre></div>
<ul dir="auto">
<li><strong>funct3=0x2 (CSRRS)</strong>: CSR Read and Set. Since rs1=x0, no bits are set (read-only operation).</li>
<li><strong>CSR 0x7b1</strong>: DPC is defined in RISC-V Debug Spec v0.13, section 4.8.2</li>
</ul>
<p dir="auto"><strong>Phase 3: Execute with postexec</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t command = (1 << 18);  // postexec=1, transfer=0
dap_write_mem32(target, DM_COMMAND, command);
wait_abstract_command(target);  // Poll ABSTRACTCS.busy"><pre><span>uint32_t</span> <span>command</span> <span>=</span> (<span>1</span> &lt;&lt; <span>18</span>);  <span>// postexec=1, transfer=0</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_COMMAND</span>, <span>command</span>);
<span>wait_abstract_command</span>(<span>target</span>);  <span>// Poll ABSTRACTCS.busy</span></pre></div>
<p dir="auto">The hart now executes:</p>
<ol dir="auto">
<li><code>csrr s0, dpc</code> → DPC value loaded into s0</li>
<li><code>ebreak</code> → Return to Debug Module, s0 contains DPC</li>
</ol>
<p dir="auto"><strong>Phase 4: Extract result via abstract command</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="result = rp2350_read_reg(target, hart_id, 8);  // Read s0 (now contains DPC)"><pre><span>result</span> <span>=</span> <span>rp2350_read_reg</span>(<span>target</span>, <span>hart_id</span>, <span>8</span>);  <span>// Read s0 (now contains DPC)</span></pre></div>
<p dir="auto"><strong>Phase 5: Restore architectural state</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="rp2350_write_reg(target, hart_id, 8, saved_s0.value);  // Restore s0"><pre><span>rp2350_write_reg</span>(<span>target</span>, <span>hart_id</span>, <span>8</span>, <span>saved_s0</span>.<span>value</span>);  <span>// Restore s0</span></pre></div>
<p dir="auto">This five-phase sequence is invisible to the hart's normal execution: when resumed, all registers appear unchanged.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">7.D.5 Writing Debug CSRs: The Inverse Operation</h4><a id="user-content-7d5-writing-debug-csrs-the-inverse-operation" aria-label="Permalink: 7.D.5 Writing Debug CSRs: The Inverse Operation" href="#7d5-writing-debug-csrs-the-inverse-operation"></a></p>
<p dir="auto">Writing DPC uses the inverse data flow (<code>rp2350.c:879-909</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Phase 1: Transfer new PC value to s0
err = rp2350_write_reg(target, hart_id, 8, new_pc_value);

// Phase 2: Write PROGBUF to copy s0 → DPC
dap_write_mem32(target, DM_PROGBUF0, 0x7b141073);  // csrw dpc, s0
dap_write_mem32(target, DM_PROGBUF1, 0x00100073);  // ebreak

// Phase 3: Execute
uint32_t command = (1 << 18);  // postexec=1
dap_write_mem32(target, DM_COMMAND, command);
wait_abstract_command(target);"><pre><span>// Phase 1: Transfer new PC value to s0</span>
<span>err</span> <span>=</span> <span>rp2350_write_reg</span>(<span>target</span>, <span>hart_id</span>, <span>8</span>, <span>new_pc_value</span>);

<span>// Phase 2: Write PROGBUF to copy s0 → DPC</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_PROGBUF0</span>, <span>0x7b141073</span>);  <span>// csrw dpc, s0</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_PROGBUF1</span>, <span>0x00100073</span>);  <span>// ebreak</span>

<span>// Phase 3: Execute</span>
<span>uint32_t</span> <span>command</span> <span>=</span> (<span>1</span> &lt;&lt; <span>18</span>);  <span>// postexec=1</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_COMMAND</span>, <span>command</span>);
<span>wait_abstract_command</span>(<span>target</span>);</pre></div>
<p dir="auto">The instruction <code>csrw dpc, s0</code> (CSR Write) has encoding 0x7b141073:</p>
<div data-snippet-clipboard-copy-content="31      20 19   15 14  12 11    7 6      0
┌─────────┬───────┬──────┬───────┬────────┐
│ 0x7b1   │ 0x08  │ 0x1  │ 0x00  │ 0x73   │
│ CSR addr│ rs1   │funct3│  rd   │ opcode │
│  DPC    │  s0   │CSRRW │  x0   │ SYSTEM │
└─────────┴───────┴──────┴───────┴────────┘"><pre><code>31      20 19   15 14  12 11    7 6      0
┌─────────┬───────┬──────┬───────┬────────┐
│ 0x7b1   │ 0x08  │ 0x1  │ 0x00  │ 0x73   │
│ CSR addr│ rs1   │funct3│  rd   │ opcode │
│  DPC    │  s0   │CSRRW │  x0   │ SYSTEM │
└─────────┴───────┴──────┴───────┴────────┘
</code></pre></div>
<p dir="auto"><strong>funct3=0x1 (CSRRW)</strong>: CSR Read and Write. The old CSR value is discarded (rd=x0), and s0's value is written to DPC.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">7.D.6 PROGBUF Execution Constraints</h4><a id="user-content-7d6-progbuf-execution-constraints" aria-label="Permalink: 7.D.6 PROGBUF Execution Constraints" href="#7d6-progbuf-execution-constraints"></a></p>
<p dir="auto">The PROGBUF execution environment imposes several constraints:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Memory Access Limitation</strong>: PROGBUF instructions execute in debug mode, where memory access depends on Debug Module configuration. Standard loads/stores may fault.</p>
</li>
<li>
<p dir="auto"><strong>Instruction Count</strong>: With only 2 entries, complex operations require multiple PROGBUF sequences. Each sequence incurs the cost of abstract command execution (~100µs typical).</p>
</li>
<li>
<p dir="auto"><strong>No Branching</strong>: PROGBUF is linear. Conditional execution requires host-side logic to decide which PROGBUF sequence to execute.</p>
</li>
<li>
<p dir="auto"><strong>Register Pressure</strong>: Only one scratch register (s0) is conventionally used. More complex operations require additional saves/restores.</p>
</li>
<li>
<p dir="auto"><strong>Ebreak Requirement</strong>: The final instruction must be <code>ebreak</code>. Omitting it causes the hart to hang in debug mode.</p>
</li>
</ol>
<p dir="auto">This execution model provides a "remote procedure call" mechanism where the host supplies short instruction sequences that execute atomically on the hart, providing a window into debug-only architectural state.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">8. SYSTEM BUS ACCESS: NON-INTRUSIVE MEMORY OPERATIONS</h2><a id="user-content-8-system-bus-access-non-intrusive-memory-operations" aria-label="Permalink: 8. SYSTEM BUS ACCESS: NON-INTRUSIVE MEMORY OPERATIONS" href="#8-system-bus-access-non-intrusive-memory-operations"></a></p>
<p dir="auto">System Bus Access (SBA) represents a fundamental departure from the traditional halt-based debugging model. Where classical debugging requires stopping the hart, transferring data through GPRs, and resuming, SBA provides a "back door" to the memory subsystem that operates concurrently with hart execution.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">8.A The SBA Architecture</h3><a id="user-content-8a-the-sba-architecture" aria-label="Permalink: 8.A The SBA Architecture" href="#8a-the-sba-architecture"></a></p>
<p dir="auto">The Debug Module contains a bus master that can initiate memory transactions on the system bus independently of the harts. This master has the following characteristics:</p>
<ol dir="auto">
<li><strong>Separate Bus Master</strong>: SBA transactions do not consume hart resources or execution time</li>
<li><strong>Concurrent Operation</strong>: Memory reads/writes occur while harts execute normally</li>
<li><strong>Cache Coherency Dependency</strong>: SBA bypasses hart caches; coherency is NOT guaranteed</li>
<li><strong>Bus Arbitration</strong>: SBA competes with harts for bus bandwidth</li>
</ol>
<p dir="auto">The SBA interface consists of three memory-mapped registers in the Debug Module:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define DM_SBCS        (0x38 * 4)  // System Bus Access Control and Status
#define DM_SBADDRESS0  (0x39 * 4)  // System Bus Address (32-bit)
#define DM_SBDATA0     (0x3C * 4)  // System Bus Data (32-bit)"><pre><span>#define</span> <span>DM_SBCS</span>        (0x38 * 4)  // System Bus Access Control and Status
<span>#define</span> <span>DM_SBADDRESS0</span>  (0x39 * 4)  // System Bus Address (32-bit)
<span>#define</span> <span>DM_SBDATA0</span>     (0x3C * 4)  // System Bus Data (32-bit)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">8.B SBCS: Control and Status Word</h3><a id="user-content-8b-sbcs-control-and-status-word" aria-label="Permalink: 8.B SBCS: Control and Status Word" href="#8b-sbcs-control-and-status-word"></a></p>
<p dir="auto">The SBCS register (offset 0x38) contains configuration and status fields defined in RISC-V Debug Spec v0.13.2, section 3.12.18:</p>
<div data-snippet-clipboard-copy-content="31:29 sbversion        (read-only)  SBA version
28:23 (reserved)       0
   22 sbbusyerror      (W1C)        Bus error occurred
   21 sbbusy           (read-only)  Bus master is busy
   20 sbreadonaddr     (read-write) Auto-read on SBADDRESS0 write
19:17 sbaccess         (read-write) Access width: 0=8-bit, 1=16-bit, 2=32-bit
   16 sbautoincrement  (read-write) Auto-increment address after access
   15 sbreadondata     (read-write) Auto-read on SBDATA0 read
14:12 sberror          (W1C)        Error status (0=none, 1=timeout, 2=bad addr, 3=alignment, 4=size, 7=other)
11:5  sbasize          (read-only)  Address width in bits (32 for RP2350)"><pre><code>31:29 sbversion        (read-only)  SBA version
28:23 (reserved)       0
   22 sbbusyerror      (W1C)        Bus error occurred
   21 sbbusy           (read-only)  Bus master is busy
   20 sbreadonaddr     (read-write) Auto-read on SBADDRESS0 write
19:17 sbaccess         (read-write) Access width: 0=8-bit, 1=16-bit, 2=32-bit
   16 sbautoincrement  (read-write) Auto-increment address after access
   15 sbreadondata     (read-write) Auto-read on SBDATA0 read
14:12 sberror          (W1C)        Error status (0=none, 1=timeout, 2=bad addr, 3=alignment, 4=size, 7=other)
11:5  sbasize          (read-only)  Address width in bits (32 for RP2350)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">8.C SBA Initialization: Capability Discovery</h3><a id="user-content-8c-sba-initialization-capability-discovery" aria-label="Permalink: 8.C SBA Initialization: Capability Discovery" href="#8c-sba-initialization-capability-discovery"></a></p>
<p dir="auto">The SBA subsystem initialization (<code>rp2350.c:958-992</code>) follows a capability discovery pattern:</p>
<p dir="auto"><strong>Phase 1: Read SBCS to detect supported features</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_result_t result = dap_read_mem32(target, DM_SBCS);"><pre><span>swd_result_t</span> <span>result</span> <span>=</span> <span>dap_read_mem32</span>(<span>target</span>, <span>DM_SBCS</span>);</pre></div>
<p dir="auto"><strong>Phase 2: Verify SBA capability</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="// Check sbasize field (bits [11:5]) to verify SBA is present
uint32_t sbasize = (result.value >> 5) &amp; 0x7F;
if (sbasize == 0) {
    return SWD_ERROR_INVALID_STATE;  // SBA not available
}"><pre><span>// Check sbasize field (bits [11:5]) to verify SBA is present</span>
<span>uint32_t</span> <span>sbasize</span> <span>=</span> (<span>result</span>.<span>value</span> &gt;&gt; <span>5</span>) <span>&amp;</span> <span>0x7F</span>;
<span>if</span> (<span>sbasize</span> <span>==</span> <span>0</span>) {
    <span>return</span> <span>SWD_ERROR_INVALID_STATE</span>;  <span>// SBA not available</span>
}</pre></div>
<p dir="auto">The <code>sbasize</code> field indicates the system bus address width (32 bits for RP2350). RP2350 supports 8-bit, 16-bit, and 32-bit access widths. We configure for 32-bit:</p>
<p dir="auto"><strong>Phase 3: Configure access mode</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t sbcs = 0;
sbcs |= (2 << 17);  // sbaccess = 2 (32-bit)
sbcs |= (1 << 20);  // sbreadonaddr = 1 (auto-read trigger)
dap_write_mem32(target, DM_SBCS, sbcs);"><pre><span>uint32_t</span> <span>sbcs</span> <span>=</span> <span>0</span>;
<span>sbcs</span> |= (<span>2</span> &lt;&lt; <span>17</span>);  <span>// sbaccess = 2 (32-bit)</span>
<span>sbcs</span> |= (<span>1</span> &lt;&lt; <span>20</span>);  <span>// sbreadonaddr = 1 (auto-read trigger)</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_SBCS</span>, <span>sbcs</span>);</pre></div>
<p dir="auto">The <code>sbreadonaddr</code> flag is critical: it converts the address write into an atomic read-trigger operation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">8.D The Auto-Read Mechanism</h3><a id="user-content-8d-the-auto-read-mechanism" aria-label="Permalink: 8.D The Auto-Read Mechanism" href="#8d-the-auto-read-mechanism"></a></p>
<p dir="auto">Without <code>sbreadonaddr</code>, a memory read requires three transactions:</p>
<div data-snippet-clipboard-copy-content="1. Write address to SBADDRESS0
2. Write SBCS with read trigger
3. Read data from SBDATA0"><pre><code>1. Write address to SBADDRESS0
2. Write SBCS with read trigger
3. Read data from SBDATA0
</code></pre></div>
<p dir="auto">With <code>sbreadonaddr=1</code>, the middle step is eliminated:</p>
<div data-snippet-clipboard-copy-content="1. Write address to SBADDRESS0  ← Triggers bus read automatically
2. Read data from SBDATA0       ← Data is ready"><pre><code>1. Write address to SBADDRESS0  ← Triggers bus read automatically
2. Read data from SBDATA0       ← Data is ready
</code></pre></div>
<p dir="auto">Implementation (<code>rp2350.c:1013-1020</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="dap_write_mem32(target, DM_SBADDRESS0, addr);  // Write triggers read
result = dap_read_mem32(target, DM_SBDATA0);   // Data is already valid"><pre><span>dap_write_mem32</span>(<span>target</span>, <span>DM_SBADDRESS0</span>, <span>addr</span>);  <span>// Write triggers read</span>
<span>result</span> <span>=</span> <span>dap_read_mem32</span>(<span>target</span>, <span>DM_SBDATA0</span>);   <span>// Data is already valid</span></pre></div>
<p dir="auto">The Debug Module's state machine looks like:</p>
<div data-snippet-clipboard-copy-content="IDLE → [SBADDRESS0 written] → BUSY → [bus read completes] → DATA_READY
                                ↓
                           [bus timeout] → SBERROR=1"><pre><code>IDLE → [SBADDRESS0 written] → BUSY → [bus read completes] → DATA_READY
                                ↓
                           [bus timeout] → SBERROR=1
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">8.E SBA Write Transactions</h3><a id="user-content-8e-sba-write-transactions" aria-label="Permalink: 8.E SBA Write Transactions" href="#8e-sba-write-transactions"></a></p>
<p dir="auto">Memory writes use SBDATA0 as the trigger register:</p>
<div dir="auto" data-snippet-clipboard-copy-content="dap_write_mem32(target, DM_SBADDRESS0, addr);   // Set address
dap_write_mem32(target, DM_SBDATA0, value);     // Write triggers bus write"><pre><span>dap_write_mem32</span>(<span>target</span>, <span>DM_SBADDRESS0</span>, <span>addr</span>);   <span>// Set address</span>
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_SBDATA0</span>, <span>value</span>);     <span>// Write triggers bus write</span></pre></div>
<p dir="auto">The write to SBDATA0 initiates the system bus write transaction. The debugger should poll SBCS.sbbusyerror to detect completion (though in practice, pipelined writes are often used).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">8.G SBA Error Handling</h3><a id="user-content-8g-sba-error-handling" aria-label="Permalink: 8.G SBA Error Handling" href="#8g-sba-error-handling"></a></p>
<p dir="auto">The SBCS.sberror field reports transaction failures:</p>
<div data-snippet-clipboard-copy-content="0: No error
1: Timeout (bus did not respond)
2: Bad address (unmapped region)
3: Bad alignment (misaligned access)
4: Bad size (unsupported width)
7: Other error"><pre><code>0: No error
1: Timeout (bus did not respond)
2: Bad address (unmapped region)
3: Bad alignment (misaligned access)
4: Bad size (unsupported width)
7: Other error
</code></pre></div>
<p dir="auto">Errors are sticky and must be explicitly cleared by writing 1 to SBCS.sberror (W1C = Write-1-to-Clear).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">9. STATE MANAGEMENT AND CACHING</h2><a id="user-content-9-state-management-and-caching" aria-label="Permalink: 9. STATE MANAGEMENT AND CACHING" href="#9-state-management-and-caching"></a></p>
<p dir="auto">The library maintains comprehensive state tracking to avoid redundant SWD transactions:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">9.A Connection State</h3><a id="user-content-9a-connection-state" aria-label="Permalink: 9.A Connection State" href="#9a-connection-state"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    bool connected;
    uint32_t idcode;
    bool resource_registered;
    // ...
} swd_target_t;"><pre><span>typedef</span> <span>struct</span> {
    <span>bool</span> <span>connected</span>;
    <span>uint32_t</span> <span>idcode</span>;
    <span>bool</span> <span>resource_registered</span>;
    <span>// ...</span>
} <span>swd_target_t</span>;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">9.B DAP State Caching</h3><a id="user-content-9b-dap-state-caching" aria-label="Permalink: 9.B DAP State Caching" href="#9b-dap-state-caching"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    uint8_t current_apsel;
    uint8_t current_bank;
    bool ctrlsel;
    uint32_t select_cache;
    bool powered;
    uint retry_count;
} dap_state_t;"><pre><span>typedef</span> <span>struct</span> {
    <span>uint8_t</span> <span>current_apsel</span>;
    <span>uint8_t</span> <span>current_bank</span>;
    <span>bool</span> <span>ctrlsel</span>;
    <span>uint32_t</span> <span>select_cache</span>;
    <span>bool</span> <span>powered</span>;
    <span>uint</span> <span>retry_count</span>;
} <span>dap_state_t</span>;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">9.C Per-Hart State Tracking</h3><a id="user-content-9c-per-hart-state-tracking" aria-label="Permalink: 9.C Per-Hart State Tracking" href="#9c-per-hart-state-tracking"></a></p>
<p dir="auto">RP2350 contains two RISC-V harts (hardware threads) that execute independently. The library maintains per-hart state to avoid redundant operations and enable concurrent debugging:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    bool halt_state_known;  // false after resume, true after halt/read status
    bool halted;            // true if hart is currently halted

    // Register cache
    bool cache_valid;       // true if cached values are current
    uint32_t cached_pc;
    uint32_t cached_gprs[32];
    uint64_t cache_timestamp;  // For LRU if needed
} hart_state_t;"><pre><span>typedef</span> <span>struct</span> {
    <span>bool</span> <span>halt_state_known</span>;  <span>// false after resume, true after halt/read status</span>
    <span>bool</span> <span>halted</span>;            <span>// true if hart is currently halted</span>

    <span>// Register cache</span>
    <span>bool</span> <span>cache_valid</span>;       <span>// true if cached values are current</span>
    <span>uint32_t</span> <span>cached_pc</span>;
    <span>uint32_t</span> <span>cached_gprs</span>[<span>32</span>];
    <span>uint64_t</span> <span>cache_timestamp</span>;  <span>// For LRU if needed</span>
} <span>hart_state_t</span>;</pre></div>
<p dir="auto">The top-level RP2350 state maintains an array of hart states:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define RP2350_NUM_HARTS 2

typedef struct {
    bool initialized;
    bool sba_initialized;

    // Per-hart state
    hart_state_t harts[RP2350_NUM_HARTS];

    // Shared cache configuration
    bool cache_enabled;
} rp2350_state_t;"><pre><span>#define</span> <span>RP2350_NUM_HARTS</span> 2

<span>typedef</span> <span>struct</span> {
    <span>bool</span> <span>initialized</span>;
    <span>bool</span> <span>sba_initialized</span>;

    <span>// Per-hart state</span>
    <span>hart_state_t</span> <span>harts</span>[<span>RP2350_NUM_HARTS</span>];

    <span>// Shared cache configuration</span>
    <span>bool</span> <span>cache_enabled</span>;
} <span>rp2350_state_t</span>;</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">9.C.1 Halt State Tracking</h4><a id="user-content-9c1-halt-state-tracking" aria-label="Permalink: 9.C.1 Halt State Tracking" href="#9c1-halt-state-tracking"></a></p>
<p dir="auto">The <code>halt_state_known</code> flag implements a three-state model:</p>
<ol dir="auto">
<li><strong>Unknown</strong> (<code>halt_state_known=false</code>): Hart state is uncertain (after resume or initialization)</li>
<li><strong>Known Halted</strong> (<code>halt_state_known=true, halted=true</code>): Hart is confirmed halted</li>
<li><strong>Known Running</strong> (<code>halt_state_known=true, halted=false</code>): Hart is confirmed running</li>
</ol>
<p dir="auto">This prevents expensive DMSTATUS polls when the state is known. State transitions:</p>
<div data-snippet-clipboard-copy-content="                ┌─────────────┐
                │   Unknown   │
                └──────┬──────┘
                       │
         ┌─────────────┼─────────────┐
         │                           │
    halt_request()             read_dmstatus()
         │                           │
         ▼                           ▼
   ┌────────────┐             ┌──────────────┐
   │   Halted   │             │   Running    │
   └─────┬──────┘             └──────┬───────┘
         │                           │
         │         resume()          │
         └───────────────────────────┘
                      │
                      ▼
                 ┌─────────┐
                 │ Unknown │  (state invalidated)
                 └─────────┘"><pre><code>                ┌─────────────┐
                │   Unknown   │
                └──────┬──────┘
                       │
         ┌─────────────┼─────────────┐
         │                           │
    halt_request()             read_dmstatus()
         │                           │
         ▼                           ▼
   ┌────────────┐             ┌──────────────┐
   │   Halted   │             │   Running    │
   └─────┬──────┘             └──────┬───────┘
         │                           │
         │         resume()          │
         └───────────────────────────┘
                      │
                      ▼
                 ┌─────────┐
                 │ Unknown │  (state invalidated)
                 └─────────┘
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">9.C.2 Register Caching</h4><a id="user-content-9c2-register-caching" aria-label="Permalink: 9.C.2 Register Caching" href="#9c2-register-caching"></a></p>
<p dir="auto">When <code>cache_enabled=true</code>, the library caches register values after reads. This optimization benefits:</p>
<ol dir="auto">
<li><strong>Repeated reads</strong> of the same register (e.g., polling loop variables)</li>
<li><strong>Bulk register dumps</strong> where <code>rp2350_read_all_regs()</code> populates the cache</li>
<li><strong>Reduced SWD traffic</strong> (each register read requires ~6 SWD transactions)</li>
</ol>
<p dir="auto">Cache invalidation occurs on:</p>
<ul dir="auto">
<li>Hart resume (execution changes registers)</li>
<li>Register write (specific register invalidated)</li>
<li>Hart halt request (conservative invalidation)</li>
</ul>
<p dir="auto">The cache is per-hart, allowing concurrent debugging of both harts without interference.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">10. RESOURCE MANAGEMENT</h2><a id="user-content-10-resource-management" aria-label="Permalink: 10. RESOURCE MANAGEMENT" href="#10-resource-management"></a></p>
<p dir="auto">PIO resources are scarce: RP2040/RP2350 provide 2 PIO blocks with 4 state machines each. The library implements a global resource tracker for multi-target support.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">10.A Global Resource Tracking</h3><a id="user-content-10a-global-resource-tracking" aria-label="Permalink: 10.A Global Resource Tracking" href="#10a-global-resource-tracking"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    swd_target_t *pio0_sm_owners[4];
    swd_target_t *pio1_sm_owners[4];
    uint active_count;
} resource_tracker_t;

extern resource_tracker_t g_resources;"><pre><span>typedef</span> <span>struct</span> {
    <span>swd_target_t</span> <span>*</span><span>pio0_sm_owners</span>[<span>4</span>];
    <span>swd_target_t</span> <span>*</span><span>pio1_sm_owners</span>[<span>4</span>];
    <span>uint</span> <span>active_count</span>;
} <span>resource_tracker_t</span>;

<span>extern</span> <span>resource_tracker_t</span> <span>g_resources</span>;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">10.B Automatic Allocation</h3><a id="user-content-10b-automatic-allocation" aria-label="Permalink: 10.B Automatic Allocation" href="#10b-automatic-allocation"></a></p>
<p dir="auto">When <code>SWD_PIO_AUTO</code> or <code>SWD_SM_AUTO</code> is specified in configuration, the library scans for free resources (<code>swd.c:105-125</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_error_t allocate_pio_sm(PIO *pio, uint *sm) {
    for (uint i = 0; i < 4; i++) {
        if (g_resources.pio0_sm_owners[i] == NULL) {
            *pio = pio0;
            *sm = i;
            return SWD_OK;
        }
    }
    // Try PIO1...
}"><pre><span>swd_error_t</span> <span>allocate_pio_sm</span>(<span>PIO</span> <span>*</span><span>pio</span>, <span>uint</span> <span>*</span><span>sm</span>) {
    <span>for</span> (<span>uint</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>4</span>; <span>i</span><span>++</span>) {
        <span>if</span> (<span>g_resources</span>.<span>pio0_sm_owners</span>[<span>i</span>] <span>==</span> <span>NULL</span>) {
            <span>*</span><span>pio</span> <span>=</span> <span>pio0</span>;
            <span>*</span><span>sm</span> <span>=</span> <span>i</span>;
            <span>return</span> <span>SWD_OK</span>;
        }
    }
    <span>// Try PIO1...</span>
}</pre></div>
<p dir="auto">Up to 8 simultaneous target connections are supported (limited by hardware resources).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">11. ERROR HANDLING AND RECOVERY</h2><a id="user-content-11-error-handling-and-recovery" aria-label="Permalink: 11. ERROR HANDLING AND RECOVERY" href="#11-error-handling-and-recovery"></a></p>
<p dir="auto">The library provides comprehensive error reporting through enumerated error codes and detailed message strings.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">11.A Error Code Taxonomy</h3><a id="user-content-11a-error-code-taxonomy" aria-label="Permalink: 11.A Error Code Taxonomy" href="#11a-error-code-taxonomy"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef enum {
    SWD_OK = 0,
    SWD_ERROR_TIMEOUT,        // Transaction timeout
    SWD_ERROR_FAULT,          // Target FAULT response
    SWD_ERROR_PROTOCOL,       // Malformed packet
    SWD_ERROR_PARITY,         // Parity check failure
    SWD_ERROR_WAIT,           // WAIT response retry exhausted
    SWD_ERROR_NOT_CONNECTED,  // No active connection
    SWD_ERROR_NOT_HALTED,     // Operation requires halted hart
    SWD_ERROR_ALREADY_HALTED, // Hart already halted (informational)
    // ...
} swd_error_t;"><pre><span>typedef</span> <span>enum</span> {
    <span>SWD_OK</span> <span>=</span> <span>0</span>,
    <span>SWD_ERROR_TIMEOUT</span>,        <span>// Transaction timeout</span>
    <span>SWD_ERROR_FAULT</span>,          <span>// Target FAULT response</span>
    <span>SWD_ERROR_PROTOCOL</span>,       <span>// Malformed packet</span>
    <span>SWD_ERROR_PARITY</span>,         <span>// Parity check failure</span>
    <span>SWD_ERROR_WAIT</span>,           <span>// WAIT response retry exhausted</span>
    <span>SWD_ERROR_NOT_CONNECTED</span>,  <span>// No active connection</span>
    <span>SWD_ERROR_NOT_HALTED</span>,     <span>// Operation requires halted hart</span>
    <span>SWD_ERROR_ALREADY_HALTED</span>, <span>// Hart already halted (informational)</span>
    <span>// ...</span>
} <span>swd_error_t</span>;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">11.B Error Detail Buffer</h3><a id="user-content-11b-error-detail-buffer" aria-label="Permalink: 11.B Error Detail Buffer" href="#11b-error-detail-buffer"></a></p>
<p dir="auto">Each target maintains a 128-byte error detail buffer for formatted diagnostic messages (<code>swd.c:67-84</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="void swd_set_error(swd_target_t *target, swd_error_t error,
                   const char *detail, ...) {
    target->last_error = error;
    va_list args;
    va_start(args, detail);
    vsnprintf(target->error_detail, sizeof(target->error_detail),
              detail, args);
    va_end(args);
}"><pre><span>void</span> <span>swd_set_error</span>(<span>swd_target_t</span> <span>*</span><span>target</span>, <span>swd_error_t</span> <span>error</span>,
                   <span>const</span> <span>char</span> <span>*</span><span>detail</span>, ...) {
    <span>target</span><span>-&gt;</span><span>last_error</span> <span>=</span> <span>error</span>;
    <span>va_list</span> <span>args</span>;
    <span>va_start</span>(<span>args</span>, <span>detail</span>);
    <span>vsnprintf</span>(<span>target</span><span>-&gt;</span><span>error_detail</span>, <span>sizeof</span>(<span>target</span><span>-&gt;</span><span>error_detail</span>),
              <span>detail</span>, <span>args</span>);
    <span>va_end</span>(<span>args</span>);
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">11.C ACK Response Mapping</h3><a id="user-content-11c-ack-response-mapping" aria-label="Permalink: 11.C ACK Response Mapping" href="#11c-ack-response-mapping"></a></p>
<p dir="auto">SWD protocol ACK responses are mapped to error codes (<code>swd.c:91-99</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_error_t swd_ack_to_error(uint8_t ack) {
    switch (ack) {
        case 0x1: return SWD_OK;            // OK
        case 0x2: return SWD_ERROR_WAIT;    // WAIT
        case 0x4: return SWD_ERROR_FAULT;   // FAULT
        default:  return SWD_ERROR_PROTOCOL;
    }
}"><pre><span>swd_error_t</span> <span>swd_ack_to_error</span>(<span>uint8_t</span> <span>ack</span>) {
    <span>switch</span> (<span>ack</span>) {
        <span>case</span> <span>0x1</span>: <span>return</span> <span>SWD_OK</span>;            <span>// OK</span>
        <span>case</span> <span>0x2</span>: <span>return</span> <span>SWD_ERROR_WAIT</span>;    <span>// WAIT</span>
        <span>case</span> <span>0x4</span>: <span>return</span> <span>SWD_ERROR_FAULT</span>;   <span>// FAULT</span>
        <span>default</span>:  <span>return</span> <span>SWD_ERROR_PROTOCOL</span>;
    }
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">11.D Retry Mechanism</h3><a id="user-content-11d-retry-mechanism" aria-label="Permalink: 11.D Retry Mechanism" href="#11d-retry-mechanism"></a></p>
<p dir="auto">WAIT responses trigger automatic retry with backoff (<code>swd_protocol.c:197-208</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="for (uint retry = 0; retry < target->dap.retry_count; retry++) {
    err = swd_io_raw(target, request, value, false);
    if (err != SWD_ERROR_WAIT) break;
    sleep_us(100);
}"><pre><span>for</span> (<span>uint</span> <span>retry</span> <span>=</span> <span>0</span>; <span>retry</span> <span>&lt;</span> <span>target</span><span>-&gt;</span><span>dap</span>.<span>retry_count</span>; <span>retry</span><span>++</span>) {
    <span>err</span> <span>=</span> <span>swd_io_raw</span>(<span>target</span>, <span>request</span>, <span>value</span>, false);
    <span>if</span> (<span>err</span> <span>!=</span> <span>SWD_ERROR_WAIT</span>) <span>break</span>;
    <span>sleep_us</span>(<span>100</span>);
}</pre></div>
<p dir="auto">Default retry count is 5, configurable via <code>swd_config_t</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">12. API USAGE</h2><a id="user-content-12-api-usage" aria-label="Permalink: 12. API USAGE" href="#12-api-usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">12.A Target Creation and Connection</h3><a id="user-content-12a-target-creation-and-connection" aria-label="Permalink: 12.A Target Creation and Connection" href="#12a-target-creation-and-connection"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_config_t config = swd_config_default();
config.pin_swclk = 2;
config.pin_swdio = 3;
config.freq_khz = 1000;
config.enable_caching = true;

swd_target_t *target = swd_target_create(&amp;config);
swd_connect(target);
rp2350_init(target);"><pre><span>swd_config_t</span> <span>config</span> <span>=</span> <span>swd_config_default</span>();
<span>config</span>.<span>pin_swclk</span> <span>=</span> <span>2</span>;
<span>config</span>.<span>pin_swdio</span> <span>=</span> <span>3</span>;
<span>config</span>.<span>freq_khz</span> <span>=</span> <span>1000</span>;
<span>config</span>.<span>enable_caching</span> <span>=</span> true;

<span>swd_target_t</span> <span>*</span><span>target</span> <span>=</span> <span>swd_target_create</span>(<span>&amp;</span><span>config</span>);
<span>swd_connect</span>(<span>target</span>);
<span>rp2350_init</span>(<span>target</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">12.B Hart Control</h3><a id="user-content-12b-hart-control" aria-label="Permalink: 12.B Hart Control" href="#12b-hart-control"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="// Halt hart 0
rp2350_halt(target, 0);

// Read program counter
swd_result_t pc = rp2350_read_pc(target, 0);
if (pc.error == SWD_OK) {
    printf(&quot;PC: 0x%08x\n&quot;, pc.value);
}

// Read all registers
uint32_t regs[32];
rp2350_read_all_regs(target, 0, regs);

// Single-step execution
rp2350_step(target, 0);

// Resume execution
rp2350_resume(target, 0);

// Reset hart
rp2350_reset(target, 0, true);  // Reset and halt"><pre><span>// Halt hart 0</span>
<span>rp2350_halt</span>(<span>target</span>, <span>0</span>);

<span>// Read program counter</span>
<span>swd_result_t</span> <span>pc</span> <span>=</span> <span>rp2350_read_pc</span>(<span>target</span>, <span>0</span>);
<span>if</span> (<span>pc</span>.<span>error</span> <span>==</span> <span>SWD_OK</span>) {
    <span>printf</span>(<span>"PC: 0x%08x\n"</span>, <span>pc</span>.<span>value</span>);
}

<span>// Read all registers</span>
<span>uint32_t</span> <span>regs</span>[<span>32</span>];
<span>rp2350_read_all_regs</span>(<span>target</span>, <span>0</span>, <span>regs</span>);

<span>// Single-step execution</span>
<span>rp2350_step</span>(<span>target</span>, <span>0</span>);

<span>// Resume execution</span>
<span>rp2350_resume</span>(<span>target</span>, <span>0</span>);

<span>// Reset hart</span>
<span>rp2350_reset</span>(<span>target</span>, <span>0</span>, true);  <span>// Reset and halt</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">12.C Memory Operations</h3><a id="user-content-12c-memory-operations" aria-label="Permalink: 12.C Memory Operations" href="#12c-memory-operations"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="// Read memory (non-intrusive via SBA)
swd_result_t result = rp2350_read_mem32(target, 0x20000000);

// Write memory
rp2350_write_mem32(target, 0x20000000, 0xDEADBEEF);

// Block operations
uint32_t buffer[256];
rp2350_read_mem_block(target, 0x20000000, buffer, 256);"><pre><span>// Read memory (non-intrusive via SBA)</span>
<span>swd_result_t</span> <span>result</span> <span>=</span> <span>rp2350_read_mem32</span>(<span>target</span>, <span>0x20000000</span>);

<span>// Write memory</span>
<span>rp2350_write_mem32</span>(<span>target</span>, <span>0x20000000</span>, <span>0xDEADBEEF</span>);

<span>// Block operations</span>
<span>uint32_t</span> <span>buffer</span>[<span>256</span>];
<span>rp2350_read_mem_block</span>(<span>target</span>, <span>0x20000000</span>, <span>buffer</span>, <span>256</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">12.D Code Execution</h3><a id="user-content-12d-code-execution" aria-label="Permalink: 12.D Code Execution" href="#12d-code-execution"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="const uint32_t program[] = {
    0x200415b7,  // lui  a1, 0x20040
    0xabcd0537,  // lui  a0, 0xabcd0
    0x23450513,  // addi a0, a0, 0x234
    0x00a5a223,  // sw   a0, 4(a1)
    0x0000006f,  // j    0 (infinite loop)
};

rp2350_execute_code(target, 0, 0x20000000, program, 5);"><pre><span>const</span> <span>uint32_t</span> <span>program</span>[] <span>=</span> {
    <span>0x200415b7</span>,  <span>// lui  a1, 0x20040</span>
    <span>0xabcd0537</span>,  <span>// lui  a0, 0xabcd0</span>
    <span>0x23450513</span>,  <span>// addi a0, a0, 0x234</span>
    <span>0x00a5a223</span>,  <span>// sw   a0, 4(a1)</span>
    <span>0x0000006f</span>,  <span>// j    0 (infinite loop)</span>
};

<span>rp2350_execute_code</span>(<span>target</span>, <span>0</span>, <span>0x20000000</span>, <span>program</span>, <span>5</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">12.E Instruction Tracing</h3><a id="user-content-12e-instruction-tracing" aria-label="Permalink: 12.E Instruction Tracing" href="#12e-instruction-tracing"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="// Trace callback receives each executed instruction
bool trace_callback(const trace_record_t *record, void *user_data) {
    printf(&quot;PC: 0x%08x  Instruction: 0x%08x\n&quot;,
           record->pc, record->instruction);

    // Optional: inspect registers
    if (record->regs) {
        printf(&quot;  x5=0x%08x\n&quot;, record->regs[5]);
    }

    return true;  // Continue tracing (false = stop)
}

// Trace 100 instructions on hart 0, capturing registers
int count = rp2350_trace(target, 0, 100, trace_callback, NULL, true);
printf(&quot;Traced %d instructions\n&quot;, count);"><pre><span>// Trace callback receives each executed instruction</span>
<span>bool</span> <span>trace_callback</span>(<span>const</span> <span>trace_record_t</span> <span>*</span><span>record</span>, <span>void</span> <span>*</span><span>user_data</span>) {
    <span>printf</span>(<span>"PC: 0x%08x  Instruction: 0x%08x\n"</span>,
           <span>record</span><span>-&gt;</span><span>pc</span>, <span>record</span><span>-&gt;</span><span>instruction</span>);

    <span>// Optional: inspect registers</span>
    <span>if</span> (<span>record</span><span>-&gt;</span><span>regs</span>) {
        <span>printf</span>(<span>"  x5=0x%08x\n"</span>, <span>record</span><span>-&gt;</span><span>regs</span>[<span>5</span>]);
    }

    <span>return</span> true;  <span>// Continue tracing (false = stop)</span>
}

<span>// Trace 100 instructions on hart 0, capturing registers</span>
<span>int</span> <span>count</span> <span>=</span> <span>rp2350_trace</span>(<span>target</span>, <span>0</span>, <span>100</span>, <span>trace_callback</span>, <span>NULL</span>, true);
<span>printf</span>(<span>"Traced %d instructions\n"</span>, <span>count</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">12.F Dual-Hart Operations</h3><a id="user-content-12f-dual-hart-operations" aria-label="Permalink: 12.F Dual-Hart Operations" href="#12f-dual-hart-operations"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="// Operate on both harts independently
rp2350_halt(target, 0);
rp2350_halt(target, 1);

// Read registers from both harts
uint32_t h0_regs[32], h1_regs[32];
rp2350_read_all_regs(target, 0, h0_regs);
rp2350_read_all_regs(target, 1, h1_regs);

// Execute different programs on each hart
rp2350_execute_code(target, 0, 0x20000000, program0, len0);
rp2350_execute_code(target, 1, 0x20001000, program1, len1);

// Trace hart 1 while hart 0 runs
rp2350_resume(target, 0);
rp2350_trace(target, 1, 50, trace_callback, NULL, false);"><pre><span>// Operate on both harts independently</span>
<span>rp2350_halt</span>(<span>target</span>, <span>0</span>);
<span>rp2350_halt</span>(<span>target</span>, <span>1</span>);

<span>// Read registers from both harts</span>
<span>uint32_t</span> <span>h0_regs</span>[<span>32</span>], <span>h1_regs</span>[<span>32</span>];
<span>rp2350_read_all_regs</span>(<span>target</span>, <span>0</span>, <span>h0_regs</span>);
<span>rp2350_read_all_regs</span>(<span>target</span>, <span>1</span>, <span>h1_regs</span>);

<span>// Execute different programs on each hart</span>
<span>rp2350_execute_code</span>(<span>target</span>, <span>0</span>, <span>0x20000000</span>, <span>program0</span>, <span>len0</span>);
<span>rp2350_execute_code</span>(<span>target</span>, <span>1</span>, <span>0x20001000</span>, <span>program1</span>, <span>len1</span>);

<span>// Trace hart 1 while hart 0 runs</span>
<span>rp2350_resume</span>(<span>target</span>, <span>0</span>);
<span>rp2350_trace</span>(<span>target</span>, <span>1</span>, <span>50</span>, <span>trace_callback</span>, <span>NULL</span>, false);</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">13. BUILDING AND INTEGRATION</h2><a id="user-content-13-building-and-integration" aria-label="Permalink: 13. BUILDING AND INTEGRATION" href="#13-building-and-integration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">13.A CMake Integration</h3><a id="user-content-13a-cmake-integration" aria-label="Permalink: 13.A CMake Integration" href="#13a-cmake-integration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="add_subdirectory(lib/pico2-swd-riscv)
target_link_libraries(your_application pico2_swd_riscv)"><pre><span>add_subdirectory</span>(lib/pico2-swd-riscv)
<span>target_link_libraries</span>(your_application pico2_swd_riscv)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">13.B Debug Level Configuration</h3><a id="user-content-13b-debug-level-configuration" aria-label="Permalink: 13.B Debug Level Configuration" href="#13b-debug-level-configuration"></a></p>
<p dir="auto">Set compile-time debug verbosity:</p>
<div dir="auto" data-snippet-clipboard-copy-content="target_compile_definitions(your_application PRIVATE PICO2_SWD_DEBUG_LEVEL=3)"><pre><span>target_compile_definitions</span>(your_application <span>PRIVATE</span> PICO2_SWD_DEBUG_LEVEL=3)</pre></div>
<p dir="auto">Levels: 0 (none), 1 (warnings), 2 (info), 3 (debug).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">14. REFERENCES</h2><a id="user-content-14-references" aria-label="Permalink: 14. REFERENCES" href="#14-references"></a></p>
<ul dir="auto">
<li>ARM Debug Interface Architecture Specification v5.2</li>
<li>ARM CoreSight SWD-DP Technical Reference Manual</li>
<li>RISC-V External Debug Support version 0.13</li>
<li>RP2350 Datasheet, Chapter 3.5: Debug</li>
<li>ADIv5.2 Supplement for Multi-drop SWD</li>
<li>IEEE 1149.1-2001 (JTAG)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">15. RISC-V SINGLE-STEP EXECUTION</h2><a id="user-content-15-risc-v-single-step-execution" aria-label="Permalink: 15. RISC-V SINGLE-STEP EXECUTION" href="#15-risc-v-single-step-execution"></a></p>
<p dir="auto">Single-step execution enables instruction-level debugging by executing exactly one instruction before re-entering debug mode. This is implemented via the DCSR.step bit (Debug Control and Status Register, bit 2).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">15.A The Step Bit Mechanism</h3><a id="user-content-15a-the-step-bit-mechanism" aria-label="Permalink: 15.A The Step Bit Mechanism" href="#15a-the-step-bit-mechanism"></a></p>
<p dir="auto">When DCSR.step=1, the hart executes one instruction after <code>resumereq</code>, then immediately re-halts:</p>
<div data-snippet-clipboard-copy-content="Debug Mode → [resumereq + DCSR.step=1] → Execute 1 instruction → Debug Mode"><pre><code>Debug Mode → [resumereq + DCSR.step=1] → Execute 1 instruction → Debug Mode
</code></pre></div>
<p dir="auto">Implementation (<code>rp2350.c:431-504</code>):</p>
<p dir="auto"><strong>Phase 1: Read current DCSR</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_result_t dcsr_result = read_dcsr(target, hart_id);"><pre><span>swd_result_t</span> <span>dcsr_result</span> <span>=</span> <span>read_dcsr</span>(<span>target</span>, <span>hart_id</span>);</pre></div>
<p dir="auto">DCSR must be read via PROGBUF (see Section 7.D) because it's a debug-only CSR.</p>
<p dir="auto"><strong>Phase 2: Set step bit</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t dcsr_stepped = dcsr_result.value | (1 << 2);
write_dcsr(target, hart_id, dcsr_stepped);"><pre><span>uint32_t</span> <span>dcsr_stepped</span> <span>=</span> <span>dcsr_result</span>.<span>value</span> | (<span>1</span> &lt;&lt; <span>2</span>);
<span>write_dcsr</span>(<span>target</span>, <span>hart_id</span>, <span>dcsr_stepped</span>);</pre></div>
<p dir="auto"><strong>Phase 3: Resume hart</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t dmcontrol = make_dmcontrol(hart_id, false, true, false);
dap_write_mem32(target, DM_DMCONTROL, dmcontrol);"><pre><span>uint32_t</span> <span>dmcontrol</span> <span>=</span> <span>make_dmcontrol</span>(<span>hart_id</span>, false, true, false);
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_DMCONTROL</span>, <span>dmcontrol</span>);</pre></div>
<p dir="auto">The hart now executes exactly one instruction, then:</p>
<ol dir="auto">
<li>PC is saved to DPC</li>
<li>Hart re-enters debug mode</li>
<li>DCSR.cause = 0x4 (single-step)</li>
</ol>
<p dir="auto"><strong>Phase 4: Wait for automatic halt</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="poll_dmstatus_halted(target, hart_id, true);"><pre><span>poll_dmstatus_halted</span>(<span>target</span>, <span>hart_id</span>, true);</pre></div>
<p dir="auto"><strong>Phase 5: Clear step bit</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="write_dcsr(target, hart_id, dcsr_result.value);  // Restore original DCSR"><pre><span>write_dcsr</span>(<span>target</span>, <span>hart_id</span>, <span>dcsr_result</span>.<span>value</span>);  <span>// Restore original DCSR</span></pre></div>
<p dir="auto">This ensures subsequent <code>rp2350_resume()</code> calls don't single-step.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">16. INSTRUCTION TRACING VIA ITERATED SINGLE-STEPPING</h2><a id="user-content-16-instruction-tracing-via-iterated-single-stepping" aria-label="Permalink: 16. INSTRUCTION TRACING VIA ITERATED SINGLE-STEPPING" href="#16-instruction-tracing-via-iterated-single-stepping"></a></p>
<p dir="auto">The library implements software instruction tracing by repeatedly single-stepping and recording each instruction. This provides a "time-travel" debugging capability at the cost of execution speed.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">16.A The Trace Callback Model</h3><a id="user-content-16a-the-trace-callback-model" aria-label="Permalink: 16.A The Trace Callback Model" href="#16a-the-trace-callback-model"></a></p>
<p dir="auto">Tracing uses a callback function to process each instruction (<code>rp2350.c:1262-1337</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
    uint32_t pc;
    uint32_t instruction;
    uint32_t regs[32];  // Valid if capture_regs=true
} trace_record_t;

typedef bool (*trace_callback_t)(const trace_record_t *record, void *user_data);"><pre><span>typedef</span> <span>struct</span> {
    <span>uint32_t</span> <span>pc</span>;
    <span>uint32_t</span> <span>instruction</span>;
    <span>uint32_t</span> <span>regs</span>[<span>32</span>];  <span>// Valid if capture_regs=true</span>
} <span>trace_record_t</span>;

<span>typedef</span> <span>bool</span> (<span>*</span><span>trace_callback_t</span>)(<span>const</span> <span>trace_record_t</span> <span>*</span><span>record</span>, <span>void</span> <span>*</span><span>user_data</span>);</pre></div>
<p dir="auto">The callback returns <code>true</code> to continue or <code>false</code> to stop.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">16.B Trace Implementation</h3><a id="user-content-16b-trace-implementation" aria-label="Permalink: 16.B Trace Implementation" href="#16b-trace-implementation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="int rp2350_trace(swd_target_t *target, uint8_t hart_id,
                 uint32_t max_instructions,
                 trace_callback_t callback, void *user_data,
                 bool capture_regs);"><pre><span>int</span> <span>rp2350_trace</span>(<span>swd_target_t</span> <span>*</span><span>target</span>, <span>uint8_t</span> <span>hart_id</span>,
                 <span>uint32_t</span> <span>max_instructions</span>,
                 <span>trace_callback_t</span> <span>callback</span>, <span>void</span> <span>*</span><span>user_data</span>,
                 <span>bool</span> <span>capture_regs</span>);</pre></div>
<p dir="auto">For each instruction:</p>
<ol dir="auto">
<li><strong>Read PC</strong> (via PROGBUF): Current instruction address</li>
<li><strong>Read memory at PC</strong>: Fetch the instruction word</li>
<li><strong>Optional: Read all GPRs</strong>: If <code>capture_regs=true</code></li>
<li><strong>Invoke callback</strong>: User processes the record</li>
<li><strong>Single-step</strong>: Execute one instruction</li>
<li><strong>Repeat</strong> until <code>max_instructions</code> or callback returns false</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">16.C Trace Use Cases</h3><a id="user-content-16c-trace-use-cases" aria-label="Permalink: 16.C Trace Use Cases" href="#16c-trace-use-cases"></a></p>
<p dir="auto"><strong>Loop Detection</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bool detect_loop(const trace_record_t *record, void *user_data) {
    static uint32_t entry_pc = 0;
    static int count = 0;

    if (count == 0) entry_pc = record->pc;
    if (record->pc == entry_pc &amp;&amp; count > 0) {
        printf(&quot;Loop detected at PC=0x%08x\n&quot;, record->pc);
        return false;  // Stop trace
    }
    count++;
    return true;
}"><pre><span>bool</span> <span>detect_loop</span>(<span>const</span> <span>trace_record_t</span> <span>*</span><span>record</span>, <span>void</span> <span>*</span><span>user_data</span>) {
    <span>static</span> <span>uint32_t</span> <span>entry_pc</span> <span>=</span> <span>0</span>;
    <span>static</span> <span>int</span> <span>count</span> <span>=</span> <span>0</span>;

    <span>if</span> (<span>count</span> <span>==</span> <span>0</span>) <span>entry_pc</span> <span>=</span> <span>record</span><span>-&gt;</span><span>pc</span>;
    <span>if</span> (<span>record</span><span>-&gt;</span><span>pc</span> <span>==</span> <span>entry_pc</span> <span>&amp;&amp;</span> <span>count</span> <span>&gt;</span> <span>0</span>) {
        <span>printf</span>(<span>"Loop detected at PC=0x%08x\n"</span>, <span>record</span><span>-&gt;</span><span>pc</span>);
        <span>return</span> false;  <span>// Stop trace</span>
    }
    <span>count</span><span>++</span>;
    <span>return</span> true;
}</pre></div>
<p dir="auto"><strong>Register State History</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bool capture_state(const trace_record_t *record, void *user_data) {
    printf(&quot;%08x: %08x  x5=%08x x6=%08x\n&quot;,
           record->pc, record->instruction,
           record->regs[5], record->regs[6]);
    return true;
}

rp2350_trace(target, 0, 100, capture_state, NULL, true);"><pre><span>bool</span> <span>capture_state</span>(<span>const</span> <span>trace_record_t</span> <span>*</span><span>record</span>, <span>void</span> <span>*</span><span>user_data</span>) {
    <span>printf</span>(<span>"%08x: %08x  x5=%08x x6=%08x\n"</span>,
           <span>record</span><span>-&gt;</span><span>pc</span>, <span>record</span><span>-&gt;</span><span>instruction</span>,
           <span>record</span><span>-&gt;</span><span>regs</span>[<span>5</span>], <span>record</span><span>-&gt;</span><span>regs</span>[<span>6</span>]);
    <span>return</span> true;
}

<span>rp2350_trace</span>(<span>target</span>, <span>0</span>, <span>100</span>, <span>capture_state</span>, <span>NULL</span>, true);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">16.D Trace Limitations</h3><a id="user-content-16d-trace-limitations" aria-label="Permalink: 16.D Trace Limitations" href="#16d-trace-limitations"></a></p>
<ol dir="auto">
<li><strong>Speed</strong>: ~5ms per instruction (200 instructions/second)</li>
<li><strong>Interrupt Masking</strong>: Tracing should occur with interrupts disabled (clear mstatus.MIE)</li>
<li><strong>Memory Consistency</strong>: Instructions are fetched via SBA; ensure I-cache coherency</li>
<li><strong>No Hardware Triggers</strong>: Trace starts immediately; no "trace until condition"</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">17. HART RESET OPERATIONS</h2><a id="user-content-17-hart-reset-operations" aria-label="Permalink: 17. HART RESET OPERATIONS" href="#17-hart-reset-operations"></a></p>
<p dir="auto">Hart reset (<code>rp2350_reset</code>) implements a controlled reset sequence via DMCONTROL.ndmreset (non-debug module reset, bit 1).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">17.A Reset Sequence</h3><a id="user-content-17a-reset-sequence" aria-label="Permalink: 17.A Reset Sequence" href="#17a-reset-sequence"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="swd_error_t rp2350_reset(swd_target_t *target, uint8_t hart_id,
                         bool halt_on_reset);"><pre><span>swd_error_t</span> <span>rp2350_reset</span>(<span>swd_target_t</span> <span>*</span><span>target</span>, <span>uint8_t</span> <span>hart_id</span>,
                         <span>bool</span> <span>halt_on_reset</span>);</pre></div>
<p dir="auto"><strong>Phase 1: Assert ndmreset</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="uint32_t dmcontrol = make_dmcontrol(hart_id, halt_on_reset, false, true);
dap_write_mem32(target, DM_DMCONTROL, dmcontrol);
sleep_ms(10);  // Hold reset"><pre><span>uint32_t</span> <span>dmcontrol</span> <span>=</span> <span>make_dmcontrol</span>(<span>hart_id</span>, <span>halt_on_reset</span>, false, true);
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_DMCONTROL</span>, <span>dmcontrol</span>);
<span>sleep_ms</span>(<span>10</span>);  <span>// Hold reset</span></pre></div>
<p dir="auto"><strong>Phase 2: Deassert ndmreset</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="dmcontrol = make_dmcontrol(hart_id, halt_on_reset, false, false);
dap_write_mem32(target, DM_DMCONTROL, dmcontrol);
sleep_ms(50);  // Wait for reset completion"><pre><span>dmcontrol</span> <span>=</span> <span>make_dmcontrol</span>(<span>hart_id</span>, <span>halt_on_reset</span>, false, false);
<span>dap_write_mem32</span>(<span>target</span>, <span>DM_DMCONTROL</span>, <span>dmcontrol</span>);
<span>sleep_ms</span>(<span>50</span>);  <span>// Wait for reset completion</span></pre></div>
<p dir="auto">If <code>halt_on_reset=true</code>, the DMCONTROL.haltreq bit remains set, causing the hart to enter debug mode immediately after reset, with PC set to the reset vector.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">17.B Reset vs Power-On</h3><a id="user-content-17b-reset-vs-power-on" aria-label="Permalink: 17.B Reset vs Power-On" href="#17b-reset-vs-power-on"></a></p>
<p dir="auto">This reset is equivalent to a power-on reset for the hart:</p>
<ul dir="auto">
<li>PC → reset vector (typically 0x00000000 for RP2350 RISC-V cores)</li>
<li>All CSRs → architectural reset values</li>
<li>GPRs → undefined</li>
<li>Cache → invalidated</li>
</ul>
<p dir="auto">Unlike a full chip reset, peripherals and other harts are unaffected.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">18. DUAL-HART ARCHITECTURE</h2><a id="user-content-18-dual-hart-architecture" aria-label="Permalink: 18. DUAL-HART ARCHITECTURE" href="#18-dual-hart-architecture"></a></p>
<p dir="auto">RP2350's two RISC-V harts (Hazard3 cores) are symmetric and independently controllable. The library provides full per-hart state tracking and concurrent operation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">18.A Hart Selection via DMCONTROL</h3><a id="user-content-18a-hart-selection-via-dmcontrol" aria-label="Permalink: 18.A Hart Selection via DMCONTROL" href="#18a-hart-selection-via-dmcontrol"></a></p>
<p dir="auto">Each Debug Module operation targets a specific hart via DMCONTROL.hartsel[9:0]:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline uint32_t make_dmcontrol(uint8_t hart_id, bool haltreq,
                                       bool resumereq, bool ndmreset) {
    uint32_t dmcontrol = (1 << 0);  // dmactive = 1
    dmcontrol |= ((uint32_t)hart_id << 16);  // hartsello[9:0] at bits 25:16
    if (haltreq) dmcontrol |= (1 << 31);
    if (resumereq) dmcontrol |= (1 << 30);
    if (ndmreset) dmcontrol |= (1 << 1);
    return dmcontrol;
}"><pre><span>static</span> <span>inline</span> <span>uint32_t</span> <span>make_dmcontrol</span>(<span>uint8_t</span> <span>hart_id</span>, <span>bool</span> <span>haltreq</span>,
                                       <span>bool</span> <span>resumereq</span>, <span>bool</span> <span>ndmreset</span>) {
    <span>uint32_t</span> <span>dmcontrol</span> <span>=</span> (<span>1</span> &lt;&lt; <span>0</span>);  <span>// dmactive = 1</span>
    <span>dmcontrol</span> |= ((<span>uint32_t</span>)<span>hart_id</span> &lt;&lt; <span>16</span>);  <span>// hartsello[9:0] at bits 25:16</span>
    <span>if</span> (<span>haltreq</span>) <span>dmcontrol</span> |= (<span>1</span> &lt;&lt; <span>31</span>);
    <span>if</span> (<span>resumereq</span>) <span>dmcontrol</span> |= (<span>1</span> &lt;&lt; <span>30</span>);
    <span>if</span> (<span>ndmreset</span>) <span>dmcontrol</span> |= (<span>1</span> &lt;&lt; <span>1</span>);
    <span>return</span> <span>dmcontrol</span>;
}</pre></div>
<p dir="auto">Before any hart-specific operation (halt, resume, register read), the library writes DMCONTROL with the correct <code>hart_id</code>, switching the Debug Module's attention to that hart.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">18.B Independent Hart Control</h3><a id="user-content-18b-independent-hart-control" aria-label="Permalink: 18.B Independent Hart Control" href="#18b-independent-hart-control"></a></p>
<p dir="auto">The test suite validates that harts operate independently:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Halt hart 0, keep hart 1 running
rp2350_halt(target, 0);
rp2350_resume(target, 1);

// Read hart 0 registers while hart 1 executes
uint32_t h0_regs[32];
rp2350_read_all_regs(target, 0, h0_regs);"><pre><span>// Halt hart 0, keep hart 1 running</span>
<span>rp2350_halt</span>(<span>target</span>, <span>0</span>);
<span>rp2350_resume</span>(<span>target</span>, <span>1</span>);

<span>// Read hart 0 registers while hart 1 executes</span>
<span>uint32_t</span> <span>h0_regs</span>[<span>32</span>];
<span>rp2350_read_all_regs</span>(<span>target</span>, <span>0</span>, <span>h0_regs</span>);</pre></div>
<p dir="auto">This enables debugging one hart while the other maintains real-time operation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">18.C Register Isolation</h3><a id="user-content-18c-register-isolation" aria-label="Permalink: 18.C Register Isolation" href="#18c-register-isolation"></a></p>
<p dir="auto">Each hart maintains independent register state. Writing x5 on hart 0 does not affect x5 on hart 1. This is validated by <code>test_dual_hart.c:69-115</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rp2350_write_reg(target, 0, 5, 0xAAAAAAAA);
rp2350_write_reg(target, 1, 5, 0x55555555);

assert(rp2350_read_reg(target, 0, 5).value == 0xAAAAAAAA);
assert(rp2350_read_reg(target, 1, 5).value == 0x55555555);"><pre><span>rp2350_write_reg</span>(<span>target</span>, <span>0</span>, <span>5</span>, <span>0xAAAAAAAA</span>);
<span>rp2350_write_reg</span>(<span>target</span>, <span>1</span>, <span>5</span>, <span>0x55555555</span>);

<span>assert</span>(<span>rp2350_read_reg</span>(<span>target</span>, <span>0</span>, <span>5</span>).<span>value</span> <span>==</span> <span>0xAAAAAAAA</span>);
<span>assert</span>(<span>rp2350_read_reg</span>(<span>target</span>, <span>1</span>, <span>5</span>).<span>value</span> <span>==</span> <span>0x55555555</span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">18.D Shared Memory, Independent Caches</h3><a id="user-content-18d-shared-memory-independent-caches" aria-label="Permalink: 18.D Shared Memory, Independent Caches" href="#18d-shared-memory-independent-caches"></a></p>
<p dir="auto">Both harts share the same physical memory space but maintain independent caches. This creates coherency considerations:</p>
<ol dir="auto">
<li><strong>SBA Writes</strong>: Visible to both harts (after cache invalidation)</li>
<li><strong>Hart 0 Writes</strong>: Not immediately visible to Hart 1 if cached</li>
<li><strong>Explicit Synchronization</strong>: Required for inter-hart communication</li>
</ol>
<p dir="auto">The test suite exercises memory access while both harts run concurrently (<code>test_mem.c:291-347</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">19. CURRENT LIMITATIONS</h2><a id="user-content-19-current-limitations" aria-label="Permalink: 19. CURRENT LIMITATIONS" href="#19-current-limitations"></a></p>
<p dir="auto">This implementation does not currently support:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">19.A Hardware Breakpoints (Trigger Module)</h3><a id="user-content-19a-hardware-breakpoints-trigger-module" aria-label="Permalink: 19.A Hardware Breakpoints (Trigger Module)" href="#19a-hardware-breakpoints-trigger-module"></a></p>
<p dir="auto">RISC-V Debug Specification defines a Trigger Module for hardware breakpoints. Implementation was removed due to complexity. Workaround: Use single-step + PC comparison in software.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">19.B Multi-Drop SWD</h3><a id="user-content-19b-multi-drop-swd" aria-label="Permalink: 19.B Multi-Drop SWD" href="#19b-multi-drop-swd"></a></p>
<p dir="auto">The SWD protocol supports multiple targets on one bus via unique addresses. This library assumes a single target. Physical wiring for multi-target is possible but requires additional multiplexing logic.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">19.C Compressed Instruction (RVC) Extension</h3><a id="user-content-19c-compressed-instruction-rvc-extension" aria-label="Permalink: 19.C Compressed Instruction (RVC) Extension" href="#19c-compressed-instruction-rvc-extension"></a></p>
<p dir="auto">RP2350's Hazard3 cores support the C extension (16-bit compressed instructions). The library:</p>
<ul dir="auto">
<li>Correctly reads compressed instructions during tracing</li>
<li>Does NOT decode compressed instruction mnemonics</li>
<li>Assumes 4-byte alignment for code upload</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">19.D Performance Profiling</h3><a id="user-content-19d-performance-profiling" aria-label="Permalink: 19.D Performance Profiling" href="#19d-performance-profiling"></a></p>
<p dir="auto">No cycle-accurate performance counters are exposed. Implementing this requires:</p>
<ol dir="auto">
<li>Access to mcycle/minstret CSRs</li>
<li>Periodic sampling without halting (not possible with current SBA coherency)</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">19.E Flash Programming</h3><a id="user-content-19e-flash-programming" aria-label="Permalink: 19.E Flash Programming" href="#19e-flash-programming"></a></p>
<p dir="auto">No routines for RP2350 flash programming. This requires:</p>
<ol dir="auto">
<li>Loading flash programmer stub to SRAM</li>
<li>Executing stub via <code>rp2350_execute_code()</code></li>
<li>Monitoring completion via polling</li>
</ol>
<p dir="auto">The architecture supports this; implementation is left to applications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">20. LICENSE</h2><a id="user-content-20-license" aria-label="Permalink: 20. LICENSE" href="#20-license"></a></p>
<p dir="auto">Copyright (c) 2025</p>
<p dir="auto">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>
<p dir="auto">The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p>
<p dir="auto">THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>