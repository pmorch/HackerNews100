<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 17 Jul 2023 14:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A Firefox-only minimap (2021) (200 pts)]]></title>
            <link>https://www.stefanjudis.com/a-firefox-only-minimap/</link>
            <guid>36757542</guid>
            <pubDate>Mon, 17 Jul 2023 12:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stefanjudis.com/a-firefox-only-minimap/">https://www.stefanjudis.com/a-firefox-only-minimap/</a>, See on <a href="https://news.ycombinator.com/item?id=36757542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="map"><main id="main"><dl><p><dt>Updated at</dt><dd><time datetime="2021-10-18T09:06:08.611Z"><time-ago datetime="2021-10-18T09:06:08.611Z">Oct 18 2021</time-ago></time></dd></p><p><dt>Reading time</dt><dd>1min</dd></p></dl><div><p>Greetings! üëã</p><p>If you discovered this page, you're part of the few Firefox desktop users out there (market share is only 4% right now) and wondered how my blog posts' minimaps are made.</p><p>If you're not using Firefox, this is how the minimap looks like.</p><div><figure><a href="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png"><picture><source type="image/avif" srcset="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=300&amp;h=136 300w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=500&amp;h=228 500w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=700&amp;h=319 700w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=900&amp;h=410 900w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=avif&amp;fit=scale&amp;q=75&amp;w=1100&amp;h=502 1100w" sizes="(max-width: 50em) 98vw, 700px"><source type="image/webp" srcset="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=300&amp;h=136 300w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=500&amp;h=228 500w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=700&amp;h=319 700w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=900&amp;h=410 900w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=webp&amp;fit=scale&amp;q=75&amp;w=1100&amp;h=502 1100w" sizes="(max-width: 50em) 98vw, 700px"><img width="1000" height="456" srcset="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=300&amp;h=136 300w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=500&amp;h=228 500w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=700&amp;h=319 700w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=900&amp;h=410 900w, https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png?fm=jpg&amp;fit=scale&amp;q=75&amp;w=1100&amp;h=502 1100w" sizes="(max-width: 50em) 98vw, 700px" src="https://images.contentful.com/f20lfrunubsq/2FuQMHycwersLjIf4dlI7A/cdee022a4a4cefea48c79c4027f5acc3/Screen_Shot_2021-10-17_at_21.45.35.png" alt="Minimap on stefanjudis.com." loading="lazy" onload="this.classList.add('kf-fade-in')"></picture></a></figure></div><p>Firefox is the only browser that supports <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/element()">the fancy <code>element()</code> CSS function</a> (with a vendor prefix, but hey ü§∑‚Äç‚ôÇÔ∏è). The function allows you to display images of arbitrary HTML elements on your page! And the best thing is: it's live! Try selecting some text or scroll around to see lazy-loaded images kicking in. It's magic!</p><p>The CSS to define another HTML element as background image is the following:</p><pre><code><span>mini-map .screen-image .canvas</span> <span>{</span>
  <span>background</span><span>:</span> white <span>-moz-element</span><span>(</span>#main<span>)</span> no-repeat scroll center center / contain<span>;</span>
<span>}</span>
</code></pre><p>There's also some JavaScript to move the minimap's current viewport box, but the CSS one-liner is responsible for painting another DOM node. Use <code>-moz-element</code> and call it a day!</p><p>And with that, keep rocking (and using Firefox)!</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LazyVim (426 pts)]]></title>
            <link>https://www.lazyvim.org/</link>
            <guid>36753225</guid>
            <pubDate>Mon, 17 Jul 2023 01:52:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lazyvim.org/">https://www.lazyvim.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36753225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><svg viewBox="0 0 570 125" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M.86 19.2V.72h8.68V19.2H.86Z"></path><path d="M8.863 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M21.905 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm53.778 0V.72h8.68V19.2h-8.68Z"></path><path d="M80.885 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M88.888 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M96.89 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M104.893 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M117.935 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V.72h8.68V19.2h-8.68Z"></path><path d="M136.903 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M144.906 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M152.908 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M160.911 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M168.913 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M176.916 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M189.958 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.763 0V.72h8.68V19.2h-8.68Z"></path><path d="M200.923 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M213.966 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V.72h8.68V19.2h-8.68Z"></path><path d="M248.938 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M261.981 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.763 0V.72h8.68V19.2h-8.68Z"></path><path d="M272.946 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M285.989 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V.72h8.68V19.2h-8.68Z"></path><path d="M320.961 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M334.004 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.762 0V.72h8.68V19.2h-8.68Z"></path><path d="M344.969 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M358.011 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.763 0V.72h8.68V19.2h-8.68Z"></path><path d="M368.976 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M376.979 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M390.021 19.2V9.12h-5.179v-1.4h6.579V19.2h-1.4Zm-2.799 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V.72h8.68V19.2h-8.68Z"></path><path d="M424.994 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M432.997 19.2V.72h8.68V19.2h-8.68Z"></path><path d="M446.039 19.2V9.12h-5.18v-1.4h6.58V19.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm87.118-4.2v-1.26l4.606-7.812h-4.536V4.78h5.81v1.26l-4.606 7.812h4.746V15h-6.02ZM.86 36.2V17.72h8.68V36.2H.86Z"></path><path d="M8.863 36.2V17.72h8.68V36.2h-8.68Zm13.042 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm45.775 0V17.72h8.68V36.2h-8.68Z"></path><path d="M72.883 36.2V17.72h8.68V36.2h-8.68Zm10.242 0V24.72h6.58v1.4h-5.18V36.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M88.748 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M96.75 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M104.893 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M112.895 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M125.938 36.2V26.12h-5.18v-1.4h6.58V36.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm8.003-7.28v-11.2h1.399v9.8h5.18v1.4h-6.579Zm2.799-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M136.763 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M144.766 26.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M152.908 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M160.911 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M168.913 36.2V17.72h8.68V36.2h-8.68Zm10.243 0V24.72h6.58v1.4h-5.18V36.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M184.778 28.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.383 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M200.923 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M208.926 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M221.968 36.2V26.12h-5.18v-1.4h6.58V36.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V17.72h8.68V36.2h-8.68Z"></path><path d="M240.936 36.2V17.72h8.68V36.2h-8.68Zm10.242 0V24.72h6.58v1.4h-5.18V36.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M256.801 28.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm8.143 10.08V17.72h8.68V36.2h-8.68Z"></path><path d="M272.946 36.2V17.72h8.68V36.2h-8.68Zm13.043 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm29.77 0V17.72h8.68V36.2h-8.68Z"></path><path d="M320.961 36.2V17.72h8.68V36.2h-8.68Zm13.043 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm5.762 0V17.72h8.68V36.2h-8.68Z"></path><path d="M344.969 36.2V17.72h8.68V36.2h-8.68Zm13.042 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm5.763 0V17.72h8.68V36.2h-8.68Z"></path><path d="M368.976 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M376.979 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M384.981 36.2V17.72h8.681V36.2h-8.681Z"></path><path d="M398.024 36.2V26.12h-5.18v-1.4h6.58V36.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V17.72h8.68V36.2h-8.68Z"></path><path d="M416.992 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M424.994 36.2V17.72h8.68V36.2h-8.68Z"></path><path d="M432.997 36.2V17.72h8.68V36.2h-8.68Zm13.042 0V17.72h1.4V36.2h-1.4Zm-2.8 0V17.72h1.4V36.2h-1.4Zm55.108-4.2v-1.26l4.606-7.812h-4.536V21.78h5.81v1.26l-4.606 7.812h4.746V32h-6.02ZM.86 53.2V34.72h8.68V53.2H.86Z"></path><path d="M8.863 53.2V34.72h8.68V53.2h-8.68Zm13.042 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm45.775 0V34.72h8.68V53.2h-8.68Z"></path><path d="M72.883 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M80.885 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M88.888 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M96.89 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M104.893 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M112.895 53.2V34.72h8.68V53.2h-8.68Zm13.043 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm21.768 0V34.72h8.68V53.2h-8.68Z"></path><path d="M152.908 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M160.911 53.2V34.72h8.68V53.2h-8.68Zm10.242 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M176.776 45.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.387 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M208.926 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M216.928 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M224.931 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M232.933 53.2V34.72h8.68V53.2h-8.68Zm10.243 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M248.798 45.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm16.146 10.08V34.72h8.68V53.2h-8.68Z"></path><path d="M272.946 53.2V34.72h8.68V53.2h-8.68Zm13.043 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm29.77 0V34.72h8.68V53.2h-8.68Z"></path><path d="M320.961 53.2V34.72h8.68V53.2h-8.68Zm13.043 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm5.762 0V34.72h8.68V53.2h-8.68Z"></path><path d="M344.969 53.2V34.72h8.68V53.2h-8.68Zm13.042 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm5.763 0V34.72h8.68V53.2h-8.68Z"></path><path d="M368.976 53.2V34.72h8.68V53.2h-8.68Zm10.243 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M384.981 53.2V34.72h8.681V53.2h-8.681Z"></path><path d="M392.984 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M400.987 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M408.989 53.2V34.72h8.68V53.2h-8.68Zm10.243 0V41.72h6.58v1.4h-5.18V53.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M424.994 53.2V34.72h8.68V53.2h-8.68Z"></path><path d="M432.997 53.2V34.72h8.68V53.2h-8.68Zm13.042 0V34.72h1.4V53.2h-1.4Zm-2.8 0V34.72h1.4V53.2h-1.4Zm31.17-4.2v-1.26l4.312-5.292h-4.2V41.3h5.614v1.26l-4.396 5.292h4.55V49h-5.88ZM.86 70.2V51.72h8.68V70.2H.86Z"></path><path d="M8.863 70.2V51.72h8.68V70.2h-8.68Zm13.042 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm45.775 0V51.72h8.68V70.2h-8.68Z"></path><path d="M72.883 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M88.748 60.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M96.75 60.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M104.893 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M112.895 70.2V51.72h8.68V70.2h-8.68Zm13.043 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm13.765 0V51.72h8.68V70.2h-8.68Z"></path><path d="M144.906 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M152.908 70.2V51.72h8.68V70.2h-8.68Zm10.243 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M168.773 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm42.393 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M216.928 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M224.931 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M240.796 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.388 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M272.946 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M280.949 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M293.991 70.2V60.12h-5.18v-1.4h6.58V70.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm13.765 0V51.72h8.68V70.2h-8.68Z"></path><path d="M312.959 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M328.824 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm8.142 10.08V51.72h8.68V70.2h-8.68Z"></path><path d="M344.969 70.2V51.72h8.68V70.2h-8.68Zm13.042 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm5.763 0V51.72h8.68V70.2h-8.68Z"></path><path d="M368.976 70.2V51.72h8.68V70.2h-8.68Zm13.043 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm8.003-7.28v-11.2h1.4v9.8h5.179v1.4h-6.579Zm2.799-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M392.984 70.2V51.72h8.68V70.2h-8.68Z"></path><path d="M400.987 70.2V51.72h8.68V70.2h-8.68Zm10.242 0V58.72h6.58v1.4h-5.18V70.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M416.852 62.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm8.142 10.08V51.72h8.68V70.2h-8.68Z"></path><path d="M432.997 70.2V51.72h8.68V70.2h-8.68Zm13.042 0V51.72h1.4V70.2h-1.4Zm-2.8 0V51.72h1.4V70.2h-1.4Zm15.165-4.2v-1.26l4.312-5.292h-4.2V58.3h5.614v1.26l-4.396 5.292h4.55V66h-5.88ZM.86 87.2V68.72h8.68V87.2H.86Z"></path><path d="M8.863 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M16.865 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M24.868 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M32.87 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M40.873 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M48.875 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M61.918 87.2V77.12h-5.18v-1.4h6.58V87.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm5.762 0V68.72h8.68V87.2h-8.68Z"></path><path d="M72.883 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm21.768 0V68.72h8.68V87.2h-8.68Z"></path><path d="M112.895 87.2V68.72h8.68V87.2h-8.68Zm13.043 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm5.762 0V68.72h8.68V87.2h-8.68Z"></path><path d="M136.903 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M144.906 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M152.908 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M160.911 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M168.913 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M176.916 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M189.958 87.2V77.12h-5.18v-1.4h6.58V87.2h-1.4Zm-2.8 0v-7.28h-2.38v-1.4h3.78v8.68h-1.4Zm29.77 0V68.72h8.68V87.2h-8.68Z"></path><path d="M224.931 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm40.013-7.28v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M280.949 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M288.951 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M296.954 87.2V68.72h8.68V87.2h-8.68Z"></path><path d="M304.956 87.2V68.72h8.68V87.2h-8.68Zm10.243 0V75.72h6.58v1.4h-5.18V87.2h-1.4Zm2.8 0v-8.68h3.78v1.4h-2.38v7.28h-1.4Z"></path><path d="M320.821 79.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm16.145 10.08V68.72h8.68V87.2h-8.68Z"></path><path d="M344.969 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm5.763 0V68.72h8.68V87.2h-8.68Z"></path><path d="M368.976 87.2V68.72h8.68V87.2h-8.68Zm13.043 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4Zm16.005-7.28v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M400.847 77.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M408.849 79.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm16.145 10.08V68.72h8.68V87.2h-8.68Z"></path><path d="M432.997 87.2V68.72h8.68V87.2h-8.68Zm13.042 0V68.72h1.4V87.2h-1.4Zm-2.8 0V68.72h1.4V87.2h-1.4ZM3.1 96.92v-11.2h1.4v9.8h5.18v1.4H3.1Zm2.8-2.8v-8.4h1.4v7h2.38v1.4H5.9Z"></path><path d="M8.723 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M16.725 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M24.728 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M32.73 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M40.733 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M48.735 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M56.738 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.382 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M72.743 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M80.745 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.388 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M112.755 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M120.758 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.383 2.8v-11.2h1.399v9.8h5.18v1.4h-6.579Zm2.799-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M136.763 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M144.766 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M152.768 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M160.771 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M168.773 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M176.776 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M184.778 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm34.39 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M224.791 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M232.793 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm50.396 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M288.811 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M296.814 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M304.816 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M312.819 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm26.387 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M344.829 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M352.831 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm10.383 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M368.836 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M376.839 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Zm50.395 2.8v-11.2h1.4v9.8h5.18v1.4h-6.58Zm2.8-2.8v-8.4h1.4v7h2.38v1.4h-3.78Z"></path><path d="M432.857 94.12v-1.4h8.96v1.4h-8.96Zm0 2.8v-1.4h8.96v1.4h-8.96Z"></path><path d="M440.859 96.92v-1.4h5.18v-9.8h1.4v11.2h-6.58Zm0-2.8v-1.4h2.38v-7h1.4v8.4h-3.78Z"></path></svg><p>LazyVim is a Neovim setup powered by <a href="https://github.com/folke/lazy.nvim" target="_blank" rel="noopener noreferrer">üí§ lazy.nvim</a>
to make it easy to customize and extend your config.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/292349/213447056-92290767-ea16-430c-8727-ce994c93e9cc.png" alt="image"></p><h2 id="-features">‚ú® Features<a href="#-features" aria-label="Direct link to ‚ú® Features" title="Direct link to ‚ú® Features">‚Äã</a></h2><ul><li>üî• Transform your Neovim into a full-fledged IDE</li><li>üí§ Easily customize and extend your config with <a href="https://github.com/folke/lazy.nvim" target="_blank" rel="noopener noreferrer">lazy.nvim</a></li><li>üöÄ Blazingly fast</li><li>üßπ Sane default settings for options, autocmds, and keymaps</li><li>üì¶ Comes with a wealth of plugins pre-configured and ready to use</li></ul><h2 id="Ô∏è-requirements">‚ö°Ô∏è Requirements<a href="#Ô∏è-requirements" aria-label="Direct link to ‚ö°Ô∏è Requirements" title="Direct link to ‚ö°Ô∏è Requirements">‚Äã</a></h2><ul><li>Neovim &gt;= <strong>0.8.0</strong> (needs to be built with <strong>LuaJIT</strong>)</li><li>Git &gt;= <strong>2.19.0</strong> (for partial clones support)</li><li>a <a href="https://www.nerdfonts.com/" target="_blank" rel="noopener noreferrer">Nerd Font</a>(v3.0 or greater) <strong><em>(optional, but needed to display some icons)</em></strong></li><li><a href="https://github.com/jesseduffield/lazygit" target="_blank" rel="noopener noreferrer">lazygit</a> <strong><em>(optional)</em></strong></li><li>a <strong>C</strong> compiler for <code>nvim-treesitter</code>. See <a href="https://github.com/nvim-treesitter/nvim-treesitter#requirements" target="_blank" rel="noopener noreferrer">here</a></li><li>for <a href="https://github.com/nvim-telescope/telescope.nvim" target="_blank" rel="noopener noreferrer">telescope.nvim</a> <strong><em>(optional)</em></strong><ul><li><strong>live grep</strong>: <a href="https://github.com/BurntSushi/ripgrep" target="_blank" rel="noopener noreferrer">ripgrep</a></li><li><strong>find files</strong>: <a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener noreferrer">fd</a></li></ul></li><li>a terminal that support true color and <em>undercurl</em>:<ul><li><a href="https://github.com/kovidgoyal/kitty" target="_blank" rel="noopener noreferrer">kitty</a> <strong><em>(Linux &amp; Macos)</em></strong></li><li><a href="https://github.com/wez/wezterm" target="_blank" rel="noopener noreferrer">wezterm</a> <strong><em>(Linux, Macos &amp; Windows)</em></strong></li><li><a href="https://github.com/alacritty/alacritty" target="_blank" rel="noopener noreferrer">alacritty</a> <strong><em>(Linux, Macos &amp; Windows)</em></strong></li><li><a href="https://iterm2.com/" target="_blank" rel="noopener noreferrer">iterm2</a> <strong><em>(Macos)</em></strong></li></ul></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Johnson and Johnson sues researchers who linked talc to cancer (381 pts)]]></title>
            <link>https://www.reuters.com/legal/litigation/johnson-johnson-sues-researchers-who-linked-talc-cancer-2023-07-13/</link>
            <guid>36753032</guid>
            <pubDate>Mon, 17 Jul 2023 01:14:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/litigation/johnson-johnson-sues-researchers-who-linked-talc-cancer-2023-07-13/">https://www.reuters.com/legal/litigation/johnson-johnson-sues-researchers-who-linked-talc-cancer-2023-07-13/</a>, See on <a href="https://news.ycombinator.com/item?id=36753032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><ul role="tablist"><li data-testid="Text" role="tab" aria-selected="true" tabindex="0">Summary</li><li data-testid="Text" role="tab" aria-selected="false" tabindex="-1">Companies</li><li data-testid="Text" role="tab" aria-selected="false" tabindex="-1">Law Firms</li></ul></div><div><ul><li data-testid="Body">J&amp;J alleges researchers used "junk science" to disparage company's products</li><li data-testid="Body">Defendants say the lawsuits are meant to "silence" scientists</li></ul></div></div><p data-testid="paragraph-0">July 12 (Reuters) - Johnson &amp; Johnson has sued four doctors who published studies citing links between talc-based personal care products and cancer, escalating an attack on scientific studies that the company alleges are inaccurate.</p><p data-testid="paragraph-1">J&amp;J's subsidiary LTL Management, which <a data-testid="Link" href="https://www.reuters.com/business/healthcare-pharmaceuticals/jj-unit-manage-talc-claims-files-bankruptcy-protection-2021-10-14/">absorbed</a> the company's talc liability in a controversial 2021 spinoff, last week filed a <a data-testid="Link" href="https://tmsnrt.rs/46NMFl6" target="_blank">lawsuit</a> in New Jersey federal court asking it to force three researchers to "retract and/or issue a correction" of a study that said asbestos-contaminated consumer talc products sometimes caused patients to develop mesothelioma.</p><p data-testid="paragraph-2">One of the researchers, Richard Kradin, declined to comment. The other two, Theresa Emory and John Maddox, did not respond to requests for comment. Lawyers who have represented the three researchers in similar litigation in the past declined to comment.</p><p data-testid="paragraph-3">J&amp;J is facing more than 38,000 lawsuits alleging that the company's talc products, including its Baby Powder, were contaminated by asbestos and caused cancers including ovarian cancer and mesothelioma. J&amp;J is attempting to resolve those lawsuits, as well as any future talc lawsuits, through an $8.9 billion <a data-testid="Link" href="https://www.reuters.com/legal/jj-unit-goes-bankrupt-second-time-pursue-89-bln-talc-settlement-2023-04-04/">settlement</a> in bankruptcy court.</p><p data-testid="paragraph-4">J&amp;J says that its talc products are safe and do not contain asbestos.</p><p data-testid="paragraph-5">J&amp;J has <a data-testid="Link" href="https://www.reuters.com/business/healthcare-pharmaceuticals/jj-stop-selling-talc-based-baby-powder-globally-2023-2022-08-11/">stopped</a> selling talc-based Baby Powder in favor of cornstarch-based products, citing an increase in lawsuits and "misinformation" about the talc product's safety.</p><p data-testid="paragraph-6">The company in 2021 began <a data-testid="Link" href="https://www.reuters.com/business/healthcare-pharmaceuticals/inside-jjs-secret-plan-cap-litigation-payouts-cancer-victims-2022-02-04/">exploring</a> bankruptcy as a potential solution to the lawsuits, which saw a mixed record at trial, including several defense wins but also a <a data-testid="Link" href="https://www.reuters.com/legal/government/us-supreme-court-declines-hear-jj-appeal-over-2-billion-baby-powder-judgment-2021-06-01/">$2.1 billion</a> verdict awarded to 22 women who blamed their ovarian cancer on asbestos in the company's talc products. J&amp;J said in bankruptcy court filings in April that the costs of its talc-related verdicts, settlements and legal fees have reached about $4.5 billion.</p><p data-testid="paragraph-7">Last week's lawsuit against Emory and Maddox, pathologists affiliated with Peninsula Pathology Associates in Newport News, Virginia, and Kradin, a pulmonologist who worked at Massachusetts General Hospital Cancer Center before his retirement, comes on the heels of another complaint LTL <a data-testid="Link" href="https://tmsnrt.rs/3OekVPq" target="_blank">filed</a> in late May against another doctor, Jacqueline Moline, who works at Northwell Health in Great Neck, New York, on similar grounds.</p><p data-testid="paragraph-8">Moline published an article in 2019 studying 33 patients who said their only exposure to asbestos came from talc products, and Emory, Kradin and Maddox followed up with a 2020 study of 75 similar patients.</p><p data-testid="paragraph-9">All four doctors have provided expert testimony in lawsuits against J&amp;J, and their research has been cited in lawsuits where they have not testified, according to the complaints.</p><p data-testid="paragraph-10">LTL said the researchers concealed the fact that some or all of the patients involved in their studies had been exposed to asbestos from other sources.</p><p data-testid="paragraph-11">The company is also asking the court to force the researchers to disclose the patients' identities.</p><p data-testid="paragraph-12">The lawsuits allege product disparagement and fraud, among other claims.</p><p data-testid="paragraph-13">Adam Zimmerman, a professor at the University of Southern California Gould School of Law, said companies rarely file lawsuits over research they disagree with. It will be very difficult for LTL to prove that the researchers intentionally harmed J&amp;J's reputation, which is required for product disparagement cases in New Jersey, but the company may view the lawsuits as a way to discourage other researchers or reclaim the narrative about talc safety, Zimmerman said.</p><p data-testid="paragraph-14">"When a litigant starts suing opposing experts, that's very aggressive," Zimmerman said. "It sends a message that the gloves are off."</p><p data-testid="paragraph-15">Moline has argued in court papers that LTL's litigation would have a profoundly chilling effect on future medical research if the company were allowed to unmask patients "in the hopes of publicly smearing them." Her court filings say that LTL's lawsuit was meant to "attack and silence" scientists, and that she has an ethical obligation to protect the identities of her research subjects.</p><p data-testid="paragraph-16">LTL's lawsuits allege that the doctors' research allowed them to collect millions of dollars from plaintiffs' lawyers to push a "false narrative" about J&amp;J. The complaint against Moline, for example, said she had made a "small fortune" testifying as a paid expert in lawsuits, receiving over $3 million from her work on asbestos lawsuits. LTL alleged that Kradin also made more than $3 million testifying as a plaintiffs' expert.</p><p data-testid="paragraph-17">The researchers could not immediately be reached for comment.</p><p data-testid="paragraph-18">LTL had filed similar lawsuits against the researchers in December 2022, but those complaints were linked to LTL‚Äôs first bankruptcy filing and were dismissed along with the rest of the bankruptcy in April.</p><p data-testid="paragraph-19">The cases are LTL Management v. Moline and LTL Management v. Emory, U.S. District Court for the District of New Jersey, Nos. 23-cv-02990 and 23-cv-03649.</p><p data-testid="paragraph-20">For LTL: Peter Harvey of Patterson Belknap Webb &amp; Tyler; Allison Brown of Skadden, Arps, Slate, Meagher &amp; Flom; and Kristen Fournier of King &amp; Spalding</p><p data-testid="paragraph-21">For Moline: Kevin Marino and John Tortorella of Marino Tortorella &amp; Boyle</p><p data-testid="paragraph-22">For Emory, Kradin and Maddox: Not yet available
Read more:</p><p data-testid="paragraph-23"><a data-testid="Link" href="https://www.reuters.com/legal/jj-unit-goes-bankrupt-second-time-pursue-89-bln-talc-settlement-2023-04-04/">J&amp;J unit files for second bankruptcy to pursue $8.9 billion talc settlement</a></p><p data-testid="paragraph-24"><a data-testid="Link" href="https://www.reuters.com/legal/cancer-plaintiffs-drill-down-jjs-support-89-bln-talc-deal-2023-06-28/">Cancer plaintiffs drill down on J&amp;J's support for $8.9 billion talc deal</a></p><p data-testid="paragraph-25"><a data-testid="Link" href="https://www.reuters.com/legal/jjs-ltl-units-bankruptcy-dismissed-by-us-appeals-court-filing-2023-01-30/">U.S. court rejects J&amp;J bankruptcy strategy for thousands of talc lawsuits</a></p><p><span data-testid="Text">Reporting by Dietrich Knauth; additional reporting by Brendan Pierson</span></p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using XPath in 2023 (118 pts)]]></title>
            <link>https://denizaksimsek.com/2023/xpath/</link>
            <guid>36752419</guid>
            <pubDate>Sun, 16 Jul 2023 23:24:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://denizaksimsek.com/2023/xpath/">https://denizaksimsek.com/2023/xpath/</a>, See on <a href="https://news.ycombinator.com/item?id=36752419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>In the latest release of <a href="https://htmx.org/">htmx</a>,
you can add event listeners to elements with <code>hx-on</code>:</p>
<pre tabindex="0"><code></code></pre>
<p>For all the other <code>hx-</code> attributes, we use CSS attribute selectors.
However, with <code>hx-on</code>, the attribute name is not fixed as it contains the event.
CSS attribute selectors support wildcards on the <em>value</em> of attributes,
but not the name:</p>
<pre tabindex="0"><code>[hx-trigger] <span>/* common and normal */</span>
[href^=<span>"https://dz4k.com"</span>] <span>/* "starts with" operator */</span>
[^<span>ƒ•x-on</span><span>:</span>] <span>/* not a thing */</span>
</code></pre>
<h2><ruby>X<rt>XML</rt>Path<rt>Path Language</rt></ruby></h2>
<p>XPath is a query language for extracting information from XML(-like) documents.
Its main use cases are XSLT and parsing API responses.</p>
<p>The XPath language is significantly more expressive than CSS,
making it possible to traverse the XML tree in any direction,
filter nodes based on arbitrary predicates,
and select any kind of node
(including comments, text nodes, and individual attributes).
Our non-existent CSS attribute could be written as follows:</p>
<pre tabindex="0"><code>//@*[starts-with(name(), "hx-on:")]
</code></pre>
<p>This post is not supposed to be an XPath tutorial, but I‚Äôll break this one down:</p>
<dl>
<dt><code>//</code></dt>
<dd>traverse the document (in CSS, this is the default)</dd>
<dt><code>@*</code></dt>
<dd>find any attribute (mnemonic: <strong>at</strong>-tribute)</dd>
<dt><code>[ ... ]</code></dt>
<dd>where‚Ä¶</dd>
<dt><code>starts-with(name(), "hx-on:")</code></dt>
<dd>its name starts with <code>"hx-on:"</code></dd>
</dl>
<p>CSS selectors don‚Äôt have these kinds of features,
and it has good reasons not to.
CSS has strict performance requirements
‚Äì to the point that ‚ÄúCSS optimization‚Äù is generally not a thing ‚Äì
and selectors that offer more control could make slow selectors possible.
In addition, CSS has well-defined specificity rules, whereas XPath does not.</p>
<p>However, while these features make CSS great for stylesheets,
CSS selectors are also the most common way to find DOM elements in
JavaScript code and lacking in that regard.
Many libraries which extend HTML do so by traversing the entire document
and finding elements manually.
This is often not needed since, if you didn‚Äôt know,
<strong>XPath is built into browsers.</strong></p>
<h2>document.evaluate</h2>
<p>The <a href="https://developer.mozilla.org/en-US/Web/XPath/Introduction_to_using_XPath_in_JavaScript"><code>document.evaluate</code> API</a> is somewhat archaic,
partly because it was designed for talking to XML APIs over <code>XMLHTTPRequest</code>.
Here‚Äôs a DOM-friendly wrapper:</p>
<pre tabindex="0"><code>function* xpath(...args) {
  let path, root = document;
  if (args.length &gt; 1) [root, path] = args;
  else [path] = args;

  const nodeIterator = document.evaluate(
    path,
    root,
    null,
    XPathResult.UNORDERED_NODE_ITERATOR_TYPE,
    null,
  );

  for (
    let node = nodeIterator.iterateNext();
    node != null;
    node = nodeIterator.iterateNext()
  ) {
    yield node;
  }
}

// TypeScript declaration
function xpath(path: string): Iterable;
function xpath(root: Element, path: string): Iterable;
</code></pre>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Red Programming Language (202 pts)]]></title>
            <link>https://www.red-lang.org/p/about.html</link>
            <guid>36752146</guid>
            <pubDate>Sun, 16 Jul 2023 22:41:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.red-lang.org/p/about.html">https://www.red-lang.org/p/about.html</a>, See on <a href="https://news.ycombinator.com/item?id=36752146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1150177033025383393" itemprop="description articleBody">
<p><b>Red </b>is a next-gen programming language,&nbsp;strongly inspired by&nbsp;<a href="http://rebol.com/">REBOL</a>. Main features are:</p><ul>
<li><b>Human-friendly</b> <a href="https://pointillistic.com/ren/" target="_blank">syntax</a></li>
<li><b><a href="http://en.wikipedia.org/wiki/Homoiconicity">Homoiconic</a></b>&nbsp;(Red is its own meta-language and own <b><a href="http://www.rebol.com/rebolsteps.html" target="_blank">data-format</a></b>)</li>
<li><b>Functional</b>, imperative, <b><a href="http://www.red-lang.org/2016/06/061-reactive-programming.html" target="_blank">reactive</a></b> and <b>symbolic </b>programming</li>
<li><b>Prototype</b>-based object support</li>
<li><b>Multi-</b>typing</li>
<li><b>Powerful pattern-matching&nbsp;</b><a href="http://www.red-lang.org/2016/12/entering-world-of-macros.html" target="_blank">Macros</a> system</li>
<li><b>Rich</b> set of built-in datatypes (50+)</li>
<li>Both <b>statically </b>and <b>JIT</b>-<b>compiled(*)&nbsp;</b>to native code</li>
<li><b>Cross-compilation</b> <a href="https://github.com/red/red/blob/master/usage.txt" target="_blank">done</a> <a href="https://github.com/red/red/blob/master/system/config.r" target="_blank">right</a></li>
<li>Produces executables of <b>less than 1MB</b>, with <b>no dependencies</b></li>
<li><b>Concurrency </b>and <b>parallelism </b>strong support (actors, parallel collections)(*)</li>
<li>Low-level <b>system programming</b> abilities through the built-in Red/System <a href="http://en.wikipedia.org/wiki/Domain-specific_language">DSL</a></li>
<li>Powerful <b>PEG <a href="http://www.red-lang.org/2013/11/041-introducing-parse.html" target="_blank">parser </a></b><a href="http://www.red-lang.org/2013/11/041-introducing-parse.html" target="_blank">DSL</a>&nbsp;built-in</li><li>Fast, compacting<b> Garbage Collector</b></li>
<li><b>Cross-platform native <a href="http://www.red-lang.org/2016/03/060-red-gui-system.html" target="_blank">GUI </a></b><a href="http://www.red-lang.org/2016/03/060-red-gui-system.html" target="_blank">system</a>, with a <a href="http://doc.red-lang.org/gui/VID.html" target="_blank">UI layout DSL</a> and <a href="http://doc.red-lang.org/gui/Draw.html" target="_blank">drawing DSL</a></li>
<li><b>Bridging</b> <a href="https://github.com/red/red/blob/master/bridges/java/hello.red" target="_blank">to the JVM</a></li>
<li>High-level <b>scripting </b>and <a href="http://en.wikipedia.org/wiki/Read-eval-print_loop"><b>REPL</b></a>&nbsp;GUI and CLI consoles included</li>
<li>Visual Studio Code<b>&nbsp;<a href="https://marketplace.visualstudio.com/items?itemName=red-auto.red" target="_blank">plugin</a></b>, with many helpful features</li>
<li>Highly <b><a href="http://www.red-lang.org/2017/03/062-libred-and-macros.html" target="_blank">embeddable</a></b></li>
<li><b>Low </b>memory footprint</li>
<li><b><a href="http://www.red-lang.org/p/download.html" target="_blank">Single-file</a></b>&nbsp;(~1MB)&nbsp;contains whole toolchain, full standard library and REPL&nbsp;(**)</li>
<li><b>No install, no setup</b></li>
<li><b>Fun</b>&nbsp;guaranteed!</li>
</ul>
<div>
<p>(*) Not implemented yet.</p><p>(**) Temporarily split in two binaries</p><p>
Red‚Äôs ambitious goal is to build the world‚Äôs first <b>full-stack language</b>, a language you can use from system programming tasks, up to high-level scripting through DSL. You've probably heard of the term "<a href="http://www.laurencegellert.com/2012/08/what-is-a-full-stack-developer/" target="_blank">Full-Stack Developer</a>". But what is a full-stack Language, exactly?</p><p>

Other languages talk about having "one tool to rule them all". Red has that mindset too, pushed to the limit - it's a single executable that takes in your source files on any platform, and produces a packaged binary for any platform, from any other. The tool doesn‚Äôt depend on anything besides what came with your OS...shipping as a single executable that about a megabyte.</p><p>

But that technical feat alone isn't enough to define Red's notion of a "Full-Stack Language". It's about the ability to bend and redefine the system to meet any need, while still working with literate code, and getting top-flight performance. &nbsp;So what's being put in your hands is more like a "language construction set" than simply "a language". Whether you‚Äôre writing a device driver, a platform-native GUI application, or a shared library... Red lets you use a common syntax to code at the right level of abstraction for the task.</p><p><img src="https://3.bp.blogspot.com/-xhOP35Dm99w/UuXFKgY2dlI/AAAAAAAAAGA/YQu98_pPDjw/s1600/reichart-abstraction-diagram.png">
</p>
</div><p>
It was announced and presented for the first time&nbsp;at <a href="http://reborcon.esperconsultancy.nl/">ReBorCon 2011</a> conference (March, 2011).&nbsp;A more recent presentation video was given at the Recode conference in Montreal (July, 2013):</p><p>
<iframe allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/-KqNO_sDqm4" width="560"></iframe>
</p>
<p>
But if you are unable to visit YouTube, here are some slide decks explaining the reasons for building it, showing the main features and the roadmap.</p><p>
Recode 2013 presentation slides:&nbsp;<a href="http://static.red-lang.org/Recode2013-Red.pdf" target="_new">PDF version</a>.
</p><p>
And for historical purposes, here are some older presentations:</p><ul><li><a href="http://static.red-lang.org/Red-SFD2011-45mn.pdf" target="_blank">SFD 2011 conference slides</a></li><li><a href="http://static.red-lang.org/red-rebor2011.pdf" target="_blank">ReBor 2011 conference slides</a></li></ul>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dusk OS: 32-bit Forth OS. Useful during first stage of civilizational collapse (167 pts)]]></title>
            <link>https://duskos.org/</link>
            <guid>36751422</guid>
            <pubDate>Sun, 16 Jul 2023 21:09:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duskos.org/">https://duskos.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36751422">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>Dusk OS is a 32-bit Forth and big brother to <a href="http://collapseos.org/">Collapse OS</a>. Its
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/purpose.txt">primary purpose</a> is to be maximally useful during the <a href="http://collapseos.org/why.html">first stage of
civilizational collapse</a>, that is, when we can't produce modern
computers anymore but that there's still many modern computers still around.</p>
<p>It does so by aggressively prioritizing <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/simple.txt">simplicity</a> at the cost of
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/limits.txt">unorthodox constraints</a>, while also aiming to make
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/shell.txt">operators happy</a>.</p>
<p>Dusk OS innovates by having an <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/cc/index.txt">"almost C" compiler</a> allowing it to
piggy-back on UNIX C code, through a modest <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/design/port.txt">porting effort</a>, to reach
its goals and stay true to its design constraints with a minimal effort.</p>
<p><a href="https://vimeo.com/800710912">Video showcasing Dusk OS</a></p>
<h2>Why build this OS?</h2>
<p>Most modern operating systems can do whatever we want them to do. Why do we
need another one? Simplicity.</p>
<p>It's difficult to predict post-collapse conditions, but we can suppose that
many <a href="#operator">operators</a> will need to use their machines in novel and
creative ways. Hackability of the operating system then becomes paramount. Open
source modern operating systems all can be modified to fit its user's needs,
but their complexity limits the likelihood that the user is able to do so. A
simpler OS increases this likelihood.</p>
<p>But we can't have our cake and eat it too, right? Either you have a simple toy
OS or a complex one. Well, maybe not?</p>
<p>Its authors believe that in the history of computing, Forth has been
under-explored. Its approach to simplicity is, we think, revolutionary. It
has significant shortcomings when system specifications become more complex
(Forth hates complexity and doesn't manage it well), but we believe it
possible to elegantly marry it with languages that like complexity better.</p>
<p>This mix, we believe, could provide the operator with computing powers rarely
seen with other approaches. We've got to try it.</p>
<p>To be clear: this is a research project, we don't know what it will yield
beforehand. We have the intuition that it might lead to a big "ah ah!" moment
and reveal a breathtaking combination of power and simplicity.</p>
<h2>Features making Dusk OS special</h2>
<h3>A whole OS built from source on boot</h3>
<p>One thing that makes Dusk OS special is that it boots from a very tiny core
(1000 lines of i386 assembly). From this tiny core, on boot, it builds its way
up to a system that has a functional C compiler, which then allows it to
bootstrap itself some more.</p>
<p>With regards to "source bootstrapping", it's even more extreme than Collapse OS
because modern machines allows this process to run quickly and the whole
process is still faster than a regular Linux boot. On Collapse OS target
machines, this process would be prohibitive, so a bigger part of the OS is
cross-compiled into the kernel.</p>
<p>This peculiarity of Dusk OS has interesting properties. The nicest one, in my
humble opinion, is that this allows us to sidestep the <em>entire</em> problems of
binary compatibility and relocation and only deal with source compatibility.
So, no ELF, no binutils, only code that is designed to run from where it was
generated in the first place. This is so much simpler!</p>
<p>Object files? Global symbols? Nah. C functions that don't have a static storage
type are simple Forth words.</p>
<h3>Harmonized Assembly Layer</h3>
<p>Dusk features what we call the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/hal.txt">Harmonized Assembly Layer</a> (HAL for short).
This is a cross-CPU assembler, on which the C compiler relies, which prioritizes
implementation and usage simplicity, but is also designed to generate efficient
native code.</p>
<h3>Shortest path to self-hosting for an "almost C" compiler</h3>
<p>Dusk OS self-hosts in about 1000 lines of assembly and a few hundred lines of
Forth (the exact number depends on the target machine). From there, it
bootstraps to DuskCC, which is roughly 1500 lines of Forth code. To my
knowledge, Dusk OS is unique in that regard.</p>
<p>You can pick any C compiler that requires POSIX and it will automatically
require order of magnitudes more lines of code to bootstrap because you need
that POSIX system in addition to the C compiler. So even if you pick a small C
compiler such as tcc, you still need a POSIX system to build it, which is
usually in the millions of LOCs. </p>
<p>To be fair, Dusk OS is not the first project thinking of optimizing that path.
<a href="https://github.com/fosslinux/live-bootstrap">Efforts at making our modern software world bootstrappable</a>
lead to an "almost C", <a href="https://git.sr.ht/~oriansj/M2-Planet">M2-Planet</a> with a feature set comparable to
DuskCC with very few lines of code. M2-Planet itself is about 5K lines of code
and the various stages that lead to it are generally a few hundred lines each.
The project initially ran on top of regular kernels (as in "fat kernels with
lots of code"), but some bare metal stages (<a href="https://github.com/ironmeld/builder-hex0">1</a>,
<a href="https://git.stikonas.eu/andrius/stage0-uefi">2</a>) were created and now this little chain end up being
comparable to Dusk in terms of lines of code. Still more than Dusk, but in the
same ballpark.</p>
<p>Although this path is short and technically leads you to an "almost C"
compiler, you can hardly use it because it has no "real kernel" (those bare
metal stages mentioned above are enough to compile M2-Planet, but really not
much else, they're extremely limited) and no shell. You'll need those if you
want to use your shiny compiler.</p>
<p>One of your best picks, should you try this path, would be <a href="https://www.fiwix.org/">Fiwix</a>, a
minimal POSIX i386 kernel weighting less than 50K lines of C+asm. But then,
M2-Planet is not enough. You need to compile tcc (which M2-Planet can compile
after having applied a few patches) which weights 80K. Userspace is worse.
Bash+coreutils are 400K, even busybox is 190K. We still end up with a pretty
minimal and simple system, but it's still a lot more code than Dusk.</p>
<p>So, unless someone tells me about some option I don't know about, DuskCC is
quite innovative on the aspect of self-hosting path length.</p>

<h2>Who is Dusk for?</h2>
<p>Dusk OS doesn't have users, but <em>operators</em>. What's the difference? Control.
You <em>use</em> a phone, you <em>use</em> a coffee machine, hell you even <em>use</em> a car these
days. But you <em>operate</em> a bulldozer, you <em>operate</em> a crane, you <em>operate</em> a
plane.</p>
<p>You <em>use</em> Linux, you <em>use</em> Windows. You <em>operate</em> Dusk OS.</p>
<p>Can you <em>operate</em> Linux? Sure, if you're some kind of god<sup id="fnref:1"><a href="#fn:1">1</a></sup>, in the same way
that you can <em>operate</em> a Tesla if you're a top Tesla engineer. But you're much
more likely to be able to <em>operate</em> a landmower than a Tesla.</p>
<p>The Dusk operator is someone who's <a href="http://collapseos.org/why.html#creative">creative</a>, close to hardware, can
read a datasheet. Dusk shines when one wants to poke around the hardware
without limit.</p>
<p>It compares favorably to other more complete OSes because there's no concurrent
process to mess with your poking and the driver structure is more approachable,
hackable due to its stricter scope and savvier target audience.</p>
<p>Let's use an example. Let's say you're on a notebook that runs on a chipset of
Intel's ICHn family. You read the datasheet and see "oh, nice, there's an SPI
interface in there. Maybe that it's not hooked to anything on the notebook,
let's play with it."</p>
<p>Now, that chipset is very, very central to the computer. There are good chances,
on a BSD or Linux system, that if you begin poking around its registers, you'll
step on someone else toes and crash the system because, for example, of some
other process that needed to read from disk at the same time.</p>
<p>In Dusk, you could completely break the SATA controller, you'll still be golden
as long as you don't access mass storage. Because Dusk doesn't have
concurrency, you have tight control over what happen or doesn't happen on the
machine, so all you need to do is to avoid words that access mass storage. That
gives you ample wiggling space for your hacking session.</p>
<p>To be clear: this is also possible with a custom made BSD or Linux, but you're
going to have to strip a lot of pieces from your distro before you get there
and some of those pieces might be useful debugging tools which will be
difficult to retrofit because they need a wider system. You'll also need a
higher cognitive space to fit BSD/Linux wider abstractions in your mind.</p>
<h2>Status</h2>
<ul>
<li>Has a VM written in C, buildable from a POSIX environment, which allows Dusk
  to build itself for any of its supported targets.</li>
<li>Has an <a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/cc/index.txt">"almost C" compiler</a> which still needs some work, but is
  already capable of compiling a nice subset of C.</li>
<li>Can run on i386 and ARM. <a href="https://git.sr.ht/~vdupras/duskos/tree/master/HARDWARE.md">Detailed list of supported hardware.</a></li>
<li>Can read, write and boot from FAT12/FAT16 (no FAT32 for now) volumes.</li>
<li>Can create new FAT12/FAT16 volumes.</li>
<li>Very small footprint. In Grid mode (TUI mode) with the Grid text editor
  and DuskCC (including its stdlib) loaded, Dusk uses 180KB of RAM on a PC.</li>
<li>It completely self-hosts on all its target machines. (Well, only PC at this
  point because the ARM port is still a WIP, but it <em>will</em> self-host)</li>
<li>Simple and terse. The core system (all kernels, drivers, filesystems, CC,
  core libraries) is less than 9K lines of code.</li>
<li>Since <code>text/ed</code> has reached a usable status, the main author of Dusk has been
  developing it from within itself on an old Pentium 75 Mhz with 16mb of RAM and
  he's having a blast.</li>
</ul>
<p>List of ported codebases:</p>
<ul>
<li>The <a href="https://wiki.xxiivv.com/site/uxn.html">uxn</a> VM</li>
<li>The <code>puff()</code> algorithm from <a href="https://github.com/madler/zlib">zlib</a></li>
<li>The <a href="https://git.sr.ht/~rabbits/left">left</a> text editor. (Still a bit glitchy...)</li>
</ul>
<p>List of homegrown applications:</p>
<ul>
<li>C Compiler (see <code>doc/cc</code>)</li>
<li>Text editor (see <code>doc/text/ed</code>)</li>
</ul>
<h2>Funding</h2>
<p>You are inspired by Dusk OS and would like to fund its development? I don't do
any kind of crowdfunding, but if you happen to be a rich philanthropist, <a href="https://duskos.org/funding.html">maybe
we can do something</a>. </p>
<h2>Resources</h2>
<ul>
<li><a href="https://sr.ht/~vdupras/duskos">Source code repository</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/fs/doc/index.txt">Documentation</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/ROADMAP.md">Roadmap</a></li>
<li><a href="https://duskos.org/discuss.html">Mailing list</a></li>
<li><a href="https://tumbleforth.hardcoded.net/">Tumble Forth, a Forth vulgarization blog</a></li>
<li><a href="https://alexw.nyc/tech/duskos-1.html">Alex's Dusk OS tutorial</a></li>
</ul>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Undocumented 8086 instructions, explained by the microcode (224 pts)]]></title>
            <link>https://www.righto.com/2023/07/undocumented-8086-instructions.html</link>
            <guid>36751399</guid>
            <pubDate>Sun, 16 Jul 2023 21:06:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2023/07/undocumented-8086-instructions.html">https://www.righto.com/2023/07/undocumented-8086-instructions.html</a>, See on <a href="https://news.ycombinator.com/item?id=36751399">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5516205124640022120" itemprop="description articleBody">



<p>What happens if you give the Intel 8086 processor an instruction that doesn't exist?
A modern microprocessor (80186 and later) will generate an exception, indicating that an illegal instruction
was executed.
However, early microprocessors didn't include the circuitry to detect illegal instructions, since the chips didn't have
transistors to spare. Instead these processors would do <em>something</em>,
but the results weren't specified.<span id="fnref:6502"><a href="#fn:6502">1</a></span></p>
<p>The 8086 has a number of undocumented instructions.
Most of them are simply duplicates of regular instructions, but a few have unexpected behavior, such as revealing the
values of internal, hidden registers.
In the 8086, most instructions are implemented in microcode, so examining the 8086's microcode can explain why these instructions
behave the way they do.</p>
<p>The photo below shows the 8086 die under a microscope, with the important functional blocks labeled. The metal layer is visible, while the underlying silicon and polysilicon wiring is mostly hidden.
The microcode ROM and the microcode address decoder are in the lower right.
The Group Decode ROM (upper center) is also important, as it performs the first step of instruction decoding.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/die-labeled.jpg"><img alt="The 8086 die under a microscope, with main functional blocks labeled. Click on this image (or any other) for a larger version." height="589" src="https://static.righto.com/images/8086-ad-undoc/die-labeled-w600.jpg" title="The 8086 die under a microscope, with main functional blocks labeled. Click on this image (or any other) for a larger version." width="600"></a></p><p>The 8086 die under a microscope, with main functional blocks labeled. Click on this image (or any other) for a larger version.</p>
<h2>Microcode and 8086 instruction decoding</h2>
<p>You might think that machine instructions are the basic steps that a computer performs.
However, instructions usually require multiple steps inside the processor.
One way of expressing these multiple steps is through microcode, a technique dating back to 1951.
To execute a machine instruction, the computer internally executes several simpler micro-instructions, specified by the microcode.
In other words, microcode forms nother layer between the machine instructions and the hardware.
The main advantage of microcode is that it turns the processor's control logic into a programming task instead of a difficult logic design task.</p>
<p>The 8068's <a href="https://www.righto.com/2022/11/how-8086-processors-microcode-engine.html">microcode ROM</a> holds 512 micro-instructions, each 21 bits wide.
Each micro-instruction performs two actions in parallel. First is a move between a source and a destination, typically registers.
Second is an operation that can range from an arithmetic (ALU) operation to a memory access.
The diagram below shows the structure of a 21-bit micro-instruction, divided into six types.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/microcode-format.jpg"><img alt="The encoding of a micro-instruction into 21 bits. Based on NEC v. Intel: Will Hardware Be Drawn into the Black Hole of Copyright?" height="203" src="https://static.righto.com/images/8086-ad-undoc/microcode-format-w700.jpg" title="The encoding of a micro-instruction into 21 bits. Based on NEC v. Intel: Will Hardware Be Drawn into the Black Hole of Copyright?" width="700"></a></p>
<p>When executing a machine instruction, the 8086 performs a decoding step.
Although the 8086 is a 16-bit processor, its instructions are based on bytes. In most cases, the first byte specifies the
opcode, which may be followed by additional instruction bytes.
In other cases, the byte is a "prefix" byte, which changes the behavior of the following instruction.
The first byte is analyzed
by something called the <a href="https://www.righto.com/2023/05/8086-processor-group-decode-rom.html">Group Decode ROM</a>.
This circuit categorizes the first byte of the instruction into about <code>35</code> categories that control how the instruction is
decoded and executed.
One category is "1-byte logic"; this indicates a one-byte instruction or prefix that is simple and implemented by logic circuitry in the 8086.
For instructions in this category, microcode is not involved while
the remaining instructions are implemented in microcode.
Many of these instructions are in the "two-byte ROM" category indicating that the instruction has a second byte
that also needs to be decoded by microcode.
This second byte, called the ModR/M byte, specifies that memory addressing mode or registers that the instruction uses.</p>
<p>The next step is the microcode's address decoder circuit, which determines where to start executing microcode based on
the opcode.
Conceptually, you can think of the microcode as stored in a ROM, indexed by the instruction opcode and a few sequence bits.
However, since many instructions can use the same microcode, it would be inefficient to store duplicate copies of these routines.
Instead, the microcode address decoder permits multiple instructions to reference the same entries in the ROM.
This decoding circuitry is similar to a PLA (Programmable Logic Array) so it matches bit patterns to determine a particular starting point.
This turns out to be important for undocumented instructions since undocumented instructions often match the pattern for a "real" instruction, making the undocumented instruction an alias.</p>
<p>The 8086 has several internal registers that are invisible to the programmer but are used by the microcode.
Memory accesses use the Indirect (<code>IND</code>) and Operand (<code>OPR</code>) registers; the <code>IND</code> register holds the address in the segment,
while the <code>OPR</code> register holds the data value that is read or written.
Although these registers are normally not accessible by the programmer, some undocumented instructions provide access to these registers, as will be described later.</p>
<p>The Arithmetic/Logic Unit (ALU) performs arithmetic, logical, and shift operations in the 8086.
The ALU uses three internal registers: <code>tmpA</code>, <code>tmpB</code>, and <code>tmpC</code>. An ALU operation requires two micro-instructions.
The first micro-instruction specifies the operation (such as <code>ADD</code>) and the temporary register that holds one argument (e.g. <code>tmpA</code>);
the second argument is always in <code>tmpB</code>.
A following micro-instruction can access the ALU result through the pseudo-register <code>Œ£</code> (sigma).</p>
<h3>The ModR/M byte</h3>
<p>A fundamental part of the 8086 instruction format is the ModR/M byte, a byte that specifies addressing for many instructions.
The 8086 has a variety of addressing modes, so the ModR/M byte is somewhat complicated.
Normally it specifies one memory address and one register. The memory address is specified through one of eight addressing
modes (below) along with an optional 8- or 16-bit displacement in the instruction.
Instead of a memory address, the ModR/M byte can also specify a second register.
For a few opcodes, the ModR/M byte selects what instruction to execute rather than a register.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/modrm.png"><img alt="The 8086's addressing modes. From The register assignments, from MCS-86 Assembly Language Reference Guide." height="220" src="https://static.righto.com/images/8086-ad-undoc/modrm-w250.png" title="The 8086's addressing modes. From The register assignments, from MCS-86 Assembly Language Reference Guide." width="250"></a></p>
<p>The implementation of the ModR/M byte plays an important role in the behavior of undocumented instructions.
Support for this byte is implemented in both microcode and hardware.
The various memory address modes above are implemented by microcode subroutines, which compute the appropriate memory address and
perform a read if necessary.
The subroutine leaves the memory address in the <code>IND</code> register, and if a read is performed, the value is in the <code>OPR</code> register.</p>
<p>The hardware hides the ModR/M byte's selection of memory versus register, by making the value available through the pseudo-register <code>M</code>, while the second register is available through <code>N</code>.
Thus, the microcode for an instruction doesn't need to know if the value was in memory or a register, or which register was selected.
The Group Decode ROM examines the first byte of the instruction to determine if a ModR/M byte is present, and if a read
is required.
If the ModR/M byte specifies memory, the Translation ROM determines which micro-subroutines to call before handling the
instruction itself.
For more on the ModR/M byte, see my post on <a href="https://www.righto.com/2023/02/8086-modrm-addressing.html">Reverse-engineering the ModR/M addressing microcode</a>.</p>
<h2>Holes in the opcode table</h2>
<p>The first byte of the instruction is a value from <code>00</code> to <code>FF</code> in hex.
Almost all of these opcode values correspond to documented 8086 instructions, but there are a few exceptions, "holes" in the opcode table.
The table below shows the 256 first-byte opcodes for the 8086, from hex <code>00</code> to <code>FF</code>. Valid opcodes for the 8086 are in white;
the colored opcodes are undefined and interesting to examine.
Orange, yellow, and green opcodes were given meaning in the 80186, 80286, and 80386 respectively.
The purple opcode is unusual: it was implemented in the 8086 and later processors but not documented.<span id="fnref:prefixes"><a href="#fn:prefixes">2</a></span>
In this section, I'll examine the microcode for these opcode holes.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/opcodes.png"><img alt="This table shows the 256 opcodes for the 8086, where the white ones are valid instructions. Click for a larger version." height="453" src="https://static.righto.com/images/8086-ad-undoc/opcodes-w450.png" title="This table shows the 256 opcodes for the 8086, where the white ones are valid instructions. Click for a larger version." width="450"></a></p><p>This table shows the 256 opcodes for the 8086, where the white ones are valid instructions. Click for a larger version.</p>
<h3><code>D6</code>: <code>SALC</code></h3>
<p>The opcode <code>D6</code> (purple above) performs a well-known but undocumented operation that is typically called <code>SALC</code>, for Set AL to Carry.
This instruction sets the <code>AL</code> register to 0 if the carry flag is 0, and sets the <code>AL</code> register to <code>FF</code> if the carry flag is 1.
The curious thing about this undocumented instruction is that it exists in all x86 CPUs, but Intel didn't mention it until 2017.
Intel probably put this instruction into the processor deliberately as a <a href="https://en.wikipedia.org/wiki/Fictitious_entry#Copyright_traps">copyright trap</a>.
The idea is that if a company created a copy of the 8086 processor and the processor included the <code>SALC</code> instruction, this
would prove that the company had copied Intel's microcode and thus had potentially violated Intel's copyright on the microcode.
This came to light when NEC created improved versions of the 8086, the NEC V20 and V30 microprocessors, and was sued by Intel.
Intel analyzed NEC's microcode but was disappointed to find that NEC's chip did not include the hidden instruction, showing
that NEC hadn't copied the microcode.<span id="fnref:magic-instruction"><a href="#fn:magic-instruction">3</a></span>
Although a Federal judge <a href="https://www.nytimes.com/1989/02/08/business/intel-loses-copyright-case-to-nec.html">ruled</a> in 1989 that NEC hadn't infringed
Intel's copyright, the 5-year trial ruined NEC's market momentum.</p>
<p>The <code>SALC</code> instruction is implemented with three micro-instructions, shown below.<span id="fnref:microcode"><a href="#fn:microcode">4</a></span>
The first micro-instruction jumps if the carry (<code>CY</code>) is set.
If not, the next instruction moves 0 to the AL register. <code>RNI</code> (Run Next Instruction) ends the microcode execution
causing the next machine instruction to run.
If the carry was set, all-ones (i.e. <code>FF</code> hex) is moved to the <code>AL</code> register and RNI ends the microcode sequence.</p>
<pre>           JMPS CY 2 <span><b>SALC</b>: jump on carry</span>
ZERO ‚Üí AL  RNI       <span>Move 0 to AL, run next instruction</span>
ONES ‚Üí AL  RNI       <span><b>2:</b>Move FF to AL, run next instruction</span>
</pre>

<h3><code>0F</code>: <code>POP CS</code></h3>
<p>The <code>0F</code> opcode is the first hole in the opcode table.
The 8086 has instructions to push and pop the four segment registers, except opcode <code>0F</code> is undefined where <code>POP CS</code> should be.
This opcode performs <code>POP CS</code> successfully, so the question is why is it undefined?
The reason is that <code>POP CS</code> is essentially useless and doesn't do what you'd expect, so Intel figured it was best not
to document it.</p>
<p>To understand why <code>POP CS</code> is useless, I need to step back and explain the 8086's segment registers.
The 8086 has a <code>20</code>-bit address space, but 16-bit registers.
To make this work, the 8086 has the concept of segments: memory is accessed in 64K chunks called segments, which are positioned
in the 1-megabyte address space.
Specifically, there are four segments: Code Segment, Stack Segment, Data Segment, and Extra Segment,
with four segment registers that define the start of the segment: <code>CS</code>, <code>SS</code>, <code>DS</code>, and <code>ES</code>.</p>
<p>An inconvenient part of segment addressing  is that if you want to access more than 64K, you need to change the segment register.
So you might push the data segment register, change it temporarily so you can access a new part of memory, and then pop the old data segment
register value off the stack.
This would use the <code>PUSH DS</code> and <code>POP DS</code> instructions.
But why not <code>POP CS</code>?</p>
<p>The 8086 executes code from the code segment, with the instruction pointer (<code>IP</code>) tracking the location in the code segment.
The main problem with <code>POP CS</code> is that it changes the code segment, but not the instruction pointer, so now you are executing
code at the old offset in a new segment.
Unless you line up your code extremely carefully, the result is that you're jumping to an unexpected place in memory.
(Normally, you want to change <code>CS</code> and the instruction pointer at the same time, using a <code>CALL</code> or <code>JMP</code> instruction.)</p>
<p>The second problem with <code>POP CS</code> is prefetching.
For efficiency, the 8086 prefetches instructions before they are needed, storing them in an 8-byte prefetch queue.
When you perform a jump, for instance, the microcode flushes the prefetch queue so execution will continue with the
new instructions, rather than the old instructions.
However, the instructions that pop a segment register don't flush the prefetch buffer.
Thus, <code>POP CS</code> not only jumps to an unexpected location in memory, but it will execute an unpredictable number of instructions
from the old code path.</p>
<p>The <code>POP segment register</code> microcode below packs a lot into three micro-instructions.
The first micro-instruction pops a value from the stack.
Specifically, it moves the stack pointer (<code>SP</code>) to the Indirect (<code>IND</code>) register.
The Indirect register is an internal register, invisible to the programmer, that holds the address offset for memory
accesses.
The first micro-instruction also performs a memory read (<code>R</code>) from the stack segment (<code>SS</code>) and then increments <code>IND</code>
by 2 (<code>P2</code>, plus 2).
The second micro-instruction moves <code>IND</code> to the stack pointer, updating the stack pointer with the new value.
It also tells the microcode engine that this micro-instruction is the next-to-last (<code>NXT</code>) and the next machine instruction
can be started.
The final micro-instruction moves the value read from memory to the appropriate segment register and runs the next instruction.
Specifically, reads and writes put data in the internal <code>OPR</code> (Operand) register.
The hardware uses the register <code>N</code> to indicate the register specified by the instruction.
That is, the value will be stored in the <code>CS</code>, <code>DS</code>, <code>ES</code>, or <code>SS</code> register, depending on the bit pattern in the instruction.
Thus, the same microcode works for all four segment registers.
This is why <code>POP CS</code> works even though <code>POP CS</code> wasn't explicitly implemented in the microcode; it uses the common code.</p>
<pre>SP ‚Üí IND  R SS,P2 <span><b>POP sr</b>: read from stack, compute IND plus 2</span>
IND ‚Üí SP  NXT     <span>Put updated value in SP, start next instruction.</span>
OPR ‚Üí N   RNI     <span>Put stack value in specified segment register</span>
</pre>

<p>But why does <code>POP CS</code> run this microcode in the first place?
The microcode to execute is selected based on the instruction, but multiple instructions can execute the same microcode.
You can think of the address decoder as pattern-matching on the instruction's bit patterns, where some of the bits can be ignored.
In this case, the <code>POP sr</code> microcode above is run by any instruction with the bit pattern 000??111, where a question mark
can be either a 0 or a 1.
You can verify that this pattern matches <code>POP ES</code> (<code>07</code>), <code>POP SS</code> (<code>17</code>), and <code>POP DS</code> (<code>1F</code>).
However, it also matches <code>0F</code>, which is why the <code>0F</code> opcode runs the above microcode and performs <code>POP CS</code>.
In other words, to make <code>0F</code> do something other than <code>POP CS</code> would require additional circuitry, so it was easier to
leave the action implemented but undocumented.</p>
<h3><code>60</code>-<code>6F</code>: conditional jumps</h3>
<p>One whole row of the opcode table is unused: values <code>60</code> to <code>6F</code>.
These opcodes simply act the same as <code>70</code> to <code>7F</code>, the conditional jump instructions.</p>
<p>The conditional jumps use the following microcode.
It fetches the jump offset from the instruction prefetch queue (<code>Q</code>) and puts the value into the ALU's <code>tmpBL</code> register,
the low byte of the <code>tmpB</code> register.
It tests the condition in the instruction (<code>XC</code>) and jumps to the <code>RELJMP</code> micro-subroutine if satisfied.
The <code>RELJMP</code> code (not shown) updates the program counter to perform the jump.</p>
<pre>Q ‚Üí tmpBL                <span><b>Jcond cb:</b> Get offset from prefetch queue</span>
           JMP XC RELJMP <span>Test condition, if true jump to RELJMP routine</span>
           RNI           <span>No jump: run next instruction</span>
</pre>

<p>This code is executed for any instruction matching the bit pattern <code>011?????</code>, i.e. anything from <code>60</code> to <code>7F</code>.
The condition is specified by the four low bits of the instruction.
The result is that any instruction <code>60</code>-<code>6F</code> is an alias for the corresponding conditional jump <code>70</code>-<code>7F</code>.</p>
<h3><code>C0</code>, <code>C8</code>: <code>RET/RETF imm</code></h3>
<p>These undocumented opcodes act like a return instruction, specifically <code>RET imm16</code> (<a href="https://www.os2museum.com/wp/undocumented-8086-opcodes-part-i/">source</a>).
Specifically, the instruction <code>C0</code> is the same as <code>C2</code>, near return, while <code>C8</code> is the same as <code>CA</code>, far return.</p>
<p>The microcode below is executed for the instruction bits <code>1100?0?0</code>, so it is executed for <code>C0</code>, <code>C2</code>, <code>C8</code>, and <code>CA</code>.
It gets two bytes from the instruction prefetch queue (<code>Q</code>) and puts them in the <code>AX</code> register.
Next, it calls <code>FARRET</code>, which performs either a near return (popping <code>PC</code> from the stack) or a far return (popping <code>PC</code> and <code>CS</code>
from the stack). Finally, it adds the original argument to the <code>SP</code>, equivalent to popping that many bytes.</p>
<pre>Q ‚Üí tmpAL    ADD tmpA    <span><b>RET/RETF iw:</b> Get word from prefetch, set up ADD</span>
Q ‚Üí tmpAH    CALL FARRET <span>Call Far Return micro-subroutine</span>
IND ‚Üí tmpB               <span>Move SP (in IND) to tmpB for ADD</span>
Œ£ ‚Üí SP       RNI         <span>Put sum in Stack Pointer, end</span>
</pre>

<p>One tricky part is that the <code>FARRET</code> micro-subroutine examines bit 3 of the instruction to determine whether it does a near
return or a far return.
This is why documented instruction <code>C2</code> is a near return and <code>CA</code> is a far return.
Since <code>C0</code> and <code>C8</code> run the same microcode, they will perform the same actions, a near return and a far return respectively.</p>
<h3><code>C1</code>: <code>RET</code></h3>
<p>The undocumented <code>C1</code> opcode is identical to the documented <code>C3</code>, near return instruction.
The microcode below is executed for instruction bits <code>110000?1</code>, i.e. <code>C1</code> and <code>C3</code>.
The first micro-instruction reads from the Stack Pointer, incrementing <code>IND</code> by 2.
Prefetching is suspended and the prefetch queue is flushed, since execution will continue at a new location.
The Program Counter is updated with the value from the stack, read into the <code>OPR</code> register.
Finally, the updated address is put in the Stack Pointer and execution ends.</p>
<pre>SP ‚Üí IND  R SS,P2  <span><b>RET: </b> Read from stack, increment by 2</span>
          SUSP     <span>Suspend prefetching</span>
OPR ‚Üí PC  FLUSH    <span>Update PC from stack, flush prefetch queue</span>
IND ‚Üí SP  RNI      <span>Update SP, run next instruction</span>
</pre>

<h3><code>C9</code>: <code>RET</code></h3>
<p>The undocumented <code>C9</code> opcode is identical to the documented <code>CB</code>, far return instruction.
This microcode is executed for instruction bits <code>110010?1</code>, i.e. <code>C9</code> and <code>CB</code>, so <code>C9</code> is identical to <code>CB</code>.
The microcode below simply calls the <code>FARRET</code> micro-subroutine to pop the Program Counter and CS register.
Then the new value is stored into the Stack Pointer.
One subtlety is that <code>FARRET</code> looks at bit 3 of the instruction to switch between a near return and a far return, as
described earlier.
Since <code>C9</code> and <code>CB</code> both have bit 3 set, they both perform a far return.</p>
<pre>          CALL FARRET  <span><b>RETF:</b> call FARRET routine</span>
IND ‚Üí SP  RNI          <span>Update stack pointer, run next instruction</span>
</pre>

<h3><code>F1</code>: <code>LOCK</code> prefix</h3>
<p>The final hole in the opcode table is <code>F1</code>.
This opcode is different because it is implemented in logic rather than microcode.
The Group Decode ROM indicates that <code>F1</code> is a prefix, one-byte logic, and LOCK.
The Group Decode outputs are the same as <code>F0</code>, so <code>F1</code> also acts as a <code>LOCK</code> prefix.</p>
<h2>Holes in two-byte opcodes</h2>
<p>For most of the 8086 instructions, the first byte specifies the instruction.
However, the 8086 has a few instructions where the second byte specifies the instruction: the <code>reg</code> field of the ModR/M byte provides an opcode extension that selects the instruction.<span id="fnref:extension"><a href="#fn:extension">5</a></span>
These fall into four categories which Intel labeled "Immed", "Shift", "Group 1", and "Group 2", corresponding to opcodes <code>80</code>-<code>83</code>, <code>D0</code>-<code>D3</code>,
<code>F6</code>-<code>F7</code>, and <code>FE</code>-<code>FF</code>.
The table below shows how the second byte selects the instruction.
Note that "Shift", "Group 1", and "Group 2" all have gaps, resulting in undocumented values.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/groups.jpg"><img alt="Meaning of the reg field in two-byte opcodes. From MCS-86 Assembly Language Reference Guide." height="129" src="https://static.righto.com/images/8086-ad-undoc/groups-w600.jpg" title="Meaning of the reg field in two-byte opcodes. From MCS-86 Assembly Language Reference Guide." width="600"></a></p>
<p>These sets of instructions are implemented in two completely different ways.
The "Immed" and "Shift" instructions run microcode in the standard way, selected by the first byte.
For a typical arithmetic/logic instruction such as <code>ADD</code>, bits 5-3 of the first instruction byte are latched into the <code>X</code> register to indicate
which ALU operation to perform.
The microcode specifies a generic ALU operation, while the <code>X</code> register controls whether the operation is an <code>ADD</code>, <code>SUB</code>, <code>XOR</code>, or
so forth.
However, the Group Decode ROM indicates that for the special "Immed" and "Shift" instructions, the <code>X</code> register latches the bits
from the <em>second</em> byte.
Thus, when the microcode executes a generic ALU operation, it ends up with the one specified in the second byte.<span id="fnref:alu"><a href="#fn:alu">6</a></span></p>
<p>The "Group 1" and "Group 2" instructions (<code>F0</code>-<code>F1</code>, <code>FE</code>-<code>FF</code>), however, run different microcode for each instruction.
Bits 5-3 of the second byte replace bits 2-0 of the instruction before executing the microcode.
Thus, <code>F0</code> and <code>F1</code> act as if they are opcodes in the range <code>F0</code>-<code>F7</code>, while <code>FE</code> and <code>FF</code> act as if they are opcodes in the range <code>F8</code>-<code>FF</code>.
Thus, each instruction specified by the second byte can have its own microcode, unlike the "Immed" and "Shift" instructions.
The trick that makes this work is that all the "real" opcodes in the range <code>F0</code>-<code>FF</code> are implemented in logic, not microcode,
so there are no collisions.</p>
<h3>The hole in "Shift": <code>SETMO</code>, <code>D0</code>..<code>D3/6</code></h3>
<p>There is a "hole" in the list of shift operations when the second byte has the bits <code>110</code> (6).
(This is typically expressed as <code>D0/6</code> and so forth; the value after the slash is the opcode-selection bits in the ModR/M byte.)
Internally, this value selects the ALU's <code>SETMO</code> (Set Minus One) operation, which simply returns <code>FF</code> or <code>FFFF</code>, for a byte or word operation respectively.<span id="fnref:setmo"><a href="#fn:setmo">7</a></span></p>
<p>The microcode below is executed for 1101000? bit patterns patterns (D0 and D1).
The first instruction gets the value from the <code>M</code> register and sets up the ALU to do whatever operation was
specified in the instruction (indicated by <code>XI</code>).
Thus, the same microcode is used for all the "Shift" instructions, including <code>SETMO</code>.
The result is written back to <code>M</code>. If no writeback to memory is required (<code>NWB</code>), then <code>RNI</code> runs the next instruction, ending
the microcode sequence.
However, if the result is going to memory, then the last line writes the value to memory.</p>
<pre>M ‚Üí tmpB  XI tmpB, NXT  <span><b>rot rm, 1</b>: get argument, set up ALU</span>
Œ£ ‚Üí M     NWB,RNI F     <span>Store result, maybe run next instruction</span>
          W DS,P0 RNI   <span>Write result to memory</span>
</pre>

<p>The D2 and D3 instructions (1101001?) perform a variable number of shifts, specified by the <code>CL</code> register, so they use different microcode (below).
This microcode loops the number of times specified by <code>CL</code>, but the control flow is a bit tricky to avoid shifting if
the intial counter value is 0.
The code sets up the ALU to pass the counter (in <code>tmpA</code>) unmodified the first time (<code>PASS</code>) and jumps to <b>4</b>, which
updates the counter and sets up the ALU for the shift operation (<code>XI</code>).
If the counter is not zero, it jumps back to <b>3</b>, which performs the previously-specified shift and sets up
the ALU to decrement the counter (<code>DEC</code>).
This time, the code at <b>4</b> decrements the counter.
The loop continues until the counter reaches zero. The microcode stores the result as in the previous microcode.</p>
<pre>ZERO ‚Üí tmpA               <span><b>rot rm,CL</b>: 0 to tmpA</span>
CX ‚Üí tmpAL   PASS tmpA    <span>Get count to tmpAL, set up ALU to pass through</span>
M ‚Üí tmpB     JMPS 4       <span>Get value, jump to loop (4)</span>
Œ£ ‚Üí tmpB     DEC tmpA F   <span><b>3</b>: Update result, set up decrement of count</span>
Œ£ ‚Üí tmpA     XI tmpB      <span><b>4</b>: update count in tmpA, set up ALU</span>
             JMPS NZ 3    <span>Loop if count not zero</span>
tmpB ‚Üí M     NWB,RNI      <span>Store result, maybe run next instruction</span>
             W DS,P0 RNI  <span>Write result to memory</span>
</pre>

<h3>The hole in "group 1": <code>TEST</code>, <code>F6/1</code> and <code>F7/1</code></h3>
<p>The <code>F6</code> and <code>F7</code> opcodes are in "group 1", with the specific instruction specified by bits 5-3 of the second byte.
The second-byte table showed a hole for the <code>001</code> bit sequence.
As explained earlier, these bits replace the low-order bits of the instruction, so <code>F6</code> with 001 is processed as if it were
the opcode <code>F1</code>.
The microcode below matches against instruction bits <code>1111000?</code>, so <code>F6/1</code> and <code>F7/1</code> have the same effect as <code>F6/0</code> and <code>F7/1</code> respectively,
that is, the byte and word <code>TEST</code> instructions.</p>
<p>The microcode below gets one or two bytes from the prefetch queue (<code>Q</code>); the <code>L8</code> condition tests if the operation is
an 8-bit (i.e. byte) operation and skips the second micro-instruction.
The third micro-instruction ANDs the argument and the fetched value.
The condition flags (<code>F</code>) are set based on the result, but the result itself is discarded.
Thus, the <code>TEST</code> instruction tests a value against a mask, seeing if any bits are set.</p>
<pre>Q ‚Üí tmpBL    JMPS L8 2     <span><b>TEST rm,i:</b> Get byte, jump if operation length = 8</span>
Q ‚Üí tmpBH                  <span>Get second byte from the prefetch queue</span>
M ‚Üí tmpA     AND tmpA, NXT <span><b>2:</b> Get argument, AND with fetched value</span>
Œ£ ‚Üí no dest  RNI F         <span>Discard result but set flags.</span>
</pre>

<p>I explained the processing of these "Group 3" instructions in more detail in my <a href="https://www.righto.com/2022/11/how-8086-processors-microcode-engine.html">microcode article</a>.</p>
<h3>The hole in "group 2": <code>PUSH</code>, <code>FE/7</code> and <code>FF/7</code></h3>
<p>The <code>FE</code> and <code>FF</code> opcodes are in "group 2", which has a hole for the <code>111</code> bit sequence in the second byte.
After replacement, this will be processed as the <code>FF</code> opcode, which matches the pattern <code>1111111?</code>.
In other words, the instruction will be processed the same as the <code>110</code> bit pattern, which is <code>PUSH</code>.
The microcode gets the Stack Pointer, sets up the ALU to decrement it by 2.
The new value is written to <code>SP</code> and <code>IND</code>. Finally, the register value is written to stack memory.</p>
<!--
For some reason, the [8086 undocumented instructions](https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_x86_instructions) page on Wikipedia doesn't list FE-FF.
-->

<pre>SP ‚Üí tmpA  DEC2 tmpA   <span><b>PUSH rm</b>: set up decrement SP by 2</span>
Œ£ ‚Üí IND                <span>Decremented SP to IND</span>
Œ£ ‚Üí SP                 <span>Decremented SP to SP</span>
M ‚Üí OPR    W SS,P0 RNI <span>Write the value to memory, done</span>
</pre>

<h3><code>82</code> and <code>83</code> "Immed" group</h3>
<p>Opcodes <code>80</code>-<code>83</code> are the "Immed" group, performing one of eight arithmetic operations, specified in the ModR/M byte.
The four opcodes differ in the size of the values: opcode <code>80</code> applies an 8-bit immediate value to an 8-bit register, <code>81</code> applies a 16-bit
value to a 16-bit register, <code>82</code> applies an 8-bit value to an 8-bit register, and <code>83</code> applies an 8-bit value to a 16-bit register.
The opcode 82 has the strange situation that <a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_instructions_that_are_widely_available_across_many_x86_CPUs_include">some sources</a> say it is undocumented, but it shows up in some Intel documentation as a valid bit combination (e.g. below).
Note that <code>80</code> and <code>82</code> have the  8-bit to 8-bit action, so the <code>82</code> opcode is redundant.</p>
<p><a href="https://static.righto.com/images/8086-ad-undoc/adc.png"><img alt="ADC is one of the instructions with opcode 80-83. From the 8086 datasheet, page 27." height="34" src="https://static.righto.com/images/8086-ad-undoc/adc-w600.png" title="ADC is one of the instructions with opcode 80-83. From the 8086 datasheet, page 27." width="600"></a></p><p>ADC is one of the instructions with opcode 80-83. From the <a href="https://www.electro-tech-online.com/datasheets/8086_intel.pdf">8086 datasheet</a>, page 27.</p>
<p>The microcode below is used for all four opcodes.
If the ModR/M byte specifies memory, the appropriate micro-subroutine is called to compute the effective address in <code>IND</code>,
and fetch the byte or word into <code>OPR</code>.
The first two instructions below get the two immediate data bytes from the prefetch queue; for an 8-bit operation, the second byte
is skipped.
Next, the second argument <code>M</code> is loaded into tmpA and the desired ALU operation (<code>XI</code>) is configured.
The result <code>Œ£</code> is stored into the specified register <code>M</code> and the operation may terminate with <code>RNI</code>.
But if the ModR/M byte specified memory, the following write micro-operation saves the value to memory.</p>
<pre>Q ‚Üí tmpBL  JMPS L8 2    <span><b>alu rm,i</b>: get byte, test if 8-bit op</span>
Q ‚Üí tmpBH               <span>Maybe get second byte</span>
M ‚Üí tmpA   XI tmpA, NXT <span><b>2</b>: </span>
Œ£ ‚Üí M      NWB,RNI F    <span>Save result, update flags, done if no memory writeback</span>
           W DS,P0 RNI  <span>Write result to memory if needed</span>
</pre>

<p>The tricky part of this is the <code>L8</code> condition, which tests if the operation is 8-bit.
You might think that bit 0 acts as the byte/word bit in a nice, orthogonal way, but the 8086 has a bunch of special cases.
Bit 0 of the instruction typically selects between a byte and a word operation, but there are a bunch of special cases.
The Group Decode ROM creates a signal indicating if bit 0 should be used as the byte/word bit.
But it generates a second signal indicating that an instruction should be forced to operate on bytes, for instructions
such as <code>DAA</code> and <code>XLAT</code>.
Another Group Decode ROM signal indicates that bit 3 of the instruction should select byte or word; this
is used for the <code>MOV</code> instructions with opcodes Bx.
Yet another Group Decode ROM signal indicates that inverted bit 1 of the instruction should select byte or word;
this is used for a few opcodes, including <code>80</code>-<code>87</code>.</p>
<p>The important thing here is that for the opcodes under discussion (<code>80</code>-<code>83</code>), the <code>L8</code> micro-condition uses <em>both</em> bits 0 and 1
to determine if the instruction is 8 bits or not.
The result is that only opcode <code>81</code> is considered 16-bit by the <code>L8</code> test, so it is the only one that uses two immediate bytes
from the instruction.
However, the register operations use only bit 0 to select a byte or word transfer.
The result is that opcode <code>83</code> has the unusual behavior of using an 8-bit immediate operand with a 16-bit register.
In this case, the 8-bit value is sign-extended to form a 16-bit value. That is, the top bit of the 8-bit value fills
the entire upper half of the 16-bit value,
converting an 8-bit signed value to a 16-bit signed value (e.g. -1 is <code>FF</code>, which becomes <code>FFFF</code>).
This makes sense for arithmetic operations, but not much sense for logical operations.</p>
<p>Intel documentation is inconsistent about which opcodes are listed for which instructions.
Intel opcode maps generally define opcodes <code>80</code>-<code>83</code>.
However, lists of specific instructions show opcodes <code>80</code>, <code>81</code>, and <code>83</code> for arithmetic operations but only <code>80</code> and <code>81</code> for logical operations.<span id="fnref:immed"><a href="#fn:immed">8</a></span>
That is, Intel omits the redundant <code>82</code> opcode as well as omitting logic operations that perform sign-extension (<code>83</code>).</p>
<h3>More <code>FE</code> holes</h3>
<p>For the "group 2" instructions, the <code>FE</code> opcode performs a byte operation while <code>FF</code> performs a word operation.
Many of these operations don't make sense for bytes: <code>CALL</code>, <code>JMP</code>, and <code>PUSH</code>.
(The only instructions supported for <code>FE</code> are <code>INC</code> and <code>DEC</code>.) But what happens if you use the unsupported instructions?
The remainder of this section examines those cases and shows that the results are not useful.</p>
<h4><code>CALL</code>: <code>FE/2</code></h4>
<p>This instruction performs an indirect subroutine call within a segment, reading the target address from the memory location specified by the ModR/M byte.</p>
<p>The microcode below is a bit convoluted because the code falls through into the shared <code>NEARCALL</code> routine, so there is
some unnecessary register movement.
Before this microcode executes, the appropriate ModR/M micro-subroutine will read the target address from memory.
The code below copies the destination address from <code>M</code> to <code>tmpB</code> and stores it into the PC later in the code
to transfer execution.
The code suspends prefetching, corrects the PC to cancel the offset from prefetching, and flushes the prefetch queue.
Finally, it decrements the SP by two and writes the old PC to the stack.</p>
<pre>M ‚Üí tmpB    SUSP        <span><b>CALL rm</b>: read value, suspend prefetch</span>
SP ‚Üí IND    CORR        <span>Get SP, correct PC</span>
PC ‚Üí OPR    DEC2 tmpC   <span>Get PC to write, set up decrement</span>
tmpB ‚Üí PC   FLUSH       <span><b>NEARCALL</b>: Update PC, flush prefetch</span>
IND ‚Üí tmpC              <span>Get SP to decrement</span>
Œ£ ‚Üí IND                 <span>Decremented SP to IND</span>
Œ£ ‚Üí SP      W SS,P0 RNI <span>Update SP, write old PC to stack</span>
</pre>

<p>This code will mess up in two ways when executed as a byte instruction.
First, when the destination address is read from memory, only a byte will be read, so the destination address will be corrupted.
(I think that the behavior here depends on the bus hardware. The 8086 will ask for a byte from memory but will
read the word that is placed on the bus.
Thus, if memory returns a word, this part may operate correctly.
The 8088's behavior will be different because of its 8-bit bus.)
The second issue is writing the old PC to the stack because only a byte of the PC will be written.
Thus, when the code returns from the subroutine call, the return address will be corrupt.</p>
<h4><code>CALL</code>: <code>FE/3</code></h4>
<p>This instruction performs an indirect subroutine call between segments, reading the target address from the memory location specified by the ModR/M byte.</p>
<pre>IND ‚Üí tmpC  INC2 tmpC    <span><b>CALL FAR rm</b>: set up IND+2</span>
Œ£ ‚Üí IND     R DS,P0      <span>Read new CS, update IND</span>
OPR ‚Üí tmpA  DEC2 tmpC    <span>New CS to tmpA, set up SP-2</span>
SP ‚Üí tmpC   SUSP         <span><b>FARCALL</b>: Suspend prefetch</span>
Œ£ ‚Üí IND     CORR         <span><b>FARCALL2</b>: Update IND, correct PC</span>
CS ‚Üí OPR    W SS,M2      <span>Push old CS, decrement IND by 2</span>
tmpA ‚Üí CS   PASS tmpC    <span>Update CS, set up for NEARCALL</span>
PC ‚Üí OPR    JMP NEARCALL <span>Continue with NEARCALL</span>
</pre>

<p>As in the previous <code>CALL</code>, this microcode will fail in multiple ways when executed in byte mode.
The new CS and PC addresses will be read from memory as bytes, which may or may not work.
Only a byte of the old CS and PC will be pushed to the stack.</p>
<h4><code>JMP</code>: <code>FE/4</code></h4>
<p>This instruction performs an indirect jump within a segment, reading the target address from the memory location specified by the ModR/M byte.
The microcode is short, since the ModR/M micro-subroutine does most of the work.
I believe this will have the same problem as the previous <code>CALL</code> instructions, that it will attempt to read a byte from
memory instead of a word.</p>
<pre>        SUSP       <span><b>JMP rm</b>: Suspend prefetch</span>
M ‚Üí PC  FLUSH RNI  <span>Update PC with new address, flush prefetch, done</span>
</pre>

<h4><code>JMP</code>: <code>FE/5</code></h4>
<p>This instruction performs an indirect jump between segments, reading the new PC and CS values from the memory location specified by the ModR/M byte.
The ModR/M micro-subroutine reads the new PC address. This microcode increments <code>IND</code> and suspends prefetching.
It updates the PC, reads the new CS value from memory, and updates the CS.
As before, the reads from memory will read bytes instead of words, so this code will not meaningfully work in byte mode.</p>
<pre>IND ‚Üí tmpC  INC2 tmpC   <span><b>JMP FAR rm</b>: set up IND+2</span>
Œ£ ‚Üí IND     SUSP        <span>Update IND, suspend prefetch</span>
tmpB ‚Üí PC   R DS,P0     <span>Update PC, read new CS from memory</span>
OPR ‚Üí CS    FLUSH RNI   <span>Update CS, flush prefetch, done</span>
</pre>

<h4><code>PUSH</code>: <code>FE/6</code></h4>
<p>This instruction pushes the register or memory value specified by the ModR/M byte.
It decrements the SP by 2 and then writes the value to the stack.
It will write one byte to the stack but decrements the SP by 2,
so one byte of old stack data will be on the stack along with the data byte.</p>
<pre>SP ‚Üí tmpA  DEC2 tmpA    <span><b>PUSH rm</b>: Set up SP decrement </span>
Œ£ ‚Üí IND                 <span>Decremented value to IND</span>
Œ£ ‚Üí SP                  <span>Decremented value to SP</span>
M ‚Üí OPR    W SS,P0 RNI  <span>Write the data to the stack</span>
</pre>

<h2>Undocumented instruction values</h2>
<p>The next category of undocumented instructions is where the first byte indicates a valid instruction, but
there is something wrong with the second byte.</p>
<h3><code>AAM</code>: ASCII Adjust after Multiply</h3>
<p>The <code>AAM</code> instruction is a fairly obscure one, designed to support binary-coded decimal
arithmetic (BCD).
After multiplying two BCD digits, you end up with a binary value between 0 and <code>81</code> (0√ó0 to 9√ó9).
If you want a BCD result, the <code>AAM</code> instruction converts this binary value to BCD, for instance splitting <code>81</code> into the
decimal digits 8 and 1, where the upper digit is <code>81</code> divided by <code>10</code>, and the lower digit is <code>81</code> modulo <code>10</code>.</p>
<p>The interesting thing about <code>AAM</code> is that the 2-byte instruction is <code>D4</code> <code>0A</code>. You might notice that hex <code>0A</code> is <code>10</code>, and this
is not a coincidence.
There wasn't an easy way to get the value <code>10</code> in the microcode, so instead they made the instruction
provide that value in the second byte.
The undocumented (but well-known) part is that if you provide a value other than <code>10</code>, the instruction will convert the binary input into
digits in that base. For example, if you provide 8 as the second byte, the instruction returns the value divided by 8
and the value modulo 8.</p>
<p>The microcode for <code>AAM</code>, below, sets up the registers.  calls
the <code>CORD</code> (Core Division) micro-subroutine to perform the division,
and then puts the results into <code>AH</code> and <code>AL</code>.
In more detail, the <code>CORD</code> routine divides <code>tmpA/tmpC</code> by <code>tmpB</code>, putting the <em>complement</em> of the quotient in <code>tmpC</code> and leaving the remainder in <code>tmpA</code>.
(If you want to know how CORD works internally, see my <a href="https://www.righto.com/2023/04/reverse-engineering-8086-divide-microcode.html">division post</a>.)
The important step is that the <code>AAM</code> microcode gets the divisor from the prefetch queue (<code>Q</code>).
After calling <code>CORD</code>, it sets up the ALU to perform a 1's complement of <code>tmpC</code> and puts the result (<code>Œ£</code>) into <code>AH</code>.
It sets up the ALU to pass <code>tmpA</code> through unchanged, puts the result (<code>Œ£</code>) into <code>AL</code>, and updates the flags accordingly (<code>F</code>).</p>
<pre>Q ‚Üí tmpB                    <span><b>AAM:</b> Move byte from prefetch to tmpB</span>
ZERO ‚Üí tmpA                 <span>Move 0 to tmpA</span>
AL ‚Üí tmpC    CALL CORD      <span>Move AL to tmpC, call CORD.</span>
             COM1 tmpC      <span>Set ALU to complement</span>
Œ£ ‚Üí AH       PASS tmpA, NXT <span>Complement AL to AH</span>
Œ£ ‚Üí AL       RNI F          <span>Pass tmpA through ALU to set flags</span>
</pre>

<p>The interesting thing is why this code has undocumented behavior.
The 8086's microcode only has support for the constants 0 and all-1's (<code>FF</code> or <code>FFFF</code>), but the microcode needs to divide by <code>10</code>.
One solution would be to implement an additional micro-instruction and more circuitry to provide the constant <code>10</code>, but every
transistor was precious back then.
Instead, the designers took the approach of simply putting the number <code>10</code> as the second byte of the instruction and loading the
constant from there.
Since the <code>AAM</code> instruction is not used very much, making the instruction two bytes long wasn't much of a drawback.
But if you put a different number in the second byte, that's the divisor the microcode will use.
(Of course you could add circuitry to verify that the number is <code>10</code>, but then the implementation is no longer simple.)</p>
<p>Intel could have documented the full behavior, but that creates several problems.
First, Intel would be stuck supporting the full behavior into the future.
Second, there are corner cases to deal with, such as divide-by-zero.
Third, testing the chip would become harder because all these cases would need to be tested.
Fourth, the documentation would become long and confusing.
It's not surprising that Intel left the full behavior undocumented.</p>
<h3><code>AAD</code>: ASCII Adjust before Division</h3>
<p>The <code>AAD</code> instruction is analogous to <code>AAM</code> but used for BCD division.
In this case, you want to divide a two-digit BCD number by something, where the BCD digits are in <code>AH</code> and <code>AL</code>.
The <code>AAD</code> instruction converts the two-digit BCD number to binary by computing <code>AH</code>√ó<code>10+AL</code>, before you perform
the division.</p>
<p>The microcode for <code>AAD</code> is shown below. The microcode sets up the registers, calls the multiplication micro-subroutine
<code>CORX</code> (Core Times), and
then puts the results in <code>AH</code> and <code>AL</code>.
In more detail, the multiplier comes from the instruction prefetch queue <code>Q</code>.
The <code>CORX</code> routine multiples <code>tmpC</code> by <code>tmpB</code>, putting the result in <code>tmpA/tmpC</code>.
Then the microcode adds the low BCD digit (<code>AL</code>) to the product (<code>tmpB + tmpC</code>), putting the sum (<code>Œ£</code>) into <code>AL</code>,
clearing <code>AH</code> and setting the status flags <code>F</code> appropriately.</p>
<p>One interesting thing is that the second-last micro-instruction jumps to <code>AAEND</code>, which is the last
micro-instruction of the <code>AAM</code> microcode above.
By reusing the micro-instruction from <code>AAM</code>, the microcode is one micro-instruction shorter, but
the jump adds one cycle to the execution time.
(The CORX routine is used for integer multiplication; I discuss the internals in <a href="https://www.righto.com/2023/03/8086-multiplication-microcode.html">this post</a>.)</p>
<pre>Q ‚Üí tmpC              <span><b>AAD:</b> Get byte from prefetch queue.</span>
AH ‚Üí tmpB   CALL CORX <span>Call CORX</span>
AL ‚Üí tmpB   ADD tmpC  <span>Set ALU for ADD</span>
ZERO ‚Üí AH   JMP AAEND <span>Zero AH, jump to AAEND</span>
i
...
Œ£ ‚Üí AL      RNI F     <span><b>AAEND:</b> Sum to AL, done.</span>
</pre>

<p>As with <code>AAM</code>, the constant <code>10</code> is provided in the second byte of the instruction.
The microcode accepts any value here, but values other than <code>10</code> are undocumented.</p>
<h3><code>8C</code>, <code>8E</code>: MOV sr</h3>
<p>The opcodes <code>8C</code> and <code>8E</code> perform a <code>MOV</code> register to or from the specified segment register, using the register specification
field in the ModR/M byte.
There are four segment registers and three selection bits, so an invalid segment register can be specified.
However, the hardware that decodes the register number ignores instruction bit 5 for a segment register. Thus,
specifying a segment register 4 to 7 is the same as specifying a segment register 0 to 3.
For more details, see my article on <a href="https://www.righto.com/2023/03/8086-register-codes.html">8086 register codes</a>.</p>
<h2>Unexpected <code>REP</code> prefix</h2>
<h3><code>REP IMUL</code> / <code>IDIV</code></h3>
<p>The <code>REP</code> prefix is used with string operations to cause the operation to be repeated across a block of memory.
However, if you use this prefix with an <code>IMUL</code> or <code>IDIV</code> instruction, it has the unexpected behavior
of negating the product or the quotient (<a href="https://www.reenigne.org/blog/8086-microcode-disassembled/">source</a>).</p>
<p>The reason for this behavior is that the string operations use an internal flag called <code>F1</code> to indicate that a <code>REP</code>
prefix has been applied.
The multiply and divide code reuses this flag to track the sign of the input values, toggling <code>F1</code> for each negative value.
If <code>F1</code> is set, the value at the end is negated. (This handles "two negatives make a positive.")
The consequence is that the <code>REP</code> prefix puts the flag in the 1 state when the multiply/divide starts, so the computed sign
will be wrong at the end and the result is the negative of the expected result.
The microcode is fairly complex, so I won't show it here; I explain it in detail in <a href="https://www.righto.com/2023/03/8086-multiplication-microcode.html">this blog post</a>.</p>
<h3><code>REP RET</code></h3>
<p><a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_x86_instructions">Wikipedia</a> lists
<code>REP RET</code> (i.e. <code>RET</code> with a <code>REP</code> prefix) as a way to implement a two-byte return instruction.
This is kind of trivial; the <code>RET</code> microcode (like almost every instruction) doesn't use the <code>F1</code> internal flag,
so the <code>REP</code> prefix has no effect.</p>
<h3><code>REPNZ MOVS/STOS</code></h3>
<p><a href="https://en.wikipedia.org/wiki/X86_instruction_listings#Undocumented_x86_instructions">Wikipedia</a> mentions that
the use of the <code>REPNZ</code> prefix (as opposed to <code>REPZ</code>) is undefined with string operations other than <code>CMPS/SCAS</code>.
An internal flag called <code>F1Z</code> distinguishes between the <code>REPZ</code> and <code>REPNZ</code> prefixes.
This flag is only used by <code>CMPS/SCAS</code>. Since the other string instructions ignore this flag, they will ignore the
difference between <code>REPZ</code> and <code>REPNZ</code>.
I wrote about string operations in more detail in <a href="https://www.righto.com/2023/04/8086-microcode-string-operations.html">this post</a>.</p>
<h2>Using a register instead of memory.</h2>
<p>Some instructions are documented as requiring a memory operand. However, the ModR/M byte can specify a register.
The behavior in these cases can be highly unusual, providing access to hidden registers.
Examining the microcode shows how this happens.</p>
<h3><code>LEA reg</code></h3>
<p>Many instructions have a ModR/M byte that indicates the memory address that the instruction should use, perhaps through
a complicated addressing mode.
The <code>LEA</code> (Load Effective Address) instruction is different: it doesn't access the memory location but returns the address itself.
The undocumented part is that the ModR/M byte can specify a register instead of a memory location. In that case,
what does the <code>LEA</code> instruction do? Obviously it can't return the address of a register, but it needs to return something.</p>
<p>The behavior of <code>LEA</code> is explained by how the 8086 handles the ModR/M byte.
Before running the microcode corresponding to the instruction, the microcode engine calls a short micro-subroutine
for the particular addressing mode.
This micro-subroutine puts the desired memory address (the effective address) into the <code>tmpA</code> register.
The effective address is copied to the <code>IND</code> (Indirect) register and the value is loaded from memory if needed.
On the other hand, if the ModR/M byte specified a register instead of memory, no micro-subroutine is called.
(I explain ModR/M handling in more detail in <a href="https://www.righto.com/2023/02/8086-modrm-addressing.html">this article</a>.)</p>
<p>The microcode for <code>LEA</code> itself is just one line. It stores the effective address in the <code>IND</code> register into the specified destination register, indicated by <code>N</code>.
This assumes that the appropriate ModR/M micro-subroutine was called before this code, putting the effective address into <code>IND</code>.</p>
<pre>IND ‚Üí N   RNI  <span><b>LEA</b>: store IND register in destination, done</span>
</pre>

<p>But if a register was specified instead of a memory location, no ModR/M micro-subroutine gets called.
Instead, the <code>LEA</code> instruction will return whatever value was left
in <code>IND</code> from before, typically the previous memory location that was accessed.
Thus, <code>LEA</code> can be used to read the value of the <code>IND</code> register, which is normally hidden from the programmer.</p>
<h3><code>LDS reg</code>, <code>LES reg</code></h3>
<p>The <code>LDS</code> and <code>LES</code> instructions load a far pointer from memory into the specified segment register and general-purpose register.
The microcode below assumes that the appropriate ModR/M micro-subroutine has set up <code>IND</code> and read the first value into <code>OPR</code>.
The microcode updates the destination register, increments <code>IND</code> by 2, reads the next value, and updates <code>DS</code>.
(The microcode for <code>LES</code> is a copy of this, but updates <code>ES</code>.)</p>
<pre>OPR ‚Üí N               <span><b>LDS</b>: Copy OPR to dest register</span>
IND ‚Üí tmpC  INC2 tmpC <span>Set up incrementing IND by 2</span>
Œ£ ‚Üí IND     R DS,P0   <span>Update IND, read next location</span>
OPR ‚Üí DS    RNI       <span>Update DS</span>
</pre>

<p>If the <code>LDS</code> instruction specifies a register instead of memory, a micro-subroutine will not be called, so <code>IND</code> and <code>OPR</code>
will have values from a previous instruction.
<code>OPR</code> will be stored in the destination register, while the <code>DS</code> value will be read from the address <code>IND+2</code>.
Thus, these instructions provide a mechanism to access the hidden <code>OPR</code> register.</p>
<h3><code>JMP FAR rm</code></h3>
<p>The <code>JMP FAR rm</code> instruction normally jumps to the far address stored in memory at the location indicated by the ModR/M byte.
(That is, the ModR/M byte indicates where the new PC and CS values are stored.)
But, as with <code>LEA</code>, the behavior is undocumented if the ModR/M byte specifies a register, since a register doesn't hold
a four-byte value.</p>
<p>The microcode explains what happens.
As with <code>LEA</code>, the code expects a micro-subroutine to put the address into the <code>IND</code> register.
In this case, the micro-subroutine also loads the value at that address (i.e. the destination <code>PC</code>) into tmpB.
The microcode increments <code>IND</code> by 2 to point to the <code>CS</code> word in memory and reads that into <code>CS</code>.
Meanwhile, it updates the <code>PC</code> with <code>tmpB</code>.
It suspends prefetching and flushes the queue, so instruction fetching will restart at the new address.</p>
<pre>IND ‚Üí tmpC  INC2 tmpC   <span><b>JMP FAR rm</b>: set up to add 2 to IND</span>
Œ£ ‚Üí IND     SUSP        <span>Update IND, suspend prefetching</span>
tmpB ‚Üí PC   R DS,P0     <span>Update PC with tmpB. Read new CS from specified address</span>
OPR ‚Üí CS    FLUSH RNI   <span>Update CS, flush queue, done</span>
</pre>

<p>If you specify a register instead of memory, the micro-subroutine won't get called.
Instead, the program counter will be loaded with whatever value was in <code>tmpB</code> and the <code>CS</code> segment register will
be loaded from the memory location two bytes after the location that <code>IND</code> was referencing.
Thus, this undocumented use of the instruction gives access to the otherwise-hidden <code>tmpB</code> register.</p>

<p>Microprocessor manufacturers soon realized that undocumented instructions were a problem, since
programmers find them and often use them.
This creates an issue for future processors, or even revisions of the current processor:
if you eliminate an undocumented instruction, previously-working code that used the instruction will break,
and it will seem like the new processor is faulty.</p>
<p>The solution was for processors to detect undocumented instructions and prevent them from executing.
By the early 1980s, processors had enough transistors (thanks to Moore's law) that they could include
the circuitry to block unsupported instructions.
In particular, the 80186/80188 and the 80286 generated a trap of type 6 when an unused opcode was executed,
blocking use of the instruction.<span id="fnref:186"><a href="#fn:186">9</a></span>
This trap is also known as #UD (Undefined instruction trap).<span id="fnref:fault"><a href="#fn:fault">10</a></span></p>
<h2>Conclusions</h2>
<p>The 8086, like many early microprocessors, has undocumented instructions but no traps to stop them from executing.<span id="fnref:references"><a href="#fn:references">11</a></span>
For the 8086, these fall into several categories.
Many undocumented instructions simply mirror existing instructions.
Some instructions are implemented but not documented for one reason or another, such as <code>SALC</code> and <code>POP CS</code>.
Other instructions can be used outside their normal range, such as <code>AAM</code> and <code>AAD</code>.
Some instructions are intended to work only with a memory address, so specifying a register can have
strange effects such as revealing the values of the hidden <code>IND</code> and <code>OPR</code> registers.</p>
<p>Keep in mind that my analysis is based on transistor-level simulation and examining the microcode; I haven't verified the behavior on a
physical 8086 processor. Please let me know if you see any errors in my analysis or undocumented instructions that I have
overlooked.
Also note that the behavior could change between different versions of the 8086; in particular, some versions by different manufacturers
(such as the NEC V20 and V30) are known to be different.</p>
<p>I plan to write more about the 8086, so
follow me on Twitter <a href="https://twitter.com/kenshirriff">@kenshirriff</a> or <a href="https://www.righto.com/feeds/posts/default">RSS</a> for updates.
I've also started experimenting with Mastodon recently as <a href="https://oldbytes.space/@kenshirriff">@<span data-cfemail="fe959b908d96978c8c979898be91929a9c878a9b8dd08d8e9f9d9b">[email&nbsp;protected]</span></a>
and Bluesky as <a href="https://staging.bsky.app/profile/righto.com">@righto.com</a> so you can follow me there too.</p>
<h2>Notes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grav is a modern open-source flat-file CMS (117 pts)]]></title>
            <link>https://getgrav.org/</link>
            <guid>36751375</guid>
            <pubDate>Sun, 16 Jul 2023 21:04:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://getgrav.org/">https://getgrav.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36751375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">

<div id="modular-choose">
<div data-animation="fadeIn" data-timeout="200">
<h2>Why Choose Grav?</h2>
<div id="cms-critic">

<p><a href="https://getgrav.org/blog/cms-critic-award-2016"><img alt="Best Open Source CMS Award Badge" src="https://getgrav.org/user/themes/planetoid/images/best-open-source-cms.png"></a>
<a href="https://www.cmscritic.com/cms-critic-awards/"><img alt="Best Flat File CMS Award Badge" src="https://getgrav.org/user/themes/planetoid/images/best-flat-file-cms.png"></a>
</p>

</div>
</div>
<div>
<div data-animation="fadeIn" data-timeout="500">
<div>
<p><img alt="fast Icon" src="https://getgrav.org/user/themes/planetoid/images/fast.svg">
<span>Fast</span>
</p>
</div>
<p>Performance is not just an afterthought, we baked it in from the start</p>
</div>
<div data-animation="fadeIn" data-timeout="900">
<div>
<p><img alt="lego Icon" src="https://getgrav.org/user/themes/planetoid/images/lego.svg">
 <span>Extensible</span>
</p>
</div>
<p>Grav has a powerful API and sophisticated Package Manager to make it super flexible</p>
</div>
<div data-animation="fadeIn" data-timeout="1300">
<div>
<p><img alt="github Icon" src="https://getgrav.org/user/themes/planetoid/images/github.svg">
<span>Open Source</span>
</p>
</div>
<p>Grav is Open Source, and all the code is available on <a href="https://github.com/getgrav/grav">GitHub.com</a></p>
</div>
</div>
</div>

<div id="modular-easy-to-use">

<div data-animation="fadeIn" data-timeout="200">
<h2>Super Easy to Use</h2>
<p>The Grav admin plugin provides a simple and intuitive interface to make configuration and content creation easy and enjoyable.</p>
<p><a href="https://getgrav.org/downloads/plugins">
Get Admin Plugin
</a></p>
</div>
<div>
<div>
<p>The Grav Admin dashboard provides a quick glance at your site state</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/001-dashboard.png?g-283e9f04" alt="001-dashboard.png" src="https://getgrav.org/001-dashboard.png">
</p></div>
<div>
<p>Easily modify Grav's flexible configuration settings with its advanced forms</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/002-config.png?g-283e9f04" alt="002-config.png" src="https://getgrav.org/002-config.png">
</p></div>
<div>
<p>Editing content is a breeze with Grav's powerful markdown editor</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/003-editpage.png?g-283e9f04" alt="003-editpage.png" src="https://getgrav.org/003-editpage.png">
</p></div>
<div>
<p>Changing your administrator information is quick and easy</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/004-user.png?g-283e9f04" alt="004-user.png" src="https://getgrav.org/004-user.png">
</p></div>
<div>
<p>Grav has a built in package manager with one-click install for plugins</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/005-plugins.png?g-283e9f04" alt="005-plugins.png" src="https://getgrav.org/005-plugins.png">
</p></div>
<div>
<p>You can also install a wide range of modern themes with one-click</p>
<p><img data-src="/user/pages/01.tour/_easy-to-use/006-themes.png?g-283e9f04" alt="006-themes.png" src="https://getgrav.org/006-themes.png">
</p></div>
</div>
</div>

<div data-animation="fadeIn" data-timeout="200" id="modular-developers">
<p><img alt="Shield Icon" src="https://getgrav.org/user/themes/planetoid/images/shield.svg"></p><h2>be a hero developer</h2>
<p>Packed to the gills with amazing features and tools, coupled with heaps of detailed documentation, Grav will make you look like a hero developer!</p>
<p><a href="http://learn.getgrav.org/">
Read the Documentation
</a></p>
</div>

<div data-animation="fadeIn" data-timeout="200" id="modular-features">
<h2>a bevy of features</h2>
<ul>
<li data-animation="fadeIn" data-timeout="250">
<p><img alt="content-creation.png" src="https://getgrav.org/user/pages/01.tour/_features/content-creation.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Enjoyable Content Creation</h4>
<p>Use your favorite Markdown editor to create your content online or offline</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="450">
<p><img alt="gpm.png" src="https://getgrav.org/user/pages/01.tour/_features/gpm.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>One-Click Installs</h4>
<p>The built-in Package Manager lets you to find, install, and easily update extensions and themes for Grav</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="650">
<p><img alt="caching.png" src="https://getgrav.org/user/pages/01.tour/_features/caching.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Stellar Performance</h4>
<p>Grav intelligently caches content to deliver great performance, regardless of hosting</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="850">
<p><img alt="taxonomy.png" src="https://getgrav.org/user/pages/01.tour/_features/taxonomy.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Powerful Content Filtering</h4>
<p>Create unlimited taxonomies such as tags, categories, and authors to filter and manage your content</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1050">
<p><img alt="frontmatter.png" src="https://getgrav.org/user/pages/01.tour/_features/frontmatter.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Dynamic Content Types</h4>
<p>The flat-file nature of Grav lets you define custom fields for any of your pages, including modular content</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1250">
<p><img alt="multilang.png" src="https://getgrav.org/user/pages/01.tour/_features/multilang.png?g-283e9f04" width="400" height="400">
</p>
<div>
<h4>Multi-Language Support</h4>
<p>A simple mechanism for presenting sites in multiple languages is built into Grav</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1450">
<p><img alt="backup.png" src="https://getgrav.org/user/pages/01.tour/_features/backup.png?g-283e9f04" width="353" height="353">
</p>
<div>
<h4>Simple Backups and Restores</h4>
<p>Being file based means backing up and restoring your data is super easy, and changing hosts/servers is a breeze!</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1650">
<p><img alt="tiger.jpg" src="https://getgrav.org/user/pages/01.tour/_features/tiger.jpg?g-283e9f04" width="386" height="386">
</p>
<div>
<h4>Image Media Processing</h4>
<p>Dynamic image manipulation to resize, crop, resample, and effects all with automatic caching of images</p>
</div>
</li>
<li data-animation="fadeIn" data-timeout="1850">
<p><img alt="theme.jpg" src="https://getgrav.org/user/pages/01.tour/_features/theme.jpg?g-283e9f04" width="222" height="222">
</p>
<div>
<h4>Easy Theme Customization</h4>
<p>No need to start from scratch, use Theme Inheritance and then modify the bits you need, allowing for easier update</p>
</div>
</li>
</ul>
<p><a href="https://getgrav.org/features">
Discover all features
</a>
</p></div>

<div data-animation="fadeIn" data-timeout="200" id="modular-pros">
<p><a href="http://trilby.media/" id="trilby-logo" aria-label="Trilby Media"><img alt="Trilby Media" src="https://getgrav.org/user/pages/01.tour/_professional-services/trilby-logo.svg?g-283e9f04"></a></p>
<h2>The Grav Professionals</h2>
<p><a href="http://trilbymedia.com/">Trilby Media</a> is a development company you can turn to for help with your Grav site. Trilby offers a variety of <strong>professional services</strong> and is run by the same team that built Grav in the first place!</p>
<ul>
<li><i></i> Grav consulting services</li>
<li><i></i> Custom Grav plugin development</li>
<li><i></i> Custom Grav theme development</li>
<li><i></i> Existing Grav plugin and theme customization</li>
<li><i></i> Porting to Grav from other platforms</li>
<li><i></i> Grav site design and development</li>
</ul>
<p><a href="http://trilby.media/">
Contact Trilby Media
</a></p>
</div>



<div data-animation="fadeIn" data-timeout="200" id="modular-limitless">
<div>
<h2>There are no limits!</h2>
<p>From simple to sophisticated, Grav has the flexibility to power all sorts of websites.
Flexible content structure and powerful Twig templating allow easy realization of any design.</p>
</div>
<div>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/blog.svg?g-283e9f04" alt="blog.svg">
<figcaption>
<p>Blogs</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/business.svg?g-283e9f04" alt="business.svg">
<figcaption>
<p>Business Sites</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/directory.svg?g-283e9f04" alt="directory.svg">
<figcaption>
<p>Directory</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/documentation.svg?g-283e9f04" alt="documentation.svg">
<figcaption>
<p>Documentation</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/landing-page.svg?g-283e9f04" alt="landing-page.svg">
<figcaption>
<p>Landing Pages</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/portfolio.svg?g-283e9f04" alt="portfolio.svg">
<figcaption>
<p>Portfolios</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/product.svg?g-283e9f04" alt="product.svg">
<figcaption>
<p>Product Sites</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/resume.svg?g-283e9f04" alt="resume.svg">
<figcaption>
<p>Personal Resumes</p>
</figcaption>
</figure>
<figure>
<img src="https://getgrav.org/user/pages/01.tour/_limitless/review.svg?g-283e9f04" alt="review.svg">
<figcaption>
<p>E-Commerce</p>
</figcaption>
</figure>
</div>
<p><a href="https://getgrav.org/downloads/skeletons">
See Skeletons
</a></p>
</div>

<div id="modular-built-with">
<h2>Built with Grav</h2>
<p>Grav is a highly versatile platform. Give your creativity wings and Grav will set you free!</p>
</div>

<div id="modular-people-saying">
<div data-animation="fadeIn" data-timeout="200">
<h2>What people are saying</h2>
<p>Still need convincing? Check out what people are saying about Grav. If you have any thoughts or questions, please reach out and ask us.</p>
</div>
<ul id="peoplesaying" data-animation="fadeIn" data-timeout="500">
<li>
<img alt="David Blum" src="https://getgrav.org/images/d/8/f/e/4/d8fe452942f9c971b51c2da92ad58fe7dee6f298-dbloom.jpg?g-283e9f04" width="200" height="200">
<h5>David Blum</h5>
<a href="https://twitter.com/dblO_Om">@dblO_Om</a>
<hr>
<p>@rhuk I just read the complete @getgrav documentation. It is so well written and styled, it's a pleasure. Brilliant!</p>
</li>
<li>
<img alt="Air Petr" src="https://getgrav.org/images/1/6/1/6/f/1616f53bc0c14913b1282e79ac966d152610e326-airpetr.jpg?g-283e9f04" width="200" height="200">
<h5>Air Petr</h5>
<a href="https://twitter.com/air_petr">@air_petr</a>
<hr>
<p>@getgrav Thank you for Grav!!! Dived in for last few days. It's realy spellbinding!</p>
</li>
<li>
<img alt="Bryan Ollendyke" src="https://getgrav.org/images/7/c/2/c/5/7c2c5b24851a5ce5fbba630070a6ed8d70247ab0-btopro.jpg?g-283e9f04" width="200" height="200">
<h5>Bryan Ollendyke</h5>
<a href="https://twitter.com/btopro">@btopro</a>
<hr>
<p>30 seconds after installing @getgrav impression: this is a wordpress killer and we need a site factory for it in @elmsln for portfolios</p>
</li>
<li>
<img alt="Vitor Costa" src="https://getgrav.org/images/c/b/4/2/2/cb422abed3e9fa09607f301058b72cd6710c1083-vmcosta.jpg?g-283e9f04" width="200" height="200">
<h5>Vitor Costa</h5>
<a href="https://twitter.com/vmcosta">@vmcosta</a>
<hr>
<p>I've just found the best flat file CMS, finally‚Ä¶ @getgrav amazing work guys, nicely done, and yes I hate databases too ;)</p>
</li>
<li>
<img alt="Richard Allen" src="https://getgrav.org/images/b/1/6/4/3/b1643fdab2ded2f6f85e1a3926e72f1bda4340be-pmrourkie.jpg?g-283e9f04" width="200" height="200">
<h5>Richard Allen</h5>
<a href="https://twitter.com/pmrourkie">@pmrourkie</a>
<hr>
<p>Absolutely loving @getgrav. Simple, powerful and very fast!</p>
</li>
<li>
<img alt="Ryan Little" src="https://getgrav.org/images/5/7/6/4/5/5764518b67ca205df5da9f8b1accf28d1e2b34f9-ryantereu.jpg?g-283e9f04" width="200" height="200">
<h5>Ryan Little</h5>
<a href="https://twitter.com/ryantereu">@ryantereu</a>
<hr>
<p>Only just began using @getgrav as a cms, but their documentation alone is swoon-worthy: friendly, thorough, well-designed.</p>
</li>
<li>
<img alt="Daniel Kao" src="https://getgrav.org/images/2/2/f/f/d/22ffdaef3925419c80c87fdf90806fee5e4d17fc-diplateevo.jpg?g-283e9f04" width="200" height="200">
<h5>Daniel Kao</h5>
<a href="https://twitter.com/diplateevo">@diplateevo</a>
<hr>
<p>@getgrav Just might be one of the biggest game changers in web CMSs.</p>
</li>
<li>
<img alt="Denis Duvauchelle" src="https://getgrav.org/images/f/e/0/7/c/fe07ce7759371475f20622f7abc3cddcf43be690-desduvauchelle.jpg?g-283e9f04" width="200" height="200">
<h5>Denis Duvauchelle</h5>
<a href="https://twitter.com/desduvauchelle">@desduvauchelle</a>
<hr>
<p>Loving @getgrav over wordpress. If you are developer, you're probably going to fall for it. If your not, you'll probably like it as well.</p>
</li>
<li>
<img alt="Parker Agee" src="https://getgrav.org/images/a/e/a/a/5/aeaa5c5d422874c7a3eaed944d24e8566dd20c1d-parkeragee.jpg?g-283e9f04" width="200" height="200">
<h5>Parker Agee</h5>
<a href="https://twitter.com/parkeragee">@parkeragee</a>
<hr>
<p>Fell in love with the @getgrav and @snipcart combo tonight. Extremely fast set up and works like a charm.</p>
</li>
<li>
<img alt="Casey Grzecka" src="https://getgrav.org/images/d/5/1/e/a/d51ea16a0165d19773ba69f0bb174de430780a17-z3cka.png?g-283e9f04" width="200" height="200">
<h5>Casey Grzecka</h5>
<a href="https://twitter.com/z3cka">@z3cka</a>
<hr>
<p>Grav is legit! I'm ready to leave Drupal in the dust of complexity.</p>
</li>
<li>
<img alt="Harley Hicks" src="https://getgrav.org/images/6/3/4/2/2/63422c586258429ed97e707ed470c2a2fa90f36f-harlshicks.jpg?g-283e9f04" width="200" height="200">
<h5>Harley Hicks</h5>
<a href="https://twitter.com/harlshicks">@harlshicks</a>
<hr>
<p>@getgrav is bringing back my joy for web development and design. So easy to get up and running, and all the puzzle pieces fit so well.</p>
</li>
<li>
<img alt="David Easton" src="https://getgrav.org/images/8/6/1/9/4/8619439d4c7323e62633e939863135cf73489b3e-dweebvid.jpg?g-283e9f04" width="200" height="200">
<h5>David Easton</h5>
<a href="https://twitter.com/dweebvid">@dweebvid</a>
<hr>
<p>Was developing around Wordpress,I wanted to tone down the over-complexity for my needs. I found a flat-file cms called @getGrav and it rocks</p>
</li>
<li>
<img alt="John Williams" src="https://getgrav.org/images/0/5/1/e/6/051e6626984559d0d04747fbdb2815c43e46c27e-thudfactor.jpg?g-283e9f04" width="200" height="200">
<h5>John Williams</h5>
<a href="https://twitter.com/thudfactor">@thudfactor</a>
<hr>
<p>The CMS from @getgrav continues to impress. "What do I need to do to implement open graph metadata? Oh‚Ä¶ nothing."</p>
</li>
<li>
<img alt="Dayle Rees" src="https://getgrav.org/images/c/b/7/7/a/cb77a6ea66f816a13618f278021602ff67f4c01d-daylerees.jpg?g-283e9f04" width="200" height="200">
<h5>Dayle Rees</h5>
<a href="https://twitter.com/daylerees">@daylerees</a>
<hr>
<p>I've just had a play, and it's honestly VERY good. Only a few SF2 components too so it's FAST.</p>
</li>
<li>
<img alt="Serge K. Keller" src="https://getgrav.org/images/5/7/c/9/c/57c9cf7b425a3948bc3a8cfbb7609f9c4ca081d5-citizenk.jpg?g-283e9f04" width="200" height="200">
<h5>Serge K. Keller</h5>
<a href="https://twitter.com/citizenk">@citizenk</a>
<hr>
<p>You know, @getgrav may finally be a CMS that does not weight me down for projects I love. And yes, the pun was definitely intended.</p>
</li>
<li>
<img alt="Stephen Foster" src="https://getgrav.org/images/0/4/d/9/8/04d98ddd24af7e635beafb19e03f71f8ef6da7ce-srfoster.jpg?g-283e9f04" width="200" height="200">
<h5>Stephen Foster</h5>
<a href="https://twitter.com/s_r_foster">@s_r_foster</a>
<hr>
<p>@getgrav Your CMS is the BEST. I was able to make my site in no time flat: http://stephenfoster.us</p>
</li>
<li>
<img alt="Yahya Zini" src="https://getgrav.org/images/6/a/2/0/e/6a20e5684c0cf24cfb769b6426591fc5595510fe-yahyazini.jpg?g-283e9f04" width="200" height="200">
<h5>Yahya Zini</h5>
<a href="https://twitter.com/YahyaZini">@YahyaZini</a>
<hr>
<p>A shout out to the @getgrav team for creating such a great</p>
</li>
<li>
<img alt="Paul Orwig" src="https://getgrav.org/images/2/1/3/7/6/21376785569cd837486cbf928911b30560f60037-porwig.jpg?g-283e9f04" width="200" height="200">
<h5>Paul Orwig</h5>
<a href="https://twitter.com/porwig">@porwig</a>
<hr>
<p>If you haven't taken a look at @getgrav, you should! Many good ideas, thoughtfully implemented. Great work, @rhuk!</p>
</li>
<li>
<img alt="Scott Edgar" src="https://getgrav.org/images/f/c/f/4/e/fcf4ec14ced257a5d6b4c66016d09cf1524b8b4f-thesneakybandit.jpg?g-283e9f04" width="200" height="200">
<h5>Scott Edgar</h5>
<a href="https://twitter.com/thesneakybandit">@thesneakybandit</a>
<hr>
<p>Just started learning a new CMS. Ok, learnt it. Brilliantly simple stuff from @getgrav.</p>
</li>
 <li>
<img alt="Jozef Maxted" src="https://getgrav.org/images/7/b/3/0/4/7b3040e4271f7d1afbb4f03ea45fbcdc5329d075-jozefmaxted.jpg?g-283e9f04" width="200" height="200">
<h5>Jozef Maxted</h5>
<a href="https://twitter.com/jozefmaxted">@jozefmaxted</a>
<hr>
<p>Played around with @getgrav this evening and I'm impressed. Super simple page creation and theming, flexible workflow, just nice to use!</p>
</li>
<li>
<img alt="Noah Betzen" src="https://getgrav.org/images/f/4/6/4/b/f464b1a4979c424feb9bec53ffd8023682632f27-nezteb.jpg?g-283e9f04" width="200" height="200">
<h5>Noah Betzen</h5>
<a href="https://twitter.com/nezteb">@nezteb</a>
<hr>
<p>[Grav] is by far the best CMS I've ever had the privilege of using. @getgrav</p>
</li>
<li>
<img alt="Steven Lain" src="https://getgrav.org/images/1/9/c/7/a/19c7a3177e1edeb93626dd3739990e3ea52e983b-stevenjlain.jpg?g-283e9f04" width="200" height="200">
<h5>Steven Lain</h5>
<a href="https://twitter.com/stevenjlain">@stevenjlain</a>
<hr>
<p>just installed grav &amp; it is such a pleasure to use. Hats off to the grav team. I look forward to using grav on many, many projects!</p>
</li>
<li>
<img alt="Mike Wink" src="https://getgrav.org/images/a/e/2/e/a/ae2ea72e7aa7a7c8eed0b9a75bb48a42a5460cc0-mikewink.png?g-283e9f04" width="200" height="200">
<h5>Mike Wink</h5>
<a href="https://twitter.com/mikewink">@mikewink</a>
<hr>
<p>Ohh how I love deployments in Grav (@getgrav)! Everything is a .md or .yaml file, no hassle with DBs or other content silos.</p>
</li>
<li>
<img alt="Reach Content" src="https://getgrav.org/images/d/a/c/2/4/dac243398f28e3a6eafe8b3a3cedc436e13f9a92-reachcontent.png?g-283e9f04" width="200" height="200">
<h5>Reach Content</h5>
<a href="https://twitter.com/reachcontent">@reachcontent</a>
<hr>
<p>Getting to know @getgrav you'll want to nominate it as Best New</p>
</li>
<li>
<img alt="Bison" src="https://getgrav.org/images/c/2/2/7/d/c227debab8386fc91e461192f1aa54d43296e7b0-bison42.jpg?g-283e9f04" width="200" height="200">
<h5>Bison</h5>
<a href="https://twitter.com/bison_42">@bison_42</a>
<hr>
<p>everyone should know that there is http://getgrav.org out there a flat-file CMS that does not suck it's awesome!</p>
</li>
<li>
<img alt="Faisal Khan" src="https://getgrav.org/images/b/e/5/e/d/be5ed7ce3fe6bcbedbbd8f4aa59df1ea02e0dbae-babushka99.jpg?g-283e9f04" width="200" height="200">
<h5>Faisal Khan</h5>
<a href="https://twitter.com/babushka99">@babushka99</a>
<hr>
<p>Grav is my new love. So uber-smooth, exceptionally fast and just the right amounts of knobs and switches! @getgrav</p>
</li>
</ul>
<p><a href="https://twitter.com/getgrav">
follow @getgrav
</a>
</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebGPU ‚Äì All of the cores, none of the canvas (101 pts)]]></title>
            <link>https://surma.dev/things/webgpu/</link>
            <guid>36751307</guid>
            <pubDate>Sun, 16 Jul 2023 20:54:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://surma.dev/things/webgpu/">https://surma.dev/things/webgpu/</a>, See on <a href="https://news.ycombinator.com/item?id=36751307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
    
  <p>WebGPU is an upcoming Web API that gives you low-level, general-purpose access GPUs.</p>
<!-- more -->
<p>I am not very experienced with graphics. I picked up bits and bobs of WebGL by reading through tutorials on how to build game engines with OpenGL and learned more about shaders by watching <a href="https://twitter.com/iquilezles">Inigo Quilez</a> do amazing things on <a href="https://shadertoy.com/">ShaderToy</a> by just using shaders, without any 3D meshes or models. This got me far enough to do build things like the background animation in <a href="https://proxx.app/">PROXX</a>, but I was never <em>comfortable</em> with WebGL, and I‚Äôll explain shortly why.</p>
<p>When WebGPU came onto my radar, I wanted to get into it but multiple people warned me that WebGPU has even more boilerplate than WebGL. Undeterred, but anticipating the worst, I scraped together all the tutorials and specifications I could find, of which there are not many because it‚Äôs still early days for WebGPU. I got my feet wet and found that I didn‚Äôt find WebGPU significantly more boilerplate-y than WebGL, but actually to be an API I am much more comfortable with.</p>
<p>So here we are. I want to share what I learned while wrapping my head around GPUs and WebGPU. The goal for this blog post is to make WebGPU accessible for web developers. But here‚Äôs an early heads up: I won‚Äôt use WebGPU to generate graphics. Instead, I will use WebGPU to access the raw computing power a GPU provides. Maybe I will do a follow-up blog post how to use WebGPU to render to your screen, but there is already <a href="https://austin-eng.com/webgpu-samples">quite a bit of content</a> out there. I will go as deep as necessary to make sense of WebGPU and hopefully enable you to use it <em>effectively</em> ‚Äî but not necessarily <em>efficiently</em>. I can‚Äôt make you a GPU performance expert; mostly because I am not one myself.</p>
<p>Enough disclaimer. This is a long one. Buckle up!</p>
<h2>WebGL</h2>
<p><a href="https://www.khronos.org/registry/webgl/specs/latest/1.0/">WebGL</a> came about in 2011 and, up to this point, was the only low-level API to access GPUs with from the web. WebGL‚Äôs API is really just OpenGL ES 2.0 with some thin wrappers and helpers to make it web-compatible. Both WebGL and OpenGL are standardized by the <a href="https://www.khronos.org/">Khronos Group</a>, which is basically the W3C for 3D graphics.</p>
<p>OpenGL‚Äôs API itself goes back even further and is, by today‚Äôs standard, not a great API. The design is centered around an internal, global state object. The design makes sense from the perspective that it minimizes the amount of data that needs to be transferred to and from the GPU for any given call. However, it also introduces a lot of mental overhead.</p>
<figure>
  <img loading="lazy" width="1193" height="1300" src="https://surma.dev/assets/internalstate.c00c7a0f.png">
  <figcaption>A visualization of WebGL‚Äôs internal, global state object. Taken from <a href="https://webglfundamentals.org/webgl/lessons/resources/webgl-state-diagram.html" target="_blank" rel="noopener">WebGL Fundamentals</a>.</figcaption>
</figure>
<p>The internal state object is basically a collection of pointers. Your API calls can affect the objects pointed <em>to</em> by the state object, but also the state object itself. As a result, the order of API calls is incredibly important and I always felt like that this makes it hard to build abstractions and libraries. You have to be extremely meticulous in sanitizing all pointers and state items that can interfere the API calls you are going to make, but also restore the pointers and values to their previous value so that your abstractions compose correctly. I often found myself staring at a black canvas (as that‚Äôs pretty much all you get in terms of error reporting in WebGL) and brute-forcing which pointer is currently not pointing the right way. Quite honestly, I have no idea how <a href="https://threejs.org/">ThreeJS</a> manages to be so robust, but it does manage somehow. I think that‚Äôs one of the main reasons why most people use ThreeJS and not WebGL directly.</p>
<blockquote>
<p><strong>It‚Äôs not you, it‚Äôs me:</strong> To be clear, me not being able to internalize WebGL is probably a shortcoming of my own. People smarter than me have been able to build <em>amazing</em> stuff with WebGL (and OpenGL outside the web), but it just never really clicked for me.</p>
</blockquote>
<p>With the advent of ML, neural networks, and dare I say cryptocurrencies, GPUs have shown that they can be useful for more than just drawing triangles on the screen. Using GPUs for calculations of any kind is often called General-Purpose GPU or GPGPU, and WebGL 1 is not great at this. If you wanted to process arbitrary data on the GPU, you have to encode it as a texture, decode it in a shader, do your calculations and then re-encode the result as a texture. WebGL 2 made this a lot easier with <a href="https://webgl2fundamentals.org/webgl/lessons/webgl-gpgpu.html">Transform Feedback</a>, but WebGL2 wasn‚Äôt supported in Safari until September 2021 (while most other browers supported WebGL2 since January 2017), so it wasn‚Äôt really an option. And even then certain limitations of WebGL2 still made it feel somewhat clunky.</p>
<h2>WebGPU</h2>
<p>Outside of the web, a new generation of graphics APIs have established themselves which expose a more low-level interface to graphics cards. These new APIs accommodate new use-cases and constraints that weren‚Äôt around when OpenGL was designed. On the one hand, GPUs are almost ubiquitous now. Even our mobile devices have capable GPUs built in. As a result, <em>both</em> modern graphics programming (3D rendering and ray tracing) and GPGPU use-cases are increasingly common. On the other hand, most of our devices have multi-core processors, so being able to interact with the GPU from multiple threads can be an important optimization vector. While the WebGPU folks were at it, they also revisited some previous design decisions and front-loaded a lot of the validation work that GPUs have to do, allowing the developer to squeeze more performance out of their GPUs.</p>
<p>The most popular of the next-gen GPU APIs are <a href="https://www.vulkan.org/">Vulkan</a> by the Khronos Group, <a href="https://developer.apple.com/metal/">Metal</a> by Apple and <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/direct3d-12-graphics">DirectX 12</a> by Microsoft. To bring these new capabilities to the web, WebGPU was born. While WebGL is just a thin wrapper around OpenGL, WebGPU chose a different approach. It introduces its own abstractions and doesn‚Äôt directly mirror any of these native APIs. This is partially because no single API is available on all systems, but also because many concepts (such as extremely low-level memory management) aren‚Äôt idiomatic for a web-facing API. Instead, WebGPU was designed to both feel ‚Äúwebby‚Äù and to comfortably sit on top of any of the native graphics APIs  while abstracting their idiosyncrasies. It‚Äôs being standardized in the W3C with all major browser vendors having a seat at the table. Due to its comparatively low-level nature and its sheer power, WebGPU has a bit of a learning curve and is relatively heavy on the setup, but I‚Äôll try to break it down as best I can.</p>
<h3>Adapters and Devices</h3>
<p>WebGPU‚Äôs first abstractions that you come into contact with are <em>adapters</em> and (logical) <em>devices</em>.</p>
<figure>
  <img loading="lazy" width="418" height="393" src="https://surma.dev/assets/architecture.498626c9.svg">
  <figcaption>Layers of abstraction, from physical GPUs to logical devices.</figcaption>
</figure>
<p>A <em>physical</em> device is the GPU itself, often distinguished between built-in GPUs and discrete GPUs. Commonly, any given device has exactly one GPU, but it also possible to have two or more GPUs. For example, Microsoft‚Äôs SurfaceBook famously has a low-powered integrated GPU and a high-performance discrete GPU between which the operating system will switch on demand.</p>
<p>The <em>driver</em> ‚Äî provided by the GPU manufacturer ‚Äî will expose the GPU‚Äôs capabilities to the operating system in a way the OS understands and expects. The operating system in turn can expose it to applications, using the graphics APIs the operating system offers, like Vulkan or Metal.</p>
<p>The GPU is a shared resource. It is not only used by many applications at the same time, but also controls what you see on your monitor. There needs to be something that enables multiple processes to use the GPU concurrently, so that each app can put their own UI on screen without interfering with other apps or even maliciously reading other apps‚Äô data. To each process, it looks like they have sole control over the physical GPU, but that is obviously not really the case. This multiplexing is mostly done by the driver and the operating system.</p>
<p>Adapters, in turn, are the translation layer from operation system‚Äôs native graphics API to WebGPU. As the browser is a single OS-level application that can run multiple web applications, there is yet again a need for multiplexing, so that each web app feels like it has sole control of the GPU. This is modelled in WebGPU with the concept of <em>logical</em> devices.</p>
<p>To get access to an adapter, you call <code>navigator.gpu.requestAdapter()</code>. At the time of writing, <a href="https://gpuweb.github.io/gpuweb/#gpu-interface"><code>requestAdapter()</code></a> takes very few options. The options allow you to request a high-performance or low-energy adapter.</p>
<blockquote>
<p><strong>Software rendering:</strong> Some implementations also offer a ‚Äúfallback adapter‚Äù for systems with no GPU or a GPU that isn‚Äôt sufficiently capable. Fallback adapters are effectively a pure software implementation, which will not be very fast but keeps your app functional.</p>
</blockquote>
<p>If this succeeds, i.e. the returned adapter is non-<code>null</code>, you can inspect the adapter‚Äôs capabilities and request a logical device from the adapter using <a href="https://gpuweb.github.io/gpuweb/#gpuadapter"><code>adapter.requestDevice()</code></a>.</p>
<pre><code><span>if</span> <span>(</span><span>!</span>navigator<span>.</span>gpu<span>)</span> <span>throw</span> <span>Error</span><span>(</span><span>"WebGPU not supported."</span><span>)</span><span>;</span>

<span>const</span> adapter <span>=</span> <span>await</span> navigator<span>.</span>gpu<span>.</span><span>requestAdapter</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>!</span>adapter<span>)</span> <span>throw</span> <span>Error</span><span>(</span><span>"Couldn‚Äôt request WebGPU adapter."</span><span>)</span><span>;</span>

<span>const</span> device <span>=</span> <span>await</span> adapter<span>.</span><span>requestDevice</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>!</span>device<span>)</span> <span>throw</span> <span>Error</span><span>(</span><span>"Couldn‚Äôt request WebGPU logical device."</span><span>)</span><span>;</span>
</code></pre>
<p>Without any options, <code>requestDevice()</code> will return a device that does <em>not</em> necessarily match the physical device‚Äôs capabilities, but rather what the WebGPU team considers a reasonable, lowest common denominator of all GPUs. The details are <a href="https://gpuweb.github.io/gpuweb/#limit">specified</a> in the WebGPU standard. For example, even though my GPU is easily capable of handling data buffers up to 4GiB in size, the <code>device</code> returned will only allow data buffers up to 1GiB, and will reject any data buffers that are bigger. This might seem restrictive, but is actually quite helpful: If your WebGPU app runs with the default device, it will run on the vast majority of devices. If necessary, you can inspect the real limits of the physical GPU via <code>adapter.limits</code> and request a <code>device</code> with raised limits by passing an options object to <code>requestDevice()</code>.</p>
<h3>Shaders</h3>
<p>If you have ever done any work with WebGL, you are probably familiar with vertex shaders and fragment shaders. Without going too much into depth, the traditional setup works something like this: You upload a data buffer to your GPU and tell it how to interpret that data as a series of triangles. Each vertex occupies a chunk of that data buffer, describing that vertex‚Äô position in 3D space, but probably also auxillary data like color, texture IDs, normals and other things. Each vertex in the list is processed by the GPU in the <em>vertex stage</em>, running the <em>vertex shader</em> on each vertex, which will apply translation, rotation or perspective distortion.</p>
<blockquote>
<p><strong>Shaders:</strong> The term ‚Äúshader‚Äù used to confuse me, because you can do <em>so much more</em> than just shading. But in the olden days (i.e. late 1980s!), that term was appropriate: It was a small piece of code that ran on the GPU to decide for each pixel what color it should be so that you could <em>shade</em> the objects being rendered, achieving the illusion of lighting and shadows. Nowadays, shaders loosely refer to any program that runs on the GPU.</p>
</blockquote>
<p>The GPU now rasterizes the triangles, meaning the GPU figures out which pixels each triangle covers on the screen. Each pixel is then processed by the <em>fragment shader</em>, which has access to the pixel coordinates but also the auxillary data to decide which color that pixel should be. When used correctly, amazing 3D graphics can be created with this process.</p>
<p>This system of passing data to a vertex shader, then to a fragment shader and then outputting it directly onto the screen is called a <em>pipeline</em>, and in WebGPU you have to explicitly define your pipeline.</p>
<h3>Pipelines</h3>
<p>Currently, WebGPU allows you to create two types of pipelines: A Render Pipeline and a Compute Pipeline. As the name suggest, the Render Pipeline renders something, meaning it creates a 2D image. That image needn‚Äôt be on screen, but could just be rendered to memory (which is called a Framebuffer). A Compute Pipeline is more generic in that it returns a buffer, which can contain any sort of data. For the remainder of this blog post I‚Äôll focus on Compute Pipelines, as I like to think of Render Pipelines as a specialization/optimization of Compute Pipelines. Now, this is both historically backwards ‚Äî the compute pipeline was built as a generalization over the very purpose-built rendering pipeline ‚Äî and also considerably understates that these pipelines are physically different circuits in your GPU. In terms of the API, however, I find this mental model quite helpful. In the future, it seems likely that more types of pipelines ‚Äî maybe a Raytracing Pipeline ‚Äî will get added to WebGPU.</p>
<p>With WebGPU, a pipeline consists of one (or more) programmable stages, where each stage is defined by a shader and an entry point. A Compute Pipline has a single <code>compute</code> stage, while a Render Pipeline would have a <code>vertex</code> and a <code>fragment</code> stage:</p>
<pre><code><span>const</span> module <span>=</span> device<span>.</span><span>createShaderModule</span><span>(</span><span>{</span>
  <span>code</span><span>:</span> <span><span>`</span><span>
    @compute @workgroup_size(64)
    fn main() {
      // Pointless!
    }
  </span><span>`</span></span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> pipeline <span>=</span> device<span>.</span><span>createComputePipeline</span><span>(</span><span>{</span>
  <span>compute</span><span>:</span> <span>{</span>
    module<span>,</span>
    <span>entryPoint</span><span>:</span> <span>"main"</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span>
</code></pre>
<p>This is the first time <a href="https://gpuweb.github.io/gpuweb/wgsl">WGSL</a> (pronounced ‚Äúwig-sal‚Äù), the WebGPU Shading Language, makes an appearance. WGSL feels like a cross-over of Rust and GLSL to me. It has a lot of Rust-y syntax with GLSL‚Äôs global functions (like <code>dot()</code>, <code>norm()</code>, <code>len()</code>, ...), types (like <code>vec2</code>, <code>mat4x4</code>, ...) and the <a href="https://en.wikipedia.org/wiki/Swizzling_(computer_graphics)">swizzling</a> notation (like <code>some_vec.xxy</code>, ...). The browser will compile your WGSL to whatever the underlying system expects. That‚Äôs likely to be HLSL for DirectX 12, MSL for Metal and <a href="https://www.khronos.org/spir/">SPIR-V</a> for Vulkan.</p>
<blockquote>
<p><strong>SPIR-V:</strong> <a href="https://www.khronos.org/spir/">SPIR-V</a> is interesting because it‚Äôs an open, binary, intermediate format standardized by the Khronos Group. You can think of SPIR-V as the LLVM of parallel programming language compilers, and there is support to compile many languages <em>to</em> SPIR-V as well as compiling SPIR-V to many other languages.</p>
</blockquote>
<p>In the shader module above we are just creating a function called <code>main</code> and marking it as an entry point for the compute stage by using the <code>@compute</code> attribute. You can have multiple functions marked as an entry point in a shader module, as you can reuse the same shader module for multiple pipelines and choose different functions to invoke via the <code>entryPoint</code> options. But what is that <code>@workgroup_size(64)</code> attribute?</p>
<h3>Parallelism</h3>
<p>GPUs are optimized for throughput at the cost of latency. To understand this, we have to look a bit at the architecture of GPUs.  I don‚Äôt want to (and, honestly, can‚Äôt) explain it in its entirety. I‚Äôll go as deep as I feel is necessary. If you want to know more, this <a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/">13-part blog post series</a> by <a href="https://twitter.com/rygorous">Fabian Giesen</a> is really good.</p>
<p>Something that is quite well-known is the fact that GPUs have an extensive number of cores that allow for massively parallel work. However, the cores are not as independent as you might be used to from when programming for a multi-core CPU. Firstly, GPU cores are grouped hierarchically. The terminology for the different layers of the hierarchy isn‚Äôt consistent across vendors and APIs. Intel has a good piece of <a href="https://www.intel.com/content/www/us/en/develop/documentation/oneapi-gpu-optimization-guide/top/intel-processors-with-intel-uhd-graphics.html">documentation</a> that gives a high-level overview of their architecture and I‚Äôve been told it‚Äôs safe to assume that other GPUs work at least similarly, despite the exact architecture of GPUs being a NDA-protected secret.</p>
<p>In the case of Intel, the lowest level in the hierarchy is the ‚ÄúExecution Unit‚Äù (EU), which has multiple (in this case seven) <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads">SIMT</a> cores. That means it has seven cores that operate in lock-step and always execute the same instructions. However, each core has its own set of registers and stack pointer. So while they <em>have</em> to execute the same operation, they can execute it on different data. This is also the reason why GPU performance experts avoid branches (like <code>if</code>/<code>else</code> or loops): If the EU encounters an <code>if</code>/<code>else</code>, all cores have to execute <em>both</em> branches, unless all cores happen to take the same branch. Each core can be told to ignore the instructions it is being fed, but that obviously wastes precious cycles that could be spent computing. The same applies to loops! If one core finishes their loop early, it will have to pretend to execute the loop body until <em>all</em> cores have finished the loop.</p>
<p>Despite the core‚Äôs frequency, getting data from memory (or pixels from textures) still takes relatively long ‚Äî Fabian says it takes a couple hundred clock cycles. These couple hundred cycles could be spent on computation instead. To make use of these otherwise idle cycles, each EU is heavily oversubscribed with work. Whenever an EU would end up idling (e.g. to wait for a value from memory), it instead switches to another work item and will only switch back once the new work item needs to wait for something. This is the key trick how GPUs optimize for throughput at the cost of latency: Individual work items will take longer as a switch to another work item might stop execution for longer than necessary, but the overall utilization is higher and results in a higher throughput. The GPU strives to always have work queued up to keep EUs busy at all times.</p>
<figure>
  <picture loading="lazy" width="548" height="492">
    <source srcset="https://surma.dev/assets/intel.0cf26a8e.avif " type="image/avif">
    <img src="https://surma.dev/assets/intel.862e630f.jpeg">
  </picture>
  <figcaption>The architecture of an Intel Iris Xe Graphics chip. EUs have 7 SIMT cores. SubSlices have 8 EUs. 8 SubSlices form a Slice.</figcaption>
</figure>
<p>EUs are just the lowest level in the hierarchy, though. Multiple EUs are grouped into what Intel calls a ‚ÄúSubSlice‚Äù. All the EUs in a SubSlice have access to a small amount of Shared Local Memory (SLM), which is about 64KiB in Intel‚Äôs case. If the program to be run has any synchronization commands, it has to be executed within the same SubSlice, as only they have shared memory for synchronization.</p>
<p>In the last layer multiple SubSlices are grouped into a Slice, which forms the GPU. For an integrated Intel GPU, you end up with a total of 170-700 cores. Discrete GPUs can easily have 1500 and more cores. Again, the naming here is taken from Intel, and other vendors probably use different names, but the general architecture is similar in every GPU.</p>
<p>To fully exploit the benefits of this architecture, programs need to be specifically set up for this architecture so that a purely programmatic GPU scheduler can maximize utilization. As a result, graphics APIs expose a <a href="https://github.com/googlefonts/compute-shader-101/blob/main/docs/glossary.md">threading model</a> that naturally allows for work to be dissected this way. In WebGPU, the important primitive here is the ‚Äúworkgroup‚Äù.</p>
<h3>Workgroups</h3>
<p>In the traditional setting, the vertex shader would get invoked once for each vertex, and the fragment shader once for each pixel (I‚Äôm glossing over some details here, I know). In the GPGPU setting, your compute shader will be invoked once for each work item that you schedule. It is up to you to define what a work item is.</p>
<p>The collection of all work items (which I will call the ‚Äúworkload‚Äù) is broken down into workgroups. All work items in a workgroup are scheduled to run together. In WebGPU, the work load is modelled as a 3-dimensional grid, where each ‚Äúcube‚Äù is a work item, and work items are grouped into bigger cuboids to form a workgroup.</p>
<figure>
	<picture loading="lazy" width="2048" height="1280">
    <source srcset="https://surma.dev/assets/workgroups.fac4b107.avif " type="image/avif">
    <img src="https://surma.dev/assets/workgroups.7887d84b.jpeg">
  </picture>
  <figcaption>This is a workload. White-bordered cubes are a work item. Red-bordered cuboids are a workgroup.</figcaption>
</figure>
<p>Finally, we have enough information to talk about the <code>@workgroup_size(x, y, z)</code> attribute, and it might even be mostly self-explanatory at this point: The attribute allows you to tell the GPU what the the size of a workgroup for this shader should be. Or in the language of the picture above, the <code>@workgroup_size</code> attribute defines the size of the red-bordered cubes. <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>√ó</mo><mi>y</mi><mo>√ó</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">x \times y \times z</annotation></semantics></math></span></span> is the number of work items per workgroup. Any skipped parameter is assumed to be 1, so <code>@workgroup_size(64)</code> is equivalent to <code>@workgroup_size(64, 1, 1)</code>.</p>
<p>Of course, the actual EUs are not arranged in the 3D grid on the chip. The aim of modelling work items in a 3D grid is to increase locality. The assumption is that it is likely that neighboring work groups will access similar areas in memory, so when running neighboring workgroups sequentially, the chances of already having values in the cache are higher, saving a couple of hundred cycles by not having to grab them from memory. However, most hardware seemingly just runs workgroups in a serial order as the difference between running a shader with <code>@workgroup_size(64)</code> or <code>@workgroup_size(8, 8)</code> is negligible. So this concept is considered somewhat legacy.</p>
<p>However, workgroups are restricted in multiple ways: <code>device.limits</code> has a bunch of properties that are worth knowing:</p>
<pre><code><span>// device.limits</span>
<span>{</span>
  <span>// ...</span>
  <span>maxComputeInvocationsPerWorkgroup</span><span>:</span> <span>256</span><span>,</span>
  <span>maxComputeWorkgroupSizeX</span><span>:</span> <span>256</span><span>,</span>
  <span>maxComputeWorkgroupSizeY</span><span>:</span> <span>256</span><span>,</span>
  <span>maxComputeWorkgroupSizeZ</span><span>:</span> <span>64</span><span>,</span>
  <span>maxComputeWorkgroupsPerDimension</span><span>:</span> <span>65535</span><span>,</span>
  <span>// ...</span>
<span>}</span>
</code></pre>
<p>The size of each dimension of a workgroup size is restricted, but even if x, y and z individually are within the limits, their product (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mi>x</mi><mo>√ó</mo><mi>y</mi><mo>√ó</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">= x \times y \times z</annotation></semantics></math></span></span>) might not be, as it has a limit of its own. Lastly, you can only have so many workgroups per dimension.</p>
<blockquote>
<p><strong>Pro tip:</strong> Don‚Äôt spawn the maximum number of threads. Despite the GPU being managed by the OS and an underlying scheduler, you may <a href="https://twitter.com/DasSurma/status/1495096911333842946">freeze your entire system with a long-running GPU program</a>.</p>
</blockquote>
<p>So what <em>is</em> the right workgroup size? It really depends on the semantics you assign the work item coordinates. I do realize that this is not really an helpful answer, so I want to give you the same advice that <a href="https://twitter.com/dakangz">Corentin</a> gave me: ‚ÄúUse [a workgroup size of] 64 unless you know what GPU you are targeting or that your workload needs something different.‚Äù It seems to be a safe number that performs well across many GPUs and allows the GPU scheduler to keep as many EUs as possible busy.</p>
<h3>Commands</h3>
<p>We have written our shader and set up the pipeline. All that‚Äôs left to do is actually invoke the GPU to execute it all. As a GPU <em>can</em> be a completely separate card with it‚Äôs own memory chip, you control it via a so-called command buffer or command queue. The command queue is a chunk of memory that contains encoded commands for the GPU to execute. The encoding is highly specific to the GPU and is taken care of by the driver. WebGPU exposes a <code>CommandEncoder</code> to tap into that functionality.</p>
<pre><code><span>const</span> commandEncoder <span>=</span> device<span>.</span><span>createCommandEncoder</span><span>(</span><span>)</span><span>;</span>
<span>const</span> passEncoder <span>=</span> commandEncoder<span>.</span><span>beginComputePass</span><span>(</span><span>)</span><span>;</span>
passEncoder<span>.</span><span>setPipeline</span><span>(</span>pipeline<span>)</span><span>;</span>
passEncoder<span>.</span><span>dispatchWorkgroups</span><span>(</span><span>1</span><span>)</span><span>;</span>
passEncoder<span>.</span><span>end</span><span>(</span><span>)</span><span>;</span>
<span>const</span> commands <span>=</span> commandEncoder<span>.</span><span>finish</span><span>(</span><span>)</span><span>;</span>
device<span>.</span>queue<span>.</span><span>submit</span><span>(</span><span>[</span>commands<span>]</span><span>)</span><span>;</span>
</code></pre>
<p><code>commandEncoder</code> has multiple methods that allows you to copy data from one GPU buffer to another and manipulate textures. It also allows you to create <code>PassEncoder</code>, which encodes the setup and invocation of pipelines. In this case, we have a compute pipline, so we have to create a compute pass, set it to use our pre-declared pipeline and finally call <code>dispatchWorkgroups(w_x, w_y, w_z)</code> to tell the GPU how many workgroups to create along each dimension. In other words, the number of times our compute shader will be invoked is equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>x</mi></msub><mo>√ó</mo><msub><mi>w</mi><mi>y</mi></msub><mo>√ó</mo><msub><mi>w</mi><mi>z</mi></msub><mo>√ó</mo><mi>x</mi><mo>√ó</mo><mi>y</mi><mo>√ó</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">w_x \times w_y \times w_z \times x \times y \times z</annotation></semantics></math></span></span>. The pass encoder, by the way, is WebGPU‚Äôs abstraction to avoid that internal, global state object I was ranting about at the start of this blog post. All data and state required to run a GPU pipeline is explicitly passed along through the pass encoder.</p>
<blockquote>
<p><strong>Abstraction:</strong> The command buffer is also the hook for the driver or operating system to let multiple applications use the GPU without them interfering with each other. When you queue up your commands, the abstraction layers below will inject additional commands into the queue to save the previous program‚Äôs state and restore your program‚Äôs state so that it feels like no one else is using the GPU.</p>
</blockquote>
<p>Running this code, we are in fact spawning 64 threads on the GPU and they do <em>absolutely nothing</em>. But it works, so that‚Äôs cool. Let‚Äôs talk about how we give the GPU some data to work.</p>
<h2>Exchanging data</h2>
<p>As promised, I won‚Äôt be using WebGPU for graphics directly, so instead I thought it‚Äôd be fun to run a physics simulation on the GPU and visualize it using Canvas2D. Maybe I am flattering myself calling it a ‚Äúphysics simulation‚Äù ‚Äî what I am doing is generating a whole bunch of circles, have them roll around on a plane in random directions and letting them collide.</p>
<p>For this to work, we need to push some simulation parameters and the initial state <em>to</em> the GPU, run the simulation <em>on</em> the GPU and read the simulation results <em>from</em> the GPU. This is arguably the hairiest part of WebGPU, as there‚Äôs a bunch of data acrobatics (not to say seemingly pointless copying), but this is what allows WebGPU to be a device-agnostic API running at the highest level of performance.</p>
<h3>Bind Group Layouts</h3>
<p>To exchange data with the GPU, we need to extend our pipeline definition with a bind group layout. A bind group is a collection of GPU entities (memory buffers, textures, samplers, etc) that are made accessible during the execution of the pipeline. The bind group <em>layout</em> pre-defines the types, purposes and uses of these GPU entities, which allows the GPU figure out how to run a pipeline most efficiently ahead of time. Let‚Äôs keep it simple in this initial step and give our pipeline access to a single memory buffer:</p>
<pre><code><span><span>const</span> bindGroupLayout <span>=</span>
 device<span>.</span><span>createBindGroupLayout</span><span>(</span><span>{</span>
    <span>entries</span><span>:</span> <span>[</span><span>{</span>
      <span>binding</span><span>:</span> <span>1</span><span>,</span>
      <span>visibility</span><span>:</span> GPUShaderStage<span>.</span><span>COMPUTE</span><span>,</span>
      <span>buffer</span><span>:</span> <span>{</span>
        <span>type</span><span>:</span> <span>"storage"</span><span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>]</span><span>,</span>
  <span>}</span><span>)</span><span>;</span></span>
<span>
<span>const</span> pipeline <span>=</span> device<span>.</span><span>createComputePipeline</span><span>(</span><span>{</span></span>
<span>  <span>layout</span><span>:</span> device<span>.</span><span>createPipelineLayout</span><span>(</span><span>{</span>
    <span>bindGroupLayouts</span><span>:</span> <span>[</span>bindGroupLayout<span>]</span><span>,</span>
  <span>}</span><span>)</span><span>,</span></span>
<span>  <span>compute</span><span>:</span> <span>{</span>
    module<span>,</span>
    <span>entryPoint</span><span>:</span> <span>"main"</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span></code></pre><p>The <code>binding</code> number can be freely chosen and is used to tie a variable in our WGSL code to the contents of the buffer in this slot of the bind group layout. Our <code>bindGroupLayout</code> also defines the purpose for each buffer, which in this case is <code>"storage"</code>. Another option is <code>"read-only-storage"</code>, which is read-only (duh!), and allows the GPU to make further optimizations on the basis that this buffer will never be written to and as such doesn‚Äôt need to be synchronized. The last possible value for the buffer type is <code>"uniform"</code>, which in the context of a compute pipeline is mostly functionally equivalent to a storage buffer.</p>
<p>The bind group layout is in place. Now we can create the bind group itself, containing the actual instances of the GPU entities the bind group layout expects. Once that bind group with the buffer inside is in place, the compute shader can fill it with data and we can read it from the GPU. But there‚Äôs a hurdle: Staging Buffers.</p>
<h3>Staging Buffers</h3>
<p>I will say it again: GPUs are heavily optimized for throughput at the cost of latency. A GPU needs to be able feed data to the cores at an incredibly high rate to sustain that throughput. Fabian did some <a href="https://fgiesen.wordpress.com/2011/07/04/a-trip-through-the-graphics-pipeline-2011-part-4/#:~:text=that%E2%80%99s%203.3%20GB/s%20just%20for%20texture%20request%20payloads.%20Lower%20bound%2C">back-of-napkin math</a> in his blog post series from 2011, and arrived at the conclusion that GPUs need to sustain 3.3GB/s <em>just for texture samples</em> for a shader running at 1280x720 resolution. To accommodate today‚Äôs graphics demands, GPUs need shovel data even faster. This is only possible to achieve if the memory of the GPU is very tightly integrated with the cores. This tight integration makes it hard to also expose the same memory to the host machine for reading and writing.</p>
<p>Instead, GPUs have additional memory banks that are accessible to both the host machine as well as the GPU, but are not as tightly integrated and can‚Äôt provide data as fast. Staging buffers are buffers that are allocated in this intermediate memory realm and can be <a href="https://en.wikipedia.org/wiki/Memory-mapped_I/O">mapped</a> to the host system for reading and writing. To read data from the GPU, we copy data from an internal, high-performance buffer to a staging buffer, and then map the staging buffer to the host machine so we can read the data back into main memory. For writing, the process is the same but in reverse.</p>
<p>Back to our code: We will create a writable buffer and add it to the bind group, so that it can be written to by the compute shader. We will also create a second buffer with the same size that will act as a staging buffer. Each buffer is created with a <code>usage</code> bitmask, where you can declare how you intend to use that buffer. The GPU will then figure out where the buffer should be located to fulfill all these use-cases or throw an error if the combination of flags is unfulfillable.</p>
<pre><code><span>const</span> <span>BUFFER_SIZE</span> <span>=</span> <span>1000</span><span>;</span>

<span>const</span> output <span>=</span> device<span>.</span><span>createBuffer</span><span>(</span><span>{</span>
  <span>size</span><span>:</span> <span>BUFFER_SIZE</span><span>,</span>
  <span>usage</span><span>:</span> GPUBufferUsage<span>.</span><span>STORAGE</span> <span>|</span> GPUBufferUsage<span>.</span><span>COPY_SRC</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> stagingBuffer <span>=</span> device<span>.</span><span>createBuffer</span><span>(</span><span>{</span>
  <span>size</span><span>:</span> <span>BUFFER_SIZE</span><span>,</span>
  <span>usage</span><span>:</span> GPUBufferUsage<span>.</span><span>MAP_READ</span> <span>|</span> GPUBufferUsage<span>.</span><span>COPY_DST</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> bindGroup <span>=</span> device<span>.</span><span>createBindGroup</span><span>(</span><span>{</span>
  <span>layout</span><span>:</span> bindGroupLayout<span>,</span>
  <span>entries</span><span>:</span> <span>[</span><span>{</span>
    <span>binding</span><span>:</span> <span>1</span><span>,</span>
    <span>resource</span><span>:</span> <span>{</span>
      <span>buffer</span><span>:</span> output<span>,</span>
    <span>}</span><span>,</span>
  <span>}</span><span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span>
</code></pre>
<p>Note that <code>createBuffer()</code> returns a <code>GPUBuffer</code>, not an <code>ArrayBuffer</code>. They can‚Äôt be read or written to just yet. For that, they need to be mapped, which is a separate API call and will only succeed for buffers that have <code>GPUBufferUsage.MAP_READ</code> or <code>GPUBufferUsage.MAP_WRITE</code>.</p>
<blockquote>
<p><strong>TypeScript:</strong> I found TypeScript to be quite helpful when exploring new APIs. Luckily, Chrome‚Äôs WebGPU team maintains <a href="https://npm.im/@webgpu/types"><code>@webgpu/types</code></a> so you can enjoy accurate auto-completion.</p>
</blockquote>
<p>Now that we not only have the bind group <em>layout</em>, but even the actual bind group itself, we need to update our dispatch code to make use of this bind group. Afterwards we map our staging buffer to read the results back into JavaScript.</p>
<pre><code><span><span>const</span> commandEncoder <span>=</span> device<span>.</span><span>createCommandEncoder</span><span>(</span><span>)</span><span>;</span>
<span>const</span> passEncoder <span>=</span> commandEncoder<span>.</span><span>beginComputePass</span><span>(</span><span>)</span><span>;</span>
passEncoder<span>.</span><span>setPipeline</span><span>(</span>pipeline<span>)</span><span>;</span></span>
<span>passEncoder<span>.</span><span>setBindGroup</span><span>(</span><span>0</span><span>,</span> bindGroup<span>)</span><span>;</span></span>
<span>passEncoder.dispatchWorkgroups(1);</span>
<span>passEncoder<span>.</span><span>dispatchWorkgroups</span><span>(</span>Math<span>.</span><span>ceil</span><span>(</span><span>BUFFER_SIZE</span> <span>/</span> <span>64</span><span>)</span><span>)</span><span>;</span></span>
<span>passEncoder<span>.</span><span>end</span><span>(</span><span>)</span><span>;</span></span>
<span>commandEncoder<span>.</span><span>copyBufferToBuffer</span><span>(</span>
  output<span>,</span>
  <span>0</span><span>,</span> <span>// Source offset</span>
  stagingBuffer<span>,</span>
  <span>0</span><span>,</span> <span>// Destination offset</span>
  <span>BUFFER_SIZE</span>
<span>)</span><span>;</span></span>
<span><span>const</span> commands <span>=</span> commandEncoder<span>.</span><span>finish</span><span>(</span><span>)</span><span>;</span>
device<span>.</span>queue<span>.</span><span>submit</span><span>(</span><span>[</span>commands<span>]</span><span>)</span><span>;</span>
</span>
<span><span>await</span> stagingBuffer<span>.</span><span>mapAsync</span><span>(</span>
  GPUMapMode<span>.</span><span>READ</span><span>,</span>
  <span>0</span><span>,</span> <span>// Offset</span>
  <span>BUFFER_SIZE</span> <span>// Length</span>
 <span>)</span><span>;</span>
<span>const</span> copyArrayBuffer <span>=</span>
  stagingBuffer<span>.</span><span>getMappedRange</span><span>(</span><span>0</span><span>,</span> <span>BUFFER_SIZE</span><span>)</span><span>;</span>
<span>const</span> data <span>=</span> copyArrayBuffer<span>.</span><span>slice</span><span>(</span><span>)</span><span>;</span>
stagingBuffer<span>.</span><span>unmap</span><span>(</span><span>)</span><span>;</span>
console<span>.</span><span>log</span><span>(</span><span>new</span> <span>Float32Array</span><span>(</span>data<span>)</span><span>)</span><span>;</span></span></code></pre><p>Since we added a bind group layout to our pipeline, any invocation without providing a bind group would now fail. After we define our ‚Äúpass‚Äù, we add an additional command via our command encoder to copy the data from our output buffer to the staging buffer and submit our command buffer to the queue. The GPU will start working through the command queue. We don‚Äôt know when the GPU will be done exactly, but we can already submit our request for the <code>stagingBuffer</code> to be mapped. This function is async as it needs to wait until the command queue has been fully processed. When the returned promise resolves, the buffer is mapped, but not exposed to JavaScript yet. <code>stagingBuffer.getMappedRange()</code> let‚Äôs us request for a subsection (or the entire buffer) to be exposed to JavaScript as a good ol‚Äô <code>ArrayBuffer</code>. This is real, mapped GPU memory, meaning the data will disappear (the <code>ArrayBuffer</code> will be ‚Äúdetached‚Äù), when <code>stagingBuffer</code> gets unmapped, so I‚Äôm using <code>slice()</code> to create JavaScript-owned copy.</p>
<figure>
	<picture loading="lazy" width="1024" height="429">
    <source srcset="https://surma.dev/assets/emptybuffer.e9a718cf.avif " type="image/avif">
    <img src="https://surma.dev/assets/emptybuffer.983daecd.jpeg">
  </picture>
  <figcaption>Not very exciting, but we copied those zeroes from the GPU‚Äôs memory.</figcaption>
</figure>
<p>Something other than zeroes would probably be a bit more convincing. Before we start doing any advanced calculation on our GPU, let‚Äôs put some hand-picked data into our buffer as proof that our pipeline is <em>indeed</em> working as intended. This is our new compute shader code, with extra spacing for clarity.</p>
<pre><code><span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>1</span><span>)</span>
var<span>&lt;</span>storage<span>,</span> read_write<span>&gt;</span> output<span>:</span> array<span>&lt;</span><span>f32</span><span>&gt;</span><span>;</span>

<span>@</span>compute <span>@</span><span>workgroup_size</span><span>(</span><span>64</span><span>)</span>
<span>fn</span> <span>main</span><span>(</span>

  <span>@</span><span>builtin</span><span>(</span>global_invocation_id<span>)</span>
  global_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>

  <span>@</span><span>builtin</span><span>(</span>local_invocation_id<span>)</span>
  local_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>

<span>)</span> <span>{</span>
  output<span>[</span>global_id<span>.</span>x<span>]</span> <span>=</span>
    <span>f32</span><span>(</span>global_id<span>.</span>x<span>)</span> <span>*</span> <span>1000</span><span>.</span> <span>+</span> <span>f32</span><span>(</span>local_id<span>.</span>x<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>The first two lines declare a module-scope variable called <code>output</code>, which is a dynamically-sized array of <code>f32</code>. The attributes declare where the data comes from: From the buffer in our first (0th) binding group, the entry with <code>binding</code> value 1. The length of the array will automatically reflect the length of the underlying buffer (rounded down).</p>
<blockquote>
<p><strong>Variables:</strong> WGSL diverges from Rust in that a variable declared with <code>let</code> is immutable. If you want a variable to be mutable, they keyword to use is <code>var</code>.</p>
</blockquote>
<p>The signature of our <code>main()</code> function has been augmented with two parameters: <code>global_id</code> and <code>local_id</code>. I could have chosen any name ‚Äî their value is determined by the attributes associated with them: The <code>global_invocation_id</code> is a built-in value that corresponds to the global x/y/z coordinates of this shader invocation in the work <em>load</em>. The <code>local_invocation_id</code> is the x/y/z coordinates of this shader vocation in the work <em>group</em>.</p>
<figure>
	<picture loading="lazy" width="2048" height="1280">
    <source srcset="https://surma.dev/assets/coordinates.436c4f95.avif " type="image/avif">
    <img src="https://surma.dev/assets/coordinates.dd59e014.jpeg">
  </picture>
  <figcaption>An example of three work items a, b and c marked in the workload.</figcaption>
</figure>
<p>This image shows one possible interpretation of the coordinate system for a workload with <code>@workgroup_size(4, 4, 4)</code>. It is up to you to define what the coordinate system is for your use-case. If we agreed on the axes as drawn above, we‚Äôd see the following <code>main()</code> parameters for a, b and c:</p>
<ul>
<li>a:
<ul>
<li><code>local_id=(x=0, y=0, z=0)</code></li>
<li><code>global_id=(x=0, y=0, z=0)</code></li>
</ul>
</li>
<li>b:
<ul>
<li><code>local_id=(x=0, y=0, z=0)</code></li>
<li><code>global_id=(x=4, y=0, z=0)</code></li>
</ul>
</li>
<li>c:
<ul>
<li><code>local_id=(x=1, y=1, z=0)</code></li>
<li><code>global_id=(x=5, y=5, z=0)</code></li>
</ul>
</li>
</ul>
<p>In our shader, we have <code>@workgroup_size(64, 1, 1)</code>, so <code>local_id.x</code> will range from 0 to 63. To be able to inspect both values, I am ‚Äúencoding‚Äù them into a single number. Note that WGSL is strictly typed: Both <code>local_id</code> and <code>global_id</code> are <code>vec3&lt;u32&gt;</code>, so we have to explicitly cast their values to <code>f32</code> to be able to assign them to our <code>f32</code> output buffer.</p>
<figure>
	<picture loading="lazy" width="1024" height="565">
    <source srcset="https://surma.dev/assets/fullbuffer.1d2e3f75.avif " type="image/avif">
    <img src="https://surma.dev/assets/fullbuffer.d31e6ebc.jpeg">
  </picture>
  <figcaption>Actual values filled in by the GPU. Notice how the local invocation ID starts wrapping around after 63, while the global invocation ID keeps going.</figcaption>
</figure>
<p>And this proves that our compute shader is indeed invoked for each value in the output memory and fills it with a unique value. We won‚Äôt know in which order this data has been filled in, as that‚Äôs intentionally unspecified and left up to the GPU‚Äôs scheduler.</p>
<h3>Overdispatching</h3>
<p>The astute observer might have noticed that the total number of shader invocations (<code>Math.ceil(BUFFER_SIZE / 64) * 64</code>) will result in <code>global_id.x</code> getting bigger than the length of our array, as each <code>f32</code> takes up 4 bytes. Luckily, accessing an array is safe-guarded by an implicit clamp, so every write past the end of the array will end up writing to the last element of the array. That avoids memory access faults, but might still generate unusable data. And indeed, if you check the last 3 elements of the returned buffer, you‚Äôll find the numbers 247055, 248056 and 608032. It‚Äôs up to us to prevent that from happening in our shader code with an early exit:</p>
<pre><code><span><span>fn</span> <span>main</span><span>(</span> <span>/* ... */</span><span>)</span> <span>{</span></span>
<span>  <span>if</span><span>(</span>global_id<span>.</span>x <span>&gt;=</span> <span>arrayLength</span><span>(</span><span>&amp;</span>output<span>)</span><span>)</span> <span>{</span>
    <span>return</span><span>;</span>
  <span>}</span></span>
<span>  output<span>[</span>global_id<span>.</span>x<span>]</span> <span>=</span>
    <span>f32</span><span>(</span>global_id<span>.</span>x<span>)</span> <span>*</span> <span>100</span><span>.</span> <span>+</span> <span>f32</span><span>(</span>local_id<span>.</span>x<span>)</span><span>;</span>
<span>}</span></span></code></pre><p>If you want, you can run this <a href="https://surma.dev/things/webgpu/step1/index.html">demo</a> and inspect the full source.</p>
<h3>A structure for the madness</h3>
<p>Our goal here is to have a whole lotta balls moving through 2D space and have happy little collisions. For that, each ball needs to have a radius, a position and a velocity vector. We could just continue working on <code>array&lt;f32&gt;</code>, and say the first float is the first ball‚Äôs x position, the second float is the first ball‚Äôs y position and so on, and so forth. That‚Äôs not what I would call ergonomic. Luckily, WGSL allows us to define our own structs to tie multiple pieces of data together in a neat bag.</p>
<blockquote>
<p><strong>Old news:</strong> If you know what memory alignment is, you can skip this section (although do take a look at the code sample). If you don‚Äôt know what it is, I won‚Äôt really explain the why, but show you how it manifests and how to work with it.</p>
</blockquote>
<p>So it makes sense to define a <code>struct Ball</code> with all these components and turn our <code>array&lt;f32&gt;</code> into <code>array&lt;Ball&gt;</code>. The downside of all this: we have to talk about <a href="https://en.wikipedia.org/wiki/Data_structure_alignment">alignment</a>.</p>
<pre><code><span><span>struct</span> <span>Ball</span> <span>{</span>
  radius<span>:</span> <span>f32</span><span>,</span>
  position<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
  velocity<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
<span>}</span></span>
<span>
<span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>1</span><span>)</span></span>
<span>var&lt;storage, read_write&gt; output: array&lt;f32&gt;;</span>
<span>var<span>&lt;</span>storage<span>,</span> read_write<span>&gt;</span> output<span>:</span> array<span>&lt;</span><span>Ball</span><span>&gt;</span><span>;</span></span>
<span>
<span>@</span>compute <span>@</span><span>workgroup_size</span><span>(</span><span>64</span><span>)</span>
<span>fn</span> <span>main</span><span>(</span>
  <span>@</span><span>builtin</span><span>(</span>global_invocation_id<span>)</span> global_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>
  <span>@</span><span>builtin</span><span>(</span>local_invocation_id<span>)</span> local_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>
<span>)</span> <span>{</span>
  <span>let</span> num_balls <span>=</span> <span>arrayLength</span><span>(</span><span>&amp;</span>output<span>)</span><span>;</span>
  <span>if</span><span>(</span>global_id<span>.</span>x <span>&gt;=</span> num_balls<span>)</span> <span>{</span>
    <span>return</span><span>;</span>
  <span>}</span>
</span>
<span>  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>radius <span>=</span> <span>999</span><span>.</span><span>;</span>
  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>position <span>=</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span>global_id<span>.</span>xy<span>)</span><span>;</span>
  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>velocity <span>=</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span>local_id<span>.</span>xy<span>)</span><span>;</span></span>
<span><span>}</span></span></code></pre><p>If you run this <a href="https://surma.dev/things/webgpu/step2/index.html">demo</a>, you‚Äôll see this in your console:</p>
<figure>
	<picture loading="lazy" width="479" height="440">
    <source srcset="https://surma.dev/assets/alignment.3ee470e2.avif " type="image/avif">
    <img src="https://surma.dev/assets/alignment.47a211c5.jpeg">
  </picture>
  <figcaption>The struct has a hole (padding) in its memory layout due to alignment constraints.</figcaption>
</figure>
<p>I put <code>999</code> the first field of the struct to make it easy to see where the struct begins in the buffer. There‚Äôs a total of 6 numbers until we reach the next <code>999</code>, which is a bit surprising because the struct really only has 5 numbers to store: <code>radius</code>, <code>position.x</code>, <code>position.y</code>, <code>velocity.x</code> and <code>velocity.y</code>. Taking a closer look, it is clear that the number after <code>radius</code> is always 0. This is because of alignment.</p>
<p>Each WGSL data type has well-defined <a href="https://gpuweb.github.io/gpuweb/wgsl/#alignment-and-size">alignment requirements</a>. If a data type has an alignment of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span>, it means that a value of that data type can only be stored at a memory address that is a multiple of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span>. <code>f32</code> has an alignment of 4, while <code>vec2&lt;f32&gt;</code> has an alignment of 8. If we assume our struct starts at address 0, then the <code>radius</code> field can be stored at address 0, as 0 is a multiple of 4. The next field in the struct is <code>vec2&lt;f32&gt;</code>, which has an alignment of 8. However, the first free address after <code>radius</code> is 4, which is <em>not</em> a multiple of 8. To remedy this, the compiler adds padding of 4 bytes to get to the next address that is a multiple of 8. This explains what we see an unused field with the value 0 in the DevTools console.</p>
<figure>
  <picture loading="lazy" width="797" height="605">
    <source srcset="https://surma.dev/assets/alignmenttable.fbedb57c.avif " type="image/avif">
    <img src="https://surma.dev/assets/alignmenttable.b958880e.jpeg">
  </picture>
  <figcaption>
<p>The (shortened) <a href="https://gpuweb.github.io/gpuweb/wgsl/#alignment-and-size">alignment table</a> from the WGSL spec.</p>
  </figcaption>
</figure>
<p>Now that we know how our struct is laid out in memory, we can populate it from JavaScript to generate our initial state of balls and also read it back to visualize it.</p>
<h3>Input &amp; Output</h3>
<p>We have successfully managed to read data from the GPU, bring it to JavaScript and ‚Äúdecode‚Äù it. It‚Äôs now time to tackle the other direction. We need to generate the initial state of all our balls in JavaScript and give it to the GPU so it can run the compute shader on it. Generating the initial state is fairly straight forward:</p>
<pre><code><span>let</span> inputBalls <span>=</span> <span>new</span> <span>Float32Array</span><span>(</span><span>new</span> <span>ArrayBuffer</span><span>(</span><span>BUFFER_SIZE</span><span>)</span><span>)</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>NUM_BALLS</span><span>;</span> i<span>++</span><span>)</span> <span>{</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>0</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>2</span><span>,</span> <span>10</span><span>)</span><span>;</span> <span>// radius</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>0</span><span>;</span> <span>// padding</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>2</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>0</span><span>,</span> ctx<span>.</span>canvas<span>.</span>width<span>)</span><span>;</span> <span>// position.x</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>3</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>0</span><span>,</span> ctx<span>.</span>canvas<span>.</span>height<span>)</span><span>;</span> <span>// position.y</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>4</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>-</span><span>100</span><span>,</span> <span>100</span><span>)</span><span>;</span> <span>// velocity.x</span>
  inputBalls<span>[</span>i <span>*</span> <span>6</span> <span>+</span> <span>5</span><span>]</span> <span>=</span> <span>randomBetween</span><span>(</span><span>-</span><span>100</span><span>,</span> <span>100</span><span>)</span><span>;</span> <span>// velocity.y</span>
<span>}</span>
</code></pre>
<blockquote>
<p><strong>Buffer-backed-object:</strong> With more complex data structures, it can get quite tedious to manipulate the data from JavaScript. While originally written for worker use-cases, my library <a href="https://github.com/GoogleChromeLabs/buffer-backed-object"><code>buffer-backed-object</code></a> can come in handy here!</p>
</blockquote>
<p>We also already know how to expose a buffer to our shader. We just need to adjust our pipeline bind group layout to expect another buffer:</p>
<pre><code><span><span>const</span> bindGroupLayout <span>=</span> device<span>.</span><span>createBindGroupLayout</span><span>(</span><span>{</span>
  <span>entries</span><span>:</span> <span>[</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>0</span><span>,</span>
      <span>visibility</span><span>:</span> GPUShaderStage<span>.</span><span>COMPUTE</span><span>,</span>
      <span>buffer</span><span>:</span> <span>{</span>
        <span>type</span><span>:</span> <span>"read-only-storage"</span><span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>1</span><span>,</span>
      <span>visibility</span><span>:</span> GPUShaderStage<span>.</span><span>COMPUTE</span><span>,</span>
      <span>buffer</span><span>:</span> <span>{</span>
        <span>type</span><span>:</span> <span>"storage"</span><span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span>
  <span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span></code></pre><p>... and create a GPU buffer that we can bind using our bind group:</p>
<pre><code><span><span>const</span> input <span>=</span> device<span>.</span><span>createBuffer</span><span>(</span><span>{</span>
  <span>size</span><span>:</span> <span>BUFFER_SIZE</span><span>,</span>
  <span>usage</span><span>:</span> GPUBufferUsage<span>.</span><span>STORAGE</span> <span>|</span> GPUBufferUsage<span>.</span><span>COPY_DST</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span>
<span>
<span>const</span> bindGroup <span>=</span> device<span>.</span><span>createBindGroup</span><span>(</span><span>{</span>
  <span>layout</span><span>:</span> bindGroupLayout<span>,</span>
  <span>entries</span><span>:</span> <span>[</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>0</span><span>,</span>
      <span>resource</span><span>:</span> <span>{</span>
        <span>buffer</span><span>:</span> input<span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span></span>
<span>    <span>{</span>
      <span>binding</span><span>:</span> <span>1</span><span>,</span>
      <span>resource</span><span>:</span> <span>{</span>
        <span>buffer</span><span>:</span> output<span>,</span>
      <span>}</span><span>,</span>
    <span>}</span><span>,</span>
  <span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span></span></code></pre><p>Now for the new part: Sending data to the GPU. Just like with reading data, we technically have to create a staging buffer that we can map, copy our data into the staging buffer and then issue a command to copy our data from the staging buffer into the storage buffer. However, WebGPU offers a convenience function that will choose the most efficient way of getting our data into the storage buffer for us, even if that involves creating a temporary staging buffer on the fly:</p>
<pre><code>device<span>.</span>queue<span>.</span><span>writeBuffer</span><span>(</span>input<span>,</span> <span>0</span><span>,</span> inputBalls<span>)</span><span>;</span>
</code></pre>
<p>That‚Äôs it? That‚Äôs it! We don‚Äôt even need a command encoder. We can just put this command directly into the command queue. <code>device.queue</code> offers some other, similar convenience functions for textures as well.</p>
<p>Now we need to bind this new buffer to a variable in WGSL and do something with it:</p>
<pre><code><span><span>struct</span> <span>Ball</span> <span>{</span>
  radius<span>:</span> <span>f32</span><span>,</span>
  position<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
  velocity<span>:</span> vec2<span>&lt;</span><span>f32</span><span>&gt;</span><span>,</span>
<span>}</span>
</span>
<span><span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>0</span><span>)</span>
var<span>&lt;</span>storage<span>,</span> read<span>&gt;</span> input<span>:</span> array<span>&lt;</span><span>Ball</span><span>&gt;</span><span>;</span></span>
<span>
<span>@</span><span>group</span><span>(</span><span>0</span><span>)</span> <span>@</span><span>binding</span><span>(</span><span>1</span><span>)</span>
var<span>&lt;</span>storage<span>,</span> read_write<span>&gt;</span> output<span>:</span> array<span>&lt;</span><span>Ball</span><span>&gt;</span><span>;</span>
</span>
<span><span>const</span> <span>TIME_STEP</span><span>:</span> <span>f32</span> <span>=</span> <span>0.016</span><span>;</span></span>
<span>
<span>@</span>compute <span>@</span><span>workgroup_size</span><span>(</span><span>64</span><span>)</span>
<span>fn</span> <span>main</span><span>(</span>
  <span>@</span><span>builtin</span><span>(</span>global_invocation_id<span>)</span>
  global_id <span>:</span> vec3<span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span>
<span>)</span> <span>{</span>
  <span>let</span> num_balls <span>=</span> <span>arrayLength</span><span>(</span><span>&amp;</span>output<span>)</span><span>;</span>
  <span>if</span><span>(</span>global_id<span>.</span>x <span>&gt;=</span> num_balls<span>)</span> <span>{</span>
    <span>return</span><span>;</span>
  <span>}</span></span>
<span>  output<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>position <span>=</span>
    input<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>position <span>+</span>
    input<span>[</span>global_id<span>.</span>x<span>]</span><span>.</span>velocity <span>*</span> <span>TIME_STEP</span><span>;</span></span>
<span><span>}</span></span></code></pre><p>I hope that the vast majority of this shader code contains no surprises for you at this point.</p>
<figure>
  <video width="512" height="512" src="https://surma.dev/assets/step3.19ad443e.webm" type="video/webm" autoplay="" muted="" loop="" controls=""></video>
  <figcaption>Every frame, WebGPU is used to update the position of the balls. They are drawn to screen using Canvas2D.</figcaption>
</figure>
<p>Lastly, all we need to do is read the <code>output</code> buffer back into JavaScript, write some Canvas2D code to visualize the contents of the buffer and put it all in a <code>requestAnimationFrame()</code> loop. You can see the result this <a href="https://surma.dev/things/webgpu/step3/index.html">demo</a>.</p>
<h2>Performance</h2>
<p>The previous demo just moves each ball along their velocity vector. Not exactly thrilling or computationally complex. Before we look at the performance characteristics of our creation, let me drop in some proper physics calculations in the shader. I won‚Äôt explain them here ‚Äî the blog post is long enough as it is ‚Äî but I will say that I took the maximally na√Øve approach: every ball checks for collisions <em>with every other ball</em>. If you are curious, you can take a look at the source code of the <a href="https://surma.dev/things/webgpu/step4/index.html">final demo</a>, which also contains links to the resources I used to write the physics-y bits.</p>
<figure>
  <video width="512" height="512" src="https://surma.dev/assets/step4.8649df2d.webm" type="video/webm" autoplay="" muted="" loop="" controls=""></video>
  <figcaption>... now with bouncy walls and bouncy balls!</figcaption>
</figure>
<p>I don‚Äôt want to take any exact measurements of this experiment as I haven‚Äôt optimized the physics algorithm nor my usage of WebGPU. However, the fact that even this na√Øve implementation performs really well (on my M1 MacBook Air) is impressive to me. I can go to around 2500 balls before we drop below 60fps. However, looking at the trace, it‚Äôs clear that at 2500 balls the bottleneck is Canvas2D trying to draw the scene, not the WebGPU calculations.</p>
<figure>
	<picture loading="lazy" width="1024" height="625">
    <source srcset="https://surma.dev/assets/performance.63b22948.avif " type="image/avif">
    <img src="https://surma.dev/assets/performance.9d5db577.jpeg">
  </picture>
  <figcaption>At 14000 balls, the raw GPU computation time reaches ~16ms on a M1 MBA.</figcaption>
</figure>
<p>To see how fast this really is, I disabled rendering and instead used <a href="https://developer.mozilla.org/en-US/docs/Web/API/Performance/measure"><code>performance.measure()</code></a> to see how many balls I can simulate before exhausting my frame budget of 16ms. This happens at around 14000 balls on my machine. Something this unoptimized running this fast really makes me drunk on power with how much computational power WebGPU gives me access to.</p>
<h2>Stability &amp; Availability</h2>
<p>WebGPU has been worked on for a while and I think the standards group is eager to declare the API as stable. That being said, the API is only available in Chrome and Firefox behind a flag. I‚Äôm optimistic about Safari shipping this API but at the time of writing there‚Äôs nothing to see in Safari TP just yet.</p>
<p>In terms of stability, some changes landed even while I was doing the research for this article. For example, the syntax for attributes was changed from <code>[[stage(compute), workgroup_size(64)]]</code> to <code>@compute @workgroup_size(64)</code>. At the time of writing, Firefox is still on the old syntax. <code>passEncoder.end()</code> used to be <code>passEncoder.endPass()</code>. There are also some things in the spec that haven‚Äôt been implemented in any browser yet like <a href="https://gpuweb.github.io/gpuweb/#dom-gpuprogrammablestage-constants">shader constants</a> or the API being available on mobile devices.</p>
<p>Basically what I am saying is: Expect some more breaking changes to happen while the browsers and standards folks are on the home stretch of this API‚Äôs journey to ‚ú®stable‚ú®.</p>
<h2>Conclusion</h2>
<p>Having a modern API to talk to GPUs on the web is going to be very interesting. After investing time to overcome the initial learning curve, I really feel empowered to run massively parallel workloads on the GPU using JavaScript. There is also <a href="https://github.com/gfx-rs/wgpu">wgpu</a>, which implements the WebGPU API in Rust, allowing you to use the API outside the browser. wgpu also support WebAssembly as a compile target, so you could run your WebGPU program natively outside the browser and inside the browser via WebAssembly. Fun fact: <a href="https://deno.land/">Deno</a> is the first runtime also support WebGPU out of the box (thanks to wgpu).</p>
<p>If you have questions or are running into problems, there is <a href="https://matrix.to/#/#WebGPU:matrix.org">a Matrix channel</a> with many WebGPU users, browser engineers and standards folks that have been incredibly helpful to me. Go get your feet wet! Exciting times.</p>
<p><em>Thanks to <a href="https://twitter.com/tojiro">Brandon Jones</a> for proof-reading this article and the <a href="https://matrix.to/#/#WebGPU:matrix.org">WebGPU Matrix channel</a> for answering all my questions.</em></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring Linear A (135 pts)]]></title>
            <link>https://lineara.xyz/</link>
            <guid>36750743</guid>
            <pubDate>Sun, 16 Jul 2023 19:47:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lineara.xyz/">https://lineara.xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=36750743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="help_menu" onclick="(function(){ help_menu.style.display='none';})();">
<p><span>  &nbsp;Click on words to filter inscriptions.<br></span>
<span>  <br></span>
<span>  &nbsp;'<b>w</b>' - highlight words according to frequency in the corpus.<br></span>
<span>  <br></span>
<span>  &nbsp;'<b>1 to 9</b>' - save the current search terms under that number. <br></span>
<span>  &nbsp;'<b>Shift + 1 to 9</b>' - retrieve the search terms saved under that number. <br></span>
<span>  &nbsp;'<b>/</b>' - Search for an inscription. To use regular expressions, precede regular<br></span>
<span>              expression operators with a '\', e.g. -JA\$ to search for -JA<br></span>
<span>              at the end of words.<br></span>
<span>  <br></span>
<span>  &nbsp;Hover the mouse over an inscription and press:<br></span>
<span>  &nbsp;'<b>i</b>' - to copy an image of the inscription to the clipboard.<br></span>
<span>  &nbsp;'<b>y</b>' - to show John Younger's commentary for the inscription.<br></span>
<span>  &nbsp;'<b>t</b>' - to toggle between translation and transliteration.<br></span>
<span>  &nbsp;'<b>z</b>' - to zoom/unzoom an inscription.<br></span></p><a href="https://linearb.xyz/" target="_blank">
    <p>Linear B</p>
  </a>
  <a href="https://linear0.xyz/" target="_blank">
    <p>Cretan Hieroglyphics</p>
  </a>
  <a href="https://linearc.xyz/" target="_blank">
    <p>Cypro-Minoan</p>
  </a>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Underwater ears everywhere (355 pts)]]></title>
            <link>https://computer.rip/2023-07-15-underwater-ears-everywhere.html</link>
            <guid>36750716</guid>
            <pubDate>Sun, 16 Jul 2023 19:44:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer.rip/2023-07-15-underwater-ears-everywhere.html">https://computer.rip/2023-07-15-underwater-ears-everywhere.html</a>, See on <a href="https://news.ycombinator.com/item?id=36750716">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<h2>&gt;&gt;&gt; 2023-07-15 underwater ears everywhere</h2>

<p>Programming note: the subscribe link was broken for a while because I am
bad at computers (yet another case of "forgot to enable the systemd unit").
It's fixed now. The unsubscribe link was also broken and is now fixed but,
you know, maybe that was a feature. Did wonders for reader retention.</p>
<p>You may have seen some recent press coverage about events surrounding the
<em>Titanic</em> and another notable loss at sea. I'm not going to rehash much of
anything around the <em>Titan</em> because it's sort of an exhaustively covered topic
in the mainstream press... although I will defend the Logitech controller by
noting that Playstation-style controllers are extremely popular interfaces in
robotics and 3D navigation (two symmetric analog sticks, unlike other major
game controllers), and considering the genuine PS4 controller's terrible
Bluetooth pairing UX with non-Playstation devices, the Logitech is probably a
more reliable choice. And they did have spares on board!</p>
<p>I actually want to talk a bit about remote sensing, but of a rather different
kind than I usually mention: hydrophones and wide-area sonar. This
little-discussed military surveillance technology played a major role in the
saga of the <em>Titan,</em> and it's one that seems poorly understood by both
journalists and internet randos. I've seen a lot of Bad Takes about the Navy's
involvement in <em>Titan</em> and I want to suggest a few things that might cause you
to interpret the situation differently.</p>
<p>Submarines are very difficult to detect. This is a bad property for tourist
ventures to the deep sea, but a very useful property to the military. Further,
radio communications underwater are extremely difficult. Salt water attenuates
radio signals very quickly, and while the effect decreases as you go to lower
frequencies, it never goes away. Even the US Navy's sophisticated VLF systems
require submarines to be relatively close to the surface (or rather use a wire
antenna relatively close to the surface) for reception---VLF signals only
penetrate seawater by up to about 40 meters. ELF offers better penetration into
hundreds of meters, but ELF facilities are extremely expensive to build and
operate and the receive antennas are formidably large, so the US Navy retired
its ELF infrastructure in 2004.</p>
<p>For this reason, submersibles like <em>Titan</em> communicate with their surface
support vessels via acoustic modems. This method is surprisingly reliable but
produces a very low bitrate, thus the limitation of text messaging. Similar
technology is used in deep-sea oil exploration, <em>Titan</em> likely used a
commercial product for the data link.</p>
<p>The thing that propagates best underwater, in fact far better than above water
and even better as you get deeper, is sound. The potential of sound for
detecting and locating submarines is well-known. The first prominent use of
this approach, widely called sonar, came about during the First World War when
an anti-submarine surface ship successfully detected a submarine directly below
it via reflected sound. This type of sonar works well for locating nearby
submarines, but it is an <em>active</em> technique. That is, an active sonar must
<em>emit</em> a sound in order to receive the reflection. This is actually quite
undesirable for many military applications, because emitting a sound reveals
the presence (and with sufficient receiving equipment, location) of the sonar
device.  Anti-submarine ships stopped using active sonar on a regular basis
fairly quickly, since it prominently advertised their presence to all of the
submarines in the area.</p>
<p>Much more appealing is <em>passive</em> sonar, which works by listening for the sounds
naturally created by underwater vehicles. With a sensitive directional
hydrophone (an underwater microphone), you can hear the noise created by the
screws of a submarine. By rotating the directional hydrophone, you can find the
point of peak amplitude and thus the bearing to the submarine. This basic
submarine hunting technique remains the state of the art today, but the receiving
equipment has become far more capable and automated.</p>
<p>There is an arms race here, an arms race of quietness. I am resisting here the
urge to quote the entire monologue from the beginning of <em>The Hunt for Red
October,</em> but rest assured that [the Americans] will tremble again, at the
sound of [the Soviet's] silence. In practice the magnetohydrodynamic propulsion
technology depicted on the <em>Red October</em> has never proven very practical for
submarines, although it was demonstrated in one very futuristic surface vessel
built by Mitsubishi and called <em>Yamato 1</em> (fortunately it fared better than the
battleship by that name). Instead, the battle of submarine silence has mostly
revolved around obscure technical problems of fluid dynamics, since one of the
loudest noises made by submarines is the cavitation around the screw. I don't
know if this is true today, but at least years ago the low-noise design of the
screw on modern US submarines was classified, and so the screw was covered by a
sheath whenever a submarine was out of the water.</p>
<p>Passive sonar can be performed from ships and even aircraft-deployed buoys, but
for the purpose of long-term maritime sovereignty it makes sense to install
permanent hydrophones that function as a defensive perimeter. Just such a
system was designed in the 1950s by (who else?) AT&amp;T. AT&amp;T had the expertise
not only in acoustic electronics, but also undersea cable laying, a key
component of any practical underwater surveillance system. Large arrays of
hydrophones, spaced along cables, were laid on the ocean floor. The sounds
detected by these hydrophones were printed on waterfall diagrams and inspected
by intelligence analysts, who relied on experience and no small amount of
educated guessing to recognize different types of marine life, geological
phenomena, and vessels at sea.</p>
<p>This system, called SOSUS for Sound Surveillance System, remained secret until
1991. The secrecy of SOSUS is no great surprise, as it was one of the most
important military intelligence systems of the Cold War. It presented a problem
as well, though, as few in the Navy were aware of the details of the system and
ship crews sometimes felt the abbreviated, zero-detail intelligence messages
from SOSUS to be confusing and unreliable. They were being told of likely
submarine detections, but knowing nothing about the system they had come from,
they didn't know whether or not to take them seriously.</p>
<p>By the 1960s, SOSUS consisted of hundreds of individual hydrophones installed
in long, cable-tethered arrays. Cables connected the hydrophone arrays to
highly secured terminal facilities on the coast, which the Navy explained with
a rather uninspiring cover story about undefined survey work. Over the
following decades, computers were applied to the task, automatically detecting
and classifying acoustic signatures. This early automation work inspired
significant research and development on signal processing and pattern matching
in both the military and Bell Laboratories, creating early precedents for the
modern field of machine learning. Additionally, computer and telecommunications
advancements allowed for remote control of the arrays, significantly reducing
the staff required for the program and leading to the eventual closure of many
of the terminal naval facilities.</p>
<p>In 1984, SOSUS was renamed to IUSS, the Integrated Underwater Surveillance
System. This new name reflected not only the increasing automation, but also
the inclusion of several surface vessels in the system. These vessels,
initially the USNS <em>Stalwart</em> and USNS <em>Worthy</em>, functioned as mobile IUSS
arrays and could be moved around to either expand coverage or provide more
accurate locating of a suspected target.</p>
<p>The existence of IUSS was finally declassified in 1991, although it was well
known before that point due to several prominent press mentions. Since the
declassification of IUSS it has enjoyed a dual-use role with the scientific
research community, and IUSS is one of the primary sources of hydrophone
data for marine biology. Today, IUSS automatically detects and classifies
both submarines and whales.</p>
<p>The potential of passive sonar systems to detect submarine accidents is
well-known. The 1968 loss of Soviet submarine <em>K-129</em> was detected by SOSUS,
and the location estimate produced by SOSUS facilitated the recovery of <em>K-129</em>
by the <em>Hughes Glomar Explorer,</em> one of the most fascinating naval intelligence
operations of American history. 1968 was a bad year for submarines with four
lost with all hands, and SOSUS data was used to locate at two of them (Soviet
<em>K-129</em> and US <em>Scorpion</em>. French <em>Minerve</em> and Israeli <em>Dakar</em> would not be
found for decades).</p>
<p>This all brings us to the modern era. <em>Titan</em> was lost on, presumably, the
18th of June. It was not located on the sea floor until the 22nd, four days
later. Press reporting after the discovery included a Navy statement that
IUSS had detected and located the implosion.</p>
<p>This has lead to a somewhat common internet hot take: that the Navy had
definitive information on the fate of <em>Titan</em> and, for some reason, suppressed
it for four days. I believe this to be an unwarranted accusation, and the
timing of the location of the wreck and the statement on IUSS are readily
explainable.</p>
<p>First, we must consider the nature of remote sensing. Remote sensing systems,
whether space-based or deep underwater, produce a large volume of data. The
primary source of actionable information in modern real-time remote sensing
are computer systems that use machine learning and other classification
methods to recognize important events. These computer systems must be trained
on those events, using either naturally or artificially created samples, in
order to correctly classify them. A major concern in naval intelligence is the
collection of up-to-date acoustic signatures for contemporary vessels so that
IUSS can correctly identify them.</p>
<p>A secondary method is retrospective analysis, in which human intelligence
analysts review historic data to look for events that were not classified by
automation when they occurred. Retrospective analysis, particularly with new
signature information, can often yield additional detections. Consider the case
I have previously discussed of the Chinese spy balloons: once signature
information (almost certainly RF emissions) were collected, retrospective
analysis yielded several earlier incidents that were not detected at the time
due to the lack of signatures.</p>
<p>Like the RF spectrum, the ocean contains a lot of noises. They come from
wildlife, from geological processes, and from commercial shipping, all besides
naval operations. The Navy does not rigorously investigate every sound
underwater, it can't possibly do so.</p>
<p>When the Navy became aware of the missing <em>Titan,</em> analysts almost certainly
began a retrospective analysis of IUSS data for anything that could indicate
its fate. They apparently detected loud noises and were able to locate the
source as near the <em>Titanic</em> wreckage, probably fairly quickly after the
<em>Titan</em> was first reported missing.</p>
<p>Here is the first challenge, though: the <em>Titan</em> was a new submersible of novel
(if not necessarily well thought out) construction. The Navy has some
familiarity with the acoustic signatures of imploding military submarines based
on incidentally lost submarines and, in at least one case, the intentional
torpedoing of a submarine to record the resulting acoustics (the <em>Sterlet</em>).
This data is used to produce a signature against which new signals can be
compared. Because of the significant differences in size and construction
between <em>Titan</em> and military submarines, the Navy likely had very low
confidence that known acoustic signatures of catastrophic losses were
applicable. The total number of submarines to have ever imploded underwater is
quite small, and none were of similar size and construction to <em>Titan</em>. The
point is that while intelligence analysts likely <em>suspected</em> they had evidence
of implosion, they probably had low confidence in that conclusion.</p>
<p>It is unwise, in the course of a search and rescue operation, to report that
you <em>think</em> the vessel was irrecoverably lost. Doing so can compromise search
operations by creating political pressure to end them, while making the
situation of families and friends worse. It is customary to be very cautious
with the release of inconclusive information in events like this. The problems
are exemplified by the Coast Guard's announcement that another passive sonar
system had detected possible banging sounds, which motivated a lot of reporting
making wild conclusions based on acoustic signatures that were likely
unrelated.</p>
<p>The more damning accusation, though, is this: did the Navy withhold information
on the detection from searchers out of concern for secrecy? Setting aside that
this makes little sense considering that SOSUS and its capabilities have been
widely known to the public for decades, and the search site was well within
historically published coverage estimates for SOSUS, this accusation doesn't
align with the timeline of the search.</p>
<p>The first search vessel capable of deep undersea exploration, the ROV <em>Pelagic
Odysseus 6k</em>, arrived on the scene on the morning of the 22nd. Just five hours
later, <em>Odysseus</em> had located the wreckage. Considering that the descent to depth
alone would have taken Odysseus over an hour, the wreckage was located extremely
quickly in the challenging undersea environment. One reason is obvious:
the wreckage of <em>Titan</em> was close to the <em>Titanic,</em> although the <em>Titanic</em> debris
field is large and searching it all would have taken hours. The second reason
became known shortly after: when <em>Odysseus</em> began its search, they had almost
certainly already been tipped off by the Navy as to the location of the possible
implosion.</p>
<p>The Navy did not withhold information on the detection for four days out of some
concern for secrecy. Instead, the information was not known to the public for
four days because that was when the search team was first able to actually
investigate the Navy's possible detection.</p>
<p>Indeed, the idea that the Navy suppressed the information seems to come only
from the rumor mill and internet repetition of half-read headlines. The
original press coverage of the IUSS detection, from the WSJ, states that the
Navy reported the finding to the Navy commander on-scene at the search effort
immediately. It does include the amusing sentence that "the Navy asked that the
specific system used not be named, citing national security concerns." This
might seem like a huge cover up to those unfamiliar with intelligence programs,
but it's perfectly in line with both normal military concerns around classified
systems (which are often known by multiple names which must be kept
compartmentalized for unclassified contracting) and the specific history of
IUSS, which during its period of secrecy had problems with being accidentally
named in unclassified reports multiple times.</p>
<p>IUSS is now a smaller system than it once was, although with improving
technology its coverage has probably expanded rather than contracted. It still
serves as a principal method of detecting submarines near the US, an
important concern since submarines are one of the main delivery mechanisms
for nuclear weapons. IUSS is just one of several semi-secret underwater sensing
systems used by the Navy.</p>
<p>A not totally related system that will nonetheless be of interest to many of my
readers (who I suspect to be somewhat concentrated in the San Francisco Bay
Area) is the San Francisco Magnetic Silencing Range. A small building in the
parking lot of Marina Green, complete with a goofy little control tower from
the era of manned operation, is the above-water extent of this system that uses
underwater magnetometers to measure the magnetic field of Navy vessels passing
through the Golden Gate. Since underwater mines are often triggered by
magnetometers, the Navy ensures that the magnetization of vessel hulls does not
exceed a certain limit. If it does, the vessel can be degaussed at one of
several specially-equipped Navy berths---inspiration for at least one episode
of The Next Generation. Similar arrays exist at several major US ports.</p>
<p>The building itself is long-disused, and the array is now fully remote
controlled. When I lived in San Francisco it was abandoned, but I see that it
has apparently been restored to function as the harbormaster's office. I
appreciate the historic preservation effort but something is lost with the
removal of the Navy's sun-faded signage.</p>
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to register a Kei truck in Pennsylvania (280 pts)]]></title>
            <link>https://danwilkerson.com/posts/2023-05-30-how-to-register-a-kei-truck-in-pa</link>
            <guid>36750554</guid>
            <pubDate>Sun, 16 Jul 2023 19:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danwilkerson.com/posts/2023-05-30-how-to-register-a-kei-truck-in-pa">https://danwilkerson.com/posts/2023-05-30-how-to-register-a-kei-truck-in-pa</a>, See on <a href="https://news.ycombinator.com/item?id=36750554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Kei trucks are a special designation of Japanese car that have very small
engines (660cc) and dimensions (~10m^3). <a href="https://en.wikipedia.org/wiki/Kei_truck">Tell 'em,
Wikipedia</a>. Whoever coined
the idea of "creative constraints" should take a gander at Kei trucks; there's
definitely an upper bound:</p>
<p><span>
      <a href="https://danwilkerson.com/static/6a0d69f1f2a05039b0372fbee9bb937a/41099/comparison.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="The Office 'corporate wants you to tell the difference between two pictures' meme with the Suzuki Carry and Honda Acty, which look
identical." title="" src="https://danwilkerson.com/static/6a0d69f1f2a05039b0372fbee9bb937a/41099/comparison.jpg" srcset="/static/6a0d69f1f2a05039b0372fbee9bb937a/d2f63/comparison.jpg 163w /static/6a0d69f1f2a05039b0372fbee9bb937a/c989d/comparison.jpg 325w https://danwilkerson.com/static/6a0d69f1f2a05039b0372fbee9bb937a/41099/comparison.jpg 500w" sizes="(max-width: 500px) 100vw, 500px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>Japan's tax incentives have created a brisk export business for often lightly
used trucks. As a result, they're available for purchase in the US.</p>
<p>I recently decided to do just that and had a heck of a time getting on the road.
This guide serves as a lamp in the dark, helping guide fellow fools to
road-legal ownership.</p>
<p>Why did I want one? I'm glad you ask - Kei trucks:</p>
<ul>
<li>Have a 6ft bed</li>
<li>Which can fold flat</li>
<li>And carry ~1000lbs</li>
<li>With 4WD (including a locking rear diff<sup id="fnref-1"><a href="#fn-1">1</a></sup> and axle)</li>
<li>While getting 40 MPG</li>
<li>For $X,000.0</li>
<li>With very low mileage (22k miles in my instance)</li>
</ul>
<p>I often need to haul stuff and had gotten sick of U-Haul rentals. I decided to
find a cheap 90s/early 00s small truck. Pandemic prices were getting me
rusted-out Rangers or 250k mile Tacomas<sup id="fnref-2"><a href="#fn-2">2</a></sup>; then the Kei came into my life.
Libby and I were pulling in to <a href="https://cjreuse.org/">Construction Junction</a>
when a couple in a Honda Acty pulled in behind us. I picked the guy's brain
about how useful it was, getting it insured, and so forth; it all sounded
pretty straightforward, and I was sold.</p>
<h2>Step 0: Find a truck</h2>
<p>You have some choices here: you can import it yourself, buy from an importer, or
risk craigslist. Prices rise as you work down that list.</p>
<p>You'll need to find a truck that is at least 25 years old. In the US, cars more
youthful than that must comply with <a href="https://helpspanish.cbp.gov/s/article/Article-278?language=en_US">FMVSS</a>
(safety standards). If you buy a more recent vintage, <strong>you won't be able to
get it registered anywhere</strong>, so pay attention to that model year!</p>
<p>There are exporter websites out there promising a truck for
~$3-5k all in, provided you can pick it up at the port and are willing to do the
paperwork. As I understand it, the importation process is high stakes - if you
make a mistake, US Customs puts your truck in a garbage compactor and you're out
the money. If you're brave enough to try it, I wish you the best - come back
here after you're leaving your port of choice.</p>
<p>The safer route is to buy from an importer. If you're smart, you'll find an
importer who has already titled the vehicle and you'll be set. Of course, you
wouldn't be Googling "how to title a kei truck Pennsylvania" in that case, but
here we are. "Don't worry," you thought, "it can't take <em>that</em> long to get a
title." <sup id="fnref-3"><a href="#fn-3">3</a></sup></p>
<h2>Step 1: Getting the car titled</h2>
<p>Wait, back it up - you're going to need some paperwork only the seller can get
you. Otherwise I honestly have no idea what will happen to you or your truck.</p>
<h2>Right, uh, Step -1: Make sure the seller has the paperwork</h2>
<p>Before you buy, make sure you'll be able to get everything you need to title
the thing. You'll need:</p>
<ul>
<li>The Export Certificate - this should look like this <span>
      <a href="https://danwilkerson.com/static/7ef6c640216900059c166e42a208bbe8/288ea/export_certificate.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="The export certificate from my Kei truck, listing the seller and exporter as well as some details about the car" title="" src="https://danwilkerson.com/static/7ef6c640216900059c166e42a208bbe8/6aca1/export_certificate.jpg" srcset="/static/7ef6c640216900059c166e42a208bbe8/d2f63/export_certificate.jpg 163w /static/7ef6c640216900059c166e42a208bbe8/c989d/export_certificate.jpg 325w /static/7ef6c640216900059c166e42a208bbe8/6aca1/export_certificate.jpg 650w https://danwilkerson.com/static/7ef6c640216900059c166e42a208bbe8/288ea/export_certificate.jpg 841w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy" decoding="async">
  </a>
    </span></li>
<li>Proof you paid sales tax - PennDOT will really, really want you to prove this, like<br>
a lot. I sent in a photocopy of the receipt from the place I bought the
truck, <a href="https://www.trlawnandgarden.com/">Twin Ridge Lawn and Garden</a>.</li>
</ul>
<h2>Great, back to Step 1a: Translate your Certificate</h2>
<p>So you're gonna need a translation of that certificate. It's not required, but
it supposedly helps to have the document notarized with an American Translation
Association seal. You definitely need the translation to be accompanied by a
sworn affadavit from the translator, though; they should know what to do.</p>
<p>I used the <a href="https://www.atanet.org/directory/">American Translators Association directory</a>
and emailed a handful of eligible translators. The prices and response times
were all over the map. In the end I ended up working with
<a href="http://japanesetoyou.com/">Patricia Pringle</a>; Patricia was super responsive
and very reasonably priced!</p>
<h2>Step, I don't know, 3? 1b? 2.5?: How much does your truck weigh?</h2>
<p>Okay, so you've got your translated export certificate, you've got your proof
of paying sales tax. Forget all that - now you're going to need to prove you
know how much your truck weighs. Why? Because in the state of Pennsylvania that
information is absolutely crucial to verifying the VIN of your vehicle. You're
going to need an <a href="http://www.ptatsvcs.com/wp-content/themes/potomactag/pdf/MV-41.pdf">MV-41</a>,
but first, you're going to need to get the <a href="https://www.falkirkvanhire.com/articles/unladen-vehicle-weight/">unladen
weight</a> of
your shiny new truck.</p>
<p>You'll need to get your truck to a salvage yard or other weigh station<sup id="fnref-4"><a href="#fn-4">4</a></sup> where a
weighmaster can weigh and certify the weight of your vehicle - I visited <a href="https://danddautosalvage.com/">D&amp;D
Auto Salvage</a> on the recommendation of my
mechanic, <a href="https://www.yelp.com/biz/walters-automotive-pittsburgh">Tim Walters</a>.
Thanks, Tim!</p>
<p>Take this slip of paper to a mechanic along with your MV-41 and your truck<sup id="fnref-4"><a href="#fn-4">4</a></sup>.
They'll need to fill out the MV-41, verifying the weight from the weighmaster
and the VIN in your truck (which may be in an unusual location - mine was
stamped on the wheel well). If you're lucky, you'll find a guy who's done this
recently and also thinks imports are super cool - <a href="https://www.yelp.com/biz/all-auto-repair-pittsburgh">thanks
Naiel</a>!</p>
<h2>Step 4: Insurance</h2>
<p>This was actually relatively easy. Don't even bother trying the "Request a
Quote" forms, just start calling shops that give you multiple quotes (e.g.
<a href="https://www.thezebra.com/">Zebra</a>). They'll help tremendously. Almost all the
providers declined to cover my <del>dangerous exotic trash hauler</del> minitruck, but
<a href="https://www.safeco.com/">Safco</a> gave me an acceptable rate on liability
insurance<sup id="fnref-5"><a href="#fn-5">5</a></sup>. Getcher self a copy of your proof of insurance.</p>
<h2>Step 1000: Submit the paperwork</h2>
<p>Once you've gotten your:</p>
<ul>
<li>Original export certificate</li>
<li>Translated export certificate</li>
<li>Proof of paid PA sales tax</li>
<li>Filled out MV-41</li>
<li>The mileage on your odometer, converted to miles</li>
<li>The gross weight of the vehicle, converted to pounds</li>
<li>Proof of insurance</li>
<li>A photo or two of the truck<sup id="fnref-6"><a href="#fn-6">6</a></sup></li>
</ul>
<p>It's finally time to fill out the big kahuna - the MV-1, a request for a title.
You won't find this form online - <a href="https://www.dmv.pa.gov/Driver-Services/New-Resident-Relocation-Information/New%20Residents/Pages/Motor-Vehicle-Information-for-New-Residents.aspx#:~:text=Form%20MV%2D1%20is%20not,issue%20you%20a%20temporary%20registration.">only an authorized agent can fill one
out</a>.
<a href="https://www.dot.state.pa.us/public/dvspubsforms/BMV/BMV%20Fact%20Sheets/fs-mv1.pdf">There are some helpful instructions for filling this out
properly</a>,
I went with <a href="https://www.aaa.com/office/detail/Pittsburgh-PA-East%20Liberty-6969">AAA here in Pittsburgh</a>.
You're also going to find out that you can <a href="https://www.dmv.pa.gov/Pages/Mini-Trucks-FAQ.aspx">only register these vehicles as
antiques in PA</a>. This isn't
as onerous as it once was. You can't use the vehicle for commercial purposes,
and you're not meant to use it more than occasionally - for most folks, that
should fit the bill just fine.</p>
<p>Once you've got an agent, they'll take a look at all your documents and ask you
a few questions about the vehicle. It's important that you:</p>
<ul>
<li>Explain that the truck can be registered as an antique</li>
<li>Demonstrate repeatedly that <em>yes, you have paid the PA State sales tax</em><sup id="fnref-7"><a href="#fn-7">7</a></sup></li>
<li>Request it be titled as a truck</li>
<li>Ensure that they send in the <strong>original export certificate</strong> and not a copy</li>
</ul>
<p>At this point I'd like to share a bit of wisdom I received early on in this process:
this is a once-in-a-lifetime type situation for whomever you're going to
work with. You are the edge case and corner case they've been warned about.
You are going to be their anecdote about the wild stuff that comes through the
door. "Some kind of crazy tiny truck from Japan," they'll laugh, "My God!
Months!". They'll accentuate that last point by waving their hands in the air,
like they're waving an imaginary beachball from side-to-side. Be ready to be
patient.</p>
<p>Once the paperwork is submitted, you'll wait about ~30 days before you hear
from PennDot. If you're super lucky you'll get a plate in the mail along with
the title in a few weeks. If you're a little lucky, they'll send your agent
feedback on your packet that you can correct and resubmit. You're going to need
to follow up with your agent - they're probably not going to reach out. Once
you've addressed their concerns, it'll be another 30 days before you'll know if
things are fixed. If you're really unlucky, you'll do this loop, oh, I don't
know, three or four times<sup id="fnref-8"><a href="#fn-8">8</a></sup>? Finally, you'll get a thick envelope in the mail
and inside will be your prize - an antique PA plate.</p>
<p><span>
      <a href="https://danwilkerson.com/static/e11b26f935ad984a52e00e0a2965c087/ac99c/me_and_truck.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="Me and the truck outside McBrooms after a 3ROC run" title="" src="https://danwilkerson.com/static/e11b26f935ad984a52e00e0a2965c087/6aca1/me_and_truck.jpg" srcset="/static/e11b26f935ad984a52e00e0a2965c087/d2f63/me_and_truck.jpg 163w /static/e11b26f935ad984a52e00e0a2965c087/c989d/me_and_truck.jpg 325w /static/e11b26f935ad984a52e00e0a2965c087/6aca1/me_and_truck.jpg 650w /static/e11b26f935ad984a52e00e0a2965c087/7c09c/me_and_truck.jpg 975w /static/e11b26f935ad984a52e00e0a2965c087/01ab0/me_and_truck.jpg 1300w https://danwilkerson.com/static/e11b26f935ad984a52e00e0a2965c087/ac99c/me_and_truck.jpg 1536w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy" decoding="async">
  </a>
    </span></p>
<h2>Step 1001: Go buy some screws</h2>
<p>Ah, crap, sorry - you could have done this way sooner. The holes in the plate
won't line up with the holes in your plate holder - damn metric system. Grab
a drill and a pencil, mark where the holes should be, apply drill, and you'll
be all set. You'll need two M6 20mm galvanized screws, which you can find at
any hardware store.</p>
<h2>So what's the downside?</h2>
<p>In case you're still weighing the pros and cons, here's my take as of a few months
into ownership.</p>
<ul>
<li>It attracts <em>a lot</em> of attention. This might be a little overwhelming.</li>
<li>You're topping out at 60 MPH, maybe.</li>
<li>No air bags, air conditioning, power steering, or crumple zones.</li>
<li>Parts are all across an ocean.</li>
<li>Right-hand drive is unsettling for everyone involved.</li>
</ul>
<p>I've been really happy with it - it's a lot cheaper than a
similarly sized truck would have run me, with way fewer miles on the odometer.
For bopping around town hauling lumber, yard waste, and furniture, it's perfect.
Just don't expect to be cruising on the interstate in one of these.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT use declines as users complain about ‚Äòdumber‚Äô answers (108 pts)]]></title>
            <link>https://www.techradar.com/computing/artificial-intelligence/chatgpt-use-declines-as-users-complain-about-dumber-answers-and-the-reason-might-be-ais-biggest-threat-for-the-future</link>
            <guid>36750200</guid>
            <pubDate>Sun, 16 Jul 2023 18:45:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techradar.com/computing/artificial-intelligence/chatgpt-use-declines-as-users-complain-about-dumber-answers-and-the-reason-might-be-ais-biggest-threat-for-the-future">https://www.techradar.com/computing/artificial-intelligence/chatgpt-use-declines-as-users-complain-about-dumber-answers-and-the-reason-might-be-ais-biggest-threat-for-the-future</a>, See on <a href="https://news.ycombinator.com/item?id=36750200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="A robot head on fire with the ChatGPT logo on one side." onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg"><source type="image/jpeg" alt="A robot head on fire with the ChatGPT logo on one side." onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg"><img src="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg" alt="A robot head on fire with the ChatGPT logo on one side." onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/FjKZ5WFYqqKmTPeGZaLu85.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: OpenAI, Shutterstock)</span>
</figcaption>
</div>

<div id="article-body">
<p>Is <a href="https://www.techradar.com/news/chatgpt-explained"><u>ChatGPT</u></a> old news already? It seems impossible, with the explosion of AI popularity seeping into every aspect of our lives - whether it‚Äôs digital masterpieces forged with the <a href="https://www.techradar.com/features/best-ai-art-generators-compared"><u>best AI art generators</u></a> or <a href="https://www.techradar.com/computing/artificial-intelligence/microsofts-new-ai-shopping-tools-will-create-a-buying-guide-just-for-you"><u>helping us with our online shopping</u></a>.</p><p>But despite being the leader in the AI arms race - and powering <a href="https://www.techradar.com/news/bing-ai-chat-is-now-waitlist-free-and-about-to-get-even-more-powerful"><u>Microsoft‚Äôs Bing AI</u></a> - it looks like <a href="https://www.techradar.com/tag/chatgpt" data-auto-tag-linker="true">ChatGPT</a> might be losing momentum. According to <a href="https://www.similarweb.com/blog/insights/ai-news/chatgpt-traffic-drops/" data-url="https://www.similarweb.com/blog/insights/ai-news/chatgpt-traffic-drops/"><u>SimilarWeb</u></a>, traffic to OpenAI‚Äôs ChatGPT site dropped by almost 10% compared to last month, while metrics from <a href="https://app.sensortower.com/overview/6448311069?metric=units&amp;utab=summary&amp;tab=about&amp;country=US" data-url="https://app.sensortower.com/overview/6448311069?metric=units&amp;utab=summary&amp;tab=about&amp;country=US"><u>Sensor Tower</u></a> also demonstrated that downloads of the iOS app are in decline too.</p><p>As reported by <a href="https://www.businessinsider.com/openai-gpt4-ai-model-got-lazier-dumber-chatgpt-2023-7?r=US&amp;IR=T" data-url="https://www.businessinsider.com/openai-gpt4-ai-model-got-lazier-dumber-chatgpt-2023-7?r=US&amp;IR=T"><u>Insider</u></a>, paying users of the more powerful GPT-4 model (access to which is included in <a href="https://www.techradar.com/news/chatgpt-plus-will-get-a-big-update-this-week-heres-why-its-a-big-deal"><u>ChatGPT Plus</u></a>) have been complaining on social media and OpenAI‚Äôs own forums about a dip in output quality from the <a href="https://www.techradar.com/tag/chatbot" data-auto-tag-linker="true">chatbot</a>.</p><p>A common consensus was that GPT-4 was able to generate outputs faster, but at a lower level of quality. <a href="https://twitter.com/petergyang" data-url="https://twitter.com/petergyang"><u>Peter Yang</u></a>, a product lead for Roblox, took to Twitter to decry the bot‚Äôs recent work, claiming that ‚Äúthe quality seems worse‚Äù. One forum user said the recent GPT-4 experience felt ‚Äúlike driving a Ferrari for a month then suddenly it turns into a beaten up old pickup‚Äù.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">GPT4's output has changed recently.It generates faster, but the quality seems worse.Perhaps <a href="https://www.techradar.com/tag/openai" data-auto-tag-linker="true">OpenAI</a> is trying to save costs.Has anyone else noticed this?<a href="https://twitter.com/petergyang/status/1660314935208054785" data-url="https://twitter.com/petergyang/status/1660314935208054785">May 21, 2023</a></p></blockquote><p><span role="button" tabindex="0" aria-label="See more">See more</span></p></div><h2 id="why-is-gpt-4-suddenly-struggling-3">Why is GPT-4 suddenly struggling?</h2><p>Some users were even harsher, calling the bot ‚Äúdumber‚Äù and ‚Äúlazier‚Äù than before, with a <a href="https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269" data-url="https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269"><u>lengthy thread on OpenAI‚Äôs forums</u></a> filled with all manner of complaints. One user, ‚Äòbitbytebit‚Äô, described it as ‚Äútotally horrible now‚Äù and ‚Äúbraindead vs. before‚Äù.</p><p>According to users, there was a point a few weeks ago where GPT-4 became massively faster - but at a cost of performance. The AI community has speculated that this could be due to a shift in OpenAI‚Äôs design ethos behind the more powerful machine learning model - namely, breaking it up into multiple smaller models trained in specific areas, which can act in tandem to provide the same end result while being cheaper for OpenAI to run.</p><p>OpenAI has yet to officially confirm this is the case, as there has been no mention of such a major change to the way GPT-4 works. It‚Äôs a credible explanation according to industry experts like Sharon Zhou, CEO of AI-building company <a href="https://www.lamini.ai/" data-url="https://www.lamini.ai/"><u>Lamini</u></a>, who described the multi-model idea as the ‚Äúnatural next step‚Äù in developing GPT-4.</p><h2 id="ais-eating-ais-3">AIs eating AIs</h2><p>However, there‚Äôs another pressing problem with ChatGPT that some users suspect could be the cause of the recent drop in performance - an issue that the AI industry seems largely unprepared to tackle.</p><p>If you‚Äôre not familiar with the term ‚ÄòAI cannibalism‚Äô, let me break it down in brief: large language models (LLMs) like ChatGPT and <a href="https://www.techradar.com/news/google-bard"><u>Google Bard</u></a> scrape the public internet for data to be used when generating responses. In recent months, a veritable boom in AI-generated content online - including an unwanted torrent of <a href="https://www.techradar.com/computing/artificial-intelligence/amazon-has-a-big-problem-as-ai-generated-books-flood-kindle-unlimited"><u>AI-authored novels on Kindle Unlimited</u></a> - means that LLMs are increasingly likely to scoop up materials that were already produced by an AI when hunting through the web for information.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="An iPhone screen showing the OpenAI ChatGPT download page on the App Store" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/techradar/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg"><source type="image/jpeg" alt="An iPhone screen showing the OpenAI ChatGPT download page on the App Store" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/techradar/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg"><img src="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" alt="An iPhone screen showing the OpenAI ChatGPT download page on the App Store" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/techradar/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25.jpg" srcset="https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Nn2BySLxgSdb6sL5JwBi25-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span>ChatGPT app downloads have slowed, indicating a decrease in overall public interest. </span><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><p>This runs the risk of creating a feedback loop, where AI models ‚Äòlearn‚Äô from content that was itself AI-generated, resulting in a gradual decline in output coherence and quality. With numerous LLMs now available both to professionals and the wider public, the risk of AI cannibalism is becoming increasingly prevalent - especially since there‚Äôs yet to be any meaningful demonstration of how AI models might accurately differentiate between ‚Äòreal‚Äô information and AI-generated content.</p><p>Discussions around AI have largely focused on <a href="https://www.techradar.com/features/ai-really-could-destroy-the-world-but-not-in-the-way-you-might-expect"><u>the risks it poses to society</u></a> - for example, Facebook owner Meta recently declined to open up its new speech-generating AI to the public after it was <a href="https://www.techradar.com/news/meta-says-its-new-speech-generating-ai-tool-is-too-dangerous-to-release"><u>deemed ‚Äòtoo dangerous‚Äô to be released</u></a>. But content cannibalization is more of a risk to the future of AI itself; something that threatens to ruin the functionality of tools such as ChatGPT, which depend upon original human-made materials in order to learn and generate content.</p><p>Do you use ChatGPT or GPT-4? If you do, have you felt that there‚Äôs been a drop in quality recently, or have you simply lost interest in the chatbot? I‚Äôd love to hear from you <a href="https://twitter.com/NotThaneKrios" data-url="https://twitter.com/NotThaneKrios"><u>on Twitter</u></a>. With so many competitors now springing up, is it possible that OpenAI‚Äôs dominance might be coming to an end?&nbsp;</p>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Sign up to receive daily breaking news, reviews, opinion, analysis, deals and more from the world of tech.</p></section></div>
<div id="slice-container-authorBio"><p>Christian is TechRadar‚Äôs UK-based Computing Editor. He came to us from Maximum PC magazine, where he fell in love with computer hardware and building PCs. He was a regular fixture amongst our freelance review team before making the jump to TechRadar, and can usually be found drooling over the latest high-end graphics card or gaming laptop before looking at his bank account balance and crying.</p>

<p>Christian is a keen campaigner for LGBTQ+ rights and the owner of a charming rescue dog named Lucy, having adopted her after he beat cancer in 2021. She keeps him fit and healthy through a combination of face-licking and long walks, and only occasionally barks at him to demand treats when he‚Äôs trying to work from home.</p></div>



</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Structured output from LLMs without reprompting (153 pts)]]></title>
            <link>https://automorphic.ai/playground</link>
            <guid>36750083</guid>
            <pubDate>Sun, 16 Jul 2023 18:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://automorphic.ai/playground">https://automorphic.ai/playground</a>, See on <a href="https://news.ycombinator.com/item?id=36750083">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google SoundStorm: Efficient Parallel Audio Generation (275 pts)]]></title>
            <link>https://google-research.github.io/seanet/soundstorm/examples/</link>
            <guid>36749059</guid>
            <pubDate>Sun, 16 Jul 2023 16:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://google-research.github.io/seanet/soundstorm/examples/">https://google-research.github.io/seanet/soundstorm/examples/</a>, See on <a href="https://news.ycombinator.com/item?id=36749059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h3>Efficient Parallel Audio Generation</h3>
        <p>
          [<a href="https://arxiv.org/abs/2305.09636">paper</a>]
        </p>
        <p>
          Zal√°n Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, Marco Tagliasacchi
        </p>
        <p><b>Google Research</b></p>
      </div><p>
        <b>Abstract.</b>  
        We present SoundStorm, a model for efficient, non-autoregressive audio generation. 
        SoundStorm receives as input the semantic tokens of AudioLM, and relies on
        bidirectional attention and confidence-based parallel decoding to generate the tokens of a
        neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model
        produces audio of the same quality and with higher consistency in voice and acoustic
        conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on
        a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences 
        by synthesizing high-quality, natural dialogue segments, given a transcript annotated with 
        speaker turns and a short prompt with the speakers' voices.
      </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A tutorial quantum interpreter in 150 lines of Lisp (219 pts)]]></title>
            <link>https://www.stylewarning.com/posts/quantum-interpreter/</link>
            <guid>36748915</guid>
            <pubDate>Sun, 16 Jul 2023 16:41:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stylewarning.com/posts/quantum-interpreter/">https://www.stylewarning.com/posts/quantum-interpreter/</a>, See on <a href="https://news.ycombinator.com/item?id=36748915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
<p><em>By Robert Smith</em></p>
<p><em>Simulating a universal, gate-based quantum computer on a classical
computer has many uses and benefits. The top benefit is the ability to
inspect the amplitudes of the system‚Äôs state directly. However, while
the mathematics is very well understood, implementing a
general-purpose simulator has largely been folk knowledge. In this
tutorial, we show how to build an interpreter for a general-purpose
quantum programming language called $\mathscr{L}$, capable of
executing most kinds of quantum circuits found in literature. It is
presented economically, allowing its implementation to take fewer than
150 lines of self-contained Common Lisp code. The language
$\mathscr{L}$ is very simple to extend, making the interpreter ripe
for testing different kinds of behavior, such as noise models.</em></p>
<div>
<hr>
<h2>Contents</h2>
<nav id="TableOfContents">
<ol>
<li><a href="#introduction">Introduction</a>
<ol>
<li><a href="#a-note-about-common-lisp">A note about Common Lisp</a></li>
<li><a href="#a-note-to-experienced-quantum-computing-practitioners">A note to experienced quantum computing practitioners</a></li>
</ol>
</li>
<li><a href="#the-language-mathscrl">The Language $\mathscr{L}$</a></li>
<li><a href="#the-quantum-state">The Quantum State</a>
<ol>
<li><a href="#where-does-one-qubit-live">Where does one qubit live?</a></li>
<li><a href="#many-qubits">Many qubits</a></li>
<li><a href="#bit-string-notation-and-a-general-quantum-state">Bit-String notation and a general quantum state</a></li>
<li><a href="#evolving-the-quantum-state">Evolving the quantum state</a></li>
</ol>
</li>
<li><a href="#measurement">Measurement</a></li>
<li><a href="#gates">Gates</a>
<ol>
<li><a href="#gates-as-matrices">Gates as matrices</a></li>
<li><a href="#gates-on-multi-qubit-machines">Gates on multi-qubit machines</a></li>
<li><a href="#single-qubit-gates-and-gates-on-adjacent-qubits">Single-qubit gates and gates on adjacent qubits</a></li>
<li><a href="#multi-qubit-gates-on-non-adjacent-qubits">Multi-qubit gates on non-adjacent qubits</a></li>
</ol>
</li>
<li><a href="#an-interpreter">An interpreter</a>
<ol>
<li><a href="#the-driver-loop">The driver loop</a></li>
<li><a href="#efficiency">Efficiency</a></li>
</ol>
</li>
<li><a href="#examples">Examples</a>
<ol>
<li><a href="#bell-state">Bell state</a></li>
<li><a href="#greenberger--horne--zeilinger-state">Greenberger‚ÄìHorne‚ÄìZeilinger state</a></li>
<li><a href="#the-quantum-fourier-transform">The quantum Fourier transform</a></li>
<li><a href="#example-transcript">Example transcript</a></li>
</ol>
</li>
<li><a href="#source-code">Source code</a></li>
</ol>
</nav>
<hr>
</div>
<h2 id="introduction">Introduction</h2>
<p>Simulating the workings of an ideal quantum computer has many
important applications, such as algorithms research and quantum
program debugging. A variety of quantum computer simulators exist,
both free and commercial. However, while the concept of the simulation
of quantum computers is generally well understood at a high level, the
devil is in the details when it comes to implementation.</p>
<p>Quantum computer simulators found in the wild often have many
limitations. The most prevalent limitation is the number of qubits an
operator can act on. Usually, one-qubit gates and controlled
one-qubit<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> gates are allowed, but nothing more. While these
together are sufficient for universal quantum computation, it leaves
much to be desired when studying quantum algorithms.</p>
<p>In this post, we present an implementation of a fully general quantum
programming language interpreter, allowing measurement as well as
arbitrary unitary operators on an arbitrary number of arbitrarily
indexed qubits. The implementation weighs in at under 150 lines<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>
of code in Common Lisp, though the ideas make implementation simple in
other languages as well. All of the code from this tutorial can be
found on
<a href="https://github.com/stylewarning/quantum-interpreter">GitHub</a>.</p>
<p>This tutorial is aimed at a quantum computing beginner who has some
familiarity with the fundamentals of linear algebra and computer
programming. Beyond those subjects, this tutorial is relatively
self-contained. We also aim this tutorial at practitioners of quantum
computing, who are interested in the brass tacks of simulation, with
all of the details filled out. To such practitioners, the bulk of this
document will be easy to skim, since we recapitulate topics such as
qubits and unitary operators.</p>
<h3 id="a-note-about-common-lisp">A note about Common Lisp</h3>
<p>We use Common Lisp, because it is an excellent platform for both
exploratory and high-performance computing. One of the fastest and
most flexible quantum simulators out there, the <a href="https://github.com/quil-lang/qvm">Quantum Virtual
Machine</a>, is written entirely in
Common Lisp.</p>
<p>We wrote this article so that it would be easy to follow along with a
Common Lisp implementation. The code has no dependencies, and should
work in any ANSI-compliant implementation (I hope).</p>
<p>With that said, this article was also written with portability in
mind. Since no especially Lisp-like features are used, the code should
be easy to port to Python or even C. At minimum, your language should
support complex numbers and arrays.</p>
<h3 id="a-note-to-experienced-quantum-computing-practitioners">A note to experienced quantum computing practitioners</h3>
<p><em>This section is written for experienced practitioners of quantum
computing who happened upon this post, and can be skipped.</em></p>
<p>In this post, we opt to simulate a quantum circuit the ‚ÄúSchrodinger‚Äù
way, that is, by evolving a wavefunction explicitly. For a circuit of
width $n$, we walk through the mathematics of how to interpret a
$k$-qubit gate $g \in \mathsf{SU}(2^k)$ for $k\le n$, specified to act
on a $k$-tuple of numbered qubits‚Äîcorresponding to each qubit‚Äôs
position in the tensor product which forms the Hilbert space of the
system‚Äîas a full operator $g'\in\mathsf{SU}(2^n)$. We do this by
providing an explicit construction of the matrix in the computational
basis of the system.</p>
<p>An alternative approach would have been to describe the action of a
$g$ on an $n$-qubit wavefunction by way of careful manipulation of
indexes, i.e., to effectively permute and partition our wavefunction
into $2^{n-k}$ groups of $2^k$-dimensional vectors corresponding to
the subsystem of qubits being operated on. The major benefit of this
approach is efficiency.</p>
<p>As a first introduction to a computer science graduate, I find this
explanation lacking in two ways:</p>
<ol>
<li>It under-emphasizes that a gate like $\mathsf{CNOT}$, typically
written as a $4\times 4$ matrix $\mathsf{I}\oplus\mathsf{X}$, in a
quantum circuit truly is a linear operator on the Hilbert space of
the entire system. ‚ÄúIt‚Äôs just linear algebra; here‚Äôs the matrix and
here‚Äôs the vector‚Äù is a point I want to drive home.</li>
<li>It requires significant labor to both explain and prove the
correctness of the method, without having significant experience with
tensor algebra, contractions, Einstein notation, and so on.</li>
</ol>
<p>The approach of this post can be used as a basis to follow up with
more efficient techniques, without relinquishing a strong mathematical
foundation. We are very careful to not be hand-wavy, and to not
conflate the different vector spaces at play. We hope that you‚Äôll find
this approach agreeable, even if it sacrifices some efficiency.</p>
<h2 id="the-language-mathscrl">The Language $\mathscr{L}$</h2>
<p>We wish to construct an interpreter for a small quantum programming
language named $\mathscr{L}$. This language supports
both of the fundamental operations of a quantum computer: gates and
measurements.</p>
<p>A <strong>gate</strong> is an operation that modifies a quantum state. (What a
quantum state is exactly we will delve into later.) Because quantum
states are large compared to the physical resources used to construct
them, gates represent the ‚Äúpowerful‚Äù operations of a quantum
computer.</p>
<p>A <strong>measurement</strong> is an observation and collapse of the quantum state,
producing one bit (i.e., $0$ or $1$) of classical information per
qubit. Measurements represent the <em>only</em> way in which one can extract
information from our simulated quantum computer, and indeed, in most
programming models for real quantum computers.</p>
<p>In some sense, one might think of the language $\mathscr{L}$ as the
simplest non-trivial quantum programming language. A program in
$\mathscr{L}$ is just a sequence of gates and measurements. The syntax
is as follows:</p>
<table>
<thead>
<tr>
<th>Non-Terminal</th>
<th></th>
<th>Defintion</th>
</tr>
</thead>
<tbody>
<tr>
<td>program</td>
<td>:=</td>
<td><code>(</code> <em>instruction</em>* <code>)</code></td>
</tr>
<tr>
<td>instruction</td>
<td>:=</td>
<td><code>(</code> <code>GATE</code> <em>matrix</em> <em>qubit</em>+ <code>)</code></td>
</tr>
<tr>
<td></td>
<td>|</td>
<td><code>(</code> <code>MEASURE</code> <code>)</code></td>
</tr>
<tr>
<td>matrix</td>
<td>:=</td>
<td><em>a complex matrix</em> <code>#2A(</code> ‚Ä¶ <code>)</code></td>
</tr>
<tr>
<td>qubit</td>
<td>:=</td>
<td><em>a non-negative integer</em></td>
</tr>
</tbody>
</table>
<p>Spaces and newlines are ignored, except to delimit the tokens of our
language.</p>
<p>We borrow Common Lisp‚Äôs two-dimensional array syntax for the syntax of
matrices. In Common Lisp, the matrix $\left(\begin{smallmatrix}1 &amp;
2\\3 &amp; 4\end{smallmatrix}\right)$ is written <code>#2A((1 2) (3 4))</code>. We
also borrow the syntax for complex numbers: $1-2i$ is written <code>#C(1 -2)</code>.</p>
<p>An example program might be one to construct and subsequently measure
two qubits labeled <code>2</code> and <code>5</code> in a Bell state configuration:</p>
<pre tabindex="0"><code>(
 (GATE #2A((0.70710677 0.70710677) (0.70710677 -0.70710677)) 2)
 (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 0 1) (0 0 1 0))          2 5)
 (MEASURE)
)
</code></pre><p>We will model the semantics of $\mathscr{L}$ operationally, by way of an <strong>abstract machine</strong>. The abstract machine for $\mathscr{L}$ is called $M_n$, where $n$ is a positive but fixed<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> number of qubits. The state of the machine $M_n$ is the pair $(v, b)$ where $v$ is a quantum state, and $b$ is an $n$-bit measurement register.</p>
<p>The quantum state is an element of the set</p>
<p>$$\{\Vert v\Vert=1\mid v\in\mathbb{C}^{2^n}\}.$$</p>
<p>In other words, $v$ is a unit vector of dimension $2^n$ over the
complex numbers. We will discuss this from first principles in the
<a href="#the-quantum-state">next section</a>.</p>
<p>The measurement register is an element of the set $\{0,1\}^n$, i.e.,
a sequence of $n$ bits, which we realize as a non-negative
integer. The $k$th least-significant bit of this integer represents
the last observation of the qubit numbered as $k$. We will <a href="#measurement">discuss
this in detail</a> as well.</p>
<p>In Common Lisp, it suffices to create a structure <code>machine</code> which holds these two pieces of state.</p>
<pre tabindex="0"><code>(defstruct machine
  quantum-state
  measurement-register)
</code></pre><p>Typically, the machine is initialized with each classical bit in the
measurement register $0$, and each qubit starting in the
zero-state. (However, for the purposes of algorithm study or
debugging, the machine may be initialized with any valid state.)</p>
<p>The precise way in which the language $\mathscr{L}$ is interpreted on
$M_n$ is what we describe in this tutorial. Before that, however, we
find it most important to describe what <em>exactly</em> a quantum state is,
and how to represent it on a computer.</p>
<h2 id="the-quantum-state">The Quantum State</h2>
<h3 id="where-does-one-qubit-live">Where does one qubit live?</h3>
<p>Quantum computers are usually just a collection of interacting computational elements called <strong>qubits</strong>. A single qubit has two distinguished states: $\ket{0}$ and $\ket{1}$. If the qubit has a name like $q$, then we label the states $\ket{0}_q$ and $\ket{1}_q$.</p>
<p>The funny notation is called <strong>Dirac notation</strong> or <strong>braket notation</strong>. It happens to be a convenient notation for doing calculations in quantum mechanics, and we just use it for consistency with other texts. The <strong>ket</strong> $\ket{\cdots}$, as a physicist would call it, doesn‚Äôt add any special significance, except to denote that the quantity is a vector. One can actually put <em>anything</em> inside the brackets. In usual linear algebra, one often writes $\mathbf{e}_i$ to denote a basis vector, where in quantum mechanics, one just writes the subscript in a ket $\ket{i}$, dropping the $\mathbf{e}$ entirely. If the notation throws you off, and you‚Äôd like to think in more traditional written linear algebra notation, you can always replace $\ket{x}$ with $\vec x$, and you‚Äôll be safe.</p>
<p>These distinguished states $\ket{0}$ and $\ket{1}$ are understood to be orthonormal basis vectors in a vector space whose scalars are complex numbers $\mathbb{C}$. As such, a qubit can be $\ket{0}$, $\ket{1}$, or a <strong>superposition</strong> $\alpha\ket 0 + \beta\ket 1$, where $\alpha$ and $\beta$ are complex numbers. The numbers $\alpha$ and $\beta$ are called <strong>probability amplitudes</strong>, because $\vert\alpha\vert^2$ (resp. $\vert\beta\vert^2$) represent the probability of the qubit being observed in the $\ket 0$ (resp. $\ket 1$) state. Since they represent probabilities, there‚Äôs an additional constraint, namely that the probabilities add to one: $\vert\alpha\vert^2 + \vert\beta\vert^2=1$.</p>
<p>To those unfamiliar, it may not be obvious why we‚Äôve opted to use the
language of linear algebra. Why do we consider a qubit as being a
linear combination? Why do we suppose that the observable states are
orthonormal vectors? Why can‚Äôt we simply say that a qubit is just a
pair of complex numbers and move on?</p>
<p>The reason for this is scientific, and not mathematical. It turns out that the best theory of quantum mechanics we have is one which describes transformations between states as being linear. In fact, the evolution of a quantum mechanical system is not only described by an operation that is just linear, but also reversible. These conditions‚Äîlinear, reversible, and length-preserving‚Äîgive rise to a special class of transformations called <strong>unitary operators</strong>, which naturally lead us to the discussion of vector spaces over complex numbers<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p>
<p>We will discuss the nature of these operations in more depth when we consider how to implement gates <a href="#gates">later on</a>. For now, however, it‚Äôs sufficient to think of a qubit named $q$ as something that lives in a complex, two-dimensional vector space, which we will call $$B_q := \operatorname{span}_{\mathbb{C}}\{\ket 0_q, \ket 1_q\}.$$ (We will use this $B_q$ notation a few times throughout this tutorial. Remember it!) We also understand that this space is equipped<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> with a way to calculate lengths of vectors‚Äîthe usual norm</p>
<p>$$
\left\Vert\alpha\ket{0}+\beta\ket{1}\right\Vert = \sqrt{\vert\alpha\vert^2+\vert\beta\vert^2}.
$$</p>
<h3 id="many-qubits">Many qubits</h3>
<p>Roughly speaking, a single qubit can be described by two
probabilities. How do we deal with more?</p>
<p>Suppose we have two qubits named $X$ and $Y$. As a pair, quantum
mechanics tells us that they can <em>interact</em>. Practically, what
that means is that their states can be correlated in some way. If
they‚Äôve interacted, knowing information about $X$ might give us a clue
about what $Y$ might be. One well-known example of this is the
<em>Bell state</em>, which can be summarized as follows:</p>
<table>
<thead>
<tr>
<th>Qubit $X$</th>
<th>Qubit $Y$</th>
<th>Prob. Amp.</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 0_Y$</td>
<td>$1/\sqrt{2}$</td>
<td>$50\%$</td>
</tr>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 1_Y$</td>
<td>$0$</td>
<td>$0\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 0_Y$</td>
<td>$0$</td>
<td>$0\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 1_Y$</td>
<td>$1/\sqrt{2}$</td>
<td>$50\%$</td>
</tr>
</tbody>
</table>
<p>Here, we have an example of a <strong>non-factorizable state</strong>; qubits $X$ and $Y$ are correlated to each other dependently. If we know $X$ is in the $\ket 0_X$ state, then we <em>necessarily</em> know that $Y$ is in the $\ket 0_Y$ state. Such a correlation means it‚Äôs not possible to express the probabilities independently. It might be tempting to think that one can simply think of $X$ having a $50\%$ probability of being in either basis state, and $Y$ having a $50\%$ probability of being in either state‚Äîfacts which are certainly true‚Äîbut considering those independently would give us a <em>different</em> distribution of probabilities of the system:</p>
<table>
<thead>
<tr>
<th>Qubit $X$</th>
<th>Qubit $Y$</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 0_Y$</td>
<td>$P(X=\ket 0_X)P(Y=\ket 0_Y)=25\%$</td>
</tr>
<tr>
<td>$\ket 0_X$</td>
<td>$\ket 1_Y$</td>
<td>$P(X=\ket 0_X)P(Y=\ket 1_Y)=25\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 0_Y$</td>
<td>$P(X=\ket 1_X)P(Y=\ket 0_Y)=25\%$</td>
</tr>
<tr>
<td>$\ket 1_X$</td>
<td>$\ket 1_Y$</td>
<td>$P(X=\ket 1_X)P(Y=\ket 1_Y)=25\%$</td>
</tr>
</tbody>
</table>
<p>This state is called <strong>factorizable</strong> because we can express each
probability as a product of probabilities pertaining to the original
qubits, i.e., each probability has a form that looks like
$P(X)P(Y)$. Note that here, knowing something about $X$ gives us <em>no</em>
information about $Y$, since they‚Äôre completely independent. With that
said, it should be emphasized that factorizable states <em>are</em> perfectly
valid states, but they don‚Äôt represent the entirety of possible
states.</p>
<p>If qubits $X$ and $Y$ live in the linear spaces $B_X$ and $B_Y$ respectively, then the composite space is written $B_X\otimes B_Y$. This is called a <strong>tensor product</strong>, which is a way to combine <em>spaces</em> with the above structure. Formally, if we have an $m$-dimensional vector spaces $V:=\operatorname{span}\{v_1,\ldots,v_m\}$ and an $n$-dimensional vector space $W:=\operatorname{span}\{w_1,\ldots,w_n\}$, then their tensor product $T:=V\otimes W$ will be an $mn$-dimensional vector space $\operatorname{span}\{t_1,\ldots,t_{mn}\}$, where each $t_i$ is a formal combination of basis vectors from $V$ and $W$. (There are of course $mn$ different combinations of $v$‚Äôs and $w$‚Äôs.) To give an example without all the abstraction, consider $V$ with a basis $\{\vec x, \vec y, \vec z\}$ and $W$ with a basis $\{\vec p, \vec q\}$. Then $V\otimes W$ will have a basis</p>
<p>$$
\left\{
\begin{array}{lll}
\vec x\otimes\vec p, &amp; \vec y\otimes\vec p, &amp; \vec z\otimes\vec p, \\
\vec x\otimes\vec q, &amp; \vec y\otimes\vec q, &amp; \vec z\otimes\vec q\hphantom{,}
\end{array}
\right\}.
$$</p>
<p>An example vector in the space $V\otimes W$ might be</p>
<p>$$
-i(\vec x\otimes\vec p) - 2(\vec y\otimes\vec p) + 3 (\vec z\otimes\vec p) +
\frac{1}{4}(\vec x\otimes\vec q) - \sqrt{5}(\vec y\otimes\vec q) + e^{6\pi}(\vec z\otimes\vec q),
$$</p>
<p>assuming these vector spaces are over $\mathbb{C}$.</p>
<p>Intuitively, a tensor product ‚Äújust‚Äù gives us a way to associate a number with each possible combination of basis vector. In our case, we need to associate a probability amplitude with each combination of distinguished qubit basis states. We need this ability since‚Äîas we‚Äôve established‚Äîwe need to consider every possible holistic outcome of a collection of qubits, as opposed to the outcomes of the qubits independently. (The former constitute both factorizable and non-factorizable states, while the latter only include factorizable states.)</p>
<h3 id="bit-string-notation-and-a-general-quantum-state">Bit-String notation and a general quantum state</h3>
<p>If we have qubits $X$, $Y$, and $Z$, then they‚Äôll live in the space $B_X\otimes B_Y\otimes B_Z$, which we‚Äôll call $Q_3$. It will be massively inconvenient to write the basis vectors as, for example, $\ket 0_X\otimes \ket 1_Y\otimes\ket 1_Z$, so we instead use the shorthand $\ket{011}$ when the space has been defined. This is called <strong>bit-string notation</strong>. A general element $\ket\psi$ of $Q_3$ can be written $$\psi_0\ket{000}+\psi_1\ket{001}+\psi_2\ket{010}+\psi_3\ket{011}+\psi_4\ket{100}+\psi_5\ket{101}+\psi_6\ket{110}+\psi_7\ket{111}.$$ There are two substantial benefits from using bit-string notation. These benefits are much more thoroughly explained in <a href="https://arxiv.org/abs/1711.02086">this paper</a>‚Äîwhich was a precursor to this very blog post.</p>
<p>The first benefit is that the names of the qubits‚Äî$X$, $Y$, and $Z$‚Äîhave been abstracted away. They‚Äôre now just positions in a bit-string, and we can canonically name the qubits according to their position. We record positions <em>from the right starting from zero</em>, so $X$ is in position $2$, $Y$ is in position $1$, and $Z$ is in position $0$.</p>
<p>The second benefit is one relevant to how we implement quantum states on a computer. As written, the probability amplitude $\psi_i$ has an index $i$ whose binary expansion matches the bit-string of the basis vector whose scalar component is $\psi_i$. This is no accident. The main outcome of this is that we can use a non-negative integer as a way of specifying a bit-string, which also acts as an index into an array of probability amplitudes. So for instance, the above state can be written further compactly as $$\ket\psi=\sum_{i=0}^7\psi_i\ket i.$$ Here, $\ket i$ refers to the $i$th bit-string in lexicographic (‚Äúdictionary‚Äù) order, or equivalently, the binary expansion of $i$ as a bit-string.</p>
<p>Since qubits live in a two-dimensional space, then $n$ qubits will live in a $2^n$-dimensional space. With a great deal of work, we‚Äôve come to our most general<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> representation of an $n$-qubit system: $$\sum_{i=0}^{2^n-1}\psi_i\ket i,$$ where $\vert\psi_i\vert^2$ gives us the probability of observing the bit-string $\ket i$, implying $$\sum_{i=0}^{2^n-1}\vert\psi_i\vert^2=1.$$</p>
<p>On a computer, representing a quantum state for an $n$-qubit system is simple: It‚Äôs just an array of $2^n$ complex numbers. An index $i$ into the array represents the probability amplitude $\psi_i$, which is the scalar component of $\ket{i}$. So, for instance, the state $\ket{000}$ in a 3-qubit system is represented by an array whose first element is $1$ and the rest $0$. Here is a function to allocate a new quantum state of $n$ qubits, initialized to be in the $\ket{\ldots 000}$ state:</p>
<pre tabindex="0"><code>(defun make-quantum-state (n)
  (let ((s (make-array (expt 2 n) :initial-element 0.0d0)))
    (setf (aref s 0) 1.0d0)
    s))
</code></pre><p>Sometimes, given a quantum state, or even an operator on a quantum
state, we will want to recover how many qubits the state represents,
or the operator acts on. In both cases, the question reduces to
determining the number of qubits that a dimension represents. Since
our dimensions are always powers of two, we need to compute the
equivalent of a binary logarithm. In Common Lisp, we can compute this
by computing the number of bits an integer takes to represent using
<code>integer-length</code>. The number $2^n$ is always a <code>1</code> followed by $n$
<code>0</code>‚Äôs, so the length of $2^n$ in binary is $n+1$.</p>
<pre tabindex="0"><code>(defun dimension-qubits (d)
  (1- (integer-length d)))
</code></pre><h3 id="evolving-the-quantum-state">Evolving the quantum state</h3>
<p>Since the quantum state is a vector, the principal way we change it is
through linear operators represented as matrices. As our quantum
program executes, we say that the quantum state
<em>evolves</em>. Matrix‚Äìvector multiplication is accomplished with
<code>apply-operator</code> and matrix‚Äìmatrix multiplication is accomplished
with <code>compose-operators</code>. There is nothing special about these
functions; they are the standard textbook algorithms.</p>
<pre tabindex="0"><code>(defun apply-operator (matrix column)
  (let* ((matrix-size (array-dimension matrix 0))
         (result (make-array matrix-size :initial-element 0.0d0)))
    (dotimes (i matrix-size)
      (let ((element 0))
        (dotimes (j matrix-size)
          (incf element (* (aref matrix i j) (aref column j))))
        (setf (aref result i) element)))
    (replace column result)))

(defun compose-operators (A B)
  (destructuring-bind (m n) (array-dimensions A)
    (let* ((l (array-dimension B 1))
           (result (make-array (list m l) :initial-element 0)))
      (dotimes (i m result)
        (dotimes (k l)
          (dotimes (j n)
            (incf (aref result i k)
                  (* (aref A i j)
                     (aref B j k)))))))))
</code></pre><p>These functions will sit at the heart of the interpreter, which will
be elaborated upon in <a href="#gates">the section about gates</a>.</p>
<h2 id="measurement">Measurement</h2>
<p>Already, through the construction of our quantum state, we‚Äôve
discussed the idea that the probability amplitudes imply a probability
of observing a state. Measurement then amounts to looking at a quantum
state as a discrete probability distribution and sampling from it.</p>
<p>Measurement in quantum mechanics is side-effectful; observation of a
quantum state also simultaneously <em>collapses</em> that state. This means
that when we measure a state to be a bit-string, then the state will
also <em>become</em> that bit-string, zeroing out every other component in the
process.</p>
<p>We thus implement the process of measurement in two steps: The
sampling of the state followed by its collapse.</p>
<pre tabindex="0"><code>(defun observe (machine)
  (let ((b (sample (machine-quantum-state machine))))
    (collapse (machine-quantum-state machine) b)
    (setf (machine-measurement-register machine) b)
    machine))
</code></pre><p>Note that we‚Äôve recorded our observation into the measurement register. We now proceed to define what we mean by <code>sample</code> and <code>collapse</code>.</p>
<p>How shall we sample? This is a classic problem in computer science. If we have $N$ events $\{0, 1,\ldots,N-1\}$, such that event $e$ has probability $P(e)$, then we can sample as follows. Consider the partial sums defined by the recurrence $S(0)=0$ and $S(k)=S(k-1) + P(k-1)$. If we draw a random number $r$ uniformly from $[0,1)$, then we wish to find the $k$ such that $S(k)\leq r &lt; S(k+1)$. Such a $k$ will be a sampling of our events according to the imposed probability distribution.</p>
<p>We can implement this simply by computing successive partial sums, until our condition is satisfied. In fact, we can be a little bit more resourceful. We can find when $r-S(k+1)&lt;0$, which amounts to successive updates $r\leftarrow r-P(k)$.</p>
<p>With a quantum system, we have $P(\ket i) = \vert\psi_i\vert^2$, and the sampled $k$ is the bit-string $\ket k$ we find.</p>
<p>Let‚Äôs do an example. Suppose we have a quantum state</p>
<p>$$
\sqrt{0.2}\ket{00} - \sqrt{0.07}\ket{01} + \sqrt{0.6}\ket{10} + \sqrt{0.13}\ket{11}.
$$</p>
<p>Then our discrete probability distribution is:</p>
<p>$$
P(\ket{00}) = 0.2\qquad P(\ket{01}) = 0.07\qquad P(\ket{10}) = 0.6\qquad P(\ket{11}) = 0.13
$$</p>
<p>Next, suppose we draw a random number $r = 0.2436$. We first check if $r &lt; 0.2$. It‚Äôs not, so $\ket{00}$ is not our sample. Subtract it from $r$ to get $r = 0.0436$. Next check if $r &lt; 0.07$. Yes, so our sample is $\ket{01}$. Pictorially, this looks like the following:</p>
<p><img src="https://www.stylewarning.com/posts/quantum-interpreter/images/sample.svg" alt="A process of selecting a random sample." decoding="async">
</p>
<p>The implementation is straightforward:</p>
<pre tabindex="0"><code>(defun sample (state)
  (let ((r (random 1.0d0)))
    (dotimes (i (length state))
      (decf r (expt (abs (aref state i)) 2))
      (when (minusp r) (return i)))))
</code></pre><p>Collapsing to $\ket k$ is simply zeroing out the array and setting $\psi_k$ to $1$.</p>
<pre tabindex="0"><code>(defun collapse (state basis-element)
  (fill state 0.0d0)
  (setf (aref state basis-element) 1.0d0))
</code></pre><h2 id="gates">Gates</h2>
<h3 id="gates-as-matrices">Gates as matrices</h3>
<p>Gates are the meat of most quantum algorithms. They represent the
‚Äúhard work‚Äù a quantum computer does. As previously described, a gate
$g$ is a transformation that is linear, invertible, and
length-preserving.</p>
<ul>
<li>
<p><strong>Linear</strong>: $g(a\ket\psi+b\ket\phi)=ag(\ket\psi)+bg(\ket\phi)$.</p>
</li>
<li>
<p><strong>Invertible</strong>: There is always an operation $h$ that can cancel out the effect of $g$: $h(g(\ket\psi))=g(h(\ket\psi))=\ket\psi$.</p>
</li>
<li>
<p><strong>Length-Preserving</strong>: $\Vert g(\ket\psi)\Vert = \Vert\ket\psi\Vert$.</p>
</li>
</ul>
<p>These ideas are captured by an overarching idea called a <strong>linear isometry</strong>, which comes from the Greek word <em>isometria</em>, with <em>isos</em> meaning ‚Äúequal‚Äù and <em>metria</em> meaning ‚Äúmeasuring‚Äù. As with all linear transformations, we can write them out as a matrix with respect to a particular basis. Matrices representing linear isometries are called <strong>unitary matrices</strong><sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p>
<p>The simplest gate must be identity, a gate which does nothing.</p>
<p>$$
\mathsf{I} := \begin{pmatrix}
1 &amp; 0\\
0 &amp; 1
\end{pmatrix}
$$</p>
<p>In Common Lisp, this would be defined as</p>
<pre tabindex="0"><code>(defparameter +I+ #2A((1 0)
                      (0 1)))
</code></pre><p>which we will make use of later. Just a notch higher in complexity
would be the quantum analog of a Boolean ‚ÄúNOT‚Äù. This is called the
$\mathsf{X}$ gate:</p>
<p>$$
\mathsf{X} := \begin{pmatrix}
0 &amp; 1\\
1 &amp; 0
\end{pmatrix}.
$$</p>
<p>This has the effect of mapping $\mathsf{X}\ket 0=\ket 1$, which means directly that $\mathsf{X}\ket 1=\ket 0$ and therefore it is its own inverse: $\mathsf{X}^{-1}\mathsf{X} = \mathsf{I}$.</p>
<p>We suggest re-reviewing how one interprets a matrix as an explicit
mapping of each element of the basis, as it helps make sense of
gates. In this tutorial, gate matrices are always specified in terms
of the bit-string basis</p>
<p>$$
\{\ket{\ldots000}, \ket{\ldots001}, \ket{\ldots010}, \ket{\ldots011}, \ldots\}.
$$</p>
<p>We again refer the reader to <a href="https://arxiv.org/abs/1711.02086">this
paper</a> for an in-depth discussion
about this basis.</p>
<p>In the rest of this section, the whole goal is to be able to apply
gates to our quantum state. There are two cases of pedagogical and
operational interest: the one-qubit gate and the many-qubit gate. We
will write two functions to accomplish each of these, in order to
implement a general function called <code>apply-gate</code> for applying any kind
of gate on any collection of qubits for any quantum state.</p>
<pre tabindex="0"><code>(defun apply-gate (state U qubits)
  (assert (= (length qubits) (dimension-qubits (array-dimension U 0))))
  (if (= 1 (length qubits))
      (%apply-1Q-gate state U (first qubits))
      (%apply-nQ-gate state U qubits)))
</code></pre><h3 id="gates-on-multi-qubit-machines">Gates on multi-qubit machines</h3>
<p>If we are working with the machine $M_n$, then our space is $2^n$-dimensional, and as such, our matrices would be written out as $2^n\times 2^n$ arrays of numbers. If we can write out such a matrix, then applying it is as simple as a matrix‚Äìvector multiplication. For instance, for a $4$-qubit machine, an $\mathsf{X}$ on qubit $0$ would be written</p>
<p>$$
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix},
$$</p>
<p>which could be readily applied to a $16$-element quantum state vector. It is easy to verify that this will swap the components of $\ket{\ldots 0}$ with the corresponding components of $\ket{\ldots 1}$.</p>
<p>But as should be plainly obvious from the obnoxious amount of paper wasted by writing out this matrix, it would be better if we could simply generate this matrix with just three pieces of information: the gate matrix $g=\left(\begin{smallmatrix}0 &amp; 1\\1 &amp; 0\end{smallmatrix}\right)$, the qubit index $i=0$, and the size of the machine $n=4$. This is a process we will call <strong>lifting</strong>.</p>
<p>Lifting requires a fundamental tool for constructing operators on spaces that were formed out of tensor products. If we have two finite-dimensional vector spaces $U$ and $V$, and operators $f$ and $g$ on the spaces respectively, then it seems reasonable to consider how $f$ and $g$ transform $U\otimes V$. In some sense, applying $f$ and $g$ ‚Äúin parallel‚Äù on $U\otimes V$ correspond to a new linear operator $h$. If $f$ and $g$ are matrices, then $h$ is defined by a <em>block matrix</em></p>
<p>$$
\begin{equation}
h_{i,j} = f_{i,j} g.
\label{eq:kron}
\end{equation}
$$</p>
<p>More specifically, let $0 \leq i,j &lt; \dim U$. The matrix $h$ will be
an array of $\dim U \times \dim U$ copies of $g$, where the entries of
the $(i,j)$th blocks are multiplied by the single
scalar $f_{i,j}$. This will lead to a matrix with $(\dim U)(\dim V)$
rows and columns, which is exactly the dimension of $U\otimes
V$. Incidentally, we write $h$ as $f\otimes g$, and this combination
of operators is called the <strong>Kronecker product</strong><sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>. As code:</p>
<pre tabindex="0"><code>(defun kronecker-multiply (A B)
  (destructuring-bind (m n) (array-dimensions A)
    (destructuring-bind (p q) (array-dimensions B)
      (let ((result (make-array (list (* m p) (* n q)))))
        (dotimes (i m result)
          (dotimes (j n)
            (let ((Aij (aref A i j))
                  (y (* i p))
                  (x (* j q)))
              (dotimes (u p)
                (dotimes (v q)
                  (setf (aref result (+ y u) (+ x v))
                        (* Aij (aref B u v))))))))))))
</code></pre><p><em>As a matter of terminology, remember that tensor products combine
vector spaces, and Kronecker products combine operator matrices.</em></p>
<h3 id="single-qubit-gates-and-gates-on-adjacent-qubits">Single-qubit gates and gates on adjacent qubits</h3>
<p>From here, we can very easily lift one-qubit gates to machines with
any number of qubits. A gate $g$ on qubit $i$ in an $n$-qubit machine
is just $g$ applied to qubit $i$ and the identity $\mathsf{I}$ on all
other qubits. Writing this out as a Kronecker product, we have</p>
<p>$$
\begin{equation}
\operatorname{lift}(g, i, n) :=
\underbrace{\mathsf{I} \otimes \mathsf{I} \otimes \cdots}_{n-i-1\text{ factors}}
\otimes g \otimes
\underbrace{\cdots \otimes \mathsf{I}}_{i\text{ factors}},
\label{eq:liftone}
\end{equation}
$$</p>
<p>where there are a total of $n$ factors, and $g$ is at positioned $i$ factors from the right.</p>
<p>This concept generalizes to higher-dimensional operators which act on <em>index-adjacent qubits</em>. In other words, if $g$ is a $k$-qubit operator <em>specifically</em> acting on qubits</p>
<p>$$
(i+k-1, i+k-2, \ldots, i+2, i+1, i),
$$</p>
<p>then the lifting operator from \eqref{eq:liftone} is much the same:</p>
<p>$$
\begin{equation}
\operatorname{lift}(g, i, n) := \underbrace{\mathsf{I} \otimes \mathsf{I} \otimes \cdots}_{n-i-k\text{ factors}}
\otimes g \otimes
\underbrace{\cdots \otimes \mathsf{I}}_{i\text{ factors}}.
\label{eq:liftmany}
\end{equation}
$$</p>
<p>It must be emphasized one last time: <em>This only works for multi-qubit operators that act on qubits that are index-adjacent.</em> We will get to how to work with non-adjacent qubits shortly, but first we will turn this into code.</p>
<p>For simplicity, we create a way to iterate a Kronecker product
multiple times, that is, compute</p>
<p>$$
\underbrace{g\otimes \cdots \otimes g}_{n\text{ factors}},
$$</p>
<p>which is usually simply written $g^{\otimes n}$. We must use care when
handling the case when we are ‚ÄúKronecker exponentiating‚Äù by a
non-positive number, so that $f\otimes g^{\otimes 0} = f$.</p>
<pre tabindex="0"><code>(defun kronecker-expt (U n)
  (cond
    ((&lt; n 1) #2A((1)))
    ((= n 1) U)
    (t (kronecker-multiply (kronecker-expt U (1- n)) U))))
</code></pre><p>With <code>kroncker-expt</code>, we can write <code>lift</code> following \eqref{eq:liftmany}:</p>
<pre tabindex="0"><code>(defun lift (U i n)
  (let ((left  (kronecker-expt +I+ (- n i (dimension-qubits
                                           (array-dimension U 0)))))
        (right (kronecker-expt +I+ i)))
    (kronecker-multiply left (kronecker-multiply U right))))
</code></pre><h3 id="multi-qubit-gates-on-non-adjacent-qubits">Multi-qubit gates on non-adjacent qubits</h3>
<p>In this section, we assume we are working on a multi-qubit machine
$M_n$ with $n\ge 2$.</p>
<h4 id="the-general-idea">The general idea</h4>
<p>So far, we‚Äôve managed to get away with lifting operators that act on
either a single qubit, or a collection of index-adjacent qubits. This
has been more-or-less trivial, because we can tack on a series of
identity operators by way of Kronecker products to simulate ‚Äúdoing
nothing‚Äù to the other qubits. However, if we want to apply a
multi-qubit gate to a collection of qubits that aren‚Äôt index-adjacent,
we have to be a little more clever.</p>
<p>The way we accomplish this is by swapping qubits around so that we can
move in and out of index-adjacency. In fact, for a given gate acting
on a given collection of qubits, we aim to compute an operator $\Pi$
which moves these qubits into index-adjacency, so that we can compute</p>
<p>$$
\begin{equation}
\Pi^{-1} \operatorname{lift}(g, 0, n) \Pi.
\label{eq:upq}
\end{equation}
$$</p>
<p>This recipe requires many ingredients, each of which we describe in
detail.</p>
<h4 id="swapping-two-qubits">Swapping two qubits</h4>
<p>To start, we need some way to swap the state of two qubits. We can do
this with the $\mathsf{SWAP}$ operator:</p>
<p>$$
\mathsf{SWAP} := \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
$$</p>
<p>In Common Lisp, we define this in the same way we defined <code>+I+</code>.</p>
<pre tabindex="0"><code>(defparameter +SWAP+ #2A((1 0 0 0)
                         (0 0 1 0)
                         (0 1 0 0)
                         (0 0 0 1)))
</code></pre><p>The $\mathsf{SWAP}$ operator takes two qubits and swaps their
state. What does this mean in a system of correlations, where qubit
state isn‚Äôt strictly compartmentalized (i.e., factorized)? Swapping is
equivalent to swapping the component of $\ket{01}$ with the component
of $\ket{10}$, which are the only two distinguishable
correlations<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>. Still, in a multi-qubit system, we can‚Äôt
immediately arbitrarily swap two qubits with the tools we‚Äôve
developed. What we can do is swap index-adjacent qubits. In
particular, we can define the transpositions</p>
<p>$$
\tau_i := \operatorname{lift}(\mathsf{SWAP}, i, n),\qquad \text{with }0\leq i &lt; n - 1.
$$</p>
<p>The transposition $\tau_i$ swaps qubit $i$ with qubit $i+1$. This is
our first ingredient.</p>
<h4 id="re-arranging-qubits-to-be-index-adjacent">Re-arranging qubits to be index-adjacent</h4>
<p>The second ingredient is a way to re-arrange our qubits so that they
are index-adjacent. Suppose we have a three-qubit operator $g$ which
acts on qubits $(2, 4, 3)$ in a machine of $n=5$ qubits. The space in
which the quantum state of $M_5$ lives is</p>
<p>$$
B_4 \otimes B_3 \otimes B_2 \otimes B_1 \otimes B_0,
$$</p>
<p>but we need to re-arrange our state vector as if we‚Äôve moved $B_2\to
B_0$, $B_4\to B_1$, and $B_3\to B_2$ so that our sub-state sits
index-adjacent. In combinatorics, this permutation is written in
two-line notation</p>
<p>$$
\begin{pmatrix}
0 &amp; 1 &amp; 2 &amp; 3 &amp; 4\\
3 &amp; 4 &amp; 0 &amp; 2 &amp; 1
\end{pmatrix}.
$$</p>
<p>Here, we‚Äôve made a few arbitrary decisions. First, we‚Äôve decided to
re-map a $k$-qubit operator to the $B_{k-1}\otimes\cdots\otimes
B_1\otimes B_0$ subspace. Any other index-adjacent subspace would
work, but this simplifies the code. Second, we see that $0\mapsto 3$
and $1\mapsto 4$, but it doesn‚Äôt matter so much where they map to, as
long as $2$, $4$, and $3$ are mapped correctly.</p>
<p>There‚Äôs no sense in writing the first line in two-line notation, so we
just write the permutation compactly as $34021$. As a quantum
operator, we write this as $\Pi_{34021}$.</p>
<p>The question is: How can we write $\Pi_{34021}$ as familiar operators?
It is a well-known fact in combinatorics that any permutation can be
decomposed into a composition of swaps, and every swap can be
decomposed into a series of adjacent transpositions. We leave this as
an exercise<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>, but we will show the code to our implementation.</p>
<p>We start with a function which takes a permutation written as a list,
like <code>(3 4 0 2 1)</code>, and converts it to a list of (possibly
non-adjacent) transpositions to be applied left-to-right, represented
as cons cells <code>((0 . 3) (1 . 4) (2 . 3))</code>.</p>
<pre tabindex="0"><code>(defun permutation-to-transpositions (permutation)
  (let ((swaps nil))
    (dotimes (dest (length permutation) (nreverse swaps))
      (let ((src (elt permutation dest)))
        (loop :while (&lt; src dest) :do
          (setf src (elt permutation src)))
        (cond
          ((&lt; src dest) (push (cons src dest) swaps))
          ((&gt; src dest) (push (cons dest src) swaps)))))))
</code></pre><p>Next, we convert these transpositions as cons cells to adjacent
transposition indexes. This is straightforward. If we are swapping
$(a,b)$ with $a&lt;b$, then we transpose $(a, a+1)$, then $(a+1, a+2)$,
and so on until $(b-1, b)$, followed by a reversal of each except
$(b-1, b)$. We can simply write this chain of adjacent transpositions
as $(a, a+1, \ldots, b-1, \ldots, a+1, a)$. In this example, we‚Äôd have
the transposition indexes <code>(0 1 2 1 0 1 2 3 2 1 2)</code>.</p>
<pre tabindex="0"><code>(defun transpositions-to-adjacent-transpositions (transpositions)
  (flet ((expand-cons (c)
           (if (= 1 (- (cdr c) (car c)))
               (list (car c))
               (let ((trans (loop :for i :from (car c) :below (cdr c)
                                  :collect i)))
                 (append trans (reverse (butlast trans)))))))
    (mapcan #'expand-cons transpositions)))
</code></pre><p>These are indexes $i_1, i_2, \ldots$ such that $\Pi = \cdots
\tau_{i_2}\tau_{i_1}$</p>
<p>The last ingredient we need is inverting $\Pi$. If we have $\Pi$
represented as a sequence of $\tau$, then we simply reverse the list
of $\tau$.</p>
<h4 id="using-transpositions-to-implement-multi-qubit-gates">Using transpositions to implement multi-qubit gates</h4>
<p>With all of these, we write what is perhaps the most important
function of our interpreter.</p>
<pre tabindex="0"><code>(defun %apply-nQ-gate (state U qubits)
  (let ((n (dimension-qubits (length state))))
    (labels ((swap (i)
               (lift +swap+ i n))
             (transpositions-to-operator (trans)
               (reduce #'compose-operators trans :key #'swap)))
      (let* ((U01 (lift U 0 n))
             (from-space (append (reverse qubits)
                                 (loop :for i :below n
                                       :when (not (member i qubits))
                                         :collect i)))
             (trans (transpositions-to-adjacent-transpositions
                     (permutation-to-transpositions
                      from-space)))
             (to-&gt;from (transpositions-to-operator trans))
             (from-&gt;to (transpositions-to-operator (reverse trans)))
             (Upq (compose-operators to-&gt;from
                                     (compose-operators U01
                                                        from-&gt;to))))
        (apply-operator Upq state)))))
</code></pre><p>A few quick notes for comprehension:</p>
<ul>
<li>
<p>The value of <code>(swap i)</code> is $\tau_i$ fully lifted.</p>
</li>
<li>
<p>The one-line zinger that defines <code>transpositions-to-operator</code> takes
a list of transposition indexes and converts it into a unitary
operator. It does so by doing what‚Äôs known in functional programming
as a <em>map-reduce</em>, by first mapping $i\mapsto\tau_i$ and reducing by
operator composition.</p>
</li>
<li>
<p>The variable <code>from-space</code> contains the permutation $p$ that encodes
the space in which we‚Äôd like to act. This permutation is calculated
based off of the <code>qubits</code> argument.</p>
</li>
<li>
<p>The variables <code>from-&gt;to</code> and <code>to-&gt;from</code> represent $\Pi_p$ and
$\Pi^{-1}_p$ respectively.</p>
</li>
<li>
<p>The variable <code>Upq</code> is our fully lifted operator, exactly by way of
\eqref{eq:upq}.</p>
</li>
</ul>
<p>The function <code>%apply-nQ-gate</code> is what allows our interpreter to be so
general. Making the interpreter more efficient ultimately is an
exercise in making this function more efficient.</p>
<p>The only thing left to do is integrate all of the topics discussed
hitherto into an interpreter!</p>

<h3 id="the-driver-loop">The driver loop</h3>
<p>The bulk of the interpreter has been written. We‚Äôve described the
semantics of the two instructions of interest: <code>MEASURE</code> and
<code>GATE</code>. Now we create the interpreter itself, which is just a driver
loop to read and execute these instructions, causing state transitions
of our abstract machine. If we see a <code>GATE</code>, we call <code>apply-gate</code>. If
we see a <code>MEASURE</code>, we call <code>observe</code>.</p>
<pre tabindex="0"><code>(defun run-quantum-program (qprog machine)
  (loop :for (instruction . payload) :in qprog
        :do (ecase instruction
              ((GATE)
               (destructuring-bind (gate &amp;rest qubits) payload
                 (apply-gate (machine-quantum-state machine) gate qubits)))
              ((MEASURE)
               (observe machine)))
        :finally (return machine)))
</code></pre><h3 id="efficiency">Efficiency</h3>
<p>Performance-focused individuals will have noticed that this
interpreter is pretty costly, in many ways. The biggest cost is also
unavoidable: The fact that our state grows exponentially with the
number of qubits. Real, physical quantum computers avoid this cost,
which makes them alluring machines to both study and construct.</p>
<p>However, even with this unavoidable cost, this interpreter has been
implemented for ease of understanding and not machine
efficiency. Writing a faster interpreter amounts to avoiding the
construction of the lifted operator matrices. This can be done with
very careful index wrangling and sensitivity to data types and
allocation. This is how the high-performance <a href="https://github.com/quil-lang/qvm">Quantum Virtual
Machine</a> is implemented.</p>
<h2 id="examples">Examples</h2>
<p>What good is writing an interpreter if we don‚Äôt write any programs
worth interpreting? Here are a few examples of programs.</p>
<h3 id="bell-state">Bell state</h3>
<p>The <strong>Bell state</strong> is one which we‚Äôve explored earlier. It is a
two-qubit state $$\frac{1}{\sqrt{2}}(\ket {00} + \ket {11}).$$ Here‚Äôs
a program to generate one, using two new gates, the <strong>controlled-not
gate</strong> $\mathsf{CNOT}$ and the <strong>Hadamard gate</strong> $\mathsf{H}$.</p>
<pre tabindex="0"><code>(defparameter +H+ (make-array '(2 2) :initial-contents (let ((s (/ (sqrt 2))))
                                                         (list (list s s)
                                                               (list s (- s))))))

(defparameter +CNOT+ #2A((1 0 0 0)
                         (0 1 0 0)
                         (0 0 0 1)
                         (0 0 1 0))))

(defun bell (p q)
  `((GATE ,+H+ ,p)
    (GATE ,+CNOT+ ,p ,q)))
</code></pre><h3 id="greenberger--horne--zeilinger-state">Greenberger‚ÄìHorne‚ÄìZeilinger state</h3>
<p>The <strong>Greenberger‚ÄìHorne‚ÄìZeilinger state</strong>, or <strong>GHZ state</strong>, is a
generalization of the Bell state on more than two qubits, namely
$$\frac{1}{\sqrt{2}}(\ket{0\ldots 000} + \ket{1\ldots 111}).$$ This is
accomplished by executing a chain of controlled-not gates:</p>
<pre tabindex="0"><code>(defun ghz (n)
  (cons `(GATE ,+H+ 0)
        (loop :for q :below (1- n)
              :collect `(GATE ,+CNOT+ ,q ,(1+ q)))))
</code></pre><h3 id="the-quantum-fourier-transform">The quantum Fourier transform</h3>
<p>The ordinary discrete Fourier transform of a complex vector is a
unitary operator, and as such, it can be encoded as a quantum
program. We will write a program which computes the Fourier transform
of the probability amplitudes of an input quantum state (a time-domain
signal), producing a new quantum state whose amplitudes represent
components in the frequency domain. This is the central subroutine to
Shor‚Äôs algorithm, which is a quantum algorithm which factors integers
faster than any known classical method.</p>
<p>First, we will need a gate called the <strong>controlled-phase gate</strong> $\mathsf{CPHASE}(\theta)$:</p>
<pre tabindex="0"><code>(defun cphase (angle)
  (make-array '(4 4) :initial-contents `((1 0 0 0)
                                         (0 1 0 0)
                                         (0 0 1 0)
                                         (0 0 0 ,(cis angle)))))
</code></pre><p>Now, we can generate the quantum Fourier transform recursively.</p>
<pre tabindex="0"><code>(defun qft (qubits)
  (labels ((bit-reversal (qubits)
             (let ((n (length qubits)))
               (if (&lt; n 2)
                   nil
                   (loop :repeat (floor n 2)
                         :for qs :in qubits
                         :for qe :in (reverse qubits)
                         :collect `(GATE ,+swap+ ,qs ,qe)))))
           (%qft (qubits)
             (destructuring-bind (q . qs) qubits
               (if (null qs)
                   (list `(GATE ,+H+ ,q))
                   (let ((cR (loop :with n := (1+ (length qs))
                                   :for i :from 1
                                   :for qi :in qs
                                   :for angle := (/ pi (expt 2 (- n i)))
                                   :collect `(GATE ,(cphase angle) ,q ,qi))))
                     (append
                      (qft qs)
                      cR
                      (list `(GATE ,+H+ ,q))))))))
    (append (%qft qubits) (bit-reversal qubits))))
</code></pre><p>The program for a three-qubit quantum Fourier transform <code>(qft '(0 1 2))</code> looks like this:</p>
<pre tabindex="0"><code>(
  (GATE #2A((0.7071067811865475d0 0.7071067811865475d0) (0.7071067811865475d0 -0.7071067811865475d0)) 2) 
  (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 1 0) (0 0 0 #C(0.0d0 1.0d0))) 1 2) 
  (GATE #2A((0.7071067811865475d0 0.7071067811865475d0) (0.7071067811865475d0 -0.7071067811865475d0)) 1) 
  (GATE #2A((1 0 0 0) (0 0 1 0) (0 1 0 0) (0 0 0 1)) 1 2) 
  (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 1 0) (0 0 0 #C(0.7071067811865476d0 0.7071067811865475d0))) 0 1) 
  (GATE #2A((1 0 0 0) (0 1 0 0) (0 0 1 0) (0 0 0 #C(0.0d0 1.0d0))) 0 2) 
  (GATE #2A((0.7071067811865475d0 0.7071067811865475d0) (0.7071067811865475d0 -0.7071067811865475d0)) 0) 
  (GATE #2A((1 0 0 0) (0 0 1 0) (0 1 0 0) (0 0 0 1)) 0 2) 
)
</code></pre><p>(Recall that <code>#C(0 1)</code> represents the complex number $i$.)</p>
<p>We can see the quantum Fourier transform in action by computing the
Fourier transform of $\ket{000}$. Here is a transcript of this
calculation:</p>
<pre tabindex="0"><code>CL-USER&gt; (run-quantum-program
          (qft '(0 1 2))
          (make-machine :quantum-state (make-quantum-state 3)
                        :measurement-register 0))
#S(MACHINE
   :QUANTUM-STATE #(#C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0))
   :MEASUREMENT-REGISTER 0)
</code></pre><p>Indeed, one can verify that the classical Fourier transform of the
vector $[1,0,0,0,0,0,0,0]$ is a vector with eight components equal to
about $0.35355$.</p>
<pre tabindex="0"><code>$ python
Python 2.7.16 (default, May 23 2023, 14:13:27) 
[GCC 8.3.0] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.fft.fft([1,0,0,0,0,0,0,0], norm="ortho")
array([0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j,
       0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j, 0.35355339+0.j])
</code></pre><h3 id="example-transcript">Example transcript</h3>
<p>Here is an example transcript downloading and using this software,
using <a href="https://www.sbcl.org/">Steel Bank Common Lisp</a>.</p>
<pre tabindex="0"><code>$ git clone https://github.com/stylewarning/quantum-interpreter.git
Cloning into 'quantum-interpreter'...
remote: Counting objects: 10, done.
remote: Compressing objects: 100% (10/10), done.
Unpacking objects: 100% (10/10), done.
remote: Total 10 (delta 2), reused 5 (delta 0), pack-reused 0

$ cd quantum-interpreter/

$ sbcl --noinform
* (load "qsim.lisp")
T

* (load "examples.lisp")
T

* (run-quantum-program (bell 0 1)
                       (make-machine :quantum-state (make-quantum-state 2)
                                     :measurement-register 0))
#S(MACHINE
   :QUANTUM-STATE #(0.7071067690849304d0 0.0d0 0.0d0 0.7071067690849304d0)
   :MEASUREMENT-REGISTER 0)

* (run-quantum-program (qft '(0 1 2))
                       (make-machine :quantum-state (make-quantum-state 3)
                                     :measurement-register 0))
#S(MACHINE
   :QUANTUM-STATE #(#C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0)
                    #C(0.3535533724408484d0 0.0d0) #C(0.3535533724408484d0 0.0d0))
   :MEASUREMENT-REGISTER 0)

* (defun flip-coin ()
    (machine-measurement-register
     (run-quantum-program
      `((GATE ,+H+ 0) (MEASURE))
      (make-machine :quantum-state (make-quantum-state 1)
                    :measurement-register 0))))
FLIP-COIN

* (loop :repeat 10 :collect (flip-coin))
(1 1 0 1 1 0 0 1 0 1)

* (quit)
</code></pre><h2 id="source-code">Source code</h2>
<p>The source code in this tutorial are published under the BSD 3-clause
license. The complete listing and most up-to-date source code can be
found on
<a href="https://github.com/stylewarning/quantum-interpreter">GitHub</a>.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>A controlled one-qubit gate is a kind of two-qubit gate.&nbsp;<a href="#fnref:1" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>It‚Äôs actually 124 SLOC, and it has <em>not</em> been ‚Äúcode golfed‚Äù. If we wanted to make an ever tinier quantum interpreter, we could‚Äîbut brevity for its own sake is not the point.&nbsp;<a href="#fnref:2" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>With only a little bit of extra work, mostly bookkeeping, we could make $n$ finite but unbounded during the execution of a program by having instead a collection of so-called <strong>quantum registers</strong>. These would be realized by instead a collection of $v$‚Äôs, which are opportunistically combined with entanglement occurs.&nbsp;<a href="#fnref:3" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>For that matter, why complex numbers, and not just real-valued probabilities? The reason is that a complex number of unit norm can be written as $e^{i\theta}$, where $\theta$ is called the <strong>phase</strong>. Phases are a wave-like property, and allow the complex probability amplitudes to <em>interfere</em>. Interference is a known and understood phenomenon of quantum mechanical systems, and in fact is critical to the function of a quantum computer.&nbsp;<a href="#fnref:4" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Spaces with all of these properties, including a way to calculate distances, are called <strong>Hilbert spaces</strong>.&nbsp;<a href="#fnref:5" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>The fact of the matter is that we can actual get <em>more</em> general by having classical probability distributions of these states, which leads one to so-called ‚Äúdensity operators‚Äù. This is extremely useful when studying imperfect quantum computers which have noisy operations.&nbsp;<a href="#fnref:6" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>While we won‚Äôt use this fact in our interpreter, even though it would be useful for error checking, it is very easy to check if a matrix is unitary. First, we compute another matrix $h$ which is the conjugate-transpose of $g$. The <strong>conjugate-transpose</strong> of a matrix is just the transpose of a matrix with each complex entry conjugated. Once we have this matrix, we check that $hg$ is an identity matrix. The matrix $g$ is unitary if and only if $hg=gh=\mathsf{I}$.&nbsp;<a href="#fnref:7" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Unfortunately, the definition \eqref{eq:kron} seems somewhat arbitrary and out of nowhere. Fortunately, there is a much more ‚Äúfirst principles‚Äù approach to understanding the tensor product and the Kronecker product, starting with how we map a <em>pair</em> of vectors $v\in V$ and $w\in W$ to a vector $v\otimes w\in V\otimes W$. Such approach is much more satisfying to a mathematician, and even essential to understanding the ‚Äútrue nature‚Äù of the tensor product, but perhaps less so to a curious implementer.&nbsp;<a href="#fnref:8" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>There is no sense in moving $\ket{00}$ or $\ket{11}$ to accomplish a swap-like operation, since we identify each qubits' respective $\ket 0$ identically, and each $\ket 1$ identically.&nbsp;<a href="#fnref:9" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>If you‚Äôre not particularly keen to figure out the math yourself, you might consult Lemma 14.1 of <a href="https://www.sfu.ca/~mdevos/notes/geom-sym/14_transpositions.pdf">these lecture notes</a>. You‚Äôre also welcome to just take my word for it!&nbsp;<a href="#fnref:10" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infrared may no longer be a punchline, as IEEE approves 9.6Gbps wireless light (129 pts)]]></title>
            <link>https://www.theverge.com/23795599/li-fi-wi-fi-802-11-bb-internet-of-light</link>
            <guid>36748448</guid>
            <pubDate>Sun, 16 Jul 2023 15:59:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/23795599/li-fi-wi-fi-802-11-bb-internet-of-light">https://www.theverge.com/23795599/li-fi-wi-fi-802-11-bb-internet-of-light</a>, See on <a href="https://news.ycombinator.com/item?id=36748448">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We‚Äôve known <a href="https://www.theverge.com/2013/10/28/5038130/internet-of-light-scientists-achieve-10-gigabit-data-speeds-using-leds">for over a decade</a> that blinking light bulbs can transfer <em>substantial</em> quantities of wireless data, not just dumb infrared commands to your TV. Now, the IEEE standards body behind Wi-Fi has decided to formally invite ‚ÄúLi-Fi‚Äù to the same table ‚Äî with speeds of between 10 megabits per second and 9.6 gigabits per second using invisible infrared light.</p><p>As of June 2023, the IEEE 802.11 wireless standard now <a href="https://standards.ieee.org/ieee/802.11bb/10823/">officially recognizes</a> wireless light communications as a physical layer for wireless local area networks, which is a fancy way of saying that that Li-Fi doesn‚Äôt need to compete with Wi-Fi. Light can be just another kind of access point and interface delivering the same networks and/or the same internet to your device. </p><p>In fact, at least one IEEE member has been experimenting with networks that use Wi-Fi and Li-Fi simultaneously to overcome one another‚Äôs disadvantages, intelligently steering some office computers to Li-Fi vs. Wi-Fi to improve the entire network. </p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="(The initial experiment used just four headless Intel NUC computers, it seems, but saw a ‚Äúcollision probability drop‚Äù from 19 percent to 10 percent.)" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/376x208/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/384x212/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/415x229/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/480x265/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/540x298/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/640x354/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/750x414/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/828x457/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/1080x597/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/1200x663/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/1440x796/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/1920x1061/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/2048x1131/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/2400x1326/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1591x879/2400x1326/filters:focal(796x440:797x441):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789553/lifi_wifi.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>(The initial experiment used just four headless Intel NUC computers, it seems, but saw a ‚Äúcollision probability drop‚Äù from 19 percent to 10 percent.)</em></figcaption> <p><cite>Image: CableLabs / IEEE</cite></p></div></div><p>See, Li-Fi products aren‚Äôt actually new: companies have tried to sell them for a number of years. There‚Äôs even already a competing standard, the International Telecommunication Union‚Äôs G.9991, which appears in <a href="https://www.theverge.com/2019/6/20/18692656/signify-philips-hue-truelifi-li-fi-wi-fi-lighting-250-mbps">data-beaming bulbs from Philips Hue maker Signify </a>among other things. </p><p>These companies have been banking on the fact that light can deliver a speedy, private, direct-line-of-sight connection with no radio interference ‚Äî amid concerns that lighting conditions can vary drastically and it‚Äôs all too easy to accidentally sever a line-of-sight connection. My colleague Jake illustrated the pros and cons <a href="https://www.theverge.com/ces/2018/1/7/16861178/lifi-internet-wifi-lamp-ces-2018">when he tried a Li-Fi lamp in 2018</a>.</p><p>In its experiment writeup, CableLabs doesn‚Äôt deny that Light Communication (LC) has room for improvement. ‚ÄúLC range is very sensitive to irradiance and incidence angles making dynamic beam steering (and LoS availability) attractive for future LC evolution,‚Äù reads one line in the study. </p><p>‚ÄúEnterprise Wi-Fi and state-of-the-art LC performance on par but LC reliability needs to be improved. A possible approach is the use of multiple, distributed optical frontends,‚Äù reads another.</p><div><p>‚ÄúReliability needs to be improved‚Äù</p></div><p><a href="https://www.businesswire.com/news/home/20230712214664/en/Global-LiFi-Firms-Welcome-the-Release-of-IEEE-802.11bb-Global-Light-Communications-Standard">The reason we‚Äôre hearing about this now</a> isn‚Äôt because the IEEE made a big deal about it, by the way ‚Äî it‚Äôs because the company that hired the man who coined ‚ÄúLi-Fi,‚Äù Dr. Harald Haas, really wants to take this opportunity to sell its newest product, and task group member Fraunhofer wants to be recognized for its contribution. </p><p>PureLiFi just launched <a href="http://https/a/www.businesswire.com/news/home/20230227005066/en/pureLiFi-Launches-Light-Antenna-ONE-a-LiFi-Module-Ready-for-Integration-Into-Billions-of-Devices-at-MWC-Barcelona">the Light Antenna One</a> in February, a module small enough it can theoretically be integrated into smartphones, which it claims can already deliver over 1Gbps  depending on the use case. (It‚Äôs only rated to communicate with devices that are under 10 feet away, and it has a 24-degree field of view when transmitting back.) PureLiFi says it‚Äôs already compliant with the 802.11bb standard and is ready ‚Äúto enable mass integration of LiFi for the first time.‚Äù</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="PureLiFi‚Äôs brochure for the Light Antenna One." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/376x441/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/384x450/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/415x487/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/480x563/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/540x633/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/640x750/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/750x879/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/828x971/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/1080x1266/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/1200x1407/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/1440x1689/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/1920x2251/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/2048x2402/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/2400x2814/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:753x883/2400x2814/filters:focal(377x442:378x443):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24789575/pure_lifi_brochure.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>PureLiFi‚Äôs brochure for the Light Antenna One.</em></figcaption> <p><cite>Image: PureLiFi</cite></p></div></div><p>The IEEE task group has <a href="https://mentor.ieee.org/802.11/dcn/23/11-23-0277-01-0000-ieee-802-standards-on-light-communication.pdf">a 59-page overview (PDF) of 802 light communication standards</a> if you‚Äôd like to learn more, including links to a number of video tech demos. It also mentions parallel efforts on 802.15.13 for industrial and medical applications. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Citizen scientists observe gamma ray glow associated with lightning flash (109 pts)]]></title>
            <link>https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023GL103612</link>
            <guid>36748393</guid>
            <pubDate>Sun, 16 Jul 2023 15:54:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023GL103612">https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023GL103612</a>, See on <a href="https://news.ycombinator.com/item?id=36748393">Hacker News</a></p>
Couldn't get https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023GL103612: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Zoom fatigue unpacked (127 pts)]]></title>
            <link>https://leadership.garden/zoom-fatigue/</link>
            <guid>36748235</guid>
            <pubDate>Sun, 16 Jul 2023 15:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leadership.garden/zoom-fatigue/">https://leadership.garden/zoom-fatigue/</a>, See on <a href="https://news.ycombinator.com/item?id=36748235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">

<p>As the world moved to virtual meetings and online communication, a new phenomenon emerged: Zoom fatigue. The term refers to the exhaustion and burnout that people experience after prolonged periods of virtual meetings. This issue has become so widespread that it has led to a spike of interest and research in the past few years.</p>
<p>Dr. Jeremy Bailenson, a professor of communication and the founding director of the Virtual Human Interaction Lab at Stanford University, has been at the forefront of this research. In a&nbsp;<a href="https://tmb.apaopen.org/pub/nonverbal-overload/release/2?ref=leadership.garden#s3">2021 article</a>, he identified four key culprits behind Zoom fatigue: excessive amounts of close-up eye contact, constantly seeing oneself on camera, reduced mobility, and increased cognitive load.</p>
<p>While virtual meetings have their benefits, they also have drawbacks that can lead to exhaustion and burnout. This blog post will delve into the phenomenon of Zoom fatigue and its causes, as well as provide practical tips and strategies to help overcome virtual exhaustion. From adjusting your camera settings to taking regular breaks, there are many ways to combat Zoom fatigue and improve your virtual meeting experience. So, let‚Äôs unpack the phenomenon of Zoom fatigue and learn how to overcome it together.</p>
<h3 id="cognitive-overload"><strong>Cognitive overload</strong></h3>
<figure><img decoding="async" width="1024" height="319" src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-1024x319.jpg" alt="zoom fatigue cognitive overload" srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload.jpg 1952w" sizes="(max-width: 1024px) 100vw, 1024px" title="Zoom Fatigue Unpacked: Understanding and Overcoming Virtual Exhaustion 3" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20319'%3E%3C/svg%3E" data-lazy-srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload.jpg 1952w" data-lazy-src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-cognitive-overload-1024x319.jpg"></figure>
<p>During video calls, non-verbal communication is limited due to the constraints of the medium. It becomes challenging to accurately interpret facial expressions and body language of others, as well as effectively convey our own non-verbal cues. In such situations, we may have to resort to making larger gestures, nodding excessively, and ensuring our hand movements are visible on camera.</p>
<p>Subtle non-verbal cues may not suffice, and as a result, communicating and understanding each other becomes more challenging compared to an in-person interaction involving the same five individuals.</p>
<p>Unlike physical interactions, where we can gauge the group‚Äôs attention through implicit consensus, such as seeing multiple faces turned towards a person, this does not apply to online meetings.</p>
<p>Since it‚Äôs more challenging to read and convey non-verbal cues, participants may need to put in more effort to understand each other, leading to mental exhaustion. Additionally, having to remain engaged and focused throughout the call without the usual breaks and change of scenery that come with in-person interactions can also contribute to fatigue.</p>
<p>Here are some tips to help you overcome cognitive load in virtual meetings and calls:</p>
<ol>
<li><strong>Use a quiet and well-lit space</strong>: Find a quiet and well-lit area to join your virtual meeting or call. This will help you focus on the content without distractions, and it will reduce the cognitive load on your brain.</li>
<li><strong>Turn off notifications and other distractions</strong>: Notifications from other apps, emails, or messages can cause distractions and increase cognitive load. Turn off notifications and close other applications to stay focused.</li>
<li><strong>Take notes</strong>: Taking notes can help you remember important points and reduce cognitive load. Use a notepad or an app to jot down key information during the meeting or call.</li>
<li><strong>Use visual aids</strong>: Visual aids, like slides or images, can help to reduce cognitive load by providing a visual representation of the information. Ask the presenter if they have any visual aids that can help you understand the content better.</li>
<li><strong>Take breaks</strong>: Similar to in-person meetings, virtual meetings and calls can benefit from taking breaks. Schedule breaks in between meetings or calls to rest your mind, stretch your body, and recharge your energy.</li>
</ol>
<p>By implementing these tips, you can reduce cognitive load and overcome Zoom fatigue in virtual meetings and calls. With a few simple adjustments to your virtual meeting routine, you can improve your concentration, engagement, and overall productivity.</p>
<h3 id="lack-of-movement"><strong>Lack of&nbsp;Movement</strong></h3>
<figure><img decoding="async" width="1024" height="319" src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-1024x319.jpg" alt="zoom fatigue lack of movement" srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement.jpg 1952w" sizes="(max-width: 1024px) 100vw, 1024px" title="Zoom Fatigue Unpacked: Understanding and Overcoming Virtual Exhaustion 4" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20319'%3E%3C/svg%3E" data-lazy-srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement.jpg 1952w" data-lazy-src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-lack-of-movement-1024x319.jpg"></figure>
<p>Virtual video meetings lack the mobility options of audio-only calls or physical meetings. During video calls, we are required to remain stationary and avoid excessive movement. In contrast, phone calls allow us to take a walk, look out the window, or make ourselves a cup of tea. Similarly, in-person meetings provide opportunities to move around, stand up, or stretch.</p>
<p>Studies have consistently shown that movement is beneficial for both&nbsp;<a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fa0036577&amp;ref=leadership.garden">creativity</a>&nbsp;and&nbsp;<a href="https://link.springer.com/article/10.1007/s10648-015-9325-3?ref=leadership.garden">learning</a>, in addition to promoting better posture and overall health. Given the sedentary nature of remote calls, it is essential to increase opportunities for movement throughout the day.</p>
<p>Here are some tips to help you overcome this issue:</p>
<ol>
<li><strong>Take advantage of audio-only calls</strong>: When possible, opt for audio-only calls instead of video calls. This allows you to move around, go for a walk, or even do some light exercises while still participating in the call. You can try a standing desk or even a&nbsp;<a href="https://www.mccoyrockford.com/the-pos-and-cons-of-treadmill-desks/?ref=leadership.garden">treadmill desk</a>!</li>
<li><strong>Stand up and stretch</strong>: If you need to participate in a video call, take breaks to stand up and stretch. Simple stretches like shoulder rolls, neck stretches, and wrist rotations can help alleviate tension and improve circulation.</li>
<li><strong>Incorporate movement breaks</strong>: Schedule regular movement breaks throughout the day, whether it‚Äôs a short walk, a quick workout, or some simple stretches. You can also use breaks between meetings to move around and get some fresh air.</li>
</ol>
<h3 id="eye-contact-at-a-close-distance">Eye Contact at a Close Distance</h3>
<figure><img decoding="async" width="1024" height="319" src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-1024x319.jpg" alt="zoom fatigue eye contact" srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact.jpg 1952w" sizes="(max-width: 1024px) 100vw, 1024px" title="Zoom Fatigue Unpacked: Understanding and Overcoming Virtual Exhaustion 5" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20319'%3E%3C/svg%3E" data-lazy-srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact.jpg 1952w" data-lazy-src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-eye-contact-1024x319.jpg"></figure>
<p>In our daily interactions, we rarely have close face-to-face contact with others, except perhaps with our significant other or occasionally with family members. From an evolutionary perspective, close facial proximity typically signals either an impending conflict or intimate engagement. When someone steps too close, we may feel uneasy and try to look elsewhere to reduce our discomfort. For instance, when riding in an elevator with a stranger, we might find ourselves staring at the walls, floor, or ceiling to avoid direct eye contact.</p>
<p>During video meetings, extended periods of virtual eye contact can create a similar sense of discomfort. Depending on the distance to your screen, you may feel as though you are in a confined space.</p>
<p>The number of participants can also impact this feeling; in a one-on-one call, the other person‚Äôs face will occupy the entire screen and appear much larger and closer than if there were many participants. Additionally, maintaining eye contact throughout a virtual meeting can be taxing.</p>
<p>Unlike in-person meetings, where eye contact shifts naturally depending on who is speaking, all participants appear to be looking directly at you for the entire meeting, which can be stressful if you are not used to being the center of attention for an extended period. If you are accustomed to performing in public, such as in stand-up comedy or music, you may be less susceptible to this stress.</p>
<p><strong>One tip to overcome this is to use speaker view</strong>: Most virtual meeting platforms have a ‚Äúspeaker view‚Äù option that highlights the person who is currently speaking. Use this feature to reduce the amount of time you spend looking at other participants‚Äô faces.</p>
<h3 id="an-all-day-mirror">An All-Day Mirror</h3>
<figure><img decoding="async" width="1024" height="319" src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-1024x319.jpg" alt="zoom fatigue mirror effect" srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect.jpg 1952w" sizes="(max-width: 1024px) 100vw, 1024px" title="Zoom Fatigue Unpacked: Understanding and Overcoming Virtual Exhaustion 6" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20319'%3E%3C/svg%3E" data-lazy-srcset="https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-1024x319.jpg 1024w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-300x93.jpg 300w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-768x239.jpg 768w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-1536x478.jpg 1536w, https://leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect.jpg 1952w" data-lazy-src="https://wp.leadership.garden/wp-content/uploads/2023/04/zoom-fatigue-mirror-effect-1024x319.jpg"></figure>
<p>Spending five hours per day on video calls means seeing our image on the screen for an equivalent amount of time, which can be exhausting.&nbsp;<a href="https://psycnet.apa.org/record/1973-26817-000?ref=leadership.garden">Studies</a>&nbsp;on this mirror effect go back to 1972, with research showing that focusing on our reflection leads to self-evaluation. Seeing ourselves can increase self-consciousness and distract us from paying attention to the conversation. This phenomenon may be related to self-objectification, where excessively focusing on our appearance throughout the day can&nbsp;<a href="https://greatergood.berkeley.edu/article/item/the_mental_health_consequences_of_starting_at_yourself_on_zoom_all_day?ref=leadership.garden">negatively impact our mental health</a>.</p>
<p>Here are some tips to help you overcome the mirror effect and reduce Zoom fatigue:</p>
<ol>
<li><strong>Hide self-view</strong>: Zoom and other virtual meeting platforms have an option to hide self-view. This will prevent you from seeing your own video feed and reduce the mirror effect.</li>
<li><strong>Change the layout</strong>: Change the layout of the virtual meeting to minimize the size of your video feed or move it to a corner of the screen. This will reduce the amount of time you spend looking at yourself.</li>
</ol>
<p>By following these tips, you can overcome the mirror effect and reduce Zoom fatigue in virtual meetings and calls. Remember that everyone experiences Zoom fatigue differently, so it‚Äôs essential to listen to your body and adjust your virtual meeting routine accordingly. With a few simple adjustments, you can improve your virtual meeting experience and feel more energized and focused throughout the day.</p>
<hr>
<h3 id="how-to-ease-zoom-fatigue">How to ease Zoom fatigue</h3>
<p>As a summary, here are some tips to lower the effects of Zoom fatigue:</p>
<ul>
<li><strong>Move further away from the screen</strong>. One effective method to achieve this is by using an external keyboard, which enables you to type while keeping a distance from your screen.</li>
<li><strong>Have&nbsp;no-camera&nbsp;sessions</strong>. Not a popular one, but you can try recommending that all participants in the call switch off their cameras, at least during a certain section of the meeting.</li>
<li><strong>Remove the mirror</strong>. The majority of video conferencing platforms (including Zoom) provide the option to hide or minimize the self-view feature, which displays your camera feed on the screen.</li>
<li><strong>Mix in&nbsp;classic&nbsp;meetings</strong>. If appropriate, consider in-person meetings or audio-only calls. Utilize these opportunities to your advantage by taking walks or moving around.</li>

<li><strong>Use visual aids</strong>: Visual aids, like slides or images, can help to reduce cognitive load by providing a visual representation of the information. Ask the presenter if they have any visual aids that can help you understand the content better.</li>
<li><strong>Turn off notifications and other distractions</strong>: Notifications from other apps, emails, or messages can cause distractions and increase cognitive load. Turn off notifications and close other applications to stay focused.</li>
<li><strong>Take breaks between calls</strong>. According to&nbsp;<a href="https://www.forbes.com/sites/brucerogers/2021/04/20/our-brains-need-breaks-from-virtual-meetings/?sh=7fb05cb21e93&amp;ref=leadership.garden">research</a>&nbsp;conducted by Microsoft‚Äôs Human Factors Lab, the absence of breaks between video calls can result in accumulating stress over the course of the day. To unwind and move smoothly from one topic to another, aim to allocate a 10-minute gap between each meeting. If you use Google Calendar, you can take advantage of the ‚ÄúSpeedy meetings‚Äù feature to automatically shorten your meetings.</li>
<li><strong>Use a standing desk or a treadmill desk</strong>. This will allow you to stand, move, and engage your core muscles, which can help reduce fatigue and improve focus.</li>
</ul>
<hr>
<p><strong>A ton of good thoughts in the <a href="https://news.ycombinator.com/item?id=36748235" target="_blank" rel="noopener">Hacker News discussion</a>.</strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Forth: The programming language that writes itself: The Web Page (261 pts)]]></title>
            <link>http://ratfactor.com/forth/the_programming_language_that_writes_itself.html</link>
            <guid>36748043</guid>
            <pubDate>Sun, 16 Jul 2023 15:24:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://ratfactor.com/forth/the_programming_language_that_writes_itself.html">http://ratfactor.com/forth/the_programming_language_that_writes_itself.html</a>, See on <a href="https://news.ycombinator.com/item?id=36748043">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
    <center>
        
        <h2><i>Charles H. Moore and the pursuit of simplicity.</i></h2>

        <img src="http://ratfactor.com/forth/talkimg/intro_chuck_web.png" alt="drawing of chuck moore with his real head - with a scroll that says The Web Page Edition">
    </center>

    <div>
        <p> <b>Author:</b> <a href="http://ratfactor.com/">Dave Gauer</a><br>
            <b>Created:</b> 2023-02-02 <br>
            <b>Updated:</b> 2023-07-16 <br>

       </p><p><b>Note:</b> This page is my personal journey to discover Forth
            and put it in the context of computing history.
            It is adapted from my
            <a href="http://ratfactor.com/forth/forth_talk_2023.html">slides</a> for a short talk.
            I've done everything in my power to make this page scale up and down
            for various screen sizes. I welcome suggestions and corrections for
            both the content and display of this page. 
            Here's my
            <a href="http://ratfactor.com/contact-me">contact page</a>.
        </p>
    </div>
</div>

<div>
    <h2>The Legend</h2>
    <p>When I was a wee programmer, I would sit around the virtual Usenet campfires listening
       to the tall tales and legends of the elders.
    <img src="http://ratfactor.com/forth/talkimg/usenet_campfires.png" alt="usenet campfires on a desert scene: comp.lang.forth comp.lang.lisp and alt.religion.kibology">
    </p><div>
        <p>In the 1990s, Usenet
        <a href="https://en.wikipedia.org/wiki/Usenet_newsgroup">newsgroups</a>
        (wikipedia.org)
        were where it was <em>at</em>.
        For example, Linus Torvalds's initial announcement of Linux was to
        comp.os.minix in 1991.
        </p><p>
        The <a href="https://en.wikipedia.org/wiki/Comp.*_hierarchy">comp.*</a>
        (wikipedia.org)
        groups and particularly comp.lang.* were great
        places to learn about and discuss programming.
        By the time I got there in the late 90s, Perl was a pretty hot topic,
        especially as it took a dominant role in the early Web as <em>the</em>
        dynamic page and form processing programming language via
        <a href="https://en.wikipedia.org/wiki/Common_Gateway_Interface">CGI</a>
        (wikipedia.org).
        </p><p>
        There were programming resources on the Web, but nothing like what's
        available now!
        To actually <em>learn to program</em>, I bought books,
        and still do.
        </p><p>
        Usenet was where the community and folklore lived.
        </p><p>
        (The "Easter egg" in this drawing is alt.religion.kibology, which should
        get a chuckle from old timers. The rest of you can look it up.)
    </p></div>
</div>

<div>
    <p>I learned about magical languages with lots of (((((parenthesis))))).
    <img src="http://ratfactor.com/forth/talkimg/ycombo.png" alt="third eye open to the y combinator">
    </p><div>
    <p>Sharp-eyed Lisp-lovers and other mutants will perhaps recognize this thing
    as the Y combinator expressed with lambdas.</p>
    <p>The only time I understood this was when I completed
    the book <i>The Little Schemer</i> by Friedman and Felliesen, which
    walks you through creating it for yourself. It is a magical book and
    I implore you to try it.</p>
    </div>
</div>

<div>
    <p>I listened, wide-eyed, to true tech tales like <i><a href="https://foldoc.org/The+Story+of+Mel">The Story of Mel</a></i> (foldoc.org).
    <img src="http://ratfactor.com/forth/talkimg/mel_rpc_4000.png" alt="Royal McBee RPC-4000 computer drawing">
    </p><div>
        <p>Mel was real and the Royal McBee RPC-4000 was real. Look at that teletype
        (aka "teleprinter"). If typewriters and "Royal" together make a little bell
        in your head go "bing" as your mental carriage hits the end of the page,
        then you're right: Royal McBee was a merger between the
        <a href="https://en.wikipedia.org/wiki/Royal_Typewriter_Company">Royal
            Typewriter Company</a> (wikipedia.org) and McBee, a manufacturer of accounting machines.</p>
        <p>For a while, Royal was owned by the Italian typewriter company, Olivetti,
        who also made some <a href="https://en.wikipedia.org/wiki/Olivetti_computers">really interesting computers</a> (wikipedia.org).</p>
        <p>And then...
    </p></div>
</div>

<p>I heard tell of a programming language so flexible that you could
    <em>change the values of integers</em>.
</p>

<center>
<img src="http://ratfactor.com/forth/talkimg/wizard_chuck.png" alt="chuck moore as an adorable wizard">
</center>

<p>They said that language was called <b>Forth</b> and it was created
       by a mad wizard called <b>Chuck Moore</b> who could write any program in
       a couple screens of code.
</p>

<div>
    <p>Years went by and I wrote a lot of PHP and JavaScript.
    I watched the Web evolve (and sometimes de-evolve).
    </p><p>But I never forgot about the legend of Forth.
</p></div>

<div>
    <p>The blog series
       <a href="https://prog21.dadgum.com/">"programming in the twenty-first century"</a>
       (prog21.dadgum.com)
    by game developer James Hague gave me the final push.
    </p><p>He made Forth a recurring theme and it just sounded so darned interesting.
</p></div>

<p>So I went on an adventure and now that I have returned, I think I have some
    answers.
    <img src="http://ratfactor.com/forth/talkimg/forthwarrior.png" alt="a tired warrior returns from forth mountain">
</p>

<div>
    <p>(Oh, and I <strong>confirmed the legend</strong>. I can make any integer
    equal anything I want.  Stick around 'til the end to see that Forth magic
    trick.)
    </p><center>
    <img src="http://ratfactor.com/forth/talkimg/wizard_chuck.png" alt="chuck moore as an adorable wizard">
    </center>
</div>

<center>
    "Viol√†!"
</center>


<div>
	<h2>Forth uses postfix (RPN) notation</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/hp35.png" alt="hp-35 calculator with rpn syntax"></p><p>At first, I thought this was what Forth was all about:
    </p><pre>3 4 +
<i>7</i>
    </pre>
    <div>
        <p>Now begins my quest to understand Forth.
        </p><p>Perhaps you've seen postfix or
        <a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation">Reverse Polish Notation (RPN)</a>
        (wikipedia.org)
        before? The principle is simple: Instead of the usual "infix" notation
        which puts operators between operands (<code>3 + 4</code>), RPN puts
        operators <em>after</em> the operands (<code> 3 4 +</code>).
        </p><p>RPN notation is one of the most <strong>visually obvious</strong>
        aspects of the Forth programming language.  But it turns out, RPN is
        not what Forth is <em>about</em> or the <em>reason</em> Forth exists.
        As we'll see, the situation is reversed.
        </p><p>In fact, as you'll see, my quest is mostly a series of incorrect
        assumptions I made by looking at the language <em>without the context
            of history</em>.
        </p><p>By the way, the <a href="https://en.wikipedia.org/wiki/HP-35">HP-35 calculator</a> (wikipedia.org) pictured here is really interesting.
        In the early 1970s, HP had powerful desktop calculators.
        Actually,
        what they had were really programmable computers, but they still
        <a href="https://en.wikipedia.org/wiki/Hewlett-Packard_9100A">called them calculators</a> (wikipedia.org) for sales reasons.
        But these were big desktop machines that ran off of wall current.
        </p><p>Putting all of that power into a "shirt pocket" calculator was
        an astounding accomplishment at the time.
        Legend has it that the
        size of the HP-35 was based on the dimensions of Bill Hewlett's
        actual shirt pocket.
        HP-35 calculators have been in space. They killed off the slide rule.
        </p><p>HP calculators are famous for using RPN syntax. If it weren't for
        these calculators, I suspect it's likely that RPN syntax would be
        virtual unknown outside of computer science.
        </p><p>RPN is considered to be highly efficient and,
        being somewhat inscrutable to outsiders, highly geeky.
        </p><p>Let's see a better example...
    </p></div>
</div>

<div>
    <p>Noob:
    </p><pre><i>$ bc
bc 1.07.1
Copyright 1991-1994, 1997, 1998, 2000, 2004, 2006,
2008, 2012-2017 Free Software Foundation, Inc.
This is free software with ABSOLUTELY NO WARRANTY.
For details type `warranty'.</i>
(3 * 4) + (5 * 6)
42
    </pre>
</div>

<div>
    <p>Pro:
    </p><pre><i>$ dc</i>
3 4 * 5 6 * + <i>p</i>
42
    </pre>

    <div>
        <p><i>I'm being cheeky here. Users of <code>bc</code>, are hardly
            noobs.  But it is arguably even geekier to use the much
            older <code>dc</code> program.  <code>bc</code> was once just an
            infix expression translator for <code>dc</code> to make it more
            palatable for people who didn't want to use RPN. Thus the gentle
            teasing.</i>
        </p><p>Besides using RPN syntax,
        <a href="https://en.wikipedia.org/wiki/Dc_%28computer_program%29">the dc calculator</a>
        (wikipedia.org) is completely programmable. Oh and it also happens to
        be one of the very first Unix programs and pre-dates the C programming
        language!
        </p><p>Anyway, the <em>point</em> here is that RPN syntax lets you express
        nested expressions without requiring parenthesis to get the order of
        operations the way you want them. This is one of the reasons RPN fans
        (including those HP calculator fans I alluded to) are so enamoured with it.
        </p><p>In this example, we input 3, then 4. <code>*</code> multiplies them.
        Now we have the result (12) available. But first, we input 5 and 6 and
        multiply them with another <code>*</code> to also store that result (30).
        The final <code>+</code> adds both stored results (12 + 30) and
        stores <em>that</em> result (42).
        Unlike an HP calculator, <code>dc</code> doesn't show us any of the
        stored results, including the last one until we "print" it with the
        <code>p</code> command.
        </p><p>As <a href="https://www.gnu.org/fun/jokes/ed-msg.html">it is known about
            "ed, the standard text editor"</a> (gnu.org), <code>dc</code> doesn't
        waste your VALUABLE time (or teletype paper) with output you don't need!
        </p><p>So this relates to Forth how?
    </p></div>
</div>

<div>
    <p>Forth pro:
    </p><pre>3 4 * 5 6 * + .
42
    </pre>

    <div>
        <p>As you can see, someone sitting at a Forth interpreter
        can perform this calculation exactly the same as with the <code>dc</code>
        calculator (or an HP calculator).
        </p><p>Sharp-eyed readers will note that we print the result with a "."
        command rather than "p". But that's the only difference.
        </p><p>So Forth is like an RPN calculator? We input values and then
        operate on them?
        Well, that statement is not <em>wrong</em>
        </p><p>But does that mean we know what Forth is all about now?
        If we know how to enter things in postfix notation, we "get" Forth?
        No! Not even close...
    </p></div>
</div>

<div>
    <p>Forth absolutely uses postfix notation.
    </p><p>But then I learned some more:
</p></div>

<div>
    <h2>Forth is stack-based</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/pushswapdup.png" alt="drawing of three stacks illustrating push swap and dup operations"></p><div>
        <p>The use of a data stack is probably the second most visible thing
        about the Forth programming language.
        </p><p>A stack is a data structure often explained with a "stack of
        plates" analogy. You <b>PUSH</b> a plate on the stack and you <b>POP</b>
        a plate off the stack. The first item you put on the stack is
        the last item out of the stack.
        </p><p>Above, we have an illustration of <b>PUSH</b> and two other common
        stack operations:
        </p><ul>
            <li><b>SWAP</b> slides a plate out (very carefully) from the second
                position and puts it on top.
            </li><li><b>DUP</b> takes the top plate and <b>dup</b>licates it using
                kitchen magic and puts the replica on the top of the stack (in
                this metaphor, I guess an equal amount of matter is removed
                somewhere else in the Universe, but we try not to worry too
                much about that).
        </li></ul>
        <p>As you may have guessed, these four stack words (PUSH, POP,
        SWAP, DUP) also happen to be Forth words.
        </p><p><b>Historical note 1:</b> In the old days, people and computers just
        WENT ABOUT SHOUTING AT EACH OTHER ALL THE TIME IN ALL CAPS BECAUSE
        LOWERCASE LETTERS WERE TOO EXPENSIVE.
        </p><p><b>Historical note 2:</b> When a computer asks, "SHALL WE PLAY A
        GAME?" in all caps, you must answer NO, as we learned in 1983's
        <a href="https://en.wikipedia.org/wiki/WarGames">WarGames</a> (wikipedia.org)
        </p><p>Let's see a stack in action:
    </p></div>

</div>

<div>
    <pre>Op   The Stack
--   ---------
3     3
4     3  4
*     12
5     12 5
6     12 5  6
*     12 30
+     42
.
    </pre>

    <div>
        <p>Let's revisit our math problem from earlier. This is the
        Forth code on the left and the results on "the stack" on the right.
        </p><p>Rather than being concerned with the syntax or notation, we're
        now interested in what these operations are doing with our data
        stack.
        </p><p>As you can see, entering a number puts it on the stack.
        The math operators take two values from the stack, do something
        with them, and put a new value back on the stack.
        </p><p>The '.' (DOT) operator is different since it only takes one
        value (to print it) and does not put anything back on the stack.
        As far as the stack is concerned, it is equivalent to DROP.
        As far as humans are concerned, it has the useful side-effect
        of letting us see the number.
        </p><p>Now let's see something you probably <em>wouldn't</em> find
        on an HP calculator. Something non-numerical...
    </p></div>

</div>

<div>
    <p>This is valid Forth, assuming CAKE, HAVE, and EAT have been defined:
    </p><pre>CAKE DUP HAVE EAT 
    </pre>

    <div>
        <p>Getting the joke here will require knowing
        <a href="https://en.wikipedia.org/wiki/You_can%27t_have_your_cake_and_eat_it">this English idiom</a> (wikipedia.org).
        </p><p>Actually, this isn't <em>just</em> a silly example.
        Forth's use of the stack can lead to a natural, if somewhat
        backward use of nouns and verbs. (Kind of like Yoda's speech habits.
        "Cake you will dup, yes? Have it and eat it you will, hmmm?")
        </p><p>There can, indeed, be some object named CAKE that we have
        placed on the stack (probably a memory reference) which
        can be DUPed, and then HAVEd and EATen.
        </p><p>It's up to the Forth developer to make harmonious
        word choices. It can get far more clever or poetic than my example.
        </p><p>Naming things is great.
        </p><p>But sometimes <em>not</em> naming things is even better.
    </p></div>
</div>

<div>
    <p>The stack frees us from being forced to create explicit names for
    intermediate values.
    </p><p>If I ask you to add these numbers:
    </p><pre>2 6 1 3 7
    </pre>
    <p>Do you feel a need to give a <i>name</i> to each sum pair...or even the running total?

    </p><div>
        <p>(Hopefully your answer is "no" or the rhetorical question doesn't work.)
        </p><p>But it's funny how our <em>programming languages</em> often require us
        to explicitly name intermediate results so that we can refer to them.
        On paper, we would never give these values names - we would just happily
        start working on the list.
        </p><p>Imagine, if you will, a factory assembly line in which
        each person working the line is a hateful fussbudget who refuses to
        work on the part in front of them until you name it. And each time the
        part has been worked on it must be given a new name. Furthermore, they
        refuse to let you re-use a name you've already used.
        </p><p>A lot of imperative languages are like that factory. As your
        values go down the line, you've got to come up with nonsense names
        like <code>result2</code>, or <code>matched_part3</code>.
        </p><p>Does <em>your</em> programming language make you do this?
        </p><p><i>(It's almost as bad as file names used as a versioning
            system: <code>my_doc_new_v5.4(copy)-final2</code>...)</i>
        </p><p>Working without names (also known as <em>implicit</em> or
        <em>tacit</em> or <em>point-free</em> programming) is sometimes a more
        natural and less irritating way to compute.  Getting rid of names can
        also lead to much more concise code. And less code is good code.
        </p><p>Great, so stacks can be a very elegant way to handle expressions.
        </p><p>Have we "cracked" Forth yet? Now we know two things:
        it uses RPN syntax and it is stack-based.
    </p></div>
</div>

<div>
    <p>Well, Forth certainly does use a stack. It is definitely a stack-based
    language.
    </p><p>But then I learned some more...
</p></div>

<div>
    <h2>Concatenative programming</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/computer_cat.png" alt="a confused cat working on an old pc"></p><p>Ah, <i>this</i> must be it because it sounds fancy.

    </p><div>
        <p>On this journey of Forth discovery, you'll inevitably run into
        the term "concatenative programming".
        </p><p>What's that?
        </p><p>An awesome resource for all things concatenative is
        <a href="https://concatenative.org/">The Concatenative Language Wiki</a>
        (concatenative.org).
            It lists many concatenative languages and has a page about Forth,
            of course.
        </p><p>For the term "concatenative programming" itself, the Factor
        programming language website has an excellent page defining the
        term:
        <a href="https://docs.factorcode.org/content/article-tour-concatenative.html">Factor documentation: Concatenative Languages</a>
        (factorcode.org).
        And, of course, there's the Wikipedia entry,
        <a href="https://en.wikipedia.org/wiki/Concatenative_programming_language">Concatenative programming language</a>
        (wikipedia.org).
        </p><p>I understand the explanations on these websites <em>now</em>, but
        it took me a while to get there. Your journey may be shorter or longer.
        Probably shorter.
        </p><p>Let's see if I can stumble through it...
    </p></div>
</div>

<div>
    <p>Contrast with <b>applicative</b> language:
    </p><pre>eat(bake(prove(mix(<b>ingredients</b>))))
    </pre>
    <p><b>Concatenative</b> language:
    </p><pre><b>ingredients</b> mix prove bake eat
    </pre>

    <div>
        <p>An applicative language has you apply a function to a value, which
        returns another value. Using familiar Algol-like (or "C-like", or
        "Java-like", or "JavaScript-like") syntax, arguments are passed to
        functions within a pair of parenthesis. In the above example, the
        parenthesis end up deeply nested as we pass the output of one function
        to another.
        </p><p>Unlike the math examples, where the infix notation looks more
        natural to most of us than the postfix notation, the concatenative
        example of this baking program looks more natural (at least in a
        <i>human language</i> sense) than the <b>inside-out</b> function application
        example, right?
        </p><p><i>(Of course, if you're a programmer used to years of something like C
        or Java or JavaScript, the inside-out parenthetical form will probably
        seem pretty natural too. Well, guess what? Your mind has been
        warped. It's okay, mine has too.)</i>
        </p><p>The point here is that concatenative style has us "composing"
        functions (which you can think of as verbs) simply by putting them
        in sequence. Each function will be called in that sequence.
        The values that are produced at each step are passed along
        to be consumed as needed.
        </p><p>No names (unless we want them), just nouns and verbs.
        </p><p>But that's just the surface. It turns out this "concatenative language"
        concept goes way past that...
    </p></div>
</div>

<p>The canonical example of a concatenative language is Joy.
</p>

<div>
    <h2>Joy</h2>
    <p><b>Manfred von Thun</b> inspired by Backus's 1977 ACM Turing Award lecture:
    <img src="http://ratfactor.com/forth/talkimg/backus.jpg" alt="top of the john backus paper Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs">


    </p><div>
        <p><a href="http://worrydream.com/refs/Backus-CanProgrammingBeLiberated.pdf">Can Programming Be Liberated from the von Neumann Style? (PDF)</a> (worrydream.com)
            This paper is dense with notation and I haven't personally
            attempted to wade through it, <em>yet</em>. I'm sure it contains
            <em>many</em> profound ideas.
        </p><p>I know just enough to believe I understand this paragraph from
        the paper's abstract:
            </p><blockquote>
                "An alternative functional style of programming is
                founded on the use of combining forms for creating
                programs. Functional programs deal with structured
                data, are often nonrepetitive and nonrecursive, are
                hierarchically constructed, do not name their
                arguments, and do not require the complex machinery of
                procedure declarations to become generally applicable.
                Combining forms can use high level programs to build
                still higher level ones in a style not possible in
                conventional languages."
            </blockquote>
        <p>Perhaps you've heard of "functional programming?" As you can
        see, that term was being used in 1977.
        </p><p>"Concatenative programming" came after. In fact,
        <b>Joy</b> is where the "concatenative" description comes from!
        (von Thun specifically credits Billy Tanksley for creating the term
        "concatenative notation".)
    </p></div>
</div>

<div>
    <p><b>Joy</b> is kind of like starting with a Lisp
    </p><p>...without variables
    </p><p>...and without traditional control structures
    </p><p>...and all functions are unary (or an "arity of 1").
    </p><p>Specifically, all functions take one stack as input and return
       one stack as output. The stack is not named, it is implied.
    </p><p>A program is simply a list of functions that is read
       from left to right.

    </p><div>
        <p>I can't describe Joy's genesis better than the man himself.
        Here is von Thun in an interview about Joy:
        </p><blockquote>"Joy then evolved from this in an entirely haphazard way:
            First I restricted the binary relations to unary functions, and
            this of course was a dramatic change. Second, to allow the usual
            arithmetic operations with their two arguments, I needed a place
            from which the arguments were to come and where the result was to
            be put - and the obvious place was a stack with a few shuffling
            combinators, originally the four inspired by Quine. Third, it
            became obvious that all these combinators could be replaced by
            unary functions, with only function composition remaining. Finally
            the very different distinctively Joy combinators emerged, which
            take one or more quoted programs from the stack and execute them in
            a specific way. Along the way of course, lists had already been
            seen as just special cases of quoted programs. This meant that
            programs could be constructed using list operations and then passed
            on to a Joy combinator."</blockquote>
        <p>From <a href="http://www.nsl.com/papers/interview.htm">A Conversation with Manfred von Thun</a> (nsl.com), which is a really great read in its entirety.
        </p><p>As you can see, <b>combinators</b> are crucial in Joy.
        Let's take a moment to dive into those, because this is a pretty
        fascinating avenue of computer science...
    </p></div>
</div>

<div>
    <h2>Combinators</h2>
    <p>Combinators are any "higher-order" functions like <b>map</b>.

    </p><div>
        <p>"Higher-order" just means functions that take <em>other</em>
        functions as input and do things with them.
        </p><p>You can even have functions that take functions that take functions
        and so on to do powerful things. But you'll need to meditate on
        them every time you have to re-read that part of your code.
        </p><p><b>map</b> is one of the more common examples, so I'll use it
        as an example.
    </p></div>

    <p>JavaScript:</p>
    <pre>inc = function(n){ return n + 1; };

bigger = [1, 2, 3, 4].<b>map</b>(inc);

<i>Result: [2,3,4,5]</i>
    </pre>

    <p>JavaScript using an "arrow function":</p>
    <pre>bigger = [1, 2, 3, 4].<b>map</b>(n =&gt; n + 1);

<i>Result: [2,3,4,5]</i>
    </pre>

    <div>
        <p><i>(The second example with the arrow function syntax works exactly
        the same way, but more compactly. I included it to make the comparison
        with Joy a little more even-handed. Feel free to pick a favorite
        and ignore the other one.)</i>
        </p><p>In the first example, we have familiar Algol-like
        syntax with functions that take arguments in parenthesis.
        </p><p>Perhaps
        <code>map()</code> is familiar to you. But if not, just know that 
        it takes two parameters like so: <code>map(array, function)</code>.
        The first parameter is implicit in these JavaScript examples, but it's
        there. The array object, <code>[1, 2, 3, 4]</code> calls its own
        <code>map()</code> method. The second parameter is a function
        (named <code>inc</code> in the first example and left anonymous in
        the second), which will be applied to every member of the list.
        </p><p>
        The output of <code>map()</code> is a <em>new</em> list containing the
        result of each application.
        </p><p>Notice how both JavaScript examples
        have variables such as the parameter <code>n</code> and the result
        <code>bigger</code>. This is an example of what I mentioned a moment
        ago when discussing the advantages of stacks: "Traditional"
        programming languages often make us name values before we can work with
        them.
    </p></div>

    <p>The same thing, but concatenatively in Joy:</p>
    <pre>[1 2 3 4] [1 +] <b>map</b>

<i>Result: [2 3 4 5]</i>
    </pre>

    <div>
       <p>
       The syntax here may require a little explanation.
       The square brackets (<code>[]</code>) are Joy's
       quote mechanism. Quotations are a lot like lists, but they can contain
       <em>programs</em> as well as data.
       </p><p>In this case, the first quotation is the number list,
           <code>[1 2 3 4]</code>.
       </p><p>The second quotation is a program, <code>[1 +]</code>.
       </p><p>As in the JavaScript examples, <code>map</code> takes two parameters.
       The first is the function (or "program" in Joy) to apply, and the second
       is the list to apply it to.
       </p><p>(It's kind of confusing to talk about "first" and "second," though
       because that's the opposite order in which we <em>supply</em> those
       arguments on the stack...)
       </p><p>Note the lack of variables <code>bigger</code> or <code>n</code>.
       Intermediate values just exist.
       </p><p>It looks pretty nice and neat, right?
       </p><p>This "point-free" style can be a blessing...
       or curse. Unlike computers, human brains have a hard time juggling too
       many things on the stack.
       </p><p>There seems to be a happy medium between named and unnamed. Also,
       the point-free style seems to benefit greatly from short (even
       <i>very short</i>) definitions to avoid mental juggling and greater
       composibility.
       </p><p>If you have the slightest interest in <b>Joy</b>, I highly recommend
       reading or skimming this delightful tutorial by Manfred von Thun
       himself:
        <a href="https://hypercubed.github.io/joy/html/j01tut.html">An informal tutorial on Joy</a>
        (hypercubed.github.io).
       </p><p>Note: I had a bit of a time actually running Joy to test out these
       examples. Thankfully, I eventually ran into
       <a href="https://github.com/calroc/joypy">Joypy</a> (github.com),
       a Joy written in Python. My Linux distro comes with Python installed,
       so the whole process for me was:
       </p><pre>git clone https://github.com/calroc/joypy.git
cd joypy
python -m joy
...
joy? [1 2 3] [1 +] map
        </pre>
        <p>Okay, that's a glimpse.
        </p><p>But we've barely touched the conceptual power of combinators with our
        <code>map</code> examples. Let's go a <em>little</em> deeper on
        this fascinating subject:
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/mock_a_mockingbird.jpg" alt="cover of the book"></p><div>
        <p>Here's something from my bookshelf. It's <i>To Mock a Mockingbird</i>
        by mathematician and
        puzzle-maker Raymond Smullyan. It uses puzzles involving birds to solve
        logic problems and classify some well-known combinators.
        </p><p>It would be impossible to write a complete catalog of
        combinators just as it would be impossible to write a complete
        catalog of integers. They're both infinite lists.
        Nevertheless, some well-known combinators have been identified as
        having special properties. In the book above, many of these have
        been given the names of birds.
        </p><p>Remember, combinators are just "higher-order"
        functions that take functions as input.
        Well, it turns out these are
        all you need to perform <em>any</em> computation. They can replace logical
        operators and even variables.
        </p><p>What?!
        </p><p>Yeah, you can re-work any expression into a combinatorial expression
        and completely replace everything, including the variables, with
        combinators.
        </p><p>It's kind of hard to imagine at first. But you can see it happen
        right before your very eyes.
        The mind-blowing tool on this page by Ben Lynn:
        <a href="https://theory.stanford.edu/~blynn/lambda/cl.html">Combinatory Logic</a>
        (stanford.edu)
        takes a term expressed in lambda calculus and replaces <b>everything</b>
        with just two combinators, K and S.
        (We'll talk more about those two in just a moment because they
        are super special.)
        <img src="http://ratfactor.com/forth/talkimg/look_ma_no_variables.png" alt="screenshot from the aforementioned calculator with buttons 'Look ma, no names, no variables, and no variables K-optimized!">
        </p><p><em>(Ben Lynn's whole website is full of neat stuff like this.
            If you're looking to entertain yourself for any amount of time from
            an afternoon to the rest your life, Lynn has you covered.)</em>
        </p><p>So combinators share something in common with lambda calculus and
        Turing machines. These systems provide all of the building blocks
        you need to perform any
        possible computation in the sense of the
        <a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis">Church-Turing thesis</a> (wikipedia.org)
        or "computability thesis". (We've also discovered some problems
        that are <em>not</em> computable and <em>no</em> system can compute
        them like "the halting problem," but these are pretty rare.)
        </p><p>It turns out that <strong>computation is a fundamental feature of
            the Universe</strong>.
        As far as we can tell, any universal system of computation is equally
        capable of solving any computational problem.  And once you realize how
        little is required, you can invent a universal computer yourself!
        </p><p>Electronically speaking, this is the same principle
        that allows a NAND gate to simulate all other gates. NAND gates are
        a fundamental computational building block. You can make an
        entire computer with nothing but NAND gates and that computer can
        (slowly) solve any computable problem you can imagine.
        </p><p>Anyway, when we use combinators, this particular flavor of universal
        computation is called 
        <a href="https://en.wikipedia.org/wiki/Combinatory_logic">combinatory logic</a> (wikipedia.org).
        </p><p>What do the building blocks of combinatory logic look like?
        </p><p>Let's start small:
    </p></div>
</div>

<div>
    <p>Identity
    </p><pre>(I x) = x
    </pre>

    <div>
        <p>The simplest of all combinators is I, the "identity combinator".
        There are a ton of different ways to write this. In lambda calculus,
        it looks like this: <code>I = Œªx</code>.
        </p><p>The way to read <code>"(I x) = x"</code> is: "<code>I</code> applied
        to some object <code>x</code> results in...<code>x</code>."
        </p><p>We say "object x" rather than "value x" because, being a
        combinator, <code>I</code> could take a function as input as well as a
        value. In fact, "object" is intentionally very abstract, so
        <code>x</code> could contain a scalar value, or
        list, or function, or another combinator, or <em>anything</em>.
        Whatever that object is, <code>I</code> returns it.
    </p></div>
</div>

<div>
    <p><b>K</b> and <b>S</b>
    </p><pre>(K x y) = x

(S x y z) = (x z (y z))
    </pre>

    <div>
        <p>Both of these take more than one parameter of input.
        But if you're used to Algol-like function syntax, the way this
        works may be surprising.
        </p><p>Since it's the simpler of the two, let's use the <code>K</code>
        combinator as an example:
        </p><p>The way to read "<code>(K x y) = x</code>" is:
        "<code>K</code> applied to <code>x</code> <strong>yields
        a combinator</strong>, which, when applied to <code>y</code> always
        evaluates to <code>x</code>."
        </p><p>(Programmers familiar with the concept of <em>currying</em> will see
        that this is like the <em>partial application</em> of a function, where
        a new function is "pre-baked" with the argument <code>x</code>.  The
        term "currying" is named in honor of mathematician
        <a href="https://en.wikipedia.org/wiki/Haskell_Curry">Haskell Curry</a>
        (wikipedia.org),
        after whom the Haskell programming language is also named.)
        </p><p>The result is that <code>K</code> makes a combinator that
        <strong>throws away</strong> any input and just returns
        <code>x</code>. Weird, right? But it turns out to be useful.
        </p><p><code>K</code> is super easy to write in a language like
        JavaScript, which is also a nice choice because you can play with
        it right in the browser console like I just did:
        </p><pre>K = function(x){
  return function(y){
    return x;
  }
}

K("hello")("bye")

<i>&gt; "hello" </i>
        </pre>
        <p>See how the result of <code>K("hello")</code> is a function that
        returns "hello" no matter what you give it as input?
        </p><p>How about <code>S</code>? I'll leave implementing <em>that</em>
        in JavaScript as an exercise for the reader.
        It's clearly much more complicated since it has three levels of
        "function that yields a combinator" on the left and the <em>result</em>
        is an equally complicated combinator that <em>first</em> applies
        parameter <code>z</code> to combinator <code>y</code>.
        </p><p>(By the way, the <code>y</code> combinator above should not be
        confused with <strong>the</strong> <code>Y</code> combinator.
        Do you remember that arcane lambda calculus artifact projected
        over that head with the third eye way up near the beginning of this
        page?  That thing was the <code>Y</code> combinator! It turns out, it's
        all, like, <em>connected</em>, you know?)
        </p><p>But the real point is this: <code>S</code> and <code>K</code> are
        special for one very interesting reason.
        Together with <code>I</code>, they form the "SKI calculus" and just
        these three combinators are <strong>all you need</strong> to perform
        any computation in the known universe.
        </p><p>Actually, it's even crazier than that. You don't even need
        <code>I</code> because that, too, can be created with <code>S</code>
        and <code>K</code>.
        </p><p>That's right, the <code>S</code> and <code>K</code> definitions
        above are a complete system for universal computation.
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/wolfram_combinators.jpg" alt="cover of the book"></p><div>
        <p>The book shown here is another from my bookshelf. It's
        <i>Combinators: A Centennial View</i> by Stephen Wolfram.
        </p><p>It starts with a (much too) terse introduction to the SKI combinator
        calculus and then launches into page after page of visualizations of S
        and K combinators being fed into each other. Like fractals or automata,
        simple inputs can produce patterns of surprising sophistication.
        </p><p>Wolfram demonstrates combinators that keep producing different
        output for a gazillion iterations and then get stuck in a loop. Some of
        them produce regular patterns for a while and then start producing
        different patterns.  Some just loop forever at the outset.
        As in other universal systems, there is no end to the complexity
        produced by these two simple constructs.  It is infinite.  And all of
        this is just S and K combinators taking combinators as input and
        returning combinators as output.
        </p><p>I think it is wild and fun to see someone play
        with a subject like Wolfram does in this book. Each page is saying,
        "Look at what is possible!"
        </p><p><i>Combinators</i> is also Wolfram's ode to the discoverer of
        combinatory logic,
        <a href="https://en.wikipedia.org/wiki/Moses_Sch%C3%B6nfinkel">Moses Sch√∂nfinkel</a> (wikipedia.org)
        who, like so many of the giants in the field of computer science,
        did his work on paper decades before the first digital electronic
        computers beeped their first boops.
        </p><p>Figuring out the output of the <code>S</code> combinator once
        was enough to keep me occupied for a while.  It boggles my mind to
        imagine feeding it another <code>S</code> as input on paper,
        let alone discovering these particular combinators in the first place.
        </p><p>Okay, we get it, combinators are a crazy way to compute.
        </p><p>But are they worth using in <em>"real"</em> programs? In limited
        doses, absolutely!
    </p></div>
</div>

<div>
    <p>Combinators let us factor out explicit loops. This:
    </p><pre>foo<b>.map</b>(bar)
    </pre><p>
    is the same as this much longer statement:
    </p><pre><b>temp = [];</b>
<b>for(i=0; i&lt;</b>foo<b>.length; i++){
    temp[i] = </b>bar(<b>foo[i]</b>)<b>;
}</b>
    </pre>

    <div>
        <p>Both of those pieces of JavaScript give us the result of applying
        the function <code>bar()</code> to an array <code>foo</code>.
        </p><p>I think <code>map()</code> is a great example of the power of
        combinators to clean up a program with abstraction.  Once you start
        using simple combinators like this to abstract away the boilerplate
        logic of <em>yet another loop over a list of items</em>, it's hard
        to go back.
        </p><p>My personal history with exploring higher order functions in
        a production setting is through the
        <a href="https://ramdajs.com/">Ramda</a> (ramdajs.com) JavaScript
        library, which I discovered from the talk
        <a href="https://www.youtube.com/watch?v=m3svKOdZijA">Hey Underscore, You're Doing It Wrong!</a>
        (youtube.com)
        by Brian Lonsdorf, which is fantastic.
        </p><p>Once I started discovering how combinators and curried functions
        could eliminate big old chunks of code, I was hooked!
        The old, dreary procedural code became a new fun puzzle!
        </p><p>Mind you, it's very easy to go overboard with this stuff and
        write something far <em>less</em> readable 
        than some simple procedural code. (Gee, ask me how I know this.)
        </p><p>But in limited doses, it's super powerful and compact.
    </p></div>
</div>

<div>
    <p><b>Joy</b> uses combinators to "factor out" all sorts of logic.
    </p><p>Even different forms of recursion can be completely handled
    for you by combinators in Joy thanks to the uniformly unary functions.
    </p><p>Here's a factorial definition:
    </p><pre>factorial == [null] [succ] [dup pred] [*] <b>linrec</b>
    </pre>
    <p>Let's try it:
    </p><pre>5 factorial
<i>120</i>
    </pre>

    <div>
        <p>Computing the factorial of a number is often used as an example of
        recursion. The final answer is the input number multiplied by the
        previous number multiplied by the previous number multiplied by...
        <em>the rest of the numbers</em> all the way down to 1.
        </p><p>Computing a factorial requires a cumulative result. Without
        recursion, you need an explicit variable to hold the intermediate
        result as you loop through the numbers.
        </p><p>As shown in the Joy <code>factorial</code> definiton above,
        <code>linrec</code> is a "linear recursion" combinator. It takes takes
        4 parameters, each of which is a quoted program. <code>null</code> is a
        predicate which tests for zero. <code>dup</code> is the same as in
        Forth. <code>pred</code> is an operator which yields a number's
        predecessor (given 4, yields 3).  "<code>*</code>" multiplies two
        numbers, just like you'd expect. Given these pieces, perhaps you can
        take a guess at how <code>linrec</code> works?
        </p><p>For comparison, here is a recursive JavaScript solution:
        </p><pre>function factorial(n) {
    if (n &lt;= 1) {
        return 1;
    }

    return n * factorial(n - 1);
 }
        </pre>
        <p>Note that the Joy example is not just shorter and has no
        variable names but it has <em>abstracted away the mechanics
           of recursion</em>. All we're left with is the
        logic specific to the factorial problem itself.
        </p><p>It's debatable which of these two are more <em>readable</em>
        because the measure of readability is in the eye of the beholder.
        But I think you can <em>imagine</em> getting good at reading the Joy
        example.
        </p><p>Okay, so we've gone pretty deep into this concatenative
        programming and combinator thing. How does this actually
        relate to Forth?
        </p><p>First of all, Forth <em>does</em> have facilities for
        dealing with combinators:
    </p></div>
</div>

<div>
    <p>Forth supports higher order functions with "execution tokens"
    (function pointers) and the <code>EXECUTE</code> word.
    </p><p>This will run the word <em>returned by</em> the word <code>FOO</code>:
    </p><pre>FOO EXECUTE
    </pre>
    <p>With this, you can very compactly define combinatorial words such as
        <b>MAP</b>, <b>FOLD</b>, and
        <b>REDUCE</b>.

    </p><div>
        <p>First, let's see how <code>EXECUTE</code> works. The syntax will be
        alien to non-Forth programmers, but the concept will be no problem for
        anyone used to using first class functions.
        </p><p>First, let's make a new word:
        </p><pre>: hello ." Hello" ;
        </pre>
        <p>This is Forth for, "Compile a word called <code>hello</code>
        that prints the string <em>Hello</em>."
        </p><p>(We'll learn how compiling words actually works later.
        For now, please just gracefully accept what you're seeing.)
        </p><p>Next:
        </p><pre> 
VARIABLE hello-token
        </pre>
        <p>This creates a new variable called <code>hello-token</code> which
        will store the "execution token" for the hello word.
        </p><p>This part will look super cryptic if you're new to Forth:
        </p><pre> 
' hello hello-token !
        </pre>
        <p>Let's examine this one piece at a time:
        </p><ul>
            <li>"<code>'</code>" gets the address of the word
                "<code>hello</code>" and puts it on the stack.
            </li><li>"<code>hello-token</code>" is a variable, which
                just leaves its address on the stack when called.
            </li><li>"<code>!</code>" stores a value from the stack
                (the address of <code>hello</code>) <em>at</em>
                an address from the stack (the address of
                variable <code>hello-token</code>).
        </li></ul>
        <p>So the code above simply reads, "Store the address of
        <code>hello</code> in the variable <code>hello-token</code>."
        </p><p>Now let's use EXECUTE to call this "execution token":
        </p><pre> 
hello-token @ EXECUTE
<i>Hello</i>
        </pre>
        <p>Behold, it printed the "Hello" string!
        </p><p>Remember, the variable <code>hello-token</code> leaves its
        address on the stack when it is called.
        </p><p>"<code>@</code>" is a standard Forth word that loads the value
        from the given address and puts that value on the stack.
        </p><p><code>EXECUTE</code> gets an address from the stack and runs
        whatever word is found at that address.
        </p><p>Perhaps it would be helpful to see that this silly statement:
        </p><pre>' hello EXECUTE
        </pre><p>
        is equivalent to just calling <code>hello</code> directly:
        </p><pre>hello
        </pre>
        <p>Anyway, now we're armed with Forth's combinatorial ability:
        Treating functions ("words") as values so other functions can
        take them as input. This allows us to define combinators in Forth.
        </p><p>For some compact higher-order function definitions
        in Forth, check out <a href="https://gist.github.com/adolfopa/64a1a59c28cbd77b71449d68f4c36dc0">this Gist by Adolfo Perez Alvarez</a> (github.com).
    </p></div>
</div>

<div>
    <p>So yes, Forth <strong>is</strong> concatenative. It implicitly passes values
    from one function invocation to the next. And it supports higher-order
    functions.
    </p><p>Nevertheless, I do <strong>not</strong> believe studying "concatenative
    programming" in general or Joy specifically is a good way to understand
    Forth!
    </p><p>For example, this simple statement:
    </p><pre>2 3 +
    </pre>
    <p>can be read two different ways:
    </p><p><b>Forth:</b> "Push 2 and then 3 on the stack; add them; push <b>result
        5</b> on the stack."
    </p><p><b>Joy:</b> "The <i>composition</i> of the functions 2, 3, and +
    is identical to the <b>function 5</b>."
    </p><div>
        <p>While both languages share a cosmetically similar syntax, 
         and both produce the same result for <em>this</em>
         expression, there is a fundamental difference between how the two
         languages "think" about the expression because they arrived at
         this place in completely different ways.
        </p><p>Forth's only concern (as a language) is to process these three
        tokens and act upon them according to some simple rules.
        (If the token is in the dictionary, execute it. If it's a number, put
        it on the stack.)
        </p><p>To Joy, it may be the same mechanical process under the hood, but
        the language itself sees these tokens more like a mathematical
        expression. It's a much more abstract outlook.
        </p><p>The point I'm making is that Forth may <em>accomodate</em> the
        abstract point of view, if the developer chooses to take it. But
        Forth is not <em>based</em> on abstract concatenative computing
        principles or combinatory logic.
        </p><p>Let's look at this from a historical perspective.
        First, the notions of postfix syntax (RPN) and a data stack for
        the basis of the language:
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/zuse_z3_computer.png" alt="drawing of konrad zuse's z3 computer"></p><p><b>Postfix notation</b> was definitely in the air when Chuck Moore
    created Forth.
    </p><p><b>Stacks</b> were known and used in the time of Forth's origins,
    though they were generally limited to 2-4 items in registers.
    </p><p>So I think it's reasonable to assume that RPN syntax and use of
    stacks are a historically accurate way to examine Forth's "origin story."

    </p><div>
        <p>Hold that thought, here's a fun aside:
        </p><p>The drawing of the computer labeled <b>"Z3"</b> on the right is of
        the
        <a href="https://en.wikipedia.org/wiki/Z3_(computer)">Z3 computer</a>
        (wikipedia.org)
        designed by engineer and computer scientist Konrad Zuse. This is widely
        considered to be the <strong>first programmable digital computer</strong>!
        It used electro-mechanical relays like the telegraph networks of the day.
        </p><p>(By the way, a certain amount of electro-mechanical logic is
        <em>still</em> used in modern nuclear reactor safety systems because
        the big mechanical components are not as vulnerable to nuclear
        radiation as semiconductors!)
        </p><p>The Z3 could do addition in less than a second and multiplication
        in three seconds. It had 64 words of 22 bits each and worked with
        the equivalent of modern floating-point numbers.
        </p><p>As mentioned above, it can be said to use RPN, though there are only
        two registers and nine instructions. Opcodes were encoded in eight
        bits.  The computer is programmable via punched paper tape (you can see
        the tape device to the right of the control console, though it's a bit
        of a scribble in my drawing).
        </p><p>It is <em>also</em> a stack machine. Again, this is with a mere
        two registers, which get juggled in a particular sequence as you
        load and store values.
        </p><p><b>Fun fact:</b> The control unit used special control
        <em>wheels</em> to encode microsequences. If the microsequence wasn't
        programmed correctly, it could short-circuit the machine and destroy
        the hardware!
        </p><p>I got most of this information from this excellent paper by
        Raul Rojas: 
        <a href="https://ed-thelen.org/comp-hist/Zuse_Z1_and_Z3.pdf">Konrad Zuse's Legacy: The Architecture of the Z1 and Z3 (PDF)</a> (ed-thelen.org).
        </p><p>Anyway, so the simple mechanics of RPN and stack-based
        operation are very natural for digital computing machines
        and their use goes back to the very beginning.
    </p></div>
</div>

<div>
    <p>But Joy and the term "<b>concatenative programming</b>" come from the
    1980s.

    </p><div>
        <p>Uh oh.
        </p><p>While the ideas of combinators and other types of
        universal computation were well known in certain mathematical
        and computational circles, I would argue they were not very amenable
        to existing computer hardware until much later when computers became
        fast enough to support "functional programming" styles and
        abstractions.
        </p><p>Until then, programming was "close to the metal."
        Even the idea of "structured programming" with programming language
        concepts like <code>if/else</code> or <code>while/for</code> loops was
        once considered novel! Until then, everything was done with address
        jumps or <code>GOTO</code>.
        </p><p>It's important to remember that <em>"coding"</em>, the
        actual act of turning an abstract program into machine code,
        was long ago considered to be a mere secretarial skill, not far
        removed from <em>typing</em> and other forms of data entry.
        This is why some people (including myself) refer themselves as
        "programmers" rather than "coders".
        </p><p>Concatenative programming, with its emphasis on combinators
        (and immutable data structures, which we haven't talked about),
        doesn't have the same historic grounding for Forth the way that RPN
        syntax and stack-based programming do.
        </p><p>So I must conclude that understanding concatenative programming
        is super cool, but it doesn't actually help us understand the
        true nature of Forth because it doesn't describe how Forth came to be.
        It is not part of Forth's "origin story."
        </p><p>As we'll soon see, Forth <em>really is</em> about the "nuts and
        bolts". You bring your own theories with you.
    </p></div>
</div>

<div>
    <p>So while all these descriptions of the Forth language are true
    (RPN, stack-based, concatenative), they all describe
    the language Forth from the vantage of <b>hindsight</b>.

    </p><div>
        <p>There's nothing wrong with thinking about Forth in these terms,
        but it doesn't answer the "why" questions:
        </p><p>"<strong>Why</strong> does Forth have this syntax?"
        </p><p>"<strong>Why</strong> does Forth work this way?"
        </p><p>I think the answers to the "why" questions are best answered by
        looking at <strong>when</strong>.
        </p><p>What is Forth's history, anyway?
    </p></div>
</div>


<div>
    <h2>We need to go back to the 1950s.</h2>
    <center>
    <img src="http://ratfactor.com/forth/talkimg/chuck_to_the_future.png" alt="chuck moore as marty in a drawing of the back to the future poster">
    </center>
</div>

<div>
    <p><em>If this image doesn't make any sense to you, citizen of
            the future, it's from the iconic movie poster by Drew Struzan for
            <a href="https://en.wikipedia.org/wiki/Back_to_the_Future">Back to the Future (1985)</a> (wikipedia.org).</em>
        </p>
</div>

<div>
    <h2>Smithsonian Astrophysical Observatory and MIT 1958</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/ibm_704.png" alt="chuck moore operating an IBM 704">
</p></div>

<div>
    <p>Chuck Moore is programming an IBM 704 with Fortran on punchards.
    </p><p>"Compiling took 30 minutes...you got one shot per day"
    </p><p>-- Chuck Moore, Forth, the Early years
    </p><div>
        <p>In <a href="http://worrydream.com/refs/Moore%20-%20Forth%20-%20The%20Early%20Years.pdf">Forth - The Early Years (PDF)</a> (worrydream.com), Chuck
        Moore recites a fairly terse history of Forth, from the earliest
        pre-Forths to the creation of the language standard.
        </p><p><i>(Note: Chuck mentions the Smithsonian Astrophysical Observatory
            (SAO) and the Massachusetts Institute of Technology (MIT) in
            roughly the same time period, and it's a bit difficult to be
            entirely sure which part is talking about which organization. But
            if you look at a map, SAO is at Harvard University. Harvard and MIT
            are about a mile apart in Cambridge, Massachusetts. It's basically a
            singular point if you zoom out a bit. So that helps explain the
            overlap.)</i>
        </p><p>The computer in question is the
        <a href="https://en.wikipedia.org/wiki/IBM_704">IBM 704</a>
        (wikipedia.org)
        It was one of those room-filling vacuum-tube computers with
        tape drives the size of refridgerators.
        </p><p>The 704 was a fully programmable "modern" computer with
        magnetic-core memory, multiple registers, a 36-bit instruction set, and
        36-bit words ("word" as in native memory size for the processor, not
        "word" as in Forth functions).
        </p><p>There were switches for each register on the control console, but
        programs could be written to and read from paper punch cards.
        </p><p>It was very modern for the time, but...
        </p><blockquote>"In its day, the 704 was an exceptionally reliable machine.
        Being a vacuum-tube machine, however, the IBM 704 had very poor
        reliability by today's standards. On average, the machine failed around
        every 8 hours, which limited the program size that the first Fortran
        compilers could successfully translate because the machine would fail
        before a successful compilation of a large program."</blockquote>
        <p>It's difficult to imagine now, but changing parameters for a program,
        re-compiling it, and running it again could take a day (assuming you
        didn't make any mistakes).
        </p><p>So Chuck solved that irritation with an extremely clever solution:
    </p></div>
</div>

<div>
    <p>Moore made an interactive interpreter
    on a computer with nothing we would recognize today as an interactive
    terminal.
    </p><p>He accomplished this by making his program programmable.
    <img src="http://ratfactor.com/forth/talkimg/fortran_punchcard.png" alt="fortran on a punchcard">
    </p><div>
        <p>Here's a quote from <a href="https://www.forth.com/resources/forth-programming-language/">The Evolution of Forth</a> (forth.com):
        </p><blockquote>"Moore's programming career began in the late 1950s at the
            Smithsonian Astrophysical Observatory with programs to compute
            ephemerides, orbital elements, satellite station positions, etc.
            His source code filled two card trays. To minimize recompiling this
            large program, he developed a simple interpreter to read cards
            controlling the program. This enabled him to compose different
            equations for several satellites without recompiling..."</blockquote>
        <p>His free-form input format turned out, ironically, to be more
        <em>reliable</em> for human use than Fortran, which required formatted
        columns. (At the time, any mis-aligned columns in Fortran punchcard
        input would require a re-run of the program!)
        </p><p>It was also faster and more compact.
        </p><p>These "programming the program" statements in Moore's simple
        interpreter did not use keywords.
        They were statement <em>numbers</em> encoded on a punchcard.
    </p></div>
</div>

<div>
    <p>This is the origin of the system that would eventually be named
    <b>Forth</b>.
    </p><p>According to Moore, the interpreter's statement numbers would have been
    roughly equivalent to these Forth words:
    </p><pre>WORD NUMBER INTERPRET ABORT
    </pre>
    <p>Free-form input was unusual at the time. It's obviously a super nice
        alternative to recompiling your calculation program every time you want
        to change some numbers!

    </p><div>
        <p>So, at last, we have discovered <strong>the true origin of the Forth
        language</strong>: Moore wrote a simple interpreter to reduce waste
        and tedium.
        </p><p>Already, Moore has exhibited the defining combination of traits
        shared by great programmers around the world: Inventive and allergic to
        tedium.
        </p><p>If it had stopped there, it would have been a clever trick and
        perhaps worthy of a footnote in history.
        </p><p>But Chuck Moore did not stop there.
    </p></div>
</div>

<div>
    <h2>Stanford 1961</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/burroughs_b5500_computer.png" alt="drawing of chuck at the stanford burroughs b5500 system"></p><div>
        <p>Now we head from Massachusetts to California where Moore found
        himself at Stanford University where he received his BA in Physics
        and started graduate school. He worked with Stanford's
        <b>Burroughs B5500</b>.
        </p><p>Let's talk about the computer first:
        </p><p>The B5500 (or "B 5500" - the official manual puts a space between
        the B and the number) was a solid-state computer. It was part of the 
        <a href="https://en.wikipedia.org/wiki/Transistor_computer">"second-generation" of computers</a>
        (wikipedia.org).
        These computers had discrete transistors on circuit boards.  By
        contrast, the <em>first generation</em> before them used vacuum tubes
        (like the aforementioned IBM 704) and the <em>third generation</em>
        after them used integrated circuits.
        </p><p>In fact, the 
        <a href="https://en.wikipedia.org/wiki/Burroughs_Large_Systems">Burroughs Large Systems</a>
        engineers were transistor computer pioneers.
        And the B5000 series was a pioneering system.
        </p><p>Here's some more resources:
        </p><ul>
            <li><a href="http://www.retrocomputingtasmania.com/home/projects/burroughs-b5500/b5000_b5500_gallery">Burroughs B5000 / B5500 / B5700 gallery</a>
                (retrocomputingtasmania.com)
                - an awesome illustrated guide including a picture of the
                actual Stanford B5500.
            </li><li><a href="http://www.bitsavers.org/pdf/burroughs/B5000_5500_5700/1021326_B5500_RefMan_May67.pdf">Burroughs B5500 Reference Manual (PDF)</a>
                (bitsavers.org)
                - The entire 224 page manual that came with the computer.
            </li><li><a href="http://infolab.stanford.edu/pub/voy/museum/computers19jan08.html">Early Computers at Stanford</a>
                (stanford.edu)
                - a description of the computer itself and a brief summary
               of its use at Stanford.
        </li></ul>
        <p>And what exactly did Chuck Moore do with that B5500 machine?
    </p></div>
</div>

<div>
    <p>Moore's CURVE was another mathematical application, written in
    Stanford's own Algol implementation.
    </p><p>It contained a much more sophisticated interpreter this time
    with a data stack and control flow operators.
    </p><p>Equivalent Forth words:
    </p><pre>IF ELSE DUP DROP SWAP + - * 
    </pre>

    <div>
        <p>(As we'll see, symbols like "+" and "-" are <i>words</i> in Forth.)
        </p><p>Moore worked on the Stanford Linear Accelerator
        as a programmer. His focus was on steering the beam of
        the electron accelerator.
        </p><p>The CURVE program was even more "programmable" than
        his Fortran program at SAO. He took those ideas and
        expanded them to include the idea of a parameter stack
        and the ability to define new procedures.
        </p><p>This made the interpreter much more flexible and capable.
        </p><p><b>Aside:</b> At this point, I also think it's interesting to
        compare Moore's budding interpreter language with another interpreter
        created specifically to be embedded in larger programs for controlling
        them:
        <a href="https://en.wikipedia.org/wiki/Tcl">The Tcl programming language</a>
        (wikipedia.org).
        27 years after Moore started his work, John Ousterhout created Tcl out
        of frustration with ad-hoc, half-baked solutions 1988 at Berkeley. The
        name comes from "Tool Command Language".  <strong>But the comparison
            goes deeper than just the shared motivation.</strong> Tcl and Forth
        have similar levels of syntactical purity and flexibility. Everything
        in Tcl is a string!  Both languages give the user the power to define
        fundamental parts of the system, such as new control structures, in the
        language itself.  If this sounds interesting, you owe it to yourself to
        play with Tcl for a while. It is extremely clever and extremely
        capable. The main implementation has been well cared-for and can be
        found on most Unix-like systems, often installed by default.
        </p><p>As Moore demonstrated with CURVE, a powerful, extensible interpreter
        is a huge time-saver (certainly when compared to re-compiling the
        program!) and allows the user of the program to add to the program's
        functionality on the fly. It's difficult to overstate how powerful this
        can be.
        </p><p>Truly, now we have the beginnings of a fully-fledged
        programming language. It's not named Forth yet, but
        we're getting closer.
    </p></div>
</div>

<div>
    <h2>Freelancing 1965</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/teletype33.png" alt="a teletype 33 with paper tape reader and writer"></p><p>"With the TTY came paper-tape and some of the
        most un-friendly software imaginable - hours of editing and punching
        and loading and assembling and printing and loading and testing
        and repeating."
    </p><p>-- Chuck Moore, Forth, the Early years
    </p><div>
        <p>First, let's talk about what "TTY" means in 1965. 
        <a href="https://en.wikipedia.org/wiki/Teleprinter">Teleprinters</a>
        (wikipedia.org) or "teletypewriters" or just "teletype"
        were all printer devices. They printed to continuous sheets of paper
        fan-folded to fit into boxes.
        </p><p>The Latin "tele-" prefix means "far" or "at a distance".  These
        machines trace a direct lineage from telegraphs and Morse code.
        </p><p>
        In the late 1800s, the concept of a typewriter which operated over
        telegraph lines had been explored and existed in a variety of forms.
        But the transmission code, paper tape, and typewriter system devised by
        <a href="https://oztypewriter.blogspot.com/2012/03/new-zealands-donald-murray-father-of.html">Donald Murray</a> (oztypewriter.blogspot.com)
        is the one that won out. And it was arguably Murray's
        choice of QWERTY keyboard that cemented it as the standard around
        the world.
        </p><p>The existing Baudot code (from which we also get the term "baud")
        was modified by Murray into something that very much resembles what we
        still use today. Murray also introduced the concept of control
        characters, which still clearly retain their typewriter origins in the
        names:
        <code>CR</code> (carriage return) and <code>LF</code> (line feed). 
        </p><p>Teletype machines started as point-to-point text communication
        tools (like the telegraph), but they were later used over switched
        networks like the world-wide Telex system which used pulse dialing
        to automatically route a connection through the network.
        </p><p>
        The <a href="https://en.wikipedia.org/wiki/Teletype_Model_33">Teletype Model 33</a>
        (wikipedia.org)
        I drew above was one of the most popular teletypes used with computers.
        It was created by The Teletype Corporation in 1963, which means it
        shares a birth year with the ASCII standard!  It remained popular until
        the mid-1970s when video terminals finally came down in price enough to
        push printer teletypes aside. In fact, Teletype Co. made the Model 33
        until 1981, which is much later than I would have guessed!
        </p><p>As for
        <a href="https://en.wikipedia.org/wiki/Punched_tape">paper-tape</a>
        (wikipedia.org), I'll just quote Wikipedia directly:
        </p><blockquote>"Punched tape was used as a way of storing messages for
            teletypewriters. Operators typed in the message to the paper tape,
            and then sent the message at the maximum line speed from the tape.
            This permitted the operator to prepare the message "off-line" at
            the operator's best typing speed, and permitted the operator to
            correct any error prior to transmission. An experienced operator
            could prepare a message at 135 words per minute (WPM) or more for
            short periods."
        </blockquote>
        <p>Donald Murray didn't invent the concept of perforated paper
        tape for data storage, but his system used it for the encoding of
        transmitted messages from the keyboard. It doesn't seem like a stretch
        to trace the origins of this storage method to Murray's system.
        </p><p>The computers of this era and earlier were paper manipulators.
        They were kind of like really complicated typewriters. They displayed
        their output on paper, they were programmed with paper, and they kept
        long-term storage on paper!
        </p><p>But as time went on, computer interactivity increased. They became
        less like typewriters and more like the machines we use today.
        </p><p>As each new ability emerged, Forth became increasingly interactive.
    </p></div>
</div>

<div>
    <p>Forth gains direct terminal input and output!
    </p><pre>KEY EMIT CR SPACE DIGIT
    </pre>
    <p>These new words turned Moore's system into a program editor.
    </p><p>Now you can edit the program within the program.
    </p><p>Moore's complete system is now kind of like an integrated development
    environment and kind of like an operating system.

    </p><div>
        <p>In the mid-1960s, "mini-computers" came out. They were 
        still huge by today's standards, but no longer required a
        large room of their own.
        </p><p>In addition to the reduction in size, the other emerging change was
        direct interactive use of a computer via teletype.
        </p><p>Specifically, the invention of
        <a href="https://web.stanford.edu/~learnest/nets/timesharing.htm">timesharing</a> (stanford.edu)
        was a huge shift away from the "batch processing" style of
        computing that had come before (like with input via punchcard).
        </p><p><i>(<b>Fun fact:</b> A "second generation" time-sharing operating system
        called <a href="https://www.multicians.org/history.html">Multics</a>
        (multicians.org)
        was the spiritual ancestor of and
        name from which Brian Kernighan made the joke name
        <strong>Unix</strong>: "One of whatever Multics was many of".)</i>
        </p><p>Moore's evolving pre-Forth language also gained
        completely interactive editing and executing of programs.
        </p><p>This would have been right around the time
        that the original
        <a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">LISP REPL (Read-eval-print loop)</a>
        (wikipedia.org)
        was created in 1964 on a PDP-1.
        </p><p>If not pre-saging, Moore was certainly on the bleeding edge
        of interactive computer usage!
        </p><p><b>Aside:</b> If you want to see an awesome demonstration of
        interactive computer usage on paper, check out this demonstration
        by Bob Spence:
        <a href="https://youtu.be/_DTpQ4Kk2wA">APL demonstration 1975</a>
        (youtube.com).
        <a href="https://en.wikipedia.org/wiki/Robert_Spence_(engineer)">Bob Spence</a>
        (wikipedia.org)
        is best known for his own contributions, including a number of early
        clever computer interaction ideas that are worth re-examining today.
        Bob's demo is extremely pleasant to watch and brilliantly presented
        in split screen. Notice how paper output lets you mark up stuff with
        a pen - pretty nice feature!
        And
        <a href="https://en.wikipedia.org/wiki/APL_(programming_language)">APL</a>
        (wikipedia.org)
        is a whole other rabbit hole which has interesting intersections with
        the point-free and higher-order function programming we've encountered
        earlier.
        </p><p>Then this happens...
    </p></div>
</div>


<div>
    <h2>1968</h2>
    <p>IBM 1130 minicomputer at Mohasco, a textiles manufacturer in New York.
    <img src="http://ratfactor.com/forth/talkimg/ibm_1130.png" alt="drawing of chuck at an IBM 1130 minicomputer">
</p></div>

<div>
    <p>16 bit, 8 KB RAM.
    </p><p>Backup was via punch/reader.
    </p><p>With disks, now we can have file names!
    </p><p>File names limited to 5 characters...
    </p><p>Moore names his "fourth generation" system "FORTH".

    </p><div>
        <p>Yup, this really is the origin of the name, "Forth". Funny how
        temporary things tend to stick and last forever, isn't it?
        </p><p>The
        <a href="https://en.wikipedia.org/wiki/IBM_1130">IBM 1130</a>
        (wikipedia.org)
        is one of those new-fangled "minicomputers" we've talked about.
        Gosh, it was so small, the CPU weighed less than a car!
        </p><p>
        And it was affordable! The base model was as low as $32,000.
        Compare that to $20,000, the median price for a house in the U.S.
        in 1965.
        Just think of that: If you could afford a house, you were well
        on your way to being able to afford a <em>computer</em>!
        </p><p>As noted, the unit Chuck Moore worked on had a disk drive,
        which would have bumped up the price an additional $9,000.
        That would be the equivalent of buying an above-average house
        and adding a couple brand-new 1965 cars in the driveway.
        </p><p>But, wow, imagine having disk drive cartridges with 512 KB of
        storage at your disposal.  What would you do with all that space?
        </p><p>As mentioned, at this time, we're still interacting with the
        computer (mostly) via paper, but these minis brought the idea of
        interactive computing to "the masses" because they were so much
        smaller, cheaper, and more reliable than the sorts of computers that
        had come before.
        </p><p>
        Quoting
        <a href="https://www.forth.com/resources/forth-programming-language/">The Evolution of Forth</a> (forth.com):
        </p><blockquote>
            "Newly married and seeking a small town environment, Moore joined
            Mohasco Industries in Amsterdam, NY, in 1968. Here he developed
            computer graphics programs for an IBM 1130 minicomputer with a 2250
            graphic display. This computer had a 16-bit CPU, 8k RAM, his first
            disk, keyboard, printer, card reader/punch (used as disk backup!),
            and Fortran compiler. He added a cross-assembler to his program to
            generate code for the 2250, as well as a primitive editor and
            source-management tools. This system could draw animated 3-D
            images, at a time when IBM's software for that configuration
            drew only static 2-D images. For fun, he also wrote a version of
            Spacewar, an early video game, and converted his Algol Chess
            program into the new language, now (for the first time) called
            FORTH. He was impressed by how much simpler it became."
        </blockquote>
        <p>As you may have gathered by now, Chuck Moore is a pretty
        extraordinary computer programmer.
        </p><p>It turns out the IBM 1130 was hugely influential to a bunch of early
        big-name programmers in addition to Moore. Something was in
        the air.
        </p><p>In addition to it's funny new name, Forth had also gained new
        abilities:
    </p></div>
</div>

<div>
    <p>Moore adds return call stack, allowing nested word definitions:
    </p><pre>: DOUBLE DUP + ;
: QUAD DOUBLE DOUBLE ;
    </pre>
    <p>And a <b>dictionary</b> of words.

    </p><div>
        <p>It's not just the name that makes this the first real Forth:
        A dictionary of named words which can be called interactively or
        recursively in the definitions of other words is one of the
        defining features of Forth. The ability to use words as building
        blocks is the Forth language's primary abstraction.
        </p><p>In the example above, we've defined a word called <code>DOUBLE</code>
        which duplicates the number on the top of the stack and adds the
        two numbers together.
        </p><p>A second word called <code>QUAD</code> uses the previous definition
        by calling <code>DOUBLE</code> twice, quadrupling the number in a
        rather amusing way.
        </p><p>A return stack makes this possible. Without a return stack, we have
        no way of telling the computer how to "get back" to the place in
        <code>QUAD</code> where we left off after <code>DOUBLE</code> is done.
        </p><p>(We'll get to the specifics of the syntax soon. That's another
        vital part of understanding Forth.)
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/univac_1108.png" alt="drawing of chuck at a univac 1108 console"></p><h2>1970</h2>
    <p>Still at Mohasco. Programming a Univac 1108.
    </p><p>A new port of Forth written in assembler and could call COBOL modules
    because that's what the corporate suits wanted in 1970.
    </p><p>Moore <b>hates complexity</b>.

    </p><div>
        <p>First of all, the <a href="https://en.wikipedia.org/wiki/UNIVAC_1100/2200_series">UNIVAC 1108</a>
        (wikipedia.org)
        is a great example of the awesome "retro-futuristic" design of
        these old machines. Just look at the sweeping angles in my drawing
        of the console. That's a cool computer console!
        </p><p>When these computers cost more than a house, it makes perfect
        sense that they were constructed into beautiful custom furniture
        that made them look like space ships.
        </p><p>You have to wonder: Did the sci-fi art of the time drive
        the design of these computers or did the computers and industrial
        design of the time inform the art? Or, more likely, did they both
        feed off of each other in the classic cycle of, "life imitates art
        imitates life?"
        </p><p>That's a teletypewriter built into the desk of the console.
        I presume the tractor-feed paper would have spooled to and from
        containers behind the sleek facade.
        </p><p>Anyway, the UNIVAC 1108 is an even more modern computer than the IBM
        1130. Now we're moving into using integrated circuits for everything,
        including the register storage. (Speaking of registers, the 1108 had
        128 of them and must have been interesting to program!)
        </p><p>As was also the trend at the time, the CPU
        was constructed of discrete cards connected together by a wire-wrapped
        backplane.

        </p><p>If you're not familiar with the technique, you should know that
        <a href="https://en.wikipedia.org/wiki/Wire_wrap">wire-wrapped</a>
        (wikipedia.org)
        connections are extremely high quality. Wire is wrapped with
        great force around a post, making a gas-tight connection that will not
        corrode (corrosion can occur outside the connection, of course). A
        little bit of the insulation gets wrapped in the last turns, which
        provides flexibility and strain relief. There are NASA guidelines for
        making a perfect wire-wrap connection.
        </p><p>Anyway, the Univac was even more powerful and modern
        than Moore's previous computer and he took advantage of it.
        </p><p>You don't have to read between the lines to see Moore's obvious
        distaste of
        <a href="https://en.wikipedia.org/wiki/COBOL">COBOL</a>
        (wikipedia.org),
        the COmmon Business-Oriented Language.
        What's impressive is that he managed to still use Forth while
        also using the required COBOL modules.
        </p><p>When this project was abandoned by the employer, Moore was
        upset by the whole situation, particularly the way business software
        was increasing in complexity. This won't be the last time we
        see this theme crop up.
        </p><p>He also wrote a book (unpublished) at this time called
        <em>Programming a Problem-Oriented Language</em>.
        It's written in typical Moore fashion, without superfluous words or
        exposition. Feel free to contrast this with the article you're reading
        now.
        </p><p>(This book will be mentioned again later.)
    </p></div>
</div>

<div>
    <h2>NRAO - Early 1970s</h2>
    <p>National Radio Astronomy Observatory
     - Computer control software for radio telescopes.
    <img src="http://ratfactor.com/forth/talkimg/nrao.png" alt="drawing of radio telescope dishes from NRAO">

    </p><div>
        <p>Radio telescopes are like visual telescopes, but they collect lower
        frequency waves. Thanks to the magic of computers, we can process these
        signals to see what the radio telescopes see.
        </p><p>Radio telescopes can work with everything from 1 kHz, which is just
        below the uses of "radio" as we think of it for navigation,
        communication, and entertainment, to 30 GHz, which is still well under
        the visible portion of the electromagnetic spectrum. Consumer microwave
        ovens operate at about 2.45 GHz.
        </p><p>(Speaking of Gigahertz, apparently Intel Core i9 processors can run
        at clock speeds up to 6 Ghz, but most CPU designs top out at around 4
        Ghz. This may be important for Forth for reasons I explain later.)
        </p><p>The visible part of the spectrum is very small by comparison. It
        starts at 420 THz (terahertz) and ends at 720 THz. The familiar
        rainbow of colors captured in the mnemonics "Roy G. Biv" or "Richard of
        York Gave Battle in Vain" (ROYGBIV) lists colors in order of lowest
        frequency (Red) to highest (Violet).
        </p><p>Here is the official website of the
        <a href="https://public.nrao.edu/">National Radio Astronomy Observatory</a>
        (nrao.edu).
        But for a better summary,
        <a href="https://en.wikipedia.org/wiki/National_Radio_Astronomy_Observatory">the Wikipedia entry</a> (wikipedia.org)
        is the way to go. Be sure to scroll down to the incredible image and
        description from 1988 of the collapsed 300ft radio telescope:
        </p><blockquote>
            "The telescope stood at 240ft in height, wieghed 600-tons, had a
            2-min arc accuracy, and had a surface accuracy of ~1 inch. The
            collapse in 1988 was found to be due to unanticipated stresses
            which cracked a hidden, yet weight and stress-supporting steel
            connector plate, in the support structure of the massive telescope.
            A cascade failure of the structure occurred at 9:43pm causing the
            entire telescope to implode."
        </blockquote>
        <p>The 300ft dish had been the world's largest radio telescope when it
        went active in 1962 at the NRAO site in West Virginia.
        </p><p>My drawing above is of the
        <a href="https://en.wikipedia.org/wiki/Very_Large_Array">Very Large Array</a>
        (wikipedia.org)
        in New Mexico.
        NRAO is also a partner in a huge international array in Chile.
        </p><p>By using radio interferometry, arrays of telescopes can be treated
        as essentially one huge telescope with the diameter of the array
        (missing the <em>sensitivity</em> a dish of that size would have).
        </p><p>But the scope for which Moore wrote software was a single 36ft (11
        meter) dish at Kitt Peak in Arizona called <b>The 36-Foot Telescope</b>.
        It was constructed in 1967 and continued
        working until it was replaced with a slightly larger and more
        accurate dish in 2013.
        </p><p>The 36ft scope was used for millimeter-wavelength molecular astronomy.
        This is the range above "microwaves" and these telescopes pretty
        much have to be constructed at dry, high altitude sites because
        water vapor in the air can interfere with the radio waves.
        </p><p>(Note that Moore stayed at the NRAO headquarters in Virginia and
        was not on-site at Kitt Peak.)
        </p><p>NRAO had a policy of using Fortran on its minicomputers, but based
        on the success of his previous work, Moore was begrudgingly given
        permission to use Forth instead.
        I couldn't possibly do justice to summarizing it, so here's Chuck's
        own words describing the software he wrote for the NRAO (also from
        <i>Forth - The Early Years</i>):
        </p><blockquote>
            <p>"There were two modes of observing, continuum and spectral-line.
            Spectral-line was the most fun, for I could display spectra as they
            were collected and fit line-shapes with least-squares."
            </p><p>It did advance the state-of-the-art in on-line data reduction.
            Astronomers used it to discover and map inter-stellar molecules
            just as that became hot research."
        </p></blockquote>
        <p>
        <a href="https://public.nrao.edu/gallery/the-36foot-molecule-hunter-at-work/">Here is a photo</a> (nrao.edu) of the 36-foot telescope.
        And
        <a href="https://www.nrao.edu/archives/items/show/35209">here is a photo of the control room in 1974</a>
        (nrao.edu)
        with what appears to be a PDP-11 in the background.
        </p><p>As you can see, the work itself was extremely interesting and
        cutting-edge. But how Moore went about it was also very interesting,
        which a series of computer drawings will demonstrate in a moment.
        </p><p>But on the Forth language front, there was another development...
    </p></div>
</div>
<div>
    <p>At this time, there are talks of patenting Forth.
    </p><p>Moore believes <b>ideas shouldn't be patented</b>.

    </p><div>
        <p>We take it for granted now that "free" or "open" software
        unencumbered by patents and restrictive corporate licenses is a good
        thing. But this was <em>absolutely not a mainstream position</em> in
        the early 1970s.
        </p><p>To put things in context, in the summer of 1970, 
        <a href="https://en.wikipedia.org/wiki/Richard_Stallman">Richard Stallman</a>
        (wikipedia.org) was just out of high school and was writing
        his first programs in Fortran (which he hated) and then APL.
        </p><p>It wasn't until 1980 that Stallman finally got fed up enough with
        the state of proprietary and legally encumbered software to start the
        "free-as-in-freedom" software revolution. Companies were increasingly
        using copyright to prevent modification, improvement, or duplication by
        the end user. Stallman, being a pretty incredible programmer, wrote free
        clones of such programs. He announced the
        <a href="https://en.wikipedia.org/wiki/GNU_Project">GNU project</a>
        (wikipedia.org)
        in 1983.
        </p><p><b>Aside:</b> I believe Stallman was right. There's absolutely
        nothing wrong with writing programs for money or selling software. But
        using the law to prevent people from truly owning that software
        by limiting how or where to run it, or even <em>preventing people from
            writing their own similar software, if they are capable</em>, is an
        abominable practice and should be countered at every step.
    </p></div>
</div>

<div>
    <p>Moore also rejects the standardization of Forth.
    </p><div><p>"All of my fears of the standard and none of the advantages of the standard have come to pass. Any spirit of innovation has been thoroughly quelched.
    </p><p>Underground Forths are still needed.
    </p><p>I said I thought the standard should be a publication standard but they wanted an execution standard."
    </p></div><p>-- Chuck Moore, 1997

    </p><div><p>
        Quote from the <b>ANSI Forth</b> section in 
        <a href="http://www.ultratechnology.com/moore4th.htm">this cool collection of Forth quotes</a>
        (ultratechnology.com) by Jeff Fox.

        </p><p>I think that when you get to the heart of what Forth is all
        about, Moore's displeasure with the ANSI standardization suddenly makes
        tons of sense. In short, the whole <em>point</em> of Forth is to create
        your own toolkit. Having an all-inclusive language standard is great
        for making sure Forths are interchangeable. Unfortunately, it's
        also antithetical to adapting the language to your specific hardware
        and software needs.
        </p><p>Alright, enough philosophizing. Let's get back to the computer
        stuff!
        </p><p>While Moore was at NRAO, he also wrote software to point the telescope.
        Elizabeth Rather (Moore credits her as Bess Rather in his paper) was
        hired for support and they worked together on at least one port.
        The Forth system migrated across multiple machines at NRAO which,
        as we'll see, highlights one of the <b>technological strengths</b> of the
        standard Forth implementation.
        </p><p>By the way, after her initial reaction of shock and horror, 
        Elizabeth Rather embraced Forth. From
        <a href="https://www.forth.com/resources/forth-programming-language/">The Evolution of Forth</a>
        (forth.com):
        </p><blockquote>
            "After about two months, Rather began to realize that something
            extraordinary was happening: despite the incredibly primitive
            nature of the on-line computers, despite the weirdness of the
            language, despite the lack of any local experts or resources, she
            could accomplish more in the few hours she spent on the Forth
            computers once a week than the entire rest of the week when she had
            virtually unlimited access to several large mainframes."
        </blockquote>
        <p>Rather went on to write the first Forth manual in 1972 and
        write papers about it for the NRAO and other astronomical organizations.
        </p><p>Later, <a href="https://en.wikipedia.org/wiki/Elizabeth_Rather">Elizabeth "Bess" Rather</a>
        (wikipedia.org)
        became the co-founder of FORTH, Inc with Chuck and
        remained one of the leading experts and promoters of the Forth language
        until her retirement in 2006.
        <img src="http://ratfactor.com/forth/talkimg/forth_nrao_spectral_line_observing_ieee1973.jpg" alt="screenshot of the top half of the first page of the article">
        </p><p>There's a great overview paper of the whole NRAO system by
        Moore and Rather in a 1973 <i>Proceedings of the IEEE</i>:
        <a href="http://home.iae.nl/users/mhx/forth_NRAO.pdf">The FORTH Program for Spectral Line Observing (PDF)</a>
        (iae.nl).
        </p><p>It includes a high-level description of the system with examples of
        interactive Forth usage and a neat diagram on the first page, which you
        can see in the screenshot.
        </p><p>As mentioned, Forth was ported to a bunch of different computers
        at NRAO.
        </p><p>Let's take a look:
    </p></div>
</div>

<div>
    <p>Forth on the IBM 360/50
    <img src="http://ratfactor.com/forth/talkimg/ibm_360_50.png" alt="drawing of chuck using an ibm 360/50 computer">

    </p><div>
        <p>Moore mentions first having ported his Forth system to the
        <a href="https://en.wikipedia.org/wiki/IBM_System/360">IBM 360/50</a>
        (wikipedia.org).
        </p><p>The System/360 (or S/360) computers were extremely successful,
        largely because of availability, longevity, and compatibility.
        IBM claims to be the first company to use
        <a href="https://en.wikipedia.org/wiki/Microcode">microcode</a>
        (wikipedia.org)
        to provide a compatible instruction set across all S/360 computers
        despite the hardware differences between models.
        </p><p>The cheaper 360 computers used microcode while the more expensive
        and powerful machines had hard-wired logic. NASA even had some one-off
        models of IBM 360 made just for them.
        </p><p>Until microcode came along, if you bought a "cheap" computer to get
        started and then upgraded to a more powerful computer, you would have
        to re-write your programs in a new instruction set. (If you happen to
        have written your programs in a high-level language like Fortran, you
        would still have to re-compile your programs from punchcards, and you
        would need the Fortran compilers on both computers to be perfectly
        compatible!) It's easy to see why being able to upgrade without
        changing your software would have been appealing.
        </p><p>System/360 computers were
        a "big bet" (5 billion dollars according to IBM themselves:
        <a href="https://www.ibm.com/ibm/history/ibm100/us/en/icons/system360/">System 360: From Computers to Computer Systems</a>
        (ibm.com)) that nearly destroyed the company.
        The bet clearly paid off because they made these machines
        from 1964 to 1978.
        </p><p>Oh, and it wasn't just the instruction set that was compatible. The
        360 computers also had standardized peripheral interfaces, which were
        compatible between machines.
        There was a huge market for peripheral devices.  IBM
        themselves made 54 different devices such as memory, printers, card
        readers, etc. The 360 also spawned a whole third-party peripheral
        industry, much like the IBM PC-compatible era that started in 1981 and
        continues to the desktop computer I'm typing on right now in 2023.
        </p><p>Moore wrote Forth from scratch in S/360 assembly.
        </p><p>Then...
    </p></div>
</div>

<div>
    <p>Forth ported to the Honeywell 316
    <img src="http://ratfactor.com/forth/talkimg/honeywell_316.png" alt="drawing of chuck using a honeywell 316computer">

    </p><div>
        <p>I drew Chuck behind the system in this one because I couldn't
        bring myself to obscure an inch of that glorious pedestal console.
        </p><p>You can see the
        <a href="https://en.wikipedia.org/wiki/Honeywell_316">Honeywell 316</a>
        (wikipedia.org)
        and <a href="https://commons.wikimedia.org/wiki/File:Honeywell316.jpg">the brochure</a>
        (wikimedia.org)
        image from which I made my drawing.
        </p><p>Just look at the space-age lines on that thing! It looks straight
        out of a Star Trek set. Sadly, there's basically no chance the one
        Moore actually worked on had this console. Less than 20 of them were
        sold. <strong>But thanks to my drawing, we can pretend.</strong>
        </p><p>Beyond just its appearance, this particular console has a really
        wild history. The extravagant gift company, Neiman Marcus, actually
        offered the Honeywell H316 with this pedestal as a "kitchen computer".
        It cost $10,000 and would have come with a two-week course to learn
        how to input recipes and balance a checkbook using toggle switches and
        lights to indicate binary data! (As far as anyone knows, none of these
        were actually sold.)
        </p><p>The ad for the Honeywell Kitchen Computer was in full "Mad Men"
        mode and was extremely patronizing, as was unfortunately typical for
        the time. But if you can look past that, the whole thing is quite
        funny:
        </p><blockquote>
            "Her souffles are supreme, her meal planning a challenge? She's
            what the Honeywell people had in mind when they devised our Kitchen
            Computer. She'll learn to program it with a cross-reference to her
            favorite recipes by N-M's own Helen Corbitt. Then by simply pushing
            a few buttons obtain a complete menu organized around the entree.
            And if she pales at reckoning her lunch tabs, she can program it to
            balance the family checkbook..."
        </blockquote>
        <p>You can see a tiny scan of the original ad with a woman admiring
        her new Honeywell Kitchen Computer that barely fits in her kitchen
        <a href="https://en.wikipedia.org/wiki/File:Kitchen_computer_ad.jpg">here</a>
        (wikipedia.org).
        </p><p>But moving on from the pedestal...
        </p><p>The implementation of Forth on the H316 is considered to be the
        first complete, stand-alone implementation because it was actually
        programmed on the computer itself <em>and</em> was used to create other
        Forths. It is at this point that Moore has achieved a fully
        ascendant system.
        </p><p>But wait, there's <strong>moore</strong>...er,
        sorry, <strong>more</strong>!
        </p><p>As is typical for a Chuck Moore endeavor, this
        telescope application pushed other new boundaries:
        The system actually ran across two computers (we're about to see
        the second one) and gave real-time access to <em>multiple</em> astronomers.
        Because it spread the load the way it did, there were no issues with
        concurrency, which is something we programmers struggle with to this day.
        </p><p>This real-time control and analysis was basically a
        luxury available on no other system at the time.
        Even Honeywell, the creator of these computers, had only been able to
        achieve the most primitive concurrency for them and it was
        nothing like this.
        </p><p>As usual, Moore was right on the very crest of
        computing with his ultra-flexible Forth system.
    </p></div>
</div>

<div>
    <p>...And ported to the Honeywell DDP-116
    <img src="http://ratfactor.com/forth/talkimg/ddp_116.png" alt="drawing of chuck using a honeywell DDP-116 computer">

    </p><div>
        <p>As mentioned above, the Forth system was <em>also</em> ported to the
        <a href="https://t-lcarchive.org/honeywell-ddp-116/">DDP-116</a>
        (t-larchive.org).
        and used with its "parent" system on the H316 featured above.
        </p><p><i>(The DDP-116 was originally manufactured by
        <a href="https://en.wikipedia.org/wiki/Computer_Control_Company">Computer Control Company</a> in 1965, but CCC was sold to Honeywell in 1966 and
        became its Computer Controls division.)</i>
        </p><p>The DDP-116 was a 16-bit computer (the first available for
        purchase), but still part of that "second generation" of computers
        we've mentioned before, with individual
        transistors and components wire-wrapped together on huge circuit
        boards. (Check out the pictures on the DDP-116 link above for all
        sorts of excellent views of the insides and outsides of an example
        machine and its peripheral devices!)
        It happens to have also been a pretty rare computer. It didn't sell
        in vast quantities like the IBM systems.
        </p><p>As you can see in the drawing, Chuck Moore began to grow in power as
        his system evolved and this manifested in <strong>additional
            arms</strong>!  Or maybe I started to get a little loopy while
        drawing old computers for these slides in the final evenings before I
        was due to give my talk?  I'll let <em>you</em> decide what is real.
        </p><p>But wait, there's one more!
    </p></div>
</div>

<div>
    <p>Forth on the DEC PDP-11
    </p><p>(Yes, <i>that</i> PDP-11.)
    <img src="http://ratfactor.com/forth/talkimg/dec_pdp_11.png" alt="drawing of chuck using a DEC PDP-11 computer">

    </p><div>
        <p>The
        <a href="https://en.wikipedia.org/wiki/PDP-11">PDP-11</a>
        (wikipedia.org) was by some measures the most popular minicomputer ever.
        </p><p>
        It was a 16-bit machine and had an orthogonal instruction set
        (meaning the same instruction could be used in multiple ways
        depending on the operand. This makes the mnemonics of the instruction
        set smaller and more logical and much easier to memorize).
        This was even more powerful because I/O was memory-mapped, so the
        same instructions used to move values around in memory and
        registers could <em>also</em> be used to transfer data to
        and from devices.
        </p><p>All told, these conveniences made the PDP-11 fun to program!
        Assembly language programmers rejoiced.  The ideas in the PDP-11 spread
        rapidly and are to be found in the most popular architectures in use
        today. Compared to what came before it, PDP-11 assembly language will
        look surprisingly familiar to modern assembly programmers.
        </p><p>The original machines were made starting in 1970 with
        wire-wrapped backplanes and discrete logic gates.
        Later models introduced "large-scale integration," which is a term
        we'll see later, so hold that question!
        These later versions of the PDP-11 were still being
        made twenty years later in 1990! There are apparently still PDP-11s
        performing crucial tasks today, with nuclear power plants being one of
        the most prominent examples.
        </p><p>It's hard to see in my drawing, but the PDP-11 front panel is one
        of the most iconic computer interfaces ever made. Hobbyists make
        working models, including ridiculously cute and awesome miniature
        versions. Here are two model versions - click on them to go to the
        original wikipedia.org files, where you can admire their full beauty:
        </p><p><a href="https://en.wikipedia.org/wiki/File:Digital_PDP11-IMG_1498_cropped.jpg"><img src="http://ratfactor.com/forth/talkimg/Digital_PDP11-IMG_1498_cropped.jpg" alt="pdp11 panel1"></a>
        <a href="https://en.wikipedia.org/wiki/File:Pdp-11-70-panel.jpg"><img src="http://ratfactor.com/forth/talkimg/Pdp-11-70-panel.jpg" alt="pdp11 panel2"></a>
        <br>
        </p><p>It would be difficult to overstate the impact of this machine.
        Probably the most famous piece of software released on the PDP-11
        was the first version of
        <a href="https://en.wikipedia.org/wiki/Unix">Unix</a>
        (wikipedia.org)
        that actually bore the name "Unix".
        </p><p>It was also the birthplace of the
        <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>
        (wikipedia.org)
        programming language.
        Dennis Ritchie ported Ken Thompson's B language to the PDP-11 to
        take advantage of its abilities. Unix was then re-written in C
        starting with Version 4.
        So the Unix we know today and a large portion of the command line
        utilities that are standard with a Unix-like system were programmed
        on the PDP-11. (And you can thank Richard Stallman's GNU project for
        freeing those for the masses. GNU stands for "GNU's Not Unix!")
        </p><p>You'll also note that Chuck Moore has gained his
        <strong>fourth and final arm</strong> in my drawing above
        ("fourth," ha ha).
        <em>This may or may not reflect actual events.</em>
        Also, I'm not sure if Moore would have been using a video terminal at
        that time. It's possible. DEC's first video terminal was the
        <a href="http://www.columbia.edu/cu/computinghistory/vt05.html">VT05</a>
        (columbia.edu),
        which came out in 1970.
    </p></div>
</div>

<div>
    <h2>So much porting!</h2>
    <p>All of this porting of Forth to new machines is possible because of
    <b>indirect threaded code</b>.
    <img src="http://ratfactor.com/forth/talkimg/threaded_code_abstract.png" alt="an abstract drawing of boxes and arrows representing threaded code in memory">
</p></div>

<div>
    <p>"Threaded code" in this usage is <strong>not</strong>
    related to concurrency, i.e. "multi-threaded programming".
    </p><p>It's code that is composed of subroutines addresses.
    </p><p>Threaded code can be machine code or interpreted.

    </p><div>
        <p>Wait, aren't <em>most</em> programs composed of calls to subroutines?
        </p><p>That's true. The big difference is that
        <a href="https://en.wikipedia.org/wiki/Threaded_code">threaded code</a>
        (wikipedia.org) in this sense
        doesn't actually contain the instructions to <em>call</em> the
        subroutines. It stores just the addresses.
        Therefore <em>another</em> routine is responsible for advancing
        a pointer over the address
        list and executing the subroutines.
        </p><p>Huh?
        </p><p>Yeah, there's no way around it, threaded code is complicated.
        </p><p>And <em>indirect</em> threaded code is even more complicated (and
        harder to explain).
        </p><p>"Hey, wait!" I hear you saying. "If Chuck hates complexity so
        much, why did he use such a complex method for Forth?"
        </p><p>That's completely fair.
        </p><p>But before we address that, I'll try to briefly explain how
        threaded code is stored and executed.
        </p><p>First, here's how <em>normal</em> machine code might be written:
    </p></div>
</div>

<div>
    <p>Direct calls (not threaded):
    </p><pre>jmp 0x0804000
jmp eax
    </pre>

    <div>
        <p>This is the simplest type of "call" to store in a program.
        We simply have the <code>jmp</code> (jump) instruction followed
        by the address to jump <em>to</em>.
        Here I show both a hard-coded address
        (<code>0x0804000</code>) and a register
        (<code>eax</code>).
        Both of these are "direct" for our purposes.
        </p><p><i>Alternatively, many processors have a more advanced <code>call</code>
        instruction. A call is more complicated because it has to do additional
        work behind the scenes. It must store a return address on "the stack"
        before jumping to the specified address. Then a <code>ret</code>
        (return) instruction at the end of the called routine can use the
        stored address to resume the execution just after the "call site" where
        the call was first made.  Why are return addresses stored on a stack?
        That's because you can nest calls. Pushing addresses as you jump and
        popping them in reverse order as you return keeps things nice and neat.
        This "the stack" is not what Forth refers to as "the stack". Forth's
        main stack is better known as "the parameter stack". Many Forth
        implementations <strong>also</strong> have a return stack!</i>
        </p><p>Anyway, this is direct and it's not threaded. Just jump to an address.
        </p><p>The first step of complication is adding indirection.
    </p></div>
</div>

<div>
    <p>Indirect calls (not threaded):
    </p><pre>jmp [eax]
    </pre>

    <div>
        <p>For this example to make sense, you need to know that the
        square brackets around the register (<code>[eax]</code>)
        is a common assembly language convention that means
        "the value at the memory address that is stored in register eax".
        </p><p>So <code>jmp [eax]</code> means "jump to the address
        stored at the address stored in register eax."
        </p><p>That's indirect.
        </p><p>So now we have the "indirect" part of "indirect threaded
        code." But what's the "threaded" part?
    </p></div>
</div>

<div>
    <p>Storing threaded code:
    </p><pre>&lt;addr pointing to code&gt;
&lt;addr pointing to code&gt;
&lt;addr pointing to code&gt;
&lt;addr pointing to code&gt;
    </pre>

    <div>
        <p>Instead of containing the actual instructions to jump or
        call subroutines:
        </p><pre>jmp 0x0804000
jmp 0x080A816
jmp 0x08C8800
jmp 0x08C8DD0
        </pre>
        <p><strong>Threaded code</strong> stores just the list of
        addresses:
        </p><pre>0x0804000
0x080A816
0x08C8800
0x08C8DD0
        </pre>
        <p>There are two consequences of storing code like this:
        </p><ul>
            <li>The address list takes up less memory than the full code to
                make the jump. (In fact, it takes a <em>lot</em> less on some
                historic machines.) This is good.
            </li><li>Some sort of "code interpreter" will need to be written to
                execute this list. You can't just send a list of addresses
                to a processor and expect it to work. This could be good or bad.
        </li></ul>
        <p>Another way to look at the list of addresses above is that, 
        conceptually, threaded code is basically a list of subroutines.
        </p><p>To complete our definition of "indirect threaded" code, we just
        need to put both concepts together:
    </p></div>
</div>

<div>
    <p>Storing <b>indirect threaded</b> code:
    </p><pre>&lt;addr pointing to addr pointing to code&gt;
&lt;addr pointing to addr pointing to code&gt;
&lt;addr pointing to addr pointing to code&gt;
&lt;addr pointing to addr pointing to code&gt;
    </pre>

    <div>
        <p>This is where it gets pretty crazy. So now we've got a second
        level of indirection. <strong>Why on Earth would we do this?</strong>
        </p><p>Well, this allows us to store a separate "code interpreter"
        (or "inner interpreter") for different <em>kinds</em> of subroutines!
        </p><p>Instead of pointing directly at subroutines, these addresses point
        at interpreters.
        Talk about ultimate flexibility - every subroutine in an indirect
        threaded program can have its own custom interpreter for the rest
        of its instructions...each of which can also be threaded...or
        indirectly threaded!
        </p><p>But what calls all of these inner interpreters?
        An outer interpreter, of course! The outer interpreter is the
        part we actually interact with when we sit down to type
        at a Forth terminal.
        </p><p>In <b>Forth</b>, indirect threaded code is a list of
        addresses pointing to the "inner interpreter" portions of
        words, which execute the rest of the word.
        What types of inner interpreters could we have, anyway?
        Well, for example, we might have one kind of word that stores a string
        in memory and another that executes machine code. But the only
        limit is your imagination.
        </p><p>Make sense?
        </p><p>I personally would not have understood
        that explanation at all until much later in my journey (I know this
        because similar - probably better - explanations flew right over
        my head). No doubt you're faster than me at apprehending this stuff
        and are already halfway through implementing your own Forth based on
        these descriptions.
        </p><p>None of the rest of the material requires understanding any
        of the above, so please don't feel you need to fully
        <a href="https://en.wikipedia.org/wiki/Grok">grok</a>
        (wikipedia.org)
        it before continuing. Indirect threading is an important part of
        Forth's history, but there are plenty of Forths that do not use it.
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/16k_memory.png" alt="drawing of a minicomputer saying 'i have 16k of core!'"></p><p><b>Threaded code</b> was much more common in the days of yore.
    </p><p>It is very dense, compact on disk and in memory.

    </p><div>
        <p>In addition to its compact storage, threaded code
        would have been <em>even more</em> efficient on the contemporary
        machines during Forth's gestation because
        calling subroutines often wasn't as simple as the
        <code>call</code> instruction found on "modern" architectures.
        </p><p>
        <a href="https://people.computing.clemson.edu/~mark/subroutines.html">Subroutine and procedure call support</a>
        (clemson.edu) by Mark Smotherman explains:
        </p><blockquote>
            "1963 - Burroughs B5000 - A stack-based computer with support for
            block-structured languages like Algol. Parameters and return address
            are stored on the stack, but subroutine entry is a fairly complex
            operation."
        </blockquote>
        <p>So the memory and performance improvements of this style of
        subroutine call were potentially
        very great indeed. This is one of the reasons for <strong>Forth's
            legendary reputation for high performance</strong>.
        </p><p>We'll revisit this topic from another angle soon. But if you're
        interested in these mechanics
        (and want to see the origin of the boxes and arrows
        drawings at the top of this section), check out this multi-part
        article series for The Computer Journal,
        <a href="https://www.bradrodriguez.com/papers/moving1.htm">MOVING FORTH Part 1: Design Decisions in the Forth Kernel</a>
        (bradrodriguez.com),
        by Brad Rodriguez.
        </p><p>The important thing is that we've now fully traced the origins
        of Forth from a simple command interpreter to the full-blown
        <strong>interactive language, editor, operating system, and
            method of code storage and execution</strong> it became.
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/chuck_hero.png" alt="drawing of chuck moore as a superhero with a cape and bowtie"></p><p>That's Forth's <b>origin story</b>.
    </p><ul>
        <li>Postfix notation (RPN)
        </li><li>Stack-oriented
        </li><li>Concatenative programming style
        </li><li>Interpreted
        </li><li>Highly adaptable to machine architectures
        </li><li>Extremely compact
    </li></ul>
    <p>This gives us the <i>why</i>.
    </p><div>
        <p>At last! Now we can put it all together:
        </p><p>Forth is <strong>postfix</strong> because that's a natural
        order for the computer and lends itself to an incredibly minimalistic
            interpreter implementation: get the values, operate on them;
        </p><p>Forth is <strong>stack oriented</strong> because that's a
            compact and convenient way to store
            values without needing to add variables or name things;
        </p><p>Forth is <strong>concatenative</strong> because building a
            language that can operate as a string of words is incredibly
            flexible and can adapt to just about any programming style without
            any help from the language itself. (And it turns out this is
            especially true when you throw in higher-order functions);
        </p><p>Forth is <strong>interpreted</strong> because that is
            interactive and allows the programmer to make fast changes on
            the fly or simply "play" with the system. This is part of
            Forth's adaptability and flexibility;
        </p><p>Forth is <strong>self-hosting</strong> because you can
            bootstrap a Forth implementation from a handful of words
            implemented in assembly and then write the rest in Forth;
        </p><p>Forth is <strong>extremely compact</strong> because machines at
            the time had limited memory and this gave Forth an edge on
            other interpreters (and even compiled languages!) on
            mainframes and mini-computers.
        </p><p>Now that we have everything in historical context, I think it's
            much clearer why Forth exists and why it takes the peculiar
            form that it does.
        </p><p><strong>None of this was planned. Chuck didn't sit down at a terminal
        in 1958 and conjure up Forth. Instead, he grew a system to
        serve his needs and to make use of new hardware as it was made
        available.</strong>
        </p><p>Reading about Forth's history is a wonderful way to understand
        what makes Forth special and what it's <i>about</i>.
        </p><p>But even knowing all of this, I was still a long way off from a true
        understanding of how this <strong>all comes together</strong> in an
        actual working system. I didn't really understand how it worked.
        And I didn't understand what Forth was actually like to <em>use</em>
        In other words, I still didn't understand Forth as a
        <em>programming language</em>.
    </p></div>
</div>

<div>
    <h2>Actually Understanding How Forth Works</h2>
    <p>Somewhere along the way, I came across these quotes...
    </p><p>"To understand Forth, you have to implement a Forth."
    </p><p>-- Somebody on the Internet
    </p><p>And</p>
    <p>"Take a look at JonesForth."
    </p><p>-- Everybody on the Internet

    </p><div>
        <p>I've mentioned it before, but I'll point it out again. Notice the
        phrasing "implement <em>a</em> Forth."
        </p><p>As we've established, Chuck Moore believes a Forth system is best
        when it is custom-tailored to the system and task at hand.  So it
        should come as little surprise that writing your own Forth or Forth-like is
        entirely "par for the course" in any would-be-Forther's quest to
        discover the True Meaning of the language and enter the mystical realm
        where All is Revealed.
        </p><p>Well, what else could I do?
        </p><p>Having no other clear course of study, I decided to heed the
            wisdom of the crowd.
        </p><p>Presenting...
    </p></div>
</div>


<div>
    <h2>JonesForth and "Assembly Nights"</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/assembly-nights.jpg" alt="My faithful Asus EeePC 701 waiting romantically on the bed. Text reads 'Assembly Nights'">
    <i></i></p><div>
        <p>To really get to know it, I took Forth to bed with me.
        </p><p>I wrote
        <a href="http://ratfactor.com/assembly-nights">Assembly Nights</a>
        when I realized how much I was enjoying myself:
        </p><blockquote>
            "Over the last three months, I've developed an unusual
            little nighttime routine..."
        </blockquote>
        <p>I prepared myself for dealing with the JonesForth source
        (i386 assembly language in the GNU GAS assembler)
        by learning some assembly and Linux ABI basics.
        JonesForth is 32-bit only and uses the Linux system call ("syscall")
        ABI directly.
        </p><p>Then I spent roughly a year porting JonesForth into a complete
        working copy in NASM assembler. (Yes, that's a "port" from one flavor
        of i386 asm to another.)
        </p><p>I did a tiny bit almost every night.  A lot of it was debugging in
        GDB.
    </p></div>
</div>

<div>
	<p><img src="http://ratfactor.com/forth/talkimg/nasmjf.png" alt="my giant gold on gray logo for nasmjf"></p><p>My NASM port of JonesForth: <code>nasmjf</code>
    </p><p>Opening the third eye by (re)implementing Forth.

    </p><div>
        <p>Here's the
        <a href="http://ratfactor.com/nasmjf/">nasmjf web page</a>
        </p><p>In the process of writing the port, I learned how a traditional
        indirect threaded Forth works.
        </p><p>And I learned that <em>it takes time</em> to absorb such a
        twisty-turny  method of code execution.
        </p><p>Especially if the x86 assembly language tricks are new to you like
        they were for me.
    </p></div>
</div>

<div>
    <p>JonesForth ascii art:
    <img src="http://ratfactor.com/forth/talkimg/jonesforth1.png" alt="jonesforth ascii art explaining flow of threaded code">

    </p><div>
        <p>One of the first things you encounter when you open up the
        <code>jonesforth.S</code> (a single file which contains the assembly
        language portion of JonesForth) are many ASCII art diagrams.
        </p><p>Richard W.M. Jones does an excellent job of walking you through
        the workings of the interpreter and explaining the i386 instruction
        set features he uses.
        </p><p>If the diagram above seems bewildering, I agree.
        </p><p>So, of course, I thought maybe I could do better...
    </p></div>
</div>

<div>
    <p>Here's my attempt (from the <code>nasmjf</code> source):
    <img src="http://ratfactor.com/forth/talkimg/nasmjf1.png" alt="my nasmjf ascii art explaining flow of threaded code">

    </p><div>
        <p>After I was done with my port, I tried to make an ASCII art diagram
        of my own to capture my new understanding.
        In fact, this is one of several.
        </p><p>With the benefit of the distance of time, it is clear to me that
        these things only make sense once you already understand them to
        some degree. But the act of <em>making them</em> is extremely useful
        for solidifying your understanding.
        </p><p>But wait, there's more!
        </p><p>Both ASCII art diagrams above are just <em>part</em> of the complete
        indirect threaded execution system. They're just showing how the "inner
        interpreter" works to execute Forth words.
        </p><p>Perhaps you recall from the section about indirect threaded code
        above that the second level of indirection allows different
        "interpreter" routines to execute different types of threaded
        subroutines?  Well, that's all those two ASCII diagrams are trying
        show.
        </p><p>But when we say that Forth is an <em>interpreted</em> language,
        this is not what we're talking about. There's also the "outer interpreter"
        that the programmer interacts with.
    </p></div>
</div>

<div>
    <p>The indirect threaded code is just the tip of the iceberg!
    </p><p><code>nasmjf</code> inner/outer interpreter diagram:
    <img src="http://ratfactor.com/forth/talkimg/forth-diagram.png" alt="my nasmjf diagram showing outer and inner interpreter">
    
    </p><div>
        <p>In the vector image I made above for nasmjf, I attempted to map out
        the <strong>whole thing</strong> in my own words.
        </p><p>If you take anything from this image, it's that
        <code>INTERPRET</code> looks up words (functions) by name and calls
        them by executing the interpreter routine whose address is stored in
        the word (again, this is the indirect threading part). In turn, there
        may be any number of interpreters, but the three main types used in
        JonesForth are:
        </p><ul>
            <li>Pure assembly language routines are their own interpreters.
            </li><li>"Regular" Forth words use the <code>DOCOL</code> interpreter.
                DOCOL executes the rest of the threaded code in the word,
                most of which is just a list of addresses, but some of
                which will be data. This is the "normal" kind of threaded
                subroutine.
            </li><li>Numeric literals have a tiny interpreter routine inline with
                the data that just pushes their value to the stack. Numeric
                literals don't have to be words, though, in JonesForth,
                they're just a bit of inlined machine code.
        </li></ul>
        <p>But even knowing this only helps to explain how code <em>starts</em>
        executing. How does this type of Forth know what to run after a word is
        complete?
        </p><p>Ah, for that we have this:
    </p></div>
</div>

<div>
    <p>To get from one <b>code word</b> to another requires a bit of
       assembly pasted at the end of each one. This is
       the <b>NEXT</b> macro. Here it is from <code>nasmjf</code>:
    </p><pre>%macro NEXT 0
    lodsd     ; NEXT: Load from memory into eax, inc esi to point to next word.
    jmp [eax] ; Jump to whatever code we're now pointing at.
%endmacro
    </pre>

    <div>
        <p>Notice the term "code word". That's the Forth term for words
        written in pure assembly language.
        </p><p>Every code word has this macro at the end. (Some Forths actually
        call a subroutine for this. JonesForth uses this two-line macro
        because the action is so efficient in i386 machine code.)
        </p><p>Remember the list of addresses in the explanation of
        "indirect threaded" code? This is how we execute them sequentially.
        </p><p>This implementation uses the i386 <code>lodsd</code> instruction
        to take care of two operations in one: move a "double word"
        from memory into a register, and then update another register
        so that it points to the next "double" spot in memory.
        </p><p><em>(Rant: And a "double" is 32 bits on Intel chips for the really
            annoying reason that they kept the definition of "word" at 16 bits
            even as the platform moved to 32 and then 64-bit architecture. So
            "word" on Intel architectures is a completely meaningless thing
            that you just have to memorize as "16 bits" even though
            "word" is supposed to be the native data size of the architecture.
            And what's worse is that the tools for working with programs on
            Intel chips like GDB then refer to everything with the
            corresponding C names for everything, which naturally assumed that
            the architecture names would be based on reality. But they aren't.
            So terms like "double" and "long" are basically just absolutely
            worthless legacy garbage to memorize and useful only to C and Intel
            architecture veterans.)</em>
        </p><p>Okay, so now the <code>eax</code> register points to the next
        threaded subroutine address in memory. The <code>jmp</code> starts
        executing whatever that points to, which will be the "inner interpreter"
        for that subroutine.
        </p><p>Got that?
        </p><p>A lot of moving parts, right?
        </p><p>There's more:
    </p></div>
</div>

<div>
    <p>To get from one <b>colon word</b> to another uses a bit of
       assembly pasted at the end of each in a chunk called
       the <b>EXIT</b> macro. Here it is from <code>nasmjf</code>:
    </p><pre>DEFCODE "EXIT",EXIT,0
    POPRSP esi            ; pop return stack into esi
NEXT
    </pre>

    <div>
        <p>Remember, there's two fundamental types of words in a
        traditional Forth like JonesForth:
        "Code" words and "colon" words.
        Code words are primitives written in  machine code.  Colon words are
        the "regular" words actually written in the Forth language.
        </p><p>These "colon" words (so-named because they are assembled
        via the "COLON" compiler, which we'll talk about in a moment),
        all end in the so-called <code>EXIT</code> macro.
        </p><p>The <code>EXIT</code> macro handles the return stack.  <em>Then</em>
        there will be a <code>NEXT</code> after that to conclude whatever code
        word primitive we were in (we're always in at least one because the
        "outer-most" interpreter is a code word primitive!), so the
        process we described above will automatically start where we left off
        at the "call site" of the word we
        just finished executing.
        </p><p>If you weren't lost before, surely this will do the trick?
        </p><p>I do have another attempt to explain how this all nests in
        a sort of indented pseudocode:
    </p></div>
</div>

<div>
    <p>My comment in <code>nasmjf</code> attempting to explain the
		execution of indirect threaded
        code as a nested
        sequence of <b>NEXT</b> and <b>EXIT</b> and <b>QUIT</b>:
    </p><pre>; QUIT (INTERPRET)
;     * regular word
;         DOCOL
;         NEXT
;         * regular word
;             DOCOL (codeword
;             NEXT
;             * code word
;                 &lt;machine code&gt;
;             NEXT
;             * code word
;                 &lt;machine code&gt;
;             NEXT
;         EXIT
;         NEXT
;    EXIT
;    NEXT
; QUIT (BRANCH -8 back to INTERPRET for more)
    </pre>

    <div>
        <p>This nested view of the process is as close as I've ever been to
        explaining (to myself) what the <em>entire</em> execution flow
        looks like at a high level.
        </p><p>I'm sure every Forth implementer has their own mental model.
        </p><p>You'll notice we didn't even talk about <code>QUIT</code>.
        Other than the name, that one's not nearly as bad - it's really
        just the end of the outer interpreter loop.
        </p><p>(So, yeah, we have <code>EXIT</code> and
        <code>QUIT</code>, neither of which leave Forth... Hey, it was the
        1960s. Things were different then.)
    </p></div>
</div>

<div>
    <p>Absolutely nothing else drives the flow of an indirect
		threaded Forth application: It's addresses stored in
		registers, a return stack, and a handful of assembly instructions
		at the end of each machine code word jumping to the next instruction.
    </p><p>It's like a delicate clockwork machine.
</p></div>

<div>
    <p><strong>Don't you see how simple it is?</strong>
    <img src="http://ratfactor.com/forth/talkimg/crazy_chuck.png" alt="drawing of chuck as crazy charlie explaining a theory with wild eyes and a wall covered in paper and strings">

    </p><div>
        <p>Historical note: The above "Crazy Chuck" drawing is a parody of
        a popular meme with actor Charlie Day's character in the episode
        "Sweet Dee Has a Heart Attack" from the show <i>It's Always Sunny
        in Philadelphia</i>:
        </p><blockquote>
            "Every day Pepe's mail's getting sent back to me. Pepe Silvia, Pepe
            Silvia, I look in the mail, this whole box is Pepe Silvia!"
        </blockquote>
        <p>You, citizen of the distant future, will not have recognized this
        parody, but at least now you can look it up.
    </p></div>
</div>

<div>
    <p>Forth <i>is</i> complex when taken as a whole. But it is made of tiny
        pieces, each of which is <i>very</i> simple. The concept was created
        over a period of years on very constrained systems.
        Each part created only as needed.
    <img src="http://ratfactor.com/forth/talkimg/threaded_code_abstract.png" alt="an abstract drawing of boxes and arrows representing threaded code in memory">

    </p><div>
        <p>I'll repeat your question from before so you don't have to:
        </p><p><strong>
        "Hey, wait! But if Chuck hates complexity so
        much, why did he use such a complex method for Forth?"</strong>
        </p><p>This is where the historical context is, once again, very revealing:
        </p><p>As we've seen, Charles H. Moore did not create Forth all at once in a
        single lightning bolt of inspiration.
        It began as a simple command interpreter and executor and grew
        from there.
        It has always consisted of tiny little parts, working together.
        </p><p><strong>Each of these tiny parts is extremely simple on its own.</strong>
        </p><p>And each was added over a period of time as the need arose.
        </p><p>I think that's the genius of Forth: That all of these little
        pieces can work together to make a running system and yet <strong>still
        remain independent</strong>.
        You can learn each of these in isolation. You can replace them
        in isolation.
        </p><p><strong>Ultimate flexibility and simplicity at the lowest level of
            the implementation comes at the cost of easy understanding at
            higher levels.</strong>
        </p><p>When growing a system like this, most of us would have thought
        bigger, Moore thought smaller.
        </p><p>Let's do the same.
        I've thrown the terms "code word" and "colon word" around a lot.
        I've explained them a bit, but we've never given a proper introduction.
        </p><p>Let's go small:
    </p></div>
</div>

<div>
    <h2>Code words</h2>

    <div>
        <p>Again, <b>Code words</b> are primitives written in machine language
        supplied by the Forth implementation.
        </p><p>Let's see some <em>real</em> code words so we can de-mystify them
        once and for all. These are extremely simple
        and extremely concrete examples of actual NASM assembly language source
        from my <code>nasmjf</code> port of JonesForth:
    </p></div>
</div>


<div>
    <p>Small and simple:
    </p><pre>DEFCODE "SWAP",SWAP,0
    pop eax
    pop ebx
    push eax
    push ebx
NEXT
    </pre>

    <div>
        <p>Is that really SWAP? Yes, it really is! We're just telling the
        CPU to pop the two most recent values from the stack and then push them
        back in the opposite order.
        </p><p>(JonesForth uses the i386 call/return stack as a Forth parameter
        stack so we can use the native "pop" and "push" to make these
        operations easy. In exchange, we lose the ability to use "call"
        and "ret" for subroutines.)
        </p><p>The <code>DEFCODE</code> macro is housekeeping - it creates the
        entry's header in the Forth word dictionary.
        </p><p>Notice the <code>NEXT</code> macro we talked about previously?
        Remember, that's just another two lines of assembly pasted at the
        end of this routine.
    </p></div>
</div>

<div>
    <p>Even Smaller:
    </p><pre>DEFCODE "DUP",DUP,0
    mov eax, [esp]
    push eax
NEXT
    </pre>

    <div>
        <p>We're down to just two instructions now! We move the value pointed
        at by the <code>esp</code> register into eax and then push it onto the
        stack.  </p><p>To understand <em>why</em> this duplicates the top item on
        the stack, you need to know how the <code>esp</code> register is used.
        Here's the relevant comment from the JonesForth source:
        </p><blockquote>
            "In this FORTH, we are using the normal stack pointer (%esp) for the
            parameter stack.  We will use the i386's "other" stack pointer (%ebp,
            usually called the "frame pointer") for our return stack."
        </blockquote>
        <p>Which means that <code>esp</code> points to the current top of 
        the parameter stack. So pushing that value on the stack duplicates
        the top value. (This could also have been written more clearly with
        <em>three</em> instructions: one "pop" and two "push"es.)
    </p></div>
</div>

<div>
    <p>The Smallest:
    </p><pre>DEFCODE "DROP",DROP,0
    pop eax
NEXT
    </pre>

    <div>
        <p>Now we have an entire Forth word defined as a <em>single</em>
        instruction! DROP just "removes" the top value from the stack. In this
        case, we pop it into the <code>eax</code> register and then don't do
        anything with it, essentially throwing it away. (Alternatively, we
        could have decremented the <code>esp</code> register, but in this case,
        the "pop" is both shorter <em>and</em> clearer.)
        </p><p>Now let's see these three words in action in a <em>real</em>
        Forth program that moves some <em>real</em> numbers around
        in memory...
    </p></div>
</div>

<div>
    <h2>Code words in action</h2>
    <pre>8 7      <i>8 7</i>
SWAP     <i>7 8</i>
DROP     <i>7</i>
DUP      <i>7 7</i>
    </pre>

    <div>
        <p>The code word primitives we've just defined are used by the
        rest of the Forth implementation to define colon words in the
        language itself. If you write Forth applications, your own
        colon words will probably use these heavily.
        </p><p>You can also call them interactively in the interpreter.
        </p><p>The above example shows what it might be like to use these
        three primitives right at the keyboard. The column on the right
        shows the state of the parameter stack after each line of input.
        </p><p>Apart from pushing the two numbers on the stack (<code>8 7</code>)
        , we've now seen the assembly language code for the entire
        program shown above. That makes this pretty "bare metal" stuff, right?
        </p><p>Here's the walk-through:
        </p><ul>
            <li>We start with 8 and then 7 on the top of the stack.
            </li><li>SWAP reverses the order of the stack so 8 is now on the top.
            </li><li>DROP pops the 8 and throws it away. Now only 7 remains.
            </li><li>DUP pushes a second copy of 7 onto the top of the stack.
        </li></ul>
        <p>Again, these instructions could exist in the definition of a word or
        you could type them interactively in the running Forth interpreter.
        The result is the same.
        </p><p>I think there's something pretty magical about realizing that
        typing these instructions is running specific machine code
        sequences exactly as they were entered. In this implementation,
        there's no optimizing compiler or virtual machine acting as middle-man.
        You really are communicating directly with the processor.
    </p></div>
</div>

<div>
    <p><code>nasmjf</code> has 130 code words. Mostly for efficiency.

    </p><div>
        <p>If you weren't already wondering, perhaps you are now:
        How many Forth words need to be defined in machine code
        to have a "bootstrappable" Forth system?
        </p><p>There are some theoretical minimums. But as you get down to an
        absurdly small number of instructions, the Forth code written with the
        primitives (to implement the rest of the language) approaches absurdly
        large amounts of convolutions that test the limits of both programmer
        ergonomics and computational inefficiency.
        </p><p>Check out this amazing article by Frank Sergeant:
        <a href="https://pygmy.utoh.org/3ins4th.html">A 3-INSTRUCTION FORTH FOR EMBEDDED SYSTEMS WORK</a>
        (utoh.org).
        </p><blockquote>"How many instructions does it take to make a Forth for
            target development work? Does memory grow on trees? Does the cost
            of the development system come out of your own pocket? A 3-
            instruction Forth makes Forth affordable for target systems with
            very limited memory. It can be brought up quickly on strange new
            hardware. You don't have to do without Forth because of memory or
            time limitations. It only takes 66 bytes for the Motorola MC68HC11.
            Full source is provided."
        </blockquote>
        <p>You read that right: <strong>66 bytes</strong>.
        </p><p>And later:
        </p><blockquote>"The absolute minimum the target must do, it seems to me,
            is fetch a byte, store a byte, and call a subroutine. Everything
            else can be done in high-level Forth on the host."
        </blockquote>
        <p>Which reminds me, did you know there is such a thing as a
        <a href="https://en.wikipedia.org/wiki/One-instruction_set_computer">one-instruction set computer</a>
        (wikipedia.org)?
        And <em>of course</em> you can run Forth on them:
        <a href="https://github.com/howerj/subleq">16-bit SUBLEQ eForth</a>
        (github.com).
        </p><p>But that's nuts.
        </p><p>How about something a little more realistic?
    </p></div>
</div>

<div>
    <p><code>sectorforth</code> has 10 code words.

    </p><div>
        <p>Cesar Blum's
        <a href="https://github.com/cesarblum/sectorforth">sectorforth</a>
        (github.com)
        is:
        </p><blockquote>"...a 16-bit x86 Forth that fits in a 512-byte
            boot sector. Inspiration to write sectorforth came from a
            1996 Usenet thread."
        </blockquote>
        <p>See? There's Usenet again. It wasn't just me reading all that lore.
        </p><p>The author's
        <a href="https://old.reddit.com/r/Forth/comments/j0lxgq/sectorforth_a_16bit_x86_forth_that_fits_in_a_boot/">posting of the project to the Forth sub-reddit</a>
        (reddit.com)
        has additional insight:
        </p><blockquote> "I've always been fascinated by the idea of having a
            minimal kernel of primitives from which "everything" can be built.
            Before Forth, I had only seen that in the form of Lisp's "Maxwell
            equations of software", which is cool, but always left me a little
            disappointed because it is too abstract to build something that you
            can actually interact with - you can't break out of its esoteric
            nature...
            <p>
            
            With Forth, however, you can start from almost nothing, and start
            adding things like ifs, loops, strings, etc., things that look more
            like your day-to-day programming. I find that there's a lot of
            beauty in that."
        </p></blockquote>
        <p>Note: The statement about Maxwell's equations surely refers to
        Alan Kay's famous quote about LISP from
        <a href="https://queue.acm.org/detail.cfm?id=1039523">A Conversation with Alan Kay</a>
        (acm.org):
        </p><blockquote>
            "Yes, that was the big revelation to me when I was in graduate
            school - when I finally understood that the half page of code on
            the bottom of page 13 of the Lisp 1.5 manual was Lisp in itself.
            These were "Maxwell's Equations of Software!" This is the whole
            world of programming in a few lines that I can put my hand over."
        </blockquote>
        <p>Okay, so we've talked about <b>code words</b>
        that are just chunks of machine code that can be called upon
        at any time.
        </p><p>Now let's see what <b>colon words</b> are all about...
    </p></div>
</div>

<div>
    <h2>Colon words are made of Forth!</h2>
    <p>Let's make one:
    </p><pre>: SDD SWAP DROP DUP ;
    </pre>

    <div>
        <p>A colon word is so-named because its definition begins with the
        "<code>:</code>" character.
        </p><p>The example colon word definition above creates a new word called
        <code>SDD</code> that is a composition of the three code words we
        defined earlier: <code>SWAP</code>, <code>DROP</code>, and
        <code>DUP</code>.
        Perhaps the word "composition" brings to mind the concatenative
        terminology we explored earlier in this quest?
        </p><p>As this example demonstrates, colon words are defined entirely
        by other words, which may be code words or other colon words.
        You can also have numeric values, e.g. 8 and 7, which
        are handled by the interpreter.
        </p><p>(You can also have strings, which looks like data...but those are
        just input that happens to follow one of the special words, e.g.
        <code>."</code> (dot quote), that knows how to handle the input!)
        </p><p>Let's see it in action:
    </p></div>
</div>

<div>
    <pre>8 7      <i>8 7</i>
SDD      <i>7 7</i>
    </pre>

    <div><p>
        The effect of calling our new <code>SDD</code> word is, of course,
        identical to calling the three separate words <code>SWAP</code>,
        <code>DROP</code>, and <code>DUP</code> in sequence.
        </p><p>In <b>indirect threaded code</b> terms,
        this colon word has been "compiled" into the addresses of
        the "inner interpreters" for each of the three code words.
        But feel free to ignore this detail!
        </p><p>Let's demystify this further because the Forth "compiler" is
        probably much, much simpler than you'd think:
    </p></div>

<div>
    <h2>How ":" works</h2>
    <p>Here's what really happens when we enter this:
    </p><pre>: SDD SWAP DROP DUP ;
    </pre>
	<p>Colon (<code>:</code>) fetches the word name (SDD) and sets "compile mode".
	</p><p>Semicolon (<code>;</code>) completes the word's entry in the <b>dictionary</b> and unsets "compile mode".
    
    </p><div>
        <p>It might still be surprising that ":" is a Forth word.
        </p><p>It looks like the sort of thing we would call "syntax" in other
        programming languages, but it really isn't. It's a word.
        </p><p>You can even <em>replace</em> ":" with your own definition
        to extend or alter Forth to do your bidding!
        </p><p>It may be hard to fully grasp for a while, but
        Forth's <em>only</em>
        syntax is the whitespace between tokens of input.
        </p><p><i>Tokens are tokenized by a word called "WORD", which is an
            incredibly confusing overload of the term. Sorry.</i>
        </p><p>(You'll also notice I've mentioned the term "dictionary" a couple
        times now. It's kind of obvious that a dictionary can hold words, but
        I haven't properly explained the Forth dictionary yet. Don't worry,
        we're almost there.)
        </p><p>Okay, so "<code>:</code>" switches the "outer interpreter" into
        compile mode and <code>;</code> switches it back. But what does
        <em>that</em> mean?
    </p></div>
</div>

<div>
    <p><b>"Compiling"</b> in Forth means putting one of two things into memory:
	</p><ul>
		<li>The address of a word, or
		</li><li>A value literal and a bit of code that pushes it on the stack
	</li></ul>
    <p>At its simplest, compiling is just like executing, but we're <em>storing</em>
	addresses instead of jumping to them.

    </p><div>
        <p>Actually, that's understating the elegance and simplicity of how this
        works, which is one of the most mind-blowing things in Forth.
        </p><p>Forth uses the <em>same</em> interpreter to both compile
        and execute code!
        </p><p>In a traditional Forth, the interpreter executes words as you
        enter them. Unless you're in "compile mode", <em>then</em> it is
        compiling those words as addresses into memory <em>on the fly</em>
        as you enter them.
        </p><p>It's straight from the keyboard to memory.
        </p><p>To make this concrete, let's step through the example.
        </p><p>Here's our definition again:
        </p><pre>: SDD SWAP DROP DUP ;
        </pre>
        <p>In "normal mode", the interpreter is executing everything as we enter it.
        </p><p>When the interpreter encouters the "<code>:</code>" word, we're
        still in "normal mode", so it looks "<code>:</code>" up in the
        dictionary, finds it, and executes the word. The definiton of
        "<code>:</code>" will collect the name "SDD" and turn on the "compile
        mode" switch.
        </p><p>Now when the interpreter hits the "<code>SWAP</code>" word, it will
        look up its address in the dictionary as usual, find it, and
        store the address in the next available memory slot where we
        compile new words (a very important built-in variable called
        "<code>HERE</code>" keeps track of this memory position).
        </p><p>The same thing happens for "<code>DROP</code>" and "<code>DUP</code>".
        We're compiling as fast as we can type!
        </p><p>Then a bunch of really interesting things happen when the interpreter
        gets to "<code>;</code>" (SEMICOLON).
        </p><p>First, "<code>;</code>" is looked up and found in the dictionary and
        then...Hey, wait!
        Why isn't the address of the "<code>;</code>" word
        <em>also</em> compiled into our new definition? <strong>That's a
            great question!</strong>
        </p><p>Time for another trick. One of the flags stored in a word's
        dictionary entry is the "immediate" flag. When this flag is turned on,
        the word is always executed immediately
        <em>even in compile mode</em>.
        The "<code>;</code>" word is an immediate word, so it executes instead
        of being compiled.
        </p><p>(Ready to have your head turned inside-out? There are <em>also</em>
        tricks for <em>compiling</em> immediate words into word definitions!
        It's simple enough, but still pretty mind-bending stuff when you first
        encounter it.)
        </p><p>The definition of "<code>;</code>" turns off compile mode. Then it
        does some housekeeping to complete the entry of the new
        <code>SDD</code> word in the dictionary.
        </p><p>As soon as "<code>;</code>" returns control to the outer
        interpreter, we're now sitting in normal mode again and our new
        <code>SDD</code> word is available to be called directly or compiled
        into <em>other</em> words.
        </p><p>See what I mean? It's all made of these tiny little parts.
        </p><p>Each part is incredibly simple, but trying to explain how the
        parts fit together takes paragraphs of text.
        </p><p>Speaking of simple...
    </p></div>
</div>

<div>
    <h2>Almost no syntax = simple interpreter and extreme extensibility</h2>
    <p>The tiny set of rules that govern the interpreter:
    </p><ul>
        <li>WORD gets a token.
        </li><li>Is it in the dictionary? <i>(And are we compiling?)</i>
        </li><li>Is it a numeric literal? <i>(And are we compiling?)</i>
        </li><li>Otherwise, error!
    </li></ul>
    <p>Let's look at our example code again. The first line
    runs, the second line compiles:
    </p><pre>8 7 SWAP DUP DROP

: SDD SWAP DROP DUP ; 8 7 SDD
    </pre>

    <div>
        <p>It would be annoyingly redundant to walk through the two lines of
        Forth above step-by-step because they are nearly identical. The only
        difference is that the first line simply executes each word as it is
        encountered (SWAP, DUP, DROP). The second line compiles those three
        words into a new word called SDD. The result of both lines is the same.
        </p><p>Only the numbers (8 and 7) and the spaces separating words have
        <em>any</em> special meaning to Forth's "outer" interpreter.
        Everything else is looked up in the dictionary.
        </p><p>Ah, but did you notice the order of the bullet points above?
        We check to see if a token is in the dictionary <em>before</em>
        we check to see if it is a numeric literal.  Yes, even numbers are
        looked up in the dictionary first! Does that perhaps give you any ideas
        about that <em>magic trick</em> I promised at the start of this article?
        </p><p>Furthermore, input is not returned to the main Forth "outer"
        interpreter until a dictionary word completes executing. So there is
        absolutely <strong>no limit</strong> to the types of
        <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a>
        (wikipedia.org)
        you can create.
        </p><p>And if that weren't enough, You can also replace every single piece
        of the Forth interpreter itself. Remember, they're all independent little
        cogs in the machine. Forth is the ultimate freedom.
        </p><p>I've alluded to this in several different ways above, but I'll make
        a bold claim:
        <strong>Forth has the simplest syntax and therefore the simplest
        parser, interpreter, and compiler <em>ever</em> used in a "mainstream"
        general-purpose programming language.</strong>
        </p><p>Two other languages previously mentioned, Lisp and Tcl, are also
        famously syntactically minimalistic languages. People have
        written incredibly tiny implementations of each:
        </p><ul>
            <li>Lisp: <a href="https://github.com/jart/sectorlisp">sectorlisp, a 512-byte implementation of LISP</a> (github.com/jart)
            </li><li>Tcp: <a href="http://oldblog.antirez.com/post/picol.html">picol, a Tcl interpreter in 550 lines of C code</a> (antirez.com)
        </li></ul>
        <p>Mind you, both of these people (Justine "jart" Tunney and Salvatore
        "antirez" Sanfilippo) are incredible programmers, but these examples
        hint at what is possible.
        </p><p>But Forth surely takes the cake. Even a certified non-genius
        like myself can write an entire Forth interpreter in a
        couple hundred assembly instructions. (See "Meow5" below.)
        </p><p>Because of its extreme simplicity, tokenizing Forth can be done in
        a mere handful of assembly instructions on many processors.
        </p><p>And as mentioned, once you've written a Forth interpreter, you're
        well on your way to a working Forth compiler.
        </p><p>I've alluded to Forth's flexibility and extensibility on several
        different occasions now. But this is no mere party trick. Forth
        relies on the fact that you can do anything in Forth.
        </p><p>In the next example, we'll see how Forth implements control structures.
    </p></div>
</div>

<div>
    <p>The definition of <b>IF...THEN</b> from jonesforth.f:
    </p><pre>: IF IMMEDIATE ' 0BRANCH , HERE @ 0 , ;

: THEN IMMEDIATE DUP HERE @ SWAP - SWAP ! ;
    </pre>

    <div>
        <p>This right here is one of the most mind-blowing things about Forth,
        and a solid reason to title this, "The programming language that writes
        itself."
        </p><p>Even something as fundamental as <code>IF</code> is defined in
        the language! Forth is not the only language that can do this, but
        few languages invite the programmer to participate so thoroughly
        in the inner workings as often or as joyfully as Forth.
        </p><p>Figuring out how the IF and THEN definitions above actually
        work is left as an exercise for the reader, but here's a brief
        explanation of the new words they use:
        </p><pre><b>'</b>       - gets the address of the word that follows, put on stack
<b>0BRANCH</b> - branch to the next value if the top of the stack has 0
<b>,</b>       - 'compile' the current stack value to the memory at HERE
<b>@</b>       - fetch value from address on stack, put value on stack
<b>!</b>       - store to memory (stack contains address, then value)
        </pre>
        <p>(By the way, I'll go on the record to say this: The
        early parts of bootstrapping Forth in Forth (at least
        the top 25% of jonesforth.f) is <em>significantly</em> more
        mind-bending than implementing the low-level code word definitions
        written in assembly language. In fact, any time I needed to return to
        the assembly, it was like a comforting blanket of simplicity compared
        to the logic puzzle of those Forth-in-Forth primitives!)
        </p><p>But, even seeing control structures like <code>IF..THEN</code>
        implemented in the language may not have prepared you for seeing this
        next trick.
        </p><p>This should drive home the fact that Forth has almost no native
        syntax:
    </p></div>
</div>

<div>
    <p>The definition of <b>( )</b> <i>nested comments</i> from jonesforth.f:
    </p><pre>: ( IMMEDIATE
    1
    BEGIN
        KEY DUP '(' = IF DROP 1+
        ELSE ')' = IF 1- THEN
        THEN
    DUP 0= UNTIL
    DROP
;

(
    From now on we can use ( ... ) for comments.
...
    </pre>

    <div>
        <p>Yeah, you read that right. Even <em>comments</em> are implemented
        in the language! And you can re-define them or add your own kind of
        comments!
        </p><p>Some of you are soiling yourselves in excitement right now.
        Some of you are soiling yourselves in fear.
        We're all just sitting here in our own filth now.
        </p><p>And now, at last, we are ready to discuss the power of the Forth
        dictionary.
    </p></div>
</div>

<div>
    <h2>The Dictionary</h2>
	<p>A Forth dictionary traditionally uses a linked list.
    </p><p>Word matching is done starting from the <em>end</em>
    (most recent entries) first, so:
	</p><ul>
		<li>You can redefine <i>any</i> word, even the ones originally
			defined in assembly!
		</li><li>Words depending on previous definitions of redefined words
            won't break because the compiled addresses still point to
            the original word, not the new definition!
		</li><li><i>You</i> are in complete control!
		</li><li>Again, Forth = freedom!
	</li></ul>

    <div>
        <p>It's not <em>just</em> minimalistic syntax. Arguably, the
        <em>real</em> reason Forth is so extensible is because of
        the dictionary.
        </p><p>As mentioned in the points above, more recent word definitions
        override older ones with the same name - the interpreter stops at the
        first match.
        </p><p>But as mentioned above, existing compiled words that use the
        <strong>old definitions</strong> are not affected because 
        <em>name</em> of the old word, they've stored the <em>address</em>.
        The address of the old word still points to the old word.
        </p><p>You don't have to strictly replace. You can <em>extend</em>
        words by calling the original word from a new one with the same name!
        </p><p>You are perhaps wondering what happens if
        you attempt to make a <strong>recursive word</strong>. By
        default, ':' (COLON) marks the word currently being compiled into the
        dictionary as hidden or disabled so that previous definitions can be
        called, as mentioned.
        This is why we have a word called RECURSE which allows inserts a
        call to the current word within itself. Because all information
        in Forth is global (including the address of the current word being
        compiled, defining RECURSE is incredibly simple (just four words in the
        JonesForth definition).
        </p><p>Besides making new control structures or other types of extensions
        to the language, what else can we do with these abilities?
    </p></div>
</div>

<div>
	<p><img src="http://ratfactor.com/forth/talkimg/apple_bw.png" alt="grayscale apple"></p><p>It's not just the language itself that is unusually malleable.
    <strong>Your program written in Forth can be flexible too.</strong>
	</p><p>Here is an example lifted and paraphrased from <i>Thinking Forth</i>
    by Leo Brodie.
    </p><p>Say we create a variable to hold a number of apples:
	</p><pre>VARIABLE APPLES
20 APPLES !
APPLES ? <i>20</i>
	</pre>
	<p>Forth variables put <i>addresses</i> on the stack.

	</p><div>
		<p>Note: I have a physical copy of <i>Thinking Forth</i> because
        I think it's great. But the publishers have kindly made it available
        for free online:
		<a href="https://www.forth.com/wp-content/uploads/2018/11/thinking-forth-color.pdf">Thinking Forth (PDF)</a>
        (forth.com)
        </p><p>Let's walk through the three lines above. Here's the first line:
        </p><pre>VARIABLE APPLES
        </pre>
        <p>The VARIABLE word creates a new spot in free memory. <em>Then</em>
        it creates a new word in the dictionary called APPLES that pushes that
        particular memory address on the stack when it is called.
        </p><p>(Note that like ":", "VARIABLE" is grabbing the next token of input
        for use as a new dictionary name. This is possible because "the little
        cogs in the Forth machine" are available for any use you can think of.
        And one of those cogs is the word WORD, which gets the next token from
        the input stream. Both ":" and "VARIABLE" use WORD to do this, just like
        Forth's own outer interpreter!)
        </p><p>Okay, so we have a variable named APPLES now. The next line is:
        </p><pre>20 APPLES !
        </pre>
        <p>This puts the value 20 on the stack, then the address for APPLES.
        The "!" (STORE) word stores the value 20 at the APPLES address.
        (In other words, "!" takes <em>two</em> values as input: an address and
        a value.  It stores the value at that address.)
        </p><p>Conceptually, you can think of the above as <code>APPLES = 20</code>
        in "normal" programming syntax.
        </p><p>And now the third line:
        </p><pre>APPLES ?
        </pre>
        <p>This line prints the value stored at APPLES. The word "?" fetches a
        numeric value from an address and prints it (which pops the value off
        the stack again).  Again, APPLES puts its address on the stack. So "?"
        simply takes an address from the stack as input for printing.
        </p><p>By the way, here's the entire definition of "?" in JonesForth:
        </p><pre>: ? @ . ;</pre>
        <p>Look at how small that is! The only thing you need to know to
        understand this definition is that "@" (FETCH) pops an address from the
        stack and fetches the value stored at that address and puts the value
        on the stack.  "." (DOT) pops a value from the stack and prints it as a
        number.
        </p><p>Okay, on with our example.
        </p><p>We're about to be dealt a terrible blow...
	</p></div>
</div>

<div>
	<p>We pepper our program with this <b>APPLES</b> variable.
    </p><p>The application works perfectly for a couple years.
	</p><p>Then we are told that we must now keep track of two different
		kinds of apples: red and green. What to do?
	</p><p><img src="http://ratfactor.com/forth/talkimg/apple_red.png" alt="red apple">
		<img src="http://ratfactor.com/forth/talkimg/apple_green.png" alt="green apple">
	</p>

    <div>
        <p>Unfortunately, this is exactly the sort of conundrum we see in real
        life software all the time.
        </p><p>You knowingly prepared for all sorts of different <em>quantities</em>
        of apples, but it never occurred to anyone that we would need to
        track different <em>types</em> of apples.
        </p><p>This problem seems very bad. Do we have to completely re-write our
        application?
        </p><p>(Well, <em>outside</em> of this example, the correct answer might be
        "yes".  Maybe this changes the whole "theory" of the program, in the
        <a href="http://ratfactor.com/papers/naur1">Programming as Theory Building</a>
        (ratfactor.com)
        sense. In which case, a re-write or big refactor of our apple counting
        program is likely the right answer. But for this example, we're
        assuming that we have <strong>thousands of lines</strong> of
        apple-handling functionality that will <strong>not</strong> need to
        change. We'll say that grouping the apples by color here is just an
        essential surface detail.)
        </p><p>All right, <em>obviously</em> we can't store two values in one
        variable and expect all of the existing code to still work. So what
        could we possibly do?
        </p><p>Here's a very clever and very Forth solution:
    </p></div>
</div>

<div>
    <p>A new variable will store the current type of apples.
	</p><pre>VARIABLE COLOR
	</pre>

    <div>
        <p>As with "APPLES" above, VARIABLE creates a memory space and a new
        word called "COLOR" that puts the address of the memory space on the
        stack when it is called.
        </p><p>Next, we'll create a second new variable <em>and</em> a new colon word.
    </p></div>
</div>

<div>
	<p><img src="http://ratfactor.com/forth/talkimg/apple_red.png" alt="red apple"></p><p>"REDS" will count red apples. 
    Colon word "RED" sets the current type of apple to red:
    COLOR = REDS:
	</p><pre>VARIABLE REDS
: RED REDS COLOR ! ;
	</pre>

    <div>
        <p>Remember, variables are also words in the dictionary, so we've
        created three additional words so far: COLOR, REDS, and RED.
        </p><p>(Only one of these, RED, is <em>recognizably</em> a function.
        But really all three of them are.)
        </p><p>As you may recall from earlier, "!" (STORE) takes two parameters,
        a value and an address, and stores the value at that address.
        </p><ul>
            <li>COLOR is the address of memory holding the address of the current apple count variable
            </li><li>REDS is the address of memory holding the red apple count
            </li><li>RED sets COLOR to the address of REDS
        </li></ul><p>
        It might be helpful to see the C equivalent of the RED word:
        </p><pre>void RED(){
    COLOR = &amp;REDS
}
        </pre>
        <p>And then...
    </p></div>
</div>

<div>
	<p><img src="http://ratfactor.com/forth/talkimg/apple_green.png" alt="green apple"></p><p>Same for green.
	</p><pre>VARIABLE GREENS
: GREEN GREENS COLOR ! ;
	</pre>

    <div>
        <p>We've added a total of five new words. The two new green ones
        are identical to the red ones above:
        </p><ul>
            <li>GREENS is the address of memory holding the green apple count
            </li><li>GREEN sets COLOR to the address of GREENS
        </li></ul>
        <p>Here's the C equivalent of GREEN:
        </p><pre>void GREEN(){
    COLOR = &amp;GREENS
}
        </pre>
        <p>One more change...
    </p></div>
</div>

<div>
	<p>Lastly, we change <b>"APPLES"</b> from a variable to a word that gets
    the current count by color:
	</p><pre>: APPLES COLOR @ ;
	</pre>

    <div>
        <p>As you may recall from earlier, "@" (FETCH) fetches the value
        stored in a variable and puts it on the stack.
        </p><p>So "APPLES" gets the value stored in COLOR and puts that
        on the stack.
        </p><p>The value stored in COLOR <em>happens to be an address</em>.
        That address happens to be the memory pointed to by either REDS or
        GREENS.
        </p><p>It would look like this C code:
        </p><pre>int *APPLES(){
    return COLOR;
}
        </pre>
        <p>This "get the address of the address" stuff may sound super
        confusing. But working with memory addresses (aka "pointers") is
        <em>how variables work</em> in Forth, so to the adept Forth programmer,
        the idea of passing addresses around will be deeply ingrained and
        <em>no big deal</em>.
        </p><p>Okay, so we've got red and green apple counts. That much
        is clear. But surely there is still a lot of work ahead of us...
    </p></div>
</div>

<div>
    <p>Now we have to re-write any use of <b>APPLES</b>, right?
    </p><p>Wrong! The use of <b>APPLES</b> is <i>identical</i>. The syntax hasn't
        changed one bit for any existing code. We just need to make sure we've
        set the right color.
    </p><p>Check it out:
	</p><pre>20 RED APPLES !
30 GREEN APPLES !

GREEN APPLES ? <i>30</i>
APPLES ? <i>30</i>

RED
APPLES ? <i>20</i>
	</pre>
	<p><img src="http://ratfactor.com/forth/talkimg/apple_bw.png" alt="grayscale apple">
		<img src="http://ratfactor.com/forth/talkimg/apple_red.png" alt="red apple">
		<img src="http://ratfactor.com/forth/talkimg/apple_green.png" alt="green apple">
	</p>

    <div>
        <p>All of the existing code that uses APPLES will still work 
        <em>exactly the same way</em> with absolutely no modifications.
        </p><p>Furthermore, look at how English-like it reads to store
        <code>"20 RED APPLES !"</code> or query <code>"GREEN APPLES ?"</code>.
        </p><p>The key to understanding why this works is to remember that
        "APPLES" was <em>already</em> a word that put an address on the stack
        because <em>that's how variables work</em> in Forth.
        So when we changed it to a colon word that puts an address on the
        stack, it's no change at all. It's still doing the exact same thing.
        It just happens that the address will change depending on the active
        apple color.
        </p><p>At every single opportunity, Forth has taken the simplest
        (you might even say, <em>laziest</em>) and most flexible method
        for implementing a feature.
        </p><p>Wait, I hear a distant screaming:
        </p><p><em>"How could this possibly be okay?! You call this 'freedom', but
            I call it unchecked chaos material! This is not okay!"</em>
        </p><p>Well, maybe.
        </p><p>But I think one reason this actually <em>is</em> okay, on a
        conceptual level, is that APPLES did <em>not</em> really change
        what it originally did.
        </p><p>Coming from the normal programming language world, we have clearly
        <strong>broken the abstraction</strong>:
        "APPLES" was a variable before, and now it's a function.
        </p><p>But you're not in the normal programming world anymore.
        Here, in Forth-land, a variable <em>is</em> a word that puts an
        address on the stack. And a function is <em>also</em> just a word.
        </p><p>"APPLES" is <em>still</em> a word that puts an
        address on the stack. There is no <em>conceptual</em> change at the
        language level. <strong>We did not break an abstraction because there
        was no abstraction to break.</strong>
        </p><p>Forth provides what you might call "atomic units of computing"
        at the language level.  It is a language where <em>you</em> make the
        abstractions.
    </p></div>
</div>

<div>
    <center>
    <img src="http://ratfactor.com/forth/talkimg/apple_bw.png" alt="grayscale apple">
    </center>
    <p>To Forth, it's all just words in a dictionary.
    "VARIABLE" is just another word
    <em>you could have written yourself</em>.

    </p><div>
        <p>Do you see now why Chuck Moore rejects the standardization
        of Forth? It ossifies concepts like VARIABLE so they lose their
        flexibility.
        </p><p>The example above is also another demonstration of the way
        the language Forth "writes itself": a tiny handful of primitives can be
        used to bootstrap the rest of the language in the language itself.  The
        enormous flexibility of the primitives allows nearly unbounded freedom.
    </p></div>
</div>

<div>
    <h2>Implement Forth to understand it</h2>
    <p>I highly recommend implementing Forth (or porting it like I did) to understand
    how it works.

    </p><div>
        <p>By examining Forth from the ground floor at the assembly language level,
        I gained considerable confidence in my understanding of how all the moving
        parts fit together.
        </p><p>To be honest, it's difficult for me to imagine being to able to understand all the
        individual parts <em>without</em> going through this process. But everybody learns
        differently.
    </p></div>
</div>

<div>
    <h2>Or invent Forth for yourself</h2>
    <p>"I didn't create Forth, I discovered it."
    </p><p>-- Chuck, apocryphally

    </p><div>
        <p><em>(I have been unable to find a source for the quote above.
        It probably comes from an interview.)</em>
        </p><p>If Forth truly is a fundamental way to express computation, then
        it's sort of like 
        G√∂del and Herbrand's general recursive functions, Church's lambda
        calculus, Turing's theoretical machines, Post's canonical systems, and
        Sch√∂nfinkel and Curry's combinators.
        (I can hear furious objections warming up from a thousand armchairs...)
        </p><p>In fact, that's true of <em>all</em> programming languages, even the
        big, messy ones. Right? Any language that can express universal
        computation is...universally powerful; it can express anything
        that is computable.
        </p><p>But I think Forth belongs to a more rarified group.  Forth is a
        fundamental <em>type</em> of programming language design.
        And I'm not alone in thinking so. For example, check out
        <a href="https://madhadron.com/programming/seven_ur_languages.html">The seven programming ur-languages</a>
        (madhadron.com).
        </p><p>I'll let philosophers angrily split hairs over what I just said above,
        but I think the principle is true. And it's true all the way down
        to the (lack of) syntax in the language.
        </p><p>Why do I believe this? Well...
    </p></div>
</div>

<div>
    <p>Making <code>nasmjf</code> gave me so many ideas, I <i>had</i> to try some
        experiments.
    </p><p>Forth is an amazing playground for ideas.

    </p><div>
        <p>I was still keenly aware that my <code>nasmjf</code> project to
        port JonesForth to NASM was still just a (very detailed) examination of
        a <strong>final artifact</strong>. I was not re-tracing Moore's footsteps, but
        imitating his work. In fine art terms, I made a "master copy" (training myself by
        copying the work of a master artist). In other words, I brought
        my sketchbook to the museum.
        </p><p>But what would happen if I tried making a painting of my very own?
    </p></div>
</div>

<p><img src="http://ratfactor.com/forth/talkimg/assembly-nights2.jpg" alt="my lenovo 11e thinkpad with assembly code waiting romantically on the bed with a candle. text reads 'Assembly Nights II'">
</p>

<div>
    <h2>Meow5</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/meow5.png" alt="meow5 cat logo"></p><p>An exercise in extreme <b>concatenative</b> programming where
		all code is concatenated (always inlined).

    </p><div>
        <p>We explored what it means to be a "concatenative" programming language
        at the beginning of my journey above. In short, in a concatenative
        language, data implicitly flows from one function to another like a
        factory assembly line.
        </p><p>Like Forth, Meow5 happens to be concatenative because it uses
        the same "parameter stack" concept.
        </p><p>Unlike Forth or most other sane languages, <strong>Meow5 is a thought
        experiment taken too far</strong>. Specifically, the thought,
        "instead of threading function calls by storing their addresses, what
        if we just store a copy of the whole function?
        </p><p>In compiler parlance, this is "inlining", short for
        <a href="https://en.wikipedia.org/wiki/Inline_expansion">inline expansion</a>
        (wikipedia.org).
        It is a common optimization technique
        for avoiding the overhead of a function call for small functions.
        </p><p>Let's use the word DUP for example.  Remember when we looked at the
        assembly language source of the DUP code word? It was just a single
        assembly instruction:
        </p><pre>pop eax
        </pre>
        <p>It would be incredibly silly to have several jumps to and from
        a single-instruction word!
        </p><p>(And, it comes as no surprise that
        "real" Forth implementations often inline small primitives such
        as DUP. Some even provide an INLINE word to allow the programmer
        to specify this explicitly.)
        </p><p>My question was: What if we do that for everything?
        At what point is this no longer a good idea?
        Obviously at <em>some</em> point, a function is too large to inline.
        But every code word in JonesForth 
        was quite tiny by modern standards. With today's CPUs and their
        relatively enormous caches it seemed to me that you could take
        this inlining concept pretty far before it got ridiculous.
        </p><p>And wouldn't the CPU just love seeing all of those instructions
        executing in one straight and continuous sequence with no jumps?
        If I were a CPU, I would love it.
        </p><p>Plus, it would make compiling a stand-alone executable almost
        trivial because <em>every word</em> in a 100% inlined language
        would contain <em>all</em> of the machine code needed for that
        word.
        </p><p>Here is the canonical example:
    </p></div>
</div>

<div>
	<pre>: meow "Meow." print ;
meow
<i>Meow.</i>

: meow5 meow meow meow meow meow ;
meow5
<i>Meow.Meow.Meow.Meow.Meow.</i>
	</pre>

    <div>
        <p>The idea is that <code>meow5</code> compiles into five complete
        copies of <code>meow</code>!
        </p><p>This example seems to be obviously naughty and wasteful. But I'm
        not a superscalar, out-of-order executing modern processor and neither
        are you. So the question remains: At what point does having a
        child function which includes a complete copy of every parent and
        grandparent and every ancestor function all the way back to the
        beginning spiral out of all sane proportions?  Well, you could spend an
        afternoon figuring it out on paper, or you could be like me and spend
        the better part of a year writing an assembly program.
        </p><p><i><b>Spoiler alert:</b> I consider Meow5 to be a
        delightful little failure. The problem isn't inlining machine code -
        that works great, and, indeed, the exported ELF executables from Meow5
        work exactly as I imagined. The problem is <b>data</b>, and most
        conspicuously, data in the form of strings.  Let's take the
        <code>meow</code> word for example: You either have to copy the string
        "Meow." five times, once for each word that uses it, <b>or</b> go
        through some complicated hoops to track which word uses the string. And
        you have to do that two different ways: Its location in memory in the
        live interpreter and in it's destination in the stand-alone ELF memory
        layout. Either way, the purity and simplicity is lost, which was the
        whole point of the experiment. Also, it will come as no surprise that I
        later discovered that Forth implementations often have an INLINE word
        (as I mentioned above), which is a much better way to selectively
        instruct the compiler about which words you wish to copy entirely.  As a
        program, Meow5 is a failure. But as a project, it is a success
        because I learned a lot.</i>
        </p><p>Think of it as an art project.
        </p><p>Anyway, the <em>point</em> is...
    </p></div>
</div>

<div>
    <p>Despite attempting to go my own way,
        it's remarkable how many times Forth's solution was the
        path of least resistance.
    </p><p>Again and again I would say, "Aha! <i>That's</i> why."

    </p><div>
        <p>First of all, you'll notice I ended up using ":" and ";" to
        define new functions.
        Forth makes liberal use of symbols and abbreviations, which
        can make it pretty hard to read. But I have to admit, ": ... ;"
        has grown on me. So I adopted that in Meow5. So that's probably
        the most visible thing. But that's just on the surface.
        </p><p>Secondly, using a postfix notation is <em>absolutely</em> the path
        of least resistance for a stack-based language - everything comes in
        the order expected by the language.  So your interpreter can be
        shockingly simple because it can execute statements in the exact order
        it gets them.
        </p><p>(Side note: This is also how the
        <a href="https://en.wikipedia.org/wiki/PostScript">PostScript</a>
        (wikipedia.org)
        printer and display language works. The printer can begin printing as
        soon as it recieves the document because everything is defined in the
        order it is needed and never depends on later information. This can
        also be a <em>disadvantage</em> of PostScript for viewing documents on
        screens: You can't just render a page mid-document because
        styling and formatting controls must be read in their entirety from the
        start of the document to the current page in order to ensure you've
        got everything!)
        </p><p>I was determined to make things easy for myself,
        so I can say with some certainty that Forth is one of the
        most "minimum effort" languages you can imagine.
        If I could have thought of an easier (or lazier) way to do something, 
        I would have done it!
        </p><p>There was just <strong>one place</strong> I decided to deviate
        from Forth even though I knew it would make implementation harder.
    </p></div>
</div>

<div>
    <p>To make a string in Forth, you use the word <code>"</code>, which
    needs a space after it to be seen as a word, which looks awkward:
    </p><pre>" Hello World."
    </pre>

    <div>
        <p>This has always bothered me. Chuck Moore even admits this in
        his unpublished book, 
        <a href="http://forth.org/POL.pdf">Programming A Problem-Oriented Language (PDF)</a>
        (forth.org)
        in the section titled <i>6.3 Character strings</i>:
        </p><blockquote>
            "What does a character string look like? Of all the ways you might
            choose, one is completely natural:
            <pre>    "ABCDEF...XYZ"
            </pre>
            A character string is enclosed in quotes. It can contain any character
            except a quote, specifically including spaces."
        </blockquote>
        <p>Right! So by golly, that's what I would do in Meow5, like
        every sensible language!
    </p></div>
</div>


<div>
    <p>Meow5 has this more natural quoting style:
    </p><pre>"Hello World."
    </pre>
    <p>But the effects are cascading. And they limit flexibility.

    </p><div>
        <p>If we keep reading Chuck's words, he explains what will happen
        if you do this:
        </p><blockquote>
            "We get in trouble immediately! How do you recognize a character
            string? By the leading quote, of course. But do you modify your word
            subroutine to recognize that quote? If you do so you may never use a
            leading quote for any other purpose. Much better that the quote is a
            word by itself, treated like any other dictionary entry, for it can then
            be re-defined. But words are terminated by spaces, and I still resist
            making quote an exception. So let's type character strings:
            <pre>    " ABCDEF . . . XYZ"
            </pre>
        </blockquote>
        <p>And he was right, of course.
        </p><p>I ended up having to put exceptions for the <code>"</code> character in
        multiple places in the Meow5 interpreter, including my
        <code>get_token</code> function, which serves the same purpose as
        the "WORD subroutine" Moore mentioned above.
        </p><p>And now <em>all</em> additional interpreter features have to work
        around or duplicate the special <code>"</code> character handling!
        </p><p>It seems one can either follow Moore's advice or re-discover
        it for oneself. As for me, I always enjoy re-discovering things for
        myself. The best part is that "aha!" moment when I realize why
        things are the way they are.
        </p><p>Though, to flip this whole thing on its head, I actually think it
        <em>was</em> worth the extra effort, trouble, and loss of purity to do
        this!  (I also included escape sequences, e.g. <code>\n</code> and
        <code>\"</code>, while I was at it.)
    </p></div>
</div>

<div>
    <p>Another example of straying from Moore's advice
    and having to discover it for myself:
    </p><p>I decided to have some of my functions leave the stack alone after using
    the top value.

    </p><div>
        <p>Some functions are mostly used to examine a value, but they pop
        that value off the stack. To keep working with the value, you have
        to do a DUP to duplicate it first.
        </p><p>Since I was sure I would always want to keep the value after these
        particular functions, it seemed very wasteful to have to do a DUP each
        time. Why not just peek at it and leave it on the stack?
        </p><p>Moore recommends just popping everything so you
        don't have to remember.
        </p><p>But I thought that was silly. So I went ahead and made some functions
        that just peek at the value and leave it on the stack.
        </p><p>But as you may have guessed, he was absolutely right.
        </p><p>Having some words pop the stack and some words peek was a nightmare.
        I kept forgetting which words did or didn't alter the stack and it
        kept causing problems. <strong>I completely regretted it and ended up
        making them all pop like Moore advised.</strong>
        </p><p>(Another option that occurred to me after I changed them all would
        have been to have a special naming scheme for non-popping words, which
        probably would have been fine, expect then I would have had to remember
        the name... so hassle either way.)
    </p></div>
</div>

<div>
    <p>Now we have <em>yet another</em> reason for the title of this
    article.
    </p><p>Once you start down the Forth path... the rest just sort of
    "writes itself".
    Chuck Moore already found the path of least resistance.
    </p><p>To sum up the ways in which "Forth writes itself" so far, we have:
    </p><ul>
        <li>Forth is boostrapping
        </li><li>Forth is metaprogramming
        </li><li>Forth can be your OS and your IDE/editor
        </li><li>Forth is the path of least resistance for writing a Forth
    </li></ul>

    <div>
        <p>If you set out to make the <em>simplest possible</em> interpreter
        for a brand new CPU architecture, <strong>you might end up writing
            a Forth whether you want to or not.</strong>
        </p><p>Forth lets you define <em>more Forth</em> in Forth so you
        can Forth while you Forth. And the Forth editor <em>is</em> Forth
        and can be extended with Forth, so can Forth Forth in Forth Forth Forth
        Forth. (I'll let you figure out which of those are nouns, adjectives,
        or verbs and whether or not I have the right number of them.)
        </p><p>And if that weren't enough, Forths often contain <em>assemblers</em>
        so you can define additional code words in Forth, too so you never
        need to leave Forth once you're in it.
        </p><p>JonesForth has the stub of an in-Forth assembler near the end so we
        can see how one might work. Here's the comment introducing it:
        </p><pre>(
    ASSEMBLER CODE --------------------------------------------

    This is just the outline of a simple assembler, allowing
    you to write FORTH primitives in assembly language.

    Assembly primitives begin ': NAME' in the normal way,
    but are ended with ;CODE.  ;CODE updates the header so that
    the codeword isn't DOCOL, but points instead to the
    assembled code (in the DFA part of the word).

    We provide a convenience macro NEXT (you guessed what it
    does).  However you don't need to use it because ;CODE will
    put a NEXT at the end of your word.

    The rest consists of some immediate words which expand
    into machine code appended to the definition of the word.
    Only a very tiny part of the i386 assembly space is covered,
    just enough to write a few assembler primitives below.
)
        </pre>
        <p>Just try not to go insane from the <strong>unlimited power</strong>.
        </p><p>And then there's this:
    </p></div>
</div>

<div>
    <h2>PlanckForth</h2>
    <p>Hand-written 1Kb binary
    <img src="http://ratfactor.com/forth/talkimg/planckforth.jpg" alt="binary layout of planckforth as taken from the repo">

    </p><div>
        <p>This image comes from the
        <a href="https://github.com/nineties/planckforth">PlankForth repo</a>
        (github.com).
        It's one of the most
        beautiful pieces of code I've ever seen. It's a complete ELF binary
        with a working Forth implementation that fits in less than 1Kb.
        As you can see, there's enough room left over for a description and
        copyright at the end.
        </p><p>The binary is stored as an ASCII hex represention that can be turned
        into a working binary using <code>xxd -r -c 8</code>.
        </p><p>But the best part is <code>bootstrap.fs</code>, written in
        line-noise-like operators and gradually becoming readable Forth
        after a couple hundred lines.
        </p><p>Thankfully, comments are one of the very first things implemented
        and it's almost like seeing bacteria spell out words in a petri dish:
        </p><pre>h@l@h@!h@C+h!k1k0-h@$k:k0-h@k1k0-+$h@C+h!ih@!h@C+h!kefh@!h@C+h!l!
h@l@h@!h@C+h!k1k0-h@$k h@k1k0-+$h@C+h!ih@!h@C+h!kefh@!h@C+h!l!

h@l@ h@!h@C+h! k1k0-h@$ k\h@k1k0-+$ h@C+h!
    i       h@!h@C+h!
    kkf     h@!h@C+h!
    kLf     h@!h@C+h!
    k:k0-   h@!h@C+h!
    k=f     h@!h@C+h!
    kJf     h@!h@C+h!
    k0k5-C* h@!h@C+h!
    kef     h@!h@C+h!
l!

\ **Now we can use single-line comments!**

\ planckforth -
\ Copyright (C) 2021 nineties
...
        </pre>
        <p>Incredible.
        </p><p>Another hand-written machine code Forth (in 1,000 bytes and with
        a Forth system in 1,000 lines!) is 
        <a href="https://dacvs.neocities.org/SF/">SmithForth</a>
        (neocities.org)
        by David Smith.
        You can see and hear Smith walk through SmithForth on YouTube:
        <a href="https://www.youtube.com/watch?v=9MSJGzYELBA">SmithForth workings</a>
        (youtube.com).
        </p><p>And as you may recall from earlier, Cesar Blum's
        <a href="https://github.com/cesarblum/sectorforth">sectorforth</a>
        (github.com)
        is a mere 512 bytes!
        </p><p>There are almost as many Forth implementations as there are
        stars in the night sky.
    </p></div>
</div>

<div>
    <p>Forth is an <b>idea</b> that has taken form in countless applications.
    </p><p>Many Forths are custom and home-grown.
    </p><p>But it has had great success in a huge variety of roles:
	</p><ul>
		<li>Power plants, robotics, missile tracking systems, industrial automation.
		</li><li>Embedded language in video games.
        </li><li>Databases, accounting, word processors, graphics, and computation
            systems. (You might say, "legacy software." But I say, "Elegant
            weapons for a more civilized age," to paraphrase a certain wise
            Jedi.)
		</li><li>In the modern Open Firmware boot loader.
		</li><li>Processors of all shapes and sizes.
		</li><li>Microcontrollers of all shapes and sizes.
	</li></ul>

	<div>
        <p>If it goes "beep" and "boop", someone has written a Forth for it!
        </p><p>For some notable uses, here are some starting points:
        </p><ul>
            <li><a href="https://www.forth.com/resources/forth-apps/">Featured Forth Applications</a> (forth.com)
		    </li><li><a href="http://www.forth.org/successes.html">Forth Success Stories</a> (forth.org)
            </li><li><a href="https://en.wikipedia.org/wiki/Forth_(programming_language)">Forth</a> (wikipedia.org)
        </li></ul>
        <p>I think
        <a href="https://en.wikipedia.org/wiki/Open_Firmware">Open Firmware</a>
        (wikipedia.org)
        is particularly interesting. It came, like many things, from
        the fine engineering minds at Sun Microsystems.
        </p><blockquote>
            "Being based upon an interactive programming language, Open
            Firmware can be used to efficiently test and bring up new hardware.
            It allows drivers to be written and tested interactively."
        </blockquote>
        <p>Perhaps one of the most exciting uses of Open Firmware was the
        <strong>Space Shuttle</strong>
        ESN, which ran on a radiation-hardened
        <a href="https://www.cpushack.com/2019/03/01/cpu-of-the-day-utmc-ut69r000-the-risc-with-a-trick/">UT69R000</a>
        (cpushack.com)
        processor!
        A paper on the ESN,
        <a href="https://zenodo.org/record/1267048/files/article.pdf">Developing plug-and-play spacecraft systems: NASA Goddard Space Flight Center's (GSFC) Essential Services Node (ESN) (PDF)</a> 
        (zenodo.org)
        notes that:
        </p><blockquote>
            "Open Firmware can debug hardware,software, plug-in drivers, and
            even the firmware itself. Open Firmware provides interactive tools
            for debugging systems."
        </blockquote>
        <p>By the way, I hope this brief mention of space technology has wet
        your appetite for more, because we're almost there!
        </p><p>But first, I have a couple more drawings of cool computers you
        should see. Perhaps you are aware of the huge variety of 1980s home
        computers?
        </p><p>Check these out:
	</p></div>
</div>

<div>
    <h2>Jupiter Ace, 1982</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/jupiter_ace.png" alt="drawing of the jupiter ace home computer"></p><p>Operating system: Forth.
    </p><p>OS and library of routines in 8 KB of ROM.
    </p><p>The onboard Forth was "Ten times faster than [interpreted] BASIC" and
    less than half the memory requirements."

    </p><div>
        <p>(The quote above is from Popular Computing Weekly, 1982.)
        </p><p>The
        <a href="https://en.wikipedia.org/wiki/Jupiter_Ace">Jupiter Ace</a>
        (wikipedia.org)
        was a British home computer of the early 1980s.
        </p><p>It has a fan-made website, the Jupiter ACE Archive from which
        has the page,
        <a href="https://www.jupiter-ace.co.uk/whatisanace.html">What is a Jupiter ACE?</a>
        (jupiter-ace.co.uk):
        </p><blockquote>
            "The major difference from the 'introductory computer' that was the
            ZX81, however, was that the Jupiter ACE's designers, from the
            outset, intended the machine to be for programmers: the machine
            came with Forth as its default programming language."
        </blockquote>
        <p>That website has tons of resources. And if you're into that sort of
        thing, you also owe it to yourself to visit the "What is..." page
        linked above and then hover your mouse over the image of the ACE's
        circuit board. Every single IC, capacitor, and resistor is identified
        and explained in little tooltips!
        </p><p>It's not every day you see a <strong>programming language listed as
            an operating system</strong> for a computer. But you may recall
        that as early as the "IBM 1130 minicomputer at a big textiles
        manufacturer" era, Moore already had an editor and file management
        features. And you can certainly write hardware drivers in Forth if you
        have the right code word primitives. And as we'll see soon, there
        is absolutely <em>no limit</em> to how low-level Forth can go.
        </p><p><em>(There's also no limit to how high-level Forth can go. The book
        </em>Thinking Forth<em> by Leo Brodie, the same book from which
        we got the apples example above, is full of examples of applications
        written in very "English like" high-level words.)</em>
        </p><p>The ACE never sold very many units, but it is prized by collectors
        today. I would take one.
        </p><p>The
        <a href="https://www.jupiter-ace.co.uk/whatisforth.html">What is Forth?</a>
        (jupiter-ace.co.uk)
        page has an excellent explanation of Forth in general, but especially
        as an all-encompassing computing system:
        </p><blockquote>
            "Classic Forth systems use no operating system. Instead of storing
            code in files, they store it as source-code in disk blocks written
            to physical disk addresses. This is more convenient than it sounds,
            because the numbers come to be familiar. Also, Forth programmers
            come to be intimately familiar with their disks' data structures,
            just by editing the disk. Forth systems use a single word "BLOCK"
            to translate the number of a 1K block of disk space into the
            address of a buffer containing the data. The Forth system
            automatically manages the buffers."
        </blockquote>
        <p>Many of us fondly remember the boot-to-BASIC computers of the 1980s,
        but can you imagine growing up with the Jupiter ACE in your home and
        actually <em>understanding it</em>?
        </p><p>The ACE ran on the
        <a href="https://en.wikipedia.org/wiki/Zilog_Z80">Zilog Z80</a>
        (wikipedia.org)
        CPU, which was incredibly popular at the time for low-power computers
        and has had an amazingly long life. It was used in the higher-end TI
        graphing calculators such as the
        <a href="https://en.wikipedia.org/wiki/TI-85">TI-85</a>
        (wikipedia.org)
        I had in high school in 1996, which I spent many a happy afternoon
        programming in TI-BASIC.
    </p></div>
</div>

<div>
    <h2>Canon Cat, 1987</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/canon_cat.png" alt="drawing of the canon cat word processor home computer"></p><p>Operating system: Forth.
    </p><p>OS, office suite, and programming environment in 256 KB of ROM.
    </p><p>Innovative interface by Jef Raskin.

    </p><div>
        <p>Another computer with Forth as an operating system!
        </p><p>The <a href="https://en.wikipedia.org/wiki/Canon_Cat">Canon Cat</a>
        (wikipedia.org)
        is a particularly fascinating machine for a number of different
        reasons, the primary of which is the keyboard-driven interface
        by UI pioneer Jef Raskin.
        </p><p>Raskin wrote a book titled
        <a href="https://en.wikipedia.org/wiki/The_Humane_Interface">The Humane Interface</a>
        (wikipedia.org)
        with some provocative ideas that are probably
        very much worth re-visiting.
        For example, I like these two design rules:
        </p><blockquote>
            <ul>
                <li>Elimination of warning screens - modern software
                    applications often ask the user "are you sure?" before some
                    potentially harmful action; Raskin argues they are
                    unhelpful because users tend to ignore them out of habit,
                    and that having a <strong>universal undo</strong>
                    eliminates the need for them.
                </li><li>Universal use of text - Raskin argues that graphic icons in
                    software without any accompanying text are often cryptic to
                    users.
            </li></ul>
        </blockquote>
        <p>The Cat was the hardware and software incarnation of Raskin's
        design philosophies.
        </p><p>Also, you <em>have</em> to check out the picture of Jef with a
        little model of the Cat on his Wikipedia page:
        <a href="https://en.wikipedia.org/wiki/Jef_Raskin">Jef Raskin</a>
        (wikipedia.org).
        Direct link to the image: <a href="https://en.wikipedia.org/wiki/Jef_Raskin#/media/File:Jef_Raskin_holding_Canon_Cat_model.png">here</a>
        (wikipedia.org).
        </p><p>The Cat ran on a
        <a href="https://en.wikipedia.org/wiki/Motorola_68000">Motorola 68000</a>
        (wikipedia.org)
        CPU, which was also used in the Apple Macintosh and was one of the
        first 32-bit processors, featuring 32-bit instruction set, registers,
        and non-segmented memory addressing.
        </p><p>Getting to the Forth interface doesn't seem to have been a top
        priority on the Cat.
        </p><p>Quoting Dwight Elvey at the DigiBarn computer museum,
        <a href="https://www.digibarn.com/collections/systems/canon-cat/forthinside.html">Canon Cat: Enabling Forth</a>
        (digibarn.com),
        the process <em>sounds</em> a bit awkward:
        </p><blockquote>
            "Highlight the string: <em>Enable Forth Language</em>.<br>
            Then do: front, answer<br>
            Then: shift, usefront, space<br>
            You are now in Forth.<br>
            You need to do: -1 wheel! savesetup re<br>
            Front the editor, use the setup to set the keyboard to ascii
            so that you can type the characters &lt; and &gt; with
            shift , and shift .<br>
            Do a usefront disk.<br>
            It will save to the disk so that it will be ready
            the next boot with just the: shift, usefront, space
            to restart Forth.<br>
            To undo the Forth mode: Forth? off 0 wheel! re [sic everything]"
        </blockquote>
        <p><em>(Note that "USE FRONT" is a dedicated key on the Canon Cat
            keyboard that lets you apply whatever function is printed on the
            front of another key on the keyboard. Clever, right?  All of the
            Cat's interactions are performed through the keyboard like
            this.)</em>
        </p><p>And if that process weren't enough to put you off, this warning
        seems particularly dire and, if anything, hilariously understated:
        </p><blockquote>
            "Use care while in Forth mode as usefront shift : will
            format the disk (a good idea to make a backup or
            at least remove the disk while experimenting)."
        </blockquote>
        <p>But all of that makes it sound worse than it is.
        Thanks to modern streaming video technology, you can
        <em>see</em> Dwight Elvey
        <a href="https://www.youtube.com/watch?v=jErqdRE5zpQ">boot up a cat and demonstrate it</a>
        (youtube.com).
        As you can see, getting to the Forth interface is really not a
        lengthy process at all once you know what to do. Just a couple keystrokes.
        And the Cat is a more compact computer than I imagined from the pictures.
        </p><p>If you like industrial design or interesting computer interfaces,
        you owe it to yourself to check out the amazing pictures of
        <a href="https://www.digibarn.com/friends/jef-raskin/slides/canon-cat/index.html">Jef Raskin's team designing the Canon Cat (1985)</a>!
        (digibarn.com)
        </p><p>If you want to see a bunch of pictures of a vintage Cat in
        amazing shape, check out Santo Nucifora's
        <a href="https://vintagecomputer.ca/canon-cat/">Canon Cat</a>
        (vintagecomputer.ca).
        </p><p>If nothing else, just let this fact marinate in your head for a
        little bit: <strong>The Canon Cat had an OS, office suite, and
            programming environment in 256 KB of ROM.</strong> This 
        document (not including the images) is almost exactly that
        size!
        </p><p>Okay, now we are ready for...
    </p></div>
</div>

<div>
    <p><img src="http://ratfactor.com/forth/talkimg/forth_in_space.png" alt="title says Forth in Space and chuck is an astronaut on EVA who says May the Forth be with you."></p><div>
        <p>Easily one of the most exciting uses of Forth is space
        exploration because space is intrinsicly awesome.
        </p><p>We've already seen how Chuck Moore was intimately
        involved in programming ground-based radio telescopes.
        But Forth has also found its way into tons (literally and idiomatically)
        of actual space craft in outer space!
        </p><p>NASA is famous for having stringent rules about software
        that runs on spacecraft. Which makes sense, given the cost of these
        machines and the difficulty or even impossibility of getting
        to them to make fixes.
	</p></div>
</div>

<div>
	<h2>NASA and the ESA</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/nasa_list.jpg" alt="unreadable list of a ton of nasa projects using forth"></p><p>The list of projects using Forth at NASA compiled by James Rash in 2003 is too long to easily list here.

	</p><div>
        <p>The image on the right is intentionally too small to read. As you
        can see, it's a big list.
        </p><p>The original NASA link has died, but the page was archived by
        the Wayback Machine at archive.org. There's a nice copy
        hosted here as well:
		<a href="https://www.forth.com/resources/space-applications/">Forth in Space Applications</a>
        (forth.com).
        </p><p>I haven't found a list like this for the ESA, but the Philae
        lander featured below would be one very prominent example.
        </p><p><i>(By the way, though Forth isn't featured here, there's a fun overview
        of some CPUs used in various space missions: 
        <a href="https://www.cpushack.com/space-craft-cpu.html">The CPUs of Spacecraft: Computers in Space</a>
        (cpushack.com).)</i>
        </p><p><i>(The image to the right is very tall. We need some more text for
            wider screens. So, while it's not about Forth,I won't miss this
            opportunity to mention one of my favorite computing-in-space books:
        <a href="http://web.mit.edu/digitalapollo/">Digital Apollo: Human and Machine in Spaceflight</a>
        (mit.edu)
        by David Mindell. It will change how you look at the Apollo missions,
        computers in general, and the role of astronauts in space craft!)</i>
	</p></div>
</div>

<div>
	<h2>Space Shuttle Small Payload Accommodations Interface Module (SPAIM)</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/nasa_ssbuv_patch.png" alt="nasa mission patch for ssbuv"></p><p>"There is always great concern about software reliability, especially with flight software."

	</p><div>
        <p>From the paper
        <a href="https://dl.acm.org/doi/pdf/10.1145/199200.316990">Forth in Space: Interfacing SSBUV, a Scientific Instrument, to the Space Shuttle (PDF)</a>
        (acm.org)
        by Robert T. Caffrey et al:
        </p><blockquote>
            "There is always a great concern about software reliability,
            especially with flight software. The effects of a software error in
            flight could be dramatic. We were able to produce reliable software
            by writing a Forth routine on the PC, downloading the software, and
            testing it interactively. We varied the inputs to a routine and
            checked the ability of the routine to operate correctly under all
            conditions. As a result, during the STS-45 Shuttle mission, the
            SPAIM flight software worked perfectly and without any problems."
        </blockquote>
        <p>Forth systems can be multi-tasking and this allowed the system to
        monitor itself. Each task had its own stack and a watchdog task could,
        for example, check the health of another task by monitoring the
        other task's stack. (Stack depth was found to be a good indication of
        task health. In other words, malfunctions would often cause the stack
        depth to grow unchecked.)
        </p><blockquote>
            "The ability of the Forth development system to debug hardware and
            software interfaces, model missing hardware, simulate system
            malfunctions, and support system integration dramatically helped in
            the quick generation of error-free software. The interactive,
            integrated and multitasking features of the Forth system proved to
            be the key elements in the success of the SPAIM systems
            development. Several techniques such as stack depth monitoring,
            address monitoring, cycle time monitoring, and error flag
            monitoring provided system checks during both the system
            integration process and the actual Shuttle mission."
        </blockquote>
        <p>The interactive nature of the Forth system is again found to be not
        just very convenient, but also a massive productivity boost for all
        phases of programming, debugging, and testing.
        </p><p>The SPAIM system used a 16-bit Intel 87C196KC16 microcontroller,
        which is a MIL-SPEC member of the
        <a href="https://en.wikipedia.org/wiki/Intel_MCS-96">Intel MCS-96</a>
        (wikipedia.org)
        family. These started out as controllers for Ford engines in the 1970s.
        They continued to be made in various incarnations until 2007 and were
        often used in common devices such as hard drives, modems, and printers.
        Unlike many chips headed to space long-term, this one wasn't "rad-hard"
        (hardened against the effects of radiation).
	</p></div>
</div>

<div>
	<h2>NASA's Robot Arm Simulator</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/nasa_robot_arm.png" alt="robot arm in space shuttle"></p><p>Given the input of three-axis joystick commands, control a
    50-foot long, six-joint arm with six different coordinate systems.
    </p><p>Entire system developed by <b>one programmer in five weeks.</b>

	</p><div>
        <p>The <a href="https://www.forth.com/space-shuttle-robot-arm/">Space Shuttle Robot Arm Simulator</a>
        (forth.com)
        was a complex machine with some challenging requirements.
        </p><p>It turns out that you can't just use the <em>same robot arm</em> on
        the ground for simulations as the one that will go into space.
        For one thing, contending with gravity changes the requirements to
        such a degree that it's a completely different robot!
        </p><blockquote>
            <p>"The GSFC arm, for example, is designed to carry up to a thousand
            pound payload at its tip. In order to do this it uses a high
            pressure (4000 psi) hydraulic system rather than electric motors as
            on the RMS.
            </p><p>...
            </p><p>"Because of the completely different nature of the joint controls,
            the original RMS software was not usable except as a source of
            algorithms."
        </p></blockquote>
        <p>So the simulator arm <strong>could not work the same way,
            but it had to pretend it did</strong>.
        </p><p>You can see in my drawing that the arm lived in a full-scale
        simulated shuttle bay and was accompanied by an enormous model
        satellite. (That satellite looks like the Hubble Space
        Telescope to me, which seems plausible, given the dates on this
        project.)
        </p><p>Just listen to these I/O requirements:
        </p><blockquote>
            "The RMSS contains fourteen separate processes: one for each joint,
            one for each joystick, one for the digital display panel, a
            simulation process, a trending process, and several supervisory
            processes."
        </blockquote>
        <p>But, as seems to be a trend with Forth-based space software,
        the work was impeccable:
        </p><blockquote>
            "Simulation testing was so thorough that when the arm software was
            installed on site, not a single change was made to the executive
            control algorithms."
        </blockquote>
        <p>Does Forth imply excellence, or does excellence imply Forth? Ha ha.
        </p><p>Seriously, though, writing a system like that in five weeks
        is pretty astounding.
	</p></div>
</div>

<div>
	<h2>Shuttle Mission Design and Operations System (SMDOS)</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/nasa_shuttle.png" alt="drawing of the shuttle launching"></p><p>JPL's ground-based control software for shuttle SIR-A and SIR-B
    radar imaging instruments.

	</p><div>
        <p>This section started off as an excuse to draw a Space Shuttle.  But
        it's actually a great example of how a "live" interactive system
        can save a mission, even if the software itself hasn't been deployed into
        space.
        </p><p>The paper:
        <a href="https://dl.forth.com:8443/jfar/vol3/no2/article2.pdf">Forth as the Basis for an Integrated Operations Environment for a Space Shuttle Scientific Experiment (PDF)</a>
        (forth.com)
        describes a number of hardware failures that had to be
        overcome.
        </p><blockquote>
           "It was in the first day of data taking that we noticed
            the first problem..."
        </blockquote>
        <p>The SIR-B's transmitting antenna had shorted, resulting in the
        expected 1000 watts of power being reduced to a faint 100 watts.
        </p><blockquote>
            "Since the returned echo was negligible as received by the SIR-B
            antenna it was decided to increase the gain of the receiver.
            The problem was in not understanding what had happened to cause
            the failure. [It] was not immediately apparent what the
            appropriate gain should be..."
        </blockquote><p>
        Forth-based, highly adaptable SMDOS to the rescue!
        </p><blockquote>
            "No problem. With the advice of the radar engineers, the Forth
            module that was used to generate the display was quickly
            modified to produce a calibrated display. The gain of the
            receiver was increased until a perfect bell-shaped pattern
            again appeared on the display."
        </blockquote><p>
        Then a <strong>second</strong> hardware failure:
        </p><blockquote>
            "This was only the start of our problems. A satellite on board
            failed to deploy properly. The shuttle had to remain in high orbit
            until the problem was resolved before it could fire its engines to
            descend to the orbit that had been planned for the SIR-B data
            taking. "
        </blockquote><p>
        Now the shuttle would not be in the planned orbit for data-taking.
        A second SMDOS adaptation fixed that.
        </p><p>Then a <strong>third</strong> hardware problem with <em>another</em>
        antenna:
        </p><blockquote>
            "A bolt had sheared in the antenna's pointing mechenism and the KU
            band antenna was trashing around, threatening to destroy itself. It
            was necessary for an astronaut to exit the shuttle (EVA) in a
            spacesuit to pin the antenna down."
        </blockquote>
        <p>Now the shuttle had to rotate to point at a relay satellite to
        gather data (to tape!) and then rotate towards Earth to transmit
        the recorded data, and repeat.
        </p><blockquote>
            "Of course this meant an entirely new data-taking strategy. Again
            the SMDOS computers were put to work displaying new plans for the
            stringent new conditions."
        </blockquote>
        <p>They lost tons of data, of course, but at least they were able to
        salvage 20% of it by rotating and capturing and rotating and
        transmitting. None of which would have a been possible if they had not
        been able to modify the software on the spot.
        </p><p>Conclusion:
        </p><blockquote>
            "When the antenna feed failed and we realized that the software had
            to adapt to that failure, it was relatively easy given the
            interactive Forth enviroment to change the required module to meet
            the new specifications. This is clearly beyond the capabilites of
            most languages."
        </blockquote>
        <p>Other systems are interactive, but Forth may be singularly unique in
        allowing <em>complete</em> freedom of modification in an interactive
        session.
        </p><p>Of course, this kind of freedom is double-edged sword if there ever
        was one. The implied danger of that powerful sword (plus the postfix
        notation) has been a hard sell in the corporate world.
        </p><p>So far, we've just seen Forth <em>software</em> in space. But it
        is often accompanied by Forth <em>hardware</em>.
        </p><p>Yup, Forth hardware. Introducing:
	</p></div>
</div>

<div>
    <h2>Forth hardware in space</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/harris_rtx.jpg" alt="block diagram of harris chip"></p><p>The <b>Harris RTX2010</b> processor.  Used in a ton of space
    applications.
    </p><p>Featuring:
	</p><ul>
		<li>Direct execution of Forth
		</li><li>Two hardware stacks, 256 words deep
		</li><li>8MHz clock, extremely low latency 
		</li><li>Radiation hardened
	</li></ul>

	<div>
        <p>The
		<a href="https://en.wikipedia.org/wiki/RTX2010">RTX2010</a>
        (wikipedia.org)
        and its predecessor, the RTX2000
        account for a good portion of the use of Forth in the space industry.
        They run Forth natively.
        </p><p>The use of the RTX line in space may not be soley due to a particular
        love for Forth per se, but because of the specific attractive properties
        of these processors - very low latency and the ability to quickly
        process the floating point mathematical operations needed for neat space
        stuff like navigation and thruster control. Either way, the 
        <em>philosophy</em> of Forth embedded in this hardware is suitable
        for the extreme environments in which they operate.
        </p><p>Largely because of the stack-based design, the RTX 2000 and 2010
        have very compact machine code. <strong>Subroutines calls take only a
        single cycle and returns are free!</strong>  All branches take
        exactly one cycle as well.
        </p><p>They are also brilliantly minimalistic designs. The entire RTX2000
        instruction set fits on a single page. See the first PDF link below:
        </p><ul>
            <li><a href="https://vfxforth.com/flag/jfar/vol6/no1/article1.pdf">The Harris RTX 2000 Microcontroller (PDF)</a>
                (vfxforth.com)
                - The RTX2000 as described in The Journal of Forth Application
                and Research by Tom Hand.
            </li><li><a href="https://www.mouser.com/catalog/specsheets/intersil_fn3961.pdf">HS-RTX2010RH Data Sheet (PDF)</a>
                (mouser.com)
                - The RTX2010 is now sold by Intersil.
            </li><li><a href="https://rocelec.widen.net/view/pdf/wezkwfjd8w/INSLS11172-1.pdf?t.download=true&amp;u=5oefqw">RTX 2000 Data Sheet (PDF)</a>
                (widen.net) as originally sold by Harris.
            </li><li><a href="https://www.digikey.com/en/products/detail/rochester-electronics-llc/RTX2000JI-8/12131997">DigiKey evidently has 800+ RTX2000s in stock</a>
                (digikey.com)
                through Rochester Electronics for a reasonable $22, but you
                have to by them in quantities of 14.  (Maybe you can find
                14 friends to do a group buy?)
        </li></ul>
        <p>So what kind of spacecraft use these Forth-native processors?
        </p><p>Let's look at a specific pair of spacecraft:
	</p></div>
</div>

<div>
	<h2>Rosetta and Philae</h2>
	<p><img src="http://ratfactor.com/forth/talkimg/rosetta_spacecraft.png" alt="drawing of rosetta approaching comet"></p><p>First mission to send a spaceship to orbit a comet and then deliver a
    lander to the comet's surface!
	</p><p>The <b>Rosetta</b> spacecraft's Ion and Electron Sensor instrument used a Harris RTX2010.
	</p><p>The <b>Philae</b> lander used two Harris RTX2010s for complete system control (CDMS) and two more to control its landing system (ADS).

	</p><div>
        <p>The ESA's <a href="https://www.esa.int/Science_Exploration/Space_Science/Rosetta">Rosetta mission</a>
        (esa.int)
        was hugely ambitious: Send a spacecraft to
        rendezvous with and then follow a comet around the Sun, deploy
        the Philae lander to the surface by dropping it into the comet's
        gravity well, observe the lander as it shoots harpoons
        into the icy surface of the comet to keep from bouncing back out
        into space, then relay the lander's communication from the surface back to
        distant Earth, 28 minutes away at the speed of light.
        </p><p>Rosetta traveled in the Solar System for a full decade (2004 to
        2014) before meeting up with comet <b>67P/"Churyumov-Gerasimenko"</b>.
        (67P is 4km wide and orbits the sun every six and a half years.)
        </p><p>Rosetta orbited the comet for three months and then deployed
        the Philae lander to the surface of the comet.
        </p><p>Both craft contained a full laboratory of advanced scientific
        instruments (11 on Rosetta, 9 on Philae) including some that doubled
        as high-resolution cameras with images suitable for humans to view.
        <a href="https://en.wikipedia.org/wiki/Rosetta_(spacecraft)">The whole mission</a>
        (wikipedia.org)
        is worth reading about. There are some fantastic images
        and animations to be seen on the mission page and on the
        <a href="https://en.wikipedia.org/wiki/67P/Churyumov%E2%80%93Gerasimenko">comet's own page</a>
        (wikipedia.org).
        <img src="http://ratfactor.com/forth/talkimg/philae_from_rosetta_via_osiris.jpg" alt="the philae lander brightly illuminated by the sun against the black backdrop of space">
        </p><p>Often described as being "the size of a washing machine," the
        <a href="https://en.wikipedia.org/wiki/Philae_(spacecraft)">Philae</a>
        (wikipedia.org)
        lander pushed away from Rosetta's orbit to drop to the surface of 67p.
        </p><p><em>The picture at the right was taken
        from Rosetta's OSIRIS imager as Philae fell slowly away from the
        orbiter.</em>
        </p><p>Because the comet's gravitational pull is so small (huge boulders
        have been observed moving around on its surface), a pair of harpoons
        were meant to fire into the surface of the comet and hold the lander
        down. These did not deploy (possibly a mechanical failure) and a
        landing thruster also failed, so Philae ended up having a long,
        low-gravity tumble on the surface.
        </p><p>It's been speculated that the harpoon failure actually
        <em>saved</em> Philae from an even more exciting trip because studies
        of the surface found it to be harder than expected. It might have
        launched itself away rather than anchoring! As it was, the lander
        bounced with a force that was <em>just shy</em> of escaping the comet's
        gravitational pull entirely. It rose a full kilometer above the surface
        before slowly returning for another two bounces to its final resting
        spot.
        </p><p>A pair of Harris RTX2010s controlled Philae's Active Descent System.
		Check out <a href="https://www.cpushack.com/2014/11/12/here-comes-philae-powered-by-an-rtx2010/">Here comes Philae! Powered by an RTX2010</a>
        (cpushack.com):
        </p><blockquote>
            "Why was the RTX2010 chosen?  Simply put the RTX2010 is the lowest
            power budget processor available that is radiation hardened, and
            powerful enough to handle the complex landing procedure.  Philae
            runs on batteries for the first phase of its mission (later it will
            switch to solar/back up batteries) so the power budget is critical.
            The RTX2010 is a Forth based stack processor which allows for very
            efficient coding, again useful for a low power budget."
        </blockquote>
        <p>Here is more information (with pictures!) about the physical design
        and components in the Philae control system:
        <a href="http://www.sgf.hu/newsgfweb3_005.htm">Command and Data Management Subsystem (CDMS) of the Rosetta Lander (Philae)</a>
        (sgf.hu).
        </p><blockquote>
            "Harris RTX2010 processor has been selected for the DPU boards
            because it is the lowest power consuming, space qualified,
            radiation hardened, 16-bit processor with features to provide so
            complicated functions as the CDMS has to perform. It is a stack
            based, Forth language oriented processor with an exotic and
            challenging instruction set. CDMS is a real-time control and data
            acquisition system, and it has to process tasks in parallel.
            Therefore, a real-time, pre-emptive multitasking operating system
            has been developed to run application tasks executing the required
            functions in parallel."
        </blockquote>
        <p>And here is the lander's
        <a href="https://www.spyr.ch/ps/ads/qm/">Active Descent System (ADS) QM User Manual</a>
        (spyr.ch)
        which has way more detail about this computer system, including a
        number of details about the Forth software:
        </p><blockquote>
             "After resetting the subsystem (power-on reset), the bootstrap
             sets up the Forth environment, copies the firmware from PROM to
             RAM and disables the PROM for further access.  
             <p>
             
             After this, the main word Do-App is called from the Forth system
             immediately after setup. You can find the main word Do-App in the
             file app.fth (see part II). Do-App calls Init-App, which itself
             calls other initilisation words like Init-ADS. Then the
             application enters the main loop. In the main loop the following
             actions are performed:
             </p><ul>
                <li>reset the watchdog (watchdog is enabled for the QM)
                </li><li>put the data into the HK registers
                </li><li>get the data from the ADC handler
                </li><li>process CDMS requests"
             </li></ul>
        </blockquote>
        <p>Despite the unfortunate landing, which put Philae in too much
        shadow to get as much solar energy as hoped and at an angle that
        made communication with Rosetta difficult, Philae was still
        robust enough to perform "80%" of its scientific mission, which
        is pretty amazing.
	</p></div>
</div>

<div>
    <p>A picture taken by the Philae lander as it lay on its side, enjoying some sunlight on
    one of its feet:
    <img src="http://ratfactor.com/forth/talkimg/philae_civa1.jpg" alt="the outline of deeply fractured rock and ice, the darkness of space in the background, and a lander foot brightly lit by the sun.">

    </p>
</div>

<div>
    <p>This is one of the final images taken by the Rosetta orbiter as it made the
           "hard descent" (controlled crash landing) to the surface of comet 67p:
	<img src="http://ratfactor.com/forth/talkimg/rosetta_comet_67p.jpg" alt="photo of comet 67p taken by rosetta (">

	</p><div>
        <p>The image and a description are here: <a href="https://www.nasa.gov/feature/jpl/final-descent-image-from-rosetta-spacecraft">Final Descent Images from Rosetta Spacecraft</a>
        (nasa.gov).
        </p><blockquote>
            "The decision to end the mission on the surface is a result of
            Rosetta and the comet heading out beyond the orbit of Jupiter
            again. Farther from the sun than Rosetta had ever journeyed before,
            there would be little power to operate the craft. Mission operators
            were also faced with an imminent month-long period when the sun is
            close to the line-of-sight between Earth and Rosetta, meaning
            communications with the craft would have become increasingly more
            difficult."
        </blockquote>
        <p>By the way, the ESA has a nice summary of the computer hardware
        used by the OSIRIS camera on Rosetta which was used to take the surface
        image above and also the little picture of the descending lander further
        above.
        <a href="https://sci.esa.int/web/sci-fmi/-/35973-osiris">Optical, Spectroscopic, and Infrared Remote Imaging System</a>
        (esa.int).
        </p><p>After finishing the first draft of this article, I was so excited about
        the Rosetta mission that I ended up ordering and reading
        <i>Rosetta: The Remarkable Story of Europe's Comet Explorer</i> by Peter Bond.
        It's a bit of a dry read, but the subject matter is thrilling
        nonetheless and the coverage is thorough.  I recommend it if you want to
        know a lot more about this awesome engineering and scientific milestone.
        (It does not, sadly, mention Forth.)
        </p><hr>
        <p><strong>Rabbit Hole Alert:</strong> This takes us away from
        Forth for a moment, but learning about the
        Virtuoso RTOS (real-time operating system) eventually leads to a deep,
        deep Wikipedia rabbit hole that takes you on a journey to the Inmos
        processors, Hoare's CSP, the occam programming language, the HeliOS
        parallel computing operating system, and the concept of the
        <b>"transputer"</b> microprocessors.
        </p><p>Transputers use whole processors as
        building blocks for a parallel computer in the same way transistors are
        used as the building blocks for processors. (Thus, transputer =
        "transistor computer," you see?) They were mostly featured in
        supercomputers, but they also saw some industrial controller use
        and there was even an Atari Transputer Workstation,
        ATW-800.
        </p><p>(I've intentionally not linked to any of these things here
        because you'll disappear into that hole and never see the end of this
        document, which would be very sad.  Also, I mention "transputers" again
        one more time below and you wouldn't want to miss that.)
        </p><hr>
        <p>The Rosetta orbiter and Philae lander now rest silently on the
        surface of 67p, where they will no doubt stay for billions of
        years or until future comet tourists pick them up and put them
        in a trophy room, whichever comes first.
	</p></div>
</div>

<div>
    <h2>Stop Writing Dead Programs</h2>
    <p>
        <img src="http://ratfactor.com/forth/talkimg/jack_rusher.jpg" alt="crop of jack rusher from the previous screenshot">
        "...Space probes written in Lisp and <b>Forth</b> have been
        debugged while off world...  If they had proven their programs
        correct by construction, shipped them into space, and then found out
        their spec was wrong, they would have just had some <b>dead junk on
            Mars</b>.  But what these guys had was the ability to fix things
        while they are running on space probes...  In addition, the spec is
        always wrong!"
    </p><p>-- Jack Rusher, Stop Writing Dead Programs (talk given at Strange Loop 2022)

    </p><div>
        <p>Here's the talk:
        <a href="https://www.youtube.com/watch?v=8Ab3ArE8W3s">"Stop Writing Dead Programs" by Jack Rusher (Strange Loop 2022)</a>
        (youtube.com).
        </p><p>You've got 43 minutes to watch it. I'm timing you. Don't get
        distracted by other YouTube suggestions. Come back here. I'm waiting.
        </p><p>Or better yet, check out Jack's <i>awesome</i> transcript,
            which was super helpful when I wanted to re-find the above quote:
        <a href="https://jackrusher.com/strange-loop-2022/">Stop Writing Dead Programs.</a>
        (jackrusher.com).
        </p><p>In his transcript, he notes:
        </p><blockquote>
            "Had I had more time, I would have done an entire series of slides
            on FORTH. It's a tiny language that combines interactive
            development, expressive metaprogramming, and tremendous machine
            sympathy. I've shipped embedded systems, bootloaders, and other
            close-to-the-metal software in FORTH."
        </blockquote>
        <p>I was extremely interested in hearing about Forth systems being
        updated in space, but had a heck of a time finding any.
        I finally found one on a page that is otherwise largely
        dedicated to Lisp's use at the Jet Propulsion Labs:
        <a href="https://mecrisp-stellaris-folkdoc.sourceforge.io/lisp.html">1992-1993 - Miscellaneous stories</a>
        (sourceforge.io)
        on the amazing, sprawling site for the Mecrisp-Stellaris Forth
        (which runs on various non-x86 CPUs):
        </p><blockquote>
           "Also in 1993 I used MCL to help generate a code patch for the
           Gallileo magnetometer. The magnetometer had an RCA1802 processor, 2k
           each of RAM and ROM, and was programmed in Forth using a development
           system that ran on a long-since-decommissioned Apple II. The
           instrument had developed a bad memory byte right in the middle of
           the code. The code needed to be patched to not use this bad byte.
           The magnetometer team had originally estimated that resurrecting the
           development environment and generating the code patch would take so
           long that they were not even going to attempt it. Using Lisp I wrote
           from scratch a Forth development environment for the instrument
           (including a simulator for the hardware) and used it to generate the
           patch. The whole project took just under 3 months of part-time
           work."
        </blockquote>
        <p><i>(If anyone has any leads to other notable Forth uses in space, I'd love to
            hear about them.)</i>
    </p></div>
</div>

<div>
    <p>When we defeat the alien kill-bots and reprogram them, it will
		surely be with a Forth of some sort.
	<img src="http://ratfactor.com/forth/talkimg/killbots_forth.png" alt="alien kill-bots being controlled by forth">

    </p><div>
        <p>In the background, one of the Invader machines lies crumpled and
        smoking amidst ruins. This was one of Earth's great cities.
        </p><p> Stomping towards us with its mechanical arms raised in victory, is
        another Invader.  But this one is different. The tell-tale giveaway is
        the opening in its protective head dome. And is that a flag? Why yes, it is!
        </p><p>At great cost, humans managed to trap one of the Invaders long
        enough penetrate its outer defenses, while otherwise leaving the
        machine unharmed and operable.
        </p><p>Working feverishly against a doomsday clock, they burrowed deep into
        the electrical heart of the machine, identifying and classifying its
        alien functions until they understood it well enough to attempt
        an interface.
        </p><p>A bus protocol was decoded. Programming work began.
        </p><p>It went poorly. The aliens had unthinkably bizarre notions of
        generalized computing that defied all known patterns of software.
        </p><p>Everything had to be done with agonizing labor, stringing
        sequences of raw bus messages together in hopes of getting a
        correct response.
        </p><p>But then someone had the bright idea to bootstrap a Forth
        from the known instruction sequences. With this, they could write
        a bare-bones interpreter. And, at last, they could experiment
        quickly and safely.
        </p><p>Days later, an arm moved. Then they crushed a barrel with a
        gripper claw:
        </p><pre>BARREL OBJECT-ID VISION TARGET
133 L-ARM-FWD 14 L-CLAW-OPEN
25 L-ARM FWD 14 L-CLAW CLOSE
        </pre>
        <p>Then a first four-legged step. Then 20 steps:
        </p><pre>PREP-QUAD-LEGS
20 STRIDE-LOOP
        </pre>
        <p>As ravaged fighters looked on in amazement, <b><i>"Defender-1"</i></b> burst
        from the old brick warehouse and, in a terrific crash, it toppled
        another Invader as it was passing by on patrol.
        </p><p>The machines grappled for a moment and it
        looked as if <i>Defender-1</i>'s clumsy movements would be no match
        for the alien, even from a superior position.
        </p><p>But humans had decoded <em>all</em> of the weapon systems by then and a
        special word had been prepared for this moment:
        </p><pre>: KILL
    100 BEAM-LEVEL
    BOT OBJECT-ID VISION TARGET
    L-BEAM FIRE-FULL
    R-BEAM FIRE-FULL
;
        </pre>
        <p>Twin blinding beams of energy struck the enemy full in the torso
        and instantly turned its mechanical guts into sizzling plasma.
        After a moment of silence, a single cheer rose up from a doorway
        nearby and was soon joined by a hundred different voices from
        places of concealment in the ruined buildings.
        </p><p>Now the humans had the upper hand at last! Other Invader
        machines were disabled or captured. <i>Defender-1</i> was joined
        by <i>Defender-2,</i> and then <i>3</i>, <i>4</i>, <i>5</i>, and more!
        </p><p>Software was passed by sneaker-net and by shortwave packet radio.
        City by city, Earth took back control. And along with victory,
        word of the <b>One True Language</b> spread across the land. Flags
        were raised in honor of its original discoverer, Chuck Moore.
        </p><p>Where other abstractions had failed, the universal machine
        truth of Forth had succeeded.
    </p></div>
</div>

<div>
    <h2>Forth is an <i>idea</i></h2>
    <p>Here's a "family tree" of some notable Forths:
    <img src="http://ratfactor.com/forth/talkimg/forth_family_tree.jpg" alt="unreadably tiny diagram of lineage of various Forth implementations">

    </p><div>
        <p>Obviously the graphic is unreadably tiny. For the full-size
            original and the gForth program used to create it, check out:
        </p><p><a href="http://www.complang.tuwien.ac.at/forth/family-tree/">Forth Family Tree and Timeline</a>
        (complang.tuwien.ac.at).
        </p><p>One of the hardest things about trying to learn "Forth" is realizing
        that there is no single implementation that can lay sole claim to that name.
        As we've seen, some of Chuck's first Forths pre-date the name entirely.
        </p><p>There are Forth standards dating back to the original ANS Forth
        document and continuing with the
        <a href="https://forth-standard.org/">Forth 2012 Standard and Forth200x committee</a>
        (forth-standard.org).
        </p><p>Forths have shared concepts. There are many common words, certainly, but purpose-built
        Forths will have their own special vocabularies.
        </p><p>Also, it is true that <em>making</em> Forths is at least as fun
            as using them.
        </p><p>The forest of computing is peppered with hobby Forths. They grow where nothing
        else can survive. They flourish in the sun and in the shade.
        Each one is a little glittering jewel.
    </p></div>
</div>

<div>
    <h2>What about Chuck?</h2>
    <p>Charles H. Moore founded Forth, Inc in 1973. He's continued to port
    Forth to various systems ever since. But he's never stopped inventing.
    <img src="http://ratfactor.com/forth/talkimg/chuck_and_crt.png" alt="drawing of chuck at a desk programming on a pc with a crt. equipment looks 1990s era">

    </p><div>
        <p>I drew this image of Chuck from a photo in this amazing quote
        collection,
        <a href="http://www.ultratechnology.com/moore4th.htm">Moore Forth: Chuck Moore's Comments on Forth </a>
        (ultratechnology.com)
        compiled by Jeff Fox.
        </p><p>You'll notice I added some color to my drawing for this one, and
        that's because I'm pretty sure that what we're seeing on Chuck's monitor
        is...
    </p></div>
</div>

<div>
    <p><span>color</span><span>Forth</span>
    <img src="http://ratfactor.com/forth/talkimg/colorforth.png" alt="screenshot of colorforth">

	</p><div>
        <p><em>The above screenshot is actually from
        <a href="http://www.etherforth.org/ef.html">a page about etherForth</a>,
        (etherforth.org),
        which is a
        <span>color</span><span>Forth</span> 
        written for GA144 chips. (Don't look up those chips yet unless you
        want a spoiler for what's coming in a moment below!)</em>
        </p><p>What the heck are we looking at here?
        </p><p>So,
		<a href="https://en.wikipedia.org/wiki/ColorForth">colorForth</a>
        (wikipedia.org)
        is:
        </p><blockquote>
            "An idiosyncratic programming environment, the colors simplify
            Forth's semantics, speed compiling, and are said to aid Moore's own
            poor eyesight: colorForth uses different colors in its source code
            (replacing some of the punctuation in standard Forth) to determine
            how different words are treated."
        </blockquote>
        <p>And, of course:
        </p><blockquote>
            "The language comes with its own tiny (63K) operating system.
            Practically everything is stored as source code and compiled when
            needed. The current colorForth environment is limited to running on
            Pentium grade PCs with limited support for
            lowest-common-denominator motherboards, AGP video, disk, and
            network hardware."
        </blockquote>
        <p>But the best description of
        <span>color</span><span>Forth</span>
        and its strengths come from Chuck Moore himself in an interview in
        2009, 
        <a href="https://www.red-gate.com/simple-talk/opinion/geek-of-the-week/chuck-moore-geek-of-the-week/">Chuck Moore: Geek of the Week</a>
        (red-gate.com):
        </p><blockquote>
            "Forth has some ugly punctuation that colorForth replaces by
            coloring source code. Each word has a tag that indicates function;
            it also determines color. This seems a small point, but it
            encourages the use of functions, such as comments or compile-time
            execution, that would be inconvenient in Forth."
        </blockquote>
        <p>It should be noted that the colors can be replaced with symbols or
        notation, so using the language without the ability to
        distinguish color is not a barrier. Color is just <i>one way</i> to
        show this information.
        </p><p>There are a ton of other enhancements beyond the obvious color aspect,
        such as:
        </p><blockquote>
            "By having words preparsed, the compiler is twice as fast. Another
            small point, since compiling is virtually instantaneous, but this
            encourages recompiling and overlaying the modules of an
            application. Smaller modules are easier to code, test and document
            than a large one."
        </blockquote>
        <p>That interview contains another Chuck Moore quote about software
        construction in general:
        </p><blockquote>
            "Instead of being rewritten, software has features added. And
            becomes more complex. So complex that no one dares change it, or
            improve it, for fear of unintended consequences. But adding to it
            seems relatively safe. We need dedicated programmers who commit
            their careers to single applications. Rewriting them over and over
            until they're perfect."
        </blockquote>
        <p>This is something I've seen repeated again and again by some of
        the most respected minds in software: You cannot just keep adding
        things to a program. You must continually re-work the program to match
        your needs as they change over time. Ideally, you re-write the program.
        Only time and deep consideration can yield the most elegant, correct,
        and <i>simple</i> program.
        </p><p>Which brings us to...
	</p></div>
</div>

<div>
    <h2>The pursuit of simplicity</h2>
    <p>Chuck Moore has been fighting against software complexity since the 1950s.
    </p><p>"I am utterly frustrated with the software I have to deal with. Windows is beyond comprehension! UNIX is no better. DOS is no better. There is no reason for an OS. It is a non-thing. Maybe it was needed at one time.
    </p><p>-- Chuck Moore, 1997

</p></div>

<div>
    <p>"If they are starting from the OS they have made the first mistake. The OS isn't going to fit on a floppy disk and boot in ten seconds."
    </p><p>-- Chuck Moore, 1999

	</p><div>
        <p><i>These quotes also come from Jeff Fox's quotes collection,
        <a href="http://www.ultratechnology.com/moore4th.htm">Moore Forth: Chuck Moore's Comments on Forth</a>
        (ultratechnology.com).</i>
        </p><p>As you've no doubt gathered over the course of this page,
        Chuck is no fan of big, heavy, complicated software such as
        operating systems.
        </p><p>He believes in compact, <em>machine-sympathetic</em> programming.
        </p><p>"Mechanical Sympathy" is not Chuck's term, but I believe it
        accurately describes his philosophy. It comes from this
        (apocryphal?) quote by 
        Formula One race car driver
        <a href="https://en.wikipedia.org/wiki/Jackie_Stewart">Jackie Stewart</a>
        (wikipedia.org):
        </p><blockquote>
            "You don't have to be an engineer to be a racing driver, but you
            do have to have <strong>mechanical sympathy</strong>."
        </blockquote>
        <p>The use of the term to describe <em>software</em> comes from Martin Thompson's
        blog of the same name.
        In <a href="https://mechanical-sympathy.blogspot.com/2011/07/why-mechanical-sympathy.html">Why Mechanical Sympathy?</a>
        (blogspot.com),
        he writes:
        </p><blockquote>
            "Why does the software we use today not feel any faster than the
            DOS based applications we used 20 years ago???  It does not have to
            be this way.  As a software developer I want to try and produce
            software which does justice to the wonderful achievements of our
            hardware friends."
        </blockquote>
        <p>Again and again, you'll see this sentiment echoed by Chuck Moore
        and fans of Forth.
        </p><p>I think it's very interesting and telling that Forth tends to be
        popular with "hardware people" such as electrical engineers and embedded
        systems designers. By contrast, it seems that "software people"
        tend to idolize a more abstract, <em>high-level</em> beauty as found
        in languages such as Lisp or Scheme.
        Of course, this is a gross generalization and may have no basis in fact,
        but I know I'm not the only person to notice this trend.
        </p><p>Maybe another way to describe this aspect of Forth is that it has a
        "mechanical purity" in the same way that Joy, with its combinators,
        has a "mathematical purity."
        </p><p>And speaking of hardware...
	</p></div>
</div>

<div>
    <h2>Processor Design</h2>
    <p>Chuck's <i>real</i> love seems to be processor design.
	Those Harris RTX2000 and RTX2010 chips used in so many space missions?
    <strong>That's basically his chip!</strong>
	<img src="http://ratfactor.com/forth/talkimg/chuck_chip_scientist.png" alt="chuck as a mad scientist chip creator">
</p></div>

<div>
    <p>No kidding.
    </p><p>Chuck, that brilliant rascal, has been designing hardware since 1983
    starting with the Novix N400 gate array. An improved design was
    sold to Harris to become the RTX chips.
    </p><p>Chuck designs processors with his own VLSI software, "OKAD", written in
    <strong>500 lines of Forth</strong>, of course.

	</p><div>
        <p>Take a moment to pause on that last sentence.
        </p><p>Processor design software written in 500 lines?
        </p><p>You read that right.
        </p><p>OKAD is one of the <strong>Great Legends of Chuck Moore</strong>.
        But what, exactly, is it?
        </p><p>First off, VLSI stands for
        <a href="https://en.wikipedia.org/wiki/Very_Large_Scale_Integration">Very Large Scale Integration</a>
        (wikipedia.org):
        </p><blockquote>
            "Very large-scale integration (VLSI) is the process of
            creating an integrated circuit (IC) by combining millions or
            billions of MOS transistors onto a single chip. VLSI began in the
            1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips
            were developed and then widely adopted, enabling complex
            semiconductor and telecommunication technologies. The
            microprocessor and memory chips are VLSI devices."
        </blockquote>
        <p>The product of VLSI is what we think of when we imagine
        the modern image of "computer chip" in our minds.
        </p><p>"Integration" is simply the shrinking of computers from whole rooms to
        microscopic thinking dust:
        </p><ul>
            <li>Computers began with processors the size of rooms with
                discrete logic gates you can touch (relays to vacuum tubes to
                transistors).
            </li><li>Then, processors were shrunk down to the size of refrigerators
                with logic boards of <strong>integrated circuits</strong> (ICs).
            </li><li>Finally, entire processors shrunk down to fit on a single chip via
                Very Large Scale <strong>Integration</strong>.
        </li></ul>
        <p><i>(Also, in a parallel path from mainstream desktop computing,
            VLSI has also produced entire computers and, increasingly,
            <b>multiple computers on a single chip</b>, also
            known as 
            <a href="https://en.wikipedia.org/wiki/System_on_a_chip">"system(s) on a chip" (SoC)</a>
            (wikipedia.org).
            The lines around the various types are extremely blurry, but
            some familiar forms are microcontrollers, embedded systems,
            various "mobile" devices, etc.)</i>
        </p><p>Anyway Moore's,
        <a href="https://colorforth.github.io/vlsi.html">VLSI Design Tools (OKAD)</a>
        (colorforth.github.io)
        system a complete processor workshop:
        </p><blockquote>
            "In 500 lines of
            <span>color</span><span>Forth</span>,
		    these tools provide everything required to design a chip."
        </blockquote>
        <p>OKAD is really more of a collection of tools that work together to:
        </p><ul>
            <li>Describe the basic logic gates (constructed of transistors),
            </li><li>Design the layout of the entire circuit (the three-dimensional multi-layered network of connections between gates),
            </li><li>Simulate the circuit electrically (voltage, temperature, capacitance, etc.),
            </li><li>And export the finished design to the industry-standard
                <a href="https://en.wikipedia.org/wiki/GDSII">GDSII</a>
                (wikipedia.org)
                file format that is given to IC foundries (or "chip fabs").
        </li></ul>
        <p>For more about OKAD, I highly recommend reading the
        excellent answers to
        <a href="https://retrocomputing.stackexchange.com/questions/25506/did-forths-inventor-charles-moore-really-write-a-cad-program-in-only-5-lines-of">Did Forth's inventor Charles Moore really write a CAD program in only 5 lines of code?</a>
        (retrocomputing.stackexchange.com).
        </p><p>Moving on from the software to Moore's chips themselves, Moore himself wrote
        a nice little summary of his designs. It is written in Moore's typical consise style,
        giving just a few key details about each chip:
        <a href="https://colorforth.github.io/chips.html">Forth Chips</a>
        (colorforth.github.io).
        </p><p>First, there was the <b>Novix NC4000</b>, which was designed
        for a CMOS gate array.
        </p><p>Here's a whole book about the NC4000 chip: <a href="http://forth.org/OffeteStore/4001-footstepsFinal.pdf">Footsteps in an Empty Valley: NC4000 Single Chip Forth Engine (8Mb PDF)</a> by Dr. Chen-Hanson Ting.
        </p><p>To quote Dr. Ting from Chapter 2:
        </p><blockquote>
            "The Novix NC4000 is a super high-speed processing engine which is
            designed to directly execute high level Forth instructions. The
            single chip microprocessor, NC4000, gains its remarkable
            performance by eliminating both the ordinary assembly language and
            internal microcode which, in most conventional processors,
            intervene between the high level application and the hardware. The
            dual stack architecture greatly reduces the overhead of subroutine
            implementation and makes NC4000 especially suited to support high
            level languages other than Forth."
        </blockquote>
        <p>As you can see, this reads just like a description of the Harris RTX
        chips used in the <b>spacecraft</b> we explored above.
        </p><p>Sure enough, if we read the History section on the
        <a href="https://en.wikipedia.org/wiki/RTX2010">RTX2010 page</a>,
        (wikipedia.org)
        the lineage is made very clear:
        </p><blockquote>
            "In 1983, Chuck Moore implemented a processor for his programming
            language Forth as a gate array. As Forth can be considered a dual
            stack virtual machine, he made the processor, Novix N4000 (later
            renamed NC4016), as a dual-stack machine. In 1988, an improved
            processor was sold to Harris Semiconductor, who marketed it for
            space applications as the RTX2000."
        </blockquote>
        <p>Another great article about Moore's early processor design work
        (and some more <b>spacecraft</b> mentions!), check out
        <a href="https://www.cpushack.com/2013/02/21/charles-moore-forth-stack-processors/">Charles Moore: From FORTH to Stack Processors and Beyond</a>
        (cpushack.com)
        which is part one of a two-part series.
        </p><p>After the Novix, came a variety of chip projects:
        </p><ul>
            <li><b>Sh-Boom</b> (32-bit, 20 Mips),
            </li><li><b>MuP20/MuP21</b> (21-bit, 100 Mips),
            </li><li><b>F21</b> (500 Mips - and be sure to check out
                <a href="http://www.ultratechnology.com/scope.htm">F21 in a Mouse</a>
               (ultratechnology.com), which is a complete F21 computer running a
               graphical environment that has been packed
               into a PC mouse...in the Pentium era!)
            </li><li><b>i21</b> (21-bit, 55 Mips)
            </li><li><b>X18</b> (18-bit, 2400 Mips)
        </li></ul>
        <p>These are all real systems that really worked. The hard part has always
        been finding customers.
        </p><p>Over the years, other people have also created Forth chips and FPGA
        implementations of hardware Forth-likes. Check out the links on
        <a href="http://forth.org/cores.html">Forth CPU Cores</a>
        (forth.org)
        and
        <a href="http://www.ultratechnology.com/chips.htm">Forth Chips</a>
        (ultratechnology.com).)
        </p><p>In addition to
        <span>color</span><span>Forth</span>,
        Moore also developed <b>"Machine Forth"</b> as an even <em>more</em>
        machine-sympathetic language than traditional Forth. It's based on
        the machine code of the MuP21 microprocessor listed above.
        </p><p>I won't go into a lot of detail about Machine Forth, but
        here are some interesting links:
        </p><ul>
            <li><a href="http://www.ultratechnology.com/mfp21.htm">MuP21 Machine Forth Tutorial #1 </a>
                (ultratechnology.com)
            </li><li><a href="http://www.ultratechnology.com/p21intro.html">P21Forth 1.02 User's Manual</a>
                (ultratechnology.com)
            </li><li><a href="https://www.complang.tuwien.ac.at/anton/euroforth/ef99/thomas99a.pdf">Machine Forth for the ARM processor (PDF)</a>
                (tuwien.ac.at)
            </li><li><a href="https://github.com/CCurl/MachineForth">MachineForth - Inspired by Chuck Moore's "Machine Forth" and the MuP21 processor.</a>
                (github.com)
            </li><li><a href="https://jjn.one/forth/machine-forth/">Machine Forth (links and bibliography)</a>
                (jjn.one)
        </li></ul>
        <p>As you can see, Moore has always been looking for new ways to work
        with computers, a partnership between the machine and the programmer.
        </p><p>Which brings us to the current state of Chuck Moore's art...
	</p></div>
</div>

<div>
    <h2>GreenArrays</h2>
    <p>"Programming a 144-computer chip to minimize power" (2013)
	<img src="http://ratfactor.com/forth/talkimg/greenarrays_144_computers.jpg" alt="screenshot from Chuck's 2013 strange loop talk about 144 computer chip">
	</p><p>144 asynchronous computers on a chip. Idle cores use 100 nW. Active ones use 4 mW, run at 666 Mips, then return to idle. All computers running flat out: 550mW (half a Watt).

	</p><div>
        <p>Check out Chuck's talk at StrangeLoop:
		    <a href="https://www.youtube.com/watch?v=0PclgBd6_Zs">Programming a 144-computer chip to minimize power - Chuck Moore (2013)</a>
            (youtube.com)
		</p><p>And here's the official website:
        <a href="https://www.greenarraychips.com/">GreenArrays, Inc.</a>
        (greenarraychips.com)
        "Ultra-low-powered multi-computer chips with integrated
            peripherals."
        </p><p>Probably the best summary comes from the architecture document,
<a href="https://www.greenarraychips.com/home/documents/greg/PB002-100822-GA-Arch.pdf">GreenArrays Architecture (PDF)</a>
        (greenarraychips.com):
        </p><blockquote>
            "<b>COMPLETE SYSTEMS:</b> We refer to our chips as Multi-Computer Systems because they are, in fact, complete systems. Supply one of our chips with power and a reset signal, and it is up and running. All of our chips can load their software at high speed using a single wire that can be daisy chained for multiple chips; if desired, most can be bootstrapped by a simple SPI flash memory.
            <p>"Contrast this with a Multi-Core CPU, which is not a computing system until other devices such as crystals, memory controllers, memories, and bus controllers have been added.  All of these things consume energy, occupy space, cost money, add complexity, and create bottlenecks.
            </p><p><b>"NO CLOCKS:</b> Most computing devices have one or more clocks that synchronize all operations. When a conventional computer is powered up and waiting to respond quickly to stimuli, clock generation and distribution are consuming energy at a huge rate by our standards, yet accomplishing nothing."
        </p></blockquote>
        <p>It goes on to explain the fine-grained power usage, how each computer
        communicates with its neighbor, and similar statements high-level
        descriptions.
        </p><p>You can buy these chips right now for as little as $20 in quantities
        of 10. The only problem is that to easily make to use of one, you either
        need to buy the $495 development board or make your own. I've found
        precious few examples of people who have done this online.
        </p><p>One rare example is
        <a href="https://web.archive.org/web/20121004044707/http://www.designspark.com/content/hands-144-core-processor">Hands on with a 144 core processor</a>
        (archive.org <i>of designspark.com</i>).
        Article author Andrew Back even has screenshots of the of the
        arrayForth environment (which is basically
        <span>color</span><span>Forth</span>)
        </p><p>The question, of course, is <em>what do you do with this thing?</em>
        </p><p>It may turn out that the answer can be found by looking back into
        computing history. You don't even have to go back very far.
        </p><p>If you read the "Rabbit Hole Alert" under the picture of the surface
        of comet 67p above, then you saw the term "transputer".
        I think it would be very interesting to compare and contrast the
        GreenArrays GA144 chips to the Inmos transputer chips.
        It seems to me, at first glance, that anything those transputers would
        have been suited for ought to be a good fit for a GreenArrays multi-computer chip
        as well.
        </p><hr>
        <p><b>Rabbit Hole Alert 2:</b> Another fun diversion into massively parallel
        computers is one of my favorites: Danny Hillis's
        <a href="https://en.wikipedia.org/wiki/Connection_Machine">Connection Machine</a>
        (wikipedia.org)
        computers featuring a "12-dimensional hypercube" routing design.
        </p><p>Hillis himself is a "human rabbit hole" of inventions, ideas, and
        writings. He's the author of one of my favorite non-fiction books, "The
        Pattern on the Stone," and co-founder of The Long Now Foundation
        (along with some other "human rabbit holes" including the incredible
        writer and thinker, Steward Brand).
        </p><p>One of the projects of the Long Now
        Foundation is the design and creation of the 10,000 year giant
        mechanical <i>Clock of the Long Now</i> which is intended to tick once
        per year and have a cuckoo that comes out once every 1,000 years.
        </p><p>There is also a direct connection between the Long Now and the
        Rosetta spacecraft: Long Now created the "Rosetta disc", an extremely
        clever physical object containing the micro-etched text of over
        a thousand human languages. The Rosetta spacecraft carried a nickel
        prototype of the disc. So that's now sitting on a comet.
        </p><p>As with the previous rabbit hole alert, I could link to all of these
        people and things, but each is part of an unfathomably deep fractal of
        fascinating stuff and I'm afraid you might never come back to finish
        this. But do look them up later!
        </p><hr>
        <p>At any rate, 
        </p><p>The only problem with parallel computers is that we're still
        not that great at programming them.
        </p><p>Heck, we're not even that great at serial programming yet.
	</p></div>
</div>

<div>
    <h2>The future: sustainable low-energy computing and Forth?</h2>
    <p>"If you talk about molecular computers that are circulating in your bloodstream, they aren't going to have very much power and they aren't going to have very much memory and they aren't going to be able to use much energy.
    </p><p>-- Chuck Moore, <i>Programming a 144-computer chip to minimize power</i>, 2013

    </p><div>
        <p>The eventual complete domination of x86 PCs in practically all areas
        of computing, followed by the current rise of powerful ARM CPUs are
        historical computing fact.  Incredible feats of processor engineering
        have made it possible to run what can only be described as
        "supercomputers" on battery power and put them in our pockets.
        </p><p>Trends in both software and hardware have been towards
        ever-increasing layers of complexity.  The layers are very deep and
        very wide.
        </p><p>As I write this, certain popular avenues of computing threaten to
        make every current piece of inefficient software seem absolutely
        <em>frugal</em> by comparison.
        </p><p><i>(Incredibly, we're not even content with the
            supercomputers on our desks and in our hands. So we rely on
            services which work remotely over the Internet on powerful networks
            of computers in huge data centers. We think of this computing as
            cheap or even free because much of it is indirectly paid for with
            advertising dollars.  Paid for, that is, with our attention and
            personal data. Those data centers with their screaming cooling fans
            and backup generators are somewhere else, not in our living rooms.
            It's easy to simply forget how all of this is made possible.)</i>
        </p><p>Increasingly, we rely on massively complex software with that seems
        to have an unending appetite for computing power.
        </p><p><b>But do these trends have to continue?</b>
        </p><p>There is absolutely no reason we have to use increasingly
        inefficient and poorly-constructed software with steeper and steeper
        hardware requirements in the decades to come.
        </p><p>In fact, the reverse could be true.
        </p><p>There are plenty of applications where low energy computing is a
        categorical requirement and I believe these applications will only
        increase.
    </p></div>
</div>

<div>
    <p>Forth-likes could have a strong future as we look towards:
    </p><ul>
        <li>Tiny, ubiquitous computers
        </li><li>Solar power
        </li><li>Heavily constrained VMs
    </li></ul>

    <div>
        <p>There are physical realities (such as the speed of light) which
        ultimately govern the speed at which we can perform a calculation or
        the maximum number of calculations which can be done with a Watt of
        electricity using computers made out of atoms. These are hard limits.
        But there will surely be other plateaus along the way to reaching these
        limits.
        </p><p>Around the year 2006, we saw Dennard scaling
        slow to a crawl.
        <a href="https://en.wikipedia.org/wiki/Dennard_scaling">Dennard scaling</a>
        (wikipedia.org)
         describes the relationship between
        the shrinking size of transistors to the increase of computing speed.
        Simply put, smaller transistors can switch at higher speeds and take
        less voltage.  This scaling law held for many years.  But we reached a
        speed plateau at around 4 GHz because of current leakage and heat.
        </p><p>In
        <a href="http://www.gotw.ca/publications/concurrency-ddj.htm">The Free Lunch Is Over</a>
        (gotw.ca),
        published in Dr. Dobb's Journal in 2005, Herb Sutter writes,
        </p><blockquote>
            "The major processor manufacturers and architectures, from Intel
            and AMD to Sparc and PowerPC, have run out of room with most of
            their traditional approaches to boosting CPU performance. Instead
            of driving clock speeds and straight-line instruction throughput
            ever higher, they are instead turning en masse to hyperthreading
            and multicore architectures."
        </blockquote>
        <p>Multicore processors and increasingly clever hardware architecture
        tricks have continued to provide increases in computing power...but it's
        not the same.
        </p><p>Near the end of the article, Sutter advises:
        </p><blockquote>
            <p>"There are two ways to deal with this sea change toward
            concurrency. One is to redesign your applications for concurrency,
            as above. <b>The other is to be frugal, by writing code that is more
            efficient and less wasteful.</b> This leads to the third interesting
            consequence:
            </p><p>"3. Efficiency and performance optimization will get more, not less, important.
            <b>Those languages that already lend themselves to heavy optimization will find new life</b>; those that don't will need to find ways to compete and become more efficient and optimizable. Expect long-term increased demand for performance-oriented languages and systems."
        </p></blockquote>
        <p>(Emphasis mine.)
        </p><p>For now, we're still eating the remains of that free lunch.
        </p><p>I'm probably fairly rare among programmers in wishing it would end.
        I'd like to see greater emphasis on the craft and art of software.
        I'd like to see us make full and intentional use of the incredible
        power available to us now.
        </p><p>The
        <a href="https://en.wikipedia.org/wiki/Retrocomputing">retrocomputing</a>
        (wikipedia.org)
        hobby has continually shown how much more we could have done with the
        home computers of the 1980s if we had continued to use them.
        In many cases, they've been shown to be able to run programs
        previously thought impossible.
        The things we could do with <em>current</em> hardware are surely
        even more amazing, but it will be perhaps decades before we find
        out.
        <img src="http://ratfactor.com/forth/talkimg/ibm_704.png" alt="chuck moore operating an IBM 704">
        </p><p>In 1958, Chuck Moore created a dirt-simple interpreter on an
        IBM 704. That computer filled a room and cost about 2 million dollars.
        </p><p>I can buy a more powerful computer (minus the awesome control panel
        with lights and switches) today for literal <em>pocket change</em>
        in the form of a "microcontroller", a complete computer on a single
        silicon chip, and write a powerful Forth system for it. That computer
        can run on a coin cell battery or even a tiny solar panel, sipping power
        where the IBM 704 inhaled it.
        </p><p>There has never been a more incredible time for small-scale computing.
        Like the explosion of personal computers in the 1980s, the time is ripe
        for fun, creative, interesting, useful, and very <em>personal</em>
        computers and software.
        </p><p>These tools can do useful work and they can also teach and delight us.
        Ideas like Forth are ripe for rediscovery as we learn exciting new
        ways to compute with arrays of inexpensive, low-power computers.
        </p><p>We can pursue this line of thinking for pragmatic reasons, or just
        because it is beautiful and fun and worth doing for its own sake.
    </p></div>
</div>

<div>
    <p>Chuck Moore is basically retired now, programming and toying with
    software with no deadlines or clients.
    </p><p>It is now on us to take up the mantle of Forth, to champion the values
    of ingenuity, elegance, efficiency, and simplicity.
    
    </p><h2>Forth is...</h2>

    <div>
        <p><b>Simple</b>
        </p><p>To really understand the value of Forth (and <em>especially</em> Chuck Moore's
        later work on Machine Forth and the GreenArrays computers), we must consider the
        difference between <b>"simple"</b> and <b>"easy"</b>.
        </p><p>We were blessed with the ability to speak of this difference by
        Rich Hickey in his brilliant talk,
        <a href="https://www.youtube.com/watch?v=SxdOUGdseq4">"Simple Made Easy" (2011)</a>
        (youtube.com)
        which every developer should see at some time in their life.
        (Or read <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/SimpleMadeEasy.md">the transcript of Simple Made Easy</a>
        (github.com)
        provided by Mattias Nehlsen.)
        </p><p>Forth is not easy. It may not always even be pleasant. But it is certainly simple.
        Forth is one of the <em>simplest</em> programming languages there has ever been.
        </p><p><b>A crafted language</b>
        </p><p>If the best software is truly crafted for problem at hand, then
        it makes sense that an idea programming language would also be
        crafted for the problem at hand.
        </p><p>An absolutely amazing talk about language design,
        Guy Steele's
        <a href="https://www.youtube.com/watch?v=_ahvzDzKdB0">Growing a Language (1998)</a>
        (youtube.com)
        demonstrates how languages are built up from primitives.
        The talk is a performance art and deeply insightful.
        </p><p>Steele helpfully also wrote up a transcript of the talk:
        <a href="https://www.cs.virginia.edu/~evans/cs655/readings/steele.pdf">Growing a Language (PDF)</a>
        (virginia.edu)
        Imagine Steele is saying "Forth" here in place of "Lisp" because
        the point is the same:
        </p><blockquote>
            "Lisp was designed by one man, a smart man, and it works in a way
            that I think he did not plan for. In Lisp, new words defined by the
            user look like primitives and, what is more, all primitives look
            like words defined by the user! In other words, if a user has good
            taste in defining new words, what comes out is a larger language
            that has no seams."
        </blockquote>
        <p>Go <em>forth</em> and create the perfect
        programming language for <em>you</em>!
    </p></div>
</div>

<div>
    <h2>The Legend Confirmed</h2>
    <p><img src="http://ratfactor.com/forth/talkimg/wizard_chuck.png" alt="chuck moore as an adorable wizard"></p><p>I promised I would show you a magic trick at the end of this article.
    </p><p>Behold, a new definition for the integer 4:
    </p><pre>: 4 12 ;
    </pre>
    <p>Which I shall now use in a sentence:
    </p><pre>." The value of 4 is " 4 . CR

<b>The value of 4 is 12</b>
    </pre>
    <p><b>Tada!</b>
</p></div>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Pacific Northwest Tree Octopus (368 pts)]]></title>
            <link>https://zapatopi.net/treeoctopus/</link>
            <guid>36747893</guid>
            <pubDate>Sun, 16 Jul 2023 15:11:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zapatopi.net/treeoctopus/">https://zapatopi.net/treeoctopus/</a>, See on <a href="https://news.ycombinator.com/item?id=36747893">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<article>


<div>
<p title="Help Save The ENDANGERED Pacific Northwest Tree Octopus From EXTINCTION!">
<h3>Help Save The ENDANGERED</h3>

<h3>From EXTINCTION!</h3>
</p>
</div>





<h3>The Pacific Northwest Tree Octopus</h3>
<div>

<p><img src="https://zapatopi.net/treeoctopus/treeocto_enlarged.jpg" width="280" height="373" alt="Tree octopus photo"><br>
Rare photo of the elusive tree octopus<br>
(Enhanced from cropped telephoto)
</p>

<p>The Pacific Northwest tree octopus (<em>Octopus paxarbolis</em>) can be found in the <a href="http://en.wikipedia.org/wiki/Temperate_rainforest">temperate rainforests</a> of the Olympic Peninsula on the west coast of North America. Their habitat lies on the Eastern side of the Olympic mountain range, adjacent to Hood Canal. These solitary cephalopods reach an average size (measured from arm-tip to mantle-tip,) of 30-33 cm. Unlike most other cephalopods, tree octopuses are amphibious, spending only their early life and the period of their mating season in their ancestral aquatic environment. Because of the moistness of the rainforests and specialized skin adaptations, they are able to keep from becoming desiccated for prolonged periods of time, but given the chance they would prefer resting in pooled water.</p>

<p>An intelligent and inquisitive being (it has the largest brain-to-body ratio for any mollusk), the tree octopus explores its arboreal world by both touch and sight. Adaptations its ancestors originally evolved in the three dimensional environment of the sea have been put to good use in the spatially complex maze of the <a href="http://en.wikipedia.org/wiki/Olympic_National_Park">coniferous Olympic rainforests</a>. The challenges and richness of this environment (and the intimate way in which it interacts with it,) may account for the tree octopus's advanced behavioral development. (Some evolutionary theorists suppose that "arboreal adaptation" is what laid the groundwork in primates for the evolution of the human mind.)</p>

<p>Reaching out with one of her eight arms, each covered in sensitive suckers, a tree octopus might grab a branch to pull herself along in a form of locomotion called tentaculation; or she might be preparing to strike at an insect or small vertebrate, such as a frog or rodent, or steal an egg from a bird's nest; or she might even be examining some object that caught her fancy, instinctively desiring to manipulate it with her dexterous limbs (really deserving the title "sensory organs" more than mere "limbs",) in order to better know it.</p>

<p><img src="https://zapatopi.net/treeoctopus/tree_octopus_range_map.png" width="280" height="361" alt="Tree octopus range map"><br>
Map of estimated tree octopus maximum range, including spawning waters
</p>

<p>Tree octopuses have eyesight comparable to humans. Besides allowing them to see their prey and environment, it helps them in inter-octopus relations. Although they are not social animals like us, they display to one-another their emotions through their ability to change the color of their skin: red indicates anger, white fear, while they normally maintain a mottled brown tone to blend in with the background.</p>

<p>The reproductive cycle of the tree octopus is still linked to its roots in the waters of the Puget Sound from where it is thought to have originated. Every year, in Spring, tree octopuses leave their homes in the <a href="http://www.fs.fed.us/r6/olympic/">Olympic National Forest</a> and migrate towards the shore and, eventually, their spawning grounds in Hood Canal. There, they congregate (the only real social time in their lives,) and find mates. After the male has deposited his sperm, he returns to the forests, leaving the female to find an aquatic lair in which to attach her strands of egg-clusters. The female will guard and care for her eggs until they hatch, refusing even to eat, and usually dying from her selflessness. The young will spend the first month or so floating through Hood Canal, Admiralty Inlet, and as far as North Puget Sound before eventually moving out of the water and beginning their adult lives.</p>

</div>


<h3 id="endangered">Why It's Endangered</h3>
<div>

<p><a href="https://zapatopi.net/treeoctopus/route101-large.jpg"><img src="https://zapatopi.net/treeoctopus/route101.jpg" width="280" height="300" title="Route 101 (Google Maps)"></a><br>
Route 101, separating the rainforests of the Olympic Peninsula from Hood Canal
</p>

<p>Although the tree octopus is not officially listed on the Endangered Species List, we feel that it should be added since its numbers are at a critically low level for its breeding needs. The reasons for this dire situation include: decimation of habitat by logging and suburban encroachment; building of roads that cut off access to the water which it needs for spawning; predation by foreign species such as house cats; and booming populations of its natural predators, including the bald eagle and <a href="https://zapatopi.net/bsa/octopus.html">sasquatch</a>. What few that make it to the Canal are further hampered in their reproduction by the growing problem of pollution from farming and residential run-off. Unless immediate action is taken to protect this species and its habitat, the Pacific Northwest tree octopus will be but a memory.</p>

<p>The possibility of Pacific Northwest tree octopus extinction is not an unwarranted fear. Other tree octopus species‚Äîincluding the Douglas octopus and the red-ringed madrona sucker‚Äîwere once abundant throughout the Cascadia region, but have since gone extinct because of threats similar to those faced by <em>paxarbolis</em>, as well as overharvesting by the now-illegal tree octopus trade.</p>

<p><img src="https://zapatopi.net/treeoctopus/cascadia_evening_post-1923-11.jpg" width="280" height="383" alt="Cascadia Evening Post" title="Cover of 1923 'Cascadia Evening Post' featuring woman wearing Tree Octopus hat"><br>
Tree Octopus hat from 1923
</p>

<p>The history of the tree octopus trade is a sad one. Their voracious appetite for bird plumes having exhausted all the worthy species of that family, the fashionistas moved on to cephalopodic accoutrements during the early 20th Century. Tree octopuses became prized by the fashion industry as ornamental decorations for hats, leading greedy trappers to wipe out whole populations to feed the vanity of the fashionable rich. While fortunately this practice has been outlawed, its effects still reverberate today as these millinery deprivations brought tree octopus numbers below the critical point where even minor environmental change could cause disaster.</p>

<p>While efforts were made in the past to preserve remaining tree octopus habitat, these were met with resistance by the timber industry, which has traditionally viewed the tree octopus <a href="https://zapatopi.net/blog/?post=200812287770.an_octopus_in_a_saw-mill">as a nuisance</a>, both because the octopuses favor the valuable, moss-shrouded trees of old growth forests‚Äîpitting conservation needs against lucrative sources of lumber‚Äîand because octopuses hiding among felled trees often gummed up sawmills and stained pulp vats with their ink.</p>

<p><a href="https://zapatopi.net/treeoctopus/bones_hartzell-tree_octopus_exhibit.jpg"><img src="https://zapatopi.net/treeoctopus/bones_hartzell-tree_octopus_exhibit-thumb280.jpg" width="280" height="376" alt="1942 Tree Octopus sideshow exhibit" title="'OCTOPUS: DEVIL OF THE TREES; TERROR OF ALL LOGGERS AND FOREST RANGERS'"></a><br>
Traveling sideshow exhibits, such as this one by Glen "Bones" Hartzell from 1942, demonized tree octopuses to the ignorant masses<br>(Click to enlarge)
</p>

<p>These nuisances led many loggers to regard tree octopuses as bad luck, resulting in the pointless killing of octopuses on sight at logging camps in a misguided attempt at eradicating the troublesome species. Anti-octopus sentiment was so strong among loggers that some even began to fear that the octopuses were prone to attacking humans.</p>

<p>These fears were fueled in no small part by gratuitous stories involving tree octopuses harassing lumberjacks and distressing damsels in Northwestern-themed pulp magazines of the 1930-40s and variously "nipping", "entangling", or "suckering the flesh" of the heroes of men's action magazines of the 1950-60s. (The magazine publishers depended on cheap paper made from wood pulp and were glad to contribute to the anti-octopus propaganda campaign of the timber industry.)</p>

<p>To this day, misunderstanding and fear of these gentle creatures can still be found among many old timers, although education campaigns‚Äîand special octopus-separators installed at sawmills‚Äîhave largely halted the practice of tree octopus eradication.</p>

</div>


<h3>How You Can Help</h3>



<h3>More Tree Octopus Information</h3>
<div>

<p><a href="http://www.cafepress.com/zapatopi/7689324"><img src="https://zapatopi.net/treeoctopus/tree_octopus_wpa_poster_280.png" width="280" height="435" alt="Tree Octopus poster" title="The Cascadian WPA has produced this attractive poster (click to buy)"></a><br>
Posters motivate the citizenry to action! Post them!
</p>

<ul>
<li><a href="https://zapatopi.net/treeoctopus/faq.html">Tree Octopus FAQs</a> ‚Äî Frequently asked questions, now with answers.</li>
<li><a href="https://zapatopi.net/treeoctopus/sightings.html">Tree Octopus Sightings</a> ‚Äî Includes photos of and behavioral research on the Pacific Northwest Tree Octopus and other tree octopus species.</li>
<li><a href="https://zapatopi.net/treeoctopus/media.html">Tree Octopus In The Media</a> ‚Äî appearances of tree octopuses, both real and fictional, in the media and popular culture.</li>
</ul>

<h4>Research On Other Tree Octopus Species:</h4>
<ul>
<li><a href="https://zapatopi.net/blog/?post=200910296120.pitch-chewing_tree_octopuses_of_british_columbia">Pitch-Chewing Tree Octopuses Of British Columbia</a> ‚Äî Octopuses in BC have long been reported chewing the pitch of Sitka spruce like gum, and will even go into the trees to forage for it.</li>
<li><a href="https://zapatopi.net/blog/?post=200809266800.olive_loving_tree_octopuses_of_antiquity">Olive Loving Tree Octopuses Of Antiquity</a> ‚Äî Octopuses in Greece were known since ancient times to climb olive trees to feast on the tasty fruit.</li>
<li><a href="https://zapatopi.net/blog/?post=200903256550.more_on_old_world_tree_octopuses">More On Old World Tree Octopuses</a> ‚Äî Ancient writers, such as Aristotle and Pliny the Elder, tell of octopuses that venture onto land, including one that used a tree to commit burglary.</li>
<li><a href="https://zapatopi.net/blog/?post=200903062770.the_ara-eaters_tree_octopuses_of_polynesia">The Ara-Eaters: Tree Octopuses Of Polynesia</a> ‚Äî Reports from the 1800s tell of island octopuses that are attracted to the fragrant flowers of the pandanus tree.</li>
<li><a href="https://zapatopi.net/blog/?post=201011185710.nicharongorong_tree_octopuses_of_micronesia"><i>Nicharongorong</i>: Tree Octopuses of Micronesia</a> ‚Äî Reports of Palauan tree octopuses that give birth in mangrove trees and eat lizards.</li>
<li><a href="https://zapatopi.net/blog/?post=201902248500.devon_hedge_octopus">Devon Hedge Octopus</a> ‚Äî Species of octopus that once lived in the primeval forests of Devon, UK, until deforestation drove them into the hedges and possibly to extinction.</li>
</ul>

</div>





<p>The author of this article and its subsections is <a href="http://zapatopi.net/">Lyle Zapato</a>.<br>
This site is not associated with any school or educational organization,<br>
other than the Kelvinic University branch of the Wild Haggis Conservation Society.<br>
Not to be confused with the <a href="http://www.youtube.com/watch?v=fAylGe8J-9Y">Pacific Northwest Octopus Tree</a>.</p>

</article>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who employs your doctor? Increasingly, a private equity firm (433 pts)]]></title>
            <link>https://www.nytimes.com/2023/07/10/upshot/private-equity-doctors-offices.html</link>
            <guid>36747572</guid>
            <pubDate>Sun, 16 Jul 2023 14:44:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2023/07/10/upshot/private-equity-doctors-offices.html">https://www.nytimes.com/2023/07/10/upshot/private-equity-doctors-offices.html</a>, See on <a href="https://news.ycombinator.com/item?id=36747572">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2023/07/10/upshot/private-equity-doctors-offices.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[SVGmix is a massive collection of free SVG icons and brand logos (102 pts)]]></title>
            <link>https://svgmix.com/</link>
            <guid>36747335</guid>
            <pubDate>Sun, 16 Jul 2023 14:22:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://svgmix.com/">https://svgmix.com/</a>, See on <a href="https://news.ycombinator.com/item?id=36747335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h3>About Us üëã</h3>

        <p>
          <span>SVGmix</span> is a massive (300K+) collection of free SVG icons and brand logos. Enjoy unrestricted access to our extensive collection, as all downloadable items are available free of charge under the respective copyright holder's license        </p>

                <h4>Upvote us on PH ‚ù§Ô∏è‚Äçüî•</h4>
        <p><a href="https://www.producthunt.com/posts/svgmix?utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-svgmix" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=404903&amp;theme=light" alt="Svgmix - 300K+ free SVG icons, collections and logos | Product Hunt" width="250" height="54"></a>
        </p>
        
      </div></div>]]></description>
        </item>
    </channel>
</rss>