<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Dec 2023 23:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[WebGPU now available for testing in Safari Technology Preview (139 pts)]]></title>
            <link>https://webkit.org/blog/14879/webgpu-now-available-for-testing-in-safari-technology-preview/</link>
            <guid>38737028</guid>
            <pubDate>Fri, 22 Dec 2023 18:34:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/14879/webgpu-now-available-for-testing-in-safari-technology-preview/">https://webkit.org/blog/14879/webgpu-now-available-for-testing-in-safari-technology-preview/</a>, See on <a href="https://news.ycombinator.com/item?id=38737028">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-14879">
            
            

            <div>
                                
                <p><a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API">WebGPU</a> is a new <a href="https://www.w3.org/TR/webgpu/">standards-compliant</a> API that enables high-performance 3D graphics and general-purpose <a href="https://en.wikipedia.org/wiki/Data_parallelism">computations</a> on the Web. WebGPU programs are written in JavaScript but expose GPU functionality, allowing GPU computing to be used in Web content for the first time. Starting in <a href="https://developer.apple.com/safari/technology-preview/">Safari Technology Preview</a> 185, WebGPU can be enabled for early testing and development.</p>
<p>To enable WebGPU, turn on the “WebGPU”, “GPU Process: DOM Rendering” and “GPU Process: Canvas Rendering” feature flags in the <a href="https://developer.apple.com/documentation/safari-developer-tools/feature-flag-settings">Feature Flags</a> tab in Safari Preferences. If you don’t see the Feature Flags tab, you need to first check “<a href="https://developer.apple.com/documentation/safari-developer-tools/enabling-developer-features">Show features for web developers</a>” in the Advanced tab.</p>
<p>Once you have WebGPU enabled in Safari Technology Preview 185, <a href="https://webgpu.github.io/webgpu-samples/samples/particles">try out this example of WebGPU</a>. It utilizes many of the best features of WebGPU.</p>
<figure><video autoplay="" loop="" muted="" src="https://webkit.org/blog-files/webgpu-particles.mov"></video></figure>
<h2>WebGPU JavaScript API</h2>
<p>The WebGPU API is accessed through JavaScript, similar to WebGL.</p>
<h3>Creating a GPUDevice</h3>
<p>In order to use WebGPU, a device must be created. Resources and pipeline state are created from a <code>GPUDevice</code> instance. To create a device with default limits and features which are supported on all devices supporting WebGPU, we can pass zero parameters to the invocations of <code>requestAdapter</code> and <code>requestDevice</code>.</p>
<pre><code><span>const</span> <span>adapter</span> <span>=</span> <span>await</span> <span>navigator</span>.<span>gpu</span>.<span>requestAdapter</span>();
<span>device</span> <span>=</span> <span>await</span> <span>adapter</span>.<span>requestDevice</span>();
</code></pre>
<h3>Configuring a GPUCanvasContext</h3>
<p>The <code>GPUCanvasContext</code> is an interface that allows you to configure how your content will be displayed in the corresponding <code>HTMLCanvas</code> element on the page.</p>
<pre><code><span>context</span> <span>=</span> <span>canvas</span>.<span>getContext</span>(<span>'webgpu'</span>);
<span>const</span> <span>canvasFormat</span> <span>=</span> <span>"bgra8unorm"</span>;

<span>const</span> <span>contextConfiguration</span> <span>=</span> {
    <span>device</span><span>:</span> <span>device</span>,
    <span>format</span><span>:</span> <span>canvasFormat</span>,
    <span>alphaMode</span><span>:</span> <span>'opaque'</span>,
};
<span>context</span>.<span>configure</span>(<span>contextConfiguration</span>);
</code></pre>
<h3>Creating a GPURenderPipeline</h3>
<p>A <code>GPURenderPipeline</code> or a corresponding <code>GPUComputePipeline</code> are used to configure the pipeline state of the graphics driver. This pipeline state is then used in a <code>GPURenderPassEncoder</code> or <code>GPUComputePassEncoder</code> as later illustrated.</p>
<pre><code><span>const</span> <span>shaderModule</span> <span>=</span> <span>device</span>.<span>createShaderModule</span>({ <span>code</span><span>:</span> <span>wgslSource</span> });
<span>const</span> <span>vertexStageDescriptor</span> <span>=</span> { <span>module</span><span>:</span> <span>shaderModule</span>, <span>entryPoint</span><span>:</span> <span>"vsmain"</span> };
<span>const</span> <span>fragmentStageDescriptor</span> <span>=</span> { <span>module</span><span>:</span> <span>shaderModule</span>, <span>entryPoint</span><span>:</span> <span>"fsmain"</span> };
<span>const</span> <span>renderPipelineDescriptor</span> <span>=</span> {
    <span>layout</span><span>:</span> <span>'auto'</span>,
    <span>vertex</span><span>:</span> <span>vertexStageDescriptor</span>,
    <span>fragment</span><span>:</span> <span>fragmentStageDescriptor</span>,
    <span>primitive</span><span>:</span> {<span>topology</span><span>:</span> <span>"triangle-list"</span> },
};
<span>const</span> <span>renderPipeline</span> <span>=</span> <span>device</span>.<span>createRenderPipeline</span>(<span>renderPipelineDescriptor</span>);
</code></pre>
<h3>Issuing draw calls</h3>
<p>A <code>GPURenderPassEncoder</code> is created to send draw calls to the graphics driver. In the below example, we draw a simple triangle which contains three vertices. A <code>GPURenderPassEncoder</code> can also draw multiple instances of the same geometry or draw from an offset of a vertex buffer.</p>
<pre><code><span>const</span> <span>colorAttachmentDescriptor</span> <span>=</span> {
    <span>view</span><span>:</span> <span>renderAttachment</span>,
    <span>loadOp</span><span>:</span> <span>"clear"</span>,
    <span>storeOp</span><span>:</span> <span>"store"</span>,
    <span>clearColor</span><span>:</span> { <span>r</span><span>:</span> <span>0.15</span>, <span>g</span><span>:</span> <span>0.15</span>, <span>b</span><span>:</span> <span>0.5</span>, <span>a</span><span>:</span> <span>1</span> }
};
<span>const</span> <span>renderPassDescriptor</span> <span>=</span> { <span>colorAttachments</span><span>:</span> [<span>colorAttachmentDescriptor</span>] };
<span>const</span> <span>commandEncoder</span> <span>=</span> <span>device</span>.<span>createCommandEncoder</span>();
<span>const</span> <span>renderPassEncoder</span> <span>=</span> <span>commandEncoder</span>.<span>beginRenderPass</span>(<span>renderPassDescriptor</span>);
<span>renderPassEncoder</span>.<span>setPipeline</span>(<span>renderPipeline</span>);
<span>const</span> <span>vertexBufferSlot</span> <span>=</span> <span>0</span>;
<span>renderPassEncoder</span>.<span>setVertexBuffer</span>(<span>vertexBufferSlot</span>, <span>vertexBuffer</span>, <span>0</span>);
<span>renderPassEncoder</span>.<span>draw</span>(<span>3</span>, <span>1</span>, <span>0</span>, <span>0</span>); <span>// 3 vertices, 1 instance, 0th vertex, 0th instance.
</span><span>renderPassEncoder</span>.<span>end</span>();
<span>const</span> <span>commandBuffer</span> <span>=</span> <span>commandEncoder</span>.<span>finish</span>();
<span>const</span> <span>queue</span> <span>=</span> <span>device</span>.<span>queue</span>;
<span>queue</span>.<span>submit</span>([<span>commandBuffer</span>]);
</code></pre>
<h2>WebGPU Shading Language</h2>
<p>WebGPU introduces WGSL, a platform independent shading language for the web. Here is an example of a WGSL shader source that would be passed in place of <code>wgslSource</code> in the above API call:</p>
<pre><code><span>const</span> <span>wgslSource</span> <span>=</span> `
    <span>struct</span> <span>Vertex</span> {
        @<span>builtin</span>(<span>position</span>) <span>Position</span><span>:</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>,
        @<span>location</span>(<span>0</span>) <span>color</span><span>:</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>,
    }

    @<span>vertex</span> <span>fn</span> <span>vsmain</span>(@<span>builtin</span>(<span>vertex_index</span>) <span>VertexIndex</span><span>:</span> <span>u32</span>) <span>-</span><span>&gt;</span> <span>Vertex</span>
    {
        <span>var</span> <span>pos</span><span>:</span> <span>array</span><span>&lt;</span><span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>, <span>3</span><span>&gt;</span> <span>=</span> <span>array</span><span>&lt;</span><span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>, <span>3</span><span>&gt;</span>(
            <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>( <span>0.0</span>,  <span>0.5</span>),
            <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>-</span><span>0.5</span>, <span>-</span><span>0.5</span>),
            <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>( <span>0.5</span>, <span>-</span><span>0.5</span>));
        <span>var</span> <span>vertex_out</span> <span>:</span> <span>Vertex</span>;
        <span>vertex_out</span>.<span>Position</span> <span>=</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>pos</span>[<span>VertexIndex</span>], <span>0.0</span>, <span>1.0</span>);
        <span>vertex_out</span>.<span>color</span> <span>=</span> <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>pos</span>[<span>VertexIndex</span>] <span>+</span> <span>vec2</span><span>&lt;</span><span>f32</span><span>&gt;</span>(<span>0.5</span>, <span>0.5</span>), <span>0.0</span>, <span>1.0</span>);
        <span>return</span> <span>vertex_out</span>;
    }

    @<span>fragment</span> <span>fn</span> <span>fsmain</span>(<span>in</span><span>:</span> <span>Vertex</span>) <span>-</span><span>&gt;</span> @<span>location</span>(<span>0</span>) <span>vec4</span><span>&lt;</span><span>f32</span><span>&gt;</span>
    {
        <span>return</span> <span>in</span>.<span>color</span>;
    }
`;
</code></pre>
<h2>Try WebGPU and file bugs!</h2>
<p>We’re very excited to have an early version of WebGPU and WGSL in the latest version of Safari Technology Preview. Please do try it out. Check out the <a href="https://webgpu.github.io/webgpu-samples/samples/helloTriangle">public repository of WebGPU samples</a>. And file bugs or issues you discover at <a href="http://bugs.webkit.org/">bugs.webkit.org</a>.</p>

                            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Granting Pardon for the Offense of Simple Possession of or Use of Marijuana (264 pts)]]></title>
            <link>https://www.whitehouse.gov/briefing-room/presidential-actions/2023/12/22/a-proclamation-on-granting-pardon-for-the-offense-of-simple-possession-of-marijuana-attempted-simple-possession-of-marijuana-or-use-of-marijuana/</link>
            <guid>38736919</guid>
            <pubDate>Fri, 22 Dec 2023 18:22:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/12/22/a-proclamation-on-granting-pardon-for-the-offense-of-simple-possession-of-marijuana-attempted-simple-possession-of-marijuana-or-use-of-marijuana/">https://www.whitehouse.gov/briefing-room/presidential-actions/2023/12/22/a-proclamation-on-granting-pardon-for-the-offense-of-simple-possession-of-marijuana-attempted-simple-possession-of-marijuana-or-use-of-marijuana/</a>, See on <a href="https://news.ycombinator.com/item?id=38736919">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	


	<div>
								


<div><p>&nbsp; &nbsp; &nbsp;In Proclamation 10467 of October 6, 2022 (Granting Pardon for the Offense of Simple Possession of Marijuana), I exercised my authority under the Constitution to pardon individuals who committed or were convicted of the offense of simple possession of marijuana in violation of the Controlled Substances Act and section 48–904.01(d)(1) of the Code of the District of Columbia (D.C. Code).&nbsp; As I have said before, convictions for simple possession of marijuana have imposed needless barriers to employment, housing, and educational opportunities.&nbsp; Through this proclamation, consistent with the grant of Proclamation 10467, I am pardoning additional individuals who may continue to experience the unnecessary collateral consequences of a conviction for simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana.&nbsp; Therefore, acting pursuant to the grant of authority in Article&nbsp;II, Section 2, of the Constitution of the United States, I, Joseph&nbsp;R. Biden Jr., do hereby grant a full, complete, and unconditional pardon to all current United States citizens and lawful permanent residents who, on or before the date of this proclamation, committed or were convicted of the offense of simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana, regardless of whether they have been charged with or prosecuted for these offenses on or before the date of this proclamation, in violation of:</p><p>(1) &nbsp;section 844 of title 21, United States Code, section 846 of title 21, United States Code, and previous provisions in the United States Code that prohibited simple possession of marijuana or attempted simple possession of marijuana;&nbsp;</p><p>(2) &nbsp;section 48-904.01(d)(1) of the D.C. Code and previous provisions in the D.C. Code that prohibited simple possession of marijuana;</p><p>(3) &nbsp;section 48-904.09 of the D.C. Code and previous provisions in the D.C. Code that prohibited attempted simple possession of marijuana; and</p><p>(4) &nbsp;provisions in the Code of Federal Regulations, including as enforced under the United States Code, that prohibit only the simple possession or use of marijuana on Federal properties or installations, or in other locales, as currently or previously codified, including but not limited to 25 C.F.R. 11.452(a); 32 C.F.R. 1903.12(b)(2); 36 C.F.R. 2.35(b)(2); 36 C.F.R. 1002.35(b)(2); 36 C.F.R. 1280.16(a)(1); 36 C.F.R. 702.6(b); 41 C.F.R. 102-74.400(a); 43 C.F.R. 8365.1-4(b)(2); and 50 C.F.R. 27.82(b)(2).</p></div>



<div><p>&nbsp;&nbsp;&nbsp;&nbsp; My intent by this proclamation is to pardon only the offenses of simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana in violation of the Federal and D.C. laws set forth in paragraphs (1) through (3) of this proclamation, as well as the provisions in the Code of Federal Regulations consistent with paragraph (4) of this proclamation, and not any other offenses involving other controlled substances or activity beyond simple possession of marijuana, attempted simple possession of marijuana, or use of marijuana, such as possession of marijuana with intent to distribute or driving offenses committed while under the influence of marijuana. &nbsp;This pardon does not apply to individuals who were non-citizens not lawfully present in the United States at the time of their offense.</p><p>&nbsp;&nbsp;&nbsp;&nbsp; Pursuant to the procedures in Proclamation 10467, the Attorney General, acting through the Pardon Attorney, shall review all properly submitted applications for certificates of pardon and shall issue such certificates of pardon to eligible applicants in due course.&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp; IN WITNESS WHEREOF, I have hereunto set my hand this twenty-second day of December, in the year of our Lord two&nbsp;thousand&nbsp;twenty-three, and of the Independence of the United&nbsp;States of&nbsp;America the two&nbsp;hundred and forty-eighth.</p></div>



<p>JOSEPH R. BIDEN JR.</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database Isolation Is Broken and You Should Care (114 pts)]]></title>
            <link>https://materializedview.io/p/database-isolation-is-broken-you-should-care</link>
            <guid>38736904</guid>
            <pubDate>Fri, 22 Dec 2023 18:20:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://materializedview.io/p/database-isolation-is-broken-you-should-care">https://materializedview.io/p/database-isolation-is-broken-you-should-care</a>, See on <a href="https://news.ycombinator.com/item?id=38736904">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>This month saw several related posts on </span><a href="https://en.wikipedia.org/wiki/ACID" rel="">ACID</a><span> (</span><a href="https://en.wikipedia.org/wiki/Atomicity_(database_systems)" rel="">atomicity</a><span>, </span><a href="https://en.wikipedia.org/wiki/Consistency_(database_systems)" rel="">consistency</a><span>, </span><a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)" rel="">isolation</a><span>, </span><a href="https://en.wikipedia.org/wiki/Durability_(database_systems)" rel="">durability</a><span>) database properties.</span></p><p><a href="https://www.linkedin.com/in/tony-solomonik-646819121" rel="">Tony Solomonik</a><span> writes a database using </span><a href="https://www.gnu.org/software/bash/" rel="">bash</a><span> in his </span><a href="https://tontinton.com/posts/database-fundementals/" rel="">database fundamentals</a><span> post. bashdb is a fascinating exercise, but also demonstrates why databases are so complex (ACID, fsync, B+ trees, LSMs, consensus, replication, and so on). Start here to brush up on the basics.</span></p><p><span>Next, </span><a href="https://www.linkedin.com/in/gwenshapira/" rel="">Gwen Shapira</a><span>, co-founder of </span><a href="https://www.thenile.dev/" rel="">Nile</a><span> [$], posted </span><a href="https://www.thenile.dev/blog/transaction-isolation-postgres" rel="">Transaction Isolation in Postgres, explained</a><span>. Though the headline says Postres, the post is mostly about database isolation levels (the </span><em>I</em><span> in ACID). She describes the various isolation levels in </span><a href="https://en.wikipedia.org/wiki/SQL-92" rel="">ANSI SQL ‘92</a><span> and shows the edge cases that break them.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png" width="1228" height="534" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:534,&quot;width&quot;:1228,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:87876,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce5c678-ea0c-4df9-b1d4-20fab2223350_1228x534.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Here’s where things get interesting. Gwen’s post says that there are many anomalies that aren’t classified in the ANSI SQL ‘92 spec.</p><blockquote><p><span>It turned out that defining a perfect state and then defining some states where certain anomalies can happen has this fundamental problem: There may be anomalies you didn't think of. So, only three years after the SQL92 standard came out, a very impressive team of researchers published </span><strong><a href="https://arxiv.org/pdf/cs/0701157.pdf" rel="">A Critique of ANSI SQL Isolation Levels (1995)</a></strong><span>. The paper introduced a whole collection of anomalies that weren't specified in the standard, and therefore were technically allowed at the Serializable level.</span></p></blockquote><p>These new-found anomalies make database isolation ambiguous:</p><blockquote><p>Different databases support different isolation levels, and the same isolation level can mean different things in different databases.</p></blockquote><p><span>This leads to the final post I want to call out: </span><a href="https://jepsen.io/analyses/mysql-8.0.34" rel="">Jepsen’s MySQL Analysis</a><span>. Jepsen is a project run by Kyle Kingsbury alter ego, </span><a href="https://aphyr.com/about" rel="">Aphyr</a><span>. Kyle and his team use Jepsen to break databases. Kyle and </span><a href="https://www.linkedin.com/in/peteralvaro/" rel="">Peter Avlaro</a><span>’s latest post finds anomalies with MySQL and Amazon RDS.</span></p><blockquote><p><em><span>We revisit Kleppmann’s 2014 </span><a href="https://github.com/ept/hermitage/blob/master/mysql.md" rel="">Hermitage</a><span> and confirm that MySQL’s Repeatable Read still allows G2-item, G-single, and lost update. Using our transaction consistency checker </span><a href="https://github.com/jepsen-io/elle" rel="">Elle</a><span>, we show that MySQL Repeatable Read also violates internal consistency. Furthermore, it violates Monotonic Atomic View: transactions can observe some of another transaction’s effects, then later fail to observe other effects of that same transaction. We demonstrate violations of ANSI SQL’s requirements for Repeatable Read.</span></em></p></blockquote><p>Similar to Gwen’s post, they find anomalies and that don’t adhere to the ANSI ‘92 spec. But the behavior they find is much worse than PostgreSQL’s behavior. If you’re using MySQL, I recommend reading the post; they provide suggestions to mitigate some of the issues.</p><p>But what caught my eye was their plea to the standards bodies:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png" width="1456" height="872" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:872,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:363384,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f8170b4-97c8-4441-8505-e9100af038a6_1590x952.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Jepsen’s call for better isolation levels mirror Gwen’s. ANSI ‘92 database isolation levels clearly aren’t cutting it.</p><p><span>I’m going to assume you, dear reader, are not part of the standards body. So your takeaway from all this is that your database might not behaving as you expect. But do you care? </span><a href="https://twitter.com/justinjaffray/status/1737506863455854750" rel="">This thread</a><span> says you don’t.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png" width="1372" height="242" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:242,&quot;width&quot;:1372,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:67801,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80eb7675-8db0-4379-8616-b88846c3caac_1372x242.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://twitter.com/justinjaffray" rel="">Justin</a><span>’s point is that we’ve been living with these “broken” databases for a long time and no one noticed.</span></p><blockquote><p>that said: it took one of the greatest software minds of a generation to prove that one of the most popular databases of all time was broken. kind of calls into question whether anyone ever cared about that brokenness!</p></blockquote><p>The problem I have with Justin’s argument is that the kinds of anomalies in these posts are hard for developers to complain about. They manifest themselves only very occasionally, and are very difficult to detect.</p><p>Developers that are bitten by database anomalies often don’t know. Application exhibits some weird anomaly, which developers can’t reproduce. Engineers have to just throw up their hands, shrug, and move on. They never even know the database was the culprit.</p><p><span>I have been personally bitten by bugs like this while working on payments </span><a href="https://go.wepay.com/" rel="">my last job</a><span>. Payments processing is a very precise thing (indeed, it’s the example Gwen uses in her post and is what </span><a href="https://materializedview.io/p/durable-execution-justifying-the-bubble" rel="">durable execution frameworks</a><span> always demonstrate). Our double entry book keeping log would no longer line up, databases would misalign, or some other oddity would occur. Such bugs resulted in hours of fruitless debugging and unhappy customers. These are things that I, and many engineers, most definitely do care about.</span></p><p>Jepsen is doing us a favor by showing us that the almighty OLTP database is in fact fallible. Make sure you understand how your database behaves (as best you can) and act accordingly. Caveat emptor.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Polymers capable of killing bacteria without inducing antibiotic resistance (120 pts)]]></title>
            <link>https://today.tamu.edu/2023/12/21/texas-am-team-develops-polymers-that-can-kill-bacteria/</link>
            <guid>38736798</guid>
            <pubDate>Fri, 22 Dec 2023 18:07:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://today.tamu.edu/2023/12/21/texas-am-team-develops-polymers-that-can-kill-bacteria/">https://today.tamu.edu/2023/12/21/texas-am-team-develops-polymers-that-can-kill-bacteria/</a>, See on <a href="https://news.ycombinator.com/item?id=38736798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div role="region" aria-label="Texas A&amp;M Team Develops Novel Antibacterial Polymers That Can Kill Bacteria content 0">
<figure><p><img decoding="async" loading="lazy" src="https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-350x233.jpg" alt="A photo of a petri dish with bacteria in a science lab." width="2400" height="1597" srcset="https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-350x233.jpg 350w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-1024x681.jpg 1024w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-768x511.jpg 768w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-1536x1022.jpg 1536w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-2048x1363.jpg 2048w, https://today.tamu.edu/wp-content/uploads/2023/12/bacteria-colonies-in-petri-dish-getty-500x333.jpg 500w" sizes="(max-width: 2400px) 100vw, 2400px"></p><div><figcaption><hr><p>Getty Images</p></figcaption></div></figure>

<p>Antibiotic-resistant bacteria have become a rapidly growing threat to public health. Each year, they account for more than 2.8 million infections, according to the U.S. Centers for Disease Control and Prevention. Without new antibiotics, even common injuries and infections harbor the potential to become lethal.</p>
<p>Scientists are now one step closer to eliminating that threat, thanks to a Texas A&amp;M University-led collaboration that has developed a new family of polymers capable of killing bacteria without inducing antibiotic resistance by disrupting the membrane of these microorganisms.</p>
<p>“The new polymers we synthesized could help fight antibiotic resistance in the future by providing antibacterial molecules that operate through a mechanism against which bacteria do not seem to develop resistance,” said&nbsp;<a href="https://www.chem.tamu.edu/faculty/quentin-michaudel/" target="_blank" rel="noopener">Dr. Quentin Michaudel</a>, an assistant professor in the&nbsp;<a href="https://artsci.tamu.edu/chemistry/index.html" target="_blank" rel="noopener">Department of Chemistry</a>&nbsp;and lead investigator in the research,&nbsp;<a href="https://www.pnas.org/doi/10.1073/pnas.2311396120" target="_blank" rel="noopener">published Dec. 11</a>&nbsp;in the&nbsp;<a href="https://www.pnas.org/" target="_blank" rel="noopener"><em>Proceedings of the National Academy of Sciences</em>&nbsp;(PNAS)</a>.</p>
<p>Working at the interface of organic chemistry and polymer science, the&nbsp;<a href="https://www.michaudellab.org/" target="_blank" rel="noopener">Michaudel Laboratory</a> was able to synthesize the new polymer by carefully designing a positively charged molecule that can be stitched many times to form a large molecule made of the same repeating charged motif using a carefully selected catalyst called AquaMet. According to Michaudel, that catalyst proves key, given that it has to tolerate a high concentration of charges and also be water-soluble — a feature he describes as uncommon for this type of process.</p>
<p>After achieving success, the Michaudel Lab put its polymers to the test against two main types of antibiotic-resistant bacteria — E. coli and Staphylococcus aureus (MRSA) — in collaboration with <a href="https://www.umass.edu/engineering/about/directory/jessica-schiffman" target="_blank" rel="noopener">Dr. Jessica Schiffman’s</a> group at the University of Massachusetts Amherst. While awaiting those results, the researchers also tested their polymers’ toxicity against human red blood cells.</p>
</div>
<div role="region" aria-label="Texas A&amp;M Team Develops Novel Antibacterial Polymers That Can Kill Bacteria content 1">
<p>“A common issue with antibacterial polymers is a lack of selectivity between bacteria and human cells when targeting the cellular membrane,” Michaudel explained. “The key is to strike a right balance between effectively inhibiting bacteria growth and killing several types of cells indiscriminately.”</p>
<p>Michaudel credits the multidisciplinary nature of scientific innovation and the generosity of dedicated researchers across the Texas A&amp;M campus and country as factors in his team’s success in determining the perfect catalyst for their molecule assembly.</p>
<p>“This project was several years in the making and would not have been possible without the help of several groups, in addition to our UMass collaborators,” Michaudel said. “For instance, we had to ship some samples to the&nbsp;<a href="https://www.letterilab.com/" target="_blank" rel="noopener">Letteri Lab</a>&nbsp;at the University of Virginia to determine the length of our polymers, which required the use of an instrument that few labs in the country have. We are also tremendously grateful to [biochemistry Ph.D. candidate] Nathan Williams and&nbsp;<a href="https://bcbp.tamu.edu/people/pellois-jean-philippe/" target="_blank" rel="noopener">Dr. Jean-Philippe Pellois</a>&nbsp;here at Texas A&amp;M, who provided their expertise in our assessment of toxicity against red blood cells.”</p>
<p>Michaudel says the team will now focus on improving the activity of its polymers against bacteria — specifically, their selectivity for bacterial cells versus human cells — before moving on to <em>in vivo</em>&nbsp;assays.</p>
<p>“We are in the process of synthesizing a variety of analogs with that exciting goal in mind,” he said.</p>
</div>
<div role="region" aria-label="Texas A&amp;M Team Develops Novel Antibacterial Polymers That Can Kill Bacteria content 2">
<p>The team’s paper, which features Michaudel Lab member and Texas A&amp;M chemistry Ph.D. graduate Dr. Sarah Hancock ’23 as first author, can be <a href="https://www.pnas.org/doi/10.1073/pnas.2311396120" target="_blank" rel="noopener">viewed online</a>&nbsp;along with related figures and captions. Other key contributors from the Michaudel Lab are chemistry graduate student An Tran ’23, postdoctoral scholar Dr. Arunava Maity and former postdoctoral scholar Dr. Nattawut Yuntawattana, who is now an assistant professor of materials science at Kasetsart University in Thailand.</p>
<p>This research was funded primarily by Michaudel’s&nbsp;<a href="https://www.nigms.nih.gov/Research/mechanisms/MIRA/Pages/default.aspx" target="_blank" rel="noopener">National Institutes of Health Maximizing Investigators’ Research Award (MIRA)</a>&nbsp;through the&nbsp;<a href="https://www.nigms.nih.gov/" target="_blank" rel="noopener">National Institute of General Medical Sciences</a>.</p>
<p>A native of La Rochelle, France, Michaudel joined the Texas A&amp;M Chemistry faculty in 2018 and holds a joint appointment in the <a href="https://engineering.tamu.edu/materials/index.html" target="_blank" rel="noopener">Department of Materials Science and Engineering</a>. In addition to an NIH MIRA in 2020, his career honors to date include a 2022 National Science Foundation Faculty Early Career Development (CAREER) Award, a 2022 American Chemical Society Polymeric Materials: Science and Engineering (PMSE) Young Investigator Award and a 2021 Thieme Chemistry Journals Award.</p>
<p>Learn more about Michaudel and his research at <a href="https://www.michaudellab.org/" target="_blank" rel="noopener">michaudellab.org</a>.</p>
</div>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[3500 arrested in global cybercrime crackdown (103 pts)]]></title>
            <link>https://www.scmagazine.com/news/3500-arrested-300m-seized-in-global-cybercrime-crackdown</link>
            <guid>38736525</guid>
            <pubDate>Fri, 22 Dec 2023 17:36:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scmagazine.com/news/3500-arrested-300m-seized-in-global-cybercrime-crackdown">https://www.scmagazine.com/news/3500-arrested-300m-seized-in-global-cybercrime-crackdown</a>, See on <a href="https://news.ycombinator.com/item?id=38736525">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Police in 34 countries arrested 3500 people and seized assets worth $300 million in the latest iteration of what has become an annual coordinated global crackdown on cybercrime.</p><p><a href="https://www.interpol.int/en/News-and-Events/News/2023/USD-300-million-seized-and-3-500-suspects-arrested-in-international-financial-crime-operation">According to Interpol</a>, Operation HAECHI IV ran from July to December and targeted seven types of scams: voice phishing, romance scams, online sextortion, investment fraud, money laundering associated with illegal online gambling, business email compromise fraud, and e-commerce fraud.</p><p>As a result of the operation, authorities blocked 82,112 suspicious bank accounts, seizing a total of $199 million in hard currency and a further $101 million worth of virtual assets.</p><p>Interpol’s executive director of police services, Stephen Kavanagh, said the “staggering” sum seized was a clear illustration of the incentives that were driving an explosive growth in transnational organized crime.</p><p>“This represents the savings and hard-earned cash of victims,” he said. “This vast accumulation of unlawful wealth is a serious threat to global security and weakens the economic stability of nations worldwide.”</p><p>Interpol said Operation HAECHI IV involved investigators working together to detect online fraud and freeze associated bank and virtual asset service provider accounts using Interpol’s Global Rapid Intervention of Payments (I-GRIP), a stop-payment mechanism which helps countries work together to block criminal proceeds.</p><p>Interpol helped frontline officers identify 367 virtual asset accounts linked to transnational organized crime. Assets in those accounts have been frozen as local police continue their investigations.</p><h2>Dragnet pulls in more AI-powered crime</h2><p>In one case resulting from the operation, Filipino and Korean authorities worked together to apprehend a “high-profile online gambling criminal” who was arrested in Manila after spending two years on the run from Korea's National Police Agency. The illegal gambling operation the man allegedly ran was dismantled.</p><p>Interpol published two “purple notices” – warnings about emerging digital investment fraud practices – during the operation.</p><p>One alerted police around the world to a new scam detected in Korea involving the sale of non-fungible tokens (NFTs) with promises of huge returns, which turned out to be a <a href="https://www.scmagazine.com/analysis/crypto-nft-losses-believed-to-hit-25-trillion-says-industry-researcher">“rug pull” scam</a> where the developers abruptly abandon a project and investors lose their money.</p><p>The second purple notice warned about the use of AI and deep fake technology to lend credibility to scams by enabling criminals to hide their identities and to pretend to be a family member, friend, or love interests of the person they are attempting to dupe.</p><p>“The UK leg of the operation reported several cases where AI-generated synthetic content was used to deceive, defraud, harass, and extort victims, particularly through impersonation scams, online sexual blackmail, and investment fraud,” Interpol said.</p><p>“Cases also involved the impersonation of people known to the victims through voice cloning technology.”</p><p>Investment fraud, business email compromise and e-commerce fraud accounted for 75 per cent of cases investigated during the operation.</p><h2>Arrests and seizures keep growing</h2><p>A similar operation last year, <a href="https://www.scmagazine.com/brief/interpol-cybercrime-crackdown-leads-to-seizure-of-130m">HAECHI III</a>, netted almost 1000 arrests and $130 million in assets.</p><p>“HAECHI IV’s 200 per cent surge in arrests shows the persistent challenge of cyber-enabled crime, reminding us to stay alert and keep refining our tactics against online fraud, which is why INTERPOL operations like this are so important” Kavanagh said.</p><p>The first operation in the series, HAECHI-I, involved police from nine countries in Asia working together between September 2020 and March 2021 to make 585 arrests and seize $83 million.</p><p>Interpol’s head of National Central Bureau in Korea, Kim Dong Kwon, praised the international policing effort that led to the increased results achieved by HAECHI IV.</p><p>“Despite criminals' endeavors to gain illicit advantages through contemporary trends, they will eventually be apprehended and face due punishment. To accomplish this, Project HAECHI will consistently evolve and expand its scope.”</p><p>As SentinalOne explained in a 2021 <a href="https://www.sentinelone.com/blog/the-good-the-bad-and-the-ugly-in-cybersecurity-week-49-3/">post about HAECHI-II</a>: in Korea, Haechi is a popular mythical animal widely used as a symbol of justice. The countries participating in this year’s operation were: Argentina, Australia, Brunei, Cambodia, Cayman Islands, Ghana, India, Indonesia, Ireland, Japan, Kyrgyzstan, Laos, Liechtenstein, Malaysia, Maldives, Mauritius, Nigeria, Pakistan, Philippines, Poland, Korea, Romania, Seychelles, Singapore, Slovenia, South Africa, Spain, Sweden, Thailand, United Arab Emirates, United Kingdom, United States and Vietnam. Hong Kong also participated.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SymbOS Z80 multitasking operating system (123 pts)]]></title>
            <link>http://www.symbos.de/</link>
            <guid>38736054</guid>
            <pubDate>Fri, 22 Dec 2023 16:57:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.symbos.de/">http://www.symbos.de/</a>, See on <a href="https://news.ycombinator.com/item?id=38736054">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[From Nand to Tetris: Building a Modern Computer from First Principles (449 pts)]]></title>
            <link>https://www.nand2tetris.org</link>
            <guid>38735066</guid>
            <pubDate>Fri, 22 Dec 2023 15:31:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nand2tetris.org">https://www.nand2tetris.org</a>, See on <a href="https://news.ycombinator.com/item?id=38735066">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="comp-j847hr0w" data-testid="richTextElement"><h6><span><span><span><span><span>And of the book </span></span></span></span><a href="https://www.amazon.com/Elements-Computing-Systems-Building-Principles/dp/0262640686/ref=ed_oe_p" target="_blank"><span><span><span><span><span>The Elements of Computing Systems,&nbsp;</span></span></span></span></span></a><span><span><span><span>By </span></span></span></span><a href="http://www.cs.huji.ac.il/~noam/" target="_blank"><span><span><span><span><span>Noam Nisan</span></span></span></span></span></a><span><span><span><span> and</span><span> </span><a href="http://www.shimonschocken.com/" target="_blank"><span><span>Shimon Schocken</span></span></a><span> (MIT Press)</span></span></span></span></span></h6></p><div id="comp-j847if12" data-testid="richTextElement"><p><span><span><span><span>The site contains all the lectures, project materials and tools necessary for building a general-purpose computer system and a modern software hierarchy from the ground up.<p>

The materials are aimed at students, instructors, and self-learners. Everything is free and open-source, as long as you operate in a non-profit, educational setting.</p></span></span></span></span><br>
&nbsp;</p>

<p><span><span><span><span>The materials also support two on-line courses:&nbsp;</span></span></span><span><span><span><span><a href="https://www.coursera.org/learn/build-a-computer" target="_blank" rel="noreferrer noopener">Nand2Tetris Part</a></span></span></span></span><span><span><span><a href="https://www.coursera.org/learn/build-a-computer" target="_blank" rel="noreferrer noopener"><span><span> </span></span></a></span></span></span><span><span><span><span><a href="https://www.coursera.org/learn/build-a-computer" target="_blank" rel="noreferrer noopener">I</a></span></span><span><span><span> </span></span></span><span>(hardware projects/chapters 1-6), and&nbsp;</span><a href="https://www.coursera.org/learn/nand2tetris2" target="_blank" rel="noreferrer noopener"><span><span>Nand2Tetris Part II</span></span></a></span></span><span><span><span> (software projects/chapters 7-12).</span></span></span></span></p>

<p><span><span>​</span></span></p>

<p><span><span><span><span>Nand to Tetris courses are taught at 400+ universities, high schools, and bootcamps. The students who take them range from high schoolers to Ph.D. students to Google engineers. Here is the&nbsp;</span></span></span><a href="https://drive.google.com/file/d/1EWCOVIcg0-dX0XtL3KwNyra6jzMogXLL/view?usp=sharing" target="_blank" rel="noreferrer noopener"><span><span><span><span>extended course syllabus.</span></span></span></span></a></span></p>

<p><span><span>​</span></span></p>

<p><span><span><span><span><span><span>Instructors:</span></span></span></span></span><span><span><span>&nbsp;For additional course materials, contact <a data-auto-recognition="true" href="mailto:schocken@gmail.com">schocken@gmail.com</a></span></span></span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Hyperloop was always a scam (151 pts)]]></title>
            <link>https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam</link>
            <guid>38734909</guid>
            <pubDate>Fri, 22 Dec 2023 15:16:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam">https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam</a>, See on <a href="https://news.ycombinator.com/item?id=38734909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic" width="1200" height="675" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/07c65282-6c6f-4629-87c7-58ed30f27658.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:675,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:31189,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07c65282-6c6f-4629-87c7-58ed30f27658.heic 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Original Hyperloop concept image.</figcaption></figure></div><p><span>Ten years after Elon Musk unveiled the white paper for the vacuum-tube transport system he dubbed the Hyperloop, it’s time to drive the final nail into its coffin. Earlier this year, </span><em>Motley Fool</em><span> was already reporting that </span><a href="https://www.fool.com/investing/2023/02/21/hyperloop-startups-are-dying-a-quiet-death/" rel="">things were tough in Hyperloop world</a><span> with startups “dying a quiet death” as higher interest rates meant investors weren’t going for projects that were clearly never going to pay off.</span></p><p><span>Hyperloop One, previously known as </span><em>Virgin</em><span> Hyperloop One, was struggling too. Last year it finally had to admit its passenger tube dreams were never going to be realized, so it tried to </span><a href="https://www.bbc.com/news/technology-60478719" rel="">convince some Emiratis</a><span> that narrow, low-capacity tubes were actually going to be a great way of moving cargo. Well, they clearly didn’t buy it for long because yesterday </span><em>Bloomberg</em><span> reported Hyperloop One is </span><a href="https://www.bloomberg.com/news/articles/2023-12-21/hyperloop-one-to-shut-down-after-raising-millions-to-reinvent-transit" rel="">finally shutting down</a><span>.</span></p><p>To be clear, Hyperloop One is not associated with Musk himself. After launching in 2014, it was partly funded from Richard Branson’s fortune for a while, but even that didn’t last, with the Virgin branding disappearing along with the vision for transforming passenger transportation. I sincerely hope the media (and investors) take its collapse as a sign to stop giving any oxygen to Hyperloop fantasies. The technology was never really meant to go anywhere. Its main goal was always to stop a better transport future from being realized.</p><p>A decade on, people often forget what was really motivating the Hyperloop when Musk first started pushing it. In the early 2010s, there was a big debate around California’s plan to build a high-speed rail line from Los Angeles to San Francisco, with further extensions to San Diego and Sacramento to follow. Naturally, conservative, automotive, and airline interests were vehemently opposed to a technology that Japan and Europe had been living with for decades arriving on American shores because it threatened their commercial interests.</p><p><span>Elon Musk, as an automaker and (let’s be honest) somewhat of a conservative himself, eagerly adopted the arguments against the bullet train with his own spin. He began incessantly repeating a line he </span><a href="https://www.youtube.com/watch?v=qox_m6jyfmA" rel="">doled out</a><span> at the D11 conference with Walt Mossberg and Kara Swisher in 2013, calling it, “the slowest bullet train in the world and the most expensive bullet train per mile in the world” — and that meant it not only had to be opposed, but that the geniuses in Silicon Valley could surely do better than people who actually had a clue about trains and transportation.</span></p><p><span>When he unveiled the Hyperloop concept, Musk claimed that building it on the Los Angeles to San Francisco route would not only result in a far faster trip, but would also cost only a fraction of the price — as little as $6 billion. That figure was immediately debunked by people who </span><a href="https://web.archive.org/web/20130819164738/http://america.aljazeera.com/articles/2013/8/14/economists-don-tbelievethehyperloop.html" rel="">actually understood what went into building that kind of infrastructure</a><span>, with the real cost estimated around at least $100 billion, even though it would have far less capacity than a high-speed train. But there’s another feature that’s often forgotten: the Hyperloop was also for cars. “You just drive on, and the pod departs,” Musk told </span><em>Bloomberg Businessweek</em><span> in </span><a href="https://web.archive.org/web/20130815003336/http://www.businessweek.com/articles/2013-08-12/revealed-elon-musk-explains-the-hyperloop" rel="">his first interview</a><span> about the idea. That fantasy would show up again in the Boring Company a few years later.</span></p><p><span>In 2013, Musk’s star was rising. He was gracing the covers of magazines and being hailed as our future-builder. He’d already served as inspiration for Robert Downey Jr.’s take on Tony Stark in </span><em>Iron Man</em><span> and was largely seen as a man who could do no wrong. When he chose to speak out against the bullet train, that amplified the campaign against it, further seeping political support from an already challenged project. And that was the point. Musk never had any intention of building the Hyperloop. He only needed it to help kill or substantially delay the high-speed rail project and the alternate vision of sustainable </span><em>collective</em><span> transportation it offered. It threatened his interests as an automaker and his elite vision of “individualized” mobility that simply worked better for him.</span></p><p><span>In 2015, Ashlee Vance’s biography </span><em>Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future</em><span> was published to an audience eager to consume a hagiography of the man supposedly saving the world and preparing us to colonize the stars. But buried within those 300-odd pages was an admission that many people seemed to have glossed over. On the subject of the Hyperloop, Vance wrote,</span></p><blockquote><p>Musk told me that the idea originated out of his hatred for California’s proposed high-speed rail system. … He insisted the Hyperloop would cost about $6 billion to $10 billion, go faster than a plane, and let people drive their cars onto a pod and drive out into a new city. At the time, it seemed that Musk had dished out the Hyperloop proposal just to make the public and legislators rethink the high-speed train. He didn’t actually intend to build the thing. … With any luck, the high-speed rail would be canceled. Musk said as much to me during a series of e-mails and phone calls leading up to the announcement.</p></blockquote><p><span>The only thing that could be clearer is if Vance released the e-mails and phone calls he’s referring to, but what he wrote is conclusive enough. Years later, after I </span><a href="https://twitter.com/parismarx/status/1167410460125097990" rel="">began</a><span> </span><a href="https://time.com/6203815/elon-musk-flaws-billionaire-visions/" rel="">resurrecting</a><span> that passage, Vance </span><a href="https://jalopnik.com/did-musk-propose-hyperloop-to-stop-california-high-spee-1849402460" rel="">claimed</a><span> my telling was “vaguely accurate but a disingenuous take on the situation.” Speaking to </span><em>Jalopnik</em><span> in 2022, he said, “I honestly do not think that was the goal of Hyperloop at all. I think if there was a better public transport system, my impression — and I think it’s genuine — is that Elon would be all for it.” Yet that completely contradicted what he had written years earlier, that Musk wanted to make lawmakers “rethink the high-speed train” and hoped it “would be canceled.”</span></p><p><span>Vance’s original telling is more credible because stopping collective transport projects has become a pattern for Musk. He proposed the Hyperloop to imperil high-speed rail. He also proposed the Boring Company tunnel system as his solution to traffic, after previously believing </span><a href="https://la.curbed.com/2018/11/9/18077612/elon-musk-mayor-garcetti-tunnels-boring-company" rel="">double-decker highways</a><span> would do the trick, instead of advocating for much better public transit because he felt transit was “</span><a href="https://www.wired.com/story/elon-musk-awkward-dislike-mass-transit/" rel="">a pain in the ass</a><span>” and suggested it was filled with serial killers. Counter to Vance’s more recent suggestion, there’s no evidence Musk would back better public transit, but plenty that he wanted to stall out plans for improved collective transport altogether. He did exactly that with the Boring Company: selling cities </span><a href="https://nymag.com/intelligencer/2022/08/elon-musks-biggest-boondoggle.html" rel="">useless tunnels</a><span> that were rarely ever built and which often displaced realistic transit plans.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.disconnect.blog/p/the-hyperloop-was-always-a-scam?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><span>Unfortunately, the harm of the Hyperloop went far beyond California. Companies claiming they’d realize the bullshit idea claimed to be moving forward with projects in </span><a href="https://indianexpress.com/article/explained/explained-after-hype-over-hyperloop-why-theres-a-question-mark-over-ultra-modern-project-6223864/" rel="">India</a><span>, </span><a href="https://www.thenationalnews.com/uae/transport/virgin-hyperloop-aims-for-2022-test-launch-as-chief-warns-against-rushing-technology-1.842589" rel="">Dubai</a><span>, </span><a href="https://lareleveetlapeste.fr/lhyperloop-delon-musk-un-fiasco-planetaire-qui-a-coute-plus-de-55-millions-deuros-a-la-metropole-de-toulouse/" rel="">France</a><span>, </span><a href="https://edmonton.ctvnews.ca/alta-hyperloop-project-awaiting-government-meeting-committing-to-stop-in-red-deer-1.6217544" rel="">Canada</a><span>, and countless other parts of the world only for every single one to amount to nothing. Sure, some short tubes got built in a few places to try to keep squeezing money out of investors and governments, but the only thing they really achieved was to distract people with fantasies while they could’ve been focused on building something real.</span></p><p><span>In the time since California started talking about high-speed rail and Elon Musk interjected with his fantasy to help sidetrack it, China moved ahead and built a network consisting of 42,000 kilometres (26,000 miles) of track. Europe is continuing to expand its own network, and Japan is building a maglev line that will run at speeds of over 500 km/h (310 mph). The first segment from Tokyo to Nagoya could </span><a href="https://www.japantimes.co.jp/news/2023/12/15/japan/maglev-shinkansen-opening-delayed/" rel="">open by 2027</a><span>. Not to be outdone, China is working on </span><a href="https://www.cnbctv18.com/travel/china-completes-first-operation-of-worlds-fastest-train-that-travels-at-600-kmhour-16321471.htm" rel="">a maglev of its own</a><span> to beat its Japanese rivals.</span></p><p><span>While the Hyperloop deception spread far and wide, nowhere was it stronger than in the United States. As countries around the world </span><a href="https://en.wikipedia.org/wiki/List_of_high-speed_railway_lines" rel="">moved forward</a><span> with real transport improvements, North Americans were distracted by the fantasies of clueless, but self-confident tech moguls. They left people trapped in their cars and denied better options to get around that people in many other parts of the world — even those that are quite a bit poorer — take for granted. Now all they can do is shovel money at automakers to try to power cars with batteries instead of internal combustion engines. They have no vision for a better, less car dependent alternative.</span></p><p>The tech industry’s move into transportation was not only a failure; it was an active campaign to deny the public access to better transit and trains because the billionaires of Silicon Valley don’t personally want to get around that way. The Hyperloop was one part of that, but so were the Boring Company, ride-hailing services, and self-driving cars. The Hyperloop’s failure provides a lesson we’re learning far too late: that Silicon Valley won’t deliver us a better world if they can’t find some way to profit off it. We need to stop falling for their grand deceptions, and tell our media to stop echoing them too.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Heynote – A Dedicated Scratchpad for Developers (654 pts)]]></title>
            <link>https://heynote.com/</link>
            <guid>38733968</guid>
            <pubDate>Fri, 22 Dec 2023 13:33:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heynote.com/">https://heynote.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38733968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3>One buffer, many blocks</h3>
          <p>
            At its core, Heynote is a large, persistent text buffer divided into blocks. 
            Creating a new block is as easy as pressing <span><span>⌘</span>-Enter</span>, 
            and pressing <span><span>⌘</span>-A</span> within 
            a block selects the content of just that block.
          </p>
          <p>
            Works great for that Slack message you don't want to accidentally send, a JSON response 
            from an API you're working with, notes from a meeting, your daily to-do list, etc.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In the Long Run, We're All Dad (250 pts)]]></title>
            <link>https://www.astralcodexten.com/p/in-the-long-run-were-all-dad</link>
            <guid>38733440</guid>
            <pubDate>Fri, 22 Dec 2023 12:20:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.astralcodexten.com/p/in-the-long-run-were-all-dad">https://www.astralcodexten.com/p/in-the-long-run-were-all-dad</a>, See on <a href="https://news.ycombinator.com/item?id=38733440">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><strong>I.</strong></p><p>In February 2023 I found myself sitting in the waiting room of a San Francisco fertility clinic, holding a cup of my own semen.</p><p>The Bible tells the story of Onan, son of Judah. Onan’s brother died. Tradition dictated that Onan should impregnate his brother’s wife, ensuring that his brother’s line would (in some sense) live on. Onan refused, instead “spilling the seed on the ground”. God smote Onan, starting a 4,000-year-old tradition of religious people getting angry about wasting sperm on anything other than procreative sex.</p><p>Modern academics have a perfectly reasonable explanation for all of this. If Onan had impregnated his brother’s wife, the resulting child would have been the heir to the family fortune. Onan refused so he could keep the fortune for himself and his descendants. So the sin of Onan was greed, not masturbation. All that stuff in the Talmud about how the hands of masturbators should be cut off, or how masturbation helped cause Noah’s Flood (really! Sanhedrin 108b!) is just a coincidence. God hates greed, just like us.</p><p>Modern academics are great, but trusting them feels somehow too convenient. So there in the waiting room, I tried to put myself in the mindset of the rabbis thousands of years ago who thought wasting semen was a such a dire offense.</p><p>The average ejaculation contains about 300 million sperm. There are about 300 million people in the United States. If every sperm in a single ejaculation got to fertilize an egg and incubate in a womb, it would be enough to populate a second America.</p><p>America has about 200 living Nobel Prize winners. 735 billionaires. 1,000,000 doctors, 5,000,000 nurses. 100,000 pilots, 700,000 cops. Also 700,000 drug dealers, 100,000 murderers, and 1,700 NYT journalists.</p><p>That doesn’t necessarily mean my cup contained exactly 100,000 future pilots. If we assume complete genetic determinism, my sperm form a normal distribution around my personal genetic average. I’m terrible at three dimensional reasoning, so let’s say I’m two standard deviations less likely than usual to become a pilot. If my wife is normal on this trait, and we average it out, that means only about 32,000 future pilots in the cup.</p><p><span>On the other hand, I’m better than average at writing. I might be among the top 20,000 most-read authors in the US, so maybe +4SD above average. Again assuming my wife is normal, that suggests even the average kid we have will be a good writer. But imagine an entire America worth of people </span><em>centered</em><span> around being a good writer. The best writer in existing America should be +6SD above average; the best writer among the sperm in the cup is +8SD. 8SD is “best in two quadrillion”. There has never been a writer that good in the whole history of the world. There is a sperm in that cup who could write at an utterly superhuman level, write things none of us could possibly imagine, things so good it’s not even clear you would still call them writing and not some entirely new semi-divine form of art.</span></p><p>There’s also, on priors, some sperm who would shoot up a school. There’s a decent chance of a few who, if given an egg and a womb, would destroy the world, and a few others who would save it. A few hundred might ruin my life so thoroughly that I would commit suicide to escape them. A few dozen might be so great that people would build statues to me just for being their father, the same way some people build statues to St. Joseph.</p><p>The nurse called my name, I handed her the cup, and she took it away to pour into some lab apparatus. Good bye, 200 Nobelists. Good by, 32,000 pilots. Good bye, son who would have destroyed the world. Good bye, daughter who would have saved it. I waited to see if God would smite me. He did not. A few weeks later the clinic called and said there was nothing wrong with my sperm. My fertility problems were just bad luck. I should just keep trying.</p><p><span>There’s an old Jewish joke. How do you make God laugh? Tell Him your plans. 1/10,000 chance of a pilot, because I’m bad at navigation and the base rates are low. 1/10 chance of a doctor, because of all the doctors in my family. I knew it was bogus. Partly because I’m bad at standard deviations and probably got the numbers wrong. But partly because anything can happen. Maybe I was having all this trouble because the lab missed something and I really </span><em>was</em><span> infertile. Maybe my </span><em>wife</em><span> was infertile. Maybe we’d eaten too many microplastics and it was all over. Maybe we’d have a kid, an amazing kid who could have changed everything, but the world would end in 2027 and they’d never get a chance. Still, you’ve got to calculate. One in three million chance of becoming a billionaire. One in thirty thousand chance of committing murder. One out of this. One in that. One one one one one, until you reach semantic satiation on the number “one” and the syllable loses all meaning.</span></p><p>This time God chose to frustrate my calculations even faster and more decisively than usual: He blessed me with twins.</p><p><strong>II.</strong></p><p>Natural selection didn’t design the female body to carry two children. It barely, grudgingly, designed it to carry one. Two is a cruel joke.</p><p>I remember cutting an onion, sometime during month one. My wife asked if they were a different variant from usual, or if they’d gone bad. They hadn’t. It had to be morning sickness. We laughed and hugged each other. This pregnancy thing was starting to feel real!</p><p>A month later - including a hunt through the kitchen to cleanse it of any shred of onion, or anything that had ever touched an onion - we agreed that actually, morning sickness was bad. Two months later, we debated bringing my wife to the ER because she hadn’t eaten anything other than plain saltine crackers in several days. We did manage to avoid the hospital, but it was rough. I’m surprised more people don’t name their children after Zofran®. Women get such positive feelings about it, right when they’re considering baby names. For a girl, you could nickname her Zoe. For a boy, Frank. </p><p><span>And after the morning sickness it was asthma. After the asthma, anemia. After the anemia, hip pain, trouble sleeping, trouble walking, trouble with </span><em>everything</em><span>.</span></p><p>I’ve heard rumors of some women who keep working all through pregnancy, with a smile on their face. Pronatalist influencer Simone Collins says she was taking business calls from her hospital room during the delivery. I think it’s a conspiracy. All the pronatalist influencers get together and say that pregnancy isn’t so bad. Young women believe them, and so the human race survives another generation. </p><p><span>As my wife labored to build our childrens’ physical forms, I toiled to give them their spiritual-semiotic identity. The theory of </span><a href="https://en.wikipedia.org/wiki/Nominative_determinism" rel="">nominative determinism</a><span> posits that a person’s name shapes the course of their future life. Its proponents have collected a mountain of evidence: British chief justice </span><a href="https://en.wikipedia.org/wiki/Igor_Judge,_Baron_Judge" rel="">Igor Judge</a><span>, neurologist </span><a href="https://en.wikipedia.org/wiki/Russell_Brain,_1st_Baron_Brain" rel="">Lord Brain</a><span>, poker champion </span><a href="https://en.wikipedia.org/wiki/Chris_Moneymaker" rel="">Chris Moneymaker</a><span>, investment CEO </span><a href="https://en.wikipedia.org/wiki/Eugene_Profit" rel="">Eugene Profit</a><span>. The Chinese think the </span><a href="https://www.thoughtco.com/number-of-stroke-chinese-names-2278472" rel="">number of strokes</a><span> in the characters that form </span><a href="https://www.bbc.com/worklife/article/20201209-why-some-chinese-believe-a-name-change-could-improve-luck" rel="">a child’s name</a><span> must add up to a lucky number; the Jews believe each letter corresponds to a number, and a person’s name resonates spiritually with all other words whose letters sum to the same amount. </span></p><p><span>Now the statisticians have joined the fray: </span><a href="https://www.experiencedmommy.com/baby-name-salary/" rel="">did you know</a><span> that children with short first names earn over $10,000 more than longer ones? Or that men named "Jim" make 50% more than men named "Isaiah"? Is this causation or confounding? Names indicate whether you are black or white, rich or poor, and whether your parents are traditional or eccentric; what is left after adjusting for this effect? The only paper I’ve seen even begin to address the question is </span><a href="https://www.nber.org/system/files/working_papers/w11195/w11195.pdf" rel="">a sibling-control study by David Figlio</a><span>, who finds that even within families, children with lower-class names perform worse. And you don’t need scientists to know that names affect how other people see you. Just ask Chad, Karen, Tyrone, or the poor doctor I worked with once named Osama (he went by “Sam”).</span></p><p>But also, some people love their names, and other people hate theirs. This was the factor I was least sure about, so I surveyed 1518 blog readers.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 424w, https://substackcdn.com/image/fetch/w_652,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 652w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1272w, https://substackcdn.com/image/fetch/w_1304,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1304w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1456w, https://substackcdn.com/image/fetch/w_1956,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1956w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_652,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png" sizes="100vw" alt="Graph of people's name preferences, showing they are happiest with names of rank 501 - 1000" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 424w, https://substackcdn.com/image/fetch/w_652,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 652w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1272w, https://substackcdn.com/image/fetch/w_1304,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1304w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1456w, https://substackcdn.com/image/fetch/w_1956,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242b644-1695-46be-bc07-6042c379a03b_636x416.png 1956w" width="652"></picture></div></figure><p><span>Here “popularity rank” comes from the </span><a href="https://www.ssa.gov/oact/babynames/" rel="">List Of Most Popular Baby Names</a><span> for the respondent’s birth year - for example, Scott was the 39th most popular boys’ name in 1984, so I am rank 39. I find that people are happiest with names in the 501 - 1000 range (a separate question, which asked people to rate their happiness with their name on a scale of 1 - 5 without reference to whether it was traditional or unusual, got the same result). </span></p><p>What about other considerations?</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 424w, https://substackcdn.com/image/fetch/w_682,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 682w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1272w, https://substackcdn.com/image/fetch/w_1364,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1364w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1456w, https://substackcdn.com/image/fetch/w_2046,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 2046w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_682,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png" sizes="100vw" alt="Another graph of name preferences, showing they are happiest with older names, historical names, or names honoring deceased relatives." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 424w, https://substackcdn.com/image/fetch/w_682,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 682w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1272w, https://substackcdn.com/image/fetch/w_1364,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1364w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 1456w, https://substackcdn.com/image/fetch/w_2046,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c790d21-2ecf-4187-a0ad-96bdf8723bfb_707x339.png 2046w" width="682"></picture></div></figure><p>I asked people how happy they would be with ten different types of names. </p><p><span>People expressed a strong preference for common older names like John and Mary. Does this contradict the finding above that people with very common names were least happy? Not </span><em>necessarily </em><span>- the common names on the question above included all common names (the #1 most common name for boys born last year is “Liam”), so maybe people like common </span><em>older</em><span> names in particular? But I looked at people in the sample named John, Michael, Mary, and Sarah, and they didn’t differ much from the overall common names category. So people may </span><em>think</em><span> they would like names like these, but actual Johns and Marys wish they were named something a little more unusual.</span></p><p>The least popular categories included “new-fangled name” and “sci-fi / fantasy name”. The most popular were “name honoring a deceased relative”, “name from your ethnic origin”, and “historical figure”. </p><p>So that’s why I decided to name my children Napoleon Herschel Siskind and Hatshepsut Tzeitel Siskind. </p><p>No, seriously, I’m not comfortable telling the Internet my kids’ names. I’ll let them get doxxed the usual way - by the NYT, the first time they express a problematic opinion.</p><p>But I need some way to refer to them online, so their nicknames are Kai and Lyra.</p><p><strong>III.</strong></p><p><span>On December 13, 2023, two surprisal-minimization engines registered an unpredecented spike in surprisal. They were thrust from a sunless sea into a blooming buzzing confusion, flooded with inexplicable data through input channels they didn’t even know they had. The engines heroically tested hyperprior after hyperprior to compress the data into something predictable. Certain patterns quickly emerged. Probability distributions resolved into solid objects. The highest-resolution input channel snapped into place as a two-dimensional surface being projected onto by a three dimensional space. But - a blur of calculations - the three-dimensional nature of space implies that it must be intractably large! And if there are n solid objects in the world, that implies the number of object-object interactions increases as n(n-1)/2, which would quickly become impossible to track. Their hearts sinking, the engines started to worry it will might take </span><em>hours</em><span> before they were fully able to predict every aspect of this new environment. A panic reflex they didn’t know they had kicked in, and they began to cry.</span></p><p><span>Some outside force picked them up, rocked them back and forth. A million inexplicable sense-data, overwhelmed by a single stimulus - a </span><em>rhythmic</em><span> stimulus. The predictability of importance-weighted sense-data shot way up! Kullback-Leibler divergence dropped to near-zero! The panic reflex subsided, and the engines - exhausted by their sudden spurt of computation - shut off to </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S1084952121000318" rel="">renormalize synaptic weights</a><span>.</span></p><p>Soon the engines will discover that things are even worse than they think. Some of their predictions are hard-coded; they will never be able to change them to match the world. Their only hope is to change the world to match their predictions: they are obligate agents. As they grow older, their goal systems will throw up increasingly complicated hard-coded forecasts; food, water, belonging, social status, sex. Their only path towards predictive accuracy will be to obtain all of these things from a hostile world. It’s a lousy deal.</p><p><span>My poor, fragile, little cognitive engines! These, then, will be the twin imperatives of your life: </span><a href="https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/" rel="">surprisal minimization</a><span> and </span><a href="https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/" rel="">active inference</a><span>. If your brains are still too small to process such esoteric terms, there are others available. Your father’s ancestors called them </span><em>Torah</em><span> and </span><em>tikkun olam</em><span>; your mother’s ancestors called them Truth and Beauty; your current social sphere calls them Rationality and Effective Altruism. You will learn other names, too: no perspective can exhaust their infinite complexity. Whatever you call them, your lives will be spent in their service, pursuing them even unto that far-off and maybe-mythical point where they blur into One.</span></p><p>If you pursue them only far enough to reduce your own predictive error, it will still be a life well-lived, and nobody will blame you for it. But if you choose, you can take an extra burden upon yourself, improving not just your own models but the broader predictions of the world. You can push forward the frontiers of knowledge, or improve the lot of all humankind. It’s a crazy thing to try, when even your own local predictions are so far from perfect accuracy. I cannot exactly tell you why you should want to do something like this. If you feel it, you feel it; if not, so it goes.</p><p><span>But a parable: when you were born, your mother kissed you.  Along with the kiss came a microdose of </span><a href="https://www.astralcodexten.com/p/defying-cavity-lantern-bioworks-faq" rel="">the BCS3-L1 genetically engineered bacterium</a><span>. Without any teeth to cling to, it fell into the pit of your stomach and died. But she’ll kiss you again and again, transferring a few more BCS3-LI each time. In a few months, one of the colonists will find an incipient tooth and hang on for dear life. It will fight off competitors, wage epic battles that will determine the fate of the mouth for decades to come. It will win, because its genetic enhancements are pretty good. Then, if some smart people got their calculations right, it will do exactly nothing. No tooth decay. No cavities. The teeth will stay safe and clean.</span></p><p>When you get older, I’ll tell you the story behind this. Your mother worked for a company synthesizing genetically engineered tooth bacteria that prevent cavities. She isn’t the kind of person who would push a product on others that she hadn’t tried herself. So she infected herself with the bacterium, fresh out of the lab. Other people in the company did the same. But only she was pregnant. Babies get their mouth bacteria from their mothers. So you might be the first children in the world to grow up without s-mutans-mediated tooth decay.</p><p><span>Tooth decay isn’t the worst thing in the world. As victories go, this is a relatively minor one. I tell it to you only because it is ours. Our drop of water in a vast ocean of victories that have improved the lot of humankind on every continent, for as long as the species lasts. There is nothing that hammers this in like being a new father - nothing like seeing two tiny rudimentary week-old cognitive engines struggle not to fade into the entropic background. Kai, you wouldn’t come out of your mother on your own - the obstetrician used </span><a href="https://en.wikipedia.org/wiki/Vacuum_extraction" rel="">vacuum extraction</a><span> to save your life. Neither of you was a great breastfeeder at first, and if we hadn’t had nurses and bottles and formula, you might not have made it. A few days after your birth, it rained two inches in fifty degree weather; if we didn’t have central heating and space heaters and warm blankets, who knows what would have happened?  In 1800, about 50% of babies died before their fifth birthday. This statistic used to feel like a brute fact. Now I’m noticing all the little cracks that Death could creep in through, if we didn’t have our cornucopia of technologies and our team of vigilant pediatricians.</span></p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 424w, https://substackcdn.com/image/fetch/w_580,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 580w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 848w, https://substackcdn.com/image/fetch/w_1160,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1160w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1456w, https://substackcdn.com/image/fetch/w_1740,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1740w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_580,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png" sizes="100vw" alt="A graph showing global child mortality plummeting from 1800 to today" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 424w, https://substackcdn.com/image/fetch/w_580,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 580w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 848w, https://substackcdn.com/image/fetch/w_1160,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1160w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1456w, https://substackcdn.com/image/fetch/w_1740,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52a819de-b799-4ab2-88d3-bd873a59158b_825x764.png 1740w" width="580"></picture></div></figure><p>There are two of you. Back in 1800, statistically, one of you would have made it. I look at you now - such beautiful, fragile cognitive engines - and I cannot bear the thought of losing either one. The statistics for the 21st century suggest I won’t have to.</p><p>I was thinking about this recently, because - well, I feel kind of bad. I instantiated two surprisal-minimization engines - two conscious algorithms designed to feel negative qualia in the presence of hard-to-predict stimuli - on a world ruled by 195 mutually-hostile and frequently-shifting coalitions of over-evolved murder-monkeys, many of whom have nuclear weapons. I cannot quite remember why I thought this would be a good idea. I blame the pronatalist influencer conspiracy. </p><p>But if I have any excuse at all, it’s excessive enthusiasm for this grand project of world-scale surprise minimization and active inference. You are here to benefit from it, to enjoy sensual and intellectual pleasures that our ancestors could never know. And also, if you choose, to continue it, push it forward into a new era. You have already contributed in a tiny way - as guinea pigs - to the conquest of tooth decay. But there are so many other worse sources of prediction error out there. What else might you conquer, my two little surprisal-minimization engines?</p><p><strong>IV.</strong></p><p><span>There is a secret known only to parents of twins, medical residents, and </span><a href="https://guzey.com/theses-on-sleep/" rel="">Alexey Guzey</a><span>: the human body does not actually need sleep. After 31 hours awake, you get </span><a href="https://www.astralcodexten.com/p/sleep-is-the-mate-of-death" rel="">an integer overflow</a><span> in God’s database and go back to being well-rested again. Also you gain the ability to see angels.</span></p><p>This has become the new rhythm of our lives. Changing, nursing, burping, first one child, then the other. Twenty minutes per child, times two children, times once every 2-3 hours; you can do the math. We do everything else - laundry, shopping, cooking, occasionally even napping - in the precious intervals when both babies are asleep.</p><p><span>The </span><a href="https://www.nytimes.com/wirecutter/reviews/snoo-smart-sleeper-what-to-know/" rel="">Snoo </a><span>is a $1500 computerized bassinet that continually assesses babies’ needs and tries to calm them with various soothing noises and automated rocking motions. We got two, both of which have been soundly rejected. The twins insist on sleeping in their carseats, which we’ve grudgingly moved to the nursery. At first I was miffed, but now I see their logic. You’ve got to learn to resist the algorithmic content mills early.</span></p><p>Kai has some baby version of Alien Hand Syndrome. His arms are controlled by a malevolent entity with a grudge against the rest of his body. If we leave them loose, they wave wildly in all directions, and he freaks out. This is apparently a common problem, best solved by heavy swaddling clothes. The malevolent entity struggles against the swaddle and occasionally breaks free, like some 1980s horror movie monster. Every nursing, we must struggle against it and bind it anew before returning him to his carseat.</p><p>Lyra is already an overachiever. She has clearly read all the How To Be A Baby textbooks, learned when crying is appropriate, and only cries at those specific times. She drinks the exact amount of milk recommended on the Baby Age-Appropriate Nursing Chart, then refuses to accept more. I’m worried that if we don’t teach her to think independently soon, she’ll end up somewhere terrible like Harvard.</p><p>I look over at them. They seem so peaceful in their stupid carseats. Let them sleep. Let them nurse as often as they want. They’ll need all their strength for what’s ahead.</p><p><span>Kai. Lyra. You’ll </span><a href="https://www.youtube.com/watch?v=VGDhrH_uLUw" rel="">live to see a million things that man was never meant to see</a><span>. You were born just in time for a high-speed collision with the hinge of history. I’m only 39, I expect to be around when whatever-it-is happens - but if not, you’re our family’s ambassador to the singularity. A thousand generations, from hardy Neolithic farmers to studious Russian rabbis to overprivileged American office workers - they all lived and died so you could be here and experience this, and maybe tilt the course of what’s coming by a couple of micro-degrees.</span></p><p><span>Parents are supposed to teach their children the skills they need to navigate the world. This already feels somewhat obsolete - where are the Google programmers who were taught Python by their fathers, or the Instagram influencers who learned content creation on their mother’s knee? Soon it will be completely hopeless. Where we’re going there are no roads. You’ll have to figure it out by yourself. If I am to pass on anything of value to you, it can only be </span><a href="https://www.lesswrong.com/posts/SXK87NgEPszhWkvQm/mundane-magic" rel="">the ultimate power</a><span>, the technique that forms all other techniques. </span></p><p>I’ve always wondered why I wrote so much. Now I realize I was leaving you bread crumbs.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 424w, https://substackcdn.com/image/fetch/w_550,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 550w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 848w, https://substackcdn.com/image/fetch/w_1100,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1100w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1456w, https://substackcdn.com/image/fetch/w_1650,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1650w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_550,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png" sizes="100vw" alt="Me, my wife, and the twins." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 424w, https://substackcdn.com/image/fetch/w_550,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 550w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 848w, https://substackcdn.com/image/fetch/w_1100,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1100w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1456w, https://substackcdn.com/image/fetch/w_1650,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fae22914c-e8a8-4cbe-b776-c677202b9f6b_683x514.png 1650w" width="550"></picture></div></figure><p>Happy holidays, from our family to yours. ACX will return to its normal posting schedule in January.</p><figure data-component-name="Image3Dynamic"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 424w, https://substackcdn.com/image/fetch/w_728,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 728w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1456w, https://substackcdn.com/image/fetch/w_2184,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 2184w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_728,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png" sizes="100vw" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 424w, https://substackcdn.com/image/fetch/w_728,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 728w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 1456w, https://substackcdn.com/image/fetch/w_2184,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb382aef1-8cfc-4394-826f-a180c9c6f946_439x115.png 2184w" width="728"></picture></div></figure><p>…of 2042.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to make LLMs go fast (158 pts)]]></title>
            <link>https://vgel.me/posts/faster-inference/</link>
            <guid>38733384</guid>
            <pubDate>Fri, 22 Dec 2023 12:09:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vgel.me/posts/faster-inference/">https://vgel.me/posts/faster-inference/</a>, See on <a href="https://news.ycombinator.com/item?id=38733384">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>In <a href="https://vgel.me/posts/handmade-transformer">my last post</a>, we made a transformer by hand.
There, we used the classic autoregressive sampler, along the lines of:</p><p>This approach to inference is elegant and cuts to the heart of how LLMs work—they're <em>autoregressive</em>, consuming their own output.
And for our toy model with merely thousands of parameters, it worked completely fine.
Unfortunately, for real models it's far too slow<sup><a href="#generate-too-slow">1</a></sup>.
Why is that, and how can we make it faster?</p><p>This post is a long and wide-ranging survey of a bunch of different ways to make LLMs go brrrr, from better hardware utilization to clever decoding tricks.
It's not completely exhaustive, and isn't the most in-depth treatment of every topic—I'm not an expert on all these things!
But hopefully you'll find the information here a useful jumping off point to learn more about the topics you're interested in.
(I tried to include links to relevant papers and blog posts where applicable.)</p><p>There are two main reasons that inference with the plain autoregressive <code>generate</code> function is slow: an algorithmic one, and a hardware one.</p><p>Algorithmically, <code>generate</code> has to process an increasing number of tokens every cycle, because each cycle we append a new token to the context.
That means to generate 100 tokens from a 10 token prompt, you don't need to run <code>model</code> on only 109 tokens. You need to run it on 10 + 11 + 12 + 13 + ... + 109 = 5,950 tokens!
<small>(The initial prompt can be processed in parallel, which is part of why prompt tokens are usually cheaper in inference APIs.)</small>
It also means that the model <em>slows down</em> as it generates, since each successive token generation has a longer and longer prefix:</p><p>Attention, at least vanilla attention, is also a quadratic algorithm: all tokens attend to all tokens, leading to N² scaling, making everything worse.</p><p>So that's the algorithmic reason.
What's the hardware reason?
Well, it's simple: LLMs are just huge.
Even a relatively small model like gpt2 (117M parameters) is hundreds of megabytes, and all that data has to live in RAM.
RAM is really slow, and modern processors (both CPUs and GPUs) make up for that by having lots of cache close to the processor that's faster to access<sup><a href="#gpu-cache">2</a></sup>.
The details of this differ based on type and model of processor, but the gist is that LLM weights do not fit in cache, so a lot of time is spent waiting to load weights from RAM.
This has some unintuitive effects!
For example, looking at the graph above, operating on 10 tokens isn't necessarily much slower than operating on a single token, even though the activation tensors are 10x larger, because the main time sink is moving the model weights around, not doing calculations!</p><p>As a sidebar, what do we mean exactly when we say <em>slow</em>?
There's a whole zoo of metrics people talk about when it comes to LLM inference:</p><p>Different optimizations affect these metrics differently.
For example, batching improves throughput and better utilizes the hardware, but can increase <abbr title="Time to First Token">TtFT</abbr> and generation latency.</p><p>A straightforward way to speed up inference (especially if you're VC funded :-)) is to just buy better hardware, or if you can't afford that, to take better advantage of the hardware you have.</p><p>If you're buying better hardware, most likely that would be some sort of accelerator—usually a GPU, or sometimes/if you're Google, a <abbr title="Tensor Processing Unit">TPU</abbr>.</p><p>Using an accelerator can produce dramatic speedups (hence the name), but keep in mind that there's a transfer bottleneck between the CPU and the accelerator.
If your model doesn't fit in the accelerator's memory, it will need to be swapped out throughout the forward pass, which can slow things down dramatically.
<small>(This is one of the reasons Apple's M1/M2/M3 chips punch above their weight for inference—they have unified CPU and GPU memory.)</small></p><p>Another thing to keep in mind with both CPU and accelerator inference is whether you're taking full advantage of the hardware—a properly optimized program can squeeze more out of weaker hardware than a poorly optimized one can get out of the best hardware.</p><p>For example, you could write attention in PyTorch as <code>F.softmax(q @ k.T / sqrt(k.size(-1)) + mask) @ v</code>, which will give you correct results.
But if you instead use <code>torch.nn.functional.scaled_dot_product_attention</code>, it will delegate the calculation to <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> when available, which can produce 3x speedups using a handwritten kernel that better takes advantage of cache.</p><p>A more general version of this is compilers like <code>torch.compile</code>, TinyGrad, and ONNX, which can fuse naive Python code into kernels optimized for your hardware.
For example, I could write the following function:</p><p>Each of these things is slow, and some of the steps require jumping the boundary between Python and native code, which doesn't help.
So what if I compile this function using <code>torch.compile</code>?</p><p>If I go into that debug trace directory and open the <code>output_code.py</code> file there, <code>torch</code> has generated an optimized C++ kernel for my CPU that fuses <code>foo</code> into a single kernel.
<small>(If I had run this with a GPU available, <code>torch</code> would have generated a <a href="https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/">CUDA kernel</a> for the GPU instead.)</small></p><p>Note that <code>torch.compile</code> specialized the code above for the specific size of tensor we passed in (<code>(10,)</code>).
If we passed in tensors of many different sizes, <code>torch.compile</code> would instead generate code generic over the size, but having a constant size can enable the compiler to generate better code in some cases (e.g. via loop unrolling or better vectorization).</p><p>This function has <em>data-dependent control flow</em>, meaning we do something different based on the runtime value of a variable.
If we compile this in the same way we compiled <code>foo</code>, we get <em>two</em> graphs (and thus two debug directories):</p><p>The first kernel implements the <code>torch.sin(x) + torch.cos(x)</code> and <code>r.sum() &lt; 0</code> parts of the function:</p><p>And the second kernel implements the <code>return r - torch.tan(x)</code> branch, since this is the branch that was taken with the example input:</p><p>This is called a <em>graph break</em>, and it's not good!
The compiled function is slower due to it, since we have to leave the optimized kernel and return to Python to evaluate the branch.
On top of that, the other branch (<code>return r + torch.tan(x)</code>) hasn't been compiled yet, since it hasn't been taken!
That means it will be compiled on the fly when needed, which could be bad if it happens at an inopportune time (such as in the middle of serving a user request).</p><p>Tools like <code>torch.compile</code> are a great way to optimize your code to get better performance out of your hardware, without dipping down to CUDA to write kernels the old-fashioned way.
<small></small></p><div>
<p>(And if you're curious about the compilers work, <a href="https://bernsteinbear.com/blog/compiling-ml-models/">this post</a> by <a href="https://twitter.com/tekknolagi">@tekknolagi</a> explores compiling models written in <a href="https://github.com/karpathy/micrograd/">micrograd</a> to C for a 1000-7500x speed increase!)</p>
<h2 id="Batching"><a href="#Batching">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Batching">
</a>Batching</h2>
<p>In the unoptimized version of <code>generate</code>, we pass the model a single sequence at once, and at each step ask it to append a token:</p>
<div>
<div>
    <p>llm(</p>
    <table><tbody><tr> <td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td></tr></tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table><tbody><tr> <td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td></tr></tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table><tbody><tr><td>…</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td><td>.</td></tr></tbody></table>
    <p>)</p>
    <p>=</p>
    
</div>
</div>
<p>To batch generation, we instead pass the model multiple sequences at once, generating a completion for each in the same forward pass.<sup><a href="#jax-vmap">3</a></sup>
This requires the sequences to be padded on either the left or right with filler tokens to equal length.
The padding tokens (which can be anything, I'm using [end] here) are masked in the attention mask so that they don't influence generation.</p>
<div>
<div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td></tr>
        <tr><td>[end]</td><td>I</td><td>like</td><td>bananas</td><td>because</td><td>they</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td><td>in</td></tr>
        <tr><td>[end]</td><td>I</td><td>like</td><td>bananas</td><td>because</td><td>they</td><td>have</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>Mark</td><td>is</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td><td>.</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td><td>in</td><td>Paris</td></tr>
        <tr><td>[end]</td><td>I</td><td>like</td><td>bananas</td><td>because</td><td>they</td><td>have</td><td>no</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>…</td><td>quick</td><td>.</td><td>He</td><td>moves</td><td>quickly</td><td>.</td><td>[end]</td></tr>
        <tr><td>…</td><td>The</td><td>Eiffel</td><td>Tower</td><td>is</td><td>in</td><td>Paris</td><td>,</td></tr>
        <tr><td>…</td><td>like</td><td>bananas</td><td>because</td><td>they</td><td>have</td><td>no</td><td>bones</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div>
</div>
<p>Because batching sequences in this way allows the model weights to be used for multiple sequences at once, running the entire batch of sequences together takes less time than running each sequence separately.
For example, on my machine, using GPT-2 to generate a next token for:</p>
<ul>
<li>20 tokens x  1 sequence  = ~70ms</li>
<li>20 tokens x  5 sequences = ~220ms (linear scaling would be ~350ms)</li>
<li>20 tokens x 10 sequences = ~400ms (linear scaling would be ~700ms)</li>
</ul>
<h3 id="Continuous_Batching"><a href="#Continuous_Batching">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Continuous_Batching">
</a>Continuous Batching</h3>
<p>Notice how in the example above, "Mark is quick. He moves quickly." finished before the other sequences, but because the batch as a whole wasn't done, we were forced to continue generating tokens for it ("Random").
This isn't a problem for correctness—we can simply clip the generated sequence to the <code>[end]</code> token—but it is unfortunate, since GPU resources are being used to generate tokens we will just throw away.</p>
<p>Continuous batching fixes this by inserting new sequences into the batch as other sequences complete, after their <code>[end]</code> tokens.
Instead of generating random tokens after the <code>[end]</code> token, a new sequence can be inserted in that row of the batch, with attention masking to prevent the sequence from being influenced by the tokens from the previous sequence in the row.
<small>(Essentially, the prior sequence acts like additional padding.)</small></p>
<div>
<div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>1</td><td>2</td><td>3</td><td>4</td><td></td><td></td></tr>
        <tr><td>[end]</td><td>[end]</td><td>a</td><td>b</td><td></td><td></td></tr>
        <tr><td>[end]</td><td>A</td><td>B</td><td>C</td><td></td><td></td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>X</td><td></td></tr>
        <tr><td>[end]</td><td>[end]</td><td>a</td><td>b</td><td>c</td><td></td></tr>
        <tr><td>[end]</td><td>A</td><td>B</td><td>C</td><td>D</td><td></td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div><p>
⬇️
</p><div>
    <p>llm(</p>
    <table>
        <tbody><tr><td>1</td><td>2</td><td>3</td><td>4</td><td>X</td><td>Y</td></tr>
        <tr><td>[end]</td><td>[end]</td><td>a</td><td>b</td><td>c</td><td>α</td></tr>
        <tr><td>[end]</td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td></tr>
    </tbody></table>
    <p>)</p>
    <p>=</p>
    
</div>
</div>
<h2 id="Shrinking_model_weights"><a href="#Shrinking_model_weights">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Shrinking_model_weights">
</a>Shrinking model weights</h2>
<p>Floating point numbers come in different sizes, and that matters for performance.
Most of the time for regular software (e.g., Javascript numbers and Python floats), we use 64 bit (double precision) IEEE 754 floating point.
Most ML, however, has traditionally used 32 bit (single precision) IEEE 754:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>gpt2.transformer.h[</span><span>0</span><span>].attn.c_attn.weight.dtype
</span><span>torch.float32
</span></code></pre>
<p>Models train and infer fine with fp32, and this saves 4 bytes (50%) per parameter, which is huge—a 7B parameter model would take up 56Gb in fp64, and only 28 Gb in fp32.
Remember that large amounts of time during training and inference are spent moving data from RAM to cache and registers—the less data there is to move, the better.
So while fp32 is better than fp64, can we do <em>even better</em>?</p>
<h3 id="16_bit_floats"><a href="#16_bit_floats">
  <img src="https://vgel.me/permalink.svg" alt="permalink for 16_bit_floats">
</a>16 bit floats</h3>
<p>fp16, or half precision, is the obvious next step—another 50% savings!
You have two main options here: fp16, and bfloat16 (short for brain float, since it was developed by Google Brain), which has better range but worse hardware support.</p>
<p>It's easiest to see the distinction with a diagram showing the size of each field:</p>
<div>
<table>
<tbody><tr><th colspan="32">fp32</th></tr>
<tr>
    <th>sign</th>
    <th colspan="8">exponent (8)</th>
    <th colspan="23">fraction (23)</th>
</tr>
<tr>
    <td>0</td>
    <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td>
    <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td> <td>0</td> <td>1</td>
</tr>
<tr><td></td></tr>
<tr><th colspan="16">fp16 (IEEE half)</th></tr>
<tr>
    <th>sign</th>
    <th colspan="5">exponent (5)</th>
    <th colspan="10">fraction (10)</th>
</tr>
<tr>
    <td>0</td>
    <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
    <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
</tr>
<tr><td></td></tr>
<tr><th colspan="16">bfp16 (brainfloat)</th></tr>
<tr>
    <th>sign</th>
    <th colspan="8">exponent (8)</th>
    <th colspan="7">fraction (7)</th>
</tr>
<tr>
    <td>0</td>
    <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>1</td> <td>1</td>
    <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td>
</tr>
</tbody></table>
</div>
<p>When reducing the fields of a fp32, fp16 and bfloat16 made different tradeoffs: fp16 tried to balance between range and precision by shrinking both the exponent and fraction fields, whereas bfloat16 preserved the range of fp32 by keeping an 8-bit exponent, while sacrificing precision by shrinking the fraction field smaller than fp16.
<a href="https://x.com/sytelus/status/1713462678226968973">The loss of range can sometimes be a problem for training in fp16</a>, but for inference either works, and fp16 is probably a better choice if your GPU doesn't support bfloat16.</p>
<h3 id="Even_smaller!"><a href="#Even_smaller!">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Even_smaller!">
</a>Even smaller!</h3>
<p>Can we go even smaller? Of course!</p>
<p>One approach is to quantize a model trained in a larger format, like fp16.
The llama.cpp project (and the associated ML library ggml) defines <a href="https://github.com/ggerganov/llama.cpp#quantization">a whole zoo of quantization formats</a> (the README is currently out of date, so make sure to check the <a href="https://github.com/ggerganov/llama.cpp/pull/1684">k-quants PR</a> as well), which can go down to less than 5 bits per weight from an fp32 or fp16 model.</p>
<p>These quantizations work a bit differently than fp16 / bfloat16—there isn't enough room to fit a whole number in that space, so instead the weights are quantized in <em>blocks</em>, where an fp16 acts as the block scale, and then the block of quantized weights are each multiplied against that scale. (In some formats, there's also a min value, and sometimes the scale and min are themselves quantized to still be smaller than fp16—it's complicated! See the k-quants PR for more details about how it's implemented in GGML, and <a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">this post</a> for more details about why quantization is challenging.)</p>
<p><a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> also implements quantization for non-llama.cpp projects.
<small>(I don't have much experience with it personally, though, besides dealing with it as a transitive dependency when it doesn't want to install on Lambda Labs instances :-))</small></p>
<p>However, the smaller you go with quantization of a model trained with wider parameters, the more it can start to affect the model's performance, reducing the quality of responses.
It's best to go with the least amount of quantization that will give you acceptable inference speed.</p>
<p>However, it's also possible to finetune or train models with datatypes smaller than fp16.
For example, you can train quantized low rank adapters with <a href="https://github.com/artidoro/qlora">qLoRA</a>, and <a href="https://arxiv.org/abs/2209.05433">a 2022 paper</a> demonstrated training 175B parameter language models in (simulated) fp8, achieving very similar results to fp16.</p>
<p>Note that, as of 2023, GPUs don't natively support datatypes smaller than fp16, except int8 (8 bit integer).
You can train and infer with int8 to some extent, but most quantization requires converting the weights from the quantized format to another type (like fp16) for calculation, and then back when they're no longer needed, which incurs some performance cost.
This can pay for itself based on how much memory your GPU has and how fast that memory is, but it's worth being aware of—quantization isn't free.</p>
<h2 id="KV_caching"><a href="#KV_caching">
  <img src="https://vgel.me/permalink.svg" alt="permalink for KV_caching">
</a>KV caching</h2>
<p>To explain this one, I'm going to borrow some diagrams from <a href="https://vgel.me/posts/handmade-transformer">my last post</a> about how Transformers work.
If this section feels too quick, please read that post for a (much) more in depth explanation!
This explanation is also based on GPT-2, since it's the model I covered in that post.
Other architectures work slightly differently—I'll explain the relevant differences, but most don't make too much of a difference for understanding KV caching.</p>
<p>Inside a Transformer, the activations run through a feed-forward layer to generate a <code>qkv</code> matrix, where each row corresponds to a token:</p>
<div>
<table>
<tbody><tr>
<th>token</th>
<th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th><th>q</th>
<th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th><th>k</th>
<th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th><th>v</th>
</tr>
<tr>
  <td>token 1</td>
  <td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>token 2</td>
  <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>token 3</td>
  <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>token 4</td>
  <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>token 5</td>
  <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>Then, the <code>qkv</code> matrix is split into <code>q</code>, <code>k</code>, and <code>v</code>, which are combined with attention like this:</p>
<div>
<p>softmax(</p>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td><td>0</td> </tr>
<tr> <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td> </tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">k.T</th></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
</tbody></table>
<p>+ mask)</p>
<p>@</p>
<table>
<tbody><tr><th colspan="8">v</th></tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>To produce a matrix like this:</p>
<div>
<table>
<tbody><tr>
  <td>Result for token 1</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(1 * 1)</td>
</tr>
<tr>
  <td>Result for token 2</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
<tr>
  <td>Result for token 3</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>(0.5*1 + 0.5*(-1))</td>
</tr>
<tr>
  <td>Result for token 4</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>
  <td>(0.5*(-1) + 0.5*1)</td>
</tr>
<tr>
  <td>Result for token 5</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
</tbody></table>
</div>
<p>Now depending on where this layer is in the Transformer, these rows might be used (after passing through an MLP) as the input to the next Transformer block, or be the predictions for the next token—but note that there's a row for every token!
That's because Transformers are trained to predict the next token for <em>every single token in the context window!</em></p>
<pre data-lang="python"><code data-lang="python"><span># the gpt2 tokenizer produces 3 tokens for this string
</span><span>&gt;&gt;&gt; </span><span>tokens </span><span>= </span><span>tokenizer(</span><span>" A B C"</span><span>).input_ids
</span><span>&gt;&gt;&gt; </span><span>tokens
</span><span>[</span><span>317</span><span>, </span><span>347</span><span>, </span><span>327</span><span>]
</span><span>
</span><span># if we put that into the model, we get 3 rows of logits
</span><span>&gt;&gt;&gt; </span><span>logits </span><span>= </span><span>gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(tokens)).logits.squeeze()
</span><span>&gt;&gt;&gt; </span><span>logits.shape
</span><span>torch.Size([</span><span>3</span><span>, </span><span>50257</span><span>])
</span><span>
</span><span># and if we argmax those, we see the model is predicting a next token
</span><span># for _every_ prompt token!
</span><span>&gt;&gt;&gt; for </span><span>i, y </span><span>in </span><span>enumerate</span><span>(logits.argmax(</span><span>-</span><span>1</span><span>)):
</span><span>...     </span><span>print</span><span>(</span><span>f</span><span>"</span><span>{tokenizer.decode(tokens[:i</span><span>+</span><span>1</span><span>])</span><span>!r</span><span>}</span><span> -&gt; </span><span>{tokenizer.decode(y)</span><span>!r</span><span>}</span><span>"</span><span>)
</span><span>' A' </span><span>-&gt; </span><span>'.'
</span><span>' A B' </span><span>-&gt; </span><span>' C'
</span><span>' A B C' </span><span>-&gt; </span><span>' D'
</span></code></pre>
<p>During training, this behavior is desirable—it means more information is flowing into the Transformer since many tokens are being graded instead of just one.
But usually during inference, all we care about is that bottom row, the prediction for the final token.</p>
<p>So how can we get just that out of a Transformer trained to predict the entire context?
Well, let's go back to the attention calculation.
What if <code>q</code> was only one row—the row corresponding to the last token?</p>
<div>
<p>softmax(</p>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr> <td>0</td><td>0</td><td>0</td><td>1024</td><td>1024</td><td>0</td><td>0</td><td>0</td> </tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">k.T</th></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
</tbody></table>
<p>+ mask)</p>
<p>@</p>
<table>
<tbody><tr><th colspan="8">v</th></tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
<tr>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
</tr>
</tbody></table>
</div>
<p>Then, we'd get this as the attention result—just the result for the last token, exactly like what we want.</p>
<div>
<table>
<tbody><tr>
  <td>Result for token 5</td>
  <td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td>
  <td>(0.5*1 + 0.5*1)</td>
</tr>
</tbody></table>
</div>
<p>So that's great, but to only generate the last row of <code>q</code>, that means we can only run the layer that generates the <code>qkv</code> matrix on a single row as well.
So where do the rest of the rows of <code>k</code> and <code>v</code> come from, since we still need them?
The answer is in the name—KV caching—we reuse them from the previous token generation step!
Inside the model, we save the KV values calculated during attention in each Transformer block.
Then on the next generation, only a single token will be passed in, and the cached KV rows will be stacked on top of the K and V row for the new token to produce the single row Q and multi-row K and V that we want.</p>
<p>Here's an example of KV caching with the HuggingFace <code>transformers</code> API, which actually returns the KV cache by default as part of the model forward pass.
The cache is a tuple with a <code>(k, v)</code> tuple for each layer.
The <code>k</code> and <code>v</code> tensors are each of shape <code>(batch_size, n_head, n_seq, head_size)</code>.</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>tokens
</span><span>[</span><span>317</span><span>, </span><span>347</span><span>, </span><span>327</span><span>] </span><span># the " A B C" string from before
</span><span>&gt;&gt;&gt; </span><span>key_values </span><span>= </span><span>gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(tokens)).past_key_values
</span><span>&gt;&gt;&gt; </span><span>tuple</span><span>(</span><span>tuple</span><span>(x.shape </span><span>for </span><span>x </span><span>in </span><span>t) </span><span>for </span><span>t </span><span>in </span><span>key_values)
</span><span>((torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])),
</span><span> (torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>]), torch.Size([</span><span>1</span><span>, </span><span>12</span><span>, </span><span>3</span><span>, </span><span>64</span><span>])))
</span></code></pre>
<p>If we pass this returned KV cache to a model forward pass, the model will treat the tokens we passed in to generate the cache as present even though we don't provide them again.
Note that we only pass a single token here, and only get a single row of logits back!</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>new_token </span><span>= </span><span>tokenizer(</span><span>" D"</span><span>).input_ids
</span><span>&gt;&gt;&gt; </span><span>new_token
</span><span>[</span><span>360</span><span>]
</span><span>&gt;&gt;&gt; </span><span>logits </span><span>= </span><span>gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(new_token), </span><span>past_key_values</span><span>=</span><span>key_values).logits
</span><span>&gt;&gt;&gt; </span><span>logits.shape
</span><span>torch.Size([</span><span>1</span><span>, </span><span>50257</span><span>])
</span><span>&gt;&gt;&gt; </span><span>tokenizer.decode(logits.argmax(</span><span>-</span><span>1</span><span>))
</span><span>' E'
</span></code></pre>
<p>Compare that to if we only pass the single token <em>without</em> passing <code>past_key_values</code>—we get a completion, but it's not conditioned on those previous tokens that the KV cache was generated from.</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; </span><span>tokenizer.decode(gpt2(</span><span>input_ids</span><span>=</span><span>torch.tensor(new_token)).logits.argmax(</span><span>-</span><span>1</span><span>))
</span><span>'.'
</span></code></pre>
<p><small>(Also note that e.g. lit-gpt has a nicer KV cache API that handles the cache for you, instead of needing to pass it around manually :-) )</small></p>
<p>KV caching helps with the algorithmic side of LLM slowness—since we're now only passing in a single token on each step, we don't have to redo <em>everything</em> for each new token.
However, it doesn't completely banish the problem, since the KV cache still grows in size each step, slowing down the attention calculation.
The size of the KV cache can also pose its own, new problem—for example, with a 1,000 token KV cache, even with the smallest GPT-2 there are 18,432,000 values being cached.
If each is an fp32, that's almost 74MB of cache, for a single generation, for a comparatively tiny model!
With modern large models, especially running on a server that needs to handle many simultaneous clients, the KV cache can quickly become unmanageable, so a few techniques have popped up to make it better.</p>
<h3 id="Multi-Query_Attention"><a href="#Multi-Query_Attention">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Multi-Query_Attention">
</a>Multi-Query Attention</h3>
<p>Multi-Query attention is a change to the model architecture that shrinks the size of the KV cache by assigning multiple heads to Q, and only a single head to K and V.
It needs to be trained into the model from the beginning—it's not just an inference-time optimization—but it's worth being aware of if you're trying to choose a model, because models with MQA can support more tokens in the KV cache than models trained with normal attention.
To understand that, first we need to understand multi-head attention, so let's digress into that for a second.</p>
<p>Modern LLMs don't usually perform attention on the entire QKV matrix at once like how I described above—instead, the KQV matrix is split into multiple smaller "heads".
That means instead of how it's shown in the diagram above, it looks more like this:</p>
<div>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>0.23</td><td>0.03</td><td>0.1</td><td>0.3</td><td>0.87</td><td>0.84</td><td>0.3</td><td>0.3</td></tr>
<tr><td>0.27</td><td>0.61</td><td>0.7</td><td>0.02</td><td>0.83</td><td>0.94</td><td>0.12</td><td>0.21</td></tr>
<tr><td>0.79</td><td>0.23</td><td>0.03</td><td>0.28</td><td>0.02</td><td>0.47</td><td>0.97</td><td>0.61</td></tr>
<tr><td>0.11</td><td>0.1</td><td>0.3</td><td>1.0</td><td>0.08</td><td>0.88</td><td>0.83</td><td>0.69</td></tr>
<tr><td>0.07</td><td>0.01</td><td>0.16</td><td>0.05</td><td>0.51</td><td>0.54</td><td>0.23</td><td>0.47</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">q/0</th></tr>
<tr><td>0.23</td><td>0.03</td></tr>
<tr><td>0.27</td><td>0.61</td></tr>
<tr><td>0.79</td><td>0.23</td></tr>
<tr><td>0.11</td><td>0.1</td></tr>
<tr><td>0.07</td><td>0.01</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/1</th></tr>
<tr><td>0.1</td><td>0.3</td></tr>
<tr><td>0.7</td><td>0.02</td></tr>
<tr><td>0.03</td><td>0.28</td></tr>
<tr><td>0.3</td><td>1.0</td></tr>
<tr><td>0.16</td><td>0.05</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/2</th></tr>
<tr><td>0.87</td><td>0.84</td></tr>
<tr><td>0.83</td><td>0.94</td></tr>
<tr><td>0.02</td><td>0.47</td></tr>
<tr><td>0.08</td><td>0.88</td></tr>
<tr><td>0.51</td><td>0.54</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/3</th></tr>
<tr><td>0.3</td><td>0.3</td></tr>
<tr><td>0.12</td><td>0.21</td></tr>
<tr><td>0.97</td><td>0.61</td></tr>
<tr><td>0.83</td><td>0.69</td></tr>
<tr><td>0.23</td><td>0.47</td></tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr><th colspan="8">k</th></tr>
<tr><td>0.08</td><td>0.41</td><td>0.36</td><td>0.1</td><td>0.15</td><td>0.03</td><td>0.95</td><td>0.16</td></tr>
<tr><td>0.7</td><td>0.77</td><td>0.57</td><td>0.9</td><td>0.65</td><td>0.36</td><td>0.58</td><td>0.32</td></tr>
<tr><td>0.77</td><td>0.29</td><td>0.42</td><td>0.58</td><td>0.16</td><td>0.49</td><td>0.17</td><td>0.73</td></tr>
<tr><td>0.94</td><td>0.36</td><td>0.16</td><td>0.03</td><td>0.31</td><td>0.67</td><td>0.81</td><td>0.94</td></tr>
<tr><td>0.76</td><td>1.0</td><td>0.45</td><td>0.94</td><td>0.6</td><td>0.49</td><td>0.68</td><td>0.54</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">k/0</th></tr>
<tr><td>0.08</td><td>0.41</td></tr>
<tr><td>0.7</td><td>0.77</td></tr>
<tr><td>0.77</td><td>0.29</td></tr>
<tr><td>0.94</td><td>0.36</td></tr>
<tr><td>0.76</td><td>1.0</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">k/1</th></tr>
<tr><td>0.36</td><td>0.1</td></tr>
<tr><td>0.57</td><td>0.9</td></tr>
<tr><td>0.42</td><td>0.58</td></tr>
<tr><td>0.16</td><td>0.03</td></tr>
<tr><td>0.45</td><td>0.94</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">k/2</th></tr>
<tr><td>0.15</td><td>0.03</td></tr>
<tr><td>0.65</td><td>0.36</td></tr>
<tr><td>0.16</td><td>0.49</td></tr>
<tr><td>0.31</td><td>0.67</td></tr>
<tr><td>0.6</td><td>0.49</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">k/3</th></tr>
<tr><td>0.95</td><td>0.16</td></tr>
<tr><td>0.58</td><td>0.32</td></tr>
<tr><td>0.17</td><td>0.73</td></tr>
<tr><td>0.81</td><td>0.94</td></tr>
<tr><td>0.68</td><td>0.54</td></tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr><th colspan="8">v</th></tr>
<tr><td>0.54</td><td>0.8</td><td>0.73</td><td>0.35</td><td>0.97</td><td>0.05</td><td>0.07</td><td>0.45</td></tr>
<tr><td>0.81</td><td>0.42</td><td>0.13</td><td>0.33</td><td>0.6</td><td>0.75</td><td>0.41</td><td>0.36</td></tr>
<tr><td>0.81</td><td>0.47</td><td>0.2</td><td>0.05</td><td>0.63</td><td>0.75</td><td>0.58</td><td>0.66</td></tr>
<tr><td>0.69</td><td>0.89</td><td>0.09</td><td>0.49</td><td>0.49</td><td>0.63</td><td>0.91</td><td>0.88</td></tr>
<tr><td>0.19</td><td>0.39</td><td>0.22</td><td>0.36</td><td>1.0</td><td>0.17</td><td>0.66</td><td>0.02</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">v/0</th></tr>
<tr><td>0.54</td><td>0.8</td></tr>
<tr><td>0.81</td><td>0.42</td></tr>
<tr><td>0.81</td><td>0.47</td></tr>
<tr><td>0.69</td><td>0.89</td></tr>
<tr><td>0.19</td><td>0.39</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">v/1</th></tr>
<tr><td>0.73</td><td>0.35</td></tr>
<tr><td>0.13</td><td>0.33</td></tr>
<tr><td>0.2</td><td>0.05</td></tr>
<tr><td>0.09</td><td>0.49</td></tr>
<tr><td>0.22</td><td>0.36</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">v/2</th></tr>
<tr><td>0.97</td><td>0.05</td></tr>
<tr><td>0.6</td><td>0.75</td></tr>
<tr><td>0.63</td><td>0.75</td></tr>
<tr><td>0.49</td><td>0.63</td></tr>
<tr><td>1.0</td><td>0.17</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">v/3</th></tr>
<tr><td>0.07</td><td>0.45</td></tr>
<tr><td>0.41</td><td>0.36</td></tr>
<tr><td>0.58</td><td>0.66</td></tr>
<tr><td>0.91</td><td>0.88</td></tr>
<tr><td>0.66</td><td>0.02</td></tr>
</tbody></table>
</div>
<p>Then each head is combined in attention as before:</p>
<div>
<p>softmax(</p>
<table>
<tbody><tr><th colspan="2">q/0</th></tr>
<tr><td>0.23</td><td>0.03</td></tr>
<tr><td>0.27</td><td>0.61</td></tr>
<tr><td>0.79</td><td>0.23</td></tr>
<tr><td>0.11</td><td>0.1</td></tr>
<tr><td>0.07</td><td>0.01</td></tr>
</tbody></table>
<p>@</p>
<table>
<tbody><tr><th colspan="5">(k/0).T</th></tr>
<tr><td>0.08</td><td>0.7</td><td>0.77</td><td>0.94</td><td>0.76</td></tr>
<tr><td>0.41</td><td>0.77</td><td>0.29</td><td>0.36</td><td>1.0</td></tr>
</tbody></table>
<p>+ mask)</p>
<p>@</p>
<table>
<tbody><tr><th colspan="2">v/0</th></tr>
<tr><td>0.54</td><td>0.8</td></tr>
<tr><td>0.81</td><td>0.42</td></tr>
<tr><td>0.81</td><td>0.47</td></tr>
<tr><td>0.69</td><td>0.89</td></tr>
<tr><td>0.19</td><td>0.39</td></tr>
</tbody></table>
</div>
<p>The individual small result matrices are then stuck back together to recreate a final result matrix of shape <code>(seq_len, embed_size)</code>, just like the result of vanilla attention.
This process allows each head to be used for a different task (e.g., one head could handle punctuation in acronyms and another French articles), instead of wastefully dedicating an entire Transformer block to a single task.</p>
<p>So what is <a href="https://arxiv.org/abs/1911.02150">Multi-Query Attention</a>?
Instead of Q, K, and V all being split into separate heads, <em>only</em> Q is split.
K and V are smaller, the size of a single head, and that single K and V is shared among all the Q heads.</p>
<div>
<table>
<tbody><tr><th colspan="8">q</th></tr>
<tr><td>0.23</td><td>0.03</td><td>0.1</td><td>0.3</td><td>0.87</td><td>0.84</td><td>0.3</td><td>0.3</td></tr>
<tr><td>0.27</td><td>0.61</td><td>0.7</td><td>0.02</td><td>0.83</td><td>0.94</td><td>0.12</td><td>0.21</td></tr>
<tr><td>0.79</td><td>0.23</td><td>0.03</td><td>0.28</td><td>0.02</td><td>0.47</td><td>0.97</td><td>0.61</td></tr>
<tr><td>0.11</td><td>0.1</td><td>0.3</td><td>1.0</td><td>0.08</td><td>0.88</td><td>0.83</td><td>0.69</td></tr>
<tr><td>0.07</td><td>0.01</td><td>0.16</td><td>0.05</td><td>0.51</td><td>0.54</td><td>0.23</td><td>0.47</td></tr>
</tbody></table>
<p>→</p>
<table>
<tbody><tr><th colspan="2">q/0</th></tr>
<tr><td>0.23</td><td>0.03</td></tr>
<tr><td>0.27</td><td>0.61</td></tr>
<tr><td>0.79</td><td>0.23</td></tr>
<tr><td>0.11</td><td>0.1</td></tr>
<tr><td>0.07</td><td>0.01</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/1</th></tr>
<tr><td>0.1</td><td>0.3</td></tr>
<tr><td>0.7</td><td>0.02</td></tr>
<tr><td>0.03</td><td>0.28</td></tr>
<tr><td>0.3</td><td>1.0</td></tr>
<tr><td>0.16</td><td>0.05</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/2</th></tr>
<tr><td>0.87</td><td>0.84</td></tr>
<tr><td>0.83</td><td>0.94</td></tr>
<tr><td>0.02</td><td>0.47</td></tr>
<tr><td>0.08</td><td>0.88</td></tr>
<tr><td>0.51</td><td>0.54</td></tr>
</tbody></table>
<table>
<tbody><tr><th colspan="2">q/3</th></tr>
<tr><td>0.3</td><td>0.3</td></tr>
<tr><td>0.12</td><td>0.21</td></tr>
<tr><td>0.97</td><td>0.61</td></tr>
<tr><td>0.83</td><td>0.69</td></tr>
<tr><td>0.23</td><td>0.47</td></tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr><th colspan="2">k</th></tr>
<tr><td>0.08</td><td>0.41</td></tr>
<tr><td>0.7</td><td>0.77</td></tr>
<tr><td>0.77</td><td>0.29</td></tr>
<tr><td>0.94</td><td>0.36</td></tr>
<tr><td>0.76</td><td>1.0</td></tr>
</tbody></table>

<table>
<tbody><tr><th colspan="2">v</th></tr>
<tr><td>0.54</td><td>0.8</td></tr>
<tr><td>0.81</td><td>0.42</td></tr>
<tr><td>0.81</td><td>0.47</td></tr>
<tr><td>0.69</td><td>0.89</td></tr>
<tr><td>0.19</td><td>0.39</td></tr>
</tbody></table>
</div>
<p>You might think this would be a serious problem for the model, but it actually has only a small effect on perplexity.
This table from the MQA paper shows slightly worse results than the Multi-Head Attention baseline, but better than alternatives involving shrinking all the dimensions of MHA.</p>
<p><a href="https://vgel.me/posts/faster-inference/multi_query_table3.png"><img src="https://vgel.me/posts/faster-inference/multi_query_table3.png" alt="dev-PPL of MHA: 29.9, MQA: 30.2, best MHA with smaller dimensions: 30.9"></a></p>
<p>The benefit is, because K and V are so much smaller than in MHA, the KV cache is proportionally smaller as well.
Both LLAMA-2 and Falcon use MQA, for this reason.</p>
<p>Mistral 7B uses a variant called <a href="https://arxiv.org/abs/2305.13245v2">Grouped-Query Attention</a> which is a hybrid between MQA and MHA.
If MHA is <code>Q_heads=K_heads=V_heads=N</code> and MQA is <code>Q_heads=N; K_heads=V_heads=1</code>, then GQA is <code>Q_heads=N; K_heads=V_heads=G</code> where <code>1 &lt; G &lt; N</code>.
GQA claims less effect on perplexity and better training stability than MQA.</p>
<h3 id="PagedAttention"><a href="#PagedAttention">
  <img src="https://vgel.me/permalink.svg" alt="permalink for PagedAttention">
</a>PagedAttention</h3>
<p>The other issue with a large KV cache is that it often needs to be stored as in <em>contiguous</em> tensors, regardless of whether all of the cache is currently in use.
That leads to multiple problems:</p>
<ul>
<li>More space than necessary needs to be allocated up front, since we need to anticipate the maximum size of the KV cache before it's needed.</li>
<li>That reserved space can't be used by other requests, even if it isn't needed <em>yet</em>.</li>
<li>Requests with the same prefix can't share KV cache for that prefix, since they may diverge later.</li>
</ul>
<p><a href="https://arxiv.org/abs/2309.06180">PagedAttention</a> fixes these problems by taking inspiration from how operating systems handle a similar issue with userspace program memory.</p>
<p>Let's take a moment to explore OS paging, as a primer.
Like tensors, programs want to see their memory as a contiguous linear space.
<small>(If I allocate a million-byte array <code>x</code>, I expect the address of <code>x[n + 1]</code> to exactly equal <code>x[n] + 1</code>, no more, no less! Much code depends on this.)</small>
However, physical memory isn't always so forgiving—operating systems have to worry about pesky things like "fragmentation" and <a href="https://vgel.me/posts/mmap-arena-alloc/">"hey you asked for a 1TiB allocation i cannot put this anywhere"</a>.</p>
<p>So the operating system collaborates with hardware, using the <abbr title="Memory Management Unit">MMU</abbr> to map virtual pages to physical pages in a page table.
When you access an address in a userspace program, that address gets translated from your program's address space via the page table <small>(and TLB cache)</small> to a physical address before being read from or written to.
Importantly, that physical address may not exist yet—it may be generated on-demand for a write.
For example, let's map 16 pages of memory in C:</p>
<pre data-lang="c"><code data-lang="c"><span>
</span><span>uint8_t </span><span>*</span><span>pages </span><span>= </span><span>mmap(</span><span>NULL</span><span>, page_size </span><span>* </span><span>16</span><span>,
</span><span>                      PROT_READ </span><span>|</span><span> PROT_WRITE,
</span><span>                      MAP_PRIVATE </span><span>|</span><span> MAP_ANONYMOUS,
</span><span>                      </span><span>-</span><span>1</span><span>, </span><span>0</span><span>);
</span></code></pre>
<p>Now if we <a href="https://stackoverflow.com/a/45500208/1159735">pagemap-dump</a> the running program, we get this list of page addresses and associated <code>pfn</code> physical addresses (along with other metadata):</p>
<pre data-lang="bash"><code data-lang="bash"><span># addr pfn soft-dirty file/shared swapped present library
</span><span>7fbe0fe6a000 0 1 0 0 0 
</span><span>7fbe0fe6b000 0 1 0 0 0 
</span><span>7fbe0fe6c000 0 1 0 0 0 
</span><span>7fbe0fe6d000 0 1 0 0 0 
</span><span>7fbe0fe6e000 0 1 0 0 0 
</span><span>7fbe0fe6f000 0 1 0 0 0 
</span><span>7fbe0fe70000 0 1 0 0 0 
</span><span>7fbe0fe71000 0 1 0 0 0 
</span><span>7fbe0fe72000 0 1 0 0 0 
</span><span>7fbe0fe73000 0 1 0 0 0 
</span><span>7fbe0fe74000 0 1 0 0 0 
</span><span>7fbe0fe75000 0 1 0 0 0 
</span><span>7fbe0fe76000 0 1 0 0 0 
</span><span>7fbe0fe77000 0 1 0 0 0 
</span><span>7fbe0fe78000 0 1 0 0 0 
</span><span>7fbe0fe79000 0 1 0 0 0 
</span></code></pre>
<p>Notice that all the pages have a physical address of zero—they don't exist yet!
This is called <em>memory overcommit</em>.
The kernel doesn't know if we're going to use these pages, so it doesn't bother to set up mappings for them yet.
Trying to read from them will just return an unspecified value (this is part of why reading uninitialized memory is UB in C).</p>
<p>However, if I then touch every page by writing to it...</p>
<pre data-lang="c"><code data-lang="c"><span>// pages is our page_size * 16 map from earlier
</span><span>for </span><span>(</span><span>int</span><span> i </span><span>= </span><span>0</span><span>; i </span><span>&lt;</span><span> page_size </span><span>* </span><span>16</span><span>; i</span><span>++</span><span>) pages[i] </span><span>= </span><span>1</span><span>;
</span></code></pre>
<p>...the dump looks different!</p>
<pre data-lang="bash"><code data-lang="bash"><span># addr pfn soft-dirty file/shared swapped present library
</span><span>7fbe0fe6a000 14a009 1 0 0 1 
</span><span>7fbe0fe6b000 50eca5 1 0 0 1 
</span><span>7fbe0fe6c000 175a5d 1 0 0 1 
</span><span>7fbe0fe6d000 148d85 1 0 0 1 
</span><span>7fbe0fe6e000 2de86c 1 0 0 1 
</span><span>7fbe0fe6f000 13a300 1 0 0 1 
</span><span>7fbe0fe70000 8f25b4 1 0 0 1 
</span><span>7fbe0fe71000 15ae63 1 0 0 1 
</span><span>7fbe0fe72000 6e1d7f 1 0 0 1 
</span><span>7fbe0fe73000 13a307 1 0 0 1 
</span><span>7fbe0fe74000 14a074 1 0 0 1 
</span><span>7fbe0fe75000 14a0a7 1 0 0 1 
</span><span>7fbe0fe76000 7e2662 1 0 0 1 
</span><span>7fbe0fe77000 1ccdc2 1 0 0 1 
</span><span>7fbe0fe78000 2a4f06 1 0 0 1 
</span><span>7fbe0fe79000 2169ef 1 0 0 1 
</span></code></pre>
<p>Now all the pages have a physical address, because they've been written to.
However, note that the physical addresses aren't contiguous like the virtual addresses!
<small>(The largest physical address is 0x7e2662, which is the mapping for virtual address 0x7fbe0fe76000, page #13.)</small>
They're scattered all over the place, wherever they can fit.
And if our C program had only touched e.g. half the pages, only those pages would've been mapped, the others would have remained unmapped.
This means that no physical memory is reserved until the exact moment that it's needed.</p>
<p>I could share only part of this memory with another process.
Imagine I mapped 4 pages of shared memory:</p>
<pre data-lang="bash"><code data-lang="bash"><span>7fbe0fe6a000 14a009
</span><span>7fbe0fe6b000 50eca5 
</span><span>7fbe0fe6c000 175a5d
</span><span>7fbe0fe6d000 148d85
</span></code></pre>
<p>The other process might see these pages as:</p>
<pre data-lang="bash"><code data-lang="bash"><span>5581627bb000 14a009
</span><span>5581627bc000 50eca5
</span><span>5581627bd000 175a5d
</span><span>5581627be000 148d85
</span></code></pre>
<p>The virtual addresses are different, but the physical addresses are identical!
Each program might see these pages interleaved in different contexts, but the underlying data can be stored, deduplicated, in a single place in physical memory.</p>
<p>So how does this apply to PagedAttention?
PagedAttention is the <em>exact same idea</em>—they say so in the paper.<sup><a href="#why-not-mmu">4</a></sup></p>
<p>Instead of pages, we have blocks of KV cache for tokens, and instead of processes accessing those pages, we have LLM requests accessing those blocks of tokens.</p>
<p>At startup, PagedAttention allocates a <em>block table</em> for the request, analogous to the role of a hardware MMU.
This block table starts out empty, with no blocks mapped, just like our C process.</p>
<p>Instead of associating requests with a large tensor of KV cache items, each request only has a comparatively small list of block indices, analogous to virtual addresses in OS paging.
Those indices point at blocks stored in the global block table.
Just like OS pages, they can be out of order, placed wherever they can fit:</p>
<p><a href="https://vgel.me/posts/faster-inference/pagedattention.png"><img src="https://vgel.me/posts/faster-inference/pagedattention.png" alt=""></a></p>
<p>During the attention computation, the PagedAttention kernel walks the request's list of block indices, and goes and fetches those blocks from the global block table to compute attention as normal in the correct order.</p>
<p>Importantly, because the blocks have been decoupled from individual requests, they can be shared, just like the shared memory example in OS paging.
If two requests use the same long prefix (such as k-shot examples for multiple parallel translation tasks, the newest Twitter prompt hack, the chain so far for self-consistency Chain of Thought, etc.), the KV cache blocks for that prefix can be shared by multiple requests, simply by placing the index of that block in the appropriate part of each request's list of block indices.</p>
<p><a href="https://vgel.me/posts/faster-inference/shared-prefix-pagedattention.drawio.png"><img src="https://vgel.me/posts/faster-inference/shared-prefix-pagedattention.drawio.png" alt=""></a></p>
<h2 id="Speculative_Decoding"><a href="#Speculative_Decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Speculative_Decoding">
</a>Speculative Decoding</h2>
<p>To understand speculative decoding, you need to remember three things.</p>
<p>First, running a small number of tokens through a model takes about the same amount of time as running a single token, thanks to memory access overhead:</p>
<p><a href="https://vgel.me/posts/faster-inference/timing_graph.png"><img src="https://vgel.me/posts/faster-inference/timing_graph.png" alt="Log-log plot of time to generate N tokens, in seconds, showing a flat region from 1-10 tokens that then goes linear"></a></p>
<p>Second, LLMs generate a prediction for <em>every</em> token in the context:</p>
<pre data-lang="python"><code data-lang="python"><span>&gt;&gt;&gt; for </span><span>i, y </span><span>in </span><span>enumerate</span><span>(logits.argmax(</span><span>-</span><span>1</span><span>)):
</span><span>...     </span><span>print</span><span>(</span><span>f</span><span>"</span><span>{tokenizer.decode(tokens[:i</span><span>+</span><span>1</span><span>])</span><span>!r</span><span>}</span><span> -&gt; </span><span>{tokenizer.decode(y)</span><span>!r</span><span>}</span><span>"</span><span>)
</span><span>' A' </span><span>-&gt; </span><span>'.'
</span><span>' A B' </span><span>-&gt; </span><span>' C'
</span><span>' A B C' </span><span>-&gt; </span><span>' D'
</span></code></pre>
<p>Finally third, some words are very easy to predict.
For example, after the word "going", you don't need to be GPT-4 to know that the word "to" is an extremely likely next token.</p>
<p>We can take advantage of these facts to optimize generation!
Imagine if, whenever the most recent token was " going", we optimistically tacked " to" onto the end as well before running generation.
Then, after running the model forward, we check if the model's prediction after "going" was indeed "to".
if so, we got a token ("to") for free!
And if not, no sweat, we simply accept the token predicted for "going" instead, with no increased perplexity, since that token is the exact token the model would have generated without our trick.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>generate</span><span>(</span><span>prompt</span><span>: </span><span>str</span><span>, </span><span>tokens_to_generate</span><span>: </span><span>int</span><span>) -&gt; </span><span>str</span><span>:
</span><span>    tokens: </span><span>list</span><span>[</span><span>int</span><span>] </span><span>= </span><span>tokenize(prompt)
</span><span>    GOING, TO </span><span>= </span><span>tokenize(</span><span>" going to"</span><span>)
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(tokens_to_generate):
</span><span>        </span><span>if </span><span>tokens[</span><span>-</span><span>1</span><span>] </span><span>== </span><span>GOING:
</span><span>          </span><span># do our speculative decoding trick
</span><span>          logits </span><span>= </span><span>model.forward(tokens </span><span>+ </span><span>[TO])
</span><span>          </span><span># the token the model predicts will follow "... going"
</span><span>          going_pred </span><span>= </span><span>argmax(logits[</span><span>-</span><span>2</span><span>, :])
</span><span>          </span><span># the token the model predicts will follow "... going to" 
</span><span>          to_pred </span><span>= </span><span>argmax(logits[</span><span>-</span><span>1</span><span>, :])
</span><span>          </span><span>if </span><span>going_pred </span><span>== </span><span>TO:
</span><span>            </span><span># if our guess was correct, accept "to" and the next token after
</span><span>            tokens </span><span>+= </span><span>[TO, to_pred]
</span><span>          </span><span>else</span><span>:
</span><span>            </span><span># otherwise, accept the real next token
</span><span>            </span><span># (e.g. "for" if the true generation was "going for broke")
</span><span>            tokens </span><span>+= </span><span>[going_pred]
</span><span>        </span><span>else</span><span>:
</span><span>          </span><span># do normal single-token generation
</span><span>          logits </span><span>= </span><span>model.forward(tokens)
</span><span>          tokens </span><span>+= </span><span>[argmax(logits[</span><span>-</span><span>1</span><span>])]
</span><span>
</span><span>    </span><span>return </span><span>detokenize(tokens)
</span></code></pre>
<p>This will absolutely work, and buy you a (minuscule) speed boost.
To improve it, you could imagine making more heuristics: predict "and" after a comma, and "the" after "is".
You could even make multi-word heuristics: if you see "The early bird", why not optimistically add "catches the worm"?
Even if they're doing a twist on the phrase, you could still win "catches" and "the", the entire phrase doesn't need to be accepted.</p>
<p>But wait a second—why make these heuristics by hand?
We're trying to come up with likely completions of a token... that's exactly what language models are good at!
If we use a language model to generate our optimistic tokens, we could pick up even more complex patterns, even ones from the earlier context.</p>
<p>We just need to use a "draft" model that's small enough (and therefore quick enough to run) that it will pay for itself by avoiding passes through the larger "oracle" model.
A good rule of thumb is for this model to be ~1/10 the size of the oracle model.
It should also use the same tokenizer (to avoid needing to detokenize and retokenize the sequence over and over).</p>
<p>Here's what the generate loop looks like with a draft model:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>generate</span><span>(</span><span>prompt</span><span>: </span><span>str</span><span>, </span><span>tokens_to_generate</span><span>: </span><span>int</span><span>, </span><span>n_draft</span><span>: </span><span>int </span><span>= </span><span>8</span><span>) -&gt; </span><span>str</span><span>:
</span><span>    tokens: </span><span>list</span><span>[</span><span>int</span><span>] </span><span>= </span><span>tokenize(prompt)
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(tokens_to_generate):
</span><span>        </span><span># generate `n_draft` draft tokens in the usual autoregressive way
</span><span>        draft </span><span>= </span><span>tokens[:]
</span><span>        </span><span>for </span><span>_ </span><span>in </span><span>range</span><span>(n_draft):
</span><span>            logits </span><span>= </span><span>draft_model.forward(draft)
</span><span>            draft.append(argmax(logits[</span><span>-</span><span>1</span><span>]))
</span><span>
</span><span>        </span><span># run the draft tokens through the oracle model all at once
</span><span>        logits </span><span>= </span><span>model.forward(draft)
</span><span>        checked </span><span>= </span><span>logits[</span><span>len</span><span>(tokens) </span><span>- </span><span>1 </span><span>:].argmax(</span><span>-</span><span>1</span><span>)
</span><span>
</span><span>        </span><span># find the index of the first draft/oracle mismatch—we'll accept every
</span><span>        </span><span># token before it
</span><span>        </span><span># (the index might be past the end of the draft, if every draft token
</span><span>        </span><span># was correct)
</span><span>        n_accepted </span><span>= </span><span>next</span><span>(
</span><span>            idx </span><span>+ </span><span>1
</span><span>            </span><span>for </span><span>idx, (checked, draft) </span><span>in </span><span>enumerate</span><span>(
</span><span>                </span><span># we add None here because the oracle model generates one extra
</span><span>                </span><span># token (the prediction for the last draft token)
</span><span>                </span><span>zip</span><span>(checked, draft[</span><span>len</span><span>(tokens) :] </span><span>+ </span><span>[</span><span>None</span><span>])
</span><span>            )
</span><span>            </span><span>if </span><span>checked </span><span>!= </span><span>draft
</span><span>        )
</span><span>        tokens.extend(checked[:n_accepted])
</span><span>
</span><span>    </span><span>return </span><span>detokenize(tokens)
</span></code></pre>
<p>Here, I used the above loop with GPT-2-XL (1558M parameters) as the oracle model, and GPT-2-small (124M parameters) as the draft model, with <code>n_draft=8</code>.
The green tokens were generated by the draft model, and the blue tokens are where the draft model was incorrect and the oracle model's corrected token had to be used.</p>
<p><span>What is a tensor in machine learning?</span>
<span>&nbsp;<br></span>
<span>A</span><span> tens</span><span>or</span><span> is</span><span> a</span><span> mathematical</span><span> object</span><span> that</span><span> represents</span><span> a</span><span> set</span><span> of</span><span> data</span><span> points</span><span>.</span><span> It</span><span> is</span><span> a</span><span> mathematical</span><span> object</span><span> that</span><span> can</span><span> be</span><span> used</span><span> to</span><span> represent</span><span> a</span><span> set</span><span> of</span><span> data</span><span> points</span><span>.</span>
</p>
<p>Note how the draft model is particularly good at quickly copying text from the question ("A tensor is a"), completing common N-grams ("It" -&gt; "is a", "can" -&gt; "can be"), and inserting stock phrases ("a set of data points"), so that the oracle model only needs to step in for a few key words.</p>
<p>Here's another example, with the same setup but a different prompt:</p>
<p><span>Index: A B C</span><span> D</span><span> E</span><span> F</span><span> G</span><span> H</span><span> I</span><span> J</span><span> K</span><span> L</span><span> M</span><span> N</span><span> O</span><span> P</span><span> Q</span><span> R</span><span> S</span><span> T</span><span> U</span><span> V</span><span> W</span><span> X</span><span> Y</span><span> Z</span><span>
</span><span>
</span><span>The</span><span> following</span><span> table</span><span> lists</span><span> the</span><span> number</span><span> of</span><span> times</span><span> each</span><span> of</span><span> the</span><span> following</span><span> words</span><span> appear</span><span> in</span><span> the</span><span> text</span><span> of</span><span> the</span><span> book</span><span>.</span><span>
</span><span>
</span><span>Word</span><span> Number</span><span> of</span><span> times</span><span> in</span><span> text</span>
</p>
<p>Here, the draft model does very well in the alphabet part, actually hitting the draft limit several times (the draft model would have correctly predicted "L", for example, but we limit it to 8 tokens at once).
But once it got into the prose generation below, the draft model couldn't keep up as well.</p>
<p>Finally, here's a pathological case:</p>
<p><span>The digits of Pi are 3.14159</span><span>265</span><span>35</span><span>89</span><span>793</span><span>238</span><span>46</span><span>264</span><span>33</span><span>83</span><span>279</span><span>50</span><span>28</span><span>84</span><span>197</span><span>16</span><span>93</span><span>99</span><span>375</span><span>10</span><span>58</span><span>20</span>
</p>
<p>At first the models are in agreement, but fairly quickly they diverge as the draft model becomes inaccurate, and generation becomes unbearably slow.
For every token, 8 draft tokens are generated and then immediately discarded.</p>
<p>This shows that speculative decoding performance can be very context dependent!
If the draft model is well-correlated with the oracle model and the text is easy to predict, you'll get lots of drafted tokens and fast inference.
But if the models aren't correlated, speculative decoding can actually make inference <em>slower</em>, because you're wasting time generating draft tokens that will just be rejected.</p>
<h3 id="Threshold_decoding?"><a href="#Threshold_decoding?">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Threshold_decoding?">
</a>Threshold decoding?</h3>
<p>An approach I came up with to mitigate the issues with using a fixed number of draft tokens is <em>threshold decoding</em>.</p>
<p>Instead of always decoding up to the maximum number of draft tokens, we keep a moving probability threshold, calibrated based on how many tokens are being accepted right now.
Draft tokens are generated until the cumulative probability of the draft so far (based on the draft model logits) falls below this threshold.</p>
<p>For example, if the threshold was 0.5, and the we generated a draft token " the" with a probability of 0.75, we'd keep going.
If the next token, " next", had a probability of 0.5, the cumulative probability 0.375 would be lower than the threshold, so we'd stop and submit the two draft tokens to the oracle.</p>
<p>Then, based on how far into the draft is accepted, the threshold is adjusted up or down to try and calibrate the draft model's confidence with the actual acceptance rate.
Right now this is just done by a simple moving average and some thresholding, but there's probably a more principled way to do it based on real statistics.</p>
<p>This is the code (using my homegrown framework, apologies):</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>speculative_threshold</span><span>(
</span><span>    </span><span>prompt</span><span>: </span><span>str</span><span>,
</span><span>    </span><span>max_draft</span><span>: </span><span>int </span><span>= </span><span>16</span><span>,
</span><span>    </span><span>threshold</span><span>: </span><span>float </span><span>= </span><span>0.4</span><span>,
</span><span>    </span><span>threshold_all_correct_boost</span><span>: </span><span>float </span><span>= </span><span>0.1</span><span>,
</span><span>):
</span><span>    tokens </span><span>= </span><span>encoder.encode(prompt)
</span><span>
</span><span>    </span><span># homegrown KV cache setup has an `n_tokens` method that returns the length
</span><span>    </span><span># of the cached sequence, and a `truncate` method to truncate that sequence
</span><span>    </span><span># to a specific token
</span><span>    model_kv </span><span>= </span><span>gpt2.KVCache()
</span><span>    draft_kv </span><span>= </span><span>gpt2.KVCache()
</span><span>
</span><span>    </span><span>while </span><span>True</span><span>:
</span><span>        </span><span># generate up to `max_draft` draft tokens autoregressively, stopping
</span><span>        </span><span># early if we fall below `threshold`
</span><span>        draft </span><span>= </span><span>tokens[:]
</span><span>        drafted_probs </span><span>= </span><span>[]
</span><span>        </span><span>for </span><span>_ </span><span>in </span><span>range</span><span>(max_draft):
</span><span>            logits </span><span>= </span><span>draft_model.forward(draft[draft_kv.n_tokens() :], draft_kv)
</span><span>            next_id </span><span>= </span><span>np.argmax(logits[</span><span>-</span><span>1</span><span>])
</span><span>            next_prob </span><span>= </span><span>gpt2.softmax(logits[</span><span>-</span><span>1</span><span>])[next_id]
</span><span>            </span><span>if not </span><span>len</span><span>(drafted_probs):
</span><span>                drafted_probs.append(next_prob)
</span><span>            </span><span>else</span><span>:
</span><span>                drafted_probs.append(next_prob </span><span>* </span><span>drafted_probs[</span><span>-</span><span>1</span><span>])
</span><span>            draft.append(</span><span>int</span><span>(next_id))
</span><span>            </span><span>if </span><span>drafted_probs[</span><span>-</span><span>1</span><span>] </span><span>&lt; </span><span>threshold:
</span><span>                </span><span>break
</span><span>        n_draft </span><span>= </span><span>len</span><span>(draft) </span><span>- </span><span>len</span><span>(tokens)
</span><span>
</span><span>        </span><span># run draft tokens through the oracle model
</span><span>        logits </span><span>= </span><span>model.forward(draft[model_kv.n_tokens() :], model_kv)
</span><span>        checked </span><span>= </span><span>logits[</span><span>-</span><span>n_draft </span><span>- </span><span>1 </span><span>:].argmax(</span><span>-</span><span>1</span><span>)
</span><span>        n_accepted </span><span>= </span><span>next</span><span>(
</span><span>            idx </span><span>+ </span><span>1
</span><span>            </span><span>for </span><span>idx, (checked, draft) </span><span>in </span><span>enumerate</span><span>(
</span><span>                </span><span>zip</span><span>(checked, draft[</span><span>len</span><span>(tokens) :] </span><span>+ </span><span>[</span><span>None</span><span>])
</span><span>            )
</span><span>            </span><span>if </span><span>checked </span><span>!= </span><span>draft
</span><span>        )
</span><span>        </span><span>yield from </span><span>checked[:n_accepted]
</span><span>        tokens.extend(checked[:n_accepted])
</span><span>
</span><span>        </span><span>if </span><span>n_accepted </span><span>&lt;= </span><span>n_draft:
</span><span>            </span><span># adjust threshold towards prob of last accepted token, if we
</span><span>            </span><span># ignored any draft tokens
</span><span>            threshold </span><span>= </span><span>(threshold </span><span>+ </span><span>drafted_probs[n_accepted </span><span>- </span><span>1</span><span>]) </span><span>/ </span><span>2
</span><span>        </span><span>else</span><span>:
</span><span>            </span><span># otherwise, lower the threshold slightly, we're probably being
</span><span>            </span><span># too conservative
</span><span>            threshold </span><span>-= </span><span>threshold_all_correct_boost
</span><span>        </span><span># clamp to avoid pathological thresholds
</span><span>        threshold </span><span>= </span><span>min</span><span>(</span><span>max</span><span>(threshold, </span><span>0.05</span><span>), </span><span>0.95</span><span>)
</span><span>
</span><span>        </span><span># don't include oracle token in kv cache
</span><span>        model_kv.truncate(</span><span>len</span><span>(tokens) </span><span>- </span><span>1</span><span>)
</span><span>        draft_kv.truncate(</span><span>len</span><span>(tokens) </span><span>- </span><span>1</span><span>)
</span></code></pre>
<p>This table compares threshold decoding to regular speculative decoding with different fixed draft lengths (along with KV caching, to make the comparison fair), for the test prompts I showed earlier.
The rightmost column is a sparkline showing how generation speed changes over time.</p>
<table>
<tbody><tr><th colspan="3">What is a tensor in machine learning?</th></tr>
<tr>
<td>threshold</td>
<td>2.01 tok/s</td>
<td>▁▂▄▅▄▅▅▆▅▄▅▅▅▅▅▆▅▅▅▅▆▅▆▆▅▆▅▆▆▆▅▆▆▆▆▇▆▆▇▇▇▆▆▆▇▇▇▇▇█</td>
</tr>
<tr>
<td>n_draft=1</td>
<td>1.40 tok/s</td>
<td>▁▃▃▅▅▆▆▇▆▆▆▆▇▆▇▇▇▇▆▇▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td>
</tr>
<tr>
<td>n_draft=8</td>
<td>1.92 tok/s</td>
<td>▁▂▃▄▅▆▇█▅▃▄▃▄▄▄▄▅▅▄▅▅▄▄▅▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆</td>
</tr>
<tr>
<td>n_draft=16</td>
<td>1.36 tok/s</td>
<td>▁▂▃▄▅▆▇█▄▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▃▄▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▅▅▅▅▅▆▆</td>
</tr>
<tr><th colspan="3">Index: A B C</th></tr>
<tr>
<td>threshold</td>
<td>1.90 tok/s</td>
<td>▁▁▁▂▂▃▃▄▄▄▅▅▆▆▇▇█▅▅▅▆▆▆▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄</td>
</tr>
<tr><td>n_draft=1</td>
<td>1.32 tok/s</td>
<td>▁▄▃▅▅▆▅▇▆▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▆▆▆▆▆▆▆</td>
</tr>
<tr><td>n_draft=8</td>
<td>1.49 tok/s</td>
<td>▁▁▂▃▃▄▅▆▆▄▄▅▅▆▆▇▇█▆▆▆▆▇▇▆▆▆▆▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄</td>
</tr>
<tr><td>n_draft=16</td>
<td>0.97 tok/s</td>
<td>▁▁▁▂▂▃▃▄▄▄▅▅▆▆▇▇█▅▅▅▅▆▆▆▄▅▅▅▅▅▆▄▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂</td>
</tr>
<tr><th colspan="3">The digits of Pi are 3.14159</th></tr>
<tr>
<td>threshold</td>
<td>0.84 tok/s</td>
<td>▁▄▇▅█▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td>
</tr>
<tr><td>n_draft=1</td>
<td>0.85 tok/s</td>
<td>▁▅▅█▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td>
</tr>
<tr><td>n_draft=8</td>
<td>0.42 tok/s</td>
<td>▁▂▄▆█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td>
</tr>
<tr><td>n_draft=16</td>
<td>0.27 tok/s</td>
<td>▁▂▄▆█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td>
</tr>
</tbody></table>
<p>Note how, for example, n_draft=16 on the "Index: A B C" prompt has a strong start, but falls off hard in the later prose section as it overgenerates incorrect tokens.
Threshold decoding, meanwhile, is able to ramp up to take advantage of the easy alphabet tokens, and then ramp back down to not overgenerate on the harder prose section:</p>
<p><span>Index: A B C</span><span> D</span><span> E</span><span> F</span><span> G</span><span> H</span><span> I</span><span> J</span><span> K</span><span> L</span><span> M</span><span> N</span><span> O</span><span> P</span><span> Q</span><span> R</span><span> S</span><span> T</span><span> U</span><span> V</span><span> W</span><span> X</span><span> Y</span><span> Z</span><span>
</span><span>
</span><span>The</span><span> following</span><span> table</span><span> lists</span><span> the</span><span> number</span><span> of</span><span> times</span><span> each</span><span> of</span><span> the</span><span> following</span><span> words</span><span> appear</span><span> in</span><span> the</span><span> text</span><span> of</span><span> the</span><span> book</span><span>.</span><span>
</span><span>
</span><span>Word</span><span> Number</span>
</p>
<h3 id="Staged_speculative_decoding"><a href="#Staged_speculative_decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Staged_speculative_decoding">
</a>Staged speculative decoding</h3>
<p><a href="https://arxiv.org/abs/2308.04623">Staged speculative decoding</a> adds two improvements to vanilla speculative decoding:</p>
<p>The first is to restructure the draft batch as a tree, instead of a single generation.
This helps because longer draft batches on complex text can quickly diverge from the base model.
It can instead make more sense to do multiple, shorter drafts, branching off from each other, and then verify them all against the oracle model using a specially-crafted attention mask.
Generating multiple draft sequences lets you reuse prior tokens and sample the draft model in batches, further accelerating the process.</p>
<p><small>(You can think of regular speculative decoding as "deep" and using a tree as "wide", which begs the question of how you should prioritize deep v.s. wide for certain text. Maybe an adjustable parameter like in the threshold decoding section would be useful here as well?)</small></p>
<p>The second improvement is to speculatively decode the draft model as well—it's usually a Transformer after all.
This could be a yet-smaller Transformer (they recommend 15-20x smaller than the oracle model), or even a simple N-gram model.</p>
<h3 id="Guided_generation"><a href="#Guided_generation">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Guided_generation">
</a>Guided generation</h3>
<p>Grammar-guided generation lets you constrain a model's output to follow some grammar, giving you output that guaranteed to match some grammar—such as JSON.
At first, this seems unrelated to speed—reliability is nice, but speed too?
That can't be possible!
But it is—let's dig into how it works to see why.</p>
<p>Imagine you're generating JSON with an LLM, and the generation so far is:</p>
<pre data-lang="json"><code data-lang="json"><span>{
</span><span>    </span><span>"key"</span><span>: 
</span></code></pre>
<p>GPT-4 could generate any of 100k+ tokens here, but only a few are actually valid: whitespace, an open bracket, a quote, a digit, <code>null</code>, etc.
During regular (non-guided) generation, you'd just hope the model had learned that properly and didn't generate syntactically invalid JSON.
During guided generation, however, the sampler <em>only</em> samples from those valid tokens, ignoring any others, even if the invalid tokens are more likely.<sup><a href="#openai-json-mode">5</a></sup></p>
<p>Even better, with libraries like <a href="https://github.com/outlines-dev/outlines">Outlines</a> or <a href="https://github.com/1rgs/jsonformer">jsonformer</a>, you can give the guided generation sampler a schema, and it will sample <em>within that schema</em>!
For example, if a key requires a digit, it will only sample digits after that key name.
Note how the returned object in this example exactly matches the Pydantic schema:</p>
<pre data-lang="python"><code data-lang="python"><span>...
</span><span>
</span><span>class </span><span>Weapon</span><span>(</span><span>str</span><span>, </span><span>Enum</span><span>):
</span><span>    sword </span><span>= </span><span>"sword"
</span><span>    axe </span><span>= </span><span>"axe"
</span><span>    mace </span><span>= </span><span>"mace"
</span><span>    spear </span><span>= </span><span>"spear"
</span><span>    bow </span><span>= </span><span>"bow"
</span><span>    crossbow </span><span>= </span><span>"crossbow"
</span><span>
</span><span>
</span><span>class </span><span>Armor</span><span>(</span><span>str</span><span>, </span><span>Enum</span><span>):
</span><span>    leather </span><span>= </span><span>"leather"
</span><span>    chainmail </span><span>= </span><span>"chainmail"
</span><span>    plate </span><span>= </span><span>"plate"
</span><span>
</span><span>class </span><span>Character</span><span>(</span><span>BaseModel</span><span>):
</span><span>    name: constr(</span><span>max_length</span><span>=</span><span>10</span><span>)
</span><span>    age: </span><span>int
</span><span>    armor: Armor
</span><span>    weapon: Weapon
</span><span>    strength: </span><span>int
</span><span>
</span><span>model </span><span>= </span><span>...
</span><span>generator </span><span>= </span><span>outlines.generate.json(model, Character, </span><span>max_tokens</span><span>=</span><span>100</span><span>)
</span><span>print</span><span>(generator(</span><span>"Give me a character description"</span><span>, </span><span>rng</span><span>=</span><span>...</span><span>))
</span><span># {
</span><span>#   "name": "clerame",
</span><span>#   "age": 7,
</span><span>#   "armor": "plate",
</span><span>#   "weapon": "mace",
</span><span>#   "strength": 4171
</span><span># }
</span></code></pre>
<p>This is great for reliability, but what does it have to do with speed?
Well, let's take a look at that response again.
If we compare it to what's required by the schema, barely any tokens are actually ambiguous—for most of the response, the model would only have one possible token to pick from.
In that case, the sampler can just pick that token, bypassing the model entirely!
Only a small fraction of the tokens (the ones highlighted in green) actually required a model forward call:</p>
<p>
{
  "name": "<span>clerame", </span>   <small>(4 ambiguous tokens: <span>cl</span> <span>er</span> <span>ame</span> <span>",\n</span>)</small>
  "age": <span>7, </span>            <small>(2 ambiguous tokens: <span>7</span> <span>,\n</span>)</small>
  "armor": "<span>plate</span>",     <small>(1 ambiguous token:  <span>plate</span>)</small>
  "weapon": "<span>m</span>ace",     <small>(1 ambiguous token:  <span>m</span>)</small>
  "strength": <span>4171 </span>     <small>(3 ambiguous tokens: <span>417</span> <span>1</span> <span>\n</span>)</small>
}
</p>
<p>Between the JSON grammar and the schema, we already know the first 7 tokens are <code>{\n "name": "</code>, so we can automatically add those into the context before the first model call.
Then after the model generates <code>clerame",\n</code>, we know the next tokens up until the model needs to actually generate the age (<code> "age": </code>), so we append those as well before calling the model again to generate <code>7,</code>.
We can keep going that way all the way down.
(We can even save a token by completing <code>m</code> as <code>"mace",</code> because no other weapon starts with <code>m</code>!)</p>
<p>The complete response was 41 tokens, but only 11 of them had to come from the model, the others were automatically inserted and only needed to be processed as prompt tokens, which are much faster.
This is a nice speedup, <em>and</em> more reliable to boot—a win-win.
If you need to generate structured data with LLMs, especially OSS LLMs that can use custom samplers, you should definitely be using a library like Outlines.</p>
<h3 id="Lookahead_decoding"><a href="#Lookahead_decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Lookahead_decoding">
</a>Lookahead decoding</h3>
<p><a href="https://lmsys.org/blog/2023-11-21-lookahead-decoding/">Lookahead decoding</a> is a new approach to speculative decoding that doesn't require a draft model.
Instead, the model itself is used in two branches: a lookahead branch, which predicts and extends candidate N-grams (short sequences of N tokens), and a verification branch, which verifies the candidates.
The lookahead branch is similar to the draft model in regular speculative decoding, and the verification branch has the same role as the oracle model.</p>
<p>But unlike regular speculative decoding, this is all done not just in a single model, but in a single model <em>call</em> using a specially-crafted attention mask:</p>
<p><a href="https://vgel.me/posts/faster-inference/lookahead.png"><img src="https://vgel.me/posts/faster-inference/lookahead.png" alt=""></a></p>
<p>I won't go too in depth because the <a href="https://lmsys.org/blog/2023-11-21-lookahead-decoding/">lmsys blog post</a> announcing lookahead decoding already has some nice animations (even if the explanation is somewhat dense).</p>
<h3 id="Prompt_lookup_decoding"><a href="#Prompt_lookup_decoding">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Prompt_lookup_decoding">
</a>Prompt lookup decoding</h3>
<p><a href="https://twitter.com/apoorv_umang/status/1728831397153104255">Prompt lookup decoding</a> is another technique, where the draft model is replaced by simple string matching over the context.
They claim it works well for tasks like code editing or RAG where the output necessarily contains lots of verbatim copying from the input.
I assume it'd also work well in staged speculative decoding, to speculatively decode a draft model.</p>
<h2 id="Training_time_optimizations"><a href="#Training_time_optimizations">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Training_time_optimizations">
</a>Training time optimizations</h2>
<p>There are a few optimizations that I'm going to speed through since they're not very relevant if you don't have the resources to pretrain a model with them from the start.</p>
<h3 id="Sparse_attention"><a href="#Sparse_attention">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Sparse_attention">
</a>Sparse attention</h3>
<p>Attention is algorithmically slow because it's quadratic: as the sequence grows in length, each of the N tokens needs to attend to each of the N tokens.
Sparse attention attempts to remedy this by calculating less attention.
For example, Mistral 7B uses sliding window attention, where tokens in some layers can only attend to nearby tokens.
<a href="https://arxiv.org/abs/2004.05150">Longformer</a> also explored some interesting sparse attention patterns, like giving all tokens access to specific positions, dilating the sliding window, using different size windows on different layers, and other tricks.
(Longformer predates Mistral, but as far as I can tell Mistral didn't use all the tricks Longformer did—I'm not sure why.)</p>
<p>Sometimes this kind of attention can be finetuned in or bolted on without tuning after the fact, but for the best performance it needs to be trained into the model from the start, like Mistral did.</p>
<h3 id="Non-Transformer_architectures"><a href="#Non-Transformer_architectures">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Non-Transformer_architectures">
</a>Non-Transformer architectures</h3>
<p>Recently there's been a renewed surge of interest in non-Transformer LLMs.
If you're new to the field you may not be familiar with RNNs/LSTMs, but they were the dominant sequence modeling architecture before Attention is All You Need was published and Transformers took off.
Unlike Transformers where the whole context is available to the model at once, RNNs do a linear scan over the sequence, building up a hidden state that models what has come before.
<small>(There are also reversed and bidirectional RNNs, it's a whole thing.)</small></p>
<p>They were outmatched by Transformers due to difficulty scaling them and problems like forgetting, but some recent papers have tried to bring them back, or invent new sub-quadratic architectures that can finally dethrone Transformers.
These include <a href="https://arxiv.org/abs/2305.13048">RWKV</a> (new type of RNN), <a href="https://arxiv.org/abs/2312.00752">Mamba</a> (state-space model), <a href="https://arxiv.org/abs/2302.10866">Hyena</a> (convolutional), and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf">Recurrent Memory Transformers</a> (use a Transformer for segments, then special memory tokens for global context).
So far the biggest and best models are still Transformers, but that might not be true in the future!</p>
<h2 id="Conclusion"><a href="#Conclusion">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Conclusion">
</a>Conclusion</h2>
<p>Thanks for reading!</p>
<p>Unfortunately I didn't get to cover everything I wanted to in this post, since it's already quite long—I didn't touch on structured sparsity, Mixture of Experts, activation quantization, static vs dynamic quantization, or lots of other great topics.
However, I think the post is a good survey of different areas of LLM optimization.
LLMs are currently slow and hard to run, but the situation is improving all the time—Lookahead Decoding was released while I was writing this post!
It seems likely that in a few years, between better hardware, better training methods, better quantization, more inference optimizations, and the continuing hard work of the open source community, we could be running models than handily beat GPT-4 on consumer hardware, and these techniques and more will be instrumental to making that happen.
(Of course, GPT-5 will probably be out by then... but always upwards!)</p>
<p>Hopefully you found the post useful and/or entertaining.
If you enjoyed it, you may also enjoy:</p>
<ul>
<li><a href="https://vgel.me/posts">My other blog posts</a>, such as <a href="https://vgel.me/posts/handmade-transformer/">I made a transformer by hand (no training!)</a>, <a href="https://vgel.me/posts/tools-not-needed/">GPT-3 will ignore tools when it disagrees with them</a>, <a href="https://vgel.me/posts/gpt4-javascript">Does GPT-4 think better in Javascript?</a> and <a href="https://vgel.me/posts/adversarial-training-data">I'm worried about adversarial training data</a></li>
<li><a href="https://vgel.me/">My other projects and writing</a></li>
<li>My <a href="https://twitter.com/voooooogel/">Twitter</a>, where I post about new blog posts, smaller AI-related thoughts (e.g. <a href="https://twitter.com/voooooogel/status/1730726744314069190">1</a>, <a href="https://twitter.com/voooooogel/status/1733590921605001677">2</a>), whatever fiction I've been reading, and other things.</li>
</ul>
<p>If you have thoughts about this post, please feel free to <a href="https://vgel.me/contact">get in touch</a>!
I love hearing from people who read my posts.</p>
<h3 id="Thanks"><a href="#Thanks">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Thanks">
</a>Thanks</h3>
<p>As always, thanks to everyone who contributed to and reviewed this post:</p>
<ul>
<li><a href="https://twitter.com/wjessup">@wjessup</a> (http://willjessup.com/) for a <em>very</em> thorough review of the draft.</li>
<li><a href="https://twitter.com/tekknolagi">@tekknolagi</a> (https://bernsteinbear.com/) for reviewing and commenting.</li>
<li><a href="https://twitter.com/MF_FOOM">@MF_FOOM</a> for reviewing and commenting (and helping me rent some GPUs for an experiment 🫡 even though the experiment didn't work rip)</li>
<li>@laikhtewari on Discord for great topic suggestions</li>
<li>Everyone on Twitter who liked and commented on the various posts I made while working on this! Really helps.</li>
</ul>
<hr>

<!-- -->

<!-- -->

<!-- -->

<!-- -->



    <ul>
      
        <li><strong>Previous entry:</strong> <a href="https://vgel.me/posts/handmade-transformer/">I made a transformer by hand (no training!)</a></li>
      
      
    </ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do I need to get out the soldering-iron again? (2018) (121 pts)]]></title>
            <link>https://www.naughtycomputer.uk/do_i_really_need_to_get_out_the_soldering_iron_again.html</link>
            <guid>38732862</guid>
            <pubDate>Fri, 22 Dec 2023 10:27:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.naughtycomputer.uk/do_i_really_need_to_get_out_the_soldering_iron_again.html">https://www.naughtycomputer.uk/do_i_really_need_to_get_out_the_soldering_iron_again.html</a>, See on <a href="https://news.ycombinator.com/item?id=38732862">Hacker News</a></p>
Couldn't get https://www.naughtycomputer.uk/do_i_really_need_to_get_out_the_soldering_iron_again.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[NewPipe 0.26 (189 pts)]]></title>
            <link>https://github.com/TeamNewPipe/NewPipe/releases/tag/v0.26.0</link>
            <guid>38732781</guid>
            <pubDate>Fri, 22 Dec 2023 10:14:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/TeamNewPipe/NewPipe/releases/tag/v0.26.0">https://github.com/TeamNewPipe/NewPipe/releases/tag/v0.26.0</a>, See on <a href="https://news.ycombinator.com/item?id=38732781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pjax="true" data-test-selector="body-content" data-view-component="true"><p>This version also includes the changes from NewPipe Extractor <a href="https://github.com/TeamNewPipe/NewPipeExtractor/releases/tag/v0.23.0">v0.23.0</a> and <a href="https://github.com/TeamNewPipe/NewPipeExtractor/releases/tag/v0.23.1">v0.23.1</a>, which <strong>fix the recurring "Could not get like count" error</strong> on YouTube streams.</p>
<h3>New</h3>
<ul>
<li>
<p>Access more content provided by <strong>channels</strong> which is grouped in different <strong>tabs</strong> (<a data-error-text="Failed to load title" data-id="1812601152" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipeExtractor/issues/1082" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipeExtractor/pull/1082/hovercard" href="https://github.com/TeamNewPipe/NewPipeExtractor/pull/1082">TeamNewPipe/NewPipeExtractor#1082</a> <a data-error-text="Failed to load title" data-id="1419970052" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/9182" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/9182/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/9182">#9182</a> <a data-error-text="Failed to load title" data-id="2034411483" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10645" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10645/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10645">#10645</a> <a data-error-text="Failed to load title" data-id="2051311795" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10670" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10670/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10670">#10670</a> <a data-error-text="Failed to load title" data-id="2051434843" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10673" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10673/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10673">#10673</a>). The content of the channel tabs can vary by service:</p>
<ul>
<li>YouTube: videos, shorts, live, playlists, about
<ul>
<li>Note that YouTube does not provide upload date and duration for shorts, so they won't show up in the feed</li>
</ul>
</li>
<li>PeerTube: videos, playlists, channels (for accounts), about</li>
<li>SoundCloud: tracks, playlists, albums, about</li>
<li>Bandcamp: albums, tracks, about</li>
<li>media.ccc.de: videos, about</li>
</ul>
</li>
<li>
<p>Allow selecting <strong>image quality</strong> among multiple images <a data-error-text="Failed to load title" data-id="1691418409" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10062" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10062/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10062">#10062</a> <a data-error-text="Failed to load title" data-id="1933206762" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10482" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10482/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10482">#10482</a><br>
NewPipe supports selecting the image quality among multiple image versions. This comes with three different presets which can be selected from within the settings (low, medium and high quality). This is most noticeable in the recently introduced card style for stream lists which is now able to show thumbnails in higher resolutions. Some services (e.g. YouTube) do not always provide the highest quality thumbnails for those lists.</p>
</li>
</ul>
<h3>Improved</h3>
<ul>
<li>Adjust empty state message for <code>ListInfoFragment</code>s depending on <code>Info</code> stream type <a data-error-text="Failed to load title" data-id="1837669205" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10304" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10304/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10304">#10304</a></li>
<li>Show loading indicator before opening the download dialog from the share menu <a data-error-text="Failed to load title" data-id="1889285253" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10407" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10407/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10407">#10407</a></li>
<li>Improved accessibility of player interfaces <a data-error-text="Failed to load title" data-id="1783815362" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10199" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10199/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10199">#10199</a></li>
<li>Include a high-resolution option in the default resolution settings <a data-error-text="Failed to load title" data-id="1647706746" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/9987" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/9987/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/9987">#9987</a></li>
<li>Show play queue button in main player when there is one stream <a data-error-text="Failed to load title" data-id="1874120333" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10396" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10396/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10396">#10396</a></li>
<li>Add option to add playlist name and video name to playlist sharing content <a data-error-text="Failed to load title" data-id="1899741263" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10427" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10427/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10427">#10427</a></li>
<li>Improve audio stream selection for video-only streams in the downloader <a data-error-text="Failed to load title" data-id="1910300762" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10446" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10446/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10446">#10446</a></li>
</ul>
<h3>Localization and Documentation</h3>
<ul>
<li>Make capitalization of "Night theme" setting consistent with others <a data-error-text="Failed to load title" data-id="1840112286" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10313" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10313/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10313">#10313</a></li>
<li>Update Weblate &amp; fix string formats <a data-error-text="Failed to load title" data-id="1865694170" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10376" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10376/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10376">#10376</a></li>
<li>Fix selection of wrong languages in language picker <a data-error-text="Failed to load title" data-id="1887533765" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10406" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10406/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10406">#10406</a></li>
<li>Make "latest release" link more obvious to bug reporters <a data-error-text="Failed to load title" data-id="1848064107" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10331" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10331/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10331">#10331</a></li>
<li>[Readme] Remove Bitcoin and Bountysource donation options <a data-error-text="Failed to load title" data-id="1942181406" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10491" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10491/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10491">#10491</a></li>
<li>[Readme] Add Matrix room link <a data-error-text="Failed to load title" data-id="2029142174" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10632" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10632/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10632">#10632</a></li>
</ul>
<h3>Fixed</h3>
<ul>
<li>Fix player audio focus not respecting mute <a data-error-text="Failed to load title" data-id="1828895982" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10275" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10275/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10275">#10275</a></li>
<li>Fix downloads of streams with missing <code>MediaFormat</code> <a data-error-text="Failed to load title" data-id="1758837550" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10165" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10165/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10165">#10165</a></li>
<li>[YouTube] Fix extraction of age-restricted music videos <a data-error-text="Failed to load title" data-id="1901722153" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipeExtractor/issues/1108" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipeExtractor/pull/1108/hovercard" href="https://github.com/TeamNewPipe/NewPipeExtractor/pull/1108">TeamNewPipe/NewPipeExtractor#1108</a> <a data-error-text="Failed to load title" data-id="1908467912" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10440" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10440/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10440">#10440</a></li>
<li>Fix restoring software license dialog <a data-error-text="Failed to load title" data-id="1905389329" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10436" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10436/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10436">#10436</a></li>
<li>Fix inconsistency between user interaction and database commit order when re-adding videos to a playlist <a data-error-text="Failed to load title" data-id="1206435598" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/8248" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/8248/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/8248">#8248</a></li>
<li>Fix playing SoundCloud songs under some conditions: now OPUS HLS streams are considered as unplayable, and thus other streams are preferred <a data-error-text="Failed to load title" data-id="1995699986" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10579" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10579/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10579">#10579</a></li>
<li>Fix app not responding, background app crashes and issues with starting the player <a data-error-text="Failed to load title" data-id="1995594629" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10578" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10578/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10578">#10578</a></li>
<li>Fix some null pointer exceptions <a data-error-text="Failed to load title" data-id="1995272772" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10576" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10576/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10576">#10576</a> <a data-error-text="Failed to load title" data-id="1995547067" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10577" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10577/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10577">#10577</a></li>
<li>Fix custom filename replacement character being interpreted as regex and crashing the app <a data-error-text="Failed to load title" data-id="1940278485" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10489" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10489/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10489">#10489</a></li>
<li>Fix notifying about old "new" streams <a data-error-text="Failed to load title" data-id="1943384759" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10494" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10494/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10494">#10494</a></li>
<li>Fix channel avatar not loading correctly sometimes <a data-error-text="Failed to load title" data-id="1997857448" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10581" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10581/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10581">#10581</a></li>
<li>Fix application lagging with many main page tabs <a data-error-text="Failed to load title" data-id="2051332864" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10671" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10671/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10671">#10671</a></li>
</ul>
<h3>Development</h3>
<ul>
<li>Simplify <code>MainActivity.tabSelected(MenuItem)</code> <a data-error-text="Failed to load title" data-id="1857851563" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10360" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10360/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10360">#10360</a></li>
<li>Image minizier: replace <code>Number.toFixed(0)</code> with <code>Math.floor()</code> <a data-error-text="Failed to load title" data-id="1865713034" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10377" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10377/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10377">#10377</a></li>
<li>Update miscellaneous libraries <a data-error-text="Failed to load title" data-id="1805818623" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10234" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10234/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10234">#10234</a> <a data-error-text="Failed to load title" data-id="1811092900" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10244" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10244/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10244">#10244</a></li>
<li>Improve the download helpers using the Java 7 NIO API. <a data-error-text="Failed to load title" data-id="1815080809" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10248" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10248/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10248">#10248</a></li>
<li>Fix memory leaks and add documentation <a data-error-text="Failed to load title" data-id="1871702250" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10394" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10394/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10394">#10394</a></li>
<li>Replace <code>MathUtils.clamp</code> with Kotlin <code>coerceIn</code> <a data-error-text="Failed to load title" data-id="1800046942" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10224" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10224/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10224">#10224</a></li>
<li>Bump AGP to 8.1.1 <a data-error-text="Failed to load title" data-id="1899967378" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10428" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10428/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10428">#10428</a></li>
<li>Improve codequality <a data-error-text="Failed to load title" data-id="1905038563" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10435" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10435/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10435">#10435</a></li>
<li>Update extractor and remove <code>DeobfuscateException</code> handling <a data-error-text="Failed to load title" data-id="1908467912" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10440" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10440/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10440">#10440</a></li>
<li>Add workflow "PR size labeler" to label PRs based on the number of changed lines <a data-error-text="Failed to load title" data-id="1761955759" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10170" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10170/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10170">#10170</a> <a data-error-text="Failed to load title" data-id="1910332881" data-permission-text="Title is private" data-url="https://github.com/TeamNewPipe/NewPipe/issues/10447" data-hovercard-type="pull_request" data-hovercard-url="/TeamNewPipe/NewPipe/pull/10447/hovercard" href="https://github.com/TeamNewPipe/NewPipe/pull/10447">#10447</a></li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Believe Your Eyes – A WhatsApp Clickjacking Vulnerability (295 pts)]]></title>
            <link>https://00xbyte.github.io/posts/Don%27t-Believe-Your-Eyes-A-WhatsApp-Clickjacking-Vulnerability/</link>
            <guid>38732770</guid>
            <pubDate>Fri, 22 Dec 2023 10:12:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://00xbyte.github.io/posts/Don%27t-Believe-Your-Eyes-A-WhatsApp-Clickjacking-Vulnerability/">https://00xbyte.github.io/posts/Don%27t-Believe-Your-Eyes-A-WhatsApp-Clickjacking-Vulnerability/</a>, See on <a href="https://news.ycombinator.com/item?id=38732770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Imagine you have received a WhatsApp message with a link to <code>ln.instagram.com</code>.<br> Where do you think the link leads? Instagram? Think again.</p><p>I have found a <a href="https://en.wikipedia.org/wiki/Clickjacking">clickjacking</a> vulnerability in WhatsApp that enables phishing attacks.<br> An attacker can send anyone a crafted message with a link that appears to lead to a legitimate website, but in fact leads to any website of the attacker’s choice.</p><h2 id="discovery-process"><span>Discovery Process</span><a href="#discovery-process"><i></i></a></h2><p>I started my research looking for a way to make a message recipient perform an HTTP request. My initial thought was to check WA’s link preview feature. I hoped the the link would be rendered twice, once by the sender and once by the receiver. In order to check my theory, I created a <a href="https://webhook.site/">webhook</a> and sent it to a friend. Sadly only one request was made, only by me.<br> <strong>That got me thinking. If the receiver does not render the link for themselves, that must mean I send both the link and the preview</strong>. If so, I wonder if I can send a link to one site with a preview of another?</p><h3 id="issue-1---link-preview-mismatch"><span>Issue #1 - Link Preview Mismatch</span><a href="#issue-1---link-preview-mismatch"><i></i></a></h3><p>I decided to take a look into what data is sent in a WA message that contains a link with a preview. My goal was to intercept the message, change the link and the preview to mismatch, and send it. <br> I decided to intercept a message via WA web, as hopefully that would be a faster setup than debugging an emulator. I then ran into a problem - WA uses and E2EE so I couldn’t simply modify the message with proxy like Burp Suite.</p><p><a href="https://00xbyte.github.io/assets/img/post/websocket.png"><img src="https://00xbyte.github.io/assets/img/post/websocket.png" data-src="/assets/img/post/websocket.png" alt="" width="540" height="400" data-proofer-ignore=""></a></p><p>Instead of understanding the encryption mechanism, I decided insert a breakpoint a moment before the message was encrypted and sent through the websocket. WA web’s javascript was uglified and minified, however after a while of searching I found the right place.</p><p><a href="https://00xbyte.github.io/assets/img/post/link%20object.png"><img src="https://00xbyte.github.io/assets/img/post/link%20object.png" data-src="/assets/img/post/link%20object.png" alt="" width="540" height="400" data-proofer-ignore=""></a></p><p>Exactly as I suspected, the link and the preview were sent separately!<br> I created a message object for<code>instagram.com</code> and changed the <code>text</code> property to <code>google.com</code>. Unfortunately, the message that was sent did not have a preview anymore. Only a link to Google.<br> Blackbox testing taught me that:</p><div><table><thead><tr><th>Property</th><th>Purpose</th></tr></thead><tbody><tr><td><code>text</code></td><td>The text of the message</td></tr><tr><td><code>canonicalURL</code></td><td>The domain that is shown at the bottom of the preview</td></tr><tr><td><code>matchedText</code></td><td>Seems to be compared against <code>canonicalURL</code>, also tested that its value apears in <code>text</code></td></tr></tbody></table></div><p>I discovered that if the <code>matchedText</code> was deleted from the object, I could create a mismatch!</p><p><a href="https://00xbyte.github.io/assets/img/post/preview%20mismatch.jpg"><img src="https://00xbyte.github.io/assets/img/post/preview%20mismatch.jpg" data-src="/assets/img/post/preview%20mismatch.jpg" alt="" width="540" height="400" data-proofer-ignore=""></a></p><p>Success! I have created a message with a preview to one site, and when clicked, leads to another. This was a great start, but I didn’t want the real link to be shown in the message. I was looking for ways to hide the message text.</p><h3 id="issue-2---disguising-links-2k2e"><span>Issue #2 - Disguising Links (2K2E)</span><a href="#issue-2---disguising-links-2k2e"><i></i></a></h3><p>I remembered that some unicode characters can change the representation of text, so I tried fuzzing characters to see their effect:</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
</pre></td><td><pre><span>function</span> <span>fuzz</span><span>(</span><span>start</span><span>,</span> <span>end</span><span>)</span> <span>{</span>
	<span>for </span><span>(</span><span>let</span> <span>i</span> <span>=</span> <span>start</span><span>;</span> <span>i</span> <span>&lt;=</span> <span>end</span> <span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
	    <span>j</span> <span>=</span> <span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>)</span>
	    <span>if </span><span>(</span><span>j</span><span>.</span><span>length</span> <span>&lt;</span> <span>4</span><span>)</span> <span>{</span> <span>j</span> <span>=</span> <span>"</span><span>0</span><span>"</span><span>.</span><span>repeat</span><span>(</span><span>4</span> <span>-</span><span>j</span><span>.</span><span>length</span><span>)</span> <span>+</span> <span>j</span><span>}</span>
 	    <span>msg</span> <span>+=</span> <span>eval</span><span>(</span><span>"</span><span>\"\\</span><span>u</span><span>"</span><span>+</span><span>j</span><span>+</span><span>"</span><span>\"</span><span>"</span><span>)</span>
			<span>+</span> <span>"</span><span>https://google.com/</span><span>"</span> <span>+</span> <span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>)</span>
			<span>+</span> <span>"</span><span>\n</span><span>"</span> 
	<span>}</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>I observed the results and one result caught my eye. One of the lines (<code>202e</code>) was in reverse:</p><p><a href="https://00xbyte.github.io/assets/img/post/fuzzing.jpg"><img src="https://00xbyte.github.io/assets/img/post/fuzzing.jpg" data-src="/assets/img/post/fuzzing.jpg" alt="" width="540" height="400" data-proofer-ignore=""></a></p><p>Apparently, the unicode character <a href="https://unicode-explorer.com/c/202E"><code>U+202E</code></a> (Right-To-Left Override) alters the way that text is presented to the user by displaying it in reverse order.<br> That was a great start but this link looks horrible. Nobody would click it.</p><h4 id="crafting-a-mirror-url"><span>Crafting A Mirror URL</span><a href="#crafting-a-mirror-url"><i></i></a></h4><p>I wanted to create a URL that, when reversed, looked like <code>https://instagram.com</code>. This means that the link should be <code>moc.margatsni//:sttph</code> (<code>\u202e</code>+<code>moc.margatsni//:sttph</code> = <code>https://instagram.com</code> )</p><p>This fist problem is the TLD. I cannot register a <code>.margatsni</code> domain.<br> My solution was to find a TLD that could look like a subdomain. For example the TLD <code>.nl</code> (Netherlands) would result in <code>ln.instagram.com</code> (<code>ln</code> as in link)</p><p>Problem number two was that the URL needed to start with <code>https://</code> in order to look legitimate.<br> My solution to this was to append the string <code>//:sptth</code> (which is a valid path) to the URL, so that <code>https://moc.margatsni.nl//:sttph</code> would appear as <code>https://ln.instagram.com//:sptth</code></p><blockquote><p>With a mirror URL and the <code>U+202E</code> character, an attacker can make any URL look like any other!<br> I call this vulnerability <strong>2K2E</strong></p></blockquote><h3 id="final-result"><span>Final Result</span><a href="#final-result"><i></i></a></h3><p>Finally I have a legitimate looking URL that seems like it leads to Instagram, when in fact it leads to my blog <a href="https://00xbyte.github.io/">Security Is Broken</a>.</p><p><a href="https://00xbyte.github.io/assets/img/post/clickjacking.png"><img src="https://00xbyte.github.io/assets/img/post/clickjacking.png" data-src="/assets/img/post/clickjacking.png" alt="" width="540" height="400" data-proofer-ignore=""></a></p><h2 id="attack-scenario"><span>Attack Scenario</span><a href="#attack-scenario"><i></i></a></h2><p>Utilizing both these issues can allow attackers to perform phishing attacks where they construct legitimate looking links that lead to malicious websites.</p><p>The full attack flow:</p><ol><li>An attacker purchases the mirror domain of the site they would like to impersonate. For example <code>moc.margatsni.nl</code> to impersonate <code>ln.instagram.com</code></li><li>The attacker creates a message with a link to original domain in order to use its preview.<div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td><pre><span>{</span><span>
 </span><span>"text"</span><span>:</span><span> </span><span>"https://instagram.com/"</span><span>,</span><span>
 </span><span>"matchedText"</span><span>:</span><span> </span><span>"https://instagram.com/"</span><span>,</span><span>
 </span><span>"canonicalUrl"</span><span>:</span><span> </span><span>"https://instagram.com/"</span><span>,</span><span>
 </span><span>"description"</span><span>:</span><span> </span><span>"Create an account..."</span><span>,</span><span>
 </span><span>"title"</span><span>:</span><span> </span><span>"Instagram"</span><span>,</span><span>
 </span><span>"jpegThumbnail"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"previewType"</span><span>:</span><span> </span><span>0</span><span>,</span><span>
 </span><span>"mediaKey"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"mediaKeyTimestamp"</span><span>:</span><span> </span><span>1693302818542</span><span>,</span><span>
 </span><span>"thumbnailDirectPath"</span><span>:</span><span> </span><span>"/v/t62.36..."</span><span>,</span><span>
 </span><span>"thumbnailSha256"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"thumbnailEncSha256"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"thumbnailHeight"</span><span>:</span><span> </span><span>1024</span><span>,</span><span>
 </span><span>"thumbnailWidth"</span><span>:</span><span> </span><span>1024</span><span>,</span><span>
 </span><span>"inviteLinkGroupTypeV2"</span><span>:</span><span> </span><span>0</span><span>
</span><span>}</span><span>
</span></pre></td></tr></tbody></table></code></p></div></li><li>The attacker then removes the <code>matchedText</code> property and changes the <code>text</code> property to the following value: <code>U+202E</code> + “URL to the mirror domain” + <code>//:sptth</code><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td><pre><span>{</span><span>
 </span><span>"text"</span><span>:</span><span> </span><span>"</span><span>\u</span><span>202ehttps://moc.margatsni.nl//:sptth"</span><span>,</span><span>
 </span><span>"canonicalUrl"</span><span>:</span><span> </span><span>"https://instagram.com/"</span><span>,</span><span>
 </span><span>"description"</span><span>:</span><span> </span><span>"Create an account..."</span><span>,</span><span>
 </span><span>"title"</span><span>:</span><span> </span><span>"Instagram"</span><span>,</span><span>
 </span><span>"jpegThumbnail"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"previewType"</span><span>:</span><span> </span><span>0</span><span>,</span><span>
 </span><span>"mediaKey"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"mediaKeyTimestamp"</span><span>:</span><span> </span><span>1693302818542</span><span>,</span><span>
 </span><span>"thumbnailDirectPath"</span><span>:</span><span> </span><span>"/v/t62.36..."</span><span>,</span><span>
 </span><span>"thumbnailSha256"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"thumbnailEncSha256"</span><span>:</span><span> </span><span>{},</span><span>
 </span><span>"thumbnailHeight"</span><span>:</span><span> </span><span>1024</span><span>,</span><span>
 </span><span>"thumbnailWidth"</span><span>:</span><span> </span><span>1024</span><span>,</span><span>
 </span><span>"inviteLinkGroupTypeV2"</span><span>:</span><span> </span><span>0</span><span>
</span><span>}</span><span>
</span></pre></td></tr></tbody></table></code></p></div></li><li>Finally the attacker sends the crafted message to their victim</li></ol><blockquote><p>Because we support many different platforms and environments, there are a significant number of ways that some platform could choose to normalize a URL differently than our server-side logic does. To address that, we have systems in place which allow us to adjust our URL normalization logic dynamically in the event of real-world spam and abuse.</p></blockquote><p>Sadly, Meta has shown no intention to resolve this security issue and from their response it seems that they will try to stop these attacks only if their systems detect them as spam. This means that WhatsApp users can only cross their fingers that they won’t fall victims to 2K2E attacks.<br> <strong>Apposed to WhatsApp, other platforms such as X, TikTok, and Pinterest, all have sanitization of the <code>U+202E</code> character.</strong></p><h2 id="mitigation"><span>Mitigation</span><a href="#mitigation"><i></i></a></h2><p>Because Meta has no intention of fixing this issue, links on WhatsApp cannot be trusted. In order to not fall victim to a 2K2E phishing attack, before clicking on a link, copy it. The clipboard preview should show the link address while sanitizing the <code>U+202E</code> character.</p><h2 id="update"><span>Update</span><a href="#update"><i></i></a></h2><p>I have found other services that do not have proper sanitization and are vulnerable to 2K2E as well.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering (133 pts)]]></title>
            <link>https://jingyechen.github.io/textdiffuser2/</link>
            <guid>38732713</guid>
            <pubDate>Fri, 22 Dec 2023 10:01:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jingyechen.github.io/textdiffuser2/">https://jingyechen.github.io/textdiffuser2/</a>, See on <a href="https://news.ycombinator.com/item?id=38732713">Hacker News</a></p>
<div id="readability-page-1" class="page">



<div>
          
          

          <p><span><sup>1</sup>HKUST,</span>
			      <span><sup>2</sup>Sun Yat-sen University,</span>
            <span><sup>3</sup>Microsoft Research</span>
          </p>

          
        </div>


 









  




<div>
        <h2>Abstract</h2>
        <p>
            The diffusion model has been proven a powerful generative model in recent years, yet remains a challenge in generating visual text. Several methods alleviated this issue by incorporating explicit text position and content as guidance on where and what text to render. However, these methods still suffer from several drawbacks, such as limited flexibility and automation, constrained capability of layout prediction, and restricted style diversity. In this paper, we present TextDiffuser-2, aiming to unleash the power of language models for text rendering. Firstly, we fine-tune a large language model for layout planning. The large language model is capable of automatically generating keywords for text rendering and also supports layout modification through chatting. Secondly, we utilize the language model within the diffusion model to encode the position and texts at the line level. Unlike previous methods that employed tight character-level guidance, this approach generates more diverse text images. We conduct extensive experiments and incorporate user studies involving human participants as well as GPT-4V, validating TextDiffuser-2's capacity to achieve a more rational text layout and generation with enhanced diversity.
          </p>


    <div>
        <br>
        <h2>Motivation - Why TextDiffuser-2 is Needed? 🤷‍♂</h2>
        <div><p>
          Although showing impressive rendering accuracy, we have noticed several drawbacks in existing text rendering methods:
          </p><ul>
            <li>(1) <b>Limited flexibility and automation</b>: GlyphControl needs users to design glyph images to provide layout guidance, while GlyphDraw and TextDiffuser rely on the manual specification of keywords. These requirements hinder the direct conversion of natural user prompts into corresponding images, thereby narrowing the flexibility and automation capabilities.</li>
            <li>(2) <b>Constrained capability of layout prediction</b>: GlyphDraw can only render images with a single text line, constraining its applicability for scenarios involving multiple text lines. For TextDiffuser, the produced text layouts are not visually appealing, which is primarily attributed to the limited capability of the Layout Transformer.</li>
            <li>(3) <b>Restricted style diversity</b>: For TextDiffuser, the utilization of character-level segmentation masks as control signals implicitly imposes constraints on the position of each character, thereby restricting the diversity of text styles and posing challenges when rendering handwritten or artistic fonts.</li>
            <li>(4) <b>No open-source code</b>: Existing methods may not provide available code, API, or demo.</li>
          </ul>
        </div>
      </div>

    <!-- Pipeline. -->
    <div>
        <br>
        <h2>TextDiffuser-2 Pipeline</h2>
          <p><img src="https://jingyechen.github.io/textdiffuser2/static/images/architecture.jpg" alt="pipeline1"></p><p>
            The architecture of TextDiffuser-2. The language model M1 and the diffusion model are trained in two stages. The language model M1 can convert the user prompt into a language-format layout and also allows users to specify keywords optionally. Further, the prompt and language-format layout is encoded with the trainable language model M2 within the diffusion model for generating images. M1 is trained via the cross-entropy loss in the first stage, while M2 and U-Net are trained using the denoising L2 loss in the second stage.
          </p>
      </div>
<!-- 
    <script
    type="module"
    src="https://gradio.s3-us-west-2.amazonaws.com/4.8.0/gradio.js"
  ></script>
  

  <gradio-app src="https://jingyechen22-textdiffuser-2.hf.space" style="width: auto;"></gradio-app> -->
  

    <!-- Pipeline. -->
    <div>
        <br>
        <h2>Text-to-Image Visualizations</h2>
        <p><img src="https://jingyechen.github.io/textdiffuser2/static/images/t2i.jpg" alt="more_results_single"></p><p>
            Visualizations of text-to-image results compared with existing methods. TextDiffuser-2 can automatically extract keywords from
            prompts for accurate rendering. Additionally, the fonts generated by TextDiffuser-2 exhibit a wide range of diversity.
          </p>
      </div>


    <div>
        <br>
        <h2>Style Diversity</h2>
        <p><img src="https://jingyechen.github.io/textdiffuser2/static/images/diverse.jpg" alt="more_results_single"></p><p>
            Visualization of diversity in generating multiple images under the same prompt. TextDiffuser-2 is capable of generating more
artistic fonts, with increased diversity in the positioning of characters and the inclination angle of text lines.
          </p>
      </div>


    <div>
        <br>
        <h2>Inpainting Ability</h2>
        <p><img src="https://jingyechen.github.io/textdiffuser2/static/images/inpaint.jpg" alt="more_results_single"></p><p>
            Visualizations of the text inpainting task compared with TextDiffuser. TextDiffuser-2 can generate more coherent text.
          </p>
      </div>


    <div>
        <br>
        <h2>Quantitative Result</h2>
        <p><img src="https://jingyechen.github.io/textdiffuser2/static/images/quanti.jpg" alt="more_results_single"></p><p>
            Demonstration of the quantitative results and user studies. We also incorporate GPT-4V into the user studies. The best and
second-best results are indicated in bold and underlined formats. TextDiffuser-2 achieves the best results under the majority of metrics.
          </p>
      </div>


</div>



<div id="BibTeX">
    <h2>Contact</h2><p>
    For help or issues using TextDiffuser-2, please email Jingye Chen <a rel="license" href="mailto:qwerty.chen@connect.ust.hk">(qwerty.chen@connect.ust.hk)</a>
    , Yupan Huang <a rel="license" href="mailto:huangyp28@mail2.sysu.edu.cn">(huangyp28@mail2.sysu.edu.cn)</a> or submit a GitHub issue. For other communications related to TextDiffuser-2, please contact Lei Cui <a rel="license" href="mailto:lecu@microsoft.com">(lecu@microsoft.com)</a> or Furu Wei <a rel="license" href="mailto:fuwei@microsoft.com">(fuwei@microsoft.com)</a>.
  </p></div>

<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@article{chen2023textdiffuser,
      title={TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering},
      author={Chen, Jingye and Huang, Yupan and Lv, Tengchao and Cui, Lei and Chen, Qifeng and Wei, Furu},
      journal={arXiv preprint arXiv:2311.16465},
      year={2023}
    }
    
</code></pre>
  </div>








</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mindustry: Open-source automation tower defense game (461 pts)]]></title>
            <link>https://mindustrygame.github.io/</link>
            <guid>38732542</guid>
            <pubDate>Fri, 22 Dec 2023 09:32:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mindustrygame.github.io/">https://mindustrygame.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38732542">Hacker News</a></p>
<div id="readability-page-1" class="page"><p> <h2> <b>Mindustry:</b> A sandbox tower-defense game. </h2> </p><div> <ul> <p> Defend your base from <span>waves of powerful enemies</span>. <br> <img alt="A high-level Mindustry base in combat with a wave of many low-level enemies." src="https://mindustrygame.github.io/1.d25af17a.webp" type="image/webp"> </p> <p> Build complex designs for <span>processing materials</span>. <br> <img alt="A peaceful Mindustry game with various end-game factories." src="https://mindustrygame.github.io/2.0d4f9a35.webp" type="image/webp"> </p> <p> Build and fight with other players on <span>multiplayer servers</span>. <br> <img alt="A peaceful Mindustry game with mid-tier factories while players wander and build." src="https://mindustrygame.github.io/3.88567664.webp" type="image/webp"> </p>   </ul> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maze Generator (140 pts)]]></title>
            <link>https://mazegenerator.net/</link>
            <guid>38732401</guid>
            <pubDate>Fri, 22 Dec 2023 09:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mazegenerator.net/">https://mazegenerator.net/</a>, See on <a href="https://news.ycombinator.com/item?id=38732401">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p><label for="ShapeDropDownList" id="ShapeLabel">Shape:</label>
                    
                </p>
                <table>
                    
                            <tbody><tr>
                                <td>
                                    <label for="S1TesselationDropDownList" id="S1StyleLabel">Style:</label>
                                </td>
                                <td>
                                    
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <label for="S1WidthTextBox" id="S1WidthLabel">Width:</label>
                                </td>
                                <td>
                                    
                                    (2 to 200 cells)
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <label for="S1HeightTextBox" id="S1HeightLabel">Height:</label>
                                </td>
                                <td>
                                    
                                    (2 to 200 cells)
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <label for="S1InnerWidthTextBox" id="S1InnerWidthLabel">Inner width:</label>
                                </td>
                                <td>
                                    
                                    (0 or 2 to width - 2 cells)
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <label for="S1InnerHeightTextBox" id="S1InnerHeightLabel">Inner height:</label>
                                </td>
                                <td>
                                    
                                    (0 or 2 to height - 2 cells)
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <label for="S1StartsAtDropDownList" id="S1StartsAtLabel">Starts at:</label>
                                </td>
                                <td>
                                    
                                </td>
                            </tr>
                        
                    <tr>
                        <td>
                            Advanced:
                        </td>
                        <td>
                            <label for="AlgorithmParameter1TextBox" id="AlgorithmParameter1Label">E:</label>
                            
                            (0 to 100), 
                            <label for="AlgorithmParameter2TextBox" id="AlgorithmParameter2Label">R:</label>
                            
                            (0 to 100)
                        </td>
                    </tr>
                </tbody></table>
                
                
                
                
            </div><div id="InfoPanel">
	
                <p>
                It saddens me to have to point this out: Contrary to what a number of less scrupulous "passive income" YouTubers lead you to believe, the mazes from this site are not free to use for commercial purposes. If you are planning to use them in something you will sell, you need to get a commercial license.  If you do not have such a license, you are committing a copyright infringement. For more information, see the link "Commercial use" above.
            
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rough.js: Create graphics with a hand-drawn, sketchy, appearance (316 pts)]]></title>
            <link>https://roughjs.com/</link>
            <guid>38732049</guid>
            <pubDate>Fri, 22 Dec 2023 08:06:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roughjs.com/">https://roughjs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38732049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h2>Rough.js</h2>
<p><strong>Rough.js</strong> is a small (&lt;9kB gzipped) graphics library that lets you draw in a <em>sketchy</em>, <em>hand-drawn-like</em>, style.
The library defines primitives to draw lines, curves, arcs, polygons, circles, and ellipses. It also supports drawing <a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/Paths">SVG paths</a>.</p>
<p>Rough.js works with both <a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API">Canvas</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/SVG">SVG</a>.</p>
<p><img src="https://roughjs.com/images/cap.png" alt="Rough.js sample"></p>
<h2>Install</h2>
<p>Install from npm:</p>
<pre><code><span>npm</span> <span>install</span> <span>--save</span> roughjs</code></pre>
<p>And use it in your code:</p>
<pre><code><span>import</span> rough <span>from</span> <span>'roughjs'</span><span>;</span></code></pre>
<h2>Usage</h2>
<p>View <a href="https://github.com/rough-stuff/rough/wiki">Full Rough.js API</a> is available on Github.</p>
<p><img src="https://roughjs.com/images/m1.png" alt="Rough.js rectangle"></p>
<pre><code><span>const</span> rc <span>=</span> rough<span>.</span><span>canvas</span><span>(</span>document<span>.</span><span>getElementById</span><span>(</span><span>'canvas'</span><span>)</span><span>)</span><span>;</span><br>rc<span>.</span><span>rectangle</span><span>(</span><span>10</span><span>,</span> <span>10</span><span>,</span> <span>200</span><span>,</span> <span>200</span><span>)</span><span>;</span> <span>// x, y, width, height</span></code></pre>
<p>or SVG</p>
<pre><code><span>const</span> rc <span>=</span> rough<span>.</span><span>svg</span><span>(</span>svg<span>)</span><span>;</span><br><span>let</span> node <span>=</span> rc<span>.</span><span>rectangle</span><span>(</span><span>10</span><span>,</span> <span>10</span><span>,</span> <span>200</span><span>,</span> <span>200</span><span>)</span><span>;</span> <span>// x, y, width, height</span><br>svg<span>.</span><span>appendChild</span><span>(</span>node<span>)</span><span>;</span></code></pre>
<h2>Lines and Ellipses</h2>
<p><img src="https://roughjs.com/images/m2.png" alt="Rough.js rectangle"></p>
<pre><code>rc<span>.</span><span>circle</span><span>(</span><span>80</span><span>,</span> <span>120</span><span>,</span> <span>50</span><span>)</span><span>;</span> <span>// centerX, centerY, diameter</span><br>rc<span>.</span><span>ellipse</span><span>(</span><span>300</span><span>,</span> <span>100</span><span>,</span> <span>150</span><span>,</span> <span>80</span><span>)</span><span>;</span> <span>// centerX, centerY, width, height</span><br>rc<span>.</span><span>line</span><span>(</span><span>80</span><span>,</span> <span>120</span><span>,</span> <span>300</span><span>,</span> <span>100</span><span>)</span><span>;</span> <span>// x1, y1, x2, y2</span></code></pre>
<h2>Filling</h2>
<p><img src="https://roughjs.com/images/m3.png" alt="Rough.js rectangle"></p>
<pre><code>rc<span>.</span><span>circle</span><span>(</span><span>50</span><span>,</span> <span>50</span><span>,</span> <span>80</span><span>,</span> <span>{</span> <span>fill</span><span>:</span> <span>'red'</span> <span>}</span><span>)</span><span>;</span> <span>// fill with red hachure</span><br>rc<span>.</span><span>rectangle</span><span>(</span><span>120</span><span>,</span> <span>15</span><span>,</span> <span>80</span><span>,</span> <span>80</span><span>,</span> <span>{</span> <span>fill</span><span>:</span> <span>'red'</span> <span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>circle</span><span>(</span><span>50</span><span>,</span> <span>150</span><span>,</span> <span>80</span><span>,</span> <span>{</span><br>  <span>fill</span><span>:</span> <span>"rgb(10,150,10)"</span><span>,</span><br>  <span>fillWeight</span><span>:</span> <span>3</span> <span>// thicker lines for hachure</span><br><span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>rectangle</span><span>(</span><span>220</span><span>,</span> <span>15</span><span>,</span> <span>80</span><span>,</span> <span>80</span><span>,</span> <span>{</span><br>  <span>fill</span><span>:</span> <span>'red'</span><span>,</span><br>  <span>hachureAngle</span><span>:</span> <span>60</span><span>,</span> <span>// angle of hachure,</span><br>  <span>hachureGap</span><span>:</span> <span>8</span><br><span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>rectangle</span><span>(</span><span>120</span><span>,</span> <span>105</span><span>,</span> <span>80</span><span>,</span> <span>80</span><span>,</span> <span>{</span><br>  <span>fill</span><span>:</span> <span>'rgba(255,0,200,0.2)'</span><span>,</span><br>  <span>fillStyle</span><span>:</span> <span>'solid'</span> <span>// solid fill</span><br><span>}</span><span>)</span><span>;</span></code></pre>
<p>Fill styles can be: <strong>hachure</strong>(default), <strong>solid</strong>, <strong>zigzag</strong>, <strong>cross-hatch</strong>, <strong>dots</strong>, <strong>sunburst</strong>, <strong>dashed</strong>, or <strong>zigzag-line</strong></p>
<p><img src="https://roughjs.com/images/m14.png" alt="Rough.js fill examples"></p>
<h2>Sketching style</h2>
<p><img src="https://roughjs.com/images/m4.png" alt="Rough.js rectangle"></p>
<pre><code>rc<span>.</span><span>rectangle</span><span>(</span><span>15</span><span>,</span> <span>15</span><span>,</span> <span>80</span><span>,</span> <span>80</span><span>,</span> <span>{</span> <span>roughness</span><span>:</span> <span>0.5</span><span>,</span> <span>fill</span><span>:</span> <span>'red'</span> <span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>rectangle</span><span>(</span><span>120</span><span>,</span> <span>15</span><span>,</span> <span>80</span><span>,</span> <span>80</span><span>,</span> <span>{</span> <span>roughness</span><span>:</span> <span>2.8</span><span>,</span> <span>fill</span><span>:</span> <span>'blue'</span> <span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>rectangle</span><span>(</span><span>220</span><span>,</span> <span>15</span><span>,</span> <span>80</span><span>,</span> <span>80</span><span>,</span> <span>{</span> <span>bowing</span><span>:</span> <span>6</span><span>,</span> <span>stroke</span><span>:</span> <span>'green'</span><span>,</span> <span>strokeWidth</span><span>:</span> <span>3</span> <span>}</span><span>)</span><span>;</span></code></pre>
<h2>SVG Paths</h2>
<p><img src="https://roughjs.com/images/m5.png" alt="Rough.js rectangle"></p>
<pre><code>rc<span>.</span><span>path</span><span>(</span><span>'M80 80 A 45 45, 0, 0, 0, 125 125 L 125 80 Z'</span><span>,</span> <span>{</span> <span>fill</span><span>:</span> <span>'green'</span> <span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>path</span><span>(</span><span>'M230 80 A 45 45, 0, 1, 0, 275 125 L 275 80 Z'</span><span>,</span> <span>{</span> <span>fill</span><span>:</span> <span>'purple'</span> <span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>path</span><span>(</span><span>'M80 230 A 45 45, 0, 0, 1, 125 275 L 125 230 Z'</span><span>,</span> <span>{</span> <span>fill</span><span>:</span> <span>'red'</span> <span>}</span><span>)</span><span>;</span><br>rc<span>.</span><span>path</span><span>(</span><span>'M230 230 A 45 45, 0, 1, 1, 275 275 L 275 230 Z'</span><span>,</span> <span>{</span> <span>fill</span><span>:</span> <span>'blue'</span> <span>}</span><span>)</span><span>;</span></code></pre>
<p>SVG Path with simplification:</p>
<p><img src="https://roughjs.com/images/m9.png" alt="Rough.js rectangle"> <img src="https://roughjs.com/images/m10.png" alt="Rough.js rectangle"></p>
<h2>Examples</h2>
<p><img src="https://roughjs.com/images/m6.png" alt="Rough.js map"></p>
<p><a href="https://github.com/rough-stuff/rough/wiki/Examples">View examples here</a></p>
<h2>API &amp; Documentation</h2>
<p><a href="https://github.com/rough-stuff/rough/wiki">Full Rough.js API</a></p>
<h2>Credits</h2>
<p>Core algorithms for drawing lines and ellipse outlines were adapted from <a href="https://www.gicentre.net/software/#/handy/">handy</a> processing lib.</p>
<p>Algorithm to convert SVG arcs to Canvas <a href="https://www.w3.org/TR/SVG/implnote.html">described here</a> was adapted from <a href="https://hg.mozilla.org/mozilla-central/file/17156fbebbc8/content/svg/content/src/nsSVGPathDataParser.cpp#l887">Mozilla codebase</a></p>
<h2>Support this project</h2>
<p>Support development of this project by sponsoring on <a href="https://github.com/sponsors/pshihn">Github Sponsors</a>.<br>
Alternatively, you can also sponsor or <a href="https://opencollective.com/rough">Open Collective</a>.</p>
<p>This project is supported by:</p>

<h2>License</h2>
<p><a href="https://github.com/rough-stuff/rough/blob/master/LICENSE">MIT License</a> (c) <a href="https://twitter.com/preetster">Preet Shihn</a></p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What We Need Instead of "Web Components" (126 pts)]]></title>
            <link>https://blog.carlana.net/post/2023/web-component-alternative-futures/</link>
            <guid>38731355</guid>
            <pubDate>Fri, 22 Dec 2023 05:50:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.carlana.net/post/2023/web-component-alternative-futures/">https://blog.carlana.net/post/2023/web-component-alternative-futures/</a>, See on <a href="https://news.ycombinator.com/item?id=38731355">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>It seems like Web Components are always just on the cusp of finally catching on. They’re like <a href="https://www.howtogeek.com/676963/why-desktop-linux-still-matters/">the year of Linux on the desktop</a> for frontend nerds. I keep reading the latest articles about Web Components as they bubble up on my social media feeds, just hoping that there is something that I missed out on and now they have more substance, but I always end up feeling disappointed. I wrote up my thoughts on Web Components <a href="https://blog.carlana.net/post/2020/web-components/">back in 2020</a>, and it doesn’t feel like the conversation has progressed in all that time. It’s like an <a href="https://en.wikipedia.org/wiki/Eternal_September">Eternal September</a> with people constantly going back to the original promise of Web Components, in spite of the reality having long since shown itself to have fallen short.</p><h2 id="what-went-wrong">What went wrong</h2><p>To TL;DR my earlier piece:</p><blockquote><p>The pitch is “get <strong>semantic elements</strong> from <strong>across the web</strong><i>!</i>” But those are <strong>wrong problems</strong> to try to solve.</p></blockquote><ul><li>Custom elements <strong>aren’t “semantic”</strong> because search engines don’t know what they mean.</li><li>“From across the web” is always going to be <strong>worse for end user performance</strong> than using one coherent, progressive enhancement compatible JavaScript framework per site.</li><li><code>customElements.define</code> is an extremely <strong>clunky API</strong>.</li><li><code>&lt;template&gt;</code> and <code>&lt;slot&gt;</code> are fine, but not a sufficient substitute for a <strong>real templating language</strong>.</li><li>Shadow DOM is also very clunky and solves an <strong>extremely niche</strong> problem.</li></ul><p>Okay, those are the basics, can we all just agree on that?</p><p>Yes, there are circumstances where using the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components/Using_custom_elements">customElement API</a> is a convenient way to namespace things and ensure that lazy loaded elements have their constructors called, but they are mostly useful for things like rendering third party embeds in a rich text content well. That doesn’t add up to the name “Web Components” referring to a meaningful thing.</p><p>If you put the name “Web Components” on your collection of JavaScript widgets powered by a lightweight framework, more people will check it out than if you had called them something else. In the end though, it’s just a marketing name for a couple of clunky DOM APIs and a dream that we all wish really existed. The dream is that you could just mix and match widgets regardless of what framework each widget is written in, but the truth is the only way to do that is by paying the full price of including a framework for each web component you include on the page. It’s never going to be a practical choice for sites where end user performance matters.</p><h2 id="browser-makers-to-the-rescue">Browser makers to the rescue</h2><p>The good news is that even though “Web Components” aren’t really A Thing, the browser makers do eventually solve real problems the right way, but sometimes it takes them a while.</p><p>As I <a href="https://blog.carlana.net/post/2020/web-components/">wrote about Shadow DOM</a> at the time:</p><blockquote><p>The fundamental thing that Shadow DOM does is to allow an element of the page to have its own CSS reset. There’s no reason we couldn’t have that as part of CSS itself instead (perhaps by changing the rules for <code>@import</code> to allow it to be nested instead of only at the top level).</p></blockquote><p>As it turns out, that is exactly what happened, so there won’t really be much reason to use Shadow DOM anymore once browser support for <a href="https://css.oddbird.net/scope/explainer/">donut scoping</a> becomes sufficiently widespread. Use CSS <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@scope">scope selectors</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@layer">import layers</a> to create a CSS reset scoped just to your component, and you get all the benefits of Shadow DOM with none of the goth language about “light DOM styles piercing the shadow root” and whatnot.</p><p>So, what should browser makers do instead of pouring more effort into Web Components per se?</p><p>I think everyone agrees that the best example of <a href="https://en.wikipedia.org/wiki/Desire_path">paving the desire lines</a> in the history of the DOM is the relationship between jQuery and querySelectorAll. In 2003, Simon Willison put together <a href="https://simonwillison.net/2003/Mar/25/getElementsBySelector/">a demo version of <code>document.getElementsBySelector</code></a>. <a href="https://jquery.com/">JQuery</a> took the idea and made it into the once ubiquitous <code>$("")</code> API. The browser makers took that and turned it into <code>document.querySelectorAll</code> which made it faster and universally available without linking to jQuery.</p><p>This is the ideal scenario for the evolution of the web: developers worked collectively over a series of years to <strong>find a solution to a recurring problem</strong> and then browser makers <strong>took that solution and made it native</strong> so that it would be universally available and more performant. By contrast, the <code>customElement</code> and Shadow DOM APIs come from <a href="https://en.wikipedia.org/wiki/Web_Components#History">about a decade ago</a> and predate many of the modern techniques for JavaScript frameworks. As a result, they ended up solving the wrong problems, just due to the inexperience that we as an industry had at that time with building large JavaScript application frameworks.</p><p>Instead of trying to improve Web Components as they exist now, what the browser makers should focus on is to find new areas where we can standardize the things developers are already doing. I have three suggestions for what they should look at.</p><h2 id="reactivity">Reactivity</h2><p>Recently, Nolan Lawson wrote <a href="https://nolanlawson.com/2023/12/02/lets-learn-how-modern-javascript-frameworks-work-by-building-one/">Let’s learn how modern JavaScript frameworks work by building&nbsp;one</a>. In the article, Lawson defines “modern” frameworks as having three components:</p><blockquote><ol><li>Using reactivity (e.g. <a href="https://dev.to/this-is-learning/the-evolution-of-signals-in-javascript-8ob">signals</a>) for DOM updates.</li><li>Using cloned templates for DOM rendering.</li><li>Using modern web APIs like <code>&lt;template&gt;</code> and <code>Proxy</code>, which make all of the above easier.</li></ol></blockquote><p>To be honest, I started drafting this blog post before Lawson’s came out, so I was happy to see that he highlighted some of the same aspects of modern frameworks that I was planning to write about here. The <code>&lt;template&gt;</code> and <code>Proxy</code> APIs are fine, and don’t especially need to be improved by browser makers. But there is a lot of room for improving reactivity/signals and DOM rendering. Let’s talk about reactivity first.</p><p>The basic idea of reactivity is that if you have a simple component like a todo item, you want to know when <code>todo.done</code> goes from <code>false</code> to <code>true</code> so that you can trigger changes to other data like <code>todos.count</code> which will go from N to N+1. In <a href="https://blog.carlana.net/post/2020/web-components/">my 2020 post</a>, I called this “the data lifecycle.”</p><p>In Lawson’s article, he creates a <a href="https://codepen.io/nolanlawson-the-selector/pen/qBgKywJ">quick and dirty reactivity system</a> with only about 50 lines of JavaScript, but more realistic reactivity systems like <a href="https://github.com/salesforce/observable-membrane">observable-membrane</a>, <a href="https://www.npmjs.com/package/@vue/reactivity">@vue/reactivity</a>, and <a href="https://github.com/preactjs/signals">@preact/signals</a> are significantly larger and more complex.</p><p>Shifting the core of these systems out of JavaScript libraries and into the browser will allow for significant optimization of the tree of dependencies between reactive data elements. It takes a lot of code to ship a performance optimized algorithm for marking nodes as dirty. For a simple todo app with only a handful of reactive elements, the complexity of the reactivity engine easily swamps the complexity of the application itself. But if the browser were tackling this problem directly, it would be possible to have this code written in C++ or Rust and made maximally efficient because it wouldn’t need to be packaged up, sent to clients, and interpreted from scratch on every page load. The cost could be paid once by a browser team and the benefits enjoyed by web developers and end users everywhere.</p><p>This is clearly a problem that has been solved independently by multiple frameworks. It’s time for the browser to tackle it too.</p><p><em>Update: <a href="https://daverupert.com/">Dave Rupert</a> has <a href="https://mastodon.social/@davatron5000/111620826173161008">pointed out on Mastodon</a> that there is <a href="https://github.com/webcomponents-cg/community-protocols/issues/43">a W3C proposal to add reactive signals</a> to JavaScript that is in the process of being forwarded to TC39. I wish them luck in the proposal process.</em></p><h2 id="morph-dom--virtual-dom">Morph DOM / Virtual DOM</h2><p>Another aspect of modern frameworks that is begging for optimization is DOM rendering. In Lawson’s article, he spends a significant amount of time explaining how to translate proxy calls into efficient DOM updates. For my part, I don’t think it would make sense for the browser makers to enter into the templating wars. The existing <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals">template literal syntax</a> in JavaScript is sufficient to allow developers to create templating systems that make sense to them, or they can keep using solutions like JSX to make templating a compile time feature of their framework. What is needed is an efficient way of merging the HTML results of the template system with the existing browser DOM of a page. If you just have some simple HTML to replace, then writing <code>node.innerHTML = newHTML;</code> is reasonably performant. The problems come if your nodes have event handlers (which can be partially mitigated by delegating to handlers higher in the DOM) or state which need to be preserved across renderings. For example, if you have an input element, using <code>node.innerHTML</code> on one of its parent elements will wipe out the state of the input and lose any text entered into the box. All systems for templating an interactive page need a way to prevent these kinds of problems from happening on each render and only overwriting state that is supposed to be overwritten.</p><p>React famously works around this problem by using a <a href="https://legacy.reactjs.org/docs/faq-internals.html">Virtual DOM</a> that is then reconciled with the browser DOM, but even frameworks that are server focused like <a href="https://htmx.org/">HTMX</a> need <a href="https://github.com/bigskysoftware/htmx/blob/master/src/htmx.js#L1023">a solution for swapping out DOM nodes</a> without wiping out the current element state. Another solution in this vein is <a href="https://www.npmjs.com/package/morphdom">MorphDOM</a> which is eight years old. More recently, <a href="https://alpinejs.dev/">Caleb Porzio</a> has been working on <a href="https://alpinejs.dev/plugins/morph">@alpinejs/morph</a> as a solution to the problem, and he occasionally <a href="https://notesonwork.transistor.fm/">podcasts</a> about the issues he runs into with difficult edge cases. Svelte works around the problem of reconciling VDOMs by pushing as much of the reconciliation process into its compiler as possible, but even for Svelte, there are limitations on what’s possible to do in advance and some things need to be <a href="https://github.com/sveltejs/svelte/blob/b8f3c49e5ff7378df2196d0c9ae24a58e282bb0a/packages/svelte/src/internal/client/render.js#L1519">done on the client</a>. In any event, while compiler side solutions are good, it is also good to have solutions that can work even without a build step.</p><p>However you tackle it, creating a diff between two DOM trees can be an extremely complex process due to the difficulties inherent in figuring out if a node has been entirely replaced or just moved to another place in the tree. There are lots of corner cases to consider and tradeoffs to think about. This is another area really ripe for consolidation by browser makers. If the browser had an API that was capable of reconciling two DOM trees, it could be faster and more efficient. Having this as a browser API would let frameworks stop trying to differentiate themselves on <a href="https://krausest.github.io/js-framework-benchmark/">rendering performance</a> and instead focus on differentiating based on the convenience of their APIs and the depth of their ecosystems. It’s another well tread field that is ready to be paved.</p><h2 id="self-sizing-iframes">Self-sizing iframes</h2><p>Let’s talk about another common developer need not mentioned in Lawson’s article and a technology that I think has fallen off the radars of a lot of developers because it is seen as being a legacy technique.</p><p>The humble <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe">iframe</a> may date back to Internet Explorer 4, but it is actually more powerful than the Shadow DOM because they have stronger style isolation and some <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe#allow">real security guarantees</a>. As a result, while iframes have basically no role in modern JavaScript frameworks, they are still extremely common in <a href="https://blog.carlana.net/post/2020/article-bits/">my field of online news publishing</a>. If you have a content well and you want to be able to put some kind of widget in the middle of it and have it work across <a href="https://kinsta.com/knowledgebase/content-management-system/">CMSes</a> and future redesigns, you use an iframe. The examples of this include everything from charts by <a href="https://www.datawrapper.de/">DataWrapper</a> and <a href="https://flourish.studio/">Flourish</a> to videos from <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">YouTube</a> and <a href="https://vimeo.com/">Vimeo</a>.</p><p>The big problem with iframes is that there isn’t a way in CSS to tell an iframe to take on the height of its content. There are good reasons for this. For example, if a host page could learn whether you’re logged into your banking site by just adding an iframe and seeing if it’s the height of the logged in page or the logged out page, that would be bad. As a result, the best way to make a responsively sized iframe is to use JavaScript to pass a message from the contained page to the host page that tells the host the size of the contained page so that the host can then resize the iframe appropriately. There are various libraries that people use to do this, like NPR’s <a href="https://blog.apps.npr.org/pym.js/">Pym.js</a> and <a href="https://apps.npr.org/sidechain/">SideChain</a> or <a href="https://www.npmjs.com/package/@newswire/frames">@newswire/frames</a>.</p><p>In spite of the security concerns, I think this could be worked around with a native solution in which the framing and framed pages use an <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Frame-Options">X-Frame-Options header</a> or a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy">CSP</a> to tell the browser which sites want to cooperate on allowing a dynamic frame height. Moving this out of JavaScript and into the browser would fix a lot of little glitches that come from polling for window size changes or contained pages with a <code>vh</code> CSS style, and make life easier for anyone who has to create or use an embed. It would also make it possible to use responsive iframes even in contexts where JavaScript isn’t allowed or needs to be heavily restricted.</p><h2 id="wrapping-it-up">Wrapping it up</h2><p>I hope you all understand what I’m arguing for. The dream of Web Components is a good one, but we need new solutions to match the problems encountered by contemporary web developers, not just a continued retreading of a path laid out a decade ago.</p><p>I asked <a href="https://claude.ai/">Claude</a> to write a stunning conclusion to this blog post, and here’s what it came up with:</p><blockquote><p>The promise of “Web Components” capturing imaginations across the web has fallen short. Instead of trying to retrofit new APIs onto old problems, we should look to the future - following the desire lines that developers are already tracing out.</p><p>Reactivity, virtual DOM reconciliation, and self-sizing iframes are all areas where developers have put in hard work over years to find solutions. Now it’s time for browser makers to pave those roads - taking the best of what developers have created and making it faster, more efficient, and universally available.</p><p>This is how real progress happens on the web - not starting from lofty top-down ideas, but from real solutions emerging organically to meet actual needs. If browser makers can shift to enabling what developers are already doing, we’ll wind up with better tools for everyone. The solutions won’t always be sexy, but they’ll be grounded in practicality.</p><p>So let’s learn from the past, meet the present where it lives, and build the web we need - not the one we imagine. The pieces are already coming together if we have the vision to see them.</p></blockquote><p>On that note, let’s keep blazing new trails for the future of the web, before the coming AI-powered coding robots inevitably <a href="https://www.gocomics.com/nancy/2022/11/05">put us all out of work</a>.</p><p><em>P.S. This is the first blog post I’ve published under the name Carlana. <a href="https://tech.lgbt/@carlana/111523437780760674">See here for details</a>.</em></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A structured note-taking app for personal use (197 pts)]]></title>
            <link>https://github.com/penxio/penx</link>
            <guid>38730979</guid>
            <pubDate>Fri, 22 Dec 2023 04:36:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/penxio/penx">https://github.com/penxio/penx</a>, See on <a href="https://news.ycombinator.com/item?id=38730979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" dir="auto">About PenX</h2>
<p dir="auto">PenX is a structured note-taking app designed for personal use. In PenX, Privacy is first important thing. our mission is building a elegant tool to manage personal digital assets, like notes, tasks, ideas, password, documents.</p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li><strong>Local-First</strong> - You own your data, in spite of the cloud</li>
<li><strong>Privacy-First</strong> - Use End-To-End Encryption to sync data</li>
<li><strong>Open Source</strong> - Trust our code, not our words</li>
<li><strong>Version control</strong> - GitHub-Based Version control Out-of-box</li>
</ul>
<h2 tabindex="-1" dir="auto">About the maker</h2>
<p dir="auto">I'm 0xZion, a freelancer, a full-stack developer, love open-source, now focusing on this application PenX. There's no massive team, no investors, just me currently. If someone is interested in PenX development, can send me a email: <a href="mailto:0xzion.penx@gmail.com">0xzion.penx@gmail.com</a>
.</p>
<h2 tabindex="-1" dir="auto">Primary tech stack</h2>
<ul dir="auto">
<li>Next.js</li>
<li>TypeScript</li>
<li>tRPC</li>
<li>Prisma</li>
<li>NextAuth.js</li>
<li>Slate.js</li>
<li>IndexedDB</li>
</ul>
<h2 tabindex="-1" dir="auto">Development</h2>
<p dir="auto">After clone the repo, in the root dir:</p>
<div dir="auto" data-snippet-clipboard-copy-content="yarn install # Install the dependencies

yarn dev # start web service"><pre>yarn install <span><span>#</span> Install the dependencies</span>

yarn dev <span><span>#</span> start web service</span></pre></div>
<h2 tabindex="-1" dir="auto">⚖️ License</h2>
<p dir="auto">AGPL 3.0</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Emu2 – A Gemini-like open-source 37B Multimodal Model (142 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38730143</link>
            <guid>38730143</guid>
            <pubDate>Fri, 22 Dec 2023 02:20:05 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38730143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hello HN,
I'm excited to introduce Emu2, the latest generative multimodal model developed by the Beijing Academy of Artificial Intelligence (BAAI). Emu2 is an open-source initiative that reflects BAAI's commitment to fostering open, secure, and responsible AI research. It's designed to enhance AI's proficiency in handling tasks across various modalities with minimal examples and straightforward instructions.</p><p>Emu2 has demonstrated superior performance over other large-scale models like Flamingo-80B in few-shot multimodal understanding tasks. It serves as a versatile base model for developers, providing a flexible platform for crafting specialized multimodal applications.</p><p>Key features of Emu2 include:</p><p>- A more streamlined modeling framework than its predecessor, Emu.</p><p>- A decoder capable of reconstructing images from the encoder's semantic space.</p><p>- An expansion to 37 billion parameters, boosting both capabilities and generalization.</p><p>BAAI has also released fine-tuned versions, Emu2-Chat for visual understanding and Emu2-Gen for visual generation, which stand as some of the most powerful open-source models available today.</p><p>Here are the resources for those interested in exploring or contributing to Emu2:</p><p>- Project: https://baaivision.github.io/emu2/</p><p>- Model: https://huggingface.co/BAAI/Emu2</p><p>- Code: https://github.com/baaivision/Emu/tree/main/Emu2</p><p>- Demo: https://huggingface.co/spaces/BAAI/Emu2</p><p>- Paper: https://arxiv.org/abs/2312.13286</p><p>We're eager to see how the HN community engages with Emu2 and we welcome your feedback to help us improve. Let's collaborate to push the boundaries of multimodal AI!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Labs.Google (126 pts)]]></title>
            <link>https://labs.google/</link>
            <guid>38730107</guid>
            <pubDate>Fri, 22 Dec 2023 02:13:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.google/">https://labs.google/</a>, See on <a href="https://news.ycombinator.com/item?id=38730107">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="0"><div><p><img src="https://storage.googleapis.com/labs-web-prod/Charlene_profile.jpeg" alt="Charlene Wang profile"></p></div><p>#NotebookLM finally made my notes useful. I have saved a lot of PDFs, Google Docs, and ebooks that I want to read but never got a chance to do so. When NotebookLM came out, I created folders of business, philosophy, and lifestyle. Then I created a NotebookLM doc for each of them. I’d go to the corresponding doc to ask questions in each area of my life. Sometimes it’d give me the answer I need or provide the source/context for me to dive deeper. Now I finally get to “use” this knowledge at the right time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leonardo Da Vinci’s Self-Powered Cart (2021) (130 pts)]]></title>
            <link>https://selmec.org.uk/articles/112-leonardo-da-vinci-s-self-powered-cart</link>
            <guid>38729707</guid>
            <pubDate>Fri, 22 Dec 2023 01:21:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://selmec.org.uk/articles/112-leonardo-da-vinci-s-self-powered-cart">https://selmec.org.uk/articles/112-leonardo-da-vinci-s-self-powered-cart</a>, See on <a href="https://news.ycombinator.com/item?id=38729707">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
	
        
        
        
        
        <div><p><a href="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-1-large.jpg" target="_blank" title="Click to enlarge"><img alt="Figure 1: Initial Meccano model of Da Vinci’s self-powered cart" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-1-small.jpg"></a></p><p><span>Figure 1</span> Initial Meccano model of Da Vinci’s self-powered cart</p>
</div>

<h2>Background</h2>

<p>In June 2016 I was asked to give a presentation and demonstration of Meccano to a class of children at Charlton Church of England Primary School in Dover, who were looking for inspiration for an in-class construction project.</p>

<p>As classroom space was limited I used my <a href="https://selmec.org.uk/articles/104-m4-high-speed-tractor">M4 High-Speed Tractor</a> as the physical demonstrator and showed a 50+ PowerPoint slide presentation to cover the origins of Meccano, typical models from simple to more complex, and to indicate the range and diversity of subject matter possible in Meccano modelling. This was very well received by an attentive and interested class.</p>

<p>An outcome of this event was that I was invited to help supervise a group of the class on a visit to <a href="https://www.dovertransportmuseum.org.uk/" target="_blank">Dover Transport Museum</a>. After recovering from the fact that I had never heard of this venue, on researching it I was amazed to discover the range of vehicles and memorabilia on display there!</p>

<p>On the day of the visit, whilst the children were being mustered and told how to behave during the visit, I had time for a preview of the museum. To my further amazement, in the foyer was displayed a large replica of Da Vinci’s self-powered chariot (figure 2). The vehicle is about 2m square, and is constructed in wood of quite massive proportions. One of the museum staff proved to be conversant with its working and gave me a credible explanation of how it was powered by the leaf springs connected to capstans driving the wheels. He had seen it operate on only one occasion when it travelled about 20 feet under its own power.</p>

<div><p><a href="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-2-large.jpg" target="_blank" title="Click to enlarge"><img alt="Figure 2: An interpretation of Da Vinci’s self-powered cart in Dover Transport Museum" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-2-small.jpg"></a></p><p><span>Figure 2</span> An interpretation of Da Vinci’s self-powered cart in Dover Transport Museum</p>
</div>

<p>I believe the wooden version on display had been produced by a university group some years ago, who claimed it to be their interpretation of Da Vinci’s famous sketch. It had been discovered in one of many buildings previously owned by the Ministry of Defence which was now used by the museum.</p>

<p>All this was a complete revelation to me as, over the years, I had studied Da Vinci’s sketch (figure 3) on several occasions, in the hope of reproducing it in Meccano one day, only to give up in despair of deciphering its working from the drawings, an outcome that many before me had also suffered apparently.</p>

<div><p><a href="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-3-large.jpg" target="_blank" title="Click to enlarge"><img alt="Figure 3: Da Vinci’s sketch, circa 1478" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-3-small.jpg"></a></p><p><span>Figure 3</span> Da Vinci’s sketch, circa 1478</p>
</div>

<h2>Further Research</h2>

<p>The internet revealed some interesting results — over the centuries, many scholars have puzzled over Da Vinci’s sketch, most of whom assumed that the motive power was provided by leaf springs. This theory was questioned in 1975 by Carlo Pedretti (see reference 1) who proposed that the springs shown in Da Vinci’s sketch were not part of the drive system.</p>

<p>In 1996, Mark Rosheim wrote in a book that he thought that the ‘car’s engines’ were coil springs located in tambours (drums) on the underside of the cart. This led to Professor Paulo Galluzzi, director of the <a href="https://www.museogalileo.it/en" target="_blank">Museo Galileo</a> (formerly the Institute and Museum of the History of Science) in Florence, Italy, commissioning a 3D CAD digital modelling project in 2004 (see reference 2). Videos of the actual and digital cart provide an excellent insight to the machine. The project realised full-size and one-third scale models of the cart, the latter being demonstrated in Florence in April 2004 (figure 4).</p>

<p>The museum referred to the machine as ‘Leonardo’s automobile’, implying it was the first car.</p>

<div><p><img alt="Figure 4: Working model of Da Vinci’s self-propelled cart — note the divided axle!" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-4.jpg"></p><p><span>Figure 4</span> Working model of Da Vinci’s self-propelled cart — note the divided axle!</p>
</div>

<h2>The Brake</h2>

<p>As indicated in Da Vinci’s sketch, the machine is equipped with a brake that jambs between the tambour gears. This can be released by a rope allowing the machine to be set in motion by an operator hiding out of sight, thereby adding to the mystique.</p>

<h2>Escapements</h2>

<p>At the top of each vertical drive shaft driven by the tambour gears are rimless spoked wheels into which light leaf springs engage to perform a speed regulating function.</p>

<h2>Programmable Steering</h2>

<p>It transpires that the leaf springs, originally thought to provide traction, were part of the steering system. The gears above each tambour are fitted with a variable number of cams which operate arms, the opposite ends of which are attached to the aforementioned leaf springs. One of these arms is secured to the smaller wheel vertical shaft, thereby turning (steering) the wheel in response to the cam action.</p>

<p>The system means that the machine can only steer in one direction, unless the cams are rearranged.</p>

<p>The number of cams employed allows one or several steering actions per revolution of the large gears to which they are fixed, hence the term ‘programmable steering’.</p>

<p>Tension adjustment of the arm activating the steer motion is by a rack and pinion arrangement — this has led to a proposal that Da Vinci invented the rack and pinion steering mechanism.</p>

<h2>A Question</h2>

<p>After much study and repeated viewing of videos and other references on the subject, I was still unsure as to how these models actually worked. My dilemma was working out how the machine could steer, when the two gears above the tambours are meshed together. This arrangement means that both driving wheels are directly geared together, thereby not allowing any differential rotation. So there would be no point in the divided axle arrangement that Da Vinci’s sketch showed so clearly, and was adopted by Prof. Galluzzi in his model (figure 5). My mental analysis and initial Meccano model confirmed that the machine would not steer effectively without the wheel(s) skidding!</p>

<p>Did Da Vinci overlook this situation, misleading all the subsequent students and model makers? Or have I missed a point?</p>

<div><p><a href="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-5-large.jpg" target="_blank" title="Click to enlarge"><img alt="Figure 5: The 3D CAD model showing the meshing tambour gears and steering mechanism of Prof. Galluzzi’s project" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-5-small.jpg"></a></p><p><span>Figure 5</span> The 3D CAD model showing the meshing tambour gears and steering mechanism of Prof. Galluzzi’s project</p>
</div>

<h2>The Meccano Model</h2>

<p>Notwithstanding the question above, figures1 and 6 show my interpretation of Da Vinci’s machine in which the tambour gears do not mesh. Figure 6 is an overhead view of the deck with the Da Vinci machine features annotated and as interpreted in the model.</p>

<div><p><a href="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-6-large.jpg" target="_blank" title="Click to enlarge"><img alt="Figure 6: Features of the Da Vinci self-propelling cart as developed in model form" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-6-small.jpg"></a></p><p><span>Figure 6</span> Features of the Da Vinci self-propelling cart as developed in model form</p>
</div>

<p>The obvious next issue was reproducing the clockwork mechanism. It could be imitated by using two Meccano clockwork motors; however, I wanted to copy Da Vinci’s intent, initially at least.</p>

<p>So a search for suitable clock springs that might fit into a Boiler End was made, without any success, until I consulted a horologist friend who referred me to Meadows &amp; Passmore’s <a href="https://www.m-p.co.uk/" target="_blank">website</a> which lists a large range of clock springs. The first hole end spring I ordered (13mm wide x 0.4mm thick x 50mm diameter) proved to be far too powerful to contain within a Boiler End without distortion. The second (10mm x 0.28mm x 28mm) proved easier to handle and I managed to capture the inner end via its hole, onto a Key Bolt with the tip ground off, fitted into a Coupling. The outer end was similarly attached to a shortened Bolt passing through the Boiler End flange.</p>

<p>With the one wheel driven by this spring the cart travelled about two feet! This proved the feasibility of the system, but obviously much more development was required in reducing friction and optimising the spring properties.</p>

<p>The Boiled Ends were attached to 95-tooth Gears which drive 25-tooth Pinions to each Spoked Wheel via 19-tooth Pinions and 50-tooth Contrate Gears. The cams for the ‘programmable steering’ are Threaded Bosses. As it is difficult to replicate the leaf springs of the steering and escapement mechanisms, I improvised using Tension and Compression Springs. The third steered wheel is a 1½” Pulley without Boss. The steering arm rack and pinion adjustment is by plastic parts.</p>

<p>The brake is a Trunnion, the flange of which engages with the 95-tooth gear teeth. This is spring-loaded in engagement to act as a ratchet when winding the springs. It is released by pressing the collar at the rear of the cart; disengagement is maintained by a toggle arrangement.</p>

<p>I was always sceptical of Da Vinci’s ‘escapement’ (speed regulator) and the first method I adopted to replicate it was not very effective. The interrupting action of the ‘flipper’ wastes power as the spring unwinds. With the minimal speed regulation control, it was necessary to fit tyres in the form of ‘Meccano’ elastic wrist bands to the driving wheels in order to prevent instantaneous wheel spin on release of the brake. The performance was short lived in time and distance and the use of compression and tension springs seriously limited the power to the wheels. A much lower rate system is required to minimize the power loss; maybe elastic bands would be better?</p>

<div><p><a href="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-7-large.jpg" target="_blank" title="Click to enlarge"><img alt="Figure 7: Underside view showing one spring installation and other detail" src="https://selmec.org.uk/assets/articles/000112-leonardo-da-vinci-s-self-powered-cart-figure-7-small.jpg"></a></p><p><span>Figure 7</span> Underside view showing one spring installation and other detail</p>
</div>

<h2>Further Development</h2>

<p>A centrifugal governor could be more efficient, so a small governor design is in development which will fit into a 1–1/8” Flanged Wheel. The design precludes the use of friction elements due to its small size, so it relies on the centrifugal force between Collars and the internal cylindrical surface of the wheel flange.</p>

<p>As Meadows &amp; Passmore’s available spring stock was limited, I searched elsewhere and found <a href="https://www.hswalsh.com/" target="_blank">HS Walsh and Sons</a>, who listed a more useful range of sizes for my application. Some of the problems of dealing with and selecting clock springs are difficulty in attaching the ends of the spring to the barrel and hub (this depends on the hole size in the spring ends) and separating the coils in order to make the connections when the spring is coiled! It can be very dangerous if the spring constraint is lost and it is allowed to freely uncoil — an aggressive jack-in-a-box! Presumably clock makers have developed techniques for handling this situation.</p>

<p>A HS Walsh and Sons 11mm x 0.3mm x 30mm spring has been earmarked for further experimentation.</p>

<p>See more <a href="https://selmec.org.uk/models/1034-leonardo-da-vinci-s-self-powered-cart">photos of this model</a>.</p>

<h2>References</h2>

<ol>
	<li>Hooper, John. “Leonardo’s car brought to life.” <em>The Guardian</em>, 24 April 2004, <a href="https://www.theguardian.com/world/2004/apr/24/italy.arts" target="_blank">www.theguardian.com/world/2004/apr/24/italy.arts</a>. Accessed 15 October 2021.</li>
	<li>“Self-Propelling Cart.” <em>Leonardo3 Museum</em>, <a href="https://www.leonardo3.net/en/l3-works/machines/1441-self-propelling-cart.html" target="_blank">www.leonardo3.net/en/l3-works/machines/1441-self-propelling-cart.html</a>. Accessed 15 October 2021.</li>
</ol>
        
<div id="cphContent_mesComments_udpPost">
			
            
            
            
            
            <p><label for="cphContent_mesComments_chkIsPublic">Allow others to view your comment or question</label>
            </p>
            <p>Your e-mail address will not be displayed in public and will not be added to mailing lists. Please see our <span><a href="https://selmec.org.uk/legal-information#privacy-policy">privacy policy</a></span> for further information.</p>
            <hr>
            <p>
                <span id="cphContent_mesComments_spnPostWait">Please wait while we post your message…</span>
            </p>
            </div>

    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modern C for Fedora (and the World) (104 pts)]]></title>
            <link>https://lwn.net/Articles/954018/</link>
            <guid>38729278</guid>
            <pubDate>Fri, 22 Dec 2023 00:34:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/954018/">https://lwn.net/Articles/954018/</a>, See on <a href="https://news.ycombinator.com/item?id=38729278">Hacker News</a></p>
Couldn't get https://lwn.net/Articles/954018/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Polish DRMed trains stop as predicted due to date-based logic-bomb (217 pts)]]></title>
            <link>https://social.hackerspace.pl/@q3k/111618420373868285</link>
            <guid>38729035</guid>
            <pubDate>Fri, 22 Dec 2023 00:10:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://social.hackerspace.pl/@q3k/111618420373868285">https://social.hackerspace.pl/@q3k/111618420373868285</a>, See on <a href="https://news.ycombinator.com/item?id=38729035">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Deep cloning objects in JavaScript (182 pts)]]></title>
            <link>https://www.builder.io/blog/structured-clone</link>
            <guid>38728435</guid>
            <pubDate>Thu, 21 Dec 2023 23:13:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.builder.io/blog/structured-clone">https://www.builder.io/blog/structured-clone</a>, See on <a href="https://news.ycombinator.com/item?id=38728435">Hacker News</a></p>
<div id="readability-page-1" class="page"><div builder-type="blocks" data-builder-component="blog-article" data-builder-content-id="f4ddb8da3c21492ea5ff448b53b33ab5" builder-content-id="f4ddb8da3c21492ea5ff448b53b33ab5" builder-model="blog-article" data-name="blog-article" data-source="Rendered by Builder.io"><p><span><p>Did you know, there's now a native way in JavaScript to do deep copies of objects?</p><p>That's right, this <code>structuredClone</code> function is built into the JavaScript runtime:</p></span></p><div builder-id="builder-36c488f34390432b94a12e0888650a33"><pre><code>const calendarEvent = {
  title: "Builder.io Conf",
  date: new Date(123),
  attendees: ["Steve"]
}

// 😍
const copied = structuredClone(calendarEvent)</code></pre></div><p><span><p>Did you notice in the example above we not only copied the object, but also the nested array, and even the Date object?</p><p>And all works precisely as expected:</p></span></p><div builder-id="builder-33474c248e954493849183f4d0a9523f"><pre><code>copied.attendees // ["Steve"]
copied.date // Date: Wed Dec 31 1969 16:00:00
cocalendarEvent.attendees === copied.attendees // false</code></pre></div><p><span><p>That’s right, <code>structuredClone</code> can not only do the above, but additionally:</p><ul><li>Clone infinitely nested objects and arrays</li><li>Clone circular references</li><li>Clone a wide variety of JavaScript types, such as <code>Date</code>, <code>Set</code>, <code>Map</code>, <code>Error</code>, <code>RegExp</code>, <code>ArrayBuffer</code>, <code>Blob</code>, <code>File</code>, <code>ImageData</code>, and <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm#supported_types" rel="noopener noreferrer" target="_blank">many more</a></li><li>Transfer any <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Transferable_objects" rel="noopener noreferrer" target="_blank">transferable objects</a></li></ul><p>So for example, this madness would even work as expected:</p></span></p><div builder-id="builder-f41b32bcf42742d2901b0ee215b78b0d"><pre><code>const kitchenSink = {
  set: new Set([1, 3, 3]),
  map: new Map([[1, 2]]),
  regex: /foo/,
  deep: { array: [ new File(someBlobData, 'file.txt') ] },
  error: new Error('Hello!')
}
kitchenSink.circular = kitchenSink

// ✅ All good, fully and deeply copied!
const clonedSink = structuredClone(kitchenSink)</code></pre></div><p builder-id="builder-c5d1bb87545c44049bdc9f0df7d7911f"><a href="#why-not-just-object-spread" id="why-not-just-object-spread"><span><h2>Why not just object spread?</h2></span></a></p><p><span><p>It is important to note we are talking about a deep copy. If you just need to do a shallow copy, aka a copy that does not copy nested objects or arrays, then we can just do an object spread:</p></span></p><div builder-id="builder-abe99c668ccd45b49b77971cd9794fc6"><pre><code>const simpleEvent = {
  title: "Builder.io Conf",
}
// ✅ no problem, there are no nested objects or arrays
const shallowCopy = {...calendarEvent}</code></pre></div><p><span><p>Or even one of these, if you prefer</p></span></p><div builder-id="builder-5967b33bd4214a1b890c0351971b19e9"><pre><code>const shallowCopy = Object.assign({}, simpleEvent)
const shallowCopy = Object.create(simpleEvent)</code></pre></div><p><span><p>But as soon as we have nested items, we run into trouble:</p></span></p><div builder-id="builder-c2218c84d0e047d7b50bd9716eeb176b"><pre><code>const calendarEvent = {
  title: "Builder.io Conf",
  date: new Date(123),
  attendees: ["Steve"]
}

const shallowCopy = {...calendarEvent}

// 🚩 oops - we just added "Bob" to both the copy *and* the original event
shallowCopy.attendees.push("Bob")

// 🚩 oops - we just updated the date for the copy *and* original event
shallowCopy.date.setTime(456)</code></pre></div><p><span><p>As you can see, we did not make a full copy of this object.</p><p>The nested date and array are still a shared reference between both, which can cause us major issues if we want to edit those thinking we are only updating the copied calendar event object.</p></span></p><p builder-id="builder-980b1f7bc0364b808c196a4df1b4c2c5"><a href="#why-not-code-json-parse-json-stringify-x-code" id="why-not-code-json-parse-json-stringify-x-code"><span><h2>Why not <code>JSON.parse(JSON.stringify(x))</code> ?</h2></span></a></p><p><span><p>Ah yes, this trick. It is actually a great one, and is surprisingly performant, but has some shortcomings that <code>structuredClone</code> addresses.</p><p>Take this as an example:</p></span></p><div builder-id="builder-2e2500c35b304aadabc88ab3ed5f93e8"><pre><code>const calendarEvent = {
  title: "Builder.io Conf",
  date: new Date(123),
  attendees: ["Steve"]
}

// 🚩 JSON.stringify converted the `date` to a string
const problematicCopy = JSON.parse(JSON.stringify(calendarEvent))</code></pre></div><p><span><p>If we log <code>problematicCopy</code>, we would get:</p></span></p><div builder-id="builder-6e83229f45ba4872aa18b7b0f43b9247"><pre><code>{
  title: "Builder.io Conf",
  date: "1970-01-01T00:00:00.123Z"
  attendees: ["Steve"]
}</code></pre></div><p><span><p>That’s not what we wanted! <code>date</code> is supposed to be a <code>Date</code> object, not a string.</p><p>This happened because <code>JSON.stringify</code> can only handle basic objects, arrays, and primitives. Any other type can be handled in hard to predict ways. For instance, Dates are converted to a string. But a <code>Set</code> is simply converted to <code>{}</code>.</p><p><code>JSON.stringify</code> even completely ignores certain things, like <code>undefined</code> or functions.</p><p>For instance, if we copied our <code>kitchenSink</code> example with this method:</p></span></p><div builder-id="builder-ac9e622fa5c4476b87a761e73520c349"><pre><code>const kitchenSink = {
  set: new Set([1, 3, 3]),
  map: new Map([[1, 2]]),
  regex: /foo/,
  deep: { array: [ new File(someBlobData, 'file.txt') ] },
  error: new Error('Hello!')
}

const veryProblematicCopy = JSON.parse(JSON.stringify(kitchenSink))</code></pre></div><p><span><p>We would get:</p></span></p><div builder-id="builder-12cab9f8152e403aaa3a1a9fa9ca6785"><pre><code>{
  "set": {},
  "map": {},
  "regex": {},
  "deep": {
    "array": [
      {}
    ]
  },
  "error": {},
}</code></pre></div><p><span><p>Ew!</p><p>Oh yeah, and we had to remove the circular reference we originally had for this, as <code>JSON.stringify</code> simply throws errors if it encounters one of those.</p><p>So while this method can be great if our requirements fit what it can do, there is a lot that we can do with <code>structuredClone</code> (aka everything above that we failed to do here) that this method cannot.</p></span></p><p builder-id="builder-6016311df60e48b4a720e9e6fd5bb96f"><a href="#why-not-code-clone-deep-code" id="why-not-code-clone-deep-code"><span><h2>Why not <code>_.cloneDeep</code>?</h2></span></a></p><p><span><p>To date, Lodash’s <code>cloneDeep</code> function has been a very common solution to this problem.</p><p>And this does, in fact, work as expected:</p></span></p><div builder-id="builder-68dd9a4555a24c7cb7900843b7c69ad3"><pre><code>import cloneDeep from 'lodash/cloneDeep'

const calendarEvent = {
  title: "Builder.io Conf",
  date: new Date(123),
  attendees: ["Steve"]
}

// ✅ All good!
const clonedEvent = structuredClone(calendarEvent)</code></pre></div><p><span><p>But, there is just one caveat here. According to the <a href="https://marketplace.visualstudio.com/items?itemName=wix.vscode-import-cost">Import Cost</a> extension in my IDE, that prints the kb cost of anything I import, this one function comes in at a whole 17.4kb minified (5.3kb gzipped):</p></span></p><div builder-id="builder-02edcbbc54254ea193c56e7495ce4ba5"><picture><source srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=548 548w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?format=webp&amp;width=728 728w" type="image/webp"><img alt="Screenshot of the import cost of 'lodash/cloneDeep' at 5.3kb gzipped" loading="lazy" src="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=800" srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=548 548w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Faa5ab14fd21741bf8e327dd6e6fb68b1?width=728 728w" sizes="(max-width: 638px) 86vw,  (max-width: 998px) 73vw, 58vw"></picture></div><p><span><p>And that assumes you import just that function. If you instead import the more common way, not realizing that tree shaking doesn’t always work the way you hoped, you could accidentally import up to <a href="https://bundlephobia.com/package/lodash@4.17.21">25kb</a> just for this one function 😱</p></span></p><div builder-id="builder-0c88e47c09194f36a6703a0e7708a129"><picture><source srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=548 548w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?format=webp&amp;width=728 728w" type="image/webp"><img alt="Screenshot of the import cost of 'lodash' at 25.2kb gzipped" loading="lazy" src="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=800" srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=548 548w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F5dbbee190753414bb31b720059a501db?width=728 728w" sizes="(max-width: 638px) 86vw,  (max-width: 998px) 73vw, 58vw"></picture></div><p><span><p>While that will not be the end of the world to anyone, it’s simply not necessary in our case, not when browsers already have <code>structuredClone</code> built in.</p></span></p><p builder-id="builder-273f873f903749e99cf0e82640fb14e4"><a href="#what-can-code-structured-clone-code-em-not-em-clone" id="what-can-code-structured-clone-code-em-not-em-clone"><span><h2>What can <code>structuredClone</code> <em>not</em> clone</h2></span></a></p><p builder-id="builder-35b8f10d3d8148b89e950c1335546219"><a href="#functions-cannot-be-cloned" id="functions-cannot-be-cloned"><span><h3>Functions cannot be cloned</h3></span></a></p><p><span><p>They will a throw a&nbsp;<code>DataCloneError</code>&nbsp;exception:</p></span></p><div builder-id="builder-16a1f929c8bf45db944fd2348a3268b6"><pre><code>// 🚩 Error!
structuredClone({ fn: () =&gt; { } })</code></pre></div><p builder-id="builder-45eb949a7f214b3d9538288483981786"><a href="#dom-nodes" id="dom-nodes"><span><h3>DOM nodes</h3></span></a></p><p><span><p>Also throws a <code>DataCloneError</code>&nbsp;exception:</p></span></p><div builder-id="builder-5646bd3bd658482f80566716fcf7282b"><pre><code>// 🚩 Error!
structuredClone({ el: document.body })</code></pre></div><p builder-id="builder-62ed3c96704d475896635daf18e8e15a"><a href="#property-descriptors-setters-and-getters" id="property-descriptors-setters-and-getters"><span><h3>Property descriptors, setters, and getters</h3></span></a></p><p><span><p>As well as similar metadata-like features are not cloned.</p><p>For instance, with a getter, the resulting value is cloned, but not the getter function itself (or any other property metadata):</p></span></p><div builder-id="builder-58148ca0ac31495fbb005565b4540dad"><pre><code>structuredClone({ get foo() { return 'bar' } })
// Becomes: { foo: 'bar' }</code></pre></div><p builder-id="builder-d244355296314a628e1d46c9c4fc90bd"><a href="#object-prototypes" id="object-prototypes"><span><h3>Object prototypes</h3></span></a></p><p><span><p>The prototype chain is not walked or duplicated. So if you clone an instance of <code>MyClass</code>, the cloned object will no longer be known to be an instance of this class (but all valid properties of this class will be cloned)</p></span></p><div builder-id="builder-8390f0d92e00423f93c5f6a1d2d0f7a9"><pre><code>class MyClass { 
  foo = 'bar' 
  myMethod() { /* ... */ }
}
const myClass = new MyClass()

const cloned = structuredClone(myClass)
// Becomes: { foo: 'bar' }

cloned instanceof myClass // false</code></pre></div><p builder-id="builder-f91c468430904bec8aa2fa661353de29"><a href="#full-list-of-supported-types" id="full-list-of-supported-types"><span><h3>Full list of supported types</h3></span></a></p><p><span><p>More simply put, anything not in the below list cannot be cloned:</p></span></p><p><span><p><code>Array</code>, <code>ArrayBuffer</code>, <code>Boolean</code>, <code>DataView</code>, <code>Date</code>, <code>Error</code>&nbsp;types (those specifically listed below), <code>Map</code> , <code>Object</code> but only plain objects (e.g. from object literals), <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#primitive_values">Primitive types</a>, except&nbsp;<code>symbol</code> (aka <code>number</code>, <code>string</code>, <code>null</code>, <code>undefined</code>, <code>boolean</code>, <code>BigInt</code>), <code>RegExp</code>, <code>Set</code>, <code>TypedArray</code></p></span></p><p builder-id="builder-bf4d36d1b5764e2c804c52b01b9fa5b4"><a href="#error-types" id="error-types"><span><h4>Error types</h4></span></a></p><p><span><p><code>Error</code>, <code>EvalError</code>, <code>RangeError</code>, <code>ReferenceError</code> , <code>SyntaxError</code>, <code>TypeError</code>, <code>URIError</code></p></span></p><p><span><p><code>AudioData</code>, <code>Blob</code>, <code>CryptoKey</code>, <code>DOMException</code>, <code>DOMMatrix</code>, <code>DOMMatrixReadOnly</code>, <code>DOMPoint</code>, <code>DomQuad</code>, <code>DomRect</code>, <code>File</code>, <code>FileList</code>, <code>FileSystemDirectoryHandle</code>, <code>FileSystemFileHandle</code>, <code>FileSystemHandle</code>, <code>ImageBitmap</code>, <code>ImageData</code>, <code>RTCCertificate</code>, <code>VideoFrame</code></p></span></p><p builder-id="builder-4b44994d751f4891a7982587725fcdcd"><a href="#browser-and-runtime-support" id="browser-and-runtime-support"><span><h2>Browser and runtime support</h2></span></a></p><p><span><p>And here is the best part - <code>structuredClone</code> is supported in all major browsers, and even Node.js and Deno.</p><p>Just note the caveat with Web Workers having more limited support:</p></span></p><div builder-id="builder-c0ca0549766546a0b613e0cff7247bbf"><picture><source srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=548 548w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?format=webp&amp;width=728 728w" type="image/webp"><img alt="Browser support table - screenshot from the link directly below this image" loading="lazy" src="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=800" srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=548 548w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F1fdbc5b0826240e487a4980dfee69661?width=728 728w" sizes="(max-width: 638px) 86vw,  (max-width: 998px) 73vw, 58vw"></picture></div><p builder-id="builder-90b3b91591394d9dabbc973da1adb95c"><a href="#conclusion" id="conclusion"><span><h2>Conclusion</h2></span></a></p><p><span><p>It’s been a long time coming, but we finally now have <code>structuredClone</code> to make deep cloning objects in JavaScript a breeze.  Thank you, <a href="https://web.dev/structured-clone/">Surma</a>.</p></span></p><p builder-id="builder-432467a43d414d05baec3d953b211180"><a href="#about-me" id="about-me"><span><h2>About Me</h2></span></a></p><p><span><p>Hi! I'm&nbsp;<a href="https://twitter.com/Steve8708?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" rel="noopener noreferrer" target="_blank">Steve</a>, CEO of&nbsp;<a href="https://www.builder.io/" rel="noopener noreferrer" target="_blank">Builder.io</a>.</p><p>We make a way to drag + drop with your components to create pages and other CMS content on your site or app, visually.</p><p>You can read more about how this can improve your workflow <a href="https://www.builder.io/blog/headless-cms-workflow" rel="noopener noreferrer" target="_blank">here</a>.</p><p>You may find it interesting or useful:</p></span></p><div data-name="symbol" data-source="Rendered by Builder.io" data-model="symbol" builder-id="builder-672f1c59927048908c2546489cd63e61" builder-type="blocks" data-builder-component="symbol" data-builder-content-id="02714711c4b647959d015e34679ca6ee" builder-content-id="02714711c4b647959d015e34679ca6ee" builder-model="symbol"><template data-template-variant-id="6d0b491a00e64dce9781e3c0a9839b71"><div class="builder-content" builder-content-id="6d0b491a00e64dce9781e3c0a9839b71" builder-model="symbol"><div data-builder-component="symbol" data-builder-content-id="de784470c6884829b5e7c7603f4c51ed"><div class="builder-blocks css-h47494" builder-type="blocks"><style data-emotion-css="kf99zz">.css-kf99zz.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-left:-130px;margin-right:-260px;margin-top:10px;z-index:11px;}@media only screen and (max-width:991px){.css-kf99zz.builder-block{margin-left:0;margin-right:0;}}</style><div class="builder-block builder-7c77e4f236f848b48c79a96f49072518 css-kf99zz" builder-id="builder-7c77e4f236f848b48c79a96f49072518"><style data-emotion-css="jgc6i5">.css-jgc6i5.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:50px;width:100%;min-height:20px;min-width:20px;overflow:hidden;mix-blend-mode:multiply;margin-left:auto;margin-right:auto;max-width:405px;}@media only screen and (max-width:991px){.css-jgc6i5.builder-block{margin-top:10px;}}</style><div class="builder-block builder-661f4bd90a3547fa85f50126d1418c87 dark-mode-invert css-jgc6i5" builder-id="builder-661f4bd90a3547fa85f50126d1418c87"><picture><source srcset="https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=100 100w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=200 200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=400 400w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=800 800w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=1200 1200w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=1600 1600w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=2000 2000w, https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2F83677fb912dd4a539a3f3bb5c0c82de1?format=webp&amp;width=405 405w" type="image/webp"></picture><style data-emotion-css="1sn22t4">.css-1sn22t4{width:100%;padding-top:35.4%;pointer-events:none;font-size:0;}</style><div class="builder-image-sizer css-1sn22t4"> </div></div><div class="builder-block builder-296e83c56835496a896d380e22d2fbd9 builder-has-component css-logohe" builder-id="builder-296e83c56835496a896d380e22d2fbd9"><style data-emotion-css="1fuo4gk">.css-1fuo4gk{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}@media (max-width:991px){.css-1fuo4gk{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;}}</style><div class="builder-columns css-1fuo4gk"><style data-emotion-css="15p9q9i">.css-15p9q9i{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;line-height:normal;width:calc(58.333% - 15px);margin-left:0;}.css-15p9q9i > .builder-blocks{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}@media (max-width:991px){.css-15p9q9i{width:100%;margin-left:0;}}</style><div class="builder-column css-15p9q9i"><div class="builder-blocks builder-blocks-child css-h47494" builder-type="blocks" builder-parent-id="builder-296e83c56835496a896d380e22d2fbd9"><style data-emotion-css="8r3wet">.css-8r3wet.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:0px;min-height:20px;min-width:20px;width:100%;border-radius:5px;overflow:hidden;margin-right:auto;}</style><div class="builder-block builder-50cefa294545480a96166693c05e207d css-8r3wet" builder-id="builder-50cefa294545480a96166693c05e207d"><style data-emotion-css="79elbk">.css-79elbk{position:relative;}</style><div class="css-79elbk"><style data-emotion-css="cz4v3a">.css-cz4v3a{width:100%;height:100%;object-fit:cover;object-position:center;z-index:2;border-radius:1px;position:absolute;}</style><video autoplay="" muted="" loop="" class="builder-video css-cz4v3a"></video><style data-emotion-css="m5569v">.css-m5569v{width:100%;padding-top:58.41%;pointer-events:none;font-size:0;}</style><div class="css-m5569v"></div></div></div></div></div><style data-emotion-css="1xaxpyc">.css-1xaxpyc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;line-height:normal;width:calc(41.667% - 15px);margin-left:30px;}.css-1xaxpyc > .builder-blocks{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}@media (max-width:991px){.css-1xaxpyc{width:100%;margin-left:0;}}</style><div class="builder-column css-1xaxpyc"><div class="builder-blocks builder-blocks-child css-h47494" builder-type="blocks" builder-parent-id="builder-296e83c56835496a896d380e22d2fbd9"><style data-emotion-css="9f7tx5">.css-9f7tx5.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:auto;margin-bottom:auto;}</style><div class="builder-block builder-2487409df20c4d6ea916563459503bd5 css-9f7tx5" builder-id="builder-2487409df20c4d6ea916563459503bd5"><style data-emotion-css="1718vnj">.css-1718vnj.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:0px;height:auto;font-size:24px;}@media only screen and (max-width:991px){.css-1718vnj.builder-block{margin-top:28px;}}</style><div class="builder-block builder-01c085f9d331407291e18ae00c2e705f builder-has-component css-1718vnj" builder-id="builder-01c085f9d331407291e18ae00c2e705f"><span class="builder-text css-1qggkls"><p style="">Visually build with your components</p></span></div><style data-emotion-css="1mbueh1">.css-1mbueh1.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:12px;height:auto;}@media only screen and (max-width:991px){.css-1mbueh1.builder-block{margin-top:13px;}}</style><div class="builder-block builder-522f74098b38447289e1bbe3e525c9d6 fix-line-height builder-has-component css-1mbueh1" builder-id="builder-522f74098b38447289e1bbe3e525c9d6"><span class="builder-text css-1qggkls"><p style=""><a href="https://www.builder.io/m/developers" rel="noopener noreferrer" target="_blank">Builder.io</a>&nbsp;is a visual editor that connects to any site or app and lets you&nbsp;<a href="https://www.builder.io/m/products" rel="noopener noreferrer" target="_blank">drag and drop</a>&nbsp;with&nbsp;<a href="https://www.builder.io/m/developers" rel="noopener noreferrer" target="_blank">your components</a>.<br></p>
</span></div><style data-emotion-css="etaca8">.css-etaca8.builder-block{position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;margin-right:auto;margin-top:22px;}@media only screen and (max-width:991px){.css-etaca8.builder-block{margin-top:16px;}}@media only screen and (max-width:640px){.css-etaca8.builder-block{margin-top:17px;}}</style><div class="builder-block builder-04ce70c139f74cba9ce6b77cd2b59966 builder-has-component css-etaca8" builder-id="builder-04ce70c139f74cba9ce6b77cd2b59966"><style data-emotion-css="k008qs">.css-k008qs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="builder-columns css-k008qs"><style data-emotion-css="1ki2g2t">.css-1ki2g2t{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;line-height:normal;width:calc(50% - 10px);margin-left:0;}.css-1ki2g2t > .builder-blocks{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}</style><div class="builder-column css-1ki2g2t"><div class="builder-blocks builder-blocks-child css-h47494" builder-type="blocks" builder-parent-id="builder-04ce70c139f74cba9ce6b77cd2b59966"><style data-emotion-css="e9su4y">.css-e9su4y.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:0px;-webkit-appearance:none;-moz-appearance:none;appearance:none;padding-top:10px;padding-bottom:10px;padding-left:35px;padding-right:35px;background-color:rgba(0,0,0,1);color:white;border-radius:8px;text-align:center;cursor:pointer;width:auto;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;font-family:"Poppins",sans-serif;font-size:16px;line-height:26px;-webkit-letter-spacing:.5px;-moz-letter-spacing:.5px;-ms-letter-spacing:.5px;letter-spacing:.5px;margin-right:auto;white-space:nowrap;}@media only screen and (max-width:991px){.css-e9su4y.builder-block{font-size:16px;line-height:18px;padding-left:23px;padding-right:23px;margin-top:0px;padding-top:13px;padding-bottom:13px;}}</style><a role="button" href="https://builder.io/signup" target="_blank" class="builder-block builder-bbfd1a8be4e54fa0bc8f887da6d8a83c builder-has-component css-e9su4y" builder-id="builder-bbfd1a8be4e54fa0bc8f887da6d8a83c">Try it out</a></div></div><style data-emotion-css="1q570rn">.css-1q570rn{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;line-height:normal;width:calc(50% - 10px);margin-left:20px;}.css-1q570rn > .builder-blocks{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}</style><div class="builder-column css-1q570rn"><div class="builder-blocks builder-blocks-child css-h47494" builder-type="blocks" builder-parent-id="builder-04ce70c139f74cba9ce6b77cd2b59966"><style data-emotion-css="1t30id">.css-1t30id.builder-block{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;box-sizing:border-box;margin-top:0px;-webkit-appearance:none;-moz-appearance:none;appearance:none;padding-top:10px;padding-bottom:10px;padding-left:0px;padding-right:0px;color:rgba(0,0,0,1);border-radius:8px;text-align:center;cursor:pointer;width:auto;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;font-family:"Poppins",sans-serif;font-size:16px;line-height:26px;-webkit-letter-spacing:.5px;-moz-letter-spacing:.5px;-ms-letter-spacing:.5px;letter-spacing:.5px;margin-right:auto;margin-left:15px;}@media only screen and (max-width:991px){.css-1t30id.builder-block{font-size:16px;line-height:18px;padding-left:0px;padding-right:18px;margin-top:auto;margin-left:2px;min-width:110px;margin-bottom:auto;}}</style><a role="button" href="/m/developers" target="_blank" class="builder-block builder-b73a1d9611ee46b180da86ffdd53fa77 builder-has-component css-1t30id" builder-id="builder-b73a1d9611ee46b180da86ffdd53fa77">Learn more</a></div></div></div></div><div class="builder-block builder-dde61f974a184a338de6b5e43b675061 builder-has-component css-nwuw40" builder-id="builder-dde61f974a184a338de6b5e43b675061"><div class="code-block css-1ijwl8d"><button class="MuiButtonBase-root MuiIconButton-root copy-to-clipboard css-185ibg" tabindex="0" type="button" title="Copy code to clipboard"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root css-joy3r2" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M19 3h-4.18C14.4 1.84 13.3 1 12 1c-1.3 0-2.4.84-2.82 2H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-7 0c.55 0 1 .45 1 1s-.45 1-1 1-1-.45-1-1 .45-1 1-1zm2 14H7v-2h7v2zm3-4H7v-2h10v2zm0-4H7V7h10v2z"></path></svg></span></button><div class=""><pre style="display:block;overflow-x:auto;padding:0.5em;color:#ddd;background:#282c34;font-family:Menlo, Monaco, &quot;Courier New&quot;, monospace;line-height:1em;font-size:1.2em"><code class="language-jsx">// Dynamically render your components
export function MyPage({ json }) {
  return &lt;BuilderComponent content={json} /&gt;
}

registerComponents([MyHero, MyProducts])</code></pre></div></div></div></div></div></div></div></div></div><style data-emotion-css="1mvsfya">.css-1mvsfya.builder-block{height:0;width:0;display:inline-block;opacity:0;overflow:hidden;pointer-events:none;}</style><img role="presentation" aria-hidden="true" src="https://cdn.builder.io/api/v1/pixel?apiKey=YJIGb4i01jvw0SRdL5Bt" class="builder-block builder-pixel-2ua8393fpcb css-1mvsfya" builder-id="builder-pixel-2ua8393fpcb"></div></div></div></template><div builder-id="builder-7fd4bfcb2f3540cca129a9e10d1254f5" builder-type="blocks" builder-parent-id="builder-fd60c008f44a4c46be618e21afe30be5" data-builder-component="symbol" data-builder-content-id="de784470c6884829b5e7c7603f4c51ed" builder-content-id="de784470c6884829b5e7c7603f4c51ed" builder-model="symbol"><p><span><div><p>Introducing Visual Copilot: a new AI model to convert Figma designs to high quality code in a click.</p><p>No setup needed. 100% free. Supports all popular frameworks.</p></div>
</span></p><p><a role="button" href="https://builder.io/signup" target="_blank" builder-id="builder-a85aff888e7849f98193f66bd662f27e">Try Visual Copilot</a></p></div></div></div></div>]]></description>
        </item>
    </channel>
</rss>