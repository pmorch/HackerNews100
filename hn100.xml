<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 11 Aug 2025 00:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I tried coding with AI, I became lazy and stupid (101 pts)]]></title>
            <link>https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</link>
            <guid>44858641</guid>
            <pubDate>Sun, 10 Aug 2025 21:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid">https://thomasorus.com/i-tried-coding-with-ai-i-became-lazy-and-stupid</a>, See on <a href="https://news.ycombinator.com/item?id=44858641">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            

<p>Around April 2025, my boss at $dayjob insisted we try AI tools for coding. It wasn't toxic pressure or anything like <em>"20% of your code needs to be AI"</em>, just a concern from him that we could miss on something. I understand why he asked that and I don't blame him. We are in difficult economic period even for software, and we have salaries to pay. If AI can increase productivity or our margins, it should be at least put on the table of negotiations. I am not happy about this coming, but <em>I get it</em>.</p>

<details><summary>My personal stance of AI</summary> I have personal reasons to dislike LLMs. My partner lost their writing job due to chatGPT convincing their manager writers were now useless. A lot of artist friends struggle because of LLMs. We recently had an intern who lost her translator role due to LLMs. And even outside my personal experience, LLMs are based on stolen content, don't respect consent, waste huge amount of electricity and water, and are overall a new weapon for the capitalists in the <a href="https://danmcquillan.org/ai_thatcherism.html">class warfare</a>. </details>

<p>The other reason why I folded comes from a toxic relationship I built with my job when I became a developer. I detailed in a previous blog post how choosing this career came with very high stakes which triggered a shift in my brain that hasn't left me since:</p>

<figure><blockquote cite="/every-web-stack-is-a-product-now">When I started web development seven years ago, I was in survival mode after years of low paying wages and unemployment. It <em>had</em> to work, and for it to work, I <em>had</em> to always learn more, read and listen about web development all the time, monitor the field, socialize as much as possible with my peers. This way, I would not get disposable and lose my job. I would build a network. I would be safe.</blockquote><figcaption>— I, <a href="https://thomasorus.com/every-web-stack-is-a-product-now">In a blog post from 2022</a></figcaption></figure>

<p>10 years and 3 burnouts later, one can tell this mindset, even if it worked out for a while, wasn't sane or desirable. I had managed to put aside this fear of being disposable, but LLMs triggered it back big time. What if AI vendors were right? What if a future company I apply to requires you to use it? Am I going to lose my job? I'm almost 40, what will I do?</p>

<p>So I tried AI. First at my day job, because I wanted answers. But outside fixing TypeScript types errors, generate inaccessible template code, or review my code for errors, I couldn't find a <em>life changing</em> use out of it that all AI influencers talk about. I asked my colleagues about their own experiments, and lots of them came to the same conclusion: it doesn't seem to help me help our clients achieve their goals.</p>

<p>When July came I was starting the image processing part of my new CMS that powers this website. Still stressed I couldn't get a real shot at coding with an LLM, and very tired by different personal events that fogged my brain, I decided it was the right task to try it seriously and get answers.</p>

<p>After setting up everything in VS Code, opening the AI panel, giving access to the codebase and detailing my needs in a prompt, the LLM produced around 200 lines of code. Mostly functions using dependencies to convert, resize, process images. It wasn't perfect but after a few changes, the task was done and it had taken around 30 minutes, far less than if I had made it by hand.</p>

<p>I was impressed. It really felt like I had superpowers! But then I had the idea to audit the code the LLM just produced, like I did at my $dayjob for a Vue application. Feeling that uploading files could be a source of security issues, I asked the same LLM to focus on this specific topic.</p>

<p>It found several dangers: directory traversal attacks, file size limits, system file overwrite, etc. I had no idea the initial code was this unsafe. I had reviewed the code, but without enough experience in backend development, how could I identify issues I didn't know existed? And why, if it knew about all those dangers, did the LLM produced unsafe code in the first place?</p>

<p>When I tried to fix the security issues, I quickly realized how this whole thing was a trap. Since I didn't wrote it, I didn't have a good bird's eye view of the code and what it did. I couldn't make changes quickly, which started to frustrated me. The easiest route was asking the LLM to do the fixes for me, so I did. More code was changed and added. It worked, but again I could not tell if it was good or not.</p>

<p>That's when I stopped the experiment.</p>

<p>I was shocked by how easily I had slipped into this slacker way of programming. The LLM had given me shitty code, made me ignorant about my own code base, and too lazy to try to fix it myself. And at the same time, the whole experience felt smooth, frictionless, empowering. On the moment I felt smarter, more productive, in control. But it was all an illusion.</p>

<p>I knew about this as we had studies showing <a href="https://www.researchgate.net/publication/392560878_Your_Brain_on_ChatGPT_Accumulation_of_Cognitive_Debt_when_Using_an_AI_Assistant_for_Essay_Writing_Task">LLM use makes us dumb</a>, and that <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">self-reported productivity gains are false</a>. But experiencing it for myself was a totally different feeling.</p>

<p>It gave me a whole different perspective and answered my initial question: will I get replaced by AI soon?</p>

<p>The answer is no. I don't think AI will take my job anytime soon because it's smarter and more productive than I am. I also don't think AI will make me <a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">10 times more productive</a>. If I lose my job due to AI, it will be because I used it so much it made me lazy and stupid to the point another human has to replace me and I become unemployable.</p>

<p>I shouldn't invest time in AI. I should invest more time studying new things that interest me. That's probably the only way to keep doing this job and, you know, <em>be safe</em>.</p>


            
    

        </article><p><small><strong>Initially published: </strong>08 Aug 2025 at 20:03</small><br>
        <small><strong>Modified and/or rebuilt: </strong>08 Aug 2025 at 20:17</small>
    </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1910: The year the modern world lost its mind (165 pts)]]></title>
            <link>https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</link>
            <guid>44858154</guid>
            <pubDate>Sun, 10 Aug 2025 20:48:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost">https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost</a>, See on <a href="https://news.ycombinator.com/item?id=44858154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><p><em>“Automobilism is an illness, a mental illness. This illness has a pretty name: speed... [Man] can no longer stand still, he shivers, his nerves tense like springs, impatient to get going once he has arrived somewhere because it is not somewhere else, somewhere else, always somewhere else.” </em></p><p><em>- Octave Mirbeau, French novelist, 1910</em></p></div><p><em><strong>About today’s piece: When we hear about technological change and social crisis in the 21st century, it is easy to imagine that we are living through a special period of history. But many eras have grappled with the problems that seem to uniquely plague our own. The beginning of the 20th century was a period of speed and technological splendor (the automobile! the airplane! the bicycle!), shattered nerves, mass anxiety, and a widespread sense that the world had been forever knocked off its historical axis: a familiar stew of ideas. I think we can learn a lot about the present by studying historical periods whose challenges rhyme with our own.</strong></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QkGQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg" width="580" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:580,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;1910 Model T ad&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="1910 Model T ad" title="1910 Model T ad" srcset="https://substackcdn.com/image/fetch/$s_!QkGQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 424w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 848w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!QkGQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5598fa1c-5c64-43cf-800c-35ce14dd1a12_580x394.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Welcome back to The Sunday Morning Post!</p><p><span>My favorite period of history is the 30- to 40-year span between the end of the 19th century and the early innings of the 20th century. It was an era of incredible change. From </span><em>Abundance</em><span>:</span></p><blockquote><p>Imagine going to sleep in 1875 in New York City and waking up thirty years later. As you shut your eyes, there is no electric lighting, Coca-Cola, basketball, or aspirin. There are no cars or “sneakers.” The tallest building in Manhattan is a church. </p><p>When you wake up in 1905, the city has been remade with towering steel-skeleton buildings called “skyscrapers.” The streets are filled with novelty: automobiles powered by new internal combustion engines, people riding bicycles in rubber-soled shoes—all recent innovations. The Sears catalog, the cardboard box, and aspirin are  new arrivals. People have enjoyed their first sip of Coca-Cola and their first bite of what we now call an American hamburger. The Wright brothers have flown the first airplane. When you passed into slumber, nobody had taken a picture with a Kodak camera or used a machine that made motion pictures, or bought a device to play recorded music. By 1905, we have the first commercial versions of all three—the simple box camera, the cinematograph,  and the phonograph. </p></blockquote><p><span>No book on turn-of-the-century history has influenced me more, or brought me more joy, than </span><em><a href="https://www.amazon.com/Vertigo-Years-Europe-1900-1914/dp/0465020291" rel="">The Vertigo Years: Europe 1900-1914</a></em><span> by Philipp Blom. I think it might be the most underrated history book ever written.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-1-170457512" target="_self" rel="">1</a></span><span> In my favorite chapters focusing on the years around 1910</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-2-170457512" target="_self" rel="">2</a></span><span>, Blom describes how turn-of-the-century technology changed the way people thought about art and human nature and how it contributed to a nervous breakdown across the west. Disoriented by the speed of modern times, Europeans and Americans suffered from record-high rates of anxiety and a sense that our inventions had destroyed our humanity. Meanwhile, some artists channeled this disorientation to create some of the greatest art of all time.</span></p><p><span>In today’s TSMP, I want to share with you my favorite passages and lessons from </span><em>The Vertigo Years</em><span>, most of which come from the chapter on the year 1910. Great history books remind us that while history never repeats itself, its themes never stop rhyming, and we would all do well to listen with open ears. I’ve tried to limit my summary to areas of overlap between the early 1900s and the 2020s, but I’m not going to press the similarities too hard throughout the piece. You’re going to have to recognize them for yourself.</span></p><p>Transportation technology remade the west in a few short decades between the 1880s and 1910. A “bicycle craze” swept America in the 1890s. The Wright Brothers took flight in 1903. The first Model Ts rolled off Ford’s production lines in 1908. In Europe, cars quickly transformed the physical environment. The number of automobiles in France increased from about 3,000 in 1900 to 100,000 by 1914. That year, Ford's factory in Detroit produced and sold more than 300,000 Model Ts. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3gex!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg" width="1456" height="978" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:978,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3gex!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3gex!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3gex!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5698f33e-71a1-4f5b-83c2-5f87dc392f6a_2418x1625.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Model T, Wikimedia Commons</figcaption></figure></div><p><span>Speed was a physical experience, Blom writes, and cultural critics of the early 1900s were confident that it was unnatural for people to move so quickly through space—women, in particular. A woman on a bicycle was a thing to be feared. She signified a high-velocity freedom that was often associated with moral and sexual deviancy. Physicians warned that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/sc-178" rel="">"diseases of the wheel"</a><span> came by "the almost universal use of the bicycle" and that </span><a href="https://pessimistsarchive.org/list/bicycle/clippings/1897/m-sc-192-184" rel="">"serious evils"</a><span> might befall the youth who rode without restraint.  Moralists condemned women who “pedaled along gleefully, having discarded their corsets and put on more practical clothing, including trousers.” </span></p><p><span>Critics and novelists considered technological speed to be a vice, and they warned that our lust for celerity might turn into literal lust; that cars and bicycles would beckon us into carnal sin. In </span><em>Le surmale</em><span> (1902), the book’s hero wins a 100,000-mile bike race and then celebrates with an act of love-making that makes one character exclaim, “This is not a man, but a machine!” The idea that cars, planes, and bicycles were turning people into “machines” was most entertainingly summarized by a 1905 article in the journal </span><em>Je sais tout</em><span> (“I know all”), which calculated just how tall a human being would have to be to naturally walk at the pace that our new machines traveled. To equal the speed of a bicycle, for example, it was calculated that a person have to be more than 40 feet tall. Blom:</span></p><blockquote><p>Comparisons with other forms of transportation showed that in a fast train, a voyager would be effectively 51 meters tall, while the chauffeur of a racing car would almost dwarf Notre Dame Cathedral in Paris. Technology had created a new race of giants — in both senses of the term — and it changed the experience of space and time itself.</p></blockquote><p>“The growing speed of daily life, of news and work and play was a fetish of artists and industrialists alike,” Blom writes. “Never before had so much social change occurred so quickly.” As daily life sped up, people in the west started to break down.</p><p><span>Around the turn of the century, a nervous disorder first diagnosed in the U.S. gradually made its way across the Atlantic. The doctor George Miller Beard had called it “neurasthenia,” or nervous exhaustion. Europeans sometimes referred to it as “American Nervousness.” According to Beard, the affliction was most common among “the in-door classes of civilized countries” and the sufferers could be found “in nearly every brain-working household.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170457512" href="https://www.derekthompson.org/p/1910-the-year-the-modern-world-lost#footnote-3-170457512" target="_self" rel="">3</a></span><span> </span></p><p>As Blom points out, those afflicted tended to be white-collar workers working at the “frontiers of technology,” as “telephone operators, typesetters on new, faster machines, railway workers, engineers, [or] factory workers handling fast machines. One 1893 hospital survey of neurasthenia found that among nearly 600 cases, “there were almost 200 businessmen, 130 civil servants, 68 teachers, 56 students and eleven farmers.” Notably, no manual workers were counted at the clinic. Neurasthenia seemed to disproportionately affect white-collar workers, who were “overwhelmed” by their labor. “Overwork was a common theme in patients’ histories,” Blom writes.</p><p>It is tempting to write off this phenomenon as just another case of the “worried well.” But the scale of the west’s mental health distress in this period was striking. Blom: </p><blockquote><p><strong>In Germany, 40,375 patients were registered in mental hospitals in 1870. The number rose to 115,882 in 1900 and 220,881 in 1910</strong><span>. Over the same period, the proportion of patients admitted to general hospitals for illnesses of the nervous system rose from 44 to 60 percent. While these numbers include those suffering from many and varied mental conditions, not just neurasthenia, they do not include the huge number of sufferers who preferred going for cures or long stays in private sanatoriums, spas or other paramedical establishments in which a doctor would look after the guests — as in the one described by Thomas Mann in </span><em>The Magic Mountain</em><span>. </span></p></blockquote><p>“Artists were fascinated by this accelerated reality and its possibilities,” Blom writes. The novelists, painters, and musicians of the era could not stop talking about the changes they saw around them and their duty to use art to enter into a dialogue with those changes. Blom: </p><blockquote><p>Their view of things was shaped by reading about races in fast machines and in children’s magazines, by over-hearing adult whispers about nervous breakdowns and fast women … their imagination was alert to the fact that an age had ended and a new one — by turns a promise and a menace — was busting onto the scene, visible as yet only in flashes and fragmented visions. </p></blockquote><p>Blom deeply considers three artistic icons of the era: the composer Igor Stravinsky and the painters Vassily Kandinsky and Pablo Picasso. Each sought to make art that felt simultaneously cutting-edge and primal. Each responded to the modern age by reaching for inspiration in the past.</p><p><span>Blom begins with Stravinsky, whose famous orchestral work </span><em>The Rite of Spring</em><span> was inspired by ancient Russian dance rituals. A melange of old folk music and arresting dissonance, the piece’s first performance in Paris 1913 triggered one of the most infamously violent reactions of any concert-hall audience in history. As Blom puts it bluntly, “all hell broke loose”: </span></p><blockquote><p>“During the first two minutes the public remained quiet,' Monteux [a musician] later recalled, “then there were boos and hissing from the upper circle, soon after from the stalls. People sitting next to one another began to hit one another on the head with fists and walking sticks, or whatever else they had to hand. Soon, their anger was turned against the dancers and especially against the orchestra... Everything to hand was thrown at them, but we continued playing. The chaos was complete when members of the audience turned on one another, on anyone supporting the other side. A heavily bejewelled lady was seen slapping her neighbour before storming off, while another one spat in her detractor's face. Fights broke out everywhere and challenges to duels were issued.”</p></blockquote><p><span>Some music critics now consider The Rite of Spring </span><a href="https://www.pittsburgh-theater.com/shows/heinz-hall/pittsburgh-symphony-orchestra-the-rite-of-spring" rel="">“undoubtedly the most famous composition of the early 20th century.”</a></p><p>As classical music disintegrated in the concert halls, visual art was undergoing its own revolution, which may have been technological in origin. For thousands of years before the turn-of-the-century, the ability to perfectly represent nature been a rare skill possessed only by the most talented painters and drawers among us. But the Kodak camera (invented in 1888, with sales accelerating into the 1900s) turned the ability to capture realist images into a consumerist trifle. It cannot be a coincidence that the rise of abstract art coincided so perfectly with the proliferation of cheap camera technology that debased the value of perfect renderings of the natural world. </p><p>In the early 1900s, Vassily Kandinsky, one of the great pioneers of abstract art, pushed back against the mind-blurring speed of modernity. Kandinsky drew inspiration from the shamans of the Ural Mountains and the sound of their drums, according to Blom, and his abstraction sought to capture their primary music in images. By turning sound into image, Kandinsky’s art sought to achieve an act of synesthesia that no Kodak machine could ever match. Other art historians are less certain about what inspired Kandinsky’s first abstract watercolors, which he painted around 1910. All that is certain is that paintings like this one reject any effort to depict the natural world as it might be seen through a retina or camera lens. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!EFhk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg" width="1456" height="1151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1151,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" title="Untitled First Abstract Watercolor, 1910–1913, Centre Pompidou, Paris[19]" srcset="https://substackcdn.com/image/fetch/$s_!EFhk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 424w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 848w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!EFhk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ad0d5b2-3932-43ea-b9a1-079d91b0f6ae_1592x1259.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Untitled </span><em>First Abstract Watercolor</em><span>, 1910–1913</span></figcaption></figure></div><p>Kandinsky is one of my favorite artists. But the critical response to the dawn of abstract painting was about as brutal as it gets. One German review that Blom cites includes the following passage:</p><blockquote><p>Looked at as painting they are the end of art, a prank. But they show a more nefarious side. The modern phrase that the object of art is indifferent, if abused here in a truly malevolent way... What is presented to us breathes the poison breath of the darkest places of vice of the big city and shows the constitution of the artists, which can only be understood in terms of pathology.</p></blockquote><p><span>Around the same time that Kandinsky was putting his mark on abstraction, Pablo Picasso was pioneering his own rejection of purely representative art, with primitivism. Drawing inspiration from African masks and carvings from West Africa, Picasso’s art “did everything to hide its underlying technical and compositional virtuosity,” Blom writes. Picasso’s 1907 classic </span><em>Les Demoiselles d'Avignon</em><span> is “a large canvas of brutal and disturbing bluntness.” While Picasso was indifferent to the actual “significance and symbolism” of the African styles he drew on, Blom writes, critics have said his aim was to represent the “unchanging structure of the human condition” in the face of civilizational change.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!9OXU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg" width="1280" height="1326" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1326,&quot;width&quot;:1280,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;undefined&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="undefined" title="undefined" srcset="https://substackcdn.com/image/fetch/$s_!9OXU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 424w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 848w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!9OXU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff78d9057-3f4d-4474-aaa8-9976a71a337a_1280x1326.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Independent of one another, Stravinsky, Kandinsky, and Picasso each reacted to the modern world and “the alienation of the human mind from its own emotions” by pulling pre-modern styles and atavistic images into their art. What we call Modernism today was in most cases a reaction to modernity. It was an effort to excavate something ancient and honest about humanity in an age obsessed with and overrun by novelty.</p><p>Blom closes his chapter “1910: Human Nature Changed” by considering two intellectual giants of the time: the sociologist Max Weber and the psychoanalyst Sigmund Freud, whose International Psychoanalytic Association was founded in 1910. The tension between their theories of human nature are profoundly relevant today.</p><p><span>In his famous work </span><em>The Protestant Ethic and the Spirit of Capitalism</em><span>, Weber, a German sociologist, argued that certain Protestant—especially Calvinist—traditions supported habits that aligned with the development of modern capitalism. He argued that the Protestant tradition of northern European worshippers cultivated a disciplined approach to work, savings, and investment that proved valuable in commerce, while the Calvinist doctrine of divine grace “could lead believers to read worldly success as a possible sign of God’s favor,” as Blom summarizes. Weber believed that Protestantism not only encouraged followers to pour their energies into labor (hence the allusion to </span><em>Work Ethic</em><span> in the book’s title) but also helped create a culture of trade and investment that supported the rise of modern capitalism.</span></p><p><span>“It is easy to see how Freud’s analysis follows on from Weber’s,” Blom writes. To Freud, human nature was at risk of being fully dissolved by capitalism and modern society, like chalk dropped in acid. Beneath the polite masks demanded by modern society, he said, there lurked a more atavistic and instinctual self. Freud saw our psyche as a tug-of-war between the id (our animal urges) and superego (the voice in our head that internalizes society’s rules), with the ego stuck in the middle trying to negotiate an authentic identity in the face of mass inauthenticity. One of Freud’s most fantastic insights was that some people can channel or redirect their most raw and unacceptable urges toward productive and acceptable work. His name for this bit of psychological alchemy was </span><em>sublimation</em><span>. </span></p><p>Modern capitalism, in Freudian terms, was the sublimation of self-interest—or, one might even say, the sublimation of greed. “The suppression of natural urges is a necessary precondition for capitalist success,” Blom writes in summary, “but while it is productive for the group and its wealth, such an approach will eventually exact its revenge on the individual.” By this interpretation, the mass anxiety of the early 1900s—whether you call it neurasthenia, American Nervousness, or Newyorkitis—was price of modernity, technological development, and even capitalism itself.</p><p><span>There is little evidence that Freud and Weber ever debated one another. Yet when you set their theories side by side, it’s hard not to hear a conversation that still shapes much modern commentary. Weber wrote that modern capitalism evolved from religious doctrines that fit our nature, while Freud argued that human nature is unfit for a modern world that distorts and represses our basic urges. </span><em>Are our most impressive inventions the ultimate expression of our humanity, or are they the ultimate threat to it? </em><span>This is the question that every generation must answer for itself, including our own. It is a question equally worthy of the automobile and artificial intelligence. The troubling answer—for Weber and for Freud; for 1910 and for 2025—is: perhaps, both.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Million Screenshots (106 pts)]]></title>
            <link>https://onemillionscreenshots.com/?q=random</link>
            <guid>44858067</guid>
            <pubDate>Sun, 10 Aug 2025 20:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemillionscreenshots.com/?q=random">https://onemillionscreenshots.com/?q=random</a>, See on <a href="https://news.ycombinator.com/item?id=44858067">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://onemillionscreenshots.com/"><h2><span>One</span><span>Million</span><span>Screenshots</span></h2></a><p>Zoom into the web's top homepages</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bolt – A super-fast, statically-typed scripting language written in C (130 pts)]]></title>
            <link>https://github.com/Beariish/bolt</link>
            <guid>44856935</guid>
            <pubDate>Sun, 10 Aug 2025 17:53:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Beariish/bolt">https://github.com/Beariish/bolt</a>, See on <a href="https://news.ycombinator.com/item?id=44856935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">⚡ Bolt</h2><a id="user-content--bolt" aria-label="Permalink: ⚡ Bolt" href="#-bolt"></a></p>
<p dir="auto">A <em>lightweight</em>, <strong>lightning-fast</strong>, type-safe embeddable language for real-time applications.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import print, error, Error from core
import abs, epsilon from math

// The return type of safe_divide is inferred to be `Error | number`
fn safe_divide(a: number, b: number) {
    if abs(b) < epsilon {
        return error(&quot;Cannot divide by zero!&quot;)
    }

    return a / b
}

match let result = safe_divide(10, 5) {
    is Error {
        // The type of result is narrowed in this branch!
        print(&quot;Failed to divide:&quot;, result.what)
    }

    is number {
        print(&quot;The answer is&quot;, result)
    }
}"><pre><span>import</span> <span>print</span><span>,</span> <span>error</span><span>,</span> <span>Error</span> <span>from</span> <span>core</span>
<span>import</span> <span>abs</span><span>,</span> <span>epsilon</span> <span>from</span> <span>math</span>

<span>// The return type of safe_divide is inferred to be `Error | number`</span>
<span>fn</span> <span>safe_divide</span><span>(</span><span>a</span>: <span>number</span><span>,</span> <span>b</span>: <span>number</span><span>)</span><span></span> <span>{</span>
    <span>if</span> <span>abs</span><span>(</span><span>b</span><span>)</span> <span>&lt;</span> <span>epsilon</span> <span>{</span>
        <span>return</span> <span>error</span><span>(</span><span>"Cannot divide by zero!"</span><span>)</span>
    <span>}</span>

    <span>return</span> <span>a</span> <span>/</span> <span>b</span>
<span>}</span>

<span>match</span> <span>let</span> <span>result</span> <span>=</span> <span>safe_divide</span><span>(</span><span>10</span><span>,</span> <span>5</span><span>)</span><span></span> <span>{</span>
    <span>is</span> <span>Error</span> <span>{</span>
        <span>// The type of result is narrowed in this branch!</span>
        <span>print</span><span>(</span><span>"Failed to divide:"</span><span>,</span> <span>result</span><span>.</span><span>what</span><span>)</span>
    <span>}</span>

    <span>is</span> <span>number</span> <span>{</span>
        <span>print</span><span>(</span><span>"The answer is"</span><span>,</span> <span>result</span><span>)</span>
    <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Performance.md">Lightning-fast performance</a>, outperforming other languages in its class</li>
<li>Compact implementation, leaving a minimal impact on build size while remaining consise enough to browse.</li>
<li>Blazingly quick compilation, plow through code at over 500kloc/thread/second. That's 50'000 lines in the blink of an eye.</li>
<li>Ease of embedding, only a handful of lines to get going</li>
<li>Rich type system to catch errors before code is ran, with plenty of support for extending it from native code</li>
<li>Embed-first design, prioritizing inter-language performance and agility</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Programming%20Guide.md">Bolt programming guide</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Standard%20Library">Bolt standard library reference</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Embedding%20Guide.md">Bolt embedding and API reference</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Performance.md">Bolt performance</a></strong></li>
<li><strong><a href="https://github.com/Beariish/bolt/blob/main/doc/Bolt%20Users.md">Notable Bolt users</a></strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Bolt only depends on the C standard library as well as <code>libm</code> on Unix-based systems.
Some standard library modules include things like file and system IO, but these can be disabled easily.
By default, Bolt sets up an environment that uses <code>malloc</code>/<code>realloc</code>/<code>free</code>, but this is also easy to configure.
Bolt also embeds my other library <a href="https://github.com/Beariish/picomatch">picomatch</a> for regex parsing</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimal embedding example</h2><a id="user-content-minimal-embedding-example" aria-label="Permalink: Minimal embedding example" href="#minimal-embedding-example"></a></p>
<p dir="auto">The <a href="https://github.com/Beariish/bolt/blob/main/bolt-cli/main.c">bolt-cli</a> program provides a very consice example of how to embed bolt an an application, see the <a href="https://github.com/Beariish/bolt/tree/main/doc/Bolt%20Embedding%20Guide.md">Bolt embedding guide</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Language examples</h2><a id="user-content-language-examples" aria-label="Permalink: Language examples" href="#language-examples"></a></p>
<p dir="auto">The <a href="https://github.com/Beariish/bolt/tree/main/examples">examples</a> folder contains a few short examples of ideomatically written bolt code. Check out the <a href="https://github.com/Beariish/bolt/tree/main/tests">tests</a> and <a href="https://github.com/Beariish/bolt/tree/main/benchmarks">benchmarks</a> folders as wel for some more in-depth language overview.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Bolt currently only builds on x64. 32-bit architectures are explicitly not supported, arm and riscv are untested.
Running <code>cmake</code> in the root directory of the project will generate a static library for the language, as well as the CLI tool.
For more information and options regarding embedding Bolt in your application, see <code>bt_config.h</code>.
See below for the status of Bolt on each relevant compiler.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compiler Status</h2><a id="user-content-compiler-status" aria-label="Permalink: Compiler Status" href="#compiler-status"></a></p>
<p dir="auto">Please note that Bolt is <strong>not</strong> yet stable, expect to encounter compiler bugs and crashes. If you do, opening an issue with replicable Bolt code would be much appreciated 😊</p>
<p dir="auto"><a href="https://github.com/Beariish/bolt/actions/workflows/cmake-multi-platform.yml"><img src="https://github.com/Beariish/bolt/actions/workflows/cmake-multi-platform.yml/badge.svg" alt="Build Status"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Compiler</th>
<th>Status</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSVC</td>
<td>✅</td>
<td>no issues</td>
</tr>
<tr>
<td>GCC</td>
<td>✅🟨</td>
<td>all functional, some warnings</td>
</tr>
<tr>
<td>Clang</td>
<td>✅🟨</td>
<td>all functional, some warnings</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Bolt is a very opinionated project, and any contributions should take the vision into account.</p>
<p dir="auto">Bugfixes are likely to be accepted as long as they're within reason and don't change any expected behaviour. Adding tests in case of regression is very much appreciated as well. A clean run of <code>/tests/all</code> is expected of course.</p>
<p dir="auto">Optimizations may also be accepted for minor versions under similar criteria. A before/after run of <code>/benchmarks/all</code> is expected to evaluate the impact and make sure nothing else regresses. If the specific optimization isn't captured in any existing benchmark, adding one is required.</p>
<p dir="auto">Feature additions will need a lot of consideration, Bolt is very intentionally minimal in its' design and featureset. I highly suggest you submit some kind of proposal or plan before starting any significant work on a feature to review. Use cases, performance, and implementation cost will all be expected to be justified.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Bolt is licensed under MIT. See LICENSE for more information.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fight Chat Control (757 pts)]]></title>
            <link>https://fightchatcontrol.eu/</link>
            <guid>44856426</guid>
            <pubDate>Sun, 10 Aug 2025 16:50:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fightchatcontrol.eu/">https://fightchatcontrol.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=44856426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!-- Hero Section -->
        <div>
                    <h2>
                        The EU (still) wants to scan <br> your private messages and photos
                    </h2>
                    <p>
                        The "Chat Control" proposal would mandate scanning of <strong>all</strong> private digital communications,
                        including encrypted messages and photos. This threatens <strong>fundamental privacy rights</strong> and digital security
                        for all EU citizens.
                    </p>
                    <div>
                        
                        <div>
                            <h3 id="support-count">15</h3>
                            <p>Member States Supporting</p>
                        </div>
                        <div>
                            <h3 id="undecided-count">9</h3>
                            <p>Member States Undecided</p>
                        </div>
                    </div>
                    
                </div>

        <!-- Overview Section -->
        <div id="overview">
                <h2>You Will Be Impacted</h2>
                <p>
                    Every photo, every message, every file you send will be automatically scanned—without your consent or suspicion. This is not about catching criminals. It is mass surveillance imposed on all 450 million citizens of the European Union.
                </p>
                <div>
                    <div>
                        <p>📱</p>
                        <h3>Mass Surveillance</h3>
                        <p>Every private message, photo, and file scanned automatically: no suspicion required, no exceptions*, even encrypted communications.</p>
                    </div>
                    <div>
                        <p>🔓️</p>
                        <h3>Breaking Encryption</h3>
                        <p>Weakening or breaking end-to-end encryption exposes everyone’s communications—including sensitive financial, medical, and private data—to hackers, criminals, and hostile actors.</p>
                    </div>
                    <div>
                        <p>⚖️</p>
                        <h3>Fundamental Rights</h3>
                        <p>Undermines your fundamental rights to privacy and data protection, as guaranteed by Articles 7 and 8 of the EU Charter—rights considered core to European democratic values.</p>
                    </div>
                    <div>
                        <p>🎯</p>
                        <h3>False Positives</h3>
                        <p>Automated scanners routinely misidentify innocent content, such as vacation photos or private jokes, as illegal, putting ordinary people at risk of false accusations and damaging investigations.</p>
                    </div>
                    <div>
                        <p>👨‍👩‍👧‍👦</p>
                        <h3>Ineffective Child Protection</h3>
                        <p>Child protection experts and organisations, including the UN, warn that mass surveillance fails to prevent abuse and actually makes children less safe—by weakening security for everyone and diverting resources from proven protective measures.</p>
                    </div>
                    <div>
                        <p>🌍</p>
                        <h3>Global Precedent</h3>
                        <p>Creates a dangerous global precedent enabling authoritarian governments, citing EU policy, to roll out intrusive surveillance at home, undermining privacy and free expression worldwide.</p>
                    </div>
                </div>
                
                <p>
                    *EU politicians exempt themselves from this surveillance under "professional secrecy" rules.
                    They get privacy.<br> You and your family do not. Demand fairness.
                </p>
            </div>

        <!-- Member States Section -->
        <div id="member-states">
                <h2>Member State Positions</h2>
                <br>
                
                
            </div>

        <!-- Delegates Section -->
        <div id="delegates">
                <h2>Find Your Representatives</h2>
                <br>
                
                
            </div>

        <!-- Contact Tool Section -->
        <div id="contact-tool">
                <h2>Take Action!<br> Contact Your MEPs</h2>
                <h3>
                    Your privacy and freedoms are at risk.
                    These policies will impact every European—your messages, photos, and private conversations will be scanned without your consent.
                    But we have the power to stop this.
                    Contact your MEPs now with a clear message: NO to mass surveillance.
                    Your voice matters. Make it heard today.
                </h3>
                
            </div>

        <!-- Timeline Section -->
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion language models are super data learners (133 pts)]]></title>
            <link>https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac</link>
            <guid>44856101</guid>
            <pubDate>Sun, 10 Aug 2025 16:04:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac">https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac</a>, See on <a href="https://news.ycombinator.com/item?id=44856101">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AOL closes its dial up internet service (149 pts)]]></title>
            <link>https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</link>
            <guid>44856090</guid>
            <pubDate>Sun, 10 Aug 2025 16:02:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html">https://www.ispreview.co.uk/index.php/2025/08/after-34-years-aol-finally-closes-its-dial-up-internet-service.html</a>, See on <a href="https://news.ycombinator.com/item?id=44856090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<div id="news_thumbpicture"><picture id="primaryimage"><img width="600" height="600" src="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp" alt="aol uk homepage from the past" decoding="async" fetchpriority="high" srcset="https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-600x600.webp 600w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-300x300.webp 300w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-768x768.webp 768w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657-150x150.webp 150w, https://www.ispreview.co.uk/wp-content/uploads/2025/08/nggallery_import/aol-uk-homepage-from-the-past-e1754805650657.webp 1000w" sizes="(max-width: 600px) 100vw, 600px"></picture></div><!-- Article Start --><p>In a somewhat surprising development, mainly because almost everybody assumed it had died a long time ago, <a href="https://www.aol.co.uk/" target="_blank" rel="noopener">AOL</a> (America Online) – one of the very first consumer ISPs in both the USA and UK – recently caused a stir again by announcing that it had “<em>decided to discontinue Dial-up Internet</em>” on 30th September 2025.<span id="more-42747"></span></p>
<p>According to <a href="https://help.aol.com/articles/dial-up-internet-to-be-discontinued" target="_blank" rel="noopener">AOL’s website</a>: “<em>AOL routinely evaluates its products and services and has decided to discontinue Dial-up Internet. This service will no longer be available in AOL plans. As a result, on September 30, 2025 this service and the associated software, the AOL Dialer software and AOL Shield browser, which are optimized for older operating systems and dial-up internet connections, will be discontinued</em>.” But their email service will continue.</p>
<p><strong>NOTE:</strong> Many <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> ISPs in the UK during 1995 – like AOL – used expensive premium rate numbers, although this did soon gravitate to local call rates and then unmetered via FRIACO. The v90 <a href="https://www.ispreview.co.uk/index.php/link/dialup" target="_blank" rel="" title="dialup" data-chref="https://www.ispreview.co.uk/broadband_dialup.php">dialup</a> standard (56Kbps capable or 0.056Mbps) didn’t arrive until 1998 and by then <a href="https://www.ispreview.co.uk/index.php/link/adsl" target="_blank" rel="" title="digital subscriber line" data-chref="https://www.ispreview.co.uk/broadband_DSL.php">ADSL</a> and cable broadband were just around the corner – ready to revolutionise the market.</p>
<p>The change appears to have been announced within the past few weeks, although it wasn’t picked up more widely until journalist <a href="https://tedium.co/" target="_blank" rel="noopener">Ernie Smith</a> noted it in a post on <a href="https://bsky.app/profile/ernie.tedium.co/post/3lvwjugziec2f" target="_blank" rel="noopener">Bluesky</a>. Just to be clear, the announcement above refers to the USA and Canada. However, we’re fairly confident that what remains of AOL UK (aka – <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a>) doesn’t have any legacy dial-up customers left, although we would have said the same about the USA and Canada too, until that announcement dropped (dial-up speeds in 2025 would be practically unusable). ISPreview is currently checking, just to be sure.</p>

<p>In case anybody has forgotten. The original AOL UK experience was somewhat of a walled-garden way of accessing the internet, which forced you to use the company’s own software and restricted your ability to access certain internet services. This had the benefit of simplifying the experience, but AOL later fell behind the curve and ended up being overtaken by rivals.</p>
<p>The <a title="carphone warehouse" href="https://www.ispreview.co.uk/index.php/go/cpw" target="_blank" rel="nofollow">Carphone Warehouse</a> (CPW) ultimately won the auction to buy AOL UK’s Internet access business in 2006 for £370m (note: AOL’s content division became a separate business). At the time, AOL were the UK’s third-largest ISP with around <strong>2.1 million customers</strong> (600,000 on dial-up and 1.5 million with broadband) and were later re-branded to AOL Broadband.</p>
<p>A second big change occurred on 29th March 2010, when CPW and <a href="https://www.ispreview.co.uk/index.php/go/tt" target="_blank" rel="nofollow" title="talktalk">TalkTalk</a> separated (demerged) – the latter became a separate business, which included customers from CPW’s prior acquisitions (e.g. AOL Broadband, Tiscali etc.). Several more years passed until May 2014, when TalkTalk confirmed that AOL Broadband (formerly AOL UK) had stopped taking on new internet and phone customers (<a href="https://www.ispreview.co.uk/index.php/2014/05/uk-isp-aol-broadband-longer-available-new-customers.html">here</a>), although no mention was made of the dial-up base.</p>
<p>We’re certain that plenty of our readers (those now of a certain age group) will have stories to share of the early AOL UK days. Yours truly only used the original service briefly, before promptly switching away as the UK’s then dialup (narrowband) internet market became more competitive, affordable and less restrictive. It’s a service I was glad to forget, but it played an important role.</p>

<p><iframe title="AOL (Sign On - Dial Up)" width="500" height="281" src="https://www.youtube.com/embed/D1UY7eDRXrs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<!-- CONTENT END 1 -->
<!-- Article End -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig's Lovely Syntax (194 pts)]]></title>
            <link>https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</link>
            <guid>44855881</guid>
            <pubDate>Sun, 10 Aug 2025 15:33:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html">https://matklad.github.io/2025/08/09/zigs-lovely-syntax.html</a>, See on <a href="https://news.ycombinator.com/item?id=44855881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Zig’s Lovely Syntax <time datetime="2025-08-09">Aug 9, 2025</time>
        </h2>
        <p>
          It’s a bit of a silly post, because syntax is the least interesting
          detail about the language, but, still, I can’t stop thinking how Zig
          gets this detail just right for the class of curly-braced languages,
          and, well, now you’ll have to think about that too.
        </p>
        <p>
          On the first glance, Zig looks almost exactly like Rust, because Zig
          borrows from Rust liberally. And I think that Rust has great syntax,
          considering all the semantics it needs to express (see
          <a href="https://matklad.github.io/2023/01/26/rusts-ugly-syntax.html">“Rust’s Ugly Syntax”</a>). But Zig improves on that, mostly by
          leveraging simpler language semantics, but also through some purely
          syntactical tasteful decisions.
        </p>
        <section id="Integer-Literals">
          <h2>
            <a href="#Integer-Literals">Integer Literals </a>
          </h2>
          <p>
            How do you spell a number ninety-two? Easy, <code>92</code>. But
            what type is that? Statically-typed languages often come with
            several flavors of integers: <code>u32</code>, <code>u64</code>,
            <code>u8</code>. And there’s often a syntax for literals of a
            particular types: <code>92u8</code>, <code>92l</code>, <code>92z</code>.
          </p>
          <p>
            Zig doesn’t have suffixes, because, in Zig, all integer literals
            have the same type: <code>comptime_int</code>:
          </p>

          <figure>
            <pre><code><span><span>const</span> an_integer = <span>92</span>;</span>
<span>assert(<span>@TypeOf</span>(an_integer) <span>==</span> <span>comptime_int</span>);</span></code></pre>
          </figure>
          <p>
            The value of an integer literal is known at compile time and is
            coerced to a specific type on assignment
            <span><code>const x: i32 = 92;</code></span>
            or ascription:
            <span><code>@as(i32, 92)</code></span>
          </p>
          <p>
            To emphasize, this is <em>not</em> type inference, this is implicit
            comptime coercion. This does mean that code like
            <span><code>var x = 92;</code></span>
            generally doesn’t work, and requires an explicit type.
          </p>
        </section>
        <section id="String-Literals">
          <h2>
            <a href="#String-Literals">String Literals </a>
          </h2>
          <p>Raw or multiline strings are spelled like this:</p>

          <figure>
            <pre><code><span><span>const</span> raw =</span>
<span>    <span>\\Roses are red</span></span>
<span>    <span>\\  Violets are blue,</span></span>
<span>    <span>\\Sugar is sweet</span></span>
<span>    <span>\\  And so are you.</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            This syntax doesn’t require a special form for escaping <code>\\</code> itself:
          </p>

          <figure>
            <pre><code><span><span>const</span> still_raw =</span>
<span>    <span>\\const raw =</span></span>
<span>    <span>\\    <span>\\</span>Roses are red</span></span>
<span>    <span>\\    <span>\\</span>  Violets are blue,</span></span>
<span>    <span>\\    <span>\\</span>Sugar is sweet</span></span>
<span>    <span>\\    <span>\\</span>  And so are you.</span></span>
<span>    <span>\\    <span>\\</span></span></span>
<span>    <span>\\;</span></span>
<span>    <span>\\</span></span>
<span>;</span></code></pre>
          </figure>
          <p>
            It nicely dodges indentation problems that plague every other
            language with a similar feature. And, the best thing ever:
            lexically, each line is a separate token. As Zig has only
            line-comments, this means that <code>\n</code> is <em>always</em>
            whitespace. Unlike most other languages, Zig can be correctly lexed
            in a line-by-line manner.
          </p>
          <p>
            Raw strings is perhaps the biggest improvement of Zig over Rust.
            Rust brute-forces the problem with
            <code>r##""##</code> syntax, which does the required job,
            technically, but suffers from the mentioned problems: indentation is
            messy, nesting quotes requires adjusting hashes, unclosed raw
            literal breaks the following lexical structure completely, and
            rustfmt’s formatting of raw strings tends to be rather ugly. On the
            plus side, this syntax at least cannot be expressed by a
            context-free grammar!
          </p>
        </section>
        <section id="Record-Literals">
          <h2>
            <a href="#Record-Literals">Record Literals </a>
          </h2>
          <p>For the record, Zig takes C syntax (not that C would notice):</p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The <code>.{</code> feels weird! It will make sense by the end of
            the post. Here, I want only to note <code>.x = 1</code>
            part, which matches the assignment syntax <code>obj.x = 1</code>.
            This is great! This means that grepping for
            <code>".x ="</code> gives you <em>all</em> instances where a field
            is written to. This is hugely valuable: most of usages are reads,
            but, to understand the flow of data, you only need to consider
            writes. Ability to mechanically partition the entire set of usages
            into majority of boring reads and a few interesting writes does
            wonders for code comprehension.
          </p>
        </section>
        <section id="Prefix-Types">
          <h2>
            <a href="#Prefix-Types">Prefix Types </a>
          </h2>
          <p>
            Where Zig departs from C the most is the syntax for types. C uses a
            needlessly confusing spiral rule. In Zig, all types are prefix:
          </p>

          <figure>
            <pre><code><span><span>u32</span>      <span>// An integer</span></span>
<span>[<span>3</span>]<span>u32</span>   <span>// An array of three integers</span></span>
<span>?[<span>3</span>]<span>u32</span>  <span>// An array of three integers or null</span></span>
<span></span>
<span><span>// A pointer to...</span></span>
<span><span>*</span><span>const</span> ?[<span>3</span>]<span>u32</span></span></code></pre>
          </figure>
          <p>
            While pointer type is prefix, pointer dereference is postfix, which
            is a more natural subject-verb order to read: <span><code>ptr.* = 92;</code></span>
          </p>
        </section>
        <section id="Identifiers">
          <h2>
            <a href="#Identifiers">Identifiers </a>
          </h2>
          <p>
            Zig has general syntax for “raw” identifiers:
            <span><code>@"a name which a space"</code></span>
            It is useful to avoid collisions with keywords, or for exporting a
            symbol whose name is otherwise not a valid Zig identifier. It is a
            bit more to type than Kotlin’s delightful
            <span><code>`a name with a space`</code>,</span> but
            manages to re-use Zig’s syntax for built-ins (<code>@TypeOf</code>)
            and strings.
          </p>
        </section>
        <section id="Functions">
          <h2>
            <a href="#Functions">Functions </a>
          </h2>
          <p>
            Like, Rust, Zig goes for <code>fn foo</code> function declaration
            syntax. This is such a massive improvement over C/Java style
            function declarations: it puts <code>fn</code> token (which is
            completely absent in traditional C family) and function name next to
            each other, which means that textual search for <code>fn name</code>
            allows you to quickly find the function. Then Zig adds a little
            twist. While in Rust we write
          </p>

          <figure>
            <pre><code><span><span>fn</span> <span>add</span>(x: <span>i32</span>, <span>i32</span>) <span>-&gt;</span> <span>i32</span></span></code></pre>
          </figure>
          <p>Zig is</p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, <span>i32</span>) <span>i32</span></span></code></pre>
          </figure>
          <p>
            The arrow is gone! Now that I’ve used this for some time, I find
            arrow very annoying to type, and adding to the visual noise. Rust
            needs the arrow: Rust has lambdas with an inferred return type, and,
            in a lambda, the return type is optional. So you need some sort of
            an explicit syntax to tell the parser if there is return type:
          </p>

          <figure>
            <pre><code><span>|| expression;</span>
<span>|| <span>-&gt;</span> Type { }</span></code></pre>
          </figure>
          <p>
            And its understandable that lambdas and functions would want to use
            compatible syntax. But Zig doesn’t have lambdas, so it just makes
            the type mandatory. So the main is
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {}</span></code></pre>
          </figure>
          <p>
            Related small thing, but, as name of the type, I think I like <code>void</code> more than <code>()</code>.
          </p>
        </section>
        <section id="Locals">
          <h2>
            <a href="#Locals">Locals </a>
          </h2>
          <p>
            Zig is using <code>const</code> and <code>var</code> for binding
            values to names:
          </p>

          <figure>
            <pre><code><span><span>const</span> mid = lo <span>+</span> <span>@divFloor</span>(hi <span>-</span> lo, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            This is ok, a bit weird after Rust’s, whose <code>const</code> would
            be <code>comptime</code> in Zig, but not really noticeable after
            some months. I do think this particular part is not great, because
            <code>const</code>, the more frequent one, is longer. I think Kotlin
            nails it: <code>val</code>, <code>var</code>, <code>fun</code>. Note
            all three are monosyllable, unlike <code>const</code> and <code>fn</code>! Number of syllables matters more than the number of
            letters!
          </p>
          <p>Like Rust, Zig uses</p>

          <figure>
            <pre><code><span><span>'name'</span> (<span>':'</span> Type)?</span></code></pre>
          </figure>
          <p>syntax for ascribing types, which is better than</p>

          <figure>
            <pre><code><span>Type <span>'name'</span></span></code></pre>
          </figure>
          <p>
            because optional suffixes are easier to parse visually and
            mechanically than optional prefixes.
          </p>
        </section>
        <section id="Conjunction-Is-Control-Flow">
          <h2>
            <a href="#Conjunction-Is-Control-Flow">Conjunction Is Control Flow
            </a>
          </h2>
          <p>
            Zig doesn’t use <code>&amp;&amp;</code> and <code>||</code> and
            spells the relevant operators as <code>and</code> and <code>or</code>:
          </p>

          <figure>
            <pre><code><span><span>while</span> (count &gt; <span>0</span> <span>and</span> ascii.isWhitespace(buffer[count <span>-</span> <span>1</span>])) {</span></code></pre>
          </figure>
          <p>
            This is easier to type and much easier to read, but there’s also a
            deeper reason why they are not sigils. Zig marks any control flow
            with a keyword. And, because boolean operators short-circuit, they
            <em>are</em> control flow! Treating them as normal binary operator
            leads to an entirely incorrect mental model. For bitwise operations,
            Zig of course uses <code>&amp;</code> and <code>|</code>.
          </p>
        </section>
        <section id="Explicit-return">
          <h2>
            <a href="#Explicit-return">Explicit return </a>
          </h2>
          <p>
            Both Zig and Rust have statements and expressions. Zig is a bit more
            statement oriented, and requires explicit returns:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> add</span>(x: <span>i32</span>, y: <span>i32</span>) <span>i32</span> {</span>
<span>  <span>return</span> x <span>+</span> y;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            Furthermore, because there are no lambdas, scope of return is always
            clear.
          </p>
          <p>
            Relatedly, the value of a block expression is void. A block is a
            list of statements, and doesn’t have an optional expression at the
            end. This removes the semicolon problem — while Rust rules around
            semicolons are sufficiently clear (until you get to macros), there’s
            some constant mental overhead to getting them right all the time.
            Zig is more uniform and mechanical here.
          </p>
          <p>
            If you need a block that yields a value, Zig supports a general
            syntax for breaking out of a labeled block:
          </p>

          <figure>
            <pre><code><span><span>const</span> header_oldest = blk: {</span>
<span>    <span>var</span> oldest: ?<span>usize</span> = <span>null</span>;</span>
<span>    <span>for</span> (headers.slice, <span>0</span>..) <span>|</span><span>*</span>header, i<span>|</span> {</span>
<span>        <span>switch</span> (Headers.dvc_header_type(header)) {</span>
<span>            .blank =&gt; assert(i &gt; <span>0</span>),</span>
<span>            .valid =&gt; oldest = i,</span>
<span>        }</span>
<span>    }</span>
<span>    <span>break</span> :blk <span>&amp;</span>headers.slice[oldest.?];</span>
<span>};</span></code></pre>
          </figure>
        </section>
        <section id="If">
          <h2>
            <a href="#If">If </a>
          </h2>
          <p>
            Rust makes pedantically correct choice regarding <code>if</code>s:
            braces are mandatory:
          </p>

          <figure>
            <pre><code><span><span>if</span> cond1 {</span>
<span>  case_a</span>
<span>} <span>else</span> {</span>
<span>  <span>if</span> cond2 {</span>
<span>    case_b</span>
<span>  } <span>else</span> {</span>
<span>    case_c</span>
<span>  }</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This removes the dreaded “dangling else” grammatical ambiguity.
            While theoretically nice, it makes
            <code>if</code>-expression one-line feel too heavy. It’s not the
            braces, it’s the whitespace around them:
          </p>

          <figure>
            <pre><code><span>if (a) b else c</span>
<span>if a { b } else { c }</span></code></pre>
          </figure>
          <p>
            But the ternary is important! Exploding a simple choice into
            multi-line condition <em>hurts</em>
            readability. Zig goes with traditional choice of making parentheses
            required and braces optional:
          </p>

          <figure>
            <pre><code><span>  .direction = <span>if</span> (prng.boolean()) .ascending <span>else</span> .descending,</span></code></pre>
          </figure>
          <p>
            By itself, this does create a risk of <code>goto: fail;</code> style
            bugs. But in Zig formatter (non-configurable, user-directed) is a
            part of the compiler, and formatting errors that can mask bugs are
            caught during compilation. For example, <code>1 -2</code> is an
            error due to inconsistent whitespace around the minus sign, which
            signals a plausible mixup of infix and binary minus. No such errors
            are currently produced for incorrect indentation (the value add
            there is relatively little, given <code>zig fmt</code>), but this is
            planned.
          </p>
          <p>
            NB: because Rust requires <code>if</code> branches to be blocks, it
            is forced to make <code>{ expr }</code> synonym with
            <code>(expr)</code>. Otherwise, the ternary <code>if</code> would be
            even more unusable! Syntax design is tricky! Whether you need <code>return</code>s and whether you make <code>()</code> or <code>{}</code> mandatory in ifs are not orthogonal!
          </p>
        </section>
        <section id="Loops">
          <h2>
            <a href="#Loops">Loops </a>
          </h2>
          <p>
            Like Python, Zig allows <code>else</code> on loops. Unlike Python,
            loops are expressions, which leads to a nicely readable imperative
            searches:
          </p>

          <figure>
            <pre><code><span><span>pub</span> <span>const</span> Word = <span>for</span> (.{ <span>u8</span>, <span>u16</span>, <span>u32</span>, <span>u64</span>, <span>u128</span>, <span>u256</span> }) <span>|</span>W<span>|</span> {</span>
<span>    <span>if</span> (<span>@bitSizeOf</span>(W) &gt;= bitset_capacity) <span>break</span> W;</span>
<span>} <span>else</span> <span>unreachable</span>;</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have syntactically-infinite loop like Rust’s <code>loop
              {</code> or Go’s <code>for {</code>. Normally I’d consider that a
            drawback, because these loops produce different control flow,
            affecting reachability analysis in the compiler, and I don’t think
            it’s great to make reachability dependent on condition being visibly
            constant. But! As Zig places <code>comptime</code> semantics front
            and center, and the rules for what is and isn’t a comptime constant
            are a backbone of every feature, “anything equivalent to
            <code>while (true)</code>” becomes sufficiently precise.
            Incidentally, these days I tend to write “infinite” loops as
          </p>

          <figure>
            <pre><code><span><span>for</span> (<span>0</span>..safety_bound) <span>|</span>_<span>|</span> {</span>
<span></span>
<span>} <span>else</span> <span>@panic</span>(<span>"loop safety counter exceeded"</span>);</span></code></pre>
          </figure>
          <p>
            Almost always there is an up-front bound for the number of
            iterations until the break, and its worth asserting this bound,
            because debugging crashes is easier than debugging hangs.
          </p>
          <p>
            <code>for</code>, <code>while</code>, <code>if</code>, <code>switch</code>, and <code>catch</code> all use the same Ruby/Rust
            inspired syntax for naming captured values:
          </p>

          <figure>
            <pre><code><span><span>for</span> (slice) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span>
<span></span>
<span><span>while</span> (iterator.next()) <span>|</span>element<span>|</span> {</span>
<span>  use(element);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I like how the iterator comes first, and then the name of an item
            follows, logically and syntactically.
          </p>
        </section>
        <section id="Clarity-of-Names">
          <h2>
            <a href="#Clarity-of-Names">Clarity of Names </a>
          </h2>
          <p>
            I have a very strong opinion about variable shadowing. It goes both
            ways: I spent hours debugging code which incorrectly tried to use a
            variable that was shadowed by something else, but I also spent hours
            debugging code that accidentally used a variable that should have
            been shadowed! I really don’t know whether on balance it is better
            to forbid or encourage shadowing!
          </p>
          <p>
            Zig of course forbids shadowing, but what’s curious is that it’s
            just one episode of the large crusade against any complexity in name
            resolution. There’s no “prelude”, if you want to use anything from
            std, you need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> std = <span>@import</span>(<span>"std"</span>);</span></code></pre>
          </figure>
          <p>
            There are no glob imports, if you want to use an item from std, you
            need to import it:
          </p>

          <figure>
            <pre><code><span><span>const</span> ArrayList = std.ArrayList;</span></code></pre>
          </figure>
          <p>
            Zig doesn’t have inheritance, mixins, argument-dependent lookup,
            extension functions, implicit or traits, so, if you see <code>x.foo()</code>, that <code>foo</code> is guaranteed to be a boring
            method declared on <code>x</code>
            type. Similarly, while Zig has powerful comptime capabilities, it
            <a href="https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html">intentionally disallows</a>
            declaring methods at compile time.
          </p>
          <p>
            Like Rust, Zig used to allow a method and a field to share a name,
            because it actually is syntactically clear enough at the call site
            which is which. But then this feature got removed from Zig.
          </p>
          <p>
            More generally, Zig doesn’t have namespaces. There can be only one
            kind of <code>foo</code> in scope, while Rust allows things like
          </p>

          <figure>
            <pre><code><span><span>struct</span> <span>Point</span> { x: <span>i32</span>, y: <span>i32</span> }</span>
<span><span>fn</span> <span>Point</span>(x: <span>i32</span>, y: <span>i32</span>) <span>-&gt;</span> Point { Point { x, y } }</span></code></pre>
          </figure>
          <p>
            I am astonished at the relative lack of inconvenience in Zig’s
            approach. Turns out that <code>foo.bar.baz</code>
            is all the syntax you’ll ever need for accessing things? For the
            historically inclined, see “The module naming situation” thread in
            the
            <a href="https://github.com/brson/rust-dev-archives">rust mailing list archive</a>
            to learn the story of how rust got its <code>std::vec</code> syntax.
          </p>
        </section>
        <section id="Everything-Is-an-Expression">
          <h2>
            <a href="#Everything-Is-an-Expression">Everything Is an Expression
            </a>
          </h2>
          <p>
            The lack of namespaces touches on the most notable (by its absence)
            feature of Zig syntax, which deeply relates to the most profound
            aspect of Zig’s semantics. Everything is an expression. By which I
            mean, there’s no separate syntactic categories of values, types, and
            patterns. Values, types, and patterns are of course different
            things. And usually in the language grammar it is <em>syntactically</em>
            obvious whether a particular text fragment refers to a type or a
            value:
          </p>

          <figure>
            <pre><code><span><span>let</span> <span>PATTERN</span>: TYPE = VALUE;</span></code></pre>
          </figure>
          <p>
            So the standard way is to have separate syntax families for the
            three categories, which need to be internally unambiguous, but <em>can</em> be ambiguous across the categories because the place in
            the grammar dictates the category: when parsing <code>let</code>,
            everything until <code>:</code> is a pattern, stuff between
            <code>:</code> and <code>=</code> is a type, and after <code>=</code> we have a value.
          </p>
          <p>
            There are two problems here. First, there’s a combinatorial
            explosion of sorts in the syntax, because, while three categories
            describe different things, it turns out that they have the same
            general tree-ish shape.
          </p>
          <p>
            The second problem is that it might be hard to maintain category
            separation in the grammar. Rust
            <em>started</em> with the three categories separated by a bright
            line. But then, changes happen. Originally, Rust only allowed
            <span><code>VALUE = VALUE;</code></span>
            syntax for assignment. But today you can also write
            <span><code>PATTERN = VALUE;</code></span>
            to do unpacking like
            <span><code>(a, b) = (b, a);</code></span>
          </p>
          <p>
            Similarly, the turbofish used to move the parser from the value to
            the type mode, but now const parameters are values that can be found
            in the type position!
          </p>
          <p>
            The alternative is not to pick this fight at all. Rather than trying
            to keep the categories separately in the syntax, use the same
            surface syntax to express all three, and categorize later, during
            semantic analysis. In fact, this is already happens in the <span><code>VALUE = VALUE</code></span>
            example — these are different things! One is a place (lvalue) and
            another is a “true” value (rvalue), but we use the same syntax for
            both.
          </p>
          <p>
            I don’t think such syntactic unification necessarily implies
            semantic unification, but Zig does treat everything uniformly, as a
            value with comptime and runtime behavior (for some values, runtime
            behavior may be missing, for others — comptime):
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> {</span>
<span>    <span>const</span> e: <span>if</span> (<span>true</span>) E <span>else</span> <span>void</span> = .a;</span>
<span>    _ = <span>switch</span> (e) {</span>
<span>        (<span>if</span> (<span>true</span>) .a <span>else</span> .b) =&gt; .a,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; .b,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            The fact that you can write an <code>if</code> where a type goes is
            occasionally useful. But the fact that simple types look like simple
            values syntactically consistently make the language feel
            significantly less busy.
          </p>
        </section>
        <section id="Generics">
          <h2>
            <a href="#Generics">Generics </a>
          </h2>
          <p>
            As a special case of everything being an expression, instances of
            generic types look like this:
            <span><code>ArrayList(u32)</code></span>
          </p>
          <p>
            Just a function call! Though, there’s some resistance to trickery
            involved to make this work. Usually, languages rely on type
            inference to allow eliding generic arguments. That in turn requires
            making argument <em>syntax</em> optional, and that in turn leads to
            separating generic and non-generic arguments into separate parameter
            lists and some introducer sigil for generics, like <code>::&lt;&gt;</code> or
            <code>!()</code>.
          </p>
          <p>
            Zig solves this syntactic challenge in the most brute-force way
            possible. Generic parameters are never inferred, if a function takes
            3 comptime arguments and 2 runtime arguments, it will always be
            called with 5 arguments syntactically. Like with the (absence of)
            importing flourishes, a reasonable reaction would be “wait, does
            this mean that I’ll have to specify the types all the time?” And,
            like with import, in practice this is a non-issue. The trick are
            comptime closures. Consider a generic
            <code>ArrayList</code>:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> ArrayListType</span>(<span>comptime</span> T: <span>type</span>) <span>type</span> {</span>
<span>    <span>return</span> <span>struct</span> {</span>
<span>        <span>const</span> ArrayList = <span>@This</span>();</span>
<span></span>
<span>        <span>fn</span><span> init</span>(gpa: Allocator) ArrayList {}</span>
<span>        <span>fn</span><span> deinit</span>(list: <span>*</span>ArrayList, gpa: Allocator) <span>void</span> {}</span>
<span>        <span>fn</span><span> push</span>(list: <span>*</span>ArrayList, item: T) <span>!</span><span>void</span> {}</span>
<span>    };</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>(gpa: Allocator) <span>!</span><span>void</span> {</span>
<span>    <span>var</span> xs: ArrayListType(<span>u32</span>) = .init(gpa);</span>
<span>    <span>defer</span> xs.deinit(gpa);</span>
<span></span>
<span>    <span>try</span> xs.push(<span>92</span>);</span>
<span>}</span></code></pre>
          </figure>
          <p>
            We have to specify type <code>T</code> when creating an instance of
            an <code>ArrayList</code>. But subsequently, when we are <em>using</em> the array list, we don’t have to specify the type
            parameter again, because the type of
            <code>xs</code> variable already closes over <code>T</code>. This is
            the major truth of object-orienting programming, the truth so
            profound that no one even notices it: in real code, 90% of functions
            are happiest as (non-virtual) methods. And, because of that, the
            annotation burden in real-world Zig programs is low.
          </p>
        </section>
        <section id="Declaration-Literals">
          <h2>
            <a href="#Declaration-Literals">Declaration Literals </a>
          </h2>
          <p>
            While Zig doesn’t have Hindley-Milner constraint-based type
            inference, it relies heavily on one specific way to propagate types.
            Let’s revisit the first <code>comptime_int</code> example:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span></code></pre>
          </figure>
          <p>
            This doesn’t compile: <code>1</code> and <code>2</code> are
            different <code>comptime</code> values, we can’t select between two
            at runtime because they are different. We need to coerce the
            constants to a specific runtime type:
          </p>

          <figure>
            <pre><code><span><span>const</span> x: <span>u32</span> = <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>;</span>
<span></span>
<span><span>const</span> x = <span>@coerceTo</span>(</span>
<span>  <span>u32</span>,</span>
<span>  <span>if</span> (condition()) <span>1</span> <span>else</span> <span>2</span>,</span>
<span>);</span></code></pre>
          </figure>
          <p>
            But this doesn’t kick the can sufficiently far enough and
            essentially reproduces the <code>if</code> with two incompatible
            branches. We need to sink coercion down the branches:
          </p>

          <figure>
            <pre><code><span><span>const</span> x = <span>if</span> (condition())</span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>1</span>)</span>
<span><span>else</span></span>
<span>    <span>@coerceTo</span>(<span>u32</span>, <span>2</span>);</span></code></pre>
          </figure>
          <p>
            And that’s exactly how Zig’s “Result Location Semantics” works. Type
            “inference” runs a simple left-to-right tree-walking algorithm,
            which resembles interpreter’s <code>eval</code>. In fact, <code>eval</code> is
            <em>exactly</em> what happens. Zig is not a compiler, it is an
            interpreter. When <code>zig</code> evaluates an expression, it gets:
          </p>
          <ul>
            <li>
              expression’s type (as a Zig value),
            </li>
            <li>
              expression’s value (if it can be evaluated at comptime),
            </li>
            <li>
              code to compute expression’s value otherwise.
            </li>
          </ul>

          <figure>
            <pre><code><span>eval("1 + 2") =</span>
<span>  3</span>
<span></span>
<span>eval("f() + g()") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = call 'g'</span>
<span>  $3 = add $1, $2</span>
<span></span>
<span>eval("f() + 2") =</span>
<span>  $1 = call 'f'</span>
<span>  $2 = add $1,  imm 2</span></code></pre>
          </figure>
          <p>When interpreting code like</p>

          <figure>
            <pre><code><span>obj.field = if (condition()) 1 else 2;</span></code></pre>
          </figure>
          <p>
            the interpreter passes the result location (<code>obj.field</code>)
            and type down the tree of subexpressions. If branches store result
            directly into object field (there’s a <code>store</code> inside each
            branch, as opposed to one <code>store</code> after the <code>if</code>), and each coerces its comptime constant to the
            appropriate runtime type of the result.
          </p>
          <p>
            This mechanism enables concise <code>.variant</code> syntax for
            specifying enums:
          </p>

          <figure>
            <pre><code><span><span>const</span> E = <span>enum</span> { a, b };</span>
<span></span>
<span><span>fn</span><span> example</span>(e: E) <span>u32</span> {</span>
<span>    <span>return</span> <span>switch</span> (e) {</span>
<span>        .a =&gt; <span>1</span>,</span>
<span>        (<span>if</span> (<span>true</span>) .b <span>else</span> .a) =&gt; <span>2</span>,</span>
<span>    };</span>
<span>}</span></code></pre>
          </figure>
          <p>
            When <code>zig</code> evaluates the switch, it first evaluates the
            scrutinee, and realizes that it has type
            <code>E</code>. When evaluating <code>switch</code> arm, it sets
            result type to <code>E</code> for the condition, and a literal <code>.a</code>
            gets coerced to <code>E</code>. The same happens for the second arm,
            where result type further sinks down the
            <code>if</code>.
          </p>
          <p>
            Result type semantics also explains the leading dot in the record
            literal syntax:
          </p>

          <figure>
            <pre><code><span><span>const</span> p: Point = .{</span>
<span>    .x = <span>1</span>,</span>
<span>    .y = <span>2</span>,</span>
<span>};</span></code></pre>
          </figure>
          <p>
            Syntactically, we just want to disambiguate records from blocks.
            But, semantically, we want to coerce the literal to whatever type we
            want to get out of this expression. In Zig, <code>.whatever</code>
            is a shorthand for <code>@ResultType().whatever</code>.
          </p>
          <p>
            I must confess that <code>.{}</code> did weird me out a lot at first
            during <em>writing</em> code (I don’t mind reading the dot). It’s
            not the easiest thing to type! But that was fixed once I added <code>..</code> snippet, expanding to <code>.{$0}</code>.
          </p>
          <p>
            The benefits to lightweight record literal syntax are huge, as they
            allow for some pretty nice APIs. In particular, you get named and
            default arguments for free:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> exec</span>(argv: []<span>const</span> <span>u8</span>, options: <span>struct</span> {</span>
<span>    working_directory: ?[]<span>const</span> <span>u8</span> = <span>null</span></span>
<span>}) <span>!</span><span>void</span> {</span>
<span>    <span>// ...</span></span>
<span>}</span>
<span></span>
<span><span>fn</span><span> usage</span>() <span>!</span><span>void</span> {</span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{});</span>
<span></span>
<span>    <span>try</span> exec(<span>&amp;</span>.{ <span>"git"</span>, <span>"status"</span>}, .{</span>
<span>        .working_directory = <span>"./src"</span>,</span>
<span>    });</span>
<span>}</span></code></pre>
          </figure>
          <p>
            I don’t really miss the absence of named arguments in Rust, you can
            always design APIs without them. But they are free in Zig, so I use
            them liberally. Syntax wise, we get two features (calling functions
            and initializing objects) for the price of one!
          </p>
        </section>
        <section id="Built-ins">
          <h2>
            <a href="#Built-ins">Built-ins </a>
          </h2>
          <p>
            Finally, the thing that weirds out some people when they see Zig
            code, and makes others reconsider their choice GitHub handles, even
            when they haven’t seen any Zig: <code>@divExact</code> syntax for
            built-in functions.
          </p>
          <p>
            Every language needs to glue “userspace” code with primitive
            operations supported by the compiler. Usually, the gluing is
            achieved by making the standard library privileged and allowing it
            to define intrinsic functions without bodies, or by adding ad-hoc
            operators directly to the language (like Rust’s <code>as</code>).
            And Zig does have a fair amount of operators, like <code>+</code> or
            <code>orelse</code>. But the release valve for a lot of
            functionality are built-in functions in distinct syntactic
            namespace, so Zig separates out <code>@bitCast</code>, <code>@addrSpaceCast</code>, <code>@alignCast</code>, <code>@constCast</code>, <code>@ptrCast</code>, <code>@intCast</code>,
            <code>@floatCast</code>, <code>@volatileCast</code>, <code>@ptrFromInt</code>, and <code>@intFromPtr</code>. There’s no need
            to overload casting when you can give each variant a name.
          </p>
          <p>
            There’s also <span><code>@as(i32, 92)</code></span>
            for type ascription. The types goes first, because the mechanism
            here is result type semantics: <code>@as</code> evaluates the first
            argument as a type, and then uses that as the type for the second
            argument. Curiously, <code>@as</code> I think actually can be
            implemented in the userspace:
          </p>

          <figure>
            <pre><code><span><span>fn</span><span> as</span>(<span>comptime</span> T: <span>type</span>, value: T) T {</span>
<span>    <span>return</span> value;</span>
<span>}</span></code></pre>
          </figure>
          <p>
            In Zig, a type of function parameter may depend on values of
            preceding (comptime) ones!
          </p>
          <p>
            My favorite builtin is <code>@import()</code>. First, it’s the most
            obvious way to import code:
            <span><code>const foo =
                @import("./foo.zig")</code></span>
            Its crystal clear where the file comes from.
          </p>
          <p>
            But, second, it is an instance of reverse syntax sugar. You see,
            import isn’t really a function. You can’t do
          </p>

          <figure>
            <pre><code><span><span>const</span> name = <span>"./foo.zig"</span>;</span>
<span><span>const</span> foo = <span>@import</span>(name);</span></code></pre>
          </figure>
          <p>
            The argument of <code>@import</code> has to be a string,
            syntactically. It really is
            <span><code>import "./path.zig"</code></span>
            syntax, except that the function-call form is re-used, because it
            already has the right shape.
          </p>
          <hr>
          <p>
            So, this is it. Just a bunch of silly syntactical decisions, which
            add up to a language which is positively enjoyable to read. As for
            big lessons, obviously, the less features your language has, the
            less syntax you’ll need. And less syntax is generally good, because
            varied syntactic constructs tend to step on each other toes.
            Languages are not combinations of orthogonal aspects. Features tug
            and pull the language in different directions and their combinations
            might turn to be miraculous features in their own right, or might
            drag the language down.
          </p>
          <p>
            Even with a small feature-set fixed, there’s still a lot of work to
            pick a good concrete syntax: unambiguous to parse, useful to grep,
            easy to read and not to painful to write. A smart thing is of course
            to steal and borrow solutions from other languages, not because of
            familiarity, but because the ruthless natural selection tends to
            weed out poor ideas. But there’s a lot of inertia in languages, so
            there’s no need to fear innovation. If an odd-looking syntax is
            actually good, people will take to it.
          </p>
          <p>
            Is there anything about Zig’s syntax I don’t like? I thought no,
            when starting this post. But in the process of writing it I did
            discover one form that annoys me. It is the while with the increment
            loop:
          </p>

          <figure>
            <pre><code><span><span>var</span> i: <span>u32</span> = <span>0</span>;</span>
<span><span>while</span> (i &lt; <span>10</span>) : (i<span>+=</span><span>1</span>) {</span>
<span>    print(<span>"{d}"</span>, .{i});</span>
<span>}</span></code></pre>
          </figure>
          <p>
            This is two-thirds of a C-style <code>for</code> loop (without the
            declarator), and it sucks for the same reason: control flow jumps
            all other the place and is unrelated to the source code order. We go
            from condition, to the body, to the increment. But in the source
            order the increment is between the condition and the body. In Zig,
            this loop sucks for one additional reason: that <code>:</code>
            separating the increment I think is the single example of control
            flow in Zig that is expressed by a sigil, rather than a keyword.
          </p>
          <p>
            This form used to be rather important, as Zig lacked a counting
            loop. It has
            <span><code>for(0..10) |i|</code></span>
            form now, so I am tempted to call the while-with-increment
            redundant.
          </p>
          <p>Annoyingly,</p>

          <figure>
            <pre><code><span><span>while</span> (condition) {</span>
<span>    <span>defer</span> increment;</span>
<span></span>
<span>    body</span>
<span>}</span></code></pre>
          </figure>
          <p>is <em>almost</em> equivalent to</p>

          <figure>
            <pre><code><span><span>while</span> (condition) : (increment) {</span>
<span>  body</span>
<span>}</span></code></pre>
          </figure>
          <p>
            But not exactly: if <code>body</code> contains a <code>return</code>, <code>break</code> or <code>try</code>, the <code>defer</code> version would run the
            <code>increment</code> one extra time, which is useless and might be
            outright buggy. Oh well.
          </p>
        </section>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2 (278 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</link>
            <guid>44855690</guid>
            <pubDate>Sun, 10 Aug 2025 15:06:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the">https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</a>, See on <a href="https://news.ycombinator.com/item?id=44855690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).</p><p>This is the first time since GPT-2 that OpenAI has shared a large, fully open-weight model. Earlier GPT models showed how the transformer architecture scales. The 2022 ChatGPT release then made these models mainstream by demonstrating concrete usefulness for writing and knowledge (and later coding) tasks. Now they have shared some long-awaited weight model, and the architecture has some interesting details.</p><p>I spent the past few days reading through the code and technical reports to summarize the most interesting details. (Just days after, OpenAI also announced GPT-5, which I will briefly discuss in the context of the gpt-oss models at the end of this article.)</p><p>Below is a quick preview of what the article covers. For easier navigation, I recommend using the Table of Contents on the left of on the article page.</p><ul><li><p>Model architecture comparisons with GPT-2</p></li><li><p>MXFP4 optimization to fit gpt-oss models onto single GPUs</p></li><li><p>Width versus depth trade-offs (gpt-oss vs Qwen3)</p></li><li><p>Attention bias and sinks</p></li><li><p>Benchmarks and comparisons with GPT-5</p></li></ul><p>I hope you find it informative!</p><p>Before we discuss the architecture in more detail, let's start with an overview of the two models, gpt-oss-20b and gpt-oss-120b, shown in Figure 1 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!rlW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png" width="1456" height="681" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:681,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243817,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Figure 1: The two gpt-oss models side by side.</figcaption></figure></div><p><span>If you have looked at recent LLM architecture diagrams before, or read my previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, you may notice that there is nothing novel or unusual at first glance. </span></p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png"><img src="https://substackcdn.com/image/fetch/$s_!83ox!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd72e5a99-1a11-42b7-8831-8f5785ed2bc1_1600x1116.png" sizes="100vw" alt="The Big LLM Architecture Comparison" width="140" height="140"></picture></div></a></div><p>This is not surprising, since leading LLM developers tend to use the same base architecture and then apply smaller tweaks. This is pure speculation on my part, but I think this is because</p><ul><li><p>There is significant rotation of employees between these labs.</p></li><li><p><span>We still have not found anything better than the transformer architecture. Even though state space models and text diffusion models exist, as far as I know no one has shown that they perform as well as transformers at this scale. (Most of the comparisons I found focus only on benchmark performance. It is still unclear how well the models handle real-world, multi-turn writing and coding tasks. At the time of writing, the highest-ranking non-purely-transformer-based model on the </span><a href="https://lmarena.ai/leaderboard/text" rel="">LM Arena</a><span> is Jamba, which is a transformer–state space model hybrid, at rank 96.)</span></p></li><li><p>Most of the gains likely come from data and algorithm tweaks rather than from major architecture changes.</p></li></ul><p>That being said, there are still many interesting aspects of their design choices. Some are shown in the figure above (while others are not, but we will discuss them later as well). In the rest of this article, I will highlight these features and compare them to other architectures, one at a time.</p><p>I should also note that I am not affiliated with OpenAI in any way. My information comes from reviewing the released model code and reading their technical reports. If you want to learn how to use these models locally, the best place to start is OpenAI's official model hub pages:</p><ul><li><p><a href="https://huggingface.co/openai/gpt-oss-20b" rel="">https://huggingface.co/openai/gpt-oss-20b</a></p></li><li><p><a href="https://huggingface.co/openai/gpt-oss-120b" rel="">https://huggingface.co/openai/gpt-oss-120b</a></p></li></ul><p>The 20B model can run on a consumer GPU with up to 16 GB of RAM. The 120B model can run on a single H100 with 80 GB of RAM or newer hardware. I will return to this later, as there are some important caveats.</p><p>Before we jump into comparisons between gpt-oss and a more recent architecture, let's hop into the time machine and take a side-by-side look at GPT-2 (Figure 2) to see just how far things have come.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AsnD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png" width="1456" height="788" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:267271,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 2: A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B.</figcaption></figure></div><p><span>Both gpt-oss and GPT-2 are decoder-only LLMs built on the transformer architecture introduced in the </span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need (2017)</a><span> paper. Over the years, many details have evolved.</span></p><p><span>However, these changes are not unique to gpt-oss. And as we will see later, they appear in many other LLMs. Since I discussed many of these aspects in the previous </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">Big Architecture Comparison</a><span> article, I will try to keep each subsection brief and focused.</span></p><p><a href="https://arxiv.org/abs/1207.0580" rel="">Dropout (2012)</a><span> is a traditional technique to prevent overfitting by randomly "dropping out" (i.e., setting to zero) a fraction of the layer activations or attention scores (Figure 3) during training. However, dropout is rarely used in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BS-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png" width="554" height="557.2781065088758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:850,&quot;width&quot;:845,&quot;resizeWidth&quot;:554,&quot;bytes&quot;:130475,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 3: An illustration of dropout applied to the attention score matrix.</figcaption></figure></div><p>I assume that dropout was originally used in GPT-2 because it was inherited from the original transformer architecture. Researchers likely noticed that it does not really improve LLM performance (I observed the same in my small-scale GPT-2 replication runs). This is likely because LLMs are typically trained for only a single epoch over massive datasets, which is in contrast to the multi-hundred-epoch training regimes for which dropout was first introduced. So, since LLMs see each token only once during training, there is little risk of overfitting.</p><p><span>Interestingly, while Dropout is kind of ignored in LLM architecture design for many years, I found a </span><a href="https://arxiv.org/abs/2505.24788" rel="">2025 research paper</a><span> with small scale LLM experiments (Pythia 1.4B) that confirms that Dropout results in worse downstream performance in these single-epoch regimes.</span></p><p>In transformer-based LLMs, positional encoding is necessary because of the attention mechanism. By default, attention treats the input tokens as if they have no order. In the original GPT architecture, absolute positional embeddings addressed this by adding a learned embedding vector for each position in the sequence (Figure 4), which is then added to the token embeddings.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!YCov!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png" width="1195" height="533" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:533,&quot;width&quot;:1195,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123823,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 4: Illustration of absolute positional embeddings.</figcaption></figure></div><p><span>RoPE (</span><a href="https://arxiv.org/abs/2104.09864" rel="">Rotary Position Embedding</a><span>) introduced a different approach: instead of adding position information as separate embeddings, it encodes position by rotating the query and key vectors in a way that depends on each token's position. (RoPE is an elegant idea but also a bit of a tricky topic to explain. I plan to cover separately in more detail one day.)</span></p><p>While first introduced in 2021, RoPE became widely adopted with the release of the original Llama model in 2023 and has since become a staple in modern LLMs.</p><p>Early GPT architectures used GELU. Why now use Swish over GELU? Swish is considered computationally slightly cheaper, and in my opinion, that all there is to it. Depending on which paper you look at, you will find that one is slightly better than the other in terms of modeling performance. In my opinion, these small differences are probably within a standard error, and your mileage will vary based on hyperparameter sensitivity.</p><p>Activation functions used to be a hot topic of debate until the deep learning community largely settled on ReLU more than a decade ago. Since then, researchers have proposed and tried many ReLU-like variants with smoother curves, and GELU and Swish (Figure 5) are the ones that stuck.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!WIz6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png" width="1407" height="775" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:775,&quot;width&quot;:1407,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:237022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 5: Comparison between Swish and GELU activations, which are both smoother versions or ReLU.</figcaption></figure></div><p><span>Early GPT architectures used GELU, which is defined as </span><code>0.5x * [1 + erf(x / sqrt(2))]</code><span>. Here, </span><code>erf</code><span> (short for error function) is the integral of a Gaussian and it is computed using polynomial approximations of the Gaussian integral, which makes it more computationally expensive than simpler functions like the sigmoid used in Swish, where Swish is simply </span><code>x * sigmoid(x)</code><span>.</span></p><p>In practice, Swish is computationally slightly cheaper than GELU, and that's probably the main reason it replaced GELU in most newer models. Depending on which paper we look at, one might be somewhat better in terms of modeling performance. But I'd say these gains are often within standard error, and the winner will depend heavily on hyperparameter tuning.</p><p>Swish is used in most architectures today. However, GELU is not entirely forgotten; for example, Google's Gemma models still use GELU.</p><p><span>What's more notable, though, is that the feed forward module (a small multi-layer perceptron) is replaced by a gated "GLU" counterpart, where GLU stands for gated linear unit and was proposed in a </span><a href="https://arxiv.org/pdf/2002.05202" rel="">2020 paper</a><span>. Concretely, the 2 fully connected layers are replaced by 3 fully connected layers that are used as shown in Figure 6 below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8gzt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png" width="655" height="550.0696517412936" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1005,&quot;resizeWidth&quot;:655,&quot;bytes&quot;:190423,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 6: A comparison between Swish and GELU and their gated counterparts, SwiGLU and GEGLU.</figcaption></figure></div><p><span>At first glance, it may appear that the GEGLU/SwiGLU variants may be better than the regular feed forward layers because there are simply more parameters due to the extra layer. But this is deceiving because in practice, the </span><code>W</code><span> and </span><code>V</code><span> weight layers in SwiGLU/GEGLU are usually chosen to be half the size each of the </span><code>W_1</code><span> layer in a traditional feed forward layer.</span></p><p>To illustrate this better, consider the concrete code implementations of the regular and GLU variants:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_JVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png" width="687" height="513.1010587102984" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:776,&quot;width&quot;:1039,&quot;resizeWidth&quot;:687,&quot;bytes&quot;:267148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 7: Regular feed forward module (top) and SwiGLU variant (bottom) next to each other.</figcaption></figure></div><p>So, suppose we have an embedding dimension of 1024. In the regular feed forward case, this would then be</p><ul><li><p>fc1: 1024 × 4096 = 4,194,304</p></li><li><p>fc2: 1024 × 4096 = 4,194,304</p></li></ul><p>That is fc1 + fc2 = 8,388,608 parameters.</p><p>For the GLU variant, we have</p><ul><li><p>fc1: 1024 × 2048 = 2,097,152</p></li><li><p>fc2: 1024 × 2048 = 2,097,152</p></li><li><p>fc3: 2048 × 1024 = 2,097,152</p></li></ul><p>I.e., 3 × 2,097,152 = 6,291,456 weight parameters.</p><p>So, overall, using the GLU variants results in fewer parameters, and they perform better as well. The reason for this better performance is that these GLU variants provide an additional multiplicative interaction, which improves expressivity (the same reason deep &amp; slim neural nets perform better than shallow &amp; wide neural nets, provided they are trained well).</p><p>In addition to upgrading the feed forward module to a SwiGLU, as discussed in the previous section, gpt-oss replaces the single feed forward module with multiple feed forward modules, using only a subset for each token generation step. This approach is known as a Mixture-of-Experts (MoE) and illustrated in Figure 8 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!SYqb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png" width="1307" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120915,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 8: The feed forward module is replaced by a Mixture-of-Expert (MoE) module.</figcaption></figure></div><p><span>So, replacing </span><em>a single</em><span> feed forward module with </span><em>multiple</em><span> feed forward modules (as done in a MoE setup) substantially increases the model's total parameter count. However, the key trick is that we don't use ("activate") all experts for every token. Instead, a router selects only a small subset of experts per token.</span></p><p><span>Because only a few experts are active at a time, MoE modules are often referred to as </span><em>sparse</em><span>, in contrast to </span><em>dense</em><span> modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don't use all the parameters at the same time.</span></p><p>(Fun fact: In most MoE models, expert weights account for more than 90% of the total model parameters.)</p><p>As mentioned in my previous articles, Grouped Query Attention (GQA) has emerged in recent years as a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA).</p><p>In MHA, each head has its own set of keys and values. GQA reduces memory usage by grouping multiple heads to share the same key and value projections.</p><p>For example, as shown in Figure 9, if there are 2 key–value groups and 4 attention heads, heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This grouping decreases the total number of key and value computations, leading to lower memory usage and improved efficiency — without noticeably affecting modeling performance, according to ablation studies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Kohq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png" width="637" height="302.7938561034762" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:1237,&quot;resizeWidth&quot;:637,&quot;bytes&quot;:83420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 9: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.</figcaption></figure></div><p>So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model's parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.</p><p><span>(If you are curious how GQA looks in code, see my</span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb" rel=""> GPT-2 to Llama 3 conversion guide</a><span> for a version without KV cache and my KV-cache variant </span><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py" rel="">here</a><span>.)</span></p><p><span>While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the</span><a href="https://arxiv.org/abs/2305.13245" rel=""> original GQA paper</a><span> and the </span><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2 paper</a><span>) show it performs comparably to standard MHA in terms of LLM modeling performance.</span></p><p><span>Sliding-window attention (Figure 10 below) was first introduced in the </span><a href="https://arxiv.org/abs/2004.05150" rel="">LongFormer paper (2020)</a><span> and later popularized by Mistral. Interestingly, gpt-oss applies it in every second layer. You can think of it as a variation of multi-head attention, or in this case grouped query attention (GQA), where the attention context is restricted to a smaller window, reducing both memory usage and compute costs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wwFe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg" width="1456" height="721" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:225815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 10: Comparison between regular attention (left) and sliding window attention (right).</figcaption></figure></div><p>Concretely, gpt-oss alternates between GQA layers that attend to the full context and GQA layers with a sliding window limited to 128 tokens.</p><p><span>As I discussed in my </span><a href="https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison" rel="">previous article</a><span>, </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 (2024)</a><span> used a similar 1:1 ratio. </span><a href="https://arxiv.org/abs/2503.19786" rel="">Gemma 3</a><span> earlier this year went much further and shifted to a 5:1 ratio, which means only one full-attention layer for every five sliding-window (local) attention layers.</span></p><p>According to the Gemma ablation studies, sliding-window attention has minimal impact on modeling performance, as shown in the figure below. Note that the window size in Gemma 2 was 4096 tokens, which Gemma 3 reduced to 1024. In gpt-oss, the window is just 128 tokens, which is remarkably small.</p><p><span>And as a fun fact, the </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">official announcement article</a><span> notes that sliding-window attention was apparently already used in GPT-3:</span></p><blockquote><p>The models use alternating dense and locally banded sparse attention patterns, similar to GPT-3</p></blockquote><p><span>Who knew!? I went back to the original </span><a href="https://arxiv.org/abs/2005.14165" rel="">GPT-3 paper</a><span>, and it was indeed mentioned there:</span></p><blockquote><p>We use the same model and architecture as GPT-2 [ RWC+19 ], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. </p></blockquote><p><span>Finally, the last small tweak, coming from GPT-2, is replacing </span><a href="https://arxiv.org/abs/1607.06450" rel="">LayerNorm (2016)</a><span> by </span><a href="https://arxiv.org/abs/1910.07467" rel="">RMSNorm (2019)</a><span>, which has been a common trend in recent years.</span></p><p>Akin to swapping GELU with Swish and SwiGLU, RMSNorm is one of these smaller but sensible efficiency improvements. RMSNorm is similar to LayerNorm in its purpose to normalize layer activations, as shown in Figure 11 below.</p><p>You might recall that not too long ago, BatchNorm was the go-to choice for this task. It has since fallen out of favor, largely because it is harder to parallelize efficiently (due to the mean and variance batch statistics) and performs poorly with small batch sizes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!H32R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png" width="1367" height="599" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:599,&quot;width&quot;:1367,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:274255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 11: A comparison between LayerNorm (left) and RMSNorm (right) for a small linear layer.</figcaption></figure></div><p>As we can see in Figure 11 above, both LayerNorm and RMSNorm scale the layer outputs to be in a reasonable range.</p><p>LayerNorm subtracts the mean and divides by the standard deviation such that the layer outputs have a zero mean and unit variance (variance of 1 and standard deviation of one).</p><p>RMSNorm divides the inputs by the root-mean-square. This doesn't force zero mean and unit variance, but the mean and variance are in a reasonable range: -1 to 1 for the mean and 0 to 1 for the variance. In this particular example shown in Figure 11, the mean is 0.77 and the variance is 0.41.</p><p>Both LayerNorm and RMSNorm stabilize activation scales and improve optimization, but RMSNorm is often preferred in large-scale LLMs because it is cheaper to compute. Unlike LayerNorm, RMSNorm has no bias (shift) term and reduces the expensive mean and variance computations to a single root-mean-square operation. This reduces the number of cross-feature reductions from two to one, which lowers communication overhead on GPUs and improving training efficiency.</p><p>Figure 12 shows what this looks like in code:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!m5aM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png" width="589" height="442.23068552774754" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:919,&quot;resizeWidth&quot;:589,&quot;bytes&quot;:259430,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 12: Code implementations of LayerNorm and RMSNorm showing that RMSNorm is computationally simpler.</figcaption></figure></div><p>I still think that GPT-2 is an excellent beginner architecture when learning about LLMs. It's simple enough to understand without getting lost in layers of optimization tricks, but still complex enough to give you a solid grasp of how modern transformer models work.</p><p>By starting with GPT-2, you can focus on the fundamentals (attention mechanisms, positional embeddings, normalization, and the overall training pipeline) without being overwhelmed by the extra features and tweaks found in newer architectures.</p><p>In fact, I think it's worth the time to learn about and even implement GPT-2 first before trying to stack newer changes on top. You will not only have an easier time understanding those changes, but you will likely also appreciate them more, because you will get a better understanding of what limitations or problems they try to solve.</p><p><span>For instance, starting with my GPT-2 code I recently implemented the </span><a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3" rel="">Qwen3 architecture from scratch</a><span>, which is super similar to gpt-oss, which brings us to the next topic: Comparing gpt-oss to a more recent architecture.</span></p><p>Now that we have walked through the evolution from GPT-2 to GPT OSS, we can take the next step and compare GPT OSS to a more recent architecture, Qwen3, which was released three months earlier in May 2025.</p><p>The reason I am selecting Qwen3 here is that it is among the top open-weight models as of the time of writing. Additionally, one of the Qwen3 MoE models is more or less directly comparable to GPT OSS due to its relatively similar overall size in terms of trainable parameters.</p><p>Figure 13 below compares gpt-oss-20b to a Qwen3 model of comparable size.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5K75!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:268927,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 13: A gpt-oss and Qwen3 model of comparable size side by side.</figcaption></figure></div><p>As we can see, gpt-oss 20B and Qwen3 30B-A3B are very similar in their architecture components. The primary difference here, aside from the dimensions, is that gpt-oss employs sliding window attention, as discussed earlier in section 1.6 (not shown in this figure), whereas Qwen3 does not.</p><p>Let's walk through the noteworthy details one by one in the following subsections.</p><p>If we look at the two models closely, we see that Qwen3 is a much deeper architecture with its 48 transformer blocks instead of 24 (Figure 14).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!G1hj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png" width="1456" height="696" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:696,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:307435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 14: Qwen3 has twice as many transformer blocks as gpt-oss-20b.</figcaption></figure></div><p>On the other hand, gpt-oss is a much wider architecture:</p><ul><li><p>An embedding dimension of 2880 instead of 2048</p></li><li><p>An intermediate expert (feed forward) projection dimension of 5760 instead of 768</p></li></ul><p>It's also worth noting that gpt-oss uses twice as many attention heads, but this doesn't directly increase the model's width. The width is determined by the embedding dimension.</p><p>Does one approach offer advantages over the other given a fixed number of parameters? As a rule of thumb, deeper models have more flexibility but can be harder to train due to instability issues, due to exploding and vanishing gradients (which RMSNorm and shortcut connections aim to mitigate).</p><p>Wider architectures have the advantage of being faster during inference (with a higher tokens/second throughput) due to better parallelization at a higher memory cost.</p><p><span>When it comes to modeling performance, there's unfortunately no good apples-to-apples comparison I am aware of (where parameter size and datasets are kept constant) except for an ablation study in the </span><a href="https://arxiv.org/abs/2408.00118" rel="">Gemma 2 paper (Table 9)</a><span>, which found that for a 9B parameter architecture, a wider setup is slightly better than a deeper setup. Across 4 benchmarks, the wider model achieved a 52.0 average score, and the deeper model achieved a 50.8 average score.</span></p><p>As shown in Figure 14 above, it's also noteworthy that gpt-oss has a surprisingly small number of experts (32 instead of 128), and only uses 4 instead of 8 active experts per token. However, each expert is much larger than the experts in Qwen3.</p><p>This is interesting because the recent trends and developments point towards more, smaller models as being beneficial. This change, at a constant total parameter size, is nicely illustrated in Figure 15 below from the DeepSeekMoE paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qYc3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png" width="1131" height="609" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:609,&quot;width&quot;:1131,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:219481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 15: An annotated figure from "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", </span><a href="https://arxiv.org/abs/2401.06066" rel="">https://arxiv.org/abs/2401.06066</a></figcaption></figure></div><p>Notably, unlike DeepSeek's models, neither gpt-oss nor Qwen3 uses shared experts, though.</p><p>To be fair, the small number of experts in gpt-oss could be a side effect of the 20B size. Looking at the 120B mode below, they indeed increased the number of experts (and transformer blocks) while keeping everything else fixed, as shown in Figure 16 below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!w8-R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png" width="1456" height="726" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:291088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 16: The two gpt-oss architectures side by side, where the larger 120B model only scales the number of transformer blocks and number of experts.</figcaption></figure></div><p>The boring explanation for the fact that the 20B and 120B models are so similar is probably that the 120B model was the main focus. And the easiest way to create a smaller model was to make it a bit shorter (fewer transformer blocks) and to reduce the number of experts, because that's where most of the parameters are. However, one might speculate whether they started training the 120B model, and then chopped some of the transformer blocks and experts for continued pre-training (instead of starting from random weights).</p><p>In any case, it's because it's quite unusual to only scale those two (transformer blocks and number of experts). For instance, when looking at Qwen3 MoE models of multiple sizes (Figure 17 below), they were scaled more proportionally to each other over many more aspects..</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0h6T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png" width="1120" height="903" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:903,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:210100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Figure 17: Architecture differences in the various Qwen3 models.</figcaption></figure></div><p>Both gpt-oss and Qwen3 use grouped query attention. The main difference is that gpt-oss restricts the context size via sliding window attention in each second layer, as mentioned earlier.</p><p>However, there's one interesting detail that caught my eye. It seems that gpt-oss uses bias units for the attention weights, as shown in the figure below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!U3bl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png" width="1456" height="441" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:441,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 18: gpt-oss models use bias units in the attention layers. See code example </span><a href="https://github.com/huggingface/transformers/blob/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py#L228-L243" rel="">here</a><span>.</span></figcaption></figure></div><p><span>I haven't seen these bias units being used since the GPT-2 days, and they are commonly regarded as redundant. Indeed, I found a </span><a href="https://arxiv.org/abs/2302.08626" rel="">recent paper</a><span> that shows mathematically that this is at least true for the key transformation (k_proj). Furthermore, the empirical results show that there is little difference between with and without bias units (see Figure 19 below).</span></p><p><span>Another detail you may have noticed is the definition of </span><code>sinks</code><span> in the code screenshot in Figure 18. In general models, attention sinks are special "always-attended" tokens placed at the start of the sequence to stabilize attention, which is especially useful in long-context scenarios. I.e., if the context gets very long, this special attended token at the beginning is still attended to, and it can learn to store some generally useful information about the entire sequence. (I think it was originally proposed in the </span><a href="https://arxiv.org/abs/2309.17453" rel="">Efficient Streaming Language Models with Attention Sinks</a><span> paper.)</span></p><p><span>In the gpt-oss implementation, </span><em>attention sinks</em><span> are not actual tokens in the input sequence. Instead, they are learned per-head bias logits that are appended to the attention scores (Figure 20). The goal is the same as with the above-mentioned attention sinks, but without modifying the tokenized inputs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Qwo6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png" width="988" height="684" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:988,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202184,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 20: The use of attention sinks in gpt-oss; based on the Hugging Face code </span><a href="https://github.com/huggingface/transformers/blame/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py" rel="">here</a><span>.</span></figcaption></figure></div><p>Lastly, and similar to Qwen3, the gpt-oss models are Apache 2.0 open-source license, which is great (it's the same license that I prefer for my own open-source projects). This means that the models can be distilled into other models or used in commercial products without restriction.</p><p><strong>Open-weight vs. open-source LLMs.</strong><span> This distinction has been debated for years, but it is worth clarifying to avoid confusion about this release and its artifacts. Some model developers release only the model weights and inference code (for example, Llama, Gemma, gpt-oss), while others (for example, OLMo) release everything including training code, datasets, and weights as true open source.</span></p><p><span>By that stricter definition, gpt-oss is an </span><em>open-weight</em><span> model (just like Qwen3) because it includes the weights and inference code but not the training code or datasets. However, the terminology is used inconsistently across the industry.</span></p><p><span>I assume the "oss" in "gpt-oss" stands for </span><em>open source software</em><span>; however, I am positively surprised that OpenAI itself clearly describes gpt-oss as an open-weight model in their official </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement article</a><span>.</span></p><p>While the previous sections described how the architecture has evolved since GPT-2 and discussed its similarities to Qwen3 (and most other recent models), there are still a few additional but noteworthy details I have not mentioned, yet. These are points that did not fit neatly into the earlier sections but are still worth mentioning.</p><p><span>Unfortunately, there is not much information about the training set sizes and algorithms available. I added the most interesting puzzle pieces from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card report</a><span> (1) and </span><a href="https://openai.com/index/introducing-gpt-oss/" rel="">announcement post</a><span> (2) below:</span></p><blockquote><p>The gpt-oss models were trained using our most advanced pre-training and post-training techniques [...] (1)</p><p>[...] required 2.1million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. (1)</p><p>[...] including a supervised fine-tuning stage and a high-compute RL stage [...] (2)</p><p>We trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. (2)</p></blockquote><p><span>So, we know that the gpt-oss models are reasoning models. The training compute of 2.1 million H100 GPU hours is roughly on par with the 2.788 million H800 GPU hours that the ~5.6x larger </span><a href="https://arxiv.org/abs/2412.19437" rel="">DeepSeek V3</a><span> model was trained for. Unfortunately, there is no information about the Qwen3 training time available yet.</span></p><p>Interestingly, the GPT-oss training hour estimate includes both the supervised learning for instruction following and the reinforcement learning for reasoning, whereas DeepSeek V3 is just a pre-trained base model on top of which DeepSeek R1 was trained separately.</p><p>As mentioned in the previous section, the gpt-oss models are reasoning models. However, what's particularly interesting is that they were trained so that users can easily control the degree of reasoning via inference time scaling.</p><p>Concretely, gpt-oss models can receive "Reasoning effort: low/medium/high" instructions as part of their system prompt, which directly affects the response length and accuracy, as shown in Figure 21.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LsLL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png" width="1219" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1219,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:175317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Figure 21: Response length and quality of gpt-oss models under different reasoning efforts (annotated figure from the </span><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf" rel="">model card</a><span>)</span></figcaption></figure></div><p>This level of adjustability is useful because it lets us balance cost, compute, and accuracy. For example, if the task is simple, such as answering a straightforward knowledge question or fixing a small typo, we can skip extended reasoning. This saves time and resources while avoiding unnecessarily long responses and verbose reasoning traces.</p><p>It is somewhat unfortunate that OpenAI did not release the base models prior to reinforcement learning-based reasoning training, unlike Qwen3 or OLMo. Base models are particularly valuable starting points for researchers working on reasoning methods (which is one reason I currently like working with Qwen3 Base). My guess is that OpenAI's decision was driven more by industry and production use cases than by research considerations.</p><p><span>Note that the original Qwen3 models also have a toggle for enabling/disabling thinking (reasoning) modes (via a </span><code>enable_thinking=True/False</code><span> setting in the tokenizer that simply adds &lt;think&gt;&lt;/think&gt; tags to disable the reasoning behavior). However, the Qwen3 team updated their models in the last few weeks and moved away from the hybrid model towards dedicated Instruct/Thinking/Coder variants.</span></p><p>The reason was that the hybrid mode resulted in lower performance compared to the individual models:</p><blockquote><p><span>After discussing with the community and reflecting on the matter, we have decided to abandon the hybrid thinking mode. We will now train the Instruct and Thinking models separately to achieve the best possible quality. </span><a href="https://www.actuia.com/en/news/alibaba-launches-qwen3-235b-a22b-instruct-2507-and-breaks-away-from-hybrid-reasoning/?utm_source=chatgpt.com" rel="">Source</a></p></blockquote><p>One interesting surprise is that OpenAI released the gpt-oss models with an MXFP4 quantization scheme for the MoE experts.</p><p>Quantization formats used to be a niche topic, mostly relevant to mobile or embedded AI, but that's changed with the push toward bigger models. In this case, the MXFP4 optimization allows the model to run on single GPU devices.</p><p>Here’s what that looks like in practice:</p><ul><li><p>The large model (think 120B) fits on a single 80GB H100 or newer GPU. Not consumer hardware, but hey, it's much cheaper to rent a 1-H100 machine than a multi-H100 machine. Plus, we don't have to worry about distributing the model across GPUs and adding communication overhead. It's really nice that AMD MI300X cards are supported from day 1 as well!</p></li><li><p>The smaller 20B model even fits into 16 GB of VRAM; the caveat is that it has to be a RTX 50-series GPU or newer to support MXFP4.</p></li></ul><p>Note that the models will also run on older hardware but without MXFP4 support and will thus consume more RAM. Without MXFP4 optimization, the models in bfloat16 will consume more like 48 GB (gpt-oss-20b) and 240 GB (gpt-oss-120b).</p><p>By the way, I can run the gpt-oss-20b model comfortably on my Mac Mini using ollama. It uses about 13.5 Gb or memory, which is really reasonable.</p><p><span>The models are still a bit too new for independent benchmarks. Checking the </span><a href="https://lmarena.ai/leaderboard" rel="">LM Arena leaderboard</a><span>, I found that gpt-oss is not listed, yet. So, Qwen3-Instruct remains the top open-weight model, according to users on the LM Arena, for now (Figure 22).</span></p><p>Looking at a reasoning benchmarks provide in the gpt-oss announcement post, we can see that the gpt-ossmodels are on par with OpenAI's proprietary models as well as Qwen3 (Figure 23).</p><p>However, this should be caveated by the fact that gpt-oss-120b is almost half the size of the Qwen3 A235B-A22B-Thinking-2507 model and can run on a single GPU.</p><p>Benchmark performance, however, does not always reflect real-world usability. In my limited use over the past few days, I have found gpt-oss to be quite capable. That said, as others have observed, it does seem to have a relatively high tendency to hallucinate (a point also mentioned in its model card).</p><p>This may stem from its heavy training focus on reasoning tasks such as math, puzzles, and code, which could have led to some "general knowledge forgetting." Still, because gpt-oss was designed with tool use in mind, this limitation may become less relevant over time. Tool integration in open-source LLMs is still in its early stages, but as it matures, I expect that we increasingly let models consult external sources (like search engines) when answering factual or knowledge-based queries.</p><p>If that happens, it could be sensible to prioritize reasoning capacity over memorization. This is much like in human learning in school (or in life in general), where problem-solving skills often matter more than memorizing facts.</p><p>OpenAI had a busy week and released the long-awaited GPT-5 model shortly after gpt-oss. The GPT-5 release was interesting. And if there's one thing I have to say here, it's that I am really surprised by how good their open-source models really are compared to their best product offering in terms of benchmark performance (Figure 24).</p><p>All in all, even though some people called the release overhyped, I am glad that we have a new set of really strong open weight models that are not too far behind the best proprietary ones. Of course, benchmarks often do not accurately reflect real-world use, and it is still too early to tell based on the limited usage. But I think these are good times for people who like to work with open-weight and local (or privately hosted) models.</p><p><em>This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:</em></p><ul><li><p><em><strong><a href="https://amzn.to/4fqvn0D" rel="">Grab a copy of my book</a></strong><span>. Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://www.manning.com/livevideo/master-and-build-large-language-models" rel="">Check out the video course</a></strong><span>. There’s now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.</span></em></p></li></ul><ul><li><p><em><strong><a href="https://magazine.sebastianraschka.com/subscribe" rel="">Subscribe</a></strong><span>. A paid subscription helps to make my writing sustainable and gives you access to additional contents.</span></em></p></li></ul><p><em>Thanks for reading, and for helping support independent research!</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg" width="1456" height="878" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Engineering.fyi – Search across tech engineering blogs in one place (269 pts)]]></title>
            <link>https://engineering.fyi/</link>
            <guid>44855157</guid>
            <pubDate>Sun, 10 Aug 2025 13:44:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fyi/">https://engineering.fyi/</a>, See on <a href="https://news.ycombinator.com/item?id=44855157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-slot="card"><p>How Airbnb upgrades tens of thousands of pods on dozens of Kubernetes clusters to new Istio versions</p></div><div data-slot="card"><p>MCP UI extends the Model Context Protocol to enable AI agents to return fully interactive UI components. It solves the critical challenge that commerce experiences require visual and interactive elements like product selectors, image galleries, and cart flows. This open-source protocol allows agents to embed commerce components while maintaining control through an intent-based messaging system, delivering shopping experiences that go far beyond traditional text-only AI interactions.</p></div><div data-slot="card"><p>What if you could control any device using only subtle hand movements? New research from Meta’s Reality Labs is pointing even more firmly toward wrist-worn devices using surface electromyography (s…</p></div><div data-slot="card"><div data-slot="card-header"><p>The Google Developer Program is rolling out major updates to make its tools and community more accessible and powerful. These enhancements include a new flexible monthly subscription tier, a centralized GDP Forum for collaboration, and increased Gemini CLI access for all members.</p></div><div data-slot="card-content"><p><span>Chris Demeke, Kevin Flores</span></p></div></div><div data-slot="card"><p>Working Together to Accelerate AI Adoption</p></div><div data-slot="card"><p>FlashList v2 is a complete rewrite, delivering faster load times, improved scrolling performance, and precise rendering without requiring item size estimates. It powers thousands of lists in the Shopify mobile app and is now production-ready.</p></div><div data-slot="card"><p>Google introduces Veo 3 Fast, an optimized model for speed and price, along with new image-to-video capabilities for both Veo 3 and Veo 3 Fast, enabling developers to efficiently create high-quality video content from text or still images, with varying pricing based on the model and audio inclusion, now available in the Gemini API.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Gemini Embedding model enhances AI applications, particularly through context engineering, which is being successfully adopted by various organizations across industries to power context-aware systems, leading to significant improvements in performance, accuracy, and efficiency.</p></div><div data-slot="card-content"><p><span>Vishal Dharmadhikari, Janie Zhang</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>LangExtract is a new open-source Python library powered by Gemini models for extracting structured information from unstructured text, offering precise source grounding, reliable structured outputs using controlled generation, optimized long-context extraction, interactive visualization, and flexible LLM backend support.</p></div><div data-slot="card-content"><p><span>Akshay Goel, Atilla Kiraly</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Max's journey introduces LQRax, a JAX-native LQR solver, which exemplifies the growing JAX robotics ecosystem that includes tools like Brax, MJX, and JaxSim, highlighting the benefits of JAX for computational efficiency in optimal control and simulation, and for seamlessly integrating model-based and learning-based approaches.</p></div><div data-slot="card-content"><p><span>Srikanth Kilaru, Max Muchen Sun</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>ExecuTorch is the PyTorch inference framework for edge devices developed by Meta with support from industry leaders like Arm, Apple, and Qualcomm.&nbsp; Running machine learning (ML) models on-device is…</p></div><div data-slot="card-content"><p><span>PyTorch Edge Team in collaboration with Family of Apps</span></p></div></div><div data-slot="card"><p>How to achieve high availability with distributed databases on Kubernetes</p></div><div data-slot="card"><div data-slot="card-header"><p>Co-hosted by Ashley Oldacre and Christina Warren, People of AI podcast's Season 5 will focus on the builders in the space of AI, highlighting the unique journeys, challenges, and triumphs of these innovators.</p></div><div data-slot="card-content"><p><span>Ashley Oldacre, Christina Warren</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Opal is a new experimental tool from Google Labs that helps you compose prompts into dynamic, multi-step mini-apps using natural language, removing the need for code, allowing users to build and deploy shareable AI apps with powerful features and seamless integration with existing Google tools.</p></div><div data-slot="card-content"><p><span>Ali Modarres, Bill Byrne, Paul Lewis</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Apigee helps enterprises integrate large language models (LLMs) into existing API ecosystems securely and scalably, addressing challenges like authentication and authorization not fully covered by the evolving Model Context Protocol (MCP), and offering an open-source MCP server example that demonstrates how to implement enterprise-ready API security for AI agents.</p></div><div data-slot="card-content"><p><span>Antony Arul, Ruben Gonzalez</span></p></div></div><div data-slot="card"><p>Automatically review your PRs with Bugbot</p></div><div data-slot="card"><div data-slot="card-header"><p>New AI capabilities for popular frameworks in Firebase Studio include AI-optimized templates, streamlined integration with Firebase backend services, and the ability to fork workspaces for experimentation and collaboration, making AI-assisted app development more intuitive and faster for developers worldwide.</p></div><div data-slot="card-content"><p><span>Jeanine Banks, Vikas Anand</span></p></div></div><div data-slot="card"><p><span>Lavanya Verma, Ryan Hang, Sung Whang, Joseph Wang</span></p></div><div data-slot="card"><div data-slot="card-header"><p>Gemini 2.5 Flash-Lite, previously in preview, is now stable and generally available. This cost-efficient model is ~1.5x faster than 2.0 Flash-Lite and 2.0 Flash, offers high quality, and includes 2.5 family features like a 1 million-token context window and multimodality.</p></div><div data-slot="card-content"><p><span>Logan Kilpatrick, Zach Gleicher</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>Gemini's advanced capability for conversational image segmentation allows intuitive interaction with visual data by understanding complex phrases, conditional logic, and abstract concepts, streamlining developer experience and opening doors for new applications in media editing, safety monitoring, and damage assessment.</p></div><div data-slot="card-content"><p><span>Paul Voigtlaender, Valentin Gabeur, Rohan Doshi</span></p></div></div><div data-slot="card"><p><span>Austin Harrison, Eddie Huang, Spencer Garth, Tim Ross, Taya Yusuf</span></p></div><div data-slot="card"><p>ChatGPT now thinks and acts, proactively choosing from a toolbox of agentic skills to complete tasks for you using its own computer.</p></div><div data-slot="card"><div data-slot="card-header"><p>Veo 3, Google’s latest AI video generation model, is now available in paid preview via the Gemini API and Google AI Studio. Unveiled at Google I/O 2025, Veo 3 can generate both video and synchronized audio, including dialogue, background sounds, and even animal noises. This model delivers realistic visuals, natural lighting, and physics, with accurate lip syncing and sound that matches on-screen action.</p></div><div data-slot="card-content"><p><span>Alisa Fortin, Luciano Martins, Seth Odoom</span></p></div></div><div data-slot="card"><p>Meta has developed an open-source AI tool to design concrete mixes that are stronger, more sustainable, and ready to build with faster—speeding up construction while reducing environmental impact. …</p></div><div data-slot="card"><p>Shopify’s Global Catalogue demonstrates the impact of multimodal LLMs on one of commerce’s hardest problems: building a unified, structured, and continuously evolving understanding of billions of product listings created by millions of merchants.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research.</p></div><div data-slot="card-content"><p><span>Srikanth Kilaru, David Hall</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>The updated Agent Development Kit (ADK) simplifies and accelerates the process of building AI agents by providing the CLI with a deep, cost-effective understanding of the ADK framework, allowing developers to quickly ideate, generate, test, and improve functional agents through conversational prompts, eliminating friction and keeping them in a productive "flow" state.</p></div><div data-slot="card-content"><p><span>Julia Wiesinger, Hangfei Lin</span></p></div></div><div data-slot="card"><p>The `logprobs` feature has been officially introduced in the Gemini API on Vertex AI, provides insight into the model's decision-making by showing probability scores for chosen and alternative tokens. This step-by-step guide will walk you through how to enable and interpret this feature and apply it to powerful use cases such as confident classification, dynamic autocomplete, and quantitative RAG evaluation.</p></div><div data-slot="card"><p>Microsoft’s AI-powered code review assistant has transformed pull request workflows by automating routine checks, suggesting improvements, and enabling conversational Q&amp;A, leading to faster PR completion, improved code quality, and enhanced developer onboarding.</p></div><div data-slot="card"><p>The Gemini Embedding text model is now generally available in the Gemini API and Vertex AI. This versatile model has consistently ranked #1 on the MTEB Multilingual leaderboard since its experimental launch in March, supports over 100 languages, has a 2048 maximum input token length, and is priced at $0.15 per 1M input tokens.</p></div><div data-slot="card"><div data-slot="card-header"><p>The Apigee API hub and Developer Portals are distinct but interconnected parts of the Apigee platform that help organizations discover and manage APIs for different personas, unlocking API potential and accelerating innovation.</p></div><div data-slot="card-content"><p><span>Venkat Sadras, David Rush</span></p></div></div><div data-slot="card"><p>Our new jurisdiction resolution system (JRS) is a faster, less resource-intensive solution to the challenging problem of determining tax obligations in places with complicated, overlapping tax jurisdictions.</p></div><div data-slot="card"><div data-slot="card-header"><p>GenAI Processors is a new open-source Python library from Google DeepMind designed to simplify the development of AI applications, especially those handling multimodal input and requiring real-time responsiveness, by providing a consistent "Processor" interface for all steps from input handling to model calls and output processing, for seamless chaining and concurrent execution.</p></div><div data-slot="card-content"><p><span>Andre Elisseeff, Alexey Guseynov, Oskar Bunyan, Shrestha Basu Mallick</span></p></div></div><div data-slot="card"><p>Updates in Firebase Studio include new Agent modes, foundational support for the Model Context Protocol (MCP), and Gemini CLI integration, all designed to redefine AI-assisted development allow developers to create full-stack applications from a single prompt and integrate powerful AI capabilities directly into their workflow.</p></div><div data-slot="card"><div data-slot="card-header"><p>T5Gemma is a new family of encoder-decoder LLMs developed by converting and adapting pretrained decoder-only models based on the Gemma 2 framework, offering superior performance and efficiency compared to its decoder-only counterparts, particularly for tasks requiring deep input understanding, like summarization and translation.</p></div><div data-slot="card-content"><p><span>Biao Zhang, Paul Suganthan, Ben Hora</span></p></div></div><div data-slot="card"><div data-slot="card-header"><p>The new batch mode in the Gemini API is designed for high-throughput, non-latency-critical AI workloads, simplifying large jobs by handling scheduling and processing, and making tasks like data analysis, bulk content creation, and model evaluation more cost-effective and scalable, so developers can process large volumes of data efficiently.</p></div><div data-slot="card-content"><p><span>Lucia Loher, Vishal Dharmadhikari</span></p></div></div><div data-slot="card"><p>Commerce is a dynamic ecosystem where our mission is to empower every merchant to succeed. We optimize each step of their journey—from product creation to customer delivery—using advanced tools, infrastructure, and partnerships to solve a complex optimization challenge.</p></div><div data-slot="card"><p>How the new Pro plan works and why we changed our pricing.</p></div><div data-slot="card"><p><span>Prateek Jain, Soheil Sadeghi, Mehrdad Bakhtiari</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Try and (396 pts)]]></title>
            <link>https://ygdp.yale.edu/phenomena/try-and</link>
            <guid>44855079</guid>
            <pubDate>Sun, 10 Aug 2025 13:32:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ygdp.yale.edu/phenomena/try-and">https://ygdp.yale.edu/phenomena/try-and</a>, See on <a href="https://news.ycombinator.com/item?id=44855079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><strong><br>
		I'm gonna try and change the course of hip hop again.<br>
	</strong>
</p>
<p>
		(Dr. Dre)
	</p>

<p>
	Typically, <i>try</i> can be followed by three kinds of phrases: a noun phrase (1a), an infinitival verb phrase with <i>to</i> (1b), or a verb phrase with -<i>ing</i> (1c).
</p>
<blockquote><p>
		1)	a. I'll try the salad.
	</p>
<p id="indented">
		b. I'll try to eat this horrible salad.
	</p>
<p id="indented">
		c. I'll try adding vinegar to the salad, to improve the taste.
	</p>
</blockquote>
<p>
	However, <i>try</i> can also combine with the conjunction <i>and</i>, followed by a bare verb form:
</p>
<blockquote><p>
		2)	I’ll try and eat the salad.
	</p>
</blockquote>
<p>
	This usage is very similar in meaning to <i>try to</i>, if not identical, but is deemed prescriptively incorrect (Routledge 1864:579 in D. Ross 2013a:120; Partridge 1947:338, Crews et al. 1989:656 in Brook &amp; Tagliamonte 2016:320). In the next few sections, we will see that it has a number of interesting properties.
</p>


<h2>Who says this?</h2>
<p>
	<i>Try and</i> is described as more prevalent in British English than American English, but is common in both varieties (Hommerberg &amp; Tottie 2007). Brook &amp; Tagliamonte (2016) show that Canadian speakers pattern with American speakers in their usage of the construction.
</p>
<p>
	<i>Try and</i> is not a recent innovation – it first emerged in the late 1500s, although the earliest textual attestion is from 1390 (Tottie 2012, D. Ross 2013a). Tottie (2012) provides some examples of <i>try and</i> from EEBO-TCP corpus, including this one:
</p>
<blockquote><p>
		3) ...howe and by what certaine and generall rule I mighte <b>trye and</b> throughly discerne the veritie of the catholike faithe, from the falsehood of wicked heresye... (1554)<br>
		4) You maie (saide I) <b>trie and</b> bring him in, and shewe him to her. (1569)
	</p>
</blockquote>
<p>
	<i>Webster’s Dictionary</i> (1989:919) suggests that <i>try and</i> in fact predates <i>try to</i>, and this conclusion is supported by Hommerberg &amp; Tottie (2007:60), Tottie &amp; Hoffman (2011) and Tottie (2012). However, D. Ross (2013a) disputes this, saying that “<i>[t]ry and</i> and <i>try to</i> developed simultaneously and independently”. What is clear is that <i>try and</i> has been around for at least as long as <i>try to</i>.
</p>

<h2>Syntactic Properties</h2>
<p>
	Carden &amp; Pesetsky (1977:86) note that <i>try and</i> does not behave like a regular case of coordination. </p>
<h3>Question words are allowed</h3>
<p>One property of ‘true’ coordination is that it is subject to the <i>Coordinate Structure Constraint</i> (J. Ross 1967), which states that a <i>wh</i>-word cannot move out of one of the conjuncts. This is shown in (5).
</p>
<blockquote><p>
		5)	a. Mary [met Bill and ignored Susie].
	</p>
<p id="indented">
		b. *Who did Mary [meet Bill and ignore __]?
	</p>
</blockquote>
<p>
	However, a <i>wh</i>-word can happily be moved out of a <i>try and structure:
</i></p>
<blockquote><p>
		6) Who did Mary [try and talk to __]?
	</p>
</blockquote>
<h3>No reordering</h3>
<p>
	A second property of pseudo-coordination that distinguishes it from regular coordination is that the two conjuncts cannot be reordered. In (6), we see that regular coordination permits the order of conjuncts to be changed, while in (7) we see that the same is not possible with <i>try and</i> (De Vos 2005:59).</p>
<blockquote><p>
		7)	a. John will wash the bathroom and kill mosquitos.
	</p>
<p id="indented">
		b.	John will kill mosquitos and wash the bathroom.
</p></blockquote>
<blockquote><p>
		8)	a. John will try and kill mosquitos.
	</p>
<p id="indented">
		b.	*John will kill mosquitos and try.
</p></blockquote>
<h3><em>Both</em> is not possible</h3>
<p>
	Another piece of evidence that <i>try and</i> is not regular coordination structure comes from the unavailability of <i>both</i>. Usually, coordinated verb phrases can be preceded by <i>both</i>:</p>
<blockquote><p>
		9)	<i>Reality is Broken</i> will both [stimulate your brain and stir your soul]. [<a href="https://janemcgonigal.com/my-book/">source</a>, February 28 2017]
	</p>
</blockquote>
<p>
However, De Vos (2005:59) points out that <i>try and</i> may not be preceded by <i>both</i>:
</p>
<blockquote><p>
		10)	a. John will try and kill mosquitos.
	</p>
<p id="indented">
		b. *John will both try and kill mosquitos.
	</p>
</blockquote>
<h3>Bare form only</h3>
<p>
Unlike with regular coordination, <i>try and</i> is available only when both <i>try</i> and the verb following <i>and</i> are uninflected, which means it must occur in its bare form. Carden &amp; Pesetsky (1977)  call this the <i>bare form condition</i>. The following examples are adapted from D. Ross (2013a:111):
</p>
<blockquote><p>
		11)	a. I will try and finish the assignment.
	</p>
<p id="indented">
		b. I try and finish an assignment every day.
	</p>
<p id="indented">
		c. *I tried and finish(ed) the assignment.
	</p>
<p id="indented">
		d. *He tries and finish(es) an assignment every day.
	</p>
<p id="indented">
		e. *It’s tough when you’re trying and finish(ing) an assignment under pressure.
	</p>
</blockquote>
<h3>Dialect variation in the Bare Form Condition</h3>
<p>
	Is the bare form condition universal? D. Ross (2013a:124-5) notes that it has weakened in some dialects, though not necessarily in the same way. In dialects of Northeastern Canada, parallel inflected forms are acceptable:
</p>
<blockquote><p>
		12)	They tries and does that.
	</p>
</blockquote>
<p>
	In South African English, on the other hand, <i>try</i> may be inflected while the second verb remains a bare form (examples from D. Ross 2013a:125):
</p>
<blockquote><p>
		13)	a. Noeleen tries and find answers and solutions. [<a href="http://www.tvsa.co.za/default.asp?blogname=coming_up_on_3Talk&amp;ArticleID=2903">source</a>, August 2006]
	</p>
<p id="indented">
		b. We’re trying and get across that nature is harsh but not necessarily full of malice and cruelty. (Dereck Joubert on “Wild about Africa,” Carte Blanche: March 18, 2007)
	</p>
</blockquote>
<h3>No separation of <i>try</i> and <i>and</i></h3>
<p>
	There are some other restrictions on the distribution of try and. Unlike with <i>try to</i>, <i>try</i> may not be separated from <i>and</i> by an adverb (Webster’s Dictionary 1989:919):
</p>
<blockquote><p>
		14)	a. Try always to tell the truth.
	</p>
<p id="indented">
		b. *Try always and tell the truth.
	</p>
</blockquote>
<p>
	Similarly, <i>try</i> may not be separated from <i>and</i> by negation (Brook &amp; Tagliamonte 2016:308):
</p>
<blockquote><p>
		15)	a. You try not to let it bother you.
	</p>
<p id="indented">
		b. *You try not and let it bother you.
	</p>
</blockquote>
<h3>No ellipsis allowed</h3>
<p>
	<i>Try and</i> is incompatible with ellipsis of the following verb phrase (Brook &amp; Tagliamonte 2016):
</p>
<blockquote><p>
		16)	a. Sure, I'll try to.
	</p>
<p id="indented">
		b. *Sure, I'll try and.
</p></blockquote>

<h2>Other instances of pseudocoordination</h2>
<p>
	Infinitival <i>to</i> can be replaced by <i>and</i> in several other cases, subject to dialectal and individual variation. Brook &amp; Tagliamonte (2016:302) state that the best candidate for a verb phrase that behaves like <i>try</i> is <i>be sure</i>:
</p>
<blockquote><p>
		17)	Be sure and visit Harry tomorrow. (Carden &amp; Pesetsky 1977:84)
	</p>
</blockquote>
<p>
	D. Ross (2013a:122) provides several examples of other verb phrases in which infinitival <i>to</i> has been replaced with <i>and</i>:
</p>
<blockquote><p>
		18)	a. <b>Mind and</b> get all right for next Saturday. (Poutsma 1905:361)
	</p>
<p id="indented">
		b. You know I go to all these different schools and I <b>start and</b> get mixed up after a while. (Hopper 2002:162)
	</p>
<p id="indented">
		c. <b>Remember and</b> wash your hair. (BNC: KE4 636, 1992)
	</p>
</blockquote>
<p>
	Another instance of pseudocoordination is found with motion verbs, such as <i>come</i> and <i>go</i>:
</p>
<blockquote><p>
		19)	a. Can you come and pick me up from the station?
	</p>
<p id="indented">
		b. I’ll go and get the mop.
	</p>
</blockquote>
<p>
	D. Ross (2013b) argues that motion verb pseudocoordination has a different syntax and semantics from <i>try and</i> pseudocoordination. Syntactically, we can see that motion verb pseudocoordination is <i>not</i> subject to the bare form condition:
</p>
<blockquote><p>
		20)	a. He came and <b>picked</b> me up from the station.
	</p>
<p id="indented">
		b. She goes and <b>gets</b> lunch every day at noon.
	</p>
</blockquote>
<p>
	Semantically, <i>go and</i> entails that the event was completed, so in (21) below it is strange to use <i>go and</i> if the book was not acquired. In contrast, its non-pseudocoordination equivalent <i>go to</i> does not have this entailment.
</p>
<blockquote><p>
		21)	The man will go to/*and buy the book, even if it is sold out.
	</p>
</blockquote>
<p><em>Page contributed by Matthew Tyler on Feb 23, 2018.</em></p>
<p><em>Updates/revisions: June 27, 2018 (Katie Martin)</em></p>
<p><b>Please cite this page as:</b> Tyler, Matthew. 2018. <em>Try and</em>. <i>Yale Grammatical Diversity Project: English in North America</i>. (Available online at <a href="http://ygdp.yale.edu/phenomena/try-and">http://ygdp.yale.edu/phenomena/try-and</a>. Accessed on YYYY-MM-DD). Updated by Katie Martin (2018).</p>
<h2>References</h2>
</div><div><p>Phenomenon Dialect:&nbsp;</p><div><p>Widespread American English</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MCP: An (Accidentally) Universal Plugin System (150 pts)]]></title>
            <link>https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin</link>
            <guid>44854860</guid>
            <pubDate>Sun, 10 Aug 2025 12:53:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin">https://worksonmymachine.ai/p/mcp-an-accidentally-universal-plugin</a>, See on <a href="https://news.ycombinator.com/item?id=44854860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>There's this thing about USB-C that nobody really talks about. Not the part where we all had to buy new dongles (RIP my dongle drawer, 2010-2023). The other part.</p><p>See, we all thought USB-C was just going to be about charging things and moving files around like the other USBs. Very serious. Very purposeful. But because of the way it is it can do... other things.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kwfs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kwfs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1960400,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kwfs!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 424w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 848w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!kwfs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01350f9f-9ecd-4cb2-a28d-103a9ccc9027_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>My friend Rex connected his toaster to his monitor last week. I don't know why. The toaster doesn't know why. But it </span><em>worked</em><span>, and now Rex's toast has HDMI output.</span></p><p>Remember car cigarette lighters? Nobody uses them for cigarettes anymore. They're just universal power outlets that happen to be shaped like something from 1952. Your car doesn't care if you're charging a phone or running a personal pizza oven. The hole is the same size. The power is there.</p><p><em>The protocol doesn't judge your life choices.</em></p><p>This brings me to something I discovered about MCP (Model Context Protocol) while trying to make my calendar app order takeout. Stay with me here.</p><p>Everyone thinks MCP is for making AI assistants smarter. You know, "Claude, please read my files and understand my soul." And sure, it does that. But here's what they put in the documentation that made me spit out my morning tea:</p><blockquote><p>"MCP provides a standardized way to connect AI models to different data sources and tools."</p></blockquote><p><span>Okay but. </span><em>But</em><span>. What if you just... removed the AI part?</span></p><p><span>What if it's just "a standardized way to connect </span><s>AI models</s><span> </span><strong>literally anything</strong><span> to different data sources and tools"?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MBI9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MBI9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 424w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 848w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1272w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png" width="1456" height="954" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:954,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:554311,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MBI9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 424w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 848w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1272w, https://substackcdn.com/image/fetch/$s_!MBI9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca785d13-391f-42fa-8109-7ae384ec5e99_1836x1203.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Or remember when someone looked at NFTs—which were supposed to just </span><em>point</em><span> at images—and thought "what if the pointer... WAS the image?"</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!C2qU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!C2qU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 424w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 848w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png" width="1456" height="475" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:475,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:417583,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!C2qU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 424w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 848w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!C2qU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77d811eb-b5b3-44be-9a9d-e88ba57c7b38_3072x1002.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>For those of you who don’t get the idea, copy and paste this into your url bar: data:application/json;base64,eyJuYW1lIjogIkJhZyAjNzQ4IiwgImRlc2NyaXB0aW9uIjogIkxvb3QgaXMgcmFuZG9taXplZCBhZHZlbnR1cmVyIGdlYXIgZ2VuZXJhdGVkIGFuZCBzdG9yZWQgb24gY2hhaW4uIFN0YXRzLCBpbWFnZXMsIGFuZCBvdGhlciBmdW5jdGlvbmFsaXR5IGFyZSBpbnRlbnRpb25hbGx5IG9taXR0ZWQgZm9yIG90aGVycyB0byBpbnRlcnByZXQuIEZlZWwgZnJlZSB0byB1c2UgTG9vdCBpbiBhbnkgd2F5IHlvdSB3YW50LiIsICJpbWFnZSI6ICJkYXRhOmltYWdlL3N2Zyt4bWw7YmFzZTY0LFBITjJaeUI0Yld4dWN6MGlhSFIwY0RvdkwzZDNkeTUzTXk1dmNtY3ZNakF3TUM5emRtY2lJSEJ5WlhObGNuWmxRWE53WldOMFVtRjBhVzg5SW5oTmFXNVpUV2x1SUcxbFpYUWlJSFpwWlhkQ2IzZzlJakFnTUNBek5UQWdNelV3SWo0OGMzUjViR1UrTG1KaGMyVWdleUJtYVd4c09pQjNhR2wwWlRzZ1ptOXVkQzFtWVcxcGJIazZJSE5sY21sbU95Qm1iMjUwTFhOcGVtVTZJREUwY0hnN0lIMDhMM04wZVd4bFBqeHlaV04wSUhkcFpIUm9QU0l4TURBbElpQm9aV2xuYUhROUlqRXdNQ1VpSUdacGJHdzlJbUpzWVdOcklpQXZQangwWlhoMElIZzlJakV3SWlCNVBTSXlNQ0lnWTJ4aGMzTTlJbUpoYzJVaVBsTm9iM0owSUZOM2IzSmtQQzkwWlhoMFBqeDBaWGgwSUhnOUlqRXdJaUI1UFNJME1DSWdZMnhoYzNNOUltSmhjMlVpUGtScGRtbHVaU0JTYjJKbElHOW1JSFJvWlNCR2IzZzhMM1JsZUhRK1BIUmxlSFFnZUQwaU1UQWlJSGs5SWpZd0lpQmpiR0Z6Y3owaVltRnpaU0krU0c5dlpEd3ZkR1Y0ZEQ0OGRHVjRkQ0I0UFNJeE1DSWdlVDBpT0RBaUlHTnNZWE56UFNKaVlYTmxJajVRYkdGMFpXUWdRbVZzZER3dmRHVjRkRDQ4ZEdWNGRDQjRQU0l4TUNJZ2VUMGlNVEF3SWlCamJHRnpjejBpWW1GelpTSStSR2wyYVc1bElGTnNhWEJ3WlhKelBDOTBaWGgwUGp4MFpYaDBJSGc5SWpFd0lpQjVQU0l4TWpBaUlHTnNZWE56UFNKaVlYTmxJajVEYUdGcGJpQkhiRzkyWlhNOEwzUmxlSFErUEhSbGVIUWdlRDBpTVRBaUlIazlJakUwTUNJZ1kyeGhjM005SW1KaGMyVWlQazVsWTJ0c1lXTmxQQzkwWlhoMFBqeDBaWGgwSUhnOUlqRXdJaUI1UFNJeE5qQWlJR05zWVhOelBTSmlZWE5sSWo1VWFYUmhibWwxYlNCU2FXNW5QQzkwWlhoMFBqd3ZjM1puUGc9PSJ9</figcaption></figure></div><p>The protocol meant for storing references became a protocol for storing reality. It's like using a library card as the actual book.</p><p><span>Here's where it gets even better. The more MCP servers people build for AI, the more capabilities </span><em>every</em><span> app can have. It's like:</span></p><ol><li><p>Someone builds an MCP server for their AI to access Spotify</p></li><li><p>Your workout app can now generate playlists</p></li><li><p>You didn't write any Spotify code</p></li><li><p>The Spotify MCP developer doesn't know your app exists</p></li><li><p>Everyone wins?</p></li></ol><p>It's like a potluck where everyone brings their specialty dish, but instead of food, it's functionality. And instead of eating, you're... actually, this metaphor is falling apart. But you get it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Laga!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Laga!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 424w, https://substackcdn.com/image/fetch/$s_!Laga!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 848w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1272w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png" width="1456" height="1132" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1132,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:629338,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Laga!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 424w, https://substackcdn.com/image/fetch/$s_!Laga!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 848w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1272w, https://substackcdn.com/image/fetch/$s_!Laga!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fea5180-6f0c-4642-8a2f-9c2f6ab5e6ab_1980x1539.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The beautiful chaos is that every MCP server built for Claude or ChatGPT or whatever becomes a free plugin for </span><em>anything</em><span> that speaks MCP. It's accidentally creating a universal plugin ecosystem. Nobody planned this (I don’t think). It's just happening.</span></p><p>They keep saying MCP is like USB-C for AI. But what does that actually mean?</p><p><span>USB-C isn't special because it's a port. It's special because it's a </span><em>possibility space</em><span>. It's a hole that says "put something here and we'll figure it out." Power? Sure. Data? Why not. Video? Apparently yes. Toaster control protocols? Rex says absolutely.</span></p><p>MCP is the same thing but for functionality. It's not saying "I'm for AI." It's saying "I'm a well-designed hole. Put something here."</p><p><span>So we’re building this thing called </span><strong>APM</strong><span> (</span><a href="https://actionsperminute.io/" rel="">Actions Per Minute</a><span>). On paper, it's a task management app. In reality? It's a shape-shifter that becomes whatever you plug into it.</span></p><p>The entire plugin system? Just MCP servers.</p><p>Want spell check? MCP server.  </p><p>Want it to order coffee when you complete 10 tasks? MCP server.  </p><p>Want your AI agents to respond like peons from Warcraft 3 when you assign them a task? Of course you do, and that MCP server is already written and ready to use.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cfIp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cfIp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 424w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 848w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1272w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png" width="1456" height="1443" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e3871464-0023-4b56-8235-fad217a9a232_3045x3018.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1443,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1310775,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://worksonmymachine.substack.com/i/166947463?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cfIp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 424w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 848w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1272w, https://substackcdn.com/image/fetch/$s_!cfIp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3871464-0023-4b56-8235-fad217a9a232_3045x3018.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Every great protocol gets used for something its creators never imagined:</p><ul><li><p>HTTP was for academic papers. Now it runs civilization.</p></li><li><p>Bluetooth was for hands-free calling. Now it unlocks your front door.</p></li><li><p>USB was for keyboards and mice. Now it charges your emotional support portable fan.</p></li></ul><p>MCP thinks it's for giving context to AI models.</p><p>But really? It's just a really good protocol for making things talk to other things.</p><p>And in a world where Rex's toast has HDMI output, maybe that's exactly what we need.</p><p>---</p><p><strong>P.S.</strong><span> If you build an MCP server that makes your computer emit the smell of fresh bread, we need to talk.</span></p><p><strong>P.P.S.</strong><span> We’ve just opened up early access for APM. Build something weird. Build something useful. Build something that makes us question our life choices. I believe in you.</span></p><p><em>(Somewhere, a protocol is being used exactly as intended. This is deeply suspicious.)</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Booting 5000 Erlangs on Ampere One 192-core (173 pts)]]></title>
            <link>https://underjord.io/booting-5000-erlangs-on-ampere-one.html</link>
            <guid>44854525</guid>
            <pubDate>Sun, 10 Aug 2025 11:41:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underjord.io/booting-5000-erlangs-on-ampere-one.html">https://underjord.io/booting-5000-erlangs-on-ampere-one.html</a>, See on <a href="https://news.ycombinator.com/item?id=44854525">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        
        <small>2025-08-05</small>
        <p>
            Underjord is an artisanal consultancy doing consulting in Elixir, Nerves with an accidental speciality in marketing and outreach. If
            you
            like
            the writing you should really <a href="https://underjord.io/services.html">try the pro version</a>.</p>
        <p>In the previous post on <a href="https://underjord.io/500-virtual-linux-devices-on-arm64.html">500 virtual linux devices on ARM64</a> I hinted that I expected serious improvements if we got KVM working. Well. We’re there. Let’s see what we got going on.</p>
<p><em>Disclosure: I am running a conference called <a href="https://goatmire.com/">Goatmire Elixir</a> which Ampere is a sponsor of. This post is not part of the sponsorship exchange as such. It is prep for my talk for the conference which uses the hardware they lent me. So this is your transparency notice, but fundamentally I am not making comparisons on whether they are better or not. I’m learning and sharing about qemu and virtual Linux machines. Now I’d love if they paid me to shill them a bit later and I’d be transparent about that too. But this is not that :)</em></p>
<p>To recap. We have an Ampere One 192-core machine with 1 TB of RAM. The goal is to run as many virtual Linux IoT devices <em>using the Nerves framework</em>. We got 500 of them last time before I tried pushing any further. I also got a bit further on the same setup when I tried. Maybe 1000, I don’t recall exactly. But there have been developments, so read on!</p>
<p>Briefly on Nerves: the framework treats the BEAM virtual machine like the OS and essentially only uses Linux for a kernel, drivers and the like. This means we can write much if not all of the embedded device in a productive high-level language running on a provenly robust and reliable environment with memory safety and solid recovery strategies. And it means your cloud integration developer doesn’t risk seg-faulting the entire device while mangling JSON back and forth. Nerves also brings some best-practice tooling and conventions. Your init process is <a href="https://github.com/nerves-project/erlinit">erlinit</a>, your updates use <a href="https://github.com/fwup-home/fwup">fwup</a> to provide A/B partitions and factory reset, auto-failback, validation of firmware viability, disk encryption, delta updates, streaming updates and a bunch more.</p>
<p>The most interesting development is the thing you can probably learn the most from. Frank Hunleth who has been my co-conspirator and a massive help saved me from fighting u-boot by .. writing another bootloader. Introducing <a href="https://github.com/fhunleth/little_loader">little_loader</a>. This adorable tractor will load up your ARM64 qemu device, consult the uboot environment that Nerves uses, find a Linux kernel from information in that and then boot. Consequently it enables the A/B upgrade features and everything else that makes Nerves great.</p>
<p>Writing a boot loader is a little bit ridiculous. Frank knows his way around C and apparently ChatGPT knows a fair bit about ARM and qemu. Enough to be dangerous. And where it was wrong he could rummage around until he found the way. How he does what he does is beyond me but the result is a very small boot loader that you can probably read through and understand. So if you are curious about booting ARM64 or about how qemu starts things the code should be a worthwhile read.</p>
<p>We got a bit tangled up in EL1 vs EL2 when we only ever needed EL1 to work. EL2 on ARM is what you’d run under if you want to be able to run VMs in your VMs so you can VM while you VM. And the version of qemu + KVM I got from Ubuntu doesn’t seem to support that. We weren’t interested in it either. At some point we might explore EL3 for secure boot and whatnot. Only time will tell.</p>
<p>One of the weirder challenges and something we haven’t disentangled yet is that we have some compilation issue where using the toolchains I was using the non-debug build would hang while the debug one runs fine. For now I run the debug build of the bootloader. I think it was fine from GCC 15? Anyway, hopefully we pin that down at some point. But it tripped us up a few times when the bootloader would hang due that issue rather than any actual problems with the implementation.</p>
<p>KVM didn’t really require anything extra aside from making sure we didn’t go to EL2. And when we tried it on MacOS it worked great with HVF as well. Host-based ARM64 VMs are ridiculously fast and practical. As in booting to the full IEx prompt in single-digit seconds instead of double-digit. And they use about 500Mb less memory. And see, that’s important. Because we want to shove as many as we can into this server I got access to.</p>
<h2 id="accelerated-on-host">Accelerated on host</h2>
<p>My very hacky project for running this stuff is <a href="https://github.com/lawik/amproj">available here</a>. This code is cribbed from <code>simple.sh</code>:</p>

  <div data-file="simple.sh">
    <p><span>shell</span>
      <span>simple.sh</span>
    </p>
    <div><pre tabindex="0"><code data-lang="shell">  qemu-system-aarch64 <span>\
</span><span></span>	-machine virt,accel<span>=</span>kvm <span>\
</span><span></span>	-cpu host <span>\
</span><span></span>	-smp <span>1</span> <span>\
</span><span></span>	-m 150M <span>\
</span><span></span>	-kernel ../little_loader/little_loader.elf <span>\
</span><span></span>	-netdev user,id<span>=</span>eth0 <span>\
</span><span></span>	-device virtio-net-device,netdev<span>=</span>eth0,mac<span>=</span>de:ad:be:ef:00:01 <span>\
</span><span></span>	-global virtio-mmio.force-legacy<span>=</span>false <span>\
</span><span></span>	-drive <span>if</span><span>=</span>none,file<span>=</span>/space/disks/special.img,format<span>=</span>raw,id<span>=</span>vdisk <span>\
</span><span></span>	-device virtio-blk-device,drive<span>=</span>vdisk,bus<span>=</span>virtio-mmio-bus.0 <span>\
</span><span></span>	-nographic</code></pre></div>
  </div>

<p>To go through it. We use <code>qemu-system-aarch64</code> to emulate an ARM64 machine. <code>aarch64</code> is the common shortname for ARM64, except sometimes on MacOS where I hear it can be <code>arm64</code>. We specify the <code>machine</code> to be <a href="https://www.qemu.org/docs/master/system/arm/virt.html">virt</a>. Previously we’d leave it there but now we use an <a href="https://www.qemu.org/docs/master/system/introduction.html">accelerator</a> named KVM (<a href="https://linux-kvm.org/page/Main_Page">Kernel-based Virtual Machine</a>). It is the virtualization mechanism included with Linux and qemu can integrate with that to accelerate the execution. This also requires <code>-cpu host</code>, meaning, we are no longer trying to emulate a <code>cortex-a53</code> processor. We are trying to run on the host processor, whatever that is. <code>host</code> means that we emulate the host CPU, or at least as much as qemu and the KVM accelerator can support of what the host can do. This is where we drop about 500Mb of memory overhead. We no longer have to have a pretend ARM chip in memory because we have an actual ARM chip to run on. That’s my understanding at least. Would love notes on that.</p>
<p>We only give it 1 core via <code>-smp 1</code> and we give it 150Mb memory with <code>-m 150</code>. Skipping ahead we give it virtual Ethernet and a <a href="https://docs.kernel.org/driver-api/virtio/virtio.html">virtio</a> block storage drive. You’ll see a lot of virt and virtio when doing this stuff. And with <code>-nographic</code> we tell it to not bother trying to pop up a GUI window, so we get our console in the terminal. I’ve done all the work over SSH so that’s definitely my preference.</p>
<p>The disk we provide runs from the raw disk image file <code>special.img</code> which was generated using <code>fwup</code> based on the Nerves project <code>amproj</code> I mentioned earlier. If you build that project with <code>mix firmware</code> you can then run:</p>

  <div>
    <p><span>shell</span>
      
    </p>
    <div><pre tabindex="0"><code data-lang="shell">fwup -a -i amproj.fw -d special.img -t complete</code></pre></div>
  </div>

<p>That’ll give you an image file that contains a full Nerves system. The stuff written to disk is:</p>
<ul>
<li>A uboot env formatted chunk of data. We don’t use uboot this time but we used that format.</li>
<li>A linux kernel, not on a filesystem. Just written to the disk. RAW!</li>
<li>An MBR and some partitions:
<ul>
<li>Root filesystem A (squashfs, read-only)</li>
<li>Root filesystem B (squashfs, read-only)</li>
<li>Application data partition (f2fs, read/write)</li>
</ul>
</li>
</ul>
<p>The uboot env is used to tell the bootloader important things about the A/B upgrade process as well as where to find the kernel to load as well as what <a href="https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html">kernel cmdline</a> to use which is how we tell it what root filesystem to use.</p>
<p>The only config I put into the loader is to set the offset where it can expect the uboot-env and I set that at build-time.</p>
<h2 id="promising-results">Promising results</h2>
<p>NervesCloud received 3389 simultaneous connected devices before the server hit me with the OOM killer. It was probably running a few more but around there. So each VM is:</p>
<ul>
<li>Bootloader</li>
<li>Linux</li>
<li>erlinit</li>
<li>BEAM/ERTS</li>
<li>Nerves base functionality</li>
<li><a href="https://github.com/nerves-hub/nerves_hub_link">NervesHubLink</a> for connecting to NervesHub</li>
</ul>
<p>I have had 3000 devices running stable and then I started to see “fun” challenges. For one thing, our NervesCloud hosts were looking a bit tight on memory because all these devices connect from the US west coast and we were only running a single node in that region. I scaled that up a smidge to make sure I didn’t bother any paying customers.</p>
<p>The VMs are super well-behaved, the Ampere CPU just works. The memory usage is roughly where I’d expect it. 150-250 total I think. There are probably things I can do to make it behave a little more tighter. Will explore that if time allows.</p>
<p>Then I ran my first demo workloads. As the purveyor of the finest Over-the-Air updates for embedded devices we here at NervesCloud.. I kid. But I wanted to shove lots of updates at them and see what that did. The updates process is a lot of compression, decompression and IO. Probably mostly IO-bound but if the devices would be struggling for CPU that’d be noticeable. If the memory usage exploded, that’d be noticed very quickly.</p>
<p>It worked. Not really any problems. I limited the concurrency of the update to 1000 and it couldn’t hand out the updates faster than they completed so it tended to hover around 200-300 concurrent updates happening. Or at least that’s my understanding of what happened. Did I mention the KVM setup is pretty fast?</p>
<p>I logged some issues about UI behavior as I was watching things live and trying to adjust things. It seems like good guy Nate Shoemaker already has a fix in flight for this. There may be more details. When you get a lot of progress reports the LiveView UI perhaps shouldn’t try to refresh all the things all the time.</p>
<h2 id="memory-tuning">Memory tuning</h2>
<p>Frank gave me some tips about tuning Linux memory usage and tuning BEAM memory usage. When I looked into his advice I ended up doing a few things:</p>
<p>For BEAM VM, we <a href="https://github.com/lawik/amproj/commit/cb5919eee5b5c58321cf67aaff388c5db05a1ccc#diff-b9fbf8080c62b37068a3fefe69eeed4d0b7e801049c87b9aaaf721c2d5ed47afR38">change the allocators</a>. This should use less memory and probably trades off in raw performance. Which is fine for this purpose.</p>
<p>Erlang release, <a href="https://github.com/lawik/amproj/commit/cb5919eee5b5c58321cf67aaff388c5db05a1ccc#diff-b9fbf8080c62b37068a3fefe69eeed4d0b7e801049c87b9aaaf721c2d5ed47afR26">use default mode</a> instead of embedded. Which probably makes it boot a bit faster, makes it use less memory but it could lead to surprising delays and growing memory usage later if it loads code ad-hoc. The use of embedded mode is helpful in making the release behave much more consistently, is my understanding.</p>
<p>Made <a href="https://github.com/nerves-project/nerves_system_qemu_aarch64/commit/149a0545312df0d422e44975babe9c9b15247ce7">a bunch of adjustments to Linux memory usage</a>. Using <code>zram</code> was suggested by Frank. Then I checked with (famous ML model) Claude to get hints about what knobs were available to tune on Linux because Frank hinted that it might be caching a bit much and I never know where to start when it comes to what I can do to the Linux kernel. It had some suggestions, I looked those up, found articles that matched the claims that this might reduce memory usage. Changing swappiness, dirty ratios and <code>vfs_cache_pressure</code> like I knew what I was doing and it sure seems to have improved things.</p>
<p>I know I could play with different allocators, my co-founder Josh has been doing that for NervesCloud recently. I think I could also do something with virtual balloons to reclaim memory and essentially over-provision VMs but I haven’t got there yet.</p>
<p>This memory tuning led to some interesting further runs where we ran a solid 5100 devices and I could have pushed it a bit further. I just didn’t have time and could be bothered to do more math at the time. The VMs are now started with 110 MB of RAM on the inside and they seem to run steady around 160 MB RES according to htop. The people I’ve talked to at Ampere indicate that I’m probably running the most VMs anyone has ever ran on their hardware. Which is fun. I’m not even running tiny VMs. I could make a Buildroot system that does nothing and run another gajillion probably. But this is much closer to a real device and workload.</p>
<h2 id="the-utility-of-it-all">The utility of it all</h2>
<p>Honestly, getting a chance to run significant, not massive, but significant workloads against a SaaS is pretty useful. But the work we’ve put in now means we can tidy up this Nerves system and make it part of supported Nerves tooling. This would make it easy to run stuff “on device” without physical hardware. It would make running more detailed tests of Nerves functionality much more feasible as well. Essentially you’d need an ARM64 Linux box with KVM or an Apple Silicon Mac and you’d get the blazing fast ones. Or you can absolutely get by with the emulated, more demanding things from the x86 side of things. There is a lot we can do with a full-featured qemu-system for ARM devices.</p>
<p>While my experimentation is a bit of a stunt and mostly for the joy of experimentation and Frank’s bootloader is mostly about learning the end result is still that we have produced something we should get good mileage out of.</p>
<p>Heck that MacOS thing. I just tried the <code>DELAY=1 COUNT=200 CHUNK=10 ./run.exs</code> after modifying the script to use <code>hvf</code>instead of <code>kvm</code> on my M2 MacBook Air. I think I had 50 VMs when I ran out of disk. Solvable problem, but not throwing out my photo library right this minute.</p>
<h2 id="further-work">Further work</h2>
<p>I need to look at how KVM and <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> interact and if/how I can pin. I don’t think I’ll hit problems where caches and pinning matter all that much but it would feel better. When the VMs are at rest after booting the overall system CPU usage is generally less than 20% running thousands of VMs. Mostly idle, yes, but there are things happening in all of them.</p>
<p>Should run the workload with some graphs to see what is actually happening big picture. Right now I’m mostly going “hey, it is STILL running, eh!?”. Which is fine enough when figuring out if it fits in memory.</p>
<h2 id="tidying-up">Tidying up</h2>
<p>We are in the process of tidying up <a href="https://github.com/nerves-project/nerves_system_qemu_aarch64">nerves_system_qemu_aarch64</a> and then it should get a proper release and some docs. It has a mix task for generating an appropriate qemu command for you. So this all becomes a part of Nerves. Over time we should be able to build some really nice tooling based off of this. And if you have ideas you should be able to pick it up and run with it already.</p>
<p>Really enjoying this deeper dive into things I’ve only been at the periphery of. Learning a lot of Linux, getting to really get into it with qemu, performance tuning for both the BEAM, Linux and virtualization. It is a ton of fun to see how far you can push the hardware.</p>
<p>Alright, that’s enough words. Let me know what you think and if there is anything in particular you’d like me try in and around this. Thanks for reading, hit me up on <a href="mailto:lars@underjord.io">lars@underjord.io</a> or <a href="https://hachyderm.io/@lawik">@lawik@hachyderm.io</a> or wherever you find me.</p>

        <p>
            Underjord is an artisanal consultancy doing consulting in Elixir, Nerves with an accidental speciality in marketing and outreach. If
            you
            like
            the writing you should really <a href="https://underjord.io/services.html">try the pro version</a>.</p>
        <p>
            Note: Or try the videos on <a href="https://youtube.com/c/underjord">the YouTube
                channel</a>.
        </p>
        
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Lovable (138 pts)]]></title>
            <link>https://github.com/mendableai/open-lovable</link>
            <guid>44854120</guid>
            <pubDate>Sun, 10 Aug 2025 10:10:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mendableai/open-lovable">https://github.com/mendableai/open-lovable</a>, See on <a href="https://news.ycombinator.com/item?id=44854120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Open Lovable</h2><a id="user-content-open-lovable" aria-label="Permalink: Open Lovable" href="#open-lovable"></a></p>
<p dir="auto">Chat with AI to build React apps instantly.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ee68ebd089163b65192416765f028cb913aa1cc4b475f4f4b94e54b0809ebeb9/68747470733a2f2f6d65646961312e67697068792e636f6d2f6d656469612f76312e59326c6b505463354d4749334e6a4578626d5a746148466c654752734d544e6c61574e7964476469616e49344e475134644868795a6a42306432566b636a52796558427563435a6c634431324d563970626e526c636d35686246396e61575a66596e6c666157516d593351395a772f5a46564c574d61366456736b5158307175312f67697068792e676966"><img src="https://camo.githubusercontent.com/ee68ebd089163b65192416765f028cb913aa1cc4b475f4f4b94e54b0809ebeb9/68747470733a2f2f6d65646961312e67697068792e636f6d2f6d656469612f76312e59326c6b505463354d4749334e6a4578626d5a746148466c654752734d544e6c61574e7964476469616e49344e475134644868795a6a42306432566b636a52796558427563435a6c634431324d563970626e526c636d35686246396e61575a66596e6c666157516d593351395a772f5a46564c574d61366456736b5158307175312f67697068792e676966" alt="Open Lovable Demo" width="100%" data-animated-image="" data-canonical-src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExbmZtaHFleGRsMTNlaWNydGdianI4NGQ4dHhyZjB0d2VkcjRyeXBucCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZFVLWMa6dVskQX0qu1/giphy.gif"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li><strong>Clone &amp; Install</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/mendableai/open-lovable.git
cd open-lovable
npm install"><pre>git clone https://github.com/mendableai/open-lovable.git
<span>cd</span> open-lovable
npm install</pre></div>
<ol start="2" dir="auto">
<li><strong>Add <code>.env.local</code></strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Required
E2B_API_KEY=your_e2b_api_key  # Get from https://e2b.dev (Sandboxes)
FIRECRAWL_API_KEY=your_firecrawl_api_key  # Get from https://firecrawl.dev (Web scraping)

# Optional (need at least one AI provider)
ANTHROPIC_API_KEY=your_anthropic_api_key  # Get from https://console.anthropic.com
OPENAI_API_KEY=your_openai_api_key  # Get from https://platform.openai.com (GPT-5)
GROQ_API_KEY=your_groq_api_key  # Get from https://console.groq.com (Fast inference - Kimi K2 recommended)"><pre><span><span>#</span> Required</span>
<span>E2B_API_KEY</span><span>=</span><span>your_e2b_api_key<span>  <span>#</span> Get from https://e2b.dev (Sandboxes)</span></span>
<span>FIRECRAWL_API_KEY</span><span>=</span><span>your_firecrawl_api_key<span>  <span>#</span> Get from https://firecrawl.dev (Web scraping)</span></span>

<span><span>#</span> Optional (need at least one AI provider)</span>
<span>ANTHROPIC_API_KEY</span><span>=</span><span>your_anthropic_api_key<span>  <span>#</span> Get from https://console.anthropic.com</span></span>
<span>OPENAI_API_KEY</span><span>=</span><span>your_openai_api_key<span>  <span>#</span> Get from https://platform.openai.com (GPT-5)</span></span>
<span>GROQ_API_KEY</span><span>=</span><span>your_groq_api_key<span>  <span>#</span> Get from https://console.groq.com (Fast inference - Kimi K2 recommended)</span></span></pre></div>
<ol start="3" dir="auto">
<li><strong>Run</strong></li>
</ol>

<p dir="auto">Open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing simple tab-completions for Bash and Zsh (213 pts)]]></title>
            <link>https://mill-build.org/blog/14-bash-zsh-completion.html</link>
            <guid>44854035</guid>
            <pubDate>Sun, 10 Aug 2025 09:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mill-build.org/blog/14-bash-zsh-completion.html">https://mill-build.org/blog/14-bash-zsh-completion.html</a>, See on <a href="https://news.ycombinator.com/item?id=44854035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>

<div id="preamble">
<p><em>Li Haoyi, 7 August 2025</em></p>
<p>Shell tab-completions can be very handy, but setting them up is complicated by the fact
that half your users would be using Bash-on-Linux, while the other half will be
using Zsh-on-OSX, each of which has different tab-completion APIs. Furthermore, most
users exploring an unfamiliar CLI tool using tab completion appreciate showing a
description along with each completion so they can read what it is, but that’s
normally only available on Zsh and not on Bash.</p>
<p>But with some work, you can make your tab-completions work on both shells, including
nice quality-of-life features like completion descriptions. This blog post will explore how it
can be done, based on our recent experience implementing this in the <a href="https://mill-build.org/">Mill build tool</a>
version <a href="https://github.com/com-lihaoyi/mill/blob/main/changelog.adoc#103">1.0.3</a>,
providing the great tab-completion experience you see below in a way that works across
both common shells. Hopefully based on this, you will know enough and have enough reference
examples to set up Bash and Zsh completions for your own command-line tooling.</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions.png" alt="CompletionDescriptions">
</p>
</div>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions2.png" alt="CompletionDescriptions2">
</p>
</div>
</div>
<div>
<h2 id="_basic_tab_completion"><a href="#_basic_tab_completion"></a>Basic Tab Completion</h2>
<div>
<p>The basic way tab-completion works in shells like Bash or Zsh is to register a handler
function that is called when a user presses <code>&lt;TAB&gt;</code> at the command line. This handler
function is then given the words currently written, and the index of the word the
user’s cursor is currently over. From this information, the completion function generates
a list of strings that are possible completions for the word at that index, and
return it to the shell. At a glance, this looks something like:</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(apple apricot banana cherry durian)
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  COMPREPLY=( "${raw[@]}" )
}

_complete_foo_zsh() {
  local -a raw
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))
  compadd -- $raw
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<div>
<ul>
<li>
<p><code>_generate_foo_completions</code> is a dummy function used
for demonstration purposes that prints out a hardcoded set of completions,
but in a real scenario would be the logic that generates completions for
your specific app or CLI tool.</p>
</li>
<li>
<p><code>_complete_foo_bash</code> and <code>_complete_foo_zsh</code> are the shell-specific
completion functions that pass the current word to <code>_generate_foo_completions</code>
and wire up the results to each shell’s unique completion APIs. Bash completion
functions need to set the <code>COMPREPLY</code> environment variable, while Zsh completion
functions need to call <code>compadd</code> (or one of the other similar functions)</p>
</li>
<li>
<p>This example snippet would typically be put (or <code>source</code>ed) in your
<code>~/.bashrc</code>, <code>~/.bash_profile</code>, and <code>~/.zshrc</code> so the <code>if</code>/<code>elif</code>/<code>fi</code> block at
the bottom registers the relevant hooks when the shell starts.
These hook into tab-completion whenever <code>foo</code> is the
first word at the prompt.</p>
</li>
</ul>
</div>
<p>For example, the Mill build tool provides a <code>./mill mill.tabcomplete/install</code>
builtin that automatically updates these files and instructs the user to
restart the shell or <code>source</code> the relevant script to begin using completions:</p>
<div>
<pre><code data-lang="console">$ ./mill mill.tabcomplete/install
[1/1] mill.tabcomplete.TabCompleteModule.install
Writing to /Users/lihaoyi/.cache/mill/download/mill-completion.sh
Writing to /Users/lihaoyi/.bash_profile
Writing to /Users/lihaoyi/.zshrc
Writing to /Users/lihaoyi/.bashrc
Please restart your shell or `source ~/.cache/mill/download/mill-completion.sh` to enable completions</code></pre>
</div>
<p>Although the Shell syntax can be very finnicky, e.g. passing arrays to as
function arguments via <code>"${words[@]}"</code>, the actual underlying logic here isn’t
too complicated. <code>_complete_foo_bash</code> and <code>_complete_foo_zsh</code> take the
local variables from the shell, pass it to <code>_generate_foo_completions</code>
that uses them to return the possible completions, and passes the completions
back to the shell via <code>COMPREPLY</code> or <code>compadd</code>.</p>
<p>You can try this out live by pasting it into your Bash or Zsh shell and
typing <code>foo &lt;TAB&gt;</code> or <code>foo a&lt;TAB&gt;</code>. Note that you don’t
actually need a <code>foo</code> command installed:</p>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple    apricot  banana   cherry   durian

$ foo a&lt;TAB&gt;
apple    apricot</code></pre>
</div>
<p>That’s all you need to get a basic tab-completer working. In real usage"</p>
<div>
<ul>
<li>
<p><code>foo</code> would be the name of the command the user would invoke your CLI program with
(e.g. <code>mill</code>)</p>
</li>
<li>
<p><code>_generate_foo_completions</code> would be your bespoke logic
to print out a line-separated list of completions. This could be a hard-coded list
for programs that change infrequently, or it could actually invoke your binary and
ask it what completions are available for the given input (what <code>mill</code> does).</p>
</li>
<li>
<p>While this example only looks up <code>words[idx]</code> to try and find a prefix
match for the current word, the completer is allowed to use the entirety of <code>words</code>
to decide what completions to offer, e.g. based on what flags or command-names are present in that array</p>
</li>
</ul>
</div>
<p>Note that when you register completion hooks for <code>foo</code> in Bash and Zsh, they apply
to commands like <code>./foo</code> as well. This is handy for programs like Mill, Maven, or Gradle
which typically use a <code>./mill</code> <a href="https://mill-build.org/mill/cli/installation-ide.html#_bootstrap_scripts" class="page">Bootstrap Script</a>
to run:</p>
<div>
<pre><code data-lang="console">$ ./foo a&lt;TAB&gt;
apple    apricot</code></pre>
</div>
</div>
</div>
<div>
<h2 id="_zsh_completion_descriptions"><a href="#_zsh_completion_descriptions"></a>Zsh Completion Descriptions</h2>
<div>
<p>The completions above work and provide a basic level of assistance for users of your CLI, but
it would be nice for users if they could also see a description of each command they could
complete in the terminal, as is done in the Mill build tool:</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionDescriptions.png" alt="CompletionDescriptions">
</p>
</div>
<p>To do this, we can make <code>_generate_foo_completions</code> generate an array of
longer strings containing both the completion and a description. Bash does not support
completion descriptions by default so we trim off the description,
but in Zsh we pass both the <code>trimmed</code> completion-words as well as the <code>raw</code> words and
descriptions to <code>compadd -d raw — $trimmed</code> as two parallel arrays.</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(
    "apple: a common fruit"
    "apricot: sour fruit with a large stone"
    "banana: starchy and high in potassium"
    "cherry: small and sweet with a large pit"
    "durian: stinky spiky fruit"
  )
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  for d in "${raw[@]}"; do trimmed+=( "${d%%:*}" ); done

  COMPREPLY=( "${trimmed[@]}" )
}

_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  compadd -d raw -- $trimmed
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<p>Zsh would then display the <code>raw</code> lines including both the completion-word as well
as the descriptions when displaying the completion options, but use the <code>trimmed</code>
lines which only contain the completion-words when completing the line</p>
<div>
<pre><code data-lang="console">$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                          apricot: sour fruit with a large stone

$ foo app&lt;TAB&gt;
$ foo apple</code></pre>
</div>
<p>However in this scenario the descriptions are entirely ignored by Bash. Because Bash
does not have a concept of tab-complete descriptions, in Bash we only pass the <code>trimmed</code>
word-completions to <code>COMPREPLY</code> and discard the <code>raw</code> lines containing the descriptions.</p>
</div>
</div>
<div>
<h2 id="_hacking_bash_completion_descriptions"><a href="#_hacking_bash_completion_descriptions"></a>Hacking Bash Completion Descriptions</h2>
<div>
<p>To make Bash show completion "descriptions", we can take advantage of the fact
that the completions are generated dynamically every time we call
<code>_generate_foo_completions</code>, and Bash and Zsh only inserts text
that is a common prefix to all completion options</p>

<p>Therefore, if we have multiple differing word-completions, we can actually append
whatever we want to the right of those words in <code>_generate_foo_completions</code>!
This "appended text" will be shown to users if there are multiple completions
available, but since the word-completions differ, Bash will never insert the entire word,
and thus never insert the appended text either.</p>
<p>The code below implements this: if there is only one completion we trim off the description
following the <code>:</code> off as normal, but if there’s more than one completion we leave the
description intact for the user to see</p>
<div>
<pre><code data-lang="bash">_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  if (( ${#raw[@]} == 1 )); then
    trimmed=( "${raw[0]%%:*}" )
  else
    trimmed=( "${raw[@]}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}</code></pre>
</div>
<p>Now when I use autocomplete in Bash, I can see the descriptions for each item, but when
the tab-completion actually completes the token it only completes the word itself and
does not include the description!</p>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple: a common fruit                     cherry: small and sweet with a large pit
apricot: sour fruit with a large stone    durian: stinky spiky fruit
banana: starchy and high in potassium

$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                   apricot: sour fruit with a large stone


$ foo app&lt;TAB&gt;
$ foo apple</code></pre>
</div>
<p>In this section, we only needed to make changes to the <code>_complete_foo_bash</code> function,
as the Zsh completion logic in <code>_complete_foo_zsh</code> is completely unchanged.</p>
</div>
</div>
<div>
<h2 id="_showing_single_completion_descriptions"><a href="#_showing_single_completion_descriptions"></a>Showing Single-Completion Descriptions</h2>
<div>
<p>The last quality of life feature we will add is the ability to show completion
descriptions when tabbing on a complete word:</p>

<p>For example, the Mill build tool does this so if you’re not sure what a flag or command
does, you can press <code>&lt;TAB&gt;</code> on it to see more details:</p>
<div>
<p><img src="https://mill-build.org/blog/_images/CompletionSingleDescription.png" alt="CompletionSingleDescription">
</p>
</div>
<p>Tab-completion is a common way to explore unfamiliar APIs, and just because someone
finished writing a flat or command doesn’t mean they aren’t curious about what
it does! But while Zsh tab-completion displays descriptions when multiple
options match the prefix, and we managed to hack Bash tab-completion to do the same
thing, neither displays any information if the word you are tab-completing is already
complete.</p>
<p>This behavior can be annoying, if the user wants to see the description, they will
need to first:</p>
<div>
<ul>
<li>
<p>Delete enough characters to make the token match multiple completions</p>
</li>
<li>
<p>Press <code>&lt;TAB&gt;</code></p>
</li>
<li>
<p>Visually scan the multiple completions printed to find the word description
they care about</p>
</li>
<li>
<p>Type back in all the missing characters so they can run the command</p>
</li>
</ul>
</div>
<p>To solve this, we can hack Bash and Zsh to print tab-completion descriptions even
if the token is already a complete word. We do this by checking if the token
is a complete word, and if so adding a second "dummy" completion: this makes
the tab-completion ambiguous, which cases Bash and Zsh to print out the completions
and descriptions for the user to see.</p>
<p>Doing this in <code>_complete_foo_bash</code> looks like the following:</p>
<div>
<pre><code data-lang="bash">_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  trimmed+=( "${raw[@]}" )

  if (( ${#raw[@]} == 1 )); then
    trimmed+=( "${raw[0]%%:*}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}</code></pre>
</div>
<p>Instead of checking the length of <code>raw</code> to decide whether we add a trimmed
and non-trimmed lines to <code>trimmed</code>, we now instead <em>always</em> add the non-trimmed lines
that contain the completion descriptions, and in the case where there’s only
one line we then add an additional word-only completion with the description
trimmed off.</p>
<p>This means that all completions are ambiguous and will print the description -
even completions with a single real choice - but the additional trimmed line
when there is only 1 real choice ensures that the description text never gets
inserted into the user’s command</p>
<p>In Zsh, this can be similarly done via:</p>
<div>
<pre><code data-lang="bash">_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  if (( ${#raw} == 1 )); then
    trimmed+=( "${raw[1]}" )
    raw+=( "${trimmed[1]}" )
  fi

  compadd -d raw -- $trimmed
}</code></pre>
</div>
<p>The change here is similar to the Bash snippet above: when the number of completions is 1,
we add an additional completion to make it ambiguous so Zsh prints the description. But
because Zsh expects to pass two parallel arrays of descriptions and tokens to <code>compadd</code>,
our <code>if</code> block needs to append items to both <code>trimmed</code> and <code>raw</code>.</p>
<p>Using this, it now looks like</p>
<div>
<pre><code data-lang="console">$ foo apple&lt;TAB&gt;
apple                  apple: a common fruit</code></pre>
</div>
<p>Although the UI is not quite perfect - the word <code>apple</code> gets duplicated twice -
this nevertheless achieves the original goal of letting users <code>&lt;TAB&gt;</code> on an
already-completed flag or command to see the description or documentation for that word.</p>
</div>
</div>
<div>
<h2 id="_conclusion"><a href="#_conclusion"></a>Conclusion</h2>
<div>
<p>At this point, our final code looks like this:</p>
<div>
<pre><code data-lang="bash">_generate_foo_completions() {
  local idx=$1; shift
  local words=( "$@" )
  local current_word=${words[idx]}

  local array=(
    "apple: a common fruit"
    "apricot: sour fruit with a large stone"
    "banana: starchy and high in potassium"
    "cherry: small and sweet with a large pit"
    "durian: stinky spiky fruit"
  )
  for elem in "${array[@]}"; do
    if [[ $elem == "$current_word"* ]]; then echo "$elem"; fi
  done
}

_complete_foo_bash() {
  local IFS=$'\n'
  local raw=($(_generate_foo_completions "$COMP_CWORD" "${COMP_WORDS[@]}"))
  local trimmed=()
  trimmed+=( "${raw[@]}" )

  if (( ${#raw[@]} == 1 )); then
    trimmed+=( "${raw[0]%%:*}" )
  fi

  COMPREPLY=( "${trimmed[@]}" )
}

_complete_foo_zsh() {
  local -a raw trimmed
  local IFS=$'\n'
  raw=($(_generate_foo_completions "$CURRENT" "${words[@]}"))

  for d in $raw; do trimmed+=( "${d%%:*}" ); done
  if (( ${#raw} == 1 )); then
    trimmed+=( "${raw[1]}" )
    raw+=( "${trimmed[1]}" )
  fi

  compadd -d raw -- $trimmed
}

if [ -n "${ZSH_VERSION:-}" ]; then
  autoload -Uz compinit
  compinit
  compdef _complete_foo_zsh foo
elif [ -n "${BASH_VERSION:-}" ]; then
  complete -F _complete_foo_bash foo
fi</code></pre>
</div>
<p>And can be used in both Bash or Zsh to provide an identical user experience:</p>
<div>
<ul>
<li>
<p>Showing possible tab-completions when there are multiple available</p>
</li>
<li>
<p>Showing command or flag descriptions (even though this is not natively supported by Bash)</p>
</li>
<li>
<p>Performing partial or entire-word completions</p>
</li>
<li>
<p>Showing the description or documentation when <code>&lt;TAB&gt;</code>ing on an already-completed word</p>
</li>
</ul>
</div>
<div>
<pre><code data-lang="console">$ foo &lt;TAB&gt;
apple: a common fruit                     banana: starchy and high in potassium     durian: stinky spiky fruit
apricot: sour fruit with a large stone    cherry: small and sweet with a large pit

$ foo a&lt;TAB&gt;
$ foo ap

$ foo ap&lt;TAB&gt;
apple: a common fruit                   apricot: sour fruit with a large stone

$ foo app&lt;TAB&gt;
$ foo apple

$ foo apple&lt;TAB&gt;
apple                  apple: a common fruit</code></pre>
</div>
<p>The actual docs for each shell’s tab-completion system contains a lot more detail (e.g.
<a href="https://zsh.sourceforge.io/Doc/Release/Completion-System.html">72 pages</a> for Zsh!), and
there are definitely many different ways you can set up your tab-completion scripts.
This blog post just aims to provide the simplest working example that works in both
Bash and Zsh, so hopefully you can understand it well enough to integrate into
your own projects.</p>
</div>
</div>

</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Abogen – Generate audiobooks from EPUBs, PDFs and text (271 pts)]]></title>
            <link>https://github.com/denizsafak/abogen</link>
            <guid>44853064</guid>
            <pubDate>Sun, 10 Aug 2025 05:56:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/denizsafak/abogen">https://github.com/denizsafak/abogen</a>, See on <a href="https://news.ycombinator.com/item?id=44853064">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">abogen <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico"><img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico"></a></h2><a id="user-content-abogen-" aria-label="Permalink: abogen " href="#abogen-"></a></div>
<p dir="auto"><a href="https://github.com/denizsafak/abogen/actions"><img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg" alt="Build Status"></a>
<a href="https://github.com/denizsafak/abogen/releases/latest"><img src="https://camo.githubusercontent.com/80f34663a4a26d077050a300729c88d9b130c4da02f733425c2146b15ea99fa1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f64656e697a736166616b2f61626f67656e" alt="GitHub Release" data-canonical-src="https://img.shields.io/github/v/release/denizsafak/abogen"></a>
<a href="https://pypi.org/project/abogen/" rel="nofollow"><img src="https://camo.githubusercontent.com/6c83cc79631886146a58d279a7b42aca1febcc6e93417836ad81ded7547cbf5e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f61626f67656e" alt="Abogen PyPi Python Versions" data-canonical-src="https://img.shields.io/pypi/pyversions/abogen"></a>
<a href="https://github.com/denizsafak/abogen/releases/latest"><img src="https://camo.githubusercontent.com/1ddac32b3e7b4a76e66e2ad641d8f0a3d0c9cc3e0defa693f169cdc84fccfc7b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f732d77696e646f77732532302537432532306c696e75782532302537432532306d61636f732532302d626c7565" alt="Operating Systems" data-canonical-src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue"></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/02e67a3f818beb1ab87c3b8fd2b7403260b1a8f77fb0899dc3ff0e683f5067ed/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d6d61726f6f6e2e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-maroon.svg"></a></p>
<p dir="auto">Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using <a href="https://huggingface.co/hexgrad/Kokoro-82M" rel="nofollow">Kokoro-82M</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png"><img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png"><img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/39929354/437639906-cb66512d-0a52-48c3-bda4-f1e6a03fb8d6.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ4MTg1MDEsIm5iZiI6MTc1NDgxODIwMSwicGF0aCI6Ii8zOTkyOTM1NC80Mzc2Mzk5MDYtY2I2NjUxMmQtMGE1Mi00OGMzLWJkYTQtZjFlNmEwM2ZiOGQ2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODEwVDA5MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZjA4YjU5MmNlODcxMzMxMDQwYjBlNWQ4YjAzMDRmZjUyM2E5YjU0MTZmODRhOTYzODFlYzI0ZDZmYzYyNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rWfnnubrVzoCKOG2qJlSqfRQTfboP0i2XrHy75xbOOI" data-canonical-src="https://private-user-images.githubusercontent.com/39929354/437639906-cb66512d-0a52-48c3-bda4-f1e6a03fb8d6.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ4MTg1MDEsIm5iZiI6MTc1NDgxODIwMSwicGF0aCI6Ii8zOTkyOTM1NC80Mzc2Mzk5MDYtY2I2NjUxMmQtMGE1Mi00OGMzLWJkYTQtZjFlNmEwM2ZiOGQ2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MTAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODEwVDA5MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZjA4YjU5MmNlODcxMzMxMDQwYjBlNWQ4YjAzMDRmZjUyM2E5YjU0MTZmODRhOTYzODFlYzI0ZDZmYzYyNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.rWfnnubrVzoCKOG2qJlSqfRQTfboP0i2XrHy75xbOOI" controls="controls" muted="muted">

  </video>
</details>

<blockquote>
<p dir="auto">This demo was generated in just 5&nbsp;seconds, producing ∼1&nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see <a href="https://github.com/denizsafak/abogen/tree/main/demo">the demo guide</a>.</p>
</blockquote>
<div dir="auto"><h2 tabindex="-1" dir="auto"><code>How to install?</code> <a href="https://pypi.org/project/abogen/" rel="nofollow"><img src="https://camo.githubusercontent.com/6c83cc79631886146a58d279a7b42aca1febcc6e93417836ad81ded7547cbf5e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f61626f67656e" alt="Abogen Compatible PyPi Python Versions" data-canonical-src="https://img.shields.io/pypi/pyversions/abogen"></a></h2><a id="user-content-how-to-install-" aria-label="Permalink: How to install?" href="#how-to-install-"></a></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Go to <a href="https://github.com/espeak-ng/espeak-ng/releases/latest">espeak-ng latest release</a> download and run the *.msi file.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">OPTION 1: Install using script</h4><a id="user-content-option-1-install-using-script" aria-label="Permalink: OPTION 1: Install using script" href="#option-1-install-using-script"></a></p>
<ol dir="auto">
<li><a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip">Download</a> the repository</li>
<li>Extract the ZIP file</li>
<li>Run <code>WINDOWS_INSTALL.bat</code> by double-clicking it</li>
</ol>
<p dir="auto">This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install <a href="https://github.com/espeak-ng/espeak-ng/releases/latest">espeak-ng</a>.)</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">You don't need to install Python separately. The script will install Python automatically.</p>
</div>
<p dir="auto"><h4 tabindex="-1" dir="auto">OPTION 2: Install using pip</h4><a id="user-content-option-2-install-using-pip" aria-label="Permalink: OPTION 2: Install using pip" href="#option-2-install-using-pip"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create a virtual environment (optional)
mkdir abogen &amp;&amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen"><pre><span><span>#</span> Create a virtual environment (optional)</span>
mkdir abogen <span>&amp;&amp;</span> <span>cd</span> abogen
python -m venv venv
venv<span>\S</span>cripts<span>\a</span>ctivate

<span><span>#</span> For NVIDIA GPUs:</span>
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

<span><span>#</span> For AMD GPUs:</span>
<span><span>#</span> Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.</span>

<span><span>#</span> Install abogen</span>
pip install abogen</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;&amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen"><pre><span><span>#</span> Install espeak-ng</span>
brew install espeak-ng

<span><span>#</span> Create a virtual environment (recommended)</span>
mkdir abogen <span>&amp;&amp;</span> <span>cd</span> abogen
python3 -m venv venv
<span>source</span> venv/bin/activate

<span><span>#</span> Install abogen</span>
pip3 install abogen</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux</h3><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;&amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4"><pre><span><span>#</span> Install espeak-ng</span>
sudo apt install espeak-ng <span><span>#</span> Ubuntu/Debian</span>
sudo pacman -S espeak-ng <span><span>#</span> Arch Linux</span>
sudo dnf install espeak-ng <span><span>#</span> Fedora</span>

<span><span>#</span> Create a virtual environment (recommended)</span>
mkdir abogen <span>&amp;&amp;</span> <span>cd</span> abogen
python3 -m venv venv
<span>source</span> venv/bin/activate

<span><span>#</span> Install abogen</span>
pip3 install abogen

<span><span>#</span> For NVIDIA GPUs:</span>
<span><span>#</span> Already supported, no need to install CUDA separately.</span>

<span><span>#</span> For AMD GPUs:</span>
<span><span>#</span> After installing abogen, we need to uninstall the existing torch package</span>
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4</pre></div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you get <code>WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.</code> error, run the following command to add it to your PATH:</p>
<div dir="auto" data-snippet-clipboard-copy-content="echo &quot;export PATH=\&quot;/home/$USER/.local/bin:\$PATH\&quot;&quot; >> ~/.bashrc &amp;&amp; source ~/.bashrc"><pre><span>echo</span> <span><span>"</span>export PATH=<span>\"</span>/home/<span>$USER</span>/.local/bin:<span>\$</span>PATH<span>\"</span><span>"</span></span> <span>&gt;&gt;</span> <span>~</span>/.bashrc <span>&amp;&amp;</span> <span>source</span> <span>~</span>/.bashrc</pre></div>
</div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use <a href="https://github.com/pyenv/pyenv">pyenv</a> to manage multiple Python versions easily in Linux. Watch this <a href="https://www.youtube.com/watch?v=MVyb-nI4KyI" rel="nofollow">video</a> by NetworkChuck for a quick guide.</p>
</div>
<blockquote>
<p dir="auto">Special thanks to <a href="https://github.com/hg000125">@hg000125</a> for his contribution in <a href="https://github.com/denizsafak/abogen/issues/23" data-hovercard-type="issue" data-hovercard-url="/denizsafak/abogen/issues/23/hovercard">#23</a>. AMD GPU support is possible thanks to his work.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>How to run?</code></h2><a id="user-content-how-to-run" aria-label="Permalink: How to run?" href="#how-to-run"></a></p>
<p dir="auto">If you installed using pip, you can simply run the following command to start Abogen:</p>

<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you installed using the Windows installer <code>(WINDOWS_INSTALL.bat)</code>, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in <code>python_embedded/Scripts/abogen.exe</code>. You can run it from there directly.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>How to use?</code></h2><a id="user-content-how-to-use" aria-label="Permalink: How to use?" href="#how-to-use"></a></p>
<ol dir="auto">
<li>Drag and drop any ePub, PDF, or text file (or use the built-in text editor)</li>
<li>Configure the settings:
<ul dir="auto">
<li>Set speech speed</li>
<li>Select a voice (or create a custom voice using voice mixer)</li>
<li>Select subtitle generation style (by sentence, word, etc.)</li>
<li>Select output format</li>
<li>Select where to save the output</li>
</ul>
</li>
<li>Hit Start</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>In action</code></h2><a id="user-content-in-action" aria-label="Permalink: In action" href="#in-action"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif"><img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" data-animated-image=""></a></p> 
<p dir="auto">Here’s Abogen in action: in this demo, it processes ∼3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end <strong>RTX&nbsp;2060&nbsp;Mobile laptop GPU</strong>. Your results may vary depending on your hardware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Configuration</code></h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Options</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Input Box</strong></td>
<td>Drag and drop <code>ePub</code>, <code>PDF</code>, or <code>.TXT</code> files (or use built-in text editor)</td>
</tr>
<tr>
<td><strong>Queue options</strong></td>
<td>Add multiple files to a queue and process them in batch, with individual settings for each file. See <a href="#queue-mode">Queue mode</a> for more details.</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Adjust speech rate from <code>0.1x</code> to <code>2.0x</code></td>
</tr>
<tr>
<td><strong>Select Voice</strong></td>
<td>First letter of the language code (e.g., <code>a</code> for American English, <code>b</code> for British English, etc.), second letter is for <code>m</code> for male and <code>f</code> for female.</td>
</tr>
<tr>
<td><strong>Voice mixer</strong></td>
<td>Create custom voices by mixing different voice models with a profile system. See <a href="#voice-mixer">Voice Mixer</a> for more details.</td>
</tr>
<tr>
<td><strong>Voice preview</strong></td>
<td>Listen to the selected voice before processing.</td>
</tr>
<tr>
<td><strong>Generate subtitles</strong></td>
<td><code>Disabled</code>, <code>Sentence</code>, <code>Sentence + Comma</code>, <code>1 word</code>, <code>2 words</code>, <code>3 words</code>, etc. (Represents the number of words in each subtitle entry)</td>
</tr>
<tr>
<td><strong>Output voice format</strong></td>
<td><code>.WAV</code>, <code>.FLAC</code>, <code>.MP3</code>, <code>.OPUS (best compression)</code> and <code>M4B (with chapters)</code> (Special thanks to <a href="https://github.com/jborza">@jborza</a> for chapter support in PR <a href="https://github.com/denizsafak/abogen/pull/10" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/10/hovercard">#10</a>)</td>
</tr>
<tr>
<td><strong>Output subtitle format</strong></td>
<td>Configures the subtitle format as <code>SRT (standard)</code>, <code>ASS (wide)</code>, <code>ASS (narrow)</code>, <code>ASS (centered wide)</code>, or <code>ASS (centered narrow)</code>.</td>
</tr>
<tr>
<td><strong>Replace single newlines with spaces</strong></td>
<td>Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.</td>
</tr>
<tr>
<td><strong>Save location</strong></td>
<td><code>Save next to input file</code>, <code>Save to desktop</code>, or <code>Choose output folder</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Book handler options</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Chapter Control</strong></td>
<td>Select specific <code>chapters</code> from ePUBs or <code>chapters + pages</code> from PDFs.</td>
</tr>
<tr>
<td><strong>Save each chapter separately</strong></td>
<td>Save each chapter in e-books as a separate audio file.</td>
</tr>
<tr>
<td><strong>Create a merged version</strong></td>
<td>Create a single audio file that combines all chapters. (If <code>Save each chapter separately</code> is disabled, this option will be the default behavior.)</td>
</tr>
<tr>
<td><strong>Save in a project folder with metadata</strong></td>
<td>Save the converted items in a project folder with available metadata files.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Menu options</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Theme</strong></td>
<td>Change the application's theme using <code>System</code>, <code>Light</code>, or <code>Dark</code> options.</td>
</tr>
<tr>
<td><strong>Configure max words per subtitle</strong></td>
<td>Configures the maximum number of words per subtitle entry.</td>
</tr>
<tr>
<td><strong>Configure max lines in log window</strong></td>
<td>Configures the maximum number of lines to display in the log window.</td>
</tr>
<tr>
<td><strong>Separate chapters audio format</strong></td>
<td>Configures the audio format for separate chapters as <code>wav</code>, <code>flac</code>, <code>mp3</code>, or <code>opus</code>.</td>
</tr>
<tr>
<td><strong>Create desktop shortcut</strong></td>
<td>Creates a shortcut on your desktop for easy access.</td>
</tr>
<tr>
<td><strong>Open config directory</strong></td>
<td>Opens the directory where the configuration file is stored.</td>
</tr>
<tr>
<td><strong>Open cache directory</strong></td>
<td>Opens the cache directory where converted text files are stored.</td>
</tr>
<tr>
<td><strong>Clear cache files</strong></td>
<td>Deletes cache files created during the conversion or preview.</td>
</tr>
<tr>
<td><strong>Check for updates at startup</strong></td>
<td>Automatically checks for updates when the program starts.</td>
</tr>
<tr>
<td><strong>Disable Kokoro's internet access</strong></td>
<td>Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.</td>
</tr>
<tr>
<td><strong>Reset to default settings</strong></td>
<td>Resets all settings to their default values.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Voice Mixer</code></h2><a id="user-content-voice-mixer" aria-label="Permalink: Voice Mixer" href="#voice-mixer"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png"><img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png"></a></p>
<p dir="auto">With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to <a href="https://github.com/jborza">@jborza</a> for making this possible through his contributions in <a href="https://github.com/denizsafak/abogen/pull/5" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/5/hovercard">#5</a>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Queue Mode</code></h2><a id="user-content-queue-mode" aria-label="Permalink: Queue Mode" href="#queue-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png"><img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png"></a></p>
<p dir="auto">Abogen supports <strong>queue mode</strong>, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.</p>
<ul dir="auto">
<li>You can add text files (<code>.txt</code>) directly using the <strong>Add files</strong> button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the <strong>Add to Queue</strong> button.</li>
<li>Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does <strong>not</strong> affect files already in the queue.</li>
<li>You can view each file's configuration by hovering over them.</li>
</ul>
<p dir="auto">Abogen will process each item in the queue automatically, saving outputs as configured.</p>
<blockquote>
<p dir="auto">Special thanks to <a href="https://github.com/jborza">@jborza</a> for adding queue mode in PR <a href="https://github.com/denizsafak/abogen/pull/35" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/35/hovercard">#35</a></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>About Chapter Markers</code></h2><a id="user-content-about-chapter-markers" aria-label="Permalink: About Chapter Markers" href="#about-chapter-markers"></a></p>
<p dir="auto">When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:</p>
<div data-snippet-clipboard-copy-content="<<CHAPTER_MARKER:Chapter Title>>"><pre><code>&lt;&lt;CHAPTER_MARKER:Chapter Title&gt;&gt;
</code></pre></div>
<p dir="auto">These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:</p>
<ul dir="auto">
<li>Allow you to split the text into separate audio files for each chapter</li>
<li>Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file</li>
</ul>
<p dir="auto">You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:</p>
<div data-snippet-clipboard-copy-content="<<CHAPTER_MARKER:Introduction>>
This is the beginning of my text...  

<<CHAPTER_MARKER:Main Content>> 
Here's another part...  "><pre><code>&lt;&lt;CHAPTER_MARKER:Introduction&gt;&gt;
This is the beginning of my text...  

&lt;&lt;CHAPTER_MARKER:Main Content&gt;&gt; 
Here's another part...  
</code></pre></div>
<p dir="auto">When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png"><img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>About Metadata Tags</code></h2><a id="user-content-about-metadata-tags" aria-label="Permalink: About Metadata Tags" href="#about-metadata-tags"></a></p>
<p dir="auto">Similar to chapter markers, it is possible to add metadata tags for <code>M4B</code> files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags <strong>at the beginning of your text file</strong> like this:</p>
<div data-snippet-clipboard-copy-content="<<METADATA_TITLE:Title>>
<<METADATA_ARTIST:Author>>
<<METADATA_ALBUM:Album Title>>
<<METADATA_YEAR:Year>>
<<METADATA_ALBUM_ARTIST:Album Artist>>
<<METADATA_COMPOSER:Narrator>>
<<METADATA_GENRE:Audiobook>>"><pre><code>&lt;&lt;METADATA_TITLE:Title&gt;&gt;
&lt;&lt;METADATA_ARTIST:Author&gt;&gt;
&lt;&lt;METADATA_ALBUM:Album Title&gt;&gt;
&lt;&lt;METADATA_YEAR:Year&gt;&gt;
&lt;&lt;METADATA_ALBUM_ARTIST:Album Artist&gt;&gt;
&lt;&lt;METADATA_COMPOSER:Narrator&gt;&gt;
&lt;&lt;METADATA_GENRE:Audiobook&gt;&gt;
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Supported Languages</code></h2><a id="user-content-supported-languages" aria-label="Permalink: Supported Languages" href="#supported-languages"></a></p>
<div data-snippet-clipboard-copy-content="# 🇺🇸 'a' => American English, 🇬🇧 'b' => British English
# 🇪🇸 'e' => Spanish es
# 🇫🇷 'f' => French fr-fr
# 🇮🇳 'h' => Hindi hi
# 🇮🇹 'i' => Italian it
# 🇯🇵 'j' => Japanese: pip install misaki[ja]
# 🇧🇷 'p' => Brazilian Portuguese pt-br
# 🇨🇳 'z' => Mandarin Chinese: pip install misaki[zh]"><pre><code># 🇺🇸 'a' =&gt; American English, 🇬🇧 'b' =&gt; British English
# 🇪🇸 'e' =&gt; Spanish es
# 🇫🇷 'f' =&gt; French fr-fr
# 🇮🇳 'h' =&gt; Hindi hi
# 🇮🇹 'i' =&gt; Italian it
# 🇯🇵 'j' =&gt; Japanese: pip install misaki[ja]
# 🇧🇷 'p' =&gt; Brazilian Portuguese pt-br
# 🇨🇳 'z' =&gt; Mandarin Chinese: pip install misaki[zh]
</code></pre></div>
<p dir="auto">For a complete list of supported languages and voices, refer to Kokoro's <a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md" rel="nofollow">VOICES.md</a>. To listen to sample audio outputs, see <a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md" rel="nofollow">SAMPLES.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>MPV Config</code></h2><a id="user-content-mpv-config" aria-label="Permalink: MPV Config" href="#mpv-config"></a></p>
<p dir="auto">I highly recommend using <a href="https://mpv.io/installation/" rel="nofollow">MPV</a> to play your audio files, as it supports displaying subtitles even without a video track. Here's my <code>mpv.conf</code>:</p>
<div data-snippet-clipboard-copy-content="# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200"><pre><code># --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Docker Guide</code></h2><a id="user-content-docker-guide" aria-label="Permalink: Docker Guide" href="#docker-guide"></a></p>
<p dir="auto">If you want to run Abogen in a Docker container:</p>
<ol dir="auto">
<li><a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip">Download the repository</a> and extract, or clone it using git.</li>
<li>Go to <code>abogen</code> folder. You should see <code>Dockerfile</code> there.</li>
<li>Open your termminal in that directory and run the following commands:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client."><pre><span><span>#</span> Build the Docker image:</span>
docker build --progress plain -t abogen <span>.</span>

<span><span>#</span> Note that building the image may take a while.</span>
<span><span>#</span> After building is complete, run the Docker container:</span>

<span><span>#</span> Windows</span>
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

<span><span>#</span> Linux</span>
docker run --name abogen -v <span><span>$(</span>pwd<span>)</span></span>:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

<span><span>#</span> MacOS</span>
docker run --name abogen -v <span><span>$(</span>pwd<span>)</span></span>:/shared -p 5800:5800 -p 5900:5900 abogen

<span><span>#</span> We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.</span></pre></div>
<p dir="auto">Abogen launches automatically inside the container.</p>
<ul dir="auto">
<li>You can access it via a web browser at <a href="http://localhost:5800/" rel="nofollow">http://localhost:5800</a> or connect to it using a VNC client at <code>localhost:5900</code>.</li>
<li>You can use <code>/shared</code> directory to share files between your host and the container.</li>
<li>For later use, start it with <code>docker start abogen</code> and stop it with <code>docker stop abogen</code>.</li>
</ul>
<p dir="auto">Known issues:</p>
<ul dir="auto">
<li>Audio preview is not working inside container (ALSA error).</li>
<li><code>Open cache directory</code> and <code>Open configuration directory</code> options in settings not working. (Tried pcmanfm, did not work with Abogen).</li>
</ul>
<p dir="auto">(Special thanks to <a href="https://www.reddit.com/user/geo38/" rel="nofollow">@geo38</a> from Reddit, who provided the Dockerfile and instructions in <a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/" rel="nofollow">this comment</a>.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Similar Projects</code></h2><a id="user-content-similar-projects" aria-label="Permalink: Similar Projects" href="#similar-projects"></a></p>
<p dir="auto">Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:</p>
<ul dir="auto">
<li><a href="https://github.com/santinic/audiblez">audiblez</a>: Generate audiobooks from e-books. <strong>(Has CLI and GUI support)</strong></li>
<li><a href="https://github.com/plusuncold/autiobooks">autiobooks</a>: Automatically convert epubs to audiobooks</li>
<li><a href="https://github.com/mateogon/pdf-narrator">pdf-narrator</a>: Convert your PDFs and EPUBs into audiobooks effortlessly.</li>
<li><a href="https://github.com/p0n1/epub_to_audiobook">epub_to_audiobook</a>: EPUB to audiobook converter, optimized for Audiobookshelf</li>
<li><a href="https://github.com/DrewThomasson/ebook2audiobook">ebook2audiobook</a>: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Roadmap</code></h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul>
<li> Add OCR scan feature for PDF files using docling/teserract.</li>
<li> Add chapter metadata for .m4a files. (Issue <a href="https://github.com/denizsafak/abogen/issues/9" data-hovercard-type="issue" data-hovercard-url="/denizsafak/abogen/issues/9/hovercard">#9</a>, PR <a href="https://github.com/denizsafak/abogen/pull/10" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/10/hovercard">#10</a>)</li>
<li> Add support for different languages in GUI.</li>
<li> Add voice formula feature that enables mixing different voice models. (Issue <a href="https://github.com/denizsafak/abogen/issues/1" data-hovercard-type="issue" data-hovercard-url="/denizsafak/abogen/issues/1/hovercard">#1</a>, PR <a href="https://github.com/denizsafak/abogen/pull/5" data-hovercard-type="pull_request" data-hovercard-url="/denizsafak/abogen/pull/5/hovercard">#5</a>)</li>
<li> Add support for kokoro-onnx (If it's necessary).</li>
<li> Add dark mode.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Troubleshooting</code></h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you encounter any issues while running Abogen, try launching it from the command line with:</p>

<p dir="auto">This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the <a href="https://github.com/denizsafak/abogen/issues">Issues</a> page with the error message and a description of your problem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Contributing</code></h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">For developers and contributors</h3><a id="user-content-for-developers-and-contributors" aria-label="Permalink: For developers and contributors" href="#for-developers-and-contributors"></a></p>
<p dir="auto">If you'd like to modify the code and contribute to development, you can <a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip">download the repository</a>, extract it and run the following commands to build <strong>or</strong> install the package:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI"><pre><span><span>#</span> Go to the directory where you extracted the repository and run:</span>
pip install -e <span>.</span>      <span><span>#</span> Installs the package in editable mode</span>
pip install build     <span><span>#</span> Install the build package</span>
python -m build       <span><span>#</span> Builds the package in dist folder (optional)</span>
abogen                <span><span>#</span> Opens the GUI</span></pre></div>
<p dir="auto">Feel free to explore the code and make any changes you like.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>Credits</code></h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Abogen uses <a href="https://github.com/hexgrad/kokoro">Kokoro</a> for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.</li>
<li>Thanks to <a href="https://github.com/wojiushixiaobai">@wojiushixiaobai</a> for <a href="https://github.com/wojiushixiaobai/Python-Embed-Win64">Embedded Python</a> packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.</li>
<li>Thanks to creators of <a href="https://github.com/aerkalov/ebooklib">EbookLib</a>, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.</li>
<li>Special thanks to the <a href="https://www.riverbankcomputing.com/software/pyqt/" rel="nofollow">PyQt</a> team for providing the cross-platform GUI toolkit that powers Abogen's interface.</li>
<li>Icons: <a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa" rel="nofollow">US</a>, <a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain" rel="nofollow">Great Britain</a>, <a href="https://icons8.com/icon/ly7tzANRt33n/spain" rel="nofollow">Spain</a>, <a href="https://icons8.com/icon/3muzEmi4dpD5/france" rel="nofollow">France</a>, <a href="https://icons8.com/icon/esGVrxg9VCJ1/india" rel="nofollow">India</a>, <a href="https://icons8.com/icon/PW8KZnP7qXzO/italy" rel="nofollow">Italy</a>, <a href="https://icons8.com/icon/McQbrq9qaQye/japan" rel="nofollow">Japan</a>, <a href="https://icons8.com/icon/zHmH8HpOmM90/brazil" rel="nofollow">Brazil</a>, <a href="https://icons8.com/icon/Ej50Oe3crXwF/china" rel="nofollow">China</a>, <a href="https://icons8.com/icon/uI49hxbpxTkp/female" rel="nofollow">Female</a>, <a href="https://icons8.com/icon/12351/male" rel="nofollow">Male</a>, <a href="https://icons8.com/icon/21698/adjust" rel="nofollow">Adjust</a> and <a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id" rel="nofollow">Voice Id</a> icons by <a href="https://icons8.com/" rel="nofollow">Icons8</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><code>License</code></h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is available under the MIT License - see the <a href="https://github.com/denizsafak/abogen/blob/main/LICENSE">LICENSE</a> file for details.
<a href="https://github.com/hexgrad/kokoro">Kokoro</a> is licensed under <a href="https://github.com/hexgrad/kokoro/blob/main/LICENSE">Apache-2.0</a> which allows commercial use, modification, distribution, and private use.</p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the <a href="https://github.com/hexgrad/kokoro">Kokoro project</a>. For more technical details, see <a href="https://github.com/hexgrad/kokoro/blob/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383">this line</a> in the Kokoro's code.</p>
</div>
<blockquote>
<p dir="auto">Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation</p>
</blockquote>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Melonking Website (126 pts)]]></title>
            <link>https://melonking.net/</link>
            <guid>44852582</guid>
            <pubDate>Sun, 10 Aug 2025 03:38:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://melonking.net/">https://melonking.net/</a>, See on <a href="https://news.ycombinator.com/item?id=44852582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="everything">
            
            

            <canvas id="field" width="1400" height="900"></canvas>

            <svg id="svg_sides" width="100%" height="100%">
                <defs>
                    <pattern id="pattern" patternUnits="userSpaceOnUse" width="128" height="128">
                        <image xlink:href="/images/pdj50186.png" x="0" y="0" width="128" height="128"></image>
                    </pattern>
                    <mask id="svg_mask_side">
                        <rect x="0" y="0" width="100%" height="100%" fill="white"></rect>
                        <image x="50%" y="50%" width="1200" height="1200" href="/images/e-pome-mask.svg" transform="translate(-600,-600)"></image>
                    </mask>
                </defs>
                <rect id="svg_overlay_side" x="0" y="0" width="100%" height="100%"></rect>
            </svg>

            <div id="wrapper">
                

                <p><a href="https://melonking.net/melon.html"><img id="enter" src="https://melonking.net/images/enter_0034-new.gif" onmouseover="enterAudio.play();"></a></p><p>
                    You are now exiting the information superhighway!<br>
                    <span>Enable Auto-Play Audio - Works best in Firefox!</span><br>
                    Welcome to Melonland :^]
                </p>

                
            </div>

            <p>Music: johnny_ripper - by the sea ☺</p>

            </div></div>]]></description>
        </item>
    </channel>
</rss>