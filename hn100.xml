<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 12 Jul 2024 17:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Intel is selling defective 13-14th Gen CPUs (104 pts)]]></title>
            <link>https://alderongames.com/intel-crashes</link>
            <guid>40946644</guid>
            <pubDate>Fri, 12 Jul 2024 15:46:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alderongames.com/intel-crashes">https://alderongames.com/intel-crashes</a>, See on <a href="https://news.ycombinator.com/item?id=40946644">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>My team at Alderon Games, working on the multiplayer dinosaur survival game <a href="https://pathoftitans.com/">Path of Titans</a>, has been encountering significant problems with Intel CPU stability. These issues, including crashes, instability, and memory corruption, are confined to the 13th and 14th generation processors. Despite all released microcode, BIOS, and firmware updates, the problem remains unresolved.</p>
<p>We have identified failures in five main areas:</p>
<ul>
<li>
<strong>End Customers:</strong> Thousands of crashes on Intel CPUs on 13th and 14th Gen CPUs in our crash reporting tools.</li>
<li>
<strong>Official Dedicated Game Servers:</strong> Experiencing constant crashes, taking entire servers down.</li>
<li>
<strong>Development Team:</strong> Developers using these CPUs face frequent instability while building and working on the game. It can also cause SSD and memory corruption.</li>
<li>
<strong>Game Server Providers:</strong> Hosting community servers with persistent crashing issues.</li>
<li>
<strong>Benchmarking Tools:</strong> Decompression and memory tests unrelated to Path of Titans also fail.</li>
</ul>
<p>Over the last 3–4 months, we have observed that CPUs initially working well deteriorate over time, eventually failing. The failure rate we have observed from our own testing is nearly 100%, indicating it's only a matter of time before affected CPUs fail. This issue is gaining attention from news outlets and has been noted by Fortnite and RAD Game Tools, which powers decompression behind Unreal Engine.</p>
<p>Users are also receiving misleading error messages about running out of video driver memory, despite having sufficient memory.</p>
<h2>Actions We Are Taking</h2>
<p>To prevent further harm to our game, we are implementing the following measures:</p>
<ul>
<li>
<strong>Server Migration:</strong> We are swapping all our servers to AMD, which experience 100 times fewer crashes compared to Intel CPUs that were found to be defective.</li>
<li>
<strong>Hosting Recommendations:</strong> We advise anyone hosting Path of Titans servers or selling game servers to avoid purchasing or using 13th and 14th gen Intel CPUs.</li>
<li>
<strong>In-Game Notifications:</strong> We are adding a popup message in-game to inform users with these processors about the issue. Many users are currently unaware of why their game is crashing and what they can do about it.</li>
</ul>
<h2>Resources</h2>
<ul>
<li>
<a href="https://www.epicgames.com/help/en-US/c-Category_Fortnite/c-Fortnite_TechnicalSupport/frequent-crashes-in-fortnite-on-i9-13900k-kf-ks-or-i9-14900k-kf-ks-cpus-a000086852?sessionInvalidated=true">Frequent Crashes in Fortnite on i9-13900K/KF/KS or i9-14900K/KF/KS CPUs</a>
</li>
<li>
<a href="https://www.radgametools.com/oodleintel.htm">RAD Game Tools Intel CPU Issues</a>
</li>
<li>
<a href="https://hardwaretimes.com/pc-gamers-amd-ryzen-intel-13900k-14900k-crash-fail/">PC Gamers are Switching to AMD Ryzen as Intel 13900K/14900K Chips Continue to Crash &amp; Fail</a>
</li>
<li>
<a href="https://www.techspot.com/review/2836-intel-cpu-crash-baseline-spec/">Intel CPUs Are Crashing and It's Intel's Fault: Intel Baseline Profile Benchmark</a>
</li>
</ul>
<p>We look forward to more information becoming available about these problems.</p>
<p>For Intel's sake, we hope they recall these CPUs and refund consumers. This post isn't a endorsement of AMD CPUs or any other PC company. Keep in mind any product can have defects and issues, we just want to let you know where these crashes are coming from and what is going on.</p>
<p>By Matthew Cassells</p>
<p>Founder of Alderon Games</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tau: Open-source PaaS – A self-hosted Vercel / Netlify / Cloudflare alternative (107 pts)]]></title>
            <link>https://github.com/taubyte/tau</link>
            <guid>40946033</guid>
            <pubDate>Fri, 12 Jul 2024 14:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/taubyte/tau">https://github.com/taubyte/tau</a>, See on <a href="https://news.ycombinator.com/item?id=40946033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a href="https://discord.gg/KbN3KN7kpQ" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/discord-btn.png" alt="Join our Discord" height="30">
  </a>
  <a href="https://tau.how/" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/docs-btn.png" alt="Read the Docs" height="30">
  </a>
  <a href="https://console.taubyte.com/" rel="nofollow">
    <img src="https://github.com/taubyte/tau/raw/main/images/sandbox-btn.png" alt="Try our Sandbox" height="30">
  </a>
</p>
<br>
<div dir="auto">
  <a href="https://taubyte.com/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/taubyte/tau/raw/main/images/logo-with-text-tau-white.png">
      <img width="160" src="https://github.com/taubyte/tau/raw/main/images/logo-with-text-tau-black.png" alt="Tau Logo">
    </picture></themed-picture>
  </a>
  
  <p dir="auto"><a href="https://github.com/taubyte/tau/releases"><img src="https://camo.githubusercontent.com/ebb1a5c3e4b55473746c5de17b99da02cab50ea1679c9d7d76e56d4d03904a1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f746175627974652f7461752e737667" alt="Release" data-canonical-src="https://img.shields.io/github/release/taubyte/tau.svg"></a>
<a href="https://github.com/taubyte/tau/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/9a7bccf3d134b27ddedef3543cff4930fdd5744a7249f17e6550aa9f111ea310/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746175627974652f746175" alt="License" data-canonical-src="https://img.shields.io/github/license/taubyte/tau"></a></p>
  <p><strong>
  <p dir="auto"><h2 tabindex="-1" dir="auto">Open Source Git-Native CDN PaaS</h2><a id="user-content-open-source-git-native-cdn-paas" aria-label="Permalink: Open Source Git-Native CDN PaaS" href="#open-source-git-native-cdn-paas"></a></p>
  </strong>
</p></div>

<hr>

<p dir="auto">Tau is a framework for building low maintenance &amp; highly scalable cloud computing platforms that software developers will love!</p>
<p dir="auto"><code>tau</code> is a single binary with no external dependencies except standard system libraries. On top of that, it requires minimal configuration. These are the main steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install Tau</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://get.tau.link/tau | sh"><pre>curl https://get.tau.link/tau <span>|</span> sh</pre></div>
</li>
<li>
<p dir="auto"><strong>Configure</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip"><pre>tau config generate -n yourdomain.com -s compute --services all --ip your_public_ip</pre></div>
</li>
<li>
<p dir="auto"><strong>Launch</strong></p>

</li>
</ol>
<p dir="auto">For a complete step-by-step guide, refer to <a href="https://tau.how/01-getting-started/04-deploy-a-cloud/" rel="nofollow">Deploy tau</a>.</p>
<p dir="auto">Building <code>tau</code> youself is a straightforward <code>go build</code> given you have Go installed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Background</h2><a id="user-content-background" aria-label="Permalink: Background" href="#background"></a></p>
<p dir="auto">The cost and time required to build software, take it from the development environment to production, and then scale it effectively to meet end-user demand are extremely high.</p>
<p dir="auto">Developer-friendly platforms, like the major cloud computing providers, are expensive, lock users in, and overlook local development and E2E testing.</p>
<p dir="auto">This is really a two-sided problem. Do you save on infrastructure cost, or do you lower development time?</p>
<p dir="auto">If you invest in your own platform, it's a rocky road that impedes the speed of development and generally ends up costing more. We all know the Kubernetes fairy tale does not end well!</p>
<p dir="auto">If you invest in development speed, you're limited by your provider's features and cost.</p>
<p dir="auto">To us, solving this problem means:</p>
<ul dir="auto">
<li>Giving you, or your very small team, the ability to build and maintain a cloud computing platform that will go head-to-head with the ones backed by thousands of engineers.</li>
<li>Setting software developers free from infrastructure and operational constraints. We refer to this as "Local Coding Equals Global Production."</li>
</ul>
<p dir="auto"><code>tau</code> solves for building and maintaining a cloud computing platform, and also provides the foundations for an amazing developer experience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimal Configuration</h2><a id="user-content-minimal-configuration" aria-label="Permalink: Minimal Configuration" href="#minimal-configuration"></a></p>
<p dir="auto">One of the reasons <code>tau</code> requires minimal configuration is because it has built-in auto-discovery.
Just like a self-driving car gathering information through sensors, <code>tau</code> will gather information and try to find the best ways to be reachable, available, etc.</p>
<p dir="auto">That said, some configuration like bootstrap peers is necessary. Unless you're running a single-node cloud, each node will need to know at least one other peer.</p>
<p dir="auto">A Cloud built with <code>tau</code> is very dynamic; at a low level, nodes communicate assets, routes, and services, and they also exchange information about other peers. Enriched by distributed services like <code>seer</code> and <code>gateway</code>, the cloud can load-balance incoming requests to ensure optimal performance and reliability.</p>
<p dir="auto">This behavior is built into cloud resources as well. For example, a protocol we call <code>hoarder</code> ensures object storages and databases are replicated; all you need to do is enable it on a few nodes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Coding Equals Global Production</h2><a id="user-content-local-coding-equals-global-production" aria-label="Permalink: Local Coding Equals Global Production" href="#local-coding-equals-global-production"></a></p>
<p dir="auto">In your traditional setup, the platform is a complex set of templates, pipelines, and integrations that ultimately help turn configuration into API calls and code into assets. Because of that complexity, and also the fact that many components need to run inside a very complex environment of their own, it's impossible to satisfy the 'local == production' equation.</p>
<p dir="auto">Granted, there are some solutions that either mock or reroute to dev/prod resources, enabling developers to build or debug locally. However, it's still a 3rd party service you need to integrate and manage.</p>
<p dir="auto">In order to satisfy the equation, we decided to build <code>tau</code> so it simplifies, ports, and/or sandboxes every aspect of the cloud.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Git-Native</h2><a id="user-content-git-native" aria-label="Permalink: Git-Native" href="#git-native"></a></p>
<p dir="auto">Traditionally, you interface with infrastructure through API calls. This is the case for every cloud computing provider alongside orchestration solutions like Kubernetes.</p>
<p dir="auto">A few years back, the concept of GitOps started to make waves, and that was around the time we started building, so we decided to cut the unnecessary garbage between the definition of a cloud resource, which should be stored in Git, and its instantiation.</p>
<p dir="auto">As a result, <code>tau</code> has no API calls to create a serverless function, for example. Instead, it adopts Git as the only way to alter infrastructure.</p>
<p dir="auto">Also, git being core to <code>tau</code> means that nodes in the cloud do tune to a specific branch, by default main or master. Among what it enables is an easy way to set up development environments, for example.</p>
<p dir="auto">A specific use case is local development in which case <a href="https://github.com/taubyte/tau/tree/main/tools/dream">dream-cli</a> nodes can also be tuned to the current branch.</p>
<p dir="auto">In addition to the nodes being on a branch, the application registry, managed by the 'tns' protocol, uses commit ids to version entries, allowing nodes serving the assets to detect new versions, or a roll-back for that matter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Networking</h2><a id="user-content-networking" aria-label="Permalink: Networking" href="#networking"></a></p>
<p dir="auto">Internally, <code>tau</code>, using <a href="https://github.com/libp2p/go-libp2p">libp2p</a>, builds an overlay peer-to-peer network between the nodes, enabling some pretty cool features like:</p>
<ul dir="auto">
<li>Automatic node and protocol discovery &amp; routing. If, for example, a node is down, changes its IP address/port, or the services it supports, other nodes will update the info automatically.</li>
<li>Transport independent. Nodes can use any combination of TCP/IP, WebSocket, QUIC, and more.</li>
<li>NAT Traversal &amp; Circuit Relay, which allow nodes that are not public to be part of the cloud.</li>
</ul>
<p dir="auto">Unless absolutely required, which is extremely rare, no well-designed software should rely on IP addresses and ports. This is why every <code>tau</code> cloud is identified with an FQDN (i.e., enterprise.starships.ws) so no absolute network reference is used in an application. Under the hood, the Cloud will transparently take care of DNS resolution and HTTP load balancing, eliminating the need to set these up.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Storage</h2><a id="user-content-storage" aria-label="Permalink: Storage" href="#storage"></a></p>
<p dir="auto">In every other cloud computing implementation, storage means a location and generally a path. For example, <code>https://tau.how/assets/logo-w.svg</code> has two main components <code>tau.how</code>, which translates to an IP address and a location, and <code>/assets/logo-w.svg</code>, which is a path relative to the location. This way of addressing, called "location-based addressing," is simply not portable. Why? you might ask. Well, for starters, nothing guarantees the data returned is an SVG logo in this case. The other issue is the <code>tau.how</code> host we connected to might not have it.</p>
<p dir="auto">To solve this issue, <code>tau</code> uses content-addressing, a concept introduced by torrent networks and popularized by <a href="https://github.com/taubyte/tau/blob/main">IPFS</a>.</p>
<p dir="auto">So when you request <code>https://tau.how/assets/logo-w.svg</code>, which is actually hosted by a <code>tau</code> Cloud, the host that handles the request will resolve (<code>host=tau.how, path=/assets/logo-w.svg</code>) to a content address, or CID, then retrieve the content reader and then forward it through an HTTP writer to you.</p>
<p dir="auto">A few cool facts about this approach:</p>
<ul dir="auto">
<li>Content is chunked and then stored in a DAG, which means it's deduplicated.</li>
<li>Content can be downloaded from multiple peers in parallel.</li>
<li>Content can be verified as the CID is its hash.</li>
<li>When content is in demand, the cloud automatically dedicates more peers to its distribution.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Computing</h2><a id="user-content-computing" aria-label="Permalink: Computing" href="#computing"></a></p>
<p dir="auto">As of today, <code>tau</code> supports <a href="https://webassembly.org/" rel="nofollow">WebAssembly</a> for computing. The reason we started with it is that it's highly portable and sandboxed. We support containers for CI/CD but not for computing yet. We're working on a way to implement containers and virtual machines while abiding by our principles of portability and sandboxing.</p>
<p dir="auto">Code, binary, images, along with any attached assets, are stored and retrieved using the same principles described in <a href="#storage">Storage</a>, which considerably reduces provisioning time and brings computing close to data (data gravity) and/or user (edge computing).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">E2E Testing</h2><a id="user-content-e2e-testing" aria-label="Permalink: E2E Testing" href="#e2e-testing"></a></p>
<p dir="auto">If you're looking to create E2E tests for projects hosted on <code>tau</code>, you can use <code>dream</code>, a sub-package within <code>tau</code>. We don't have documentation for it yet, but you can quickly learn from tests like <a href="https://github.com/taubyte/tau/blob/main/services/seer/tests/dns_test.go#L35">services/seer/tests/dns_test.go</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running a Local Cloud</h2><a id="user-content-running-a-local-cloud" aria-label="Permalink: Running a Local Cloud" href="#running-a-local-cloud"></a></p>
<p dir="auto">While you can't practically run <code>tau</code> on your local machine, you can do so using <a href="https://github.com/taubyte/tau/tree/main/tools/dream">dream-cli</a>, which is a CLI wrapper around <code>dream</code>. It creates local cloud environments mirroring production settings. Unlike <code>tau</code>, it offers an API for real-time configuration and testing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extending Tau</h2><a id="user-content-extending-tau" aria-label="Permalink: Extending Tau" href="#extending-tau"></a></p>
<p dir="auto"><code>tau</code> can be extended using a plugin system we call <a href="https://github.com/taubyte/tau/tree/main/pkg/vm-orbit">orbit</a>. An open-source example is <a href="https://github.com/ollama-cloud">ollama-cloud</a>, which demonstrates how to add LLM capabilities to your cloud.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">To learn more, check:</p>
<ul dir="auto">
<li><a href="https://taubyte.com/blog/introduction-to-taubyte/" rel="nofollow">Introduction to Taubyte</a></li>
<li><a href="https://taubyte.com/blog/be-competitive-in-few-minutes/" rel="nofollow">Be Competitive in a Few Minutes: Deployment Guide</a></li>
</ul>
<p dir="auto">For comprehensive documentation, visit our <a href="https://tau.how/" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Questions or need assistance? Ping us on <a href="https://discord.com/invite/KbN3KN7kpQ" rel="nofollow">Discord</a>!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows NT for Power Macintosh (182 pts)]]></title>
            <link>https://github.com/Wack0/maciNTosh</link>
            <guid>40945076</guid>
            <pubDate>Fri, 12 Jul 2024 12:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Wack0/maciNTosh">https://github.com/Wack0/maciNTosh</a>, See on <a href="https://news.ycombinator.com/item?id=40945076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Windows NT for Power Macintosh</h2><a id="user-content-windows-nt-for-power-macintosh" aria-label="Permalink: Windows NT for Power Macintosh" href="#windows-nt-for-power-macintosh"></a></p>
<p dir="auto">This repository currently contains the source code for the ARC firmware and its loader, targeting New World Power Macintosh systems using the <em>Gossamer</em> architecture (that is, MPC106 "Grackle" memory controller and PCI host, and "Heathrow" or "Paddington" super-I/O chip on the PCI bus). That is, the following systems:</p>
<ul dir="auto">
<li>iMac G3 (tray-loading)</li>
<li>Power Macintosh G3 (Blue &amp; White) <em>"Yosemite"</em></li>
<li>Macintosh PowerBook G3 Bronze Keyboard <em>"Lombard"</em></li>
<li>Power Macintosh G4 PCI <em>"Yikes!"</em></li>
</ul>
<p dir="auto">The ARC firmware itself runs at a low enough level that it should be compatible with Old World systems using the same chipset too, but there is currently no loader for these systems; these are the following:</p>
<ul dir="auto">
<li>Power Macintosh G3 (beige)</li>
<li>Macintosh PowerBook G3 Series <em>"Wallstreet"</em>, <em>"PDQ"</em></li>
</ul>
<p dir="auto">There may be issues on your hardware; with real hardware, this has only been tested on a Lombard.</p>
<p dir="auto">NT HAL and drivers have no source present for now.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Drivers present in ARC firmware</h2><a id="user-content-drivers-present-in-arc-firmware" aria-label="Permalink: Drivers present in ARC firmware" href="#drivers-present-in-arc-firmware"></a></p>
<ul dir="auto">
<li>Cuda and PMU (albeit Cuda is untested on real hardware)
<ul dir="auto">
<li>ADB keyboard</li>
</ul>
</li>
<li>Flat 32bpp video framebuffer, set up by the loader. Currently the loader only supports ATI hardware (there may be issues with any ATI hardware with fcode version prior to 1.69, only the ATI Rage Pro LT (as present in Lombard) has been tested)</li>
<li>Mac I/O internal IDE controllers, forked from OpenBIOS (<strong>there are no drivers for PCI IDE controllers!</strong>)</li>
<li>USB OHCI forked from OpenBIOS (<strong>broken, nonworking, and initialisation code commented out</strong>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Drivers currently done for NT</h2><a id="user-content-drivers-currently-done-for-nt" aria-label="Permalink: Drivers currently done for NT" href="#drivers-currently-done-for-nt"></a></p>
<ul dir="auto">
<li>HAL, including: NT boot time framebuffer, super I/O interrupt controller, Grackle PCI bus support, Cuda and PMU (including low level ADB), serial port for kernel debugging only
<ul dir="auto">
<li>(please note Cuda support is currently untested on real hardware)</li>
</ul>
</li>
<li>Mac I/O internal IDE controller, forked from <code>atapi.sys</code> from NT4 DDK</li>
<li>General HID/storage driver, intended to also contain a USB stack in future but currently only implements ADB keyboard/mouse and ramdisk as floppy drive for installing drivers at text setup time</li>
<li>Flat 32bpp video framebuffer miniport driver</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software compatibility</h2><a id="user-content-software-compatibility" aria-label="Permalink: Software compatibility" href="#software-compatibility"></a></p>
<p dir="auto">NT4 only, currently. NT 3.51 may become compatible if HAL and drivers get ported to it. NT 3.5 will never be compatible, as it only supports PowerPC 601.
(The additional suspend/hibernation features in NT 3.51 PMZ could be made compatible in theory but in practise would require all of the additional drivers for that to be reimplemented.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preliminary</h3><a id="user-content-preliminary" aria-label="Permalink: Preliminary" href="#preliminary"></a></p>
<ul dir="auto">
<li>Grab binaries from the releases page. Burn the image to optical media.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Partitioning Disk</h3><a id="user-content-partitioning-disk" aria-label="Permalink: Partitioning Disk" href="#partitioning-disk"></a></p>
<ul dir="auto">
<li>Boot your PowerMac from the burned optical media. When you get to ARC firmware menu, go to <code>Run firmware setup</code>, then <code>Repartition disk for NT installation</code>.</li>
<li>The disk partitioner will first let you enter partition size of the NT partition (up to the 16383x16x63 CHS limit, minus 32 MB ARC system partition + 1 MB for partition tables / MBR backup / OS 9 drivers / ARC environment variable storage, giving a maximum possible size of 8030 MB), then will drop to a menu allowing the creation of additional Mac partitions.
<ul dir="auto">
<li>After adding a partition to the list, the only way to remove from the list is by cancelling the operation and starting the partitioner again.</li>
</ul>
</li>
<li>After you have created all Mac partitions you want, choose <code>Finish partitioning and install</code>, and confirm the operation.</li>
<li>When finished, the partitioner will ask to <code>Press any key to restart</code>. Do so, and boot your PowerMac from the CD again.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing NT</h3><a id="user-content-installing-nt" aria-label="Permalink: Installing NT" href="#installing-nt"></a></p>
<ul dir="auto">
<li>Eject CD and insert your NT4 CD.</li>
<li>Go to <code>Run a program</code> and enter the path <code>cd:\ppc\setupldr</code> - this may be <code>cd01:</code> or <code>cd02:</code> (...) if you have multiple optical drives present on your system.
<ul dir="auto">
<li>This may error with <code>The file or device does not exist</code>, just go back to <code>Run a program</code> and try again if so.</li>
</ul>
</li>
<li>NT4 setupldr will start.
<ul dir="auto">
<li>You will receive the message <code>Setup could not determine the type of computer you have</code>.</li>
<li>Choose <code>Other</code> (default selected option), just press <code>Enter</code> when asked for hardware support disk.</li>
<li>Pick your system from the list - all are equivalent and will load the Gossamer chipset HAL <code>halgoss</code>.</li>
</ul>
</li>
<li>Next you will receive the message <code>Setup could not determine the type of one or more mass storage drivers installed in your system</code>. Two drivers need to be loaded at this point:
<ul dir="auto">
<li>press <code>S</code> to pick a driver, choose <code>Other</code> from the list, press <code>Enter</code> when asked for hardware support disk</li>
<li>Choose the first driver <code>Mac I/O IDE Controller</code></li>
<li>follow the previous steps again, but this time choose the second driver <code>PowerMac General HID &amp; Storage</code></li>
<li>finally, press Enter to continue</li>
</ul>
</li>
<li>You will receive the message <code>Setup could not determine the type of video adapter installed in the system</code>. Choose <code>Other</code> from the list, press <code>Enter</code> when asked for hardware support disk, and choose the only option <code>Open Firmware Frame Buffer</code>.</li>
<li>NT will boot and text setup will start. Go through the text setup.</li>
<li>Under <code>Setup has determined that your computer contains the following hardware and software components</code>, change <code>Keyboard</code> from <code>Unknown</code> to <code>XT, AT or Enhanced Keyboard (83-104 keys)</code> and <code>Pointing Device</code> from <code>Unknown</code> to <code>No Mouse or Other Pointing Device</code>.</li>
<li>Choose the <code>C:</code> drive from the partition list. If you chose to create an NT partition of size 2GB or less, it must be formatted.</li>
<li>If you chose to create an NT partition of over 2GB in size, <code>chkdsk</code> will find errors and require a reboot. Boot your PowerMac from the ARC firmware CD again and follow the steps to boot the NT4 text setup again.</li>
<li>Proceed through the rest of NT text and graphical setup as normal.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known issues</h2><a id="user-content-known-issues" aria-label="Permalink: Known issues" href="#known-issues"></a></p>
<ul dir="auto">
<li>If you are looking for a stable operating system, this is not it. Expect bugchecks, expect graphical setup to fail and restart because of bugchecks, etc.
<ul dir="auto">
<li>On a laptop system you may wish to remove the battery. At least on Lombard, the only way to power off the system when it bugchecks is via PMU reset or via total power removal.</li>
</ul>
</li>
<li>Currently the implemented drivers are the bare minimum to run and use NT.</li>
<li>I have observed PMU hard shutdowns on NT boot, fixed only by a PMU reset. No idea what caused this.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dualboot quirks</h2><a id="user-content-dualboot-quirks" aria-label="Permalink: Dualboot quirks" href="#dualboot-quirks"></a></p>
<p dir="auto">If you create additional Mac partitions, please make note of the following:</p>
<ul dir="auto">
<li>The Mac partitions are listed in the partition table as HFS partitions but are not formatted. Use Disk Utility from OS X 10.1 or above to format the partitions. (Erase the <strong>volumes</strong>, not the <strong>drive</strong>!)</li>
<li>The OS X installer, and just booting OS 8/OS 9, will error if a valid MBR is present on the disk at all, which is required for NT. In ARC firmware, go to <code>Run firmware setup</code> then <code>Reboot to OSX install or OS8/OS9</code> if you wish to boot to those listed operating systems.
<ul dir="auto">
<li>Booting back to the ARC firmware will fix the MBR, so be sure to always use this option when unsure.</li>
<li>In particular, formatting the created HFS partitions in OS X 10.2 and 10.3 will not work when a valid MBR is present!</li>
</ul>
</li>
<li>To allow OS 9 to mount the hard disk, boot from an OS 9 CD, run Drive Setup, select the drive and use the <code>Update Driver</code> option from the <code>Functions</code> menu.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building ARC firmware</h2><a id="user-content-building-arc-firmware" aria-label="Permalink: Building ARC firmware" href="#building-arc-firmware"></a></p>
<p dir="auto">You need devkitPPC. Additionally, a <code>libgcc.a</code> compiled for <code>powerpcle</code> must be present in <code>arcgrackle/gccle</code>. If you need to find one, it should be present on any Void Linux mirror, the current filename to search for as of 2024-07-12 is <code>cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps</code> - decompress it by <code>zstdcat cross-powerpcle-linux-gnu-0.34_1.x86_64.xbps -o cross-powerpcle-linux-gnu-0.34_1.x86_64.tar</code>, then pull the file out of the tarball: <code>usr/lib/gcc/powerpcle-linux-gnu/10.2/libgcc.a</code>.</p>
<ul dir="auto">
<li>Ensure <code>DEVKITPPC</code> environment variable is set to your devkitPPC directory, usually <code>/opt/devkitpro/devkitPPC</code></li>
<li>Build the big endian libc: <code>cd baselibc ; make ; cd ..</code></li>
<li>Build the ARC firmware loader: <code>cd arcloader_grackle ; make ; cd ..</code></li>
<li>Build the little endian libc: <code>cd arcgrackle/baselibc ; make ; cd ../..</code></li>
<li>Build the ARC firmware itself: <code>cd arcgrackle ; make ; cd ..</code></li>
</ul>
<p dir="auto">Replace <code>stage1.elf</code> and <code>stage2.elf</code> inside the release image. For recreating the image from a folder dump, use your preferred tool to create a hybrid HFS+ISO image, make sure <code>System</code> folder is blessed and <code>BootX</code> file is of type <code>tbxi</code>.</p>
<p dir="auto">Please note that <code>stage1.elf</code> must not be larger than 16KB and <code>stage2.elf</code> must not be larger than 224KB.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li>libc used is <a href="https://github.com/PetteriAimonen/Baselibc">baselibc</a></li>
<li>ELF loader and makefiles adapted from <a href="https://github.com/fail0verflow/hbc">The Homebrew Channel</a></li>
<li>Some lowlevel powerpc stuff, and ARC firmware framebuffer console implementation and font, adapted from <a href="https://github.com/devkitPro/libogc">libogc</a></li>
<li>Some ARC firmware drivers (IDE, USB) adapted from <a href="https://github.com/openbios/openbios">OpenBIOS</a>
<ul dir="auto">
<li>USB drivers in OpenBIOS were themselves adapted from <a href="https://github.com/coreboot/coreboot">coreboot</a></li>
</ul>
</li>
<li>ISO9660 FS implementation inside ARC firmware is <a href="https://github.com/erincandescent/lib9660">lib9660</a> with some modifications.</li>
<li>FAT FS implementation inside ARC firmware is <a href="http://elm-chan.org/fsw/ff/00index_p.html" rel="nofollow">Petit FatFs</a> with some modifications.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AT&T says criminals stole phone records of 'nearly all' customers in data breach (262 pts)]]></title>
            <link>https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</link>
            <guid>40944505</guid>
            <pubDate>Fri, 12 Jul 2024 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/">https://techcrunch.com/2024/07/12/att-phone-records-stolen-data-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=40944505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">U.S. phone giant AT&amp;T confirmed Friday it will begin notifying millions of consumers about a fresh data breach that allowed cybercriminals to steal the phone records of “nearly all” of its customers, a company spokesperson told TechCrunch.</p>

<p>In a statement, AT&amp;T said that the stolen data contains phone numbers of both cellular and landline customers, as well as AT&amp;T records of calls and text messages — such as who contacted who by phone or text — during a six-month period between May 1, 2022 and October 31, 2022.&nbsp;</p>

	
	


<p>AT&amp;T said some of the stolen data includes more recent records from January 2, 2023 for a smaller but unspecified number of customers.</p>

	
	


<p>The stolen data also includes call records of customers with phone service from other cell carriers that rely on AT&amp;T’s network, the company said.&nbsp;</p>

<p>AT&amp;T said the stolen data “does not contain the content of calls or texts,” but does include calling and texting records that an AT&amp;T phone number interacted with during the six-month period, as well as the total count of a customer’s calls and texts, and call durations — information that is often referred to as metadata. The stolen data does not include the time or date of calls or texts, AT&amp;T said.</p>

<p>Some of the stolen records include cell site identification numbers associated with phone calls and text messages, information that can be used to determine the approximate location of where a call was made or text message sent.</p>

<p>In all, the phone giant said it will notify around 110 million AT&amp;T customers of the data breach, company spokesperson Andrea Huguely told TechCrunch.&nbsp;</p>

	
	


	
	


<p>AT&amp;T published <a href="https://www.att.com/DataIncident" target="_blank" rel="noreferrer noopener nofollow">a website with information for customers</a> about the data incident.<strong> </strong>AT&amp;T also disclosed the data breach in <a rel="nofollow" href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0000732717/000073271724000046/t-20240506.htm">a filing with regulators</a> before the market opened on Friday.</p>

<h2 id="h-breach-linked-to-snowflake">Breach linked to Snowflake</h2>

<p>AT&amp;T said it learned of the data breach on April 19, and that it was <a href="https://techcrunch.com/2024/03/30/att-reset-account-passcodes-customer-data/" target="_blank" rel="noreferrer noopener">unrelated to its earlier security incident</a> in March.&nbsp;</p>

<p>AT&amp;T’s Huguely told TechCrunch that the most recent compromise of customer records were stolen from the cloud data giant Snowflake <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">during a recent spate of data thefts</a> targeting Snowflake’s customers.</p>

	
	


<p>Snowflake allows its corporate customers, like tech companies and telcos, to analyze huge amounts of customer data in the cloud. It’s not clear for what reason AT&amp;T was storing customer data in Snowflake, and the spokesperson would not say.</p>

<p>AT&amp;T is the latest company in recent weeks to confirm it had data stolen from Snowflake, <a href="https://techcrunch.com/2024/05/31/live-nation-confirms-ticketmaster-was-hacked-says-personal-information-stolen-in-data-breach/" target="_blank" rel="noreferrer noopener">following Ticketmaster</a> and <a href="https://techcrunch.com/2024/06/07/snowflake-ticketmaster-lendingtree-customer-data-breach/" target="_blank" rel="noreferrer noopener">LendingTree subsidiary QuoteWizard</a>, and others.</p>

	
	


<p>Snowflake blamed the data thefts on its customers for not using multi-factor authentication to secure their Snowflake accounts, a security feature that the cloud data giant did not enforce or require its customers to use.&nbsp;</p>

<p>Cybersecurity incident response firm Mandiant, which Snowflake called in to help with notifying customers, later said <a href="https://techcrunch.com/2024/06/10/mandiant-hackers-snowflake-stole-significant-volume-data-customers/" target="_blank" rel="noreferrer noopener">about 165 Snowflake customers had a “significant volume of data” stolen from their customer accounts</a>.&nbsp;</p>

	
	


<p>Mandiant attributed the breach to an as-yet-uncategorized cybercriminal group tracked only as UNC5537. Mandiant’s researchers say the hackers are financially motivated and have members in North America and at least one member in Turkey.&nbsp;</p>

<p>Some of the other corporate victims of the Snowflake account thefts had data subsequently published on known cybercrime forums. For AT&amp;T’s part, the company said that it does not believe that the data is publicly available at this time.</p>

<p>AT&amp;T’s statement said it was working with law enforcement to arrest the cybercriminals involved in the breach. AT&amp;T said that “at least one person has been apprehended.” AT&amp;T’s spokesperson said that the arrested individual was not an AT&amp;T employee, but deferred questions about the alleged criminals to the FBI. </p>

	
	


<p>An FBI spokesperson confirmed to TechCrunch on Friday that that after the phone giant contacted the agency to report the breach, AT&amp;T, the FBI and the Department of Justice agreed to delay notifying the public and customers on two occasions, citing “potential risks to national security and/or public safety.” </p>

	
	


<p>“AT&amp;T, FBI, and DOJ worked collaboratively through the first and second delay process, all while sharing key threat intelligence to bolster FBI investigative equities and to assist AT&amp;T’s incident response work,” the FBI spokesperson said.</p>

<p>The FBI did not comment on the arrest of one of the alleged cybercriminals.</p>

<p>This is <a href="https://techcrunch.com/2024/06/29/2024-in-data-breaches-1-billion-stolen-records-and-rising/" target="_blank" rel="noreferrer noopener">the second security incident AT&amp;T has disclosed this year</a>. AT&amp;T was forced to reset the account passcodes of millions of its customers after a cache of customer account information — including encrypted passcodes for accessing AT&amp;T customer accounts — was published on a cybercrime forum. A security researcher told TechCrunch at the time that the encrypted passcodes could be easily decrypted, prompting AT&amp;T to <a href="https://techcrunch.com/2024/04/10/att-notifies-regulators-after-customer-data-breach/" target="_blank" rel="noreferrer noopener">take precautionary action to protect customer accounts</a>.</p>

<p><strong>Read more on TechCrunch:</strong></p>

	
	


<ul>
<li><a href="https://techcrunch.com/2024/07/11/mspy-spyware-millions-customers-data-breach/" target="_blank" rel="noreferrer noopener">Data breach exposes millions of mSpy spyware customers</a></li>



<li><a href="https://techcrunch.com/2024/07/10/apple-alerts-iphone-users-in-98-countries-to-mercenary-spyware-attacks/" target="_blank" rel="noreferrer noopener">Apple warns iPhone users in 98 countries of spyware attacks</a></li>



<li><a href="https://techcrunch.com/2024/07/09/evolve-bank-says-ransomware-gang-stole-personal-data-on-millions-of-customers/" target="_blank" rel="noreferrer noopener">Evolve Bank says ransomware gang stole personal data on millions of customers</a></li>



<li><a href="https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/" target="_blank" rel="noreferrer noopener">OpenAI breach is a reminder that AI companies are treasure troves for hackers</a></li>
</ul>

<p><em>Updated with comment from the FBI.</em></p>

<figure></figure>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[As an Employee, You Are Disposable (2023) (235 pts)]]></title>
            <link>https://nelson.cloud/as-an-employee-you-are-disposable/</link>
            <guid>40943436</guid>
            <pubDate>Fri, 12 Jul 2024 07:41:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nelson.cloud/as-an-employee-you-are-disposable/">https://nelson.cloud/as-an-employee-you-are-disposable/</a>, See on <a href="https://news.ycombinator.com/item?id=40943436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>The recent tech layoffs have shown that employees are disposable in the eyes of executives. This isn’t surprising though and I’m definitely not the first person that has written about this. I just want to highlight the current situation.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/tech-layoff-tracker.webp" alt="TrueUp tech layoff tracker"><figcaption><p><em>Source:
<a href="https://www.trueup.io/layoffs?ref=nelson.cloud" target="_blank">TrueUp: Tech Layoff Tracker</a></em></p></figcaption></figure><p>It doesn’t matter if investor expectations are surpassed, layoffs can still take place.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/cnbc-shopify.webp" alt="CNBC Shopfiy headline"><figcaption><p><em>Source:
<a href="https://www.cnbc.com/2023/05/04/shopify-cuts-20percent-of-its-workforce-shares-surge-on-earnings-beat.html?ref=nelson.cloud" target="_blank">CNBC: Shopify cuts 20% of its workforce; shares surge on earnings beat</a></em></p></figcaption></figure><p>It’s somewhat understandable if a company is struggling financially and resorts to layoffs. However, there’s plenty of companies that are profitable and still lay off the people that earned the company those profits.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/microsoft.webp" alt="Polygon Microsoft headline"><figcaption><p><em>Source:
<a href="https://www.polygon.com/23561210/microsoft-layoffs-xbox-bethesda-halo-infinite-343-industries?ref=nelson.cloud" target="_blank">Polygon: Microsoft mass layoffs reportedly impact Bethesda, Halo Infinite teams</a></em></p></figcaption></figure><p>Many companies are not only profitable, but their executives continue to earn huge sums of money amidst layoffs.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/ars-google.webp" alt="Ars Technica headline"><figcaption><p><em>Source:
<a href="https://arstechnica.com/tech-policy/2023/05/googlers-angry-about-ceos-226m-pay-after-cuts-in-perks-and-12000-layoffs/?ref=nelson.cloud" target="_blank">Ars Technica: Googlers angry about CEO’s $226M pay after cuts in perks and 12,000 layoffs</a></em></p></figcaption></figure><p>Aside from layoffs, employees may have their pay frozen even though company revenues are up. That’s what happened at Microsoft. Let’s not forget that Microsoft is a $2.5 trillion dollar company (at the time of this writing).</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/microsoft-pay-freeze.webp" alt="Techradar headline"><figcaption><p><em>Source:
<a href="https://www.techradar.com/pro/microsoft-workers-protest-landmark-year-ceo-memo-following-pay-freeze?ref=nelson.cloud" target="_blank">Techradar: Microsoft workers protest ’landmark year’ CEO memo following pay freeze</a></em></p></figcaption></figure><p>It doesn’t matter how much value you’ve delivered. It doesn’t matter how much impact you’ve had in a company. It doesn’t matter how long you’ve been at a company. You are still disposable.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/jeremy-joslin.webp" alt="Tweet from @jcj"><figcaption><p><em>Source:
<a href="https://twitter.com/jcj/status/1616482322278420481?ref=nelson.cloud" target="_blank">Jeremy Joslin (@jcj) on Twitter</a></em></p></figcaption></figure><p>This article shows the mindset some very wealthy executives have about the average worker/employee.</p><figure><img loading="lazy" src="https://nelson.cloud/employees-are-disposable/tim-gurner.webp" alt="BBC Tim Gurner headline"><figcaption><p><em>Source:
<a href="https://www.bbc.com/news/business-66803279?ref=nelson.cloud" target="_blank">BBC: Tim Gurner apologises over call for more unemployment to fix worker attitudes</a></em></p></figcaption></figure><p>There are some bits I want to highlight:</p><blockquote><p>“There’s been a systematic change where employees feel the employer is extremely lucky to have them,” Mr Gurner said. “We need to remind people they work for the employer, not the other way around.”</p></blockquote><blockquote><p>[Mr Gurner] has previously made headlines by suggesting young people cannot afford homes because they spend too much on avocado toast.</p></blockquote><h2 id="in-conclusion">In Conclusion…</h2><p>It’s okay to like your job and employer. Just understand that, <strong>as an employee, you are disposable</strong>.</p><h2 id="further-reading">Further Reading</h2><p>Here are some articles I’ve come across that share similar sentiments or are very relevant. I highly recommend giving them a read.</p><ul><li><a href="https://www.qword.net/2023/04/30/maybe-you-should-store-passwords-in-plaintext?ref=nelson.cloud" target="_blank">Maybe you should store passwords in plaintext</a></li><li><a href="https://www.mcsweeneys.net/articles/our-company-is-doing-so-well-that-youre-all-fired?ref=nelson.cloud" target="_blank">Our Company Is Doing So Well That You’re All Fired</a></li><li><a href="https://hbr.org/2022/12/what-companies-still-get-wrong-about-layoffs?ref=nelson.cloud" target="_blank">What Companies Still Get Wrong About Layoffs</a></li><li><a href="https://ludic.mataroa.blog/blog/i-accidentally-saved-half-a-million-dollars/?ref=nelson.cloud" target="_blank">I Accidentally Saved Half A Million Dollars</a></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GE Aerospace Successfully Develops and Tests New Hypersonic Dual-Mode Ramjet (105 pts)]]></title>
            <link>https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet</link>
            <guid>40943253</guid>
            <pubDate>Fri, 12 Jul 2024 06:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet">https://www.geaerospace.com/news/press-releases/ge-aerospace-successfully-develops-and-tests-new-hypersonic-dual-mode-ramjet</a>, See on <a href="https://news.ycombinator.com/item?id=40943253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><ul><li><em>A dual-mode ramjet was developed and tested in less than 11 months</em></li><li><em>Testing demonstrates a threefold increase in airflow compared to previously flight-tested hypersonic technology demonstrators</em></li></ul><p><strong>CINCINNATI – July 10, 2024</strong> – GE Aerospace announced today the successful demonstration of a new, cutting-edge hypersonic dual-mode ramjet. This achievement – which could enable high-speed flight and longer range across numerous multi-mission aircraft – represents the most&nbsp;recent milestone in a diverse portfolio of hypersonic programs.</p><p>The dual-mode ramjet began testing in March of this year in the clean air, continuous flow, high-speed propulsion testing facility in Evendale, OH, just 11 months after the launch of the design effort. The testing delivered promising results, exceeding performance expectations and demonstrating robust operation of a dual-mode ramjet with a threefold (3X) increase in airflow compared to previously flight-tested hypersonic technology demonstrators.</p><p>"The rapid progression from design to testing underscores our commitment to driving innovation in hypersonic technologies,” said Amy Gowder, president and CEO of Defense &amp; Systems at GE Aerospace. “This milestone not only shows the exceptional talent and dedication of our team but also reaffirms our position as a leader in the pursuit of hypersonic flight."</p><p>The successful development and testing of the dual-mode ramjet in such a short period of time was made possible through the collaboration of GE Aerospace’s team of engineers, Innoveering – a company acquired by GE Aerospace in 2022 that specializes in hypersonic propulsion – and GE Aerospace’s Research Center.</p><p>“The technology’s robust performance paves the way for the next phase of development, which will focus on continued testing and technology demonstration in alignment with our roadmap for integrated high-speed propulsion solutions,” said Mark Rettig, vice president &amp; general manager of Edison Works Business &amp; Technology Development at GE Aerospace.</p><p><strong>About&nbsp;GE Aerospace</strong><br>GE Aerospace (NYSE: GE) is a global aerospace propulsion, services, and systems leader with an installed base of approximately 44,000 commercial and 26,000 military aircraft engines. With a global team of 52,000 employees building on more than a century of innovation and learning, GE Aerospace is committed to inventing the future of flight, lifting people up, and bringing them home safely. Learn more about how GE Aerospace and its partners are defining flight for today, tomorrow and the future at&nbsp;<a href="https://www.geaerospace.com/">www.geaerospace.com</a>.&nbsp;</p><p><strong>Media Contact:</strong></p><p>Amanda Mayfield<br><a href="https://www.geaerospace.com/cdn-cgi/l/email-protection#e6a78b87888287c8ab879f808f838a82a68183c885898b"><span data-cfemail="89c8e4e8e7ede8a7c4e8f0efe0ece5edc9eeeca7eae6e4">[email&nbsp;protected]</span></a><br>321-442-1259</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using S3 as a Container Registry (234 pts)]]></title>
            <link>https://ochagavia.nl/blog/using-s3-as-a-container-registry/</link>
            <guid>40942732</guid>
            <pubDate>Fri, 12 Jul 2024 04:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ochagavia.nl/blog/using-s3-as-a-container-registry/">https://ochagavia.nl/blog/using-s3-as-a-container-registry/</a>, See on <a href="https://news.ycombinator.com/item?id=40942732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>For the last four months I’ve been developing a custom container image builder, collaborating with Outerbounds<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. The technical details of the builder itself might be the topic of a future article, but there’s something surprising I wanted to share already: you can use <a href="https://en.wikipedia.org/wiki/Amazon_S3">S3</a> as a container registry! You heard it right. All it takes is to expose an S3 bucket through HTTP and to upload the image’s files to specific paths. With that in place, you can actually <code>docker pull</code> from it. Isn’t that neat?</p>
<p>In the rest of this post I’ll explain how it all works, but let’s start with a demo for the impatient among us. I created a container image that runs <a href="https://en.wikipedia.org/wiki/Cowsay">cowsay</a> and mirrored it to a bucket. Here’s what happens when you pull and run it from the bucket’s url:</p>
<pre tabindex="0"><code>$ docker run --rm pub-40af5d7df1e0402d9a92b982a6599860.r2.dev/cowsay

 _________________________
&lt; This is seriously cool! &gt;
 -------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
</code></pre><p>Don’t you agree with the cow? Note that, for this demo, I’m using <a href="https://www.cloudflare.com/developer-platform/r2/">R2</a> instead of S3 (because it has free egress 😎). Fortunately, it doesn’t matter whether you use R2 or S3, since they are API-compatible. As a matter of fact, I used the AWS SDK to push my image to R2.</p>
<h3 id="but-why">But why?</h3>
<p>Using S3 is not the traditional approach for hosting container images. You’d normally use a container registry, such as <a href="https://hub.docker.com/">DockerHub</a>, <a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry">GitHub Container Registry</a>, <a href="https://aws.amazon.com/ecr/">ECR</a>, etc. What benefits does S3 bring, then, to make deviating from the trodden, <a href="https://boringtechnology.club/">boring</a>, path worthwhile?</p>
<p>Let’s take a step back. We are developing a custom image builder (or bakery, as we affectionately call it) because of speed. We want to go from requirements to a ready-to-pull image in a few seconds. The easiest container registry to use in our case is ECR, because we are on AWS. However, it turns out there’s a substantial performance difference between S3 and ECR when it comes to upload speed!</p>
<p>I discovered the performance gap somewhat by accident. Since speed is important for us, and the first rule of performance optimization is to measure, I instrumented the code to generate <a href="https://medium.com/jaegertracing/jaeger-tracing-a-friendly-guide-for-beginners-7b53a4a568ca">traces</a>. Having that, I went hunting for optimization opportunities and came across something unexpected: the traces showed that pushing layers to the container registry accounted for a significant amount of time! That felt off, so I decided to run a small benchmark: to upload a 198 MiB layer to ECR and to S3, and observe the difference in duration.</p>
<p>Here’s the outcome:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Minimum observed speed</strong></th>
<th><strong>Maximum observed speed</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ECR</strong></td>
<td>24 MiB/s (8.2 s)</td>
<td>28 MiB/s (7.0 s)</td>
</tr>
<tr>
<td><strong>S3</strong></td>
<td>115 MiB/s (1.7 s)</td>
<td>190 MiB/s (1.0 s)</td>
</tr>
</tbody>
</table>
<p>The table shows that S3 is up to 8x faster than ECR, which is almost an order of magnitude<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>! Of course, there are <a href="#caveats">caveats</a>, but “raw” S3 container registries are nevertheless a promising avenue of optimization.</p>
<h3 id="what-makes-s3-faster-than-ecr">What makes S3 faster than ECR?</h3>
<p>The big difference between pushing to ECR and uploading objects to S3 is that the latter allows uploading a single layer’s chunks in parallel. Given enough bandwidth, this yields a massive increase in throughput. In fact, parallel chunked uploads are recommended in the <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html#optimizing-performance-guidelines-scale">AWS docs</a> to maximize bandwidth usage<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p>Why can’t ECR support this kind of parallel uploads? The “problem” is that it implements the <a href="https://github.com/opencontainers/distribution-spec/blob/2291163927cae6f5105a07d32c675c00ff39244c/spec.md">OCI Distribution Spec</a>, which is the standard for container registries (i.e. the reason why you can <code>docker pull</code> and <code>docker push</code> to different registry implementations). According to the specification, a layer push must happen sequentially: even if you upload the layer in chunks, each chunk needs to finish uploading before you can move on to the next one. Needless to say, having a single active connection per layer leaves a significant amount of bandwidth unused!</p>
<p><em>Aside: we also tested the performance of sequential uploads to S3. The result? Throughput went down to ECR-like levels!</em></p>
<h3 id="but-s3-is-not-a-container-registry">But S3 is not a container registry!</h3>
<p>Indeed, S3 is not a container registry in the strict sense of the word. You can’t <code>docker push</code> to it, and the fact that you can <code>docker pull</code> is mostly a happy coincidence. So how does it work?</p>
<p>The answer to our question is revealed by looking at the inner workings of <code>docker pull</code>. Spoiler: it’s HTTP requests all the way down. More specifically, I logged<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> the requests issued by <code>docker pull</code> and saw that they are “just” a bunch of <code>HEAD</code> and <code>GET</code> requests. As an example, see the log of a <code>docker pull my-image:latest</code> at my self-hosted registry (lines starting with <code>#</code> are comments):</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span># Check whether the image's manifest is present in the registry</span>
</span></span><span><span>HEAD /v2/my-image/manifests/latest
</span></span><span><span><span># Download the image's manifest</span>
</span></span><span><span>GET /v2/my-image/manifests/latest
</span></span><span><span><span># Re-download the image's manifest, now addressed using the manifest's hash</span>
</span></span><span><span><span># (I think this is a sanity check by docker)</span>
</span></span><span><span>GET /v2/my-image/manifests/sha256:dabf91b69c191a1a0a1628fd6bdd029c0c4018041c7f052870bb13c5a222ae76
</span></span><span><span><span># Download one of the image's blobs (which happens to be the image's metadata)</span>
</span></span><span><span>GET /v2/my-image/blobs/sha256:a606584aa9aa875552092ec9e1d62cb98d486f51f389609914039aabd9414687
</span></span><span><span><span># Download the remaining image's blob (which happens to be its only layer)</span>
</span></span><span><span>GET /v2/my-image/blobs/sha256:ec99f8b99825a742d50fb3ce173d291378a46ab54b8ef7dd75e5654e2a296e99
</span></span></code></pre></div><p>That’s it! A <code>docker pull</code> is merely downloading files through HTTP! Which means… You can pull containers from <em>any</em> static file server, as long as it has the necessary files at the expected paths and sets the right <code>Content-Type</code> header for each request. Since a S3 bucket is capable of both, a carefully crafted bucket can become a container registry!</p>
<p><em>Aside: if you want to know more about “manifests”, “blobs”, and such, check out my article on <a href="https://ochagavia.nl/blog/crafting-container-images-without-dockerfiles/">Crafting container images without dockerfiles</a> and the <a href="https://github.com/opencontainers/image-spec/blob/036563a4a268d7c08b51a08f05a02a0fe74c7268/spec.md">OCI Image Format Specification</a>.</em></p>
<h3 id="caveats">Caveats</h3>
<p>In case it’s not already clear: this is all very experimental. I’m waiting to do more research before making any serious claims. Will it end up in production? Or will you, my dear reader, send me an email explaining how my approach is utterly flawed?</p>
<p>Note that, while I haven’t made a survey of the container registry offerings out there, it’s obvious they come with features that make them more attractive than dumping files on a bucket. For instance: you can trust the images you upload are actually valid (because the registry uses the standard push method); you can run automated security scans against your layers and receive warnings if there’s anything fishy; you can natively specify who has access to private repositories; etc.</p>
<p>Don’t let these caveats discourage you, though. If it all works as well as I’m hoping, maybe we’ll see a new trend of hosting public container images in Cloudflare’s R2! What would you say to free egress?</p>
<h5 id="ps-what-about-the-whale">PS. What about the whale?</h5>
<p>It’s a pun… <a href="https://www.google.com/search?q=docker+logo&amp;hl=en">go have a look</a> at the docker logo 😉</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Floppy8 – A Tiny Computer, in a Floppy Drive (2023) (148 pts)]]></title>
            <link>https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive</link>
            <guid>40942141</guid>
            <pubDate>Fri, 12 Jul 2024 01:45:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive">https://abe.today/blogs/main/floppy8-a-tiny-computer-in-a-floppy-drive</a>, See on <a href="https://news.ycombinator.com/item?id=40942141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          <p>This post covers the design and creation of the Floppy8, a microcomputer and cartridge system which fits inside a floppy drive.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-07-54_600x600.png?v=1677186494" alt=""></p>
<p>To see it in action, checkout this video!</p>
<p><iframe src="https://www.youtube.com/embed/zvp-eqWCjGU" title="Floppy8 Demo" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="337" height="600" frameborder="0"></iframe></p>
<p>The Floppy8 plays 4K movies and games on custom cartridges, features wireless controllers, status lights, motorized cartridge ejection and more!</p>
<p>Similar to a Famicon or Super Nintendo, the cartridge sticks out, allowing the label to still be partially visible. The front button safely unmounts and ejects the cart when pressed. The RGB indicator LED flashes different colors for waiting, mounting, running and error states.</p>
<p>The wireless controllers are modified off-the-shelf NES clones which have a beige color and replaced d-pads along with extra added weights for that premium feel.</p>
<h2>How (and why) I Made It</h2>
<p><span>Please note: This post contains affiliate links to many of the parts used in this build!</span></p>
<p>If you're like me, you often find yourself browsing Ebay (the online auction site) for weird items in the middle of the night (the time that isn't day) . One evening, while searching for floppy drives, I came across this little gorgeous little guy</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/s-l1600_240x240.jpg?v=1676999242" alt=""><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/s-l16002_240x240.jpg?v=1676999255" alt=""></p>
<div><p>Strangely, the listing identified it as an <a href="https://www.google.com/search?tbm=isch&amp;q=amiga%201010&amp;tbs=imgo:1" target="_blank" rel="noopener noreferrer">Amiga 1010 disk drive</a>. Maybe it is? But it doesn't look like one and I can not find anything that looks similar to it. The mystery intrigued me slightly, but the beautifully simple industrial design intrigued me more. </p><p> I was led to this disk drive that night by a growing fixation on physical media which had been bubbling inside me for the previous months. I began to realize that when you have the ability to watch any film or play any game in seconds... it removes some of the joy.</p><p> Not to mention, physical media which become tarnished or worn over years of love come to tell a story beyond the media contained within. There's a warmth to holding your favorite movie in your hands and that idea had led me to this drive.</p><p> The longer I laid there, basking in the bright white light of the photos of this mysterious Ebay floppy drive, the more I became obsessed. The texture of the front cover, the smooth shiny beige metal outside, the playful and complex rainbow ribbon cable. Before I knew it, I had spent much too much for a disk drive that had to travel much too far to arrive at my door. </p><p> Luckily, I was on a three week work trip while I waited the three weeks of shipping, so the time passed quickly and the question rattled in my brain "what do I do with such a satisfying device?" After all I don't own an Amiga and I don't intend to start, so what's a lad to do.</p><p> I came to the conclusion that the best use of this device was to pull the innards (as carefully as possible) and replace them with a framework of my own design which allowed the drive to be useful and fun for years to come.</p><p> But in 2022 we don't use floppy disks - they are much too impractical, so we'll need a new media, which means we'll need a new drive mechanism, which means we'll need the spirit of adventure because I have no idea how to do that.</p></div>
<h2>Week 0: The Plan</h2>
<p>I had a simple dream for this project...</p>
<ol>
<li>Design a new physical media format that hit a mix of nostalgia and practicality</li>
<li>Mount some computer inside the case to read that media</li>
<li>Design a drive that had a satisfying tactical feel when inserting a cartridge / floppy</li>
<li>Replace the status LED</li>
<li>Replace the mechanical front-eject button with one to eject the new cart type</li>
<li>Leave all original pieces unmodified so they could be put back if needed</li>
<li>Make it actually practical to use and enjoy!</li>
</ol>
<p>On paper a simple task, in practice this project tested my patience and was one of the hardest things I've ever done. As someone who has never done any really practical 3D printing... or electrical engineering... or anything - we had a lot to tackle.</p>
<h2>Week 1: It's real small</h2>
<p>Once the drive finally arrived, I realized how small it really was (which implies that if it <i>is</i> some Amiga 1010 variant that it's probably a late model due to its scale).</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/tiny_600x600.png?v=1677002508" alt=""></p>
<p>The device shipped from Australia and still has a "Bruining Headlam Computers" sticker on it, I assume this is from when it was last serviced many decades ago. This did not help in solving the mystery of what this drive actually was, but I kept the sticker for nostalgia and to keep with my goal of being able to restore the drive to it's original state if needed.</p>
<p>Although it was about 105mm by 43mm and around 5" deep the real concern for me was the height of the floppy disk hole in the front cover. I knew I needed to retain this part so whatever new disk /&nbsp;cartridge format I invented would need to fit within this narrow slow. Luckily, this drive has a slightly taller than average hole, which allows our new cartridges about 4mm of height to play with.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy8_240x240.png?v=1677002638" alt=""> <img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy8-2_240x240.png?v=1677002653" alt="" width="224" height="180"></p>
<p>The other mind-blowing thing was the amazing condition of the device. It looked almost new, with just a few hints of wear. I was as careful as possible to avoid new scratches, dings or other tarnish as I began work on the device.</p>
<p>Sadly over the weeks of working with this device, some of the paint around the front has some very small chips. This saddens me, but I'll be even more careful in the future.</p>
<h2>Week 2: We need Cartridges</h2>
<div><p>Before I could begin laying out any internals, I needed a sense of how the new physical media would work so designing the new media / floppy / cartridge (whatever you'd like to call it) was the top concern in the first few days. I needed to validate that I could manufacture something which was only 4mm tall and held some form of flash media <i>and</i> fit my aesthetic goals. </p><p> Initially, I thought I might retain the visual elements of a 3.5" floppy. I enlisted my brother to help develop our first idea - SD cards attached to custom circuit boards which looked like floppies but extended the SD card wires to an edge-connected pin set.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy-circuit_240x240.png?v=1677002986" alt=""><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/floppy-circuit-2_240x240.png?v=1677003001" alt="" width="233" height="237"></p>
<div><p>Wow! My brother's designs, visually, looked fantastic. They replicted the original disks very closely and they were easy to manufacture. </p><p> The simplicity was great, but I began to realize I needed something which lended itself well to a high-quality glossy label (more like a video game cart than a floppy) and more importantly, it needed to be made of a fairly soft material. I was worried that the sharp edges of a circuit board could damage the front cover of the drive. This concern, of doing this project non-destructively, held for the whole build. With the exception of the back sticker which was cut to reach a screw, this drive was never harmed and could be restored for historic value.</p><p> I investigated a few ideas to encase these circuits, but with only 4mm of space, I could never conquer an idea which fit all the requirements, so we moved to fully 3D printed carts.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/cart-0_480x480.png?v=1677003044" alt=""></p>
<div><p>The initial prototype was compelling. I used the free tool, <a href="https://penpot.app/">Penpot</a> to modify existing art for a game and printed a label on a cart I designed. Initially, I intended to use small USB flash memory sticks embedded in the cart, but I found that the square edges of a USB make it hard to align into a socket (more on this later). They were also slow when benchmarked and just kinda ugly. I would soon pivot to embedding micro SD cards in the 3D printed carts - which works well. </p><p> This was the first tangible element of the device I sometimes called the Floppy-puter or occasionally Floppy8. I designed the carts in the free browser-based, grade-school focused 3D modeling tool <a href="https://tinkercad.com/">Tinkercad</a>. I used this tool for all 3D models for this project. </p><p> While designing the carts, I purchased a small dye-sub printer, called the <a href="https://amzn.to/3kF3Wcn">Polaroid Hi-print</a> and fit the label indent on the cart to the photo size the printer produced. I also bought a <a href="https://amzn.to/3ZVicy1">small tool to round paper corners</a>. Together along with my existing straight cutter, I was able to produce high-quality labels. Here is an overview of the whole cart creation process.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/cart_480x480.gif?v=1677003075" alt="">&nbsp;</p>
<div><p>The cart design was finalized pretty quickly and aside from small fit tweaks the design stayed the same from early on. It was a nice constant to have and a good motivator to hold and look at during frustrating days. </p><p> Eventually, I would make a few carts of games or films I liked. I wanted to make them out of different filament as I loved the mismatched look of different colored 3.5" floppies and wanted to retain that.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/carts_480x480.png?v=1677003603" alt=""></p>
<p>Around this same time, I also began work on the software which would power the computer. I initially began with a Raspbian base (when I thought I would use a Raspberry Pi) and began customizing it to my liking, reducing the desktop UI, changing splash screens, etc.</p>
<p>I knew I would need a fair bit of logic to recognize the insertion of the carts, mount them, run an autostart, then once the program exited or the eject button was pressed it needed to attempt to safely kill the program (if that failed, non-safely kill it) then unmount the drive and trigger a servo to eject the cart. While this was happening it would also be updating the front LED to indicate the device's status.</p>
<p>Originally, I wrote the code as <a href="https://gist.github.com/abeisgoat/b0567184214297cbfe2b2912d132f848" title="a bash script">a bash script</a> and this worked fine until I had many signals coming from different places (e.g. a program exit or eject button press may trigger the eject process). I eventually gave up and wrote the whole software stack in Node and used <a href="https://www.npmjs.com/package/firmata">firmata</a> to talk to the Arduino hardware.&nbsp;</p>
<p>In order to get the LED status to indicate an inserted, but unmounted cart, I used dmesg to read the kernel logs early and reflect insertion before the cart was mounted. I then used fairly straight forward logic to handle cart, LED, and program status.</p>
<p>The final setup requires each cart to have <strong></strong><em>autostart.sh</em> and an <em>autokill.sh</em> along with any binaries / media it needs. Ideally those scripts will boot the program then safely kill it once the eject button is pressed, but it will also force quit if the program hangs.</p>
<p>The software never changed much after this point, just tweaks to make it more reliable.</p>
<p>However, what did change a lot after this point was my approach to getting a Micro SD card reader to fit in the small gap inside the carts where the edge connector of the Micro SD card sticks out.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_16-42-43_480x480.png?v=1677192177" alt=""></p>
<p>After buying and ripping apart a dozen or so SD card readers, I finally found this incredibly tiny USB-C reader which fits perfectly in the gap in the cart. Designing the rest of the drive bay would mean figuring out how to mount this card reader while also aligning the rest of the cart properly so it would smoothly insert when pushed in.</p>
<p>This wasn't <em>that</em> hard but did involve a lot of prototyping as tolerances on 3D printers are not great, so even though something was modeled to fit, that was no guarantee.</p>
<h2>Week 3-6: So many unknowns...</h2>
<p>Once I had a mostly-finished cart design I had to begin prototyping the computer, I knew I had a few big unknowns I had to explore. Some easier than others - but all challenges none-the-less.</p>
<ol>
<li>Is a Raspberry Pi powerful enough to do everything I want to do?</li>
<li>How can I create an eject mechanism for the cartridges?</li>
<li>How can I control the front-facing LED?</li>
<li>Do I need fans?</li>
<li>Will the metal case block wifi?</li>
<li>How will I fit all the wires?</li>
</ol>
<p>Let's start at the beginning...</p>
<h3>Is a Raspberry Pi powerful enough to do everything I want to do?</h3>
<div><p>No - they are not. My initial tests showed the Pi of <i>barely</i> being capable of playing 4K video and definitely not at an acceptable framerate. In my tests, emulator and game performance was terrible for anything but the oldest consoles. This made me question all the Raspberry Pi emulator devices around - either my hardware / software was just terrible or folks aren't really playing those devices much.</p><p> I ended up purchasing a <a href="https://www.lattepanda.com/">Latte Panda 3 Delta</a> - an obscure but really wonderful device. Aside from having a built-in Arduino, it also has headers for USB, power, and more. This made running wires for buttons, ports, lights, etc trivial and despite my constant wiring mistakes the board held up and worked really well.</p><p> In my tests the performance was really reasonable for a small single board computer and in many ways, I don't think the Floppy8 would exist without this device. Sure other single board options exist, but the Latte Panda was really perfect. The only complaint I have is that documentation is minimal, but because the hardware is basically an x86 machine with an embedded Arduino in a tiny package, I did not feel the need for complex docs.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/latte-panda_480x480.png?v=1677003630" alt=""></p>
<p><br> Perhaps the luckiest element of this build was the fact that the Latte Panda just barely fit into the floppy drive. I printed this rose-gold plastic piece with the internal dimensions of the case to see the fit and as you can see, it is snug length-wise, but it sure fits.</p>
<h3>How can I create an eject mechanism for the cartridges?</h3>
<p>I knew early on that I wanted the device to be able to "eject" carts. I experimented with many types of spring-loaded mechanisms, latches, etc but in the end I ended up simply placing a servo with a cog which rotates and pushes the cart out. The actual implementation was simple, but getting power to the servo was tricky. I tried various electircal-engineery ways to get a high amperage 5v line for the servo, but nothing worked. At one point I thought I'd put a battery pack inside the device which would be kept charged and used only for the high-amp spikes for the servo, but somehow in testing I managed to miswire that and burn up the servo. So my final decision was to pull from the 5v line on the LattePanda but put a 0.4amp fuse to protect the board in the case of a jammed servo causing an amperage spike and harming the board. In practice this has worked perfectly fine and despite jams / etc the fuse has never tripped and the board has never shown any issue.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/servo_480x480.png?v=1677003666" alt=""></p>
<h3>How can I control the front-facing LED?</h3>
<p>Controlling an RGB LED is, apparently, trivial.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/led_480x480.png?v=1677003693" alt=""></p>
<p>After soldering up some resistors (for some reason, idk it's what the internet said) you can control the colors straight from a couple PWM pins on an Arduino. This worked quickly and reliably. The terrible mess pictured above was replaced by a nicer heat-shrunk LED later, but the first attempt also worked, it was just ugly.</p>
<h3>Do I need fans?</h3>
<p>Early on, I did some heat stress testing by placing the Latte Panda inside the case and completely blocking the front and back with scrap plastic.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/heat_480x480.png?v=1677003722" alt=""></p>
<div><p>I let the device sit playing 4K video for several hours. The Latte Panda kept a steady 69c internal temp. Warm, but reasonable. The metal case works as a decent radiator and helps expell the warmth into the surrounding air. This was not completely similar to the final build, but it was close enough that I was confident the onboard fan would keep the device cool enough. </p><p> Eventually I printed a heat sensitive cartridge, placed it in the front slot and let the device run for a while.</p></div>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/heat2_480x480.png?v=1677003743" alt=""></p>
<p>This was not informative, but it was cool to look at.</p>
<h3>Will the metal case block wifi?</h3>
<p>This is a quick one - no. Nice.</p>
<h3>How will I fit all the wires?</h3>
<p>I ended up crimping many custom wires for this project. I invested in some <a href="https://amzn.to/3kA3n3G">nice wire</a> for another project and used it all over this one. I also bought a <a href="https://amzn.to/3Dcs2lh">wire-stripper</a> and a <a href="https://amzn.to/3H1vr7M">crimp kit</a>. The most critical and difficult wire to create was the short USB-C wire which attached the Micro SD reader for the carts to the USB headers on the Latte Panda.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/wire_480x480.png?v=1677003772" alt=""></p>
<p>This required soldering to a small <a href="https://amzn.to/3wrefUm">USB-C female port</a>.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/usbc_240x240.png?v=1677003792" alt=""></p>
<p>Most of the other wires were simple - just short :)</p>
<h2>Week 7-10: 3D Printing!</h2>
<p>As the prototyping of core elements began to subside it became time to begin iterating on the most complex element of the Floppy8, the designs for the internal brackets / drive which would actually hold all the components securely.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/3d-breakout_600x600.png?v=1677003823" alt=""></p>
<div><p>As I mentioned before, I used Tinkercad for all the CAD work. This screenshot shows a small percentage of all the digital artifacts of this build. In total I would design and print 201 STL files for this project. Slowly tweaking and iterating fits, tolerance, feels - not to mention iterating on the overall designs as needs came up. </p><p> I've never 3D printed anything which required multiple parts so figuring out how to make parts which could be screwed together and hold strongly took time. The final internal design uses 11 parts...</p></div>
<ol>
<li>
<b>Button (Dark Grey)</b> - This is used to replace the stock front button with one which has a short throw and flairs at the bottom.</li>
<li>
<b>Button Mount (Orange)</b> - This bracket holds the small circuit board which the front button is mounted to.</li>
<li>
<b>Cover Mount (Red)</b> - The original front cover and the button mount screw to this bracket. It has additional holes to attach it to the cart shifter.</li>
<li>
<b>Cart Shifter (Light Blue)</b> - This L shaped piece has two feet on the backside which the Latte Panda rests on and a slight slope on the front to help shift the cart upward and into the actual bay.</li>
<li>
<b>Drive Bay (Brown)</b> - This holds the actual cart when inserted along with the Micro SD card reader. It also serves as a mount for the Servo bracket (Pink) and LED bracket (Light Purple / Beige).</li>
<li>
<b>Servo Bracket (Pink)</b> - This bracket holds the micro-sized servo and screws to the Drive Bay and Cover Mount to add rigidity.</li>
<li>
<b>LED Bracket (Beige and Light Purple)</b> - This wraps about the LED assembly and holds it against the Cover Mount while being screwed to the Drive Bay.</li>
<li>
<b>Rear Bracket (Yellow)</b> - This bracket has the feet for the rear Latte Panda screw holes and also has a mount point for the power switch in the top left. It also has screw holes for the back cover.</li>
<li>
<b>Back Cover (Green)</b> - This cover / vent serve as the back plate for the device.</li>
<li>
<b>Port Cover (Orange)</b> - This removable port cover has carefully cut holes for the USB-C and HDMI ports on the Latte panda, but because the ports are a bit sunk into the device, the piece pops out in case the cables need to go deeper than the plate allows.</li>
<li>
<b>Metal Rods (Dark Grey)</b> - These pieces are made from 3mm risers which have been screwed together to the proper length, then sealed in heat-shrink tubing. They are included in the model for context.</li>
</ol>
<p>It's worth nothing all the parts are printed without supports except the Cover Mount which has those small corners cut out to allow for the heads of screws to attach to the metal riser rods. As that corner is an unsupported overhand on two sides, it needed supports.</p>
<h2>Week 11: Assembly</h2>
<p>As I printed I would attempt to put pieces together, iterate, print again. I learned to design pieces to be small to save print time and slowly over weeks the device started to emerge.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/near-final-assembly_480x480.png?v=1677003861" alt=""></p>
<p>With some assembly working correctly, I was able to check how the Floppy8 would feel with a cart inserted. I wanted it to resemble a Super Nintendo or a Famicom, with the cart label still visible when inserted.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/fit_480x480.png?v=1677003887" alt=""></p>
<p>One of the last elements to sort out was how to mount the internal switch for the front button (and how to replace the front button with one which matched the plastic of the front cover). I tried various options to recreate the visible element of the button, I tried making a mold and casting it in resin, etc. I eventually just printed it and resigned to sanding it smooth. I also found <a href="https://amzn.to/3EDmlOa">these adorable PCBs</a> on Amazon which are perfect to mount a single button.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-49-17_480x480.png?v=1677188971" alt=""></p>
<p>I like this close up because it helps to show how fucked up projects are when you look at them up close. In real life the Floppy8 looks fantastic because it fits in the original shell, but up close the internals have lots of bits which were printed bad or screw holes I had to drill out or whatever. These things aren't as perfect as photos make them look, so don't get down on yourself if your project isn't perfect.</p>
<p>Anyway, after getting these last elements sorted the whole thing felt fantastic to me - it felt real. I kept iterating then one day, like the flip of a switch, I realized we were almost done. I reprinted all the existing parts in black to help reduce unintended internal LED glow and began putting together the final bits.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/assembly_600x600.png?v=1677003908" alt=""></p>
<p>Putting all the pieces together, including a lot of wire crimping, we ended up with a functional but unhoused Floppy8.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/assembled_600x600.png?v=1677003936" alt=""></p>
<p>Testing at this point confirmed that our cartridges were read, the servo ejects them, the LED reflects the status, it all worked. Perhaps amazingly it also all fit in the case.</p>
<h2>Week 12: Final Polish and Controllers</h2>
<p>As the final bits and bobs began to line up I was finally able to do some real testing for heat, cartridge seating, and ~vibe~.</p>
<p>Heat wise, amazingly, even with all the junk in the case the Latte Panda stayed cool. However, there are some air vents directly below the cartridge slot which pump a lot of heat up around the cart. Originally the SD cart reader was mounted in the place with hot glue... you can see where this is going.</p>
<p>Aside from dripping glue down onto the Panda (which luckily missed the fan by a few mil) the glue was also prone to shifting over the course of a long gaming session or film. I redesigned the cartridge slot as two parts, one which aligns the carts and one which firmly holds the SD reader.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-44-05_480x480.png?v=1677188782" alt=""></p>
<p>This redesign made the feel of inserting a cart <em>way</em> better.</p>
<p>I also took this opportunity to work on input devices! I picked up this pair of <a href="https://amzn.to/3Kxryur">clone NES controllers</a>. I also purchase the official Nintendo Switch Bluetooth NES controllers, but the after-market clones are actually beige, not grey like the official ones - so I opted to go with those.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-25-46_480x480.png?v=1677266875" alt=""></p>
<p>I made a few small modifications to the controllers. I replaced the cheap looking matte d-pad with the one from the official controllers so it had the imprinted arrows and felt more premium. Then I took a note out of the book of toy-makers I added a bunch of weight to the controller to make it feel more real.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-27-09_480x480.png?v=1677266988" alt=""></p>
<p>I glued in a ton of ball bearings and tbh this makes way more difference than you'd think! The other very small detail was the light bleed from the connection LED.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-26-48_480x480.png?v=1677266981" alt=""></p>
<p>This small detail highlighted the cheapness / thinness of the plastic, so I used a small piece of heat-shrink tubing around the LED to help focus the light.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-26-57_480x480.png?v=1677267126" alt=""></p>
<p>It's another super tiny detail, but it makes everything feel more polished.</p>
<p>I also purchased a replacement <a href="https://amzn.to/3XTj6Ja">Fire TV remote</a> (which is just bluetooth and works great with Linux) along with an after-market beige shell for it.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-24_13-34-44_240x240.png?v=1677267398" alt=""></p>
<p>Sadly the shell is shipping from China and has a ~2mo wide delivery window. So it hasn't arrived yet. When it does, I'll look into removing branding from the remote's buttons and replacing the shell so we have a proper remote! I considered buying an actual old beige remote, but they realistically didn't have all the buttons I wanted, but if I find the right one, I may migrate the insides of the Fire TV remote to a whole new home in something truly retro!</p>
<h2>Anyway, we're done...</h2>
<p>Finally after weeks of tweaking, ups and down, frustrations and almost giving up at least once a day for weeks, it was done.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/Screenshot_from_2023-02-23_15-10-03_600x600.png?v=1677186628" alt=""></p>
<p>I was able to sit, watch a film, play a game, etc. I made a lot of mistakes, wasted a lot of filament, but at the end of the day I'm very proud that I kept pushing and didn't give up. I'll probably never use the Floppy8 much, but I'm proud to have it on my desk.</p>
<p><img src="https://cdn.shopify.com/s/files/1/0688/6146/0502/files/IMG_4722_600x600.jpg?v=1677189226" alt=""></p>
<p>Thanks for reading and happy hacking!</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AuraFlow v0.1: a open source alternative to Stable Diffusion 3 (132 pts)]]></title>
            <link>https://blog.fal.ai/auraflow/</link>
            <guid>40941853</guid>
            <pubDate>Fri, 12 Jul 2024 00:42:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.fal.ai/auraflow/">https://blog.fal.ai/auraflow/</a>, See on <a href="https://news.ycombinator.com/item?id=40941853">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>Open-source AI is in jeopardy. As community interest in AI models skyrocketed over the past year, we noticed that development of new open-source foundational models came to a halt. Some even boldly announced that open-source AI is dead. Not so fast!</p><p>We are excited to present you the first release of our AuraFlow model series, the largest yet completely open sourced flow-based generation model that is capable of text-to-image generation. AuraFlow is a reaffirmation of the open-source community's resilience and relentless determination.</p><h2 id="how-do-i-use-it">How do I use it?</h2><p>If you want to try out a few quick prompts, go to <a href="https://fal.ai/models/fal-ai/aura-flow/playground?ref=blog.fal.ai">fal’s model gallery</a> to start playing around.</p><p>If you want to build some cool Comfy workflows with the model, get the latest version of <a href="https://github.com/comfyanonymous/ComfyUI?ref=blog.fal.ai" rel="noreferrer">Comfy</a> and download the model weights from our <a href="https://huggingface.co/fal/AuraFlow?ref=blog.fal.ai" rel="noreferrer">HuggingFace page</a>.</p><p>We would love to give a huge shout out to ComfyUI and the HuggingFace 🤗 diffusers 🧨 teams for supporting AuraFlow natively on Comfy and <code>diffusers</code> on day 0!</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-1.png" alt="" loading="lazy" width="1257" height="593" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-1.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-1.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-1.png 1257w" sizes="(min-width: 720px) 720px"><figcaption><span>A fine selection of AuraFlow v0.1 generations</span></figcaption></figure><h2 id="how-this-collaboration-happened">How this collaboration happened</h2><p><a href="https://x.com/cloneofsimo?ref=blog.fal.ai" rel="noreferrer">Simo</a> is one of our favorite researchers in the wild world of generative media models. You may know him from the amazing <a href="https://github.com/cloneofsimo/lora?ref=blog.fal.ai" rel="noreferrer">adaptation of the LoRA paper</a> for text-to-image models. Few months ago, Simo wanted to implement MMDiT from scratch, and see if he would be able to reproduce it. His initial attempts at <a href="https://github.com/cloneofsimo/minRF?ref=blog.fal.ai">https://github.com/cloneofsimo/minRF</a> and its initial result <a href="https://huggingface.co/cloneofsimo/lavenderflow-5.6B?ref=blog.fal.ai">Lavenderflow-v0</a> came out to be promising. Soon, he found various aspects that could be optimized to train the model on a larger scale more efficiently.</p><p>Timing couldn’t have worked out better. Right around this time, we were convinced that a SOTA open-sourced model is the way forward for this space to move forward. We wanted to bring serious resources and compute to scale up the model. We were aligned very well, and thus begun the collaboration.</p><p>AuraFlow demonstrates that collaborative, transparent AI development is not only alive but thriving, ready to tackle the challenges and opportunities of tomorrow's AI landscape.</p><h2 id="technical-details">Technical Details</h2><p>Here, we wanted to share some initial technical details that stand out. We are planning on following up with a more detailed report and possibly a paper as well.</p><p><strong>1. MFU as a first-class citizen</strong></p><p><strong>Most layers don’t need MMDiT Blocks</strong>: While MMDiT achieved good performance, we found that removing many layers to just be single DiT block were much more scalable and compute efficient way to train these models. With careful search in the small-scale proxy, we’ve removed most of the MMDiT blocks and replaced them with large DiT Encoder blocks. These improved the model flops utilization at 6.8B scale by 15%. </p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-2.png" alt="" loading="lazy" width="1110" height="710" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-2.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-2.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-2.png 1110w" sizes="(min-width: 720px) 720px"><figcaption><span>Number of Double Layers and Optimal Learning Rate</span></figcaption></figure><p><strong>Improved training with torch.compile:</strong> At fal, we are already big fans of Torch Dynamo + Inductor, and build on top of this tooling (with a custom dynamo backend) to run our inference workloads super fast (and efficiently utilizing the underlying hardware). Since PT2’s torch.compile is able to handle both forward and backwards passes, AuraFlow’s training was further optimized with its primitives on each layers forward method, and further able to improve MFU by extra 10% ~ 15% depending on the stage.</p><p>2.<strong> Unlock zero-shot learning rate transfer</strong>It is clear that we are not Meta, and would like to have very good hyperparameters even without sweeping them. Fortunately, we noticed MMDiT architectures were also zero-shot LR transferred with maximal-update-parameterization was utilized.Compared to SP, muP was clearly the winner in terms of predictability of learning rate at scale.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-3.png" alt="" loading="lazy" width="1042" height="668" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-3.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-3.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-3.png 1042w" sizes="(min-width: 720px) 720px"><figcaption><span>Standard Parametrization</span></figcaption></figure><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-4.png" alt="" loading="lazy" width="1196" height="774" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-4.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-4.png 1000w, https://blog.fal.ai/content/images/2024/07/Untitled-4.png 1196w" sizes="(min-width: 720px) 720px"><figcaption><span>Maximal Update Parametrization</span></figcaption></figure><p><strong>3. Re-captioned, everything.</strong>It is common trick to recaption everything to make sure there are no faulty text conditions in the dataset. We used our in-house captioner &amp; external captioned dataset to train these models, which improves the quality of the instruction-following significantly. We followed the DALL·E 3 approach to the extreme, and we had no captions that were alt-texts.</p><p><strong>4. Wider, shorter, better!</strong>To further investigate the optimal architecture, we were interested into making a fatter model, i.e., making the architecture overall utilize largest matmul divisible by 256. This lead us into searching for optimal aspect ratio under optimal learning rate found by muP.With these findings, we were confident that aspect ratio of 20 ~ 100 is indeed suitable at larger scale, which was similar with findings from <a href="https://arxiv.org/abs/2010.14701?ref=blog.fal.ai">Scaling Laws for Autoregressive Generative Modeling</a>. We ended up using 3072 / 36, which resulted in model size of 6.8B parameters.</p><figure><img src="https://blog.fal.ai/content/images/2024/07/Untitled-6.png" alt="" loading="lazy" width="1945" height="1409" srcset="https://blog.fal.ai/content/images/size/w600/2024/07/Untitled-6.png 600w, https://blog.fal.ai/content/images/size/w1000/2024/07/Untitled-6.png 1000w, https://blog.fal.ai/content/images/size/w1600/2024/07/Untitled-6.png 1600w, https://blog.fal.ai/content/images/2024/07/Untitled-6.png 1945w" sizes="(min-width: 720px) 720px"><figcaption><span>Number of Parameters / Loss</span></figcaption></figure><p>In the end, we did the best of our ability to improve and effectively find the optimal configurations for large scale training. Utilizing the findings from above, we were able to train a text-to-image model from scratch in our largest possible settings for 4 week of compute time, including 256x256, 512x512, 1024x1024 pre-training and aspect ratio fine-tuning. Final model achieves a GenEval score of 0.63~0.67 during pretraining, and similarly 0.64 after 1024x1024 pretraining. But with prompt-enhancement pipeline similar to DALL·E 3, we were able to achieve 0.703!</p><div data-kg-toggle-state="close">
            <div>
                <h4><span>Prompt for prompt-enhancement</span></h4>
                </div>
            <p><span>A caption is a way that a person would describe an image separated by commas when necessary. All in lower case. Expand the input below into a more detailed caption without changing the original relative positions or interactions between objects, colors or any other specific attributes if they are disclosed in the original prompt. Clarify positional information, colors, counts of objects, other visual aspects and features. Make sure to include as much detail as possible. Make sure to describe the spatial relationships seen in the image. You can use words like left/right, above/below, front/behind, far/near/adjacent, inside/outside. Make sure to include object interactions like "a table is in front of the kitchen pot" and "there are baskets on the table". Also describe relative sizes of objects seen in the image. Make sure to include counts of prominent objects in the image, especially when there is humans in the image. When its a photograph, include photographic details like bokeh, large field of view etc but dont just say it to say something, do it only when it makes sense. When its art, include details about the style like minimalist, impressionist, oil painting etc. Include world and period knowledge if it makes sense to, like 1950s chevrolet etc.</span></p>
        </div><h3 id="challenges-of-distributed-training-on-multi-modal-data">Challenges of distributed training on multi-modal data</h3><p>One of the harshest realities of training image models is that, unlike LLMs, the modality of the data itself can be a real pain to deal with. During AuraFlow’s training, we leveraged our expertise from dealing with distributed storage as well as managing a large fleet of thousands of GPUs.</p><p>Some of this expertise was directly transferable from production grade inference/fine-tuning systems, where we were able to use open source projects like <a href="https://github.com/juicedata/juicefs?ref=blog.fal.ai">JuiceFS</a> and some were more novel challenges like how do you stream massive amounts of data in and out of multiple nodes while leveraging local NVME space as a staging ground to not to reduce the MFU.</p><p>Be on the lookout for a detailed post on how we choose our storage mediums, where we trained this model, how we evaluated GPU performance and managed large clusters!</p><h2 id="what-is-next">What is next?</h2><p>We are not done training! This model is an initial release to kickstart some community engagement. We will continue training the model and apply our learnings from this first attempt. We also noticed that smaller models or MoE’s might be more efficient for consumer GPU cards which have a limiter amount of compute power, so follow closely for a mini version of model that is still as powerful yet much much faster to run. In the meantime, we encourage the community to experiment with what we are releasing today.</p><p>Our goal is to make this model a standard backbone that other innovative work can be built on top of. We look forward to community contributions. If you want to train finetunes, IP-Adapters, or quantizations of the current model, we are happy to support you in any way we can. There is already a vibrant community around fal and Aura models in our Discord. We <a href="https://discord.gg/fal-ai?ref=blog.fal.ai">invite</a> you to join if you want to get involved.</p><p>For business inquiries, please email us at <a href="mailto:hello@fal.ai">hello@fal.ai</a> 😄</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Third Places and Neighborhood Entrepreneurship: Evidence from Starbucks Cafés (102 pts)]]></title>
            <link>https://www.nber.org/papers/w32604</link>
            <guid>40941645</guid>
            <pubDate>Thu, 11 Jul 2024 23:50:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nber.org/papers/w32604">https://www.nber.org/papers/w32604</a>, See on <a href="https://news.ycombinator.com/item?id=40941645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>Working Paper</span> 32604
  </p>

        <p><span>DOI</span> 10.3386/w32604
  </p>

        <p><span>Issue Date</span> <time datetime="2024-06-21T12:00:00Z">June 2024</time>

  </p>

          </div><div>
    <p>
Sociologists have shown that “third places” such as neighborhood cafés help people maintain and use their network ties. Do they help local entrepreneurs, for whom networks are important? We examine whether the introduction of Starbucks cafés into U.S. neighborhoods with no coffee shops increased entrepreneurship. We find that, when compared to census tracts that were scheduled to receive a Starbucks but did not do so, tracts that received a Starbucks saw an increase in the number of startups of 5.0% to 11.8% (or 1.1 to 3.5 firms) per year, over the subsequent 7 years. There was no effect on neighborhoods with prior cafés. A partnership between Starbucks and Magic Johnson focused on underprivileged neighborhoods produced larger effects. Starbucks locations with more square footage and those with a higher number of visits also produced larger effects.
</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physics-Based Deep Learning Book (275 pts)]]></title>
            <link>https://physicsbaseddeeplearning.org/intro.html</link>
            <guid>40941056</guid>
            <pubDate>Thu, 11 Jul 2024 22:10:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://physicsbaseddeeplearning.org/intro.html">https://physicsbaseddeeplearning.org/intro.html</a>, See on <a href="https://news.ycombinator.com/item?id=40941056">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main-content" role="main">
<h2>Welcome …<a href="#welcome" title="Permalink to this headline">#</a></h2>
<figure id="pbdl-logo-large">
<img alt="_images/logo-xl.jpg" src="https://physicsbaseddeeplearning.org/_images/logo-xl.jpg">
</figure>
<p>Welcome to the <em>Physics-based Deep Learning Book</em> (v0.2) 👋</p>
<p><strong>TL;DR</strong>:
This document contains a practical and comprehensive introduction of everything
related to deep learning in the context of physical simulations.
As much as possible, all topics come with hands-on code examples in the
form of Jupyter notebooks to quickly get started.
Beyond standard <em>supervised</em> learning from data, we’ll look at <em>physical loss</em> constraints,
more tightly coupled learning algorithms with <em>differentiable simulations</em>,
training algorithms tailored to physics problems,
as well as
reinforcement learning and uncertainty modeling.
We live in exciting times: these methods have a huge potential to fundamentally
change what computer simulations can achieve.</p>

<hr>
<section id="coming-up">
<h2>Coming up<a href="#coming-up" title="Permalink to this headline">#</a></h2>
<p>As a <em>sneak preview</em>, the next chapters will show:</p>
<ul>
<li><p>How to train networks to infer a fluid flow around shapes like airfoils, and estimate the uncertainty of the prediction. This gives a <em>surrogate model</em> that replaces a traditional numerical simulation.</p></li>
<li><p>How to use model equations as residuals to train networks that represent solutions, and how to improve upon these residual constraints by using <em>differentiable simulations</em>.</p></li>
<li><p>How to more tightly interact with a full simulator for <em>inverse problems</em>. E.g., we’ll demonstrate how to circumvent the convergence problems of standard reinforcement learning techniques by leveraging simulators in the training loop.</p></li>
<li><p>We’ll also discuss the importance of <em>inversion</em> for the update steps, and how higher-order information can be used to speed up convergence, and obtain more accurate neural networks.</p></li>
</ul>
<p>Throughout this text,
we will introduce different approaches for introducing physical models
into deep learning, i.e., <em>physics-based deep learning</em> (PBDL) approaches.
These algorithmic variants will be introduced in order of increasing
tightness of the integration, and the pros and cons of the different approaches
will be discussed. It’s important to know in which scenarios each of the
different techniques is particularly useful.</p>
<div>
<p>Executable code, right here, right now</p>
<p>We focus on Jupyter notebooks, a key advantage of which is that all code examples
can be executed <em>on the spot</em>, from your browser. You can modify things and
immediately see what happens – give it a try by
<a href="https://colab.research.google.com/github/tum-pbs/pbdl-book/blob/main/intro-teaser.ipynb">[running this teaser example in your browser]</a>.</p>
<p>Plus, Jupyter notebooks are great because they’re a form of <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.</p>
</div>
</section>
<section id="comments-and-suggestions">
<h2>Comments and suggestions<a href="#comments-and-suggestions" title="Permalink to this headline">#</a></h2>
<p>This <em>book</em>, where “book” stands for a collection of digital texts and code examples,
is maintained by the
<a href="https://ge.in.tum.de/">Physics-based Simulation Group</a> at <a href="https://www.tum.de/">TUM</a>.
Feel free to contact us if you have any comments, e.g., via <a href="mailto:i15ge%40cs.tum.edu">old fashioned email</a>.
If you find mistakes, please also let us know! We’re aware that this document is far from perfect,
and we’re eager to improve it. Thanks in advance 😀!
Btw., we also maintain a <a href="https://github.com/thunil/Physics-Based-Deep-Learning">link collection</a> with recent research papers.</p>
<figure id="divider-mult">
<a href="https://physicsbaseddeeplearning.org/_images/divider-mult.jpg"><img alt="_images/divider-mult.jpg" src="https://physicsbaseddeeplearning.org/_images/divider-mult.jpg"></a>
<figcaption>
<p><span>Fig. 1 </span><span>Some visual examples of numerically simulated time sequences. In this book, we explain how to realize algorithms that use neural networks alongside numerical solvers.</span><a href="#divider-mult" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="thanks">
<h2>Thanks!<a href="#thanks" title="Permalink to this headline">#</a></h2>
<p>This project would not have been possible without the help of many people who contributed. Thanks to everyone 🙏 Here’s an alphabetical list:</p>
<ul>
<li><p><a href="https://ge.in.tum.de/about/philipp-holl/">Philipp Holl</a></p></li>
<li><p><a href="https://ge.in.tum.de/">Maximilian Mueller</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/patrick-schnell/">Patrick Schnell</a></p></li>
<li><p><a href="https://ge.in.tum.de/">Felix Trost</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/n-thuerey/">Nils Thuerey</a></p></li>
<li><p><a href="https://ge.in.tum.de/about/kiwon/">Kiwon Um</a></p></li>
</ul>
<p>Additional thanks go to
Georg Kohl for the nice divider images (cf. <span id="id1">[<a href="https://physicsbaseddeeplearning.org/references.html#id8" title="Georg Kohl, Kiwon Um, and Nils Thuerey. Learning similarity metrics for numerical simulations. International Conference on Machine Learning, 2020. URL: https://ge.in.tum.de/publications/2020-lsim-kohl/.">KUT20</a>]</span>),
Li-Wei Chen for the airfoil data image,
and to
Chloe Paillard for proofreading parts of the document.</p>
</section>
<section id="citation">
<h2>Citation<a href="#citation" title="Permalink to this headline">#</a></h2>
<p>If you find this book useful, please cite it via:</p>
<div><pre><span></span><span>@book</span><span>{</span><span>thuerey2021pbdl</span><span>,</span>
  <span>title</span><span>=</span><span>{</span><span>Physics</span><span>-</span><span>based</span> <span>Deep</span> <span>Learning</span><span>},</span>
  <span>author</span><span>=</span><span>{</span><span>Nils</span> <span>Thuerey</span> <span>and</span> <span>Philipp</span> <span>Holl</span> <span>and</span> <span>Maximilian</span> <span>Mueller</span> <span>and</span> <span>Patrick</span> <span>Schnell</span> <span>and</span> <span>Felix</span> <span>Trost</span> <span>and</span> <span>Kiwon</span> <span>Um</span><span>},</span>
  <span>url</span><span>=</span><span>{</span><span>https</span><span>:</span><span>//</span><span>physicsbaseddeeplearning</span><span>.</span><span>org</span><span>},</span>
  <span>year</span><span>=</span><span>{</span><span>2021</span><span>},</span>
  <span>publisher</span><span>=</span><span>{</span><span>WWW</span><span>}</span>
<span>}</span>
</pre></div>
</section>









</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating the L1 cache with value speculation (2021) (103 pts)]]></title>
            <link>https://mazzo.li/posts/value-speculation.html</link>
            <guid>40940241</guid>
            <pubDate>Thu, 11 Jul 2024 20:17:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mazzo.li/posts/value-speculation.html">https://mazzo.li/posts/value-speculation.html</a>, See on <a href="https://news.ycombinator.com/item?id=40940241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">



<p>If we have a heuristic to guess some value cheaply, we can remove a data dependency in a tight loop using the branch predictor. This allows the CPU to run more instructions in parallel, increasing performance. If this explanation does not make much sense to you, keep reading to learn about some of the magic making your CPU fast!</p>
<hr>
<p><a href="https://twitter.com/pervognsen">Per Vognsen</a>’s twitter feed is full of neat low-level curiosities, usually leveraging CPU features for some performance benefit.</p>
<p><a href="https://twitter.com/pervognsen/status/1412611878140874757">Recently</a> he tweeted about a trick that I had never heard of – value speculation.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> The trick exploits the branch predictor to guess values, enabling more instruction parallelism and therefore removing a bottleneck on the L1 cache. Note that the bottleneck is <em>not</em> due to L1 cache misses, but on L1 cache <em>hits</em> introducing unwanted data dependencies.</p>
<section id="footnotes" role="doc-endnotes">
<ol>
<li id="fn1"><p>Per, in turn, referenced <a href="https://pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers/">a blog post by Paul Khuong</a> with a real-world example deploying this trick. Paul, in turn, references side-channel attacks.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>In this post I explain the machinery involved, including a primer on branch prediction and CPU caches, so that anybody with a passing knowledge of C and how code is executed on CPUs should be able to follow.</p>
<p>The code for the post is available <a href="https://gist.github.com/bitonic/78887f5d3238bab5e31f3c5a41d404b2">here</a>. All the numbers are from a Xeon E5-1650 v3, an Intel Haswell processor with L1 / L2 / L3 cache of 32kB, 256kB, and 15MB respectively. The code was compiled with <code>clang -O3</code>, and not with <code>gcc</code>, for reasons explained <a href="#compiling">later</a>.</p>
<p>Before starting, I’d like to stress that L1 cache <em>hits</em> are almost certainly <em>not</em> the bottleneck of your application! This is just a very neat trick that illuminates some CPU features, not a guide on how to improve the performance of your average piece of C code.</p>
<h2 id="the-setup-summing-linked-lists">The setup – summing linked lists <a href="#the-setup-summing-linked-lists">#</a></h2>
<p>We have a simple linked list data type, and a function summing all the elements of a given linked list:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>typedef</span> <span>struct</span> Node <span>{</span></span>
<span id="cb1-2">  <span>uint64_t</span> value<span>;</span></span>
<span id="cb1-3">  <span>struct</span> Node <span>*</span>next<span>;</span> <span>// NULL for the last node</span></span>
<span id="cb1-4"><span>}</span> Node<span>;</span></span>
<span id="cb1-5"></span>
<span id="cb1-6"><span>uint64_t</span> sum1<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb1-7">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb1-8">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb1-9">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb1-10">    node <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb1-11">  <span>}</span></span>
<span id="cb1-12">  <span>return</span> value<span>;</span></span>
<span id="cb1-13"><span>}</span></span></code></pre></div>
<p>So far so good. Our test case works as follows: build a linked list where the nodes live sequentially in contiguous memory, then see how long it takes to sum them all up:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>// Allocate 5MB of linked list nodes, and link them sequentially, with</span></span>
<span id="cb2-2"><span>// random data in the `value`s.</span></span>
<span id="cb2-3"><span>uint64_t</span> n <span>=</span> <span>312500</span><span>llu</span><span>;</span> <span>// 312500 * sizeof(Node) = 312500 * 16 bytes = 5000000 bytes</span></span>
<span id="cb2-4">Node <span>*</span>nodes <span>=</span> malloc<span>(</span>n <span>*</span> <span>sizeof</span><span>(</span>Node<span>));</span></span>
<span id="cb2-5"><span>for</span> <span>(</span><span>uint64_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> n <span>-</span> <span>1</span><span>;</span> i<span>++)</span> <span>{</span></span>
<span id="cb2-6">  nodes<span>[</span>i<span>].</span>value <span>=</span> random_uint64<span>();</span></span>
<span id="cb2-7">  nodes<span>[</span>i<span>].</span>next <span>=</span> <span>&amp;</span>nodes<span>[</span>i<span>+</span><span>1</span><span>];</span></span>
<span id="cb2-8"><span>}</span></span>
<span id="cb2-9">nodes<span>[</span>n<span>-</span><span>1</span><span>].</span>value <span>=</span> random_uint64<span>();</span></span>
<span id="cb2-10">nodes<span>[</span>n<span>-</span><span>1</span><span>].</span>next <span>=</span> NULL<span>;</span></span>
<span id="cb2-11"></span>
<span id="cb2-12"><span>// Now sum.</span></span>
<span id="cb2-13">sum1<span>(&amp;</span>nodes<span>[</span><span>0</span><span>]);</span></span></code></pre></div>
<p>On a server with a relatively old Xeon E5-1650 v3, running <code>sum1</code> with the sample data takes 0.36 milliseconds, which means that we’re processing our linked list at roughly 14GB/s. In the rest of the post will will identify the bottleneck and get around it with value speculation, bringing the throughput for this dataset to 30GB/s.</p>
<p>The impact of the fix varies depending on the size of the dataset. If it is already entirely in the CPU cache, the improvement is much more pronounced, since otherwise we are quickly constrained by how fast we can read data from RAM. This graph shows the performance improvement over differently sized datasets (higher is better):</p>
<div>
<figure>
<img src="https://mazzo.li/assets/images/value-speculation-chart.svg" alt="Chart showing the performance of various versions of sum, including sum1 as described above. Multiple iterations of the same functions are run, to ensure the data is already in the cache if possible.">

</figure>
</div>
<p>The chart shows the performance of <code>sum1</code> together with the performance of two improved functions, <code>sum2</code> and <code>sum3</code>. We go from a throughput of 14GB/s in <code>sum1</code> to more than 45GB/s in <code>sum3</code> if the data fits entirely in the L1 cache (the 16kB dataset), with the performance decreasing slightly for datasets fitting in the L2 and L3 cache (128kB and 5MB datasets). If the dataset does not fit entirely in any CPU cache (~4GB dataset) we go from 10GB/s to 15GB/s, which is as fast as the RAM allows.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="2">
<li id="fn2"><p>See remarks in the <a href="#results">last section</a> for more data on why I think 15GB/s is the limit without resorting to deeper changes.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<h2 id="instruction-parallelism-and-branch-prediction">Instruction parallelism and branch prediction <a href="#instruction-parallelism-and-branch-prediction">#</a></h2>
<div>

<p>Modern CPUs do not process instructions serially, but rather handle many at the same time. They read many instructions at once, break them down in stages, and then try to fill all the computation units they have with as many tasks from as many instructions as possible.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> For instance, modern Intel processors are designed for a throughput of 4 instructions per clock cycle, and AMD Zen processors for up to 5 or 6.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>However, branches pose a challenge when wanting to execute instructions in parallel. Let’s go back to our function <code>sum1</code>:</p>
</div>
<section id="footnotes" role="doc-endnotes">
<ol start="3">
<li id="fn3"><p>To expand on this topic, you can start reading on
<a href="https://en.wikipedia.org/wiki/Out-of-order_execution">out-of-order execution</a> and
<a href="https://en.wikipedia.org/wiki/Instruction_pipelining">pipelining</a>.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Agner Fog’s <a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture document</a> contains tons of details about the pipeline characteristics for Intel and AMD x86 processors. The numbers on throughput for each architecture are usually in the “Pipeline” section.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<div id="cb3"><pre><code><span id="cb3-1"><span>uint64_t</span> sum1<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb3-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb3-3">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb3-4">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb3-5">    node <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb3-6">  <span>}</span></span>
<span id="cb3-7">  <span>return</span> value<span>;</span></span>
<span id="cb3-8"><span>}</span></span></code></pre></div>
<p>and its very readable assembly version:</p>
<div id="cb4"><pre><code><span id="cb4-1"><span>; rdi = node and rax = value.</span></span>
<span id="cb4-2"><span>; rax is the return value register (we're returning value)</span></span>
<span id="cb4-3"><span>sum1:</span></span>
<span id="cb4-4">  <span>xor</span>     <span>rax</span><span>,</span> <span>rax</span>                 <span>; value = 0</span></span>
<span id="cb4-5">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                 <span>; if node is NULL, exit, otherwise start loop</span></span>
<span id="cb4-6">  <span>je</span>      end</span>
<span id="cb4-7"><span>loop:</span></span>
<span id="cb4-8">  <span>add</span>     <span>rax</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span><span>]</span>     <span>; value += node-&gt;value</span></span>
<span id="cb4-9">  <span>mov</span>     <span>rdi</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span> <span>+</span> <span>8</span><span>]</span> <span>; node = node-&gt;next</span></span>
<span id="cb4-10">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                 <span>; if node is not NULL, repeat loop,</span></span>
<span id="cb4-11">  <span>jne</span>     loop                     <span>; otherwise exit</span></span>
<span id="cb4-12"><span>end:</span></span>
<span id="cb4-13">  <span>ret</span></span></code></pre></div>
<p>The loop body is made out of 4 instructions, the last of which a jump. Without special measures, every instruction up to the <code>jne</code> must be executed before proceeding to the next instruction, since we need to know if we’ll go to the beginning of the loop or continue. In other words the conditional jump would introduce a barrier in the instruction level parallelism internal to the CPU.</p>
<p>However, executing many instructions at once is so important that dedicated hardware – the <em>branch predictor</em> – is present in all modern CPUs to make an educated guess on which way we’ll go at every conditional jump. The details of how this works are beyond the scope of this blog post, but conceptually your CPU observes your program as it runs and tries to predict which branch will be taken by remembering what happened in the past.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="5">
<li id="fn5"><p>Apart from the ever useful Agner Fog (see Section 3 of the <a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture document</a>), Dan Luu has a <a href="https://danluu.com/branch-prediction/">nice blogpost</a> explaining less dryly various ways of performing branch prediction.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>Even without knowing much about the branch prediction, we expect the predictor to do a great job for our test case – we always go back to the beginning of the loop apart from when we stop consuming the list. On Linux, we can verify that this is the case with <code>perf stat</code>:</p>
<pre><code>$ perf stat ./value-speculation-linux
...
         2,507,580      branch-misses             #    0.04% of all branches</code></pre>
<p>The branch predictor gets it right 99.96% of the time. So the CPU can parallelize our instructions with abandon, right? …right?</p>
<h2 id="data-dependencies-tripping-us-up">Data dependencies tripping us up <a href="#data-dependencies-tripping-us-up">#</a></h2>
<p>Let’s focus on the loop body of <code>sum1</code>:</p>
<div id="cb6"><pre><code><span id="cb6-1"><span>; rdi = node and rax = value.</span></span>
<span id="cb6-2"><span>loop:</span></span>
<span id="cb6-3">  <span>add</span>     <span>rax</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span><span>]</span>     <span>; value += node-&gt;value</span></span>
<span id="cb6-4">  <span>mov</span>     <span>rdi</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span> <span>+</span> <span>8</span><span>]</span> <span>; node = node-&gt;next</span></span>
<span id="cb6-5">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                 <span>; if node is not NULL, repeat loop,</span></span>
<span id="cb6-6">  <span>jne</span>     loop                     <span>; otherwise exit</span></span></code></pre></div>
<p>To increment <code>value</code> (<code>rax</code>), we need to know the value of <code>node</code> (<code>rdi</code>), which depends on the <code>mov</code> in the previous iteration of the loop. The same is true for the <code>mov</code> itself – it is also dependent on the result of the previous <code>mov</code> to operate. So there’s a <em>data dependency</em> between each iteration of the loop: we must have finished reading <code>node-&gt;next</code> (<code>[rdi + 8]</code>) at iteration <span>n</span> before we can start executing the <code>add</code> and <code>mov</code> at iteration <span>n+1</span>.</p>
<p>Moreover, reading the <code>node-&gt;next</code> (<code>[rdi + 8]</code>) is slower than you might think.</p>
<div>
<figure>
<img src="https://mazzo.li/assets/images/lstopo-ram256g-1.svg" alt="Diagram showing the CPU caches for the processor used in this post. Generated with lstopo.">

</figure>
</div>
<div>

<p>Modern CPUs are a lot better at adding numbers than reading from memory. For this reason, a series of fast caches exist between the CPU and main memory. All reading and writing from main memory normally goes through the cache – if the data we are interested in is not already present, the CPU will load a chunk of memory (a “cache line”, 64 bytes on x86) which contains our desired data into the cache.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> The fastest cache is usually called L1 (successive caching layers being predictably called L2, L3, …).</p>
<p>Our setup is the best-case scenario when it comes to CPU caches – we read a bunch of memory sequentially, utilizing every byte along the way. However, even if the L1 cache is very fast, it is not free: it takes around 4 CPU cycles to read from it. This will make our <code>mov</code> and <code>add</code> take at least 4 cycles to complete. The other two instructions, <code>je</code> and <code>test</code>, will take only one cycle.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>So the number of cycles needed to go through a single loop iteration is bounded by the 4 cycles it takes to read from L1 cache. The data I get from the Xeon I tested the program with is roughly consistent with this:</p>
</div>
<section id="footnotes" role="doc-endnotes">
<ol start="6">
<li id="fn6"><p>I say “normally” because the cache can be avoided using streaming SIMD instructions, which can write or copy memory bypassing the cache. However these methods are opt-in, and by default all memory goes through the cache.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Again, Agner Fog’s <a href="https://www.agner.org/optimize/">page on performance</a> is the best resource I could find to source these numbers. For example, if one wanted to find these numbers for a Haswell CPU:</p>
<ul>
<li>The L1 latency (4 cycles) is in section 10.11 of the <a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture guide</a>;</li>
<li>The numbers of cycles it takes to execute <code>mov</code>, <code>add</code>, <code>test</code>, and <code>jne</code> are in the Haswell section of the <a href="https://www.agner.org/optimize/instruction_tables.pdf">instruction tables</a>.</li>
</ul>
<a href="#fnref7" role="doc-backlink">↩︎</a></li>
</ol>
</section>
<pre><code>16kB, 10000 iterations
  sum1:  8465052097154389858,  1.12us,  14.25GB/s,  3.91 cycles/elem,  1.03 instrs/cycle,  3.48GHz,  4.01 instrs/elem
128kB, 10000 iterations
  sum1:  6947699366156898439,  9.06us,  14.13GB/s,  3.95 cycles/elem,  1.01 instrs/cycle,  3.49GHz,  4.00 instrs/elem
5000kB, 100 iterations
  sum1:  2134986631019855758,  0.36ms,  14.07GB/s,  3.96 cycles/elem,  1.01 instrs/cycle,  3.48GHz,  4.00 instrs/elem
4294MB, 1 iterations
  sum1: 15446485409674718527,  0.43 s,   9.94GB/s,  5.60 cycles/elem,  0.71 instrs/cycle,  3.48GHz,  4.00 instrs/elem</code></pre>
<p>The important numbers are <code>cycles/elem</code> and <code>instrs/cycle</code>. We spend roughly 4 cycles per list element (that is to say, per loop iteration), corresponding to a throughput of roughly 1 instruction per cycle. Given that the CPU in question is designed for a throughput of 4 instructions per cycle, we’re wasting a lot of the CPU magic at our disposal, because we’re stuck waiting on the L1 cache.</p>
<h2 id="value-speculation-bailing-us-out">Value speculation bailing us out <a href="#value-speculation-bailing-us-out">#</a></h2>
<p>We finally get to the trick. As discussed, we are stuck waiting on reading what the next node address is. However, in our setup we allocate the list in a contiguous block of memory, and therefore the nodes are always next to each other.</p>
<p>So here’s the key idea: try to guess the next node by just bumping the previous value. If the guess is wrong, set the node to the “real” next value. In C, this is how it would look like:</p>
<div id="cb8"><pre><code><span id="cb8-1"><span>uint64_t</span> faster_sum<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb8-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb8-3">  Node<span>*</span> next <span>=</span> NULL<span>;</span></span>
<span id="cb8-4">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb8-5">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb8-6">    next <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb8-7">    <span>// Guess the next value</span></span>
<span id="cb8-8">    node<span>++;</span></span>
<span id="cb8-9">    <span>// But fix it up if we guessed wrong (in case the nodes are not</span></span>
<span id="cb8-10">    <span>// next to each other).</span></span>
<span id="cb8-11">    <span>if</span> <span>(</span>node <span>!=</span> next<span>)</span> <span>{</span></span>
<span id="cb8-12">      node <span>=</span> next<span>;</span></span>
<span id="cb8-13">    <span>}</span></span>
<span id="cb8-14">  <span>}</span></span>
<span id="cb8-15">  <span>return</span> value<span>;</span></span>
<span id="cb8-16"><span>}</span></span></code></pre></div>
<p>This looks quite bizarre. We are still reading <code>node-&gt;next</code> in the comparison <code>node != next</code> to make sure our guess is right. So at first glance this might not seem like an improvement.</p>
<p>This is where the branch predictor comes in. In the case of lists where most nodes <em>are</em> next to each other (as is the case in our test code), the branch predictor will guess that the <code>if (node != next) { ... }</code> branch is not taken, and therefore we’ll go through loop iterations without having to wait for the L1 read.</p>
<p>Note that when the branch predictor <em>is</em> wrong (for example when the list ends, or if we have non-contiguous nodes) the CPU will need to backtrack and re-run from the failed branch prediction, which is costly (15 to 20 cycles on our processor<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>). However, if the list is mostly contiguous, the trick works and makes our function 50-200% faster.</p>
<section id="footnotes" role="doc-endnotes">
<ol start="8">
<li id="fn8"><p>See “Misprediction penalty” for Haswell processor in Agner Fog’s
<a href="https://www.agner.org/optimize/microarchitecture.pdf">microarchitecture document</a>.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<p>However there is one last challenge remaining to reach the final code and show you numbers – convincing compilers that our code is worth compiling.</p>
<h2 id="compiling">Getting compilers to emit the right code <a href="#compiling">#</a></h2>
<p>Let’s go back to the code we showed for value speculation in C:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>uint64_t</span> faster_sum<span>(</span>Node<span>*</span> node<span>)</span> <span>{</span></span>
<span id="cb9-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb9-3">  Node<span>*</span> next <span>=</span> NULL<span>;</span></span>
<span id="cb9-4">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb9-5">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb9-6">    next <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb9-7">    node<span>++;</span></span>
<span id="cb9-8">    <span>if</span> <span>(</span>node <span>!=</span> next<span>)</span> <span>{</span></span>
<span id="cb9-9">      node <span>=</span> next<span>;</span></span>
<span id="cb9-10">    <span>}</span></span>
<span id="cb9-11">  <span>}</span></span>
<span id="cb9-12">  <span>return</span> value<span>;</span></span>
<span id="cb9-13"><span>}</span></span></code></pre></div>
<p>Both <code>gcc</code> and <code>clang</code> easily deduce that the guessing is semantically pointless, and compile our trick away, making the compiled version of <code>faster_sum</code> the same as <code>sum1</code>. This is an instance where the compiler smartness undoes human knowledge about the underlying platform we’re compiling for.</p>
<p>Per Vognsen’s gist uses the following trick to get compilers to behave – this is the first improvement to our <code>sum1</code>, <code>sum2</code>:</p>
<div id="cb10"><pre><code><span id="cb10-1"><span>static</span> <span>uint64_t</span> sum2<span>(</span>Node <span>*</span>node<span>)</span> <span>{</span></span>
<span id="cb10-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb10-3">  <span>while</span> <span>(</span>node<span>)</span> <span>{</span></span>
<span id="cb10-4">    value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb10-5">    Node <span>*</span>predicted_next <span>=</span> node <span>+</span> <span>1</span><span>;</span></span>
<span id="cb10-6">    Node <span>*</span>next <span>=</span> node<span>-&gt;</span>next<span>;</span></span>
<span id="cb10-7">    <span>if</span> <span>(</span>next <span>==</span> predicted_next<span>)</span> <span>{</span></span>
<span id="cb10-8">      <span>// Prevent compilers optimizing this apparently meaningless branch away</span></span>
<span id="cb10-9">      <span>// by making them think we're changing predicted_next here.</span></span>
<span id="cb10-10">      <span>//</span></span>
<span id="cb10-11">      <span>// This trick, however, does not work with GCC, only with clang. GCC here</span></span>
<span id="cb10-12">      <span>// derives that `next` and `predicted_next` are the same, and therefore</span></span>
<span id="cb10-13">      <span>// merges them into the same variable, which re-introduces the data</span></span>
<span id="cb10-14">      <span>// dependency we wanted to get rid of.</span></span>
<span id="cb10-15">      asm<span>(</span><span>""</span> <span>:</span> <span>"+r"</span><span>(</span>predicted_next<span>));</span></span>
<span id="cb10-16">      node <span>=</span> predicted_next<span>;</span></span>
<span id="cb10-17">    <span>}</span> <span>else</span> <span>{</span></span>
<span id="cb10-18">      node <span>=</span> next<span>;</span></span>
<span id="cb10-19">    <span>}</span></span>
<span id="cb10-20">  <span>}</span></span>
<span id="cb10-21">  <span>return</span> value<span>;</span></span>
<span id="cb10-22"><span>}</span></span></code></pre></div>
<p>However <code>gcc</code> still doesn’t fully fall for it, as explained in the comment.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> Moreover, <code>clang</code>’s generated loop is not as tight as it could, taking 10 instructions per element. So I resorted to manually writing out a better loop, which we’ll call <code>sum3</code>:<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="9">
<li id="fn9"><p>This is why I stuck to <code>clang</code> for this post. I don’t know what compiler Per is using for his tests.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Here I show the assembly version in Intel syntax, but <a href="https://gist.github.com/bitonic/78887f5d3238bab5e31f3c5a41d404b2#file-value-speculation-linux-c-L121">in the code</a> I write inline assembly, using AT&amp;T syntax since it is better supported.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<div id="cb11"><pre><code><span id="cb11-1"><span>; rax = value, rcx = next, rdi = node</span></span>
<span id="cb11-2"><span>; Note that rax is the return value register (we are returning the value)</span></span>
<span id="cb11-3"><span>sum3:</span></span>
<span id="cb11-4">  <span>xor</span>     <span>rax</span><span>,</span> <span>rax</span>                   <span>; value = 0</span></span>
<span id="cb11-5">  <span>xor</span>     <span>rcx</span><span>,</span> <span>rcx</span>                   <span>; next = NULL</span></span>
<span id="cb11-6">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                   <span>; if node is null, go to the end,</span></span>
<span id="cb11-7">  <span>je</span>      end                        <span>; otherwise start loop</span></span>
<span id="cb11-8"><span>loop_body:</span></span>
<span id="cb11-9">  <span>add</span>     <span>rax</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span><span>]</span>       <span>; value += node-&gt;value</span></span>
<span id="cb11-10">  <span>mov</span>     <span>rcx</span><span>,</span> <span>qword</span> <span>ptr</span> <span>[</span><span>rdi</span> <span>+</span> <span>8</span><span>]</span>   <span>; next = node-&gt;next</span></span>
<span id="cb11-11">  <span>add</span>     <span>rdi</span><span>,</span> <span>16</span>                    <span>; node++</span></span>
<span id="cb11-12">  <span>cmp</span>     <span>rcx</span><span>,</span> <span>rdi</span>                   <span>; if node is equal to next,</span></span>
<span id="cb11-13">  <span>je</span>      loop_body                  <span>; restart loop, otherwise fix up node</span></span>
<span id="cb11-14">  <span>mov</span>     <span>rdi</span><span>,</span> <span>rcx</span>                   <span>; node = next</span></span>
<span id="cb11-15">  <span>test</span>    <span>rdi</span><span>,</span> <span>rdi</span>                   <span>; if node is not NULL restart the loop,</span></span>
<span id="cb11-16">  <span>jne</span>     loop_body                  <span>; otherwise exit.</span></span>
<span id="cb11-17"><span>end:</span></span>
<span id="cb11-18">  <span>ret</span></span></code></pre></div>
<p>The code relies on the fact that <code>node</code> can’t be <code>NULL</code> after we increment it if it is equal to <code>next</code>, avoiding an additional test, and taking only 5 instructions per element (from <code>loop_body</code> to <code>je loop_body</code> in the happy path).<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<section id="footnotes" role="doc-endnotes">
<ol start="11">
<li id="fn11"><p>The original version of <code>sum3</code> took 6 instructions per cycle, until <a href="https://twitter.com/RhialtoTheM/status/1418926515526459394">Rihalto pointed out</a> a needless jump.<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<h2 id="results">Results <a href="#results">#</a></h2>
<p>These are the final numbers for our four functions:</p>
<pre><code>16kB, 10000 iterations
  sum1:  8465052097154389858,  1.12us,  14.25GB/s,  3.91 cycles/elem,  1.03 instrs/cycle,  3.48GHz,  4.01 instrs/elem
  sum2:  8465052097154389858,  0.57us,  27.97GB/s,  1.99 cycles/elem,  5.02 instrs/cycle,  3.48GHz, 10.01 instrs/elem
  sum3:  8465052097154389858,  0.36us,  44.96GB/s,  1.24 cycles/elem,  4.05 instrs/cycle,  3.48GHz,  5.01 instrs/elem
128kB, 10000 iterations
  sum1:  6947699366156898439,  9.05us,  14.14GB/s,  3.95 cycles/elem,  1.01 instrs/cycle,  3.49GHz,  4.00 instrs/elem
  sum2:  6947699366156898439,  4.51us,  28.38GB/s,  1.97 cycles/elem,  5.09 instrs/cycle,  3.49GHz, 10.00 instrs/elem
  sum3:  6947699366156898439,  3.79us,  33.80GB/s,  1.65 cycles/elem,  3.03 instrs/cycle,  3.49GHz,  5.00 instrs/elem
5000kB, 100 iterations
  sum1:  2134986631019855758,  0.35ms,  14.09GB/s,  3.95 cycles/elem,  1.01 instrs/cycle,  3.48GHz,  4.00 instrs/elem
  sum2:  2134986631019855758,  0.19ms,  26.27GB/s,  2.12 cycles/elem,  4.72 instrs/cycle,  3.48GHz, 10.00 instrs/elem
  sum3:  2134986631019855758,  0.17ms,  28.93GB/s,  1.93 cycles/elem,  2.60 instrs/cycle,  3.48GHz,  5.00 instrs/elem
4294MB, 1 iterations
  sum1: 15446485409674718527,  0.44 s,   9.66GB/s,  5.76 cycles/elem,  0.69 instrs/cycle,  3.48GHz,  4.00 instrs/elem
  sum2: 15446485409674718527,  0.33 s,  13.19GB/s,  4.22 cycles/elem,  2.37 instrs/cycle,  3.48GHz, 10.00 instrs/elem
  sum3: 15446485409674718527,  0.30 s,  14.20GB/s,  3.91 cycles/elem,  1.28 instrs/cycle,  3.47GHz,  5.00 instrs/elem</code></pre>
<p><img src="https://mazzo.li/assets/images/value-speculation-chart.svg"></p>
<p>The numbers are provided <a href="https://gist.github.com/bitonic/78887f5d3238bab5e31f3c5a41d404b2#file-value-speculation-linux-c-L262">by the Linux <code>perf_event_open</code> syscall</a>.</p>
<p>The first three datasets are meant to fit in the L1 / L2 / L3 cache. In those cases, the improvements are very pronounced, and <code>sum3</code> is crunching the data at around 4 instructions per cycle, which should be close to the limit on the processor I tested the code on. When the data does not fit in the cache, the bottleneck becomes filling it, and we process the data at roughly 15 GB/s.</p>
<p>I believe that this is as fast as one can go with “simple” single-threaded reading from RAM,
and it’s consistent with data from <code>sysbench</code>:</p>
<pre><code>$ sysbench memory --memory-block-size=1G --memory-oper=read --threads=1 run
...
102400.00 MiB transferred (15089.75 MiB/sec)
...</code></pre>
<p>The RAM-reading speed could probably be improved using SIMD streaming instructions or by reading from multiple threads, although the implementation would be significantly more complicated.</p>
<p>And so we complete our journey into this low-level trick! If you want more of this, I can’t reccomend <a href="https://twitter.com/pervognsen">Per’s account</a> enough – figuring out how his tricks works has been very educational.</p>
<p>Thanks to <a href="https://scvalex.net/">Alexandru Scvortov</a>, <a href="https://nh2.me/">Niklas Hambüchen</a>, Alex Appetiti, and <a href="https://twitter.com/cartazio">Carter T Schonwald</a> for reading drafts of this post. Niklas also clarified some details regarding RAM speeds, and suggested <code>sysbench</code> to measure single threaded RAM reading speed in particular. Also thanks to Per Vognsen and Jason Rohem for spotting a few typos, and to <a href="https://twitter.com/RhialtoTheM">Rihalto</a> for pointing out a better <code>sum3</code> and some misleading wording.</p>
<h2 id="bonus-track-a-compiler-friendly-c-version">Bonus track – a compiler friendly C version <a href="#bonus-track-a-compiler-friendly-c-version">#</a></h2>
<p><a href="https://twitter.com/_monoid/status/1418663360871141376">Alexander Monakov suggested</a> a more robust C function which works well with both <code>gcc</code> and <code>clang</code>, performs as well as <code>sum3</code>, and does not resort to any assembly:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>uint64_t</span> sum5<span>(</span>Node <span>*</span>node<span>)</span> <span>{</span></span>
<span id="cb14-2">  <span>uint64_t</span> value <span>=</span> <span>0</span><span>;</span></span>
<span id="cb14-3">  Node <span>*</span>next <span>=</span> NULL<span>;</span></span>
<span id="cb14-4">  <span>for</span> <span>(;</span> node<span>;</span> node <span>=</span> node<span>-&gt;</span>next<span>)</span> <span>{</span></span>
<span id="cb14-5">    <span>for</span> <span>(;;)</span> <span>{</span></span>
<span id="cb14-6">      value <span>+=</span> node<span>-&gt;</span>value<span>;</span></span>
<span id="cb14-7">      <span>if</span> <span>(</span>node <span>+</span> <span>1</span> <span>!=</span> node<span>-&gt;</span>next<span>)</span> <span>{</span></span>
<span id="cb14-8">        <span>break</span><span>;</span></span>
<span id="cb14-9">      <span>}</span></span>
<span id="cb14-10">      node<span>++;</span></span>
<span id="cb14-11">    <span>}</span></span>
<span id="cb14-12">  <span>}</span></span>
<span id="cb14-13">  <span>return</span> value<span>;</span></span>
<span id="cb14-14"><span>}</span></span></code></pre></div>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebVM is a server-less virtual Linux environment running client-side (559 pts)]]></title>
            <link>https://webvm.io/</link>
            <guid>40940225</guid>
            <pubDate>Thu, 11 Jul 2024 20:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webvm.io/">https://webvm.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40940225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="loginLink">
                    <p><span id="networkStatus">Connect via Tailscale </span>
	              <span id="ipCopied">Copied! </span>
                    </p>
	            <p><img src="https://webvm.io/assets/tailscale.svg" height="35px">
		  </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Karpathy: Let's reproduce GPT-2 (1.6B): one 8XH100 node 24h $672 in llm.c (177 pts)]]></title>
            <link>https://github.com/karpathy/llm.c/discussions/677</link>
            <guid>40939707</guid>
            <pubDate>Thu, 11 Jul 2024 19:21:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/karpathy/llm.c/discussions/677">https://github.com/karpathy/llm.c/discussions/677</a>, See on <a href="https://news.ycombinator.com/item?id=40939707">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="6923990" data-target-translation-type="discussion">
        <tr>
    <td>
        <p dir="auto">In this post we are reproducing GPT-2 in llm.c. This is <strong>"the GPT-2"</strong>, the full, 1558M parameter version that was introduced in OpenAI's blog post <a href="https://openai.com/index/better-language-models/" rel="nofollow">Better Language Models and their Implications</a> in February 14, 2019. llm.c does so directly in C/CUDA (total of ~5,000 lines of code), without the typical training stack that would involve the Python interpreter and a significantly more complex deep learning library like PyTorch/JAX, huggingface/transformers, or etc. In 2019, training GPT-2 was an involved project from an entire team and considered a big model run but, ~5 years later, due to improvements in compute (H100 GPUs), software (CUDA, cuBLAS, cuDNN, FlashAttention) and data (e.g. the FineWeb-Edu dataset), we can reproduce this model on a single 8XH100 node in 24 hours, and for $672, which is quite incredible. There are some caveats and asterisks involved though - llm.c is still not perfectly tuned, sufficiently stabilized (we still see loss spikes and bad activation ranges now and then), and our evals are not comprehensive (e.g. we do not carefully eval multilingual, code, math); A lot of this work is still ongoing.</p>
<p dir="auto"><strong>Unicorns</strong>. The natural place to start is probably with unicorns. In the GPT-2 blog post, OpenAI prompted GPT-2 with this unicorn prompt. Let's see what our model thinks about English-speaking unicorns in the Andes mountains:</p>
<p dir="auto"><strong>Prompt</strong>: <em>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</em><br>
<strong>Completion</strong>: <em>It may sound like a hoax, or just a dream of the mythical animals, but it appears this may be a real case of the unicorn language. The Andean area has an abundance of minerals and is also very rich in biodiversity. According to the researchers' study, the unicorns' language is called Elveseo. To produce it, the animal forms a consonant from the lips and makes a vowel sound with the teeth, then produces a consonant sound with the mouth and makes an 'ow' sound from its chest. "The question is, what caused this, and what does this tell us about intelligence and communication?" commented study co-author Paul Marek. Speaking in the video, the unicorns had the right way to speak, the animals were able to communicate and talk in perfect English.</em></p>
<p dir="auto">Fun! :) The model is fairly coherent and qualitatively somewhere around the level of GPT-2. You can find 20 samples from both GPT-2 and the llm.c model <a href="http://llmc.s3-us-west-2.amazonaws.com/html/gpt2_vs_llmc30kedu.html" rel="nofollow">here</a>, or generate many more using instructions down below.</p>
<p dir="auto"><strong>Training</strong>. Training a GPT-2 with llm.c is quite simple because it is written in C/CUDA, so there is no need for minconda, Python, PyTorch, etc. You will want an 8XH100 GPU box, I recommend spinning one up from <a href="https://lambdalabs.com/" rel="nofollow">Lambda labs</a>. But llm.c is flexible on its compute - if you have only 1 GPU you can still get your GPT-2, you'll just have to wait 8 days instead of 1. If you have 16 GPUs (e.g. using the new Lambda 1 Click Clusters), you'll be able to train multinode and only have to wait 12 hours. Once you spin up your node, here are the complete instructions to train your GPT-2 (this only takes a ~minute from blank box to start stepping):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install cudnn so we can use FlashAttention and run fast (optional)
# https://developer.nvidia.com/cudnn-downloads
# for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

# &quot;install&quot; cudnn-frontend to ~/
git clone https://github.com/NVIDIA/cudnn-frontend.git

# install MPI (optional, if you intend to use multiple GPUs)
# (you might also have to install NVIDIA NCCL if it doesn't come with your setup)
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

# download and enter llm.c repo
git clone https://github.com/karpathy/llm.c.git
cd llm.c

# download the &quot;starter pack&quot; (~1GB download)
# contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s
./dev/download_starter_pack.sh

# download the training dataset (FineWeb-Edu 100B token) .bin data shards
# note: this is a total of 1001 data shards. If you only want to test things
# out and don't want to do an actual run, feel free to append the number of
# training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)
# the full dataset is ~200GB, we can store it here in dev/data directory.
cd dev/data
./edu_fineweb.sh

# compile (~1 min 1st time for cuDNN mostly, few sec from then on)
cd ../../
make train_gpt2cu USE_CUDNN=1

# and train! (wait 24 hours here)
mpirun -np 8 ./train_gpt2cu \
	-i &quot;dev/data/edu_fineweb100B/edu_fineweb_train_*.bin&quot; \
	-j &quot;dev/data/edu_fineweb100B/edu_fineweb_val_*.bin&quot; \
	-o &quot;log_gpt2_1558M&quot; \
	-v 250 -s 300000 -g 384 \
	-h 1 \
	-b 16 -t 1024 \
	-d 1048576 \
	-r 0 \
	-z 1 \
	-c 0.1 \
	-k &quot;cosine&quot; \
	-l 0.0006 \
	-q 0.1 \
	-u 700 \
	-n 2000 \
	-x 32000 \
	-ge 1 \
	-y 1 \
	-e &quot;d48&quot;"><pre><span><span>#</span> install cudnn so we can use FlashAttention and run fast (optional)</span>
<span><span>#</span> https://developer.nvidia.com/cudnn-downloads</span>
<span><span>#</span> for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04</span>
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

<span><span>#</span> "install" cudnn-frontend to ~/</span>
git clone https://github.com/NVIDIA/cudnn-frontend.git

<span><span>#</span> install MPI (optional, if you intend to use multiple GPUs)</span>
<span><span>#</span> (you might also have to install NVIDIA NCCL if it doesn't come with your setup)</span>
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

<span><span>#</span> download and enter llm.c repo</span>
git clone https://github.com/karpathy/llm.c.git
<span>cd</span> llm.c

<span><span>#</span> download the "starter pack" (~1GB download)</span>
<span><span>#</span> contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s</span>
./dev/download_starter_pack.sh

<span><span>#</span> download the training dataset (FineWeb-Edu 100B token) .bin data shards</span>
<span><span>#</span> note: this is a total of 1001 data shards. If you only want to test things</span>
<span><span>#</span> out and don't want to do an actual run, feel free to append the number of</span>
<span><span>#</span> training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)</span>
<span><span>#</span> the full dataset is ~200GB, we can store it here in dev/data directory.</span>
<span>cd</span> dev/data
./edu_fineweb.sh

<span><span>#</span> compile (~1 min 1st time for cuDNN mostly, few sec from then on)</span>
<span>cd</span> ../../
make train_gpt2cu USE_CUDNN=1

<span><span>#</span> and train! (wait 24 hours here)</span>
mpirun -np 8 ./train_gpt2cu \
	-i <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_train_*.bin<span>"</span></span> \
	-j <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_val_*.bin<span>"</span></span> \
	-o <span><span>"</span>log_gpt2_1558M<span>"</span></span> \
	-v 250 -s 300000 -g 384 \
	-h 1 \
	-b 16 -t 1024 \
	-d 1048576 \
	-r 0 \
	-z 1 \
	-c 0.1 \
	-k <span><span>"</span>cosine<span>"</span></span> \
	-l 0.0006 \
	-q 0.1 \
	-u 700 \
	-n 2000 \
	-x 32000 \
	-ge 1 \
	-y 1 \
	-e <span><span>"</span>d48<span>"</span></span></pre></div>
<p dir="auto">I will describe the args in a second. You'll see a bunch of prints scroll through and then the optimization will begin:</p>
<div data-snippet-clipboard-copy-content="num_parameters: 1557686400 => bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=> setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
..."><pre><code>num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
...
</code></pre></div>
<p dir="auto">We can see that each step is about 2.75 seconds and there are 32,000 of them, so now we wait ~24 hours. At every step, this training run takes a chunk of ~1 million tokens of <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="nofollow">FineWeb-EDU</a> (these are educational web pages from the internet), and updates the 1558 million weights of the model to be slightly better at predicting the next token in a sequence. By the end we'll have processed 32,000 * 1048576 = 33.6B tokens in total. The loss goes down as we do a better job predicting the next token. The norm will stabilize around 0.1-1, the learning rate is being warmed up over the first few steps. Our model flops utilization (MFU) is around 50%, i.e. quite efficient.</p>
<p dir="auto">Now wait 24 hours for this to finish, then you can visualize the <code>main.log</code> log file using the <a href="https://github.com/karpathy/llm.c/blob/master/dev/vislog.ipynb">dev/vislog.ipynb</a> jupyter notebook. For this you will need to also have Python and matplotlib installed, and you will see the following:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/241138/345149886-0ddc8c19-aa6a-4342-9292-81f40e49d5ad.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ1MTQ5ODg2LTBkZGM4YzE5LWFhNmEtNDM0Mi05MjkyLTgxZjQwZTQ5ZDVhZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wMjBhOGYxYzdhOTY4YTUyMjk0MzRhMmU4NmUyZTBiYzk4ODkzODIyZWI3OTMxMGM2ZWY5YzA1NDRhNDFkN2JiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.D8gaGy45j3mIysF2wxtmR9MbL8gjVwPGM4Zxxv7wr10"><img width="849" alt="image" src="https://private-user-images.githubusercontent.com/241138/345149886-0ddc8c19-aa6a-4342-9292-81f40e49d5ad.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ1MTQ5ODg2LTBkZGM4YzE5LWFhNmEtNDM0Mi05MjkyLTgxZjQwZTQ5ZDVhZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wMjBhOGYxYzdhOTY4YTUyMjk0MzRhMmU4NmUyZTBiYzk4ODkzODIyZWI3OTMxMGM2ZWY5YzA1NDRhNDFkN2JiJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.D8gaGy45j3mIysF2wxtmR9MbL8gjVwPGM4Zxxv7wr10"></a>
<p dir="auto"><strong>Evals</strong>. On the left we are tracking the loss on FineWeb-EDU validation data. If you simply run the GPT-2 released by OpenAI and evaluate its loss on this split, you get the red horizontal line (loss 2.83). You see that our run outperforms this very very quickly, by step ~5,000. However, this is not a fair comparison because GPT-2 was trained on the never-released WebText dataset, so there is a possibly large distribution shift. So e.g. if you finetune the OpenAI model for 1,000 steps at LR 1e-4, the loss quickly plunges to the blue line (loss 2.61), because it's quickly adapting to the new data statistics. I like to look at the validation loss as a sanity check, but for the actual comparison we'd want to look at fixed, 3rd party evaluations. One of the well-behaved, smooth, common, often-cited evals that also offer early signal is the <a href="https://rowanzellers.com/hellaswag/" rel="nofollow">HellaSwag</a> eval. These are simple common sense scenarios and the model has to pick the correct continuation. We evaluate HellaSwag on the right pane, where we see that we cross over the GPT-2 model around step ~25K (earlier than GPT-2, which is estimated to have been trained on ~100B tokens. This possibly has to do with increased data quality, as we also observed in our earlier <a href="https://github.com/karpathy/llm.c/discussions/481" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">124M run</a>). The green line is the GPT-3 model of the same size, which is pretty much the same model architecture as GPT-2 with minor differences (context length 1024 -&gt; 2048) but trained for 300B tokens (i.e. ~10X more tokens than what we trained on here). I should say that even HellaSwag is not an ideal single point of comparison because it tests simple English and common sense, it does not test e.g. multilingual, math or code. It could have been that the WebText data mixture was a lot heavier on these, and these domains were "stealing" model capacity to some extent, we don't know because it was never released. Lastly, in general, good evals are harder at low model capability like GPT-2 because e.g. the models don't understand multiple choice, and their samples are not high enough quality to make above chance dent into standard math or code evals.</p>
<p dir="auto"><strong>Args guide</strong>. Let's look at the args we passed into the training now in more detail. The GPT-2 release from OpenAI included model weights but very few details, while GPT-3 release had no weights but many details. So in many cases, we follow the GPT-3 paper hyperparameters because the GPT-2 paper has very very little information:</p>
<ul dir="auto">
<li><code>-i -j</code> are training and validation splits token files, downloaded earlier with <code>edu_fineweb.sh</code></li>
<li><code>-o</code> is the output directory to write logs and checkpoints into</li>
<li><code>-v 250</code> asks to evaluate and log the validation loss every 250 steps</li>
<li><code>-s 300000</code> asks to sample some tokens every 300000 steps. Because the total number of steps will be less than this, this is hacky way to turn sampling off and we will only sample a single time at the very end.</li>
<li><code>-g 384</code> sets the number of tokens to be sampled at the end to be 384</li>
<li><code>-h 1</code> asks to evaluate the HellaSwag accuracy</li>
<li><code>-b 16</code> sets the micro-batch size to 16 . If you are running out of memory, decrease this value, e.g. try 8, 4, 2, all the way down to 1 potentially.</li>
<li><code>-t 1024</code> sets the maximum sequence length to 1024, as GPT-2 did</li>
<li><code>-d 1048576</code> asks that the total batch size be 2 to the power 20, following the GPT-3 paper hyperparameters table. The code will make sure to meet this desired total batch size and calculate the needed gradient accumulation "inner loop" steps of the optimization. For example up above, we saw that we have 8 GPUs each doing 16 X 1024 tokens, so that is 8 X 16 X 1024 = 131,072 tokens per micro-step (a single forward backward), so the code calculated gradient accumulation steps of 8 to meet the desired 1M batch size per step. i.e. it does forward+backward 8 times and then a single update.</li>
<li><code>-r 0</code> sets recompute to zero. Recompute is a way to trade off compute and memory. If <code>-r 1</code>, then we recompute a piece of the forward pass (the GeLU) during backward. This means we don't have to cache it and save memory, at the cost of some  more compute. So if you're running out of memory, try -r 1, or -r 2 (also recompute layernorms).</li>
<li><code>-z 1</code> turns on ZeRO-1 (i.e. optimizer state sharding) across multiple GPUs. If you're training with &gt; 1 GPU, this setting is a no-brainer and should basically always be on. On 1 GPU this setting is a no-op.</li>
<li><code>-c 0.1</code> sets the weight decay to 0.1. Only (2D) weights are decayed exactly as in GPT-2, and this number comes from the GPT-3 paper</li>
<li><code>-k "cosine"</code> sets the cosine learning rate schedule, which is the default so this is a bit spurious.</li>
<li><code>-l 0.0006</code> sets the maximum learning rate to 6e-4. The GPT-3 paper says to use 2e-4 for this model size, but here we triple and it and seems to train faster and without any issues. This wasn't tuned very carefully yet.</li>
<li><code>-q 0.1</code> says that we will decay the learning rate to 10% of max LR over the course of training, following GPT-3 paper.</li>
<li><code>-u 700</code> says that we will ramp up the learning rate from 0 to max learning rate over the first 700 iterations, which at total batch size 0.5M is 350M tokens, following GPT-3 paper.</li>
<li><code>-n 2000</code> asks to save model checkpoints every 2000 steps.</li>
<li><code>-x 32000</code> asks for 32K steps in total. I chose this number because it is a nice number, and just fits into 24 hours.</li>
<li><code>-ge 1</code> sets a very recently merged gelu recompute setting for CublasLt (optional)</li>
<li><code>-y 1</code> sets the "resume" flag on. If your training for any reason crashes or hangs, you can CTRL+C and re-run this command, and it will attempt to resume the optimization. llm.c is bitwise-deterministic, so you'll get the identical result as if you didn't crash.</li>
<li><code>-e "d48"</code> asks to initialize, a depth 48 GPT-2 model from scratch.</li>
</ul>
<p dir="auto"><strong>Memory guide.</strong> The biggest constraint most people will probably face is that their GPU doesn't have 80GB. That's okay you should still be able to run everything above if you are patient, it would just run slower. So if the model doesn't fit, what do you play with? The most important one is the micro batch size <code>-b</code>. Try to decrease it but keep it to nice numbers. So e.g. 16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1. From there, try to also play with the recompute setting <code>-r</code> which is 0 (fastest, a lot of memory), 1 (very slightly slower, but a huge memory saving), or 2 (slightly slower, smaller memory saving). The next thing you can do is disable master weights in fp32, which you can do with <code>-w 0</code> (1 is default). We won't maintain fp32 copy of params. Empirically in a few runs before this seems to be okay, likely due to our use of stochastic rounding. If even that doesn't fit (that's unlikely right?), you could try to decrease the maximum sequence length with <code>-t</code>, default is 1024 you can take it down to 512, 256, etc., but now you are making your model worse because you're decreasing its maximum attention span.</p>
<p dir="auto"><strong>Code.</strong> Certainly I feel biased but llm.c is quite beautiful:</p>
<ul dir="auto">
<li>It only requires basic CUDA dependencies to run.</li>
<li>It is a direct, minimal and readable implementation in C/CUDA. llm.c totals about 5,000 lines of C/CUDA code. We try to be mostly C, not C++ to keep it simple. Neural net training is just one while loop of the same, simple arithmetic operations (think +, -, *, /) on a single float array, it really shouldn't be that complicated.</li>
<li>It compiles and runs very quickly (few seconds), so you're doing more stepping and less waiting.</li>
<li>It allocates all of its GPU memory a single time at the start and from then on during training has an exactly constant memory footprint. So once you start stepping, you know you're good for the rest of the run and won't OOM.</li>
<li>It is bitwise deterministic.</li>
<li>It is efficient, at just below ~50% MFU.</li>
</ul>
<p dir="auto">The main entry point and the majority of the code is in the file <a href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu">train_gpt2.cu</a>. It contains the GPT-2 model definition and the training loop in ~2,000 LOC, and it imports a bunch of helper files with various utilities and the individual layer implementations from the <code>llmc</code> directory. <code>cloc llmc</code> reports 23 files with 3170 LOC, and <code>cloc train_gpt2.cu</code> is 1353 LOC atm.</p>
<p dir="auto"><strong>Multi-node training</strong>. If you are part of the privileged GPU-rich upper class, llm.c supports multi-node training and the most GPUs I've seen someone train llm.c with is ~500 GPUs. This biggest run I've done personally so far is on Lambda's new 1-click cluster feature with 16XH100 GPUs in 2 nodes. The downsides of unemployment. The lambda team has put up <a href="https://github.com/LambdaLabsML/llm.c-1cc/tree/main">detailed instructions</a> on how you can train llm.c models on their 1-click clusters. E.g. with the 512-GPU H100 cluster for $2,300/hr, you might be able to train your GPT-2 in ~30 minutes. You'd have to increase the total batch size (e.g. to ~8M) and possibly tune the hyperparameters a little. I haven't tried but it probably works and would be very cool :)</p>
<p dir="auto"><strong>PyTorch comparison</strong>. A relatively comparable run in PyTorch would I think look something like this, using our parallel PyTorch implementation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin &quot;dev/data/edu_fineweb100B/edu_fineweb_train_*.bin&quot; \
    --input_val_bin &quot;dev/data/edu_fineweb100B/edu_fineweb_val_*.bin&quot; \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1"><pre>torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_train_*.bin<span>"</span></span> \
    --input_val_bin <span><span>"</span>dev/data/edu_fineweb100B/edu_fineweb_val_*.bin<span>"</span></span> \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1</pre></div>
<p dir="auto">The PyTorch code is meant as a testing reference not an actual implementation, so the training loop is a little bit different in some places (e.g. the dataloader doesn't permute the shards, etc.), but this is still possibly useful as a point of reference. I also hacked the default vocab size to be 50257 -&gt; 50304 to get added efficiency, then the currently PyTorch nightly gives:</p>
<div data-snippet-clipboard-copy-content="step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
..."><pre><code>step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
...
</code></pre></div>
<p dir="auto">Now I wouldn't say I have full confidence that the PyTorch script is maximally tuned, but the following observations can be made. PyTorch seems to be taking a lot more memory (this run is ~80GB), while llm.c is at 57GB (29% improvement). Memory is important because it allows you to crank up the batch size (e.g. llm.c can go up to 24 microbatch here), which goes a bit faster. Second, we're seeing about 3386 vs. 2750ms per iteration, so llm.c is stepping ~19% faster. Some of the gains here have known origin, e.g. llm.c includes optimizations like the Fused classifier that kicks off the backward pass, which is something torch.compile does not do today afaik. But it's also possible that this script isn't fully maximally tuned, but in any case I'm showing the comparison in case 1) others would like to take a look, play with, compare, help tune and 2) to just say that llm.c is quite optimized and fast - in the specific case of GPT-2/3 training.</p>
<p dir="auto"><strong>The final model</strong>. A few links that may be helpful, for posterity:</p>
<ul dir="auto">
<li>The <a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/main.log" rel="nofollow">main.log</a> file.</li>
<li>The <a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/model_00032000.bin" rel="nofollow">model_00032000.bin</a> llm.c bin model file</li>
<li>The model converted to huggingface transformers GPT-2 model I uploaded here: <a href="https://huggingface.co/karpathy/gpt2_1558M_final2_hf" rel="nofollow">karpathy/gpt2_1558M_final2_hf</a>.</li>
</ul>
<p dir="auto"><strong>Model export</strong>. The model export can be done as follows, for example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export"><pre>python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export</pre></div>
<p dir="auto">This then lets you run the Eleuther eval harness, or run the huggingface sampling pipeline to get model samples:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# take model for spin
import torch

output = &quot;./gpt2_1558M_final2_hf&quot;

# set pytorch seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

prompt = &quot;In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&quot;
from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(output)
model = AutoModelForCausalLM.from_pretrained(output, attn_implementation=&quot;flash_attention_2&quot;, torch_dtype=torch.bfloat16, device_map='cuda')
model.eval()
tokens = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;)
tokens = tokens.to('cuda')

output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4)
samples = tokenizer.batch_decode(output)
for sample in samples:
    print('-'*30)
    print(sample)"><pre><span># take model for spin</span>
<span>import</span> <span>torch</span>

<span>output</span> <span>=</span> <span>"./gpt2_1558M_final2_hf"</span>

<span># set pytorch seeds</span>
<span>torch</span>.<span>manual_seed</span>(<span>42</span>)
<span>torch</span>.<span>cuda</span>.<span>manual_seed</span>(<span>42</span>)

<span>prompt</span> <span>=</span> <span>"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>output</span>)
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>output</span>, <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>, <span>device_map</span><span>=</span><span>'cuda'</span>)
<span>model</span>.<span>eval</span>()
<span>tokens</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>)
<span>tokens</span> <span>=</span> <span>tokens</span>.<span>to</span>(<span>'cuda'</span>)

<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>tokens</span>, <span>max_new_tokens</span><span>=</span><span>500</span>, <span>pad_token_id</span><span>=</span><span>tokenizer</span>.<span>eos_token_id</span>, <span>do_sample</span><span>=</span><span>True</span>, <span>top_k</span><span>=</span><span>50</span>, <span>num_return_sequences</span><span>=</span><span>4</span>)
<span>samples</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>output</span>)
<span>for</span> <span>sample</span> <span>in</span> <span>samples</span>:
    <span>print</span>(<span>'-'</span><span>*</span><span>30</span>)
    <span>print</span>(<span>sample</span>)</pre></div>
<p dir="auto">Also have a look at <a href="https://github.com/karpathy/llm.c/tree/master/dev/eval">dev/eval</a> for instructions on how to run the Eleuther Evaluation Harness, the evals from the HuggingFace Open LLM Leaderboard, etc.</p>
<p dir="auto"><strong>400B token run</strong>. I have also made the attempt to train GPT-2 for significantly longer than 33B tokens. In particular, I changed -x to 400,000 to train for 420B tokens (even more than GPT-3 model of this size, which was trained with 300B). This model run looked great until about step 330,000:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/241138/347626140-8708850a-c29e-427e-8e14-fb6ba7d7776a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ3NjI2MTQwLTg3MDg4NTBhLWMyOWUtNDI3ZS04ZTE0LWZiNmJhN2Q3Nzc2YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hZTUwOTVmZTZjZjZhYWNmZTg1OTc3YzllYzVjODQzNzk1NWY4MzA4YWVkNjIzYmQ5MTllMDU1MGEwMGJkYmYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.Bp_T57O0jNGvZvwEP2gCddZAPRmuyJVJ6m8m_VENnxw"><img width="1293" alt="image" src="https://private-user-images.githubusercontent.com/241138/347626140-8708850a-c29e-427e-8e14-fb6ba7d7776a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MzAxMDUsIm5iZiI6MTcyMDcyOTgwNSwicGF0aCI6Ii8yNDExMzgvMzQ3NjI2MTQwLTg3MDg4NTBhLWMyOWUtNDI3ZS04ZTE0LWZiNmJhN2Q3Nzc2YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDMwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hZTUwOTVmZTZjZjZhYWNmZTg1OTc3YzllYzVjODQzNzk1NWY4MzA4YWVkNjIzYmQ5MTllMDU1MGEwMGJkYmYzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.Bp_T57O0jNGvZvwEP2gCddZAPRmuyJVJ6m8m_VENnxw"></a>
<p dir="auto">This model dramatically beats GPT-2 and GPT-3 of its size on HellaSwag (it gets up to ~61%), but sadly becomes unstable there on and explodes. There are more smaller spikes along the way but the code is configured to detect the more simple instantaneous instability and skips update (I used the flags <code>-sl 5.0 -sg 5.0</code>), which helps mitigate and defers issues. However, I think we're not yet being sufficiently careful with our initialization, activation ranges, and overall model training stability and there are deeper issues that gradually drift the model into instability, especially for larger models and over long training duration. To be continued. If you have ideas or recommendations for stabilizing LLM model training please contribute your experience in the discussion below.</p>
<p dir="auto"><strong>FAQ</strong>:</p>
<ul dir="auto">
<li>Can I <strong>sample</strong> from the model in llm.c? kind of, but it's inefficient and a bit weird, and even more hacky if you'd like to prompt the model. Use the huggingface paths above for now.</li>
<li>Can I <strong>chat</strong> with it? no, this is currently only pretraining, not chat finetuning.</li>
<li>Can you train in <strong>fp8</strong>? No, we're currently mostly training in bf16, but early versions are very much work in progress.</li>
<li>I have a non-NVIDIA GPU can I run llm.c? No, llm.c supports C/CUDA only, but good forks exist (see main README). For example there is an actively maintained <a href="https://github.com/anthonix/llm.c">AMD fork</a> by <a data-hovercard-type="user" data-hovercard-url="/users/anthonix/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/anthonix">@anthonix</a> that is quite good.</li>
</ul>
<p dir="auto"><strong>GPT-2 (124M)</strong>. I wanted to also link to an earlier post on training the <a href="https://github.com/karpathy/llm.c/discussions/481" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">GPT-2 (124M) model</a> in llm.c, which has some more related information to llm.c runs. 124M is a smaller model in the GPT-2 miniseries, only 124M parameters compared to 1558M parameters.</p>
<p dir="auto"><strong>Authors</strong></p>
<p dir="auto">Substantial contributions to llm.c came from what now feels like the llm.c core dev team, in addition to self:</p>
<ul dir="auto">
<li><a data-hovercard-type="user" data-hovercard-url="/users/ngc92/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ngc92">@ngc92</a> in all aspects of the code base</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/ademeure/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ademeure">@ademeure</a> in CUDA kernel optimization, low precision training, cudnn, cublas, ...</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/gordicaleksa/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/gordicaleksa">@gordicaleksa</a> in all aspects of whatever is next on the TODO list, from algorithms to code to multi-node or etc.</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/rosslwheeler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/rosslwheeler">@rosslwheeler</a> in CI and Windows support. If you're happily running llm.c on Windows you should definitely thank Ross :)</li>
<li><a href="https://lambdalabs.com/" rel="nofollow">Lambda labs</a> for sponsoring the GPUs used in the development of llm.c. The history here is that I've happily used Lambda for several years and then a few months ago I pretty please asked if they are open to not charging my account for llm.c dev work and they agreed so here we are thank you for supporting llm.c!</li>
</ul>
<p dir="auto"><strong>Coming up</strong>. Some of the next big steps we are interested in and looking at these days:</p>
<ol dir="auto">
<li>Further optimize GPT-2 training hyperparameters. For some reason, the hyperparameters cited by OpenAI in the GPT-3 paper appear to be quite suboptimal, e.g. @Yuchenj_UW on X found that you can 3X the learning rate and get faster training with no apparent downsides. There might be other similar low-hanging fruit.</li>
<li>Improve training and scaling stability, e.g. more stable optimizers, schedulers, clipping, norming, muP. (Some of these PRs already exist, if you have tips on stabilizing LLM runs please reach out with ideas to try!).</li>
<li>Mixed precision++: training with fp8 (imminent!).</li>
<li>Model inference, e.g. KV cache is the low hanging fruit here.</li>
<li>Finetuning: SFT, RLHF</li>
<li>Multimodal extensions, VQVAE and friends</li>
<li>More modern architectures, support for Llama / Gemma model series.</li>
</ol>
<p dir="auto">The goal of llm.c remains to have a simple, minimal, clean training stack for a full-featured LLM agent, in direct C/CUDA, and companion educational materials to bring many people up to speed in this awesome field.</p>
<p dir="auto">Please feel free to use the Discussions for any FAQ and related, or if you'd like something faster, #llmc on <a href="https://discord.gg/3zy8kqD9Cp" rel="nofollow">Discord</a>, or #llmdotc on CUDA MODE Discord.</p>
<p dir="auto">We'll see you next time!</p>
    </td>
  </tr>

    </tbody>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gene-silencing tool shows promise as a future therapy against prion diseases (200 pts)]]></title>
            <link>https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627</link>
            <guid>40939703</guid>
            <pubDate>Thu, 11 Jul 2024 19:21:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627">https://news.mit.edu/2024/charmed-collaboration-creates-therapy-candidate-fatal-prion-diseases-0627</a>, See on <a href="https://news.ycombinator.com/item?id=40939703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>Drug development is typically slow: The pipeline from basic research discoveries that provide the basis for a new drug to clinical trials and then production of a widely available medicine can take decades. But decades can feel impossibly far off to someone who currently has a fatal disease. Broad Institute of MIT and Harvard Senior Group Leader Sonia Vallabh is acutely aware of that race against time, because the topic of her research is a neurodegenerative and ultimately fatal disease — fatal familial insomnia, a type of prion disease — that she will almost certainly develop as she ages.&nbsp;</p><p>Vallabh and her husband, Eric Minikel, switched careers and became researchers after they learned that Vallabh carries a disease-causing version of the prion protein gene and that there is no effective therapy for fatal prion diseases. The two now run a lab at the Broad Institute, where they are working to develop drugs that can prevent and treat these diseases, and their deadline for success is not based on grant cycles or academic expectations but on the ticking time bomb in Vallabh’s genetic code.</p><p>That is why Vallabh was excited to discover, when she entered into a collaboration with Whitehead Institute for Biomedical Research member Jonathan Weissman, that Weissman’s group likes to work at full throttle. In less than two years, Weissman, Vallabh, and their collaborators have developed a set of molecular tools called CHARMs that can turn off disease-causing genes such as the prion protein gene — as well as, potentially, genes coding for many other proteins implicated in neurodegenerative and other diseases — and they are refining those tools to be good candidates for use in human patients. Although the tools still have many hurdles to pass before the researchers will know if they work as therapeutics, the team is encouraged by the speed with which they have developed the technology thus far.</p><p>“The spirit of the collaboration since the beginning has been that there was no waiting on formality,” Vallabh says. “As soon as we realized our mutual excitement to do this, everything was off to the races.”</p><p>Co-corresponding authors Weissman and Vallabh and co-first authors Edwin Neumann, a graduate student in Weissman’s lab, and Tessa Bertozzi, a postdoc in Weissman’s lab, describe CHARM — which stands for Coupled Histone tail for Autoinhibition Release of Methyltransferase — in a <a href="https://doi.org/10.1126/science.ado7082">paper</a> published today in the journal <em>Science</em>.</p><p>“With the Whitehead and Broad Institutes right next door to each other, I don’t think there’s any better place than this for a group of motivated people to move quickly and flexibly in the pursuit of academic science and medical technology,” says Weissman, who is also a professor of biology at MIT and a Howard Hughes Medical Institute Investigator. “CHARMs are an elegant solution to the problem of silencing disease genes, and they have the potential to have an important position in the future of genetic medicines.”</p><p><strong>To treat a genetic disease, target the gene</strong></p><p>Prion disease, which leads to swift neurodegeneration and death, is caused by the presence of misshapen versions of the prion protein. These cause a cascade effect in the brain: the faulty prion proteins deform other proteins, and together these proteins not only stop functioning properly but also form toxic aggregates that kill neurons. The most famous type of prion disease, known colloquially as mad cow disease, is infectious, but other forms of prion disease can occur spontaneously or be caused by faulty prion protein genes.</p><p>Most conventional drugs work by targeting a protein. CHARMs, however, work further upstream, turning off the gene that codes for the faulty protein so that the protein never gets made in the first place. CHARMs do this by epigenetic editing, in which a chemical tag gets added to DNA in order to turn off or silence a target gene. Unlike gene editing, epigenetic editing does not modify the underlying DNA — the gene itself remains intact. However, like gene editing, epigenetic editing is stable, meaning that a gene switched off by CHARM should remain off. This would mean patients would only have to take CHARM once, as opposed to protein-targeting medications that must be taken regularly as the cells’ protein levels replenish.</p><p>Research in animals suggests that the prion protein isn’t necessary in a healthy adult, and that in cases of disease, removing the protein improves or even eliminates disease symptoms. In a person who hasn’t yet developed symptoms, removing the protein should prevent disease altogether. In other words, epigenetic editing could be an effective approach for treating genetic diseases such as inherited prion diseases. The challenge is creating a new type of therapy.</p><p>Fortunately, the team had a good template for CHARM: a research tool called CRISPRoff that Weissman’s group previously developed for silencing genes. CRISPRoff uses building blocks from CRISPR gene editing technology, including the guide protein Cas9 that directs the tool to the target gene. CRISPRoff silences the targeted gene by adding methyl groups, chemical tags that prevent the gene from being transcribed, or read into RNA, and so from being expressed as protein. When the researchers tested CRISPRoff’s ability to silence the prion protein gene, they found that it was effective and stable.</p><p>Several of its properties, though, prevented CRISPRoff from being a good candidate for a therapy. The researchers’ goal was to create a tool based on CRISPRoff that was just as potent but also safe for use in humans, small enough to deliver to the brain, and designed to minimize the risk of silencing the wrong genes or causing side effects.</p><p><strong>From research tool to drug candidate</strong></p><p>Led by Neumann and Bertozzi, the researchers began engineering and applying their new epigenome editor. The first problem that they had to tackle was size, because the editor needs to be small enough to be packaged and delivered to specific cells in the body. Delivering genes into the human brain is challenging; many clinical trials have used adeno-associated viruses (AAVs) as gene-delivery vehicles, but these are small and can only contain a small amount of genetic code. CRISPRoff is way too big; the code for Cas9 alone takes up most of the available space.</p><p>The Weissman lab researchers decided to replace Cas9 with a much smaller zinc finger protein (ZFP). Like Cas9, ZFPs can serve as guide proteins to direct the tool to a target site in DNA. ZFPs are also common in human cells, meaning they are less likely to trigger an immune response against themselves than the bacterial Cas9.</p><p>Next, the researchers had to design the part of the tool that would silence the prion protein gene. At first, they used part of a methyltransferase, a molecule that adds methyl groups to DNA, called DNMT3A. However, in the particular configuration needed for the tool, the molecule was toxic to the cell. The researchers focused on a different solution: Instead of delivering outside DNMT3A as part of the therapy, the tool is able to recruit the cell’s own DNMT3A to the prion protein gene. This freed up precious space inside of the AAV vector and prevented toxicity.</p><p>The researchers also needed to activate DNMT3A. In the cell, DNMT3A is usually inactive until it interacts with certain partner molecules. This default inactivity prevents accidental methylation of genes that need to remain turned on. Neumann came up with an ingenious way around this by combining sections of DNMT3A’s partner molecules and connecting these to ZFPs that bring them to the prion protein gene. When the cell’s DNMT3A comes across this combination of parts, it activates, silencing the gene.</p><p>“From the perspectives of both toxicity and size, it made sense to recruit the machinery that the cell already has; it was a much simpler, more elegant solution,” Neumann says. “Cells are already using methyltransferases all of the time, and we’re essentially just tricking them into turning off a gene that they would normally leave turned on.”</p><p>Testing in mice showed that ZFP-guided CHARMs could eliminate more than 80 percent of the prion protein in the brain, while previous research has shown that as little as 21 percent elimination can improve symptoms.</p><p>Once the researchers knew that they had a potent gene silencer, they turned to the problem of off-target effects. The genetic code for a CHARM that gets delivered to a cell will keep producing copies of the CHARM indefinitely. However, after the prion protein gene is switched off, there is no benefit to this, only more time for side effects to develop, so they tweaked the tool so that after it turns off the prion protein gene, it then turns itself off.</p><p>Meanwhile, a complementary project from Broad Institute scientist and collaborator Benjamin Deverman’s lab, focused on brain-wide gene delivery and <a href="https://www.science.org/doi/10.1126/science.adm8386">published</a> in <em>Science</em> on May 17, has brought the CHARM technology one step closer to being ready for clinical trials. Although naturally occurring types of AAV have been used for gene therapy in humans before, they do not enter the adult brain efficiently, making it impossible to treat a whole-brain disease like prion disease. Tackling the delivery problem, Deverman’s group has designed an AAV vector that can get into the brain more efficiently by leveraging a pathway that naturally shuttles iron into the brain. Engineered vectors like this one make a therapy like CHARM one step closer to reality.</p><p>Thanks to these creative solutions, the researchers now have a highly effective epigenetic editor that is small enough to deliver to the brain, and that appears in cell culture and animal testing to have low toxicity and limited off-target effects.</p><p>“It’s been a privilege to be part of this; it’s pretty rare to go from basic research to therapeutic application in such a short amount of time,” Bertozzi says. “I think the key was forming a collaboration that took advantage of the Weissman lab’s tool-building experience, the Vallabh and Minikel lab’s deep knowledge of the disease, and the Deverman lab’s expertise in gene delivery.”</p><p><strong>Looking ahead</strong></p><p>With the major elements of the CHARM technology solved, the team is now fine-tuning their tool to make it more effective, safer, and easier to produce at scale, as will be necessary for clinical trials. They have already made the tool modular, so that its various pieces can be swapped out and future CHARMs won’t have to be programmed from scratch. CHARMs are also currently being tested as therapeutics in mice.&nbsp;</p><p>The path from basic research to clinical trials is a long and winding one, and the researchers know that CHARMs still have a way to go before they might become a viable medical option for people with prion diseases, including Vallabh, or other diseases with similar genetic components. However, with a strong therapy design and promising laboratory results in hand, the researchers have good reason to be hopeful. They continue to work at full throttle, intent on developing their technology so that it can save patients’ lives not someday, but as soon as possible.</p>        

      </div><div>

    


            
          

            
      

                          <div>
  
  
  

      <header>
      <h2>Press Mentions</h2>
    </header>
  
  
  

  <div><h3>USA Today</h3><p>Sonia Vallabh and Eric Minikel, senior group leaders from the Broad Institute have created a gene-editing tool to combat prion diseases, reports Karen Weintraub for <em>USA Today</em>.&nbsp;The approach “should also work against diseases such as Huntington's, Parkinson's, ALS and even Alzheimer's, which result from the accumulation of toxic proteins,” Weintraub writes.</p></div>


    

  
  

  
  
</div>
           

                <div>
      <h2>Related Links</h2>
      <div><ul><li><a href="https://www.broadinstitute.org/bios/sonia-vallabh" target="_blank">Sonia Vallabh</a></li><li><a href="https://wi.mit.edu/people/member/weissman" target="_blank">Jonathan Weissman</a></li><li><a href="https://wi.mit.edu/" target="_blank">Whitehead Institute for Biomedical Research</a></li><li><a href="https://www.broadinstitute.org/" target="_blank">The Broad Institute of MIT and Harvard</a></li><li><a href="https://biology.mit.edu/" target="_blank">Department of Biology</a></li><li><a href="https://science.mit.edu/" target="_blank">School of Science</a></li></ul></div>

    </div>
      
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Vision Pro U.S. Sales Are All but Dead, Market Analysts Say (121 pts)]]></title>
            <link>https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302</link>
            <guid>40939627</guid>
            <pubDate>Thu, 11 Jul 2024 19:12:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302">https://gizmodo.com/apple-vision-pro-u-s-sales-2000469302</a>, See on <a href="https://news.ycombinator.com/item?id=40939627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p><span>Those still holding on to their </span><a href="https://gizmodo.com/apple-vision-pro-1851249913"><span>Apple Vision Pros</span></a><span> may remain in a </span><a href="https://gizmodo.com/apple-vision-pro-already-forgotten-about-1851426564"><span>rather exclusive club</span></a><span> throughout this year. Market research shows that sales for Apple’s first big, expensive headset will remain low in 2024. The latest reports from those keeping tabs on the Cupertino, California company say AVP will have dropped off 75% by the end of August. The true test for Apple’s spatial dreams may rest on the rumored (slightly) </span><a href="https://gizmodo.com/apple-vision-pro-stopped-production-ar-headset-1851547035"><span>cheaper headset</span></a><span>.</span></p>

 <p><span>The market analyst firm IDC told </span><a href="https://www.bloomberg.com/news/articles/2024-07-11/apple-s-vision-pro-won-t-cross-500-000-sales-this-year-idc-says"><span>Bloomberg</span></a><span> the Apple Vision Pro has yet to sell 100,000 units. It’s an expensive headset, and Apple wasn’t expecting it to sell like an iPhone. Supply chain analysts have reported that <a href="https://gizmodo.com/apple-vision-pro-reality-check-1851429764">Apple cut its sales expectations for its $3,500 “spatial computer” </a></span><span>in April. But this latest report shows that sales will have dropped off a cliff in the U.S. in the third quarter of this year and will continue to slacken through the holidays.</span></p> <p><span>Last month, Apple launched the Vision Pro in international markets, including Europe, the U.K., China, Japan, and Singapore. IDC expects the AVP’s interest in those markets to keep the headset’s sales afloat until next year. The real pick-me-up for Apple’s spatial dreams would be a new, less expensive headset. Those in the know have hinted Apple is working on a “budget” Vision device slated for the latter half of 2025.</span></p>

 <p><span>Even if the <a href="https://gizmodo.com/cheaper-weaker-apple-vision-pro-1851556515">next Vision device</a> costs half the Pro model, it will still cost $1,750 and one of the most expensive consumer-end VR/AR headsets you can buy. Rumors hint that the next device could remove the pointless exterior display to save on manufacturing costs. It could also reduce the FOV and use a less-capable chip than the current M2. Bloomberg hinted that Apple was even considering tethering it to an iPhone or Mac for daily use, which would drastically reduce its portability.</span></p> <p><span>We don’t have pure statistics on how many folks returned their Vision Pro after buying it during the initial hype rush, but analysts have noted that </span><a href="https://gizmodo.com/apple-vision-pro-returns-users-didnt-know-set-it-up-1851293527"><span>many who bought one were confused by</span></a><span> its more complicated setup and what they were supposed to use it for in their daily lives.</span></p>

 <p><span>Sales expectations will put even more pressure on Apple engineers to design something that can compete with devices like the $500 </span><a href="https://gizmodo.com/meta-android-of-vr-1851521814"><span>Meta Quest 3</span></a><span> while justifying the higher price tag. Fans of the Cupertino company are already used to paying an “Apple tax” on their products, but not when the cost is literally thousands of dollars more.&nbsp;</span></p> <p><span>Apple is </span><a href="https://gizmodo.com/everything-announced-at-wwdc-2024-ios-apple-ai-1851529902"><span>working on a visionOS update</span></a><span> to improve the faux-3D spatial photos, add a few new gesture controls, and allow for a panoramic Mac screen mirroring. The latest version of visionOS won’t have a public beta, so we’ll have to wait and see if the changes will give the few on-the-fence customers a reason to pick up the ultra-expensive headset.</span></p>            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Capturing Linux SSL/TLS plaintext without a CA certificate using eBPF (169 pts)]]></title>
            <link>https://github.com/gojue/ecapture</link>
            <guid>40938810</guid>
            <pubDate>Thu, 11 Jul 2024 17:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gojue/ecapture">https://github.com/gojue/ecapture</a>, See on <a href="https://news.ycombinator.com/item?id=40938810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/ecapture-logo-400x400.png"><img src="https://github.com/gojue/ecapture/raw/master/images/ecapture-logo-400x400.png" alt=""></a></p>
<p dir="auto"><a href="https://github.com/gojue/ecapture/blob/master/README_CN.md">中文介绍</a> | English | <a href="https://github.com/gojue/ecapture/blob/master/README_JA.md">日本語</a></p>
<p dir="auto"><a href="https://github.com/gojue/ecapture"><img src="https://camo.githubusercontent.com/270192c3bdf7383aaf60e58e1257325923ad8a10e93ce348b011438b31e0e7b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6a75652f65636170747572652e7376673f6c6162656c3d5374617273266c6f676f3d676974687562" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/gojue/ecapture.svg?label=Stars&amp;logo=github"></a>
<a href="https://github.com/gojue/ecapture"><img src="https://camo.githubusercontent.com/2a1ba577b9b9d6298b29c621597f4adf0705ef187c20677eb75d246494aa1cd5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f676f6a75652f65636170747572653f6c6162656c3d466f726b73266c6f676f3d676974687562" alt="GitHub forks" data-canonical-src="https://img.shields.io/github/forks/gojue/ecapture?label=Forks&amp;logo=github"></a>
<a href="https://github.com/gojue/ecapture/actions/workflows/code-analysis.yml"><img src="https://github.com/gojue/ecapture/actions/workflows/codeql-analysis.yml/badge.svg" alt="CI"></a>
<a href="https://github.com/gojue/ecapture/releases"><img src="https://camo.githubusercontent.com/9441198ce3414f84ecb7fca20a61c221fe44be49f1b2b1acff425972a88a2486/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f676f6a75652f65636170747572653f646973706c61795f6e616d653d74616726696e636c7564655f70726572656c656173657326736f72743d73656d766572" alt="Github Version" data-canonical-src="https://img.shields.io/github/v/release/gojue/ecapture?display_name=tag&amp;include_prereleases&amp;sort=semver"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">eCapture(旁观者): capture SSL/TLS text content without a CA certificate using eBPF.</h3><a id="user-content-ecapture旁观者-capture-ssltls-text-content-without-a-ca-certificate-using-ebpf" aria-label="Permalink: eCapture(旁观者): capture SSL/TLS text content without a CA certificate using eBPF." href="#ecapture旁观者-capture-ssltls-text-content-without-a-ca-certificate-using-ebpf"></a></p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">Supports Linux/Android kernel versions x86_64 4.18 and above, <strong>aarch64 5.5</strong> and above.
Does not support Windows and macOS system.</p>
</blockquote>
<hr>

<ul dir="auto">
<li><a href="#how-ecapture-works">How eCapture works</a></li>
<li><a href="#ecapture-user-manual">eCapture User Manual</a></li>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#ecapture-architecture">eCapture Architecture</a></li>
<li><a href="#whats-ebpf">What's eBPF</a></li>
<li><a href="#how-to-compile">How to compile</a></li>
<li><a href="#contributing">Contributing</a></li>
</ul>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">How eCapture works</h2><a id="user-content-how-ecapture-works" aria-label="Permalink: How eCapture works" href="#how-ecapture-works"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/how-ecapture-works.png"><img src="https://github.com/gojue/ecapture/raw/master/images/how-ecapture-works.png" alt=""></a></p>
<ul dir="auto">
<li>SSL/TLS plaintext capture, support openssl\libressl\boringssl\gnutls\nspr(nss) libraries.</li>
<li>GoTLS plaintext support go tls library, which refers to encrypted communication in https/tls programs written in the golang language.</li>
<li>bash audit, capture bash command for Host Security Audit.</li>
<li>mysql query SQL audit, support mysqld 5.6\5.7\8.0, and mariadDB.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">eCapture User Manual</h2><a id="user-content-ecapture-user-manual" aria-label="Permalink: eCapture User Manual" href="#ecapture-user-manual"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/ecapture-help-v0.7.4.png"><img src="https://github.com/gojue/ecapture/raw/master/images/ecapture-help-v0.7.4.png" alt=""></a></p>
<p dir="auto">Youtube video: <a href="https://www.youtube.com/watch?v=CoDIjEQCvvA" title="eCapture User Manual" rel="nofollow">How to use eCapture v0.1.0</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">use ELF binary file</h2><a id="user-content-use-elf-binary-file" aria-label="Permalink: use ELF binary file" href="#use-elf-binary-file"></a></p>
<p dir="auto">Download ELF zip file <a href="https://github.com/gojue/ecapture/releases">release</a> , unzip and use by
command <code>./ecapture --help</code>.</p>
<ul dir="auto">
<li>Linux kernel version &gt;= 4.18 is required.</li>
<li>Enable BTF <a href="https://www.kernel.org/doc/html/latest/bpf/btf.html" rel="nofollow">BPF Type Format (BTF)</a>  (Optional, 2022-04-17)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">use docker image</h2><a id="user-content-use-docker-image" aria-label="Permalink: use docker image" href="#use-docker-image"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# pull docker image
docker pull gojue/ecapture:latest
# run
docker run --rm --privileged=true --net=host -v ${HOST_PATH}:${CONTAINER_PATH} gojue/ecapture ARGS"><pre><span><span>#</span> pull docker image</span>
docker pull gojue/ecapture:latest
<span><span>#</span> run</span>
docker run --rm --privileged=true --net=host -v <span>${HOST_PATH}</span>:<span>${CONTAINER_PATH}</span> gojue/ecapture ARGS</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Command line options</h2><a id="user-content-command-line-options" aria-label="Permalink: Command line options" href="#command-line-options"></a></p>
<blockquote>
<p dir="auto"><strong>Note</strong></p>
<p dir="auto">Need ROOT permission.</p>
</blockquote>
<p dir="auto">eCapture search <code>/etc/ld.so.conf</code> file default, to search load directories of  <code>SO</code> file, and search <code>openssl</code> shard
libraries location. or you can use <code>--libssl</code>
flag to set shard library path.</p>
<p dir="auto">If target program is compile statically, you can set program path as <code>--libssl</code> flag value directly。</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Modules</h2><a id="user-content-modules" aria-label="Permalink: Modules" href="#modules"></a></p>
<p dir="auto">The eCapture tool comprises 8 modules that respectively support plaintext capture for TLS/SSL encryption libraries like OpenSSL, GnuTLS, NSPR, BoringSSL, and GoTLS. Additionally, it facilitates software audits for Bash, MySQL, and PostgreSQL applications.</p>
<ul dir="auto">
<li>bash		capture bash command</li>
<li>gnutls	capture gnutls text content without CA cert for gnutls libraries.</li>
<li>gotls		Capturing plaintext communication from Golang programs encrypted with TLS/HTTPS.</li>
<li>mysqld	capture sql queries from mysqld 5.6/5.7/8.0 .</li>
<li>nss		capture nss/nspr encrypted text content without CA cert for nss/nspr libraries.</li>
<li>postgres	capture sql queries from postgres 10+.</li>
<li>tls		use to capture tls/ssl text content without CA cert. (Support openssl 1.0.x/1.1.x/3.0.x or newer).
You can use <code>ecapture -h</code> to view the list of subcommands.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">OpenSSL Module</h2><a id="user-content-openssl-module" aria-label="Permalink: OpenSSL Module" href="#openssl-module"></a></p>
<p dir="auto">The OpenSSL module supports three capture modes:</p>
<ul dir="auto">
<li><code>pcap</code>/<code>pcapng</code> mode stores captured plaintext data in pcap-NG format.</li>
<li><code>keylog</code>/<code>key</code> mode saves the TLS handshake keys to a file.</li>
<li><code>text</code> mode directly captures plaintext data, either outputting to a specified file or printing to the command line.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pcap Mode</h3><a id="user-content-pcap-mode" aria-label="Permalink: Pcap Mode" href="#pcap-mode"></a></p>
<p dir="auto">You can specify <code>-m pcap</code> or <code>-m pcapng</code> and use it in conjunction with <code>--pcapfile</code> and <code>-i</code> parameters. The default value for <code>--pcapfile</code> is <code>ecapture_openssl.pcapng</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="./ecapture tls -m pcap -i eth0 --pcapfile=ecapture.pcapng tcp port 443"><pre>./ecapture tls -m pcap -i eth0 --pcapfile=ecapture.pcapng tcp port 443</pre></div>
<p dir="auto">This command saves captured plaintext data packets as a pcapng file, which can be viewed using <code>Wireshark</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keylog Mode</h3><a id="user-content-keylog-mode" aria-label="Permalink: Keylog Mode" href="#keylog-mode"></a></p>
<p dir="auto">You can specify <code>-m keylog</code> or <code>-m key</code> and use it in conjunction with the <code>--keylogfile</code> parameter, which defaults to <code>ecapture_masterkey.log</code>.</p>
<p dir="auto">The captured OpenSSL TLS <code>Master Secret</code> information is saved to <code>--keylogfile</code>. You can also enable <code>tcpdump</code> packet capture and then use <code>Wireshark</code> to open the file and set the <code>Master Secret</code> path to view plaintext data packets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="./ecapture tls -m keylog -keylogfile=openssl_keylog.log"><pre>./ecapture tls -m keylog -keylogfile=openssl_keylog.log</pre></div>
<p dir="auto">You can also directly use the <code>tshark</code> software for real-time decryption and display:</p>
<div dir="auto" data-snippet-clipboard-copy-content="tshark -o tls.keylog_file:ecapture_masterkey.log -Y http -T fields -e http.file_data -f &quot;port 443&quot; -i eth0"><pre>tshark -o tls.keylog_file:ecapture_masterkey.log -Y http -T fields -e http.file_data -f <span><span>"</span>port 443<span>"</span></span> -i eth0</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text Mode</h3><a id="user-content-text-mode" aria-label="Permalink: Text Mode" href="#text-mode"></a></p>
<p dir="auto"><code>./ecapture tls -m text</code> will output all plaintext data packets. (Starting from v0.7.0, it no longer captures SSLKEYLOG information.)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">GoTLS Module</h2><a id="user-content-gotls-module" aria-label="Permalink: GoTLS Module" href="#gotls-module"></a></p>
<p dir="auto">Similar to the OpenSSL module.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">check your server BTF config：</h3><a id="user-content-check-your-server-btf-config" aria-label="Permalink: check your server BTF config：" href="#check-your-server-btf-config"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="cfc4n@vm-server:~$# uname -r
4.18.0-305.3.1.el8.x86_64
cfc4n@vm-server:~$# cat /boot/config-`uname -r` | grep CONFIG_DEBUG_INFO_BTF
CONFIG_DEBUG_INFO_BTF=y"><pre>cfc4n@vm-server:<span>~</span><span>$#</span> uname -r
4.18.0-305.3.1.el8.x86_64
cfc4n@vm-server:<span>~</span><span>$#</span> cat /boot/config-<span><span>`</span>uname -r<span>`</span></span> <span>|</span> grep CONFIG_DEBUG_INFO_BTF
CONFIG_DEBUG_INFO_BTF=y</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">gotls command</h3><a id="user-content-gotls-command" aria-label="Permalink: gotls command" href="#gotls-command"></a></p>
<p dir="auto">capture tls text context.</p>
<p dir="auto">Step 1:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./ecapture gotls --elfpath=/home/cfc4n/go_https_client --hex"><pre>./ecapture gotls --elfpath=/home/cfc4n/go_https_client --hex</pre></div>
<p dir="auto">Step 2:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/home/cfc4n/go_https_client"><pre>/home/cfc4n/go_https_client</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">more help</h3><a id="user-content-more-help" aria-label="Permalink: more help" href="#more-help"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">bash Module</h2><a id="user-content-bash-module" aria-label="Permalink: bash Module" href="#bash-module"></a></p>
<p dir="auto">capture bash command : <code>ecapture bash</code></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">eCapture Architecture</h2><a id="user-content-ecapture-architecture" aria-label="Permalink: eCapture Architecture" href="#ecapture-architecture"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gojue/ecapture/blob/master/images/ecapture-architecture.png"><img src="https://github.com/gojue/ecapture/raw/master/images/ecapture-architecture.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's eBPF</h2><a id="user-content-whats-ebpf" aria-label="Permalink: What's eBPF" href="#whats-ebpf"></a></p>
<p dir="auto"><a href="https://ebpf.io/" rel="nofollow">eBPF</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to compile</h2><a id="user-content-how-to-compile" aria-label="Permalink: How to compile" href="#how-to-compile"></a></p>
<p dir="auto">Linux Kernel: &gt;= 4.18.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tools</h2><a id="user-content-tools" aria-label="Permalink: Tools" href="#tools"></a></p>
<ul dir="auto">
<li>golang 1.21 or newer</li>
<li>clang 9.0 or newer</li>
<li>cmake 3.18.4 or newer</li>
<li>clang backend: llvm 9.0 or newer</li>
<li>kernel config:CONFIG_DEBUG_INFO_BTF=y (Optional, 2022-04-17)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">command</h2><a id="user-content-command" aria-label="Permalink: command" href="#command"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ubuntu</h3><a id="user-content-ubuntu" aria-label="Permalink: ubuntu" href="#ubuntu"></a></p>
<p dir="auto">If you are using Ubuntu 20.04 or later versions, you can use a single command to complete the initialization of the compilation environment.</p>
<div dir="auto" data-snippet-clipboard-copy-content="/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/gojue/ecapture/master/builder/init_env.sh)&quot;"><pre>/bin/bash -c <span><span>"</span><span><span>$(</span>curl -fsSL https://raw.githubusercontent.com/gojue/ecapture/master/builder/init_env.sh<span>)</span></span><span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">other Linux</h3><a id="user-content-other-linux" aria-label="Permalink: other Linux" href="#other-linux"></a></p>
<p dir="auto">In addition to the software listed in the 'Toolchain Version' section above, the following software is also required for the compilation environment. Please install it yourself.</p>
<ul dir="auto">
<li>linux-tools-common</li>
<li>linux-tools-generic</li>
<li>pkgconf</li>
<li>libelf-dev</li>
</ul>
<p dir="auto"><strong>Clone the repository code and compile it</strong></p>
<p dir="auto">Caution: The following <code>make</code> command will install libpcap into the system
directory if <code>libpcap.a</code> does not exist under <code>/usr/local/lib</code>. If you have
installed libpcap in system without <code>libpcap.a</code>, it maybe break your libpcap's
headers.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --recurse-submodules git@github.com:gojue/ecapture.git
cd ecapture
make
bin/ecapture"><pre>git clone --recurse-submodules git@github.com:gojue/ecapture.git
<span>cd</span> ecapture
make
bin/ecapture</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">compile without BTF</h2><a id="user-content-compile-without-btf" aria-label="Permalink: compile without BTF" href="#compile-without-btf"></a></p>
<p dir="auto">eCapture support BTF disabled with command <code>make nocore</code> to compile at 2022/04/17. It can work normally even on Linux systems that do not support BTF.</p>
<div dir="auto" data-snippet-clipboard-copy-content="make nocore
bin/ecapture --help"><pre>make nocore
bin/ecapture --help</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">cross-compilation</h2><a id="user-content-cross-compilation" aria-label="Permalink: cross-compilation" href="#cross-compilation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Kernel header files</h3><a id="user-content-kernel-header-files" aria-label="Permalink: Kernel header files" href="#kernel-header-files"></a></p>
<p dir="auto">To cross-compile the eCapture tool, you need to install the kernel header files for the target architecture. you need to install the <code>linux-source</code> package.</p>
<div dir="auto" data-snippet-clipboard-copy-content="kernel_ver=`uname -r | cut -d'-' -f 1`
sudo apt-get install -y linux-source-$kernel_ver
cd /usr/src
sudo tar -xf linux-source-${kernel_ver}.tar.bz2
cd /usr/src/linux-source-${kernel_ver}
test -f .config || yes &quot;&quot; | sudo make oldconfig"><pre>kernel_ver=<span><span>`</span>uname -r <span>|</span> cut -d<span><span>'</span>-<span>'</span></span> -f 1<span>`</span></span>
sudo apt-get install -y linux-source-<span>$kernel_ver</span>
<span>cd</span> /usr/src
sudo tar -xf linux-source-<span>${kernel_ver}</span>.tar.bz2
<span>cd</span> /usr/src/linux-source-<span>${kernel_ver}</span>
<span>test</span> -f .config <span>||</span> yes <span><span>"</span><span>"</span></span> <span>|</span> sudo make oldconfig</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">ToolChains</h3><a id="user-content-toolchains" aria-label="Permalink: ToolChains" href="#toolchains"></a></p>
<p dir="auto">To cross-compile binary files for the aarch64 architecture on an amd64 architecture system, you need to install the gcc-aarch64-linux-gnu toolchain. Similarly, to cross-compile binary files for the amd64 architecture on an aarch64 system, you need to install the gcc-x86-64-linux-gnu toolchain.</p>
<ul dir="auto">
<li>amd64 arch: gcc-aarch64-linux-gnu</li>
<li>arm64 arch: gcc-x86-64-linux-gnu</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build Commands</h3><a id="user-content-build-commands" aria-label="Permalink: Build Commands" href="#build-commands"></a></p>
<p dir="auto">To build an <code>arm64</code> artifact on an ubuntu <code>amd64</code> system, you can set the <code>CROSS_ARCH</code> environment variable to achieve cross-compilation.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Stargazers over time</h2><a id="user-content-stargazers-over-time" aria-label="Permalink: Stargazers over time" href="#stargazers-over-time"></a></p>
<p dir="auto"><a href="https://starchart.cc/gojue/ecapture" rel="nofollow"><img src="https://camo.githubusercontent.com/82f9137fa72bb9a2660aa993e601d83c1c661035ee4b8cfbabbad9160e1334a6/68747470733a2f2f7374617263686172742e63632f676f6a75652f65636170747572652e737667" alt="Stargazers over time" data-canonical-src="https://starchart.cc/gojue/ecapture.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See <a href="https://github.com/gojue/ecapture/blob/master/CONTRIBUTING.md">CONTRIBUTING</a> for details on submitting patches and the contribution workflow.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>