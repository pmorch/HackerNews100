<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 22 Dec 2025 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The biggest CRT ever made: Sony's PVM-4300 (111 pts)]]></title>
            <link>https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/</link>
            <guid>46353777</guid>
            <pubDate>Mon, 22 Dec 2025 12:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/">https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/</a>, See on <a href="https://news.ycombinator.com/item?id=46353777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Move over, GE Widescreen 1000. In 1989 in Japan, Sony introduced to the largest Trinitron CRT ever built, the KV-45ED1, also known as the PVM-4300. And in 1990, they imported 20 of them to the United States, just in time for the recession. About 34 years later, one of these enigmatic TVs surfaced.</p><h2>Sony’s PVM-4300/KV-45ED1</h2><figure id="attachment_35264" aria-describedby="caption-attachment-35264"><a href="https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/sony_pvm_4300/" rel="attachment wp-att-35264"><img data-recalc-dims="1" fetchpriority="high" decoding="async" src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/06/Sony_PVM_4300.jpg?resize=300%2C209&amp;ssl=1" alt="The Sony PVM-4300, next to a 29-inch TV" width="300" height="209" srcset="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/06/Sony_PVM_4300.jpg?resize=300%2C209&amp;ssl=1 300w, https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/06/Sony_PVM_4300.jpg?w=607&amp;ssl=1 607w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption id="caption-attachment-35264">The Sony PVM-4300 was the largest CRT TV ever made. Its 45-inch tube provided 43 inches of visible improved definition TV. It stood about 27 inches tall.</figcaption></figure><p>Sony’s part number suggests it has a 45 inch tube inside. But in a rare case of truth in advertising, Sony advertised it as a 43-inch model. It weighed about 450 pounds, stood about 27 inches tall, and it wouldn’t fit through a standard door frame. That’s probably okay, it’s not like someone was going to use this as a bedroom TV. This thing was going in your living room.</p><p>In Japan, it sold for 2.6 million yen, but in the United States, it retailed for $40,000, a significant markup. To be fair, shipping them across the Atlantic and then throughout the United States must have been expensive. And news articles in 1990 said Sony dealers would not allow any bickering. They would throw in a couple of options like the separate tuner or speakers. But no discounts.</p><p>Sony said at the time they hoped to sell 80 of them that year, but the recession may have kept that from happening.</p><h2>The biggest conventional CRT ever</h2><p>The Sony PVM-4300 was a conventional CRT, unlike the <a href="https://dfarq.homeip.net/ge-widescreen-1000/">GE Widescreen 1000</a>, which was a projection set. Projection TVs could be bigger and cheaper. But if you wanted the clearest picture, a big CRT was where it was at.</p><p>It was a conventional CRT that worked with over the air signals, but like many larger TVs of the era, it used a technology called IDTV to enhance the picture quality. The “ID” stood for “improved definition.” IDTV sets had a buffer so they would store successive frames and interpolate them rather than interlacing them the way a conventional CRT TV worked. They also had circuitry to detect motion and perform image stabilization to further enhance the image. The result wasn’t as good as HDTV. But it gave high rollers a better picture until HDTV. HDTV arrived in 1998, but articles at the time estimated 2005. The Chicago Tribune warned in 1990 that these $40,000 TVs would be obsolete in 15 years, but the salesperson countered that every TV would be obsolete in 15 years.</p><p>It’s also likely that someone in the market for a $40,000 TV didn’t worry about obsolescence. In 1990, the GE Widescreen 1000 looked dated and it wasn’t 15 years old yet.</p><h2>Why so expensive?</h2><p>The KV-45ED1 or PVM-4300 cost about 8 times as much as Sony’s second most expensive model at the time, which had a 29-inch screen. That’s largely because the KV-45ED1 had to be built by hand. Sony could mass produce its smaller TVs. This was a product for buyers who weren’t worried about the price.</p><p><a href="https://dfarq.homeip.net/the-last-sony-crt-ever-made/">Sony continued making CRTs into the 21st century</a>, bowing out with its high-def KD-34XBR970, 36-inch KD-36FS170, 32-inch KV-32FS170 and 27-inch KV27FS170 in February 2006.</p><p>It is unclear how many of these enormous 43-inch units Sony sold, and some people even questioned if it was ever built. A Chicago area dealer told the <em>Chicago Tribune</em> in 1990 that someone had purchased one, but that the buyer wanted to remain anonymous. That was good enough for me; a TV dealer wasn’t going to tell a newspaper that they have a $40,000 item and then not have it. That’s just bad business. Good business is taking the free advertising, having an example on display to show knowing most won’t buy it, but they may buy one of the smaller units. But luring someone into the store with a lie makes it much more difficult to sell anything.</p><p>And the Tribune wasn’t going to make something like this up. It would anger the TV dealers and risk losing their advertising. And someone who could afford a $40,000 TV was likely a business owner or high-ranking executive who could pull their advertising. In the days of print newspapers, advertisers and potential advertisers held a lot of sway. This could be both good and bad. I’m not going to say capitalism solves every problem but this was a case where it helped keep people honest.</p><h2>Shank Mods’ surviving Sony PVM-4300</h2><p>On December 22, 2024, Youtuber Shank Mods released a video telling the story of a Sony PVM-4300 and how he acquired it. One of the photos of a purported surviving unit turned out to be very real. It was taken in a restaurant in Japan, and the owner was actually aware of the photo. Unfortunately the restaurant was having to move, and needed to get rid of the set.&nbsp;Shank Mods was able to contact some people in Japan who could help race against time and remove the TV from the restaurant and then ship it to the United States.</p><p><a href="https://www.youtube.com/watch?v=JfZxOuc9Qwk&amp;pp=ygUOc2hhbmsgbW9kcyBjcnQ%3D">The 35-minute video</a> is well worth watching if you have interest in vintage CRTs, or even if you just like stories of strangers coming together and helping each other just for the sake of being helpful. Actually, I take that back. At the end of the video, Shank Mods played a prank on his fellow CRT fans that is absolutely hilarious and makes the video worth watching for that reason alone. I won’t ruin it for you.</p><p>We can only guess how many other examples may survive. But we now know that at least one survives and is in the hands of a retro hobbyist.</p><div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&amp;ssl=1" data-src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&amp;ssl=1" width="100" height="100" alt="" itemprop="image"></p><div><p>David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If you don't design your career, someone else will (2014) (266 pts)]]></title>
            <link>https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/</link>
            <guid>46352930</guid>
            <pubDate>Mon, 22 Dec 2025 10:27:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/">https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/</a>, See on <a href="https://news.ycombinator.com/item?id=46352930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

	<p>A client once responded to one of my questions by saying, “Oh Greg, I am too busy living to think <a href="https://gregmckeown.com/about/">about</a> life!” His off-the-cuff comment named a trap all of us fall into sometimes. In just one example, it is easy to become so consumed&nbsp;<em>in&nbsp;</em>our careers we fail to really think&nbsp;<em><a href="https://gregmckeown.com/about/">about</a>&nbsp;</em>our careers.</p>
<p>To avoid this trap, I suggest carving out a couple of hours over the holiday break to follow these simple steps for reflecting on your career.</p>
<p><strong>Step 1: <a href="https://gregmckeown.com/books/effortless/review/">Review</a> the last 12 months</strong>. <a href="https://gregmckeown.com/books/effortless/review/">Review</a> the year, month by month. Make a list of where you spent your time: include your major projects, responsibilities and accomplishments. No need to overcomplicate this.</p>
<p><strong>Step 2: Ask, “What is the <a href="https://gregmckeown.com/news/">news</a>?”</strong>&nbsp;Look over your list and reflect on what is&nbsp;<em>really</em>&nbsp;going on. Think like a journalist and ask yourself: Why does this matter? What are the trends here? What happens if these trends continue?</p>
<p><strong>Step 3: Ask “What would I do in my career if I could do</strong><strong>&nbsp;<em>anything</em>?”&nbsp;</strong>Just brainstorm with no voice of criticism to hold you back. Just write out all the ideas that come to mind.</p>
<p><strong>Step 4: Go back and spend a bit more time on Step 3.</strong>&nbsp;Too often we begin our career planning with our second best option in mind. We have a sense of what we would most love to do but we&nbsp;<em>immediately</em>&nbsp;push it aside. Why? Typically because “it is not realistic” which is code for, “I can’t make money doing this.” In this economy—in any economy—I understand why making money is critical. However, sometimes we pass by legitimate career paths because we set them aside too quickly.</p>
<p><strong>Step 5: Write down six objectives for the next 12 months.</strong>&nbsp;Make a list of the top six items you would like to accomplish in your career this year and place them in priority order.</p>
<p><strong>Step 6: Cross off the bottom five.</strong>&nbsp;Once you’re back to the whirlwind of work you’ll benefit from having a single “true north” career objective for the year.</p>
<p><strong>Step 7: Make an action plan for this month.</strong>&nbsp;Make a list of some quick wins you’d like to have in place over the next 3-4 weeks.</p>
<p><strong>Step 8: Decide what you will say no to.</strong>&nbsp;Make a list of the “good” things that will keep you from achieving your one “great” career objective. Think <a href="https://gregmckeown.com/about/">about</a> how to delete, defer or delegate these other tasks.&nbsp;<a href="http://en.wikipedia.org/wiki/Ralph_Waldo_Emerson" target="_blank">Ralph Waldo Emerson</a>&nbsp;said, “The crime which bankrupts men and nations is that of turning aside from one’s main purpose to serve a job here and there.”</p>
<p>Many years ago I followed this process and, without exaggeration, it changed the course of my life. The insight I gained led me to quit law school, leave England and move to America and start down the path as a teacher and author. You’re reading this because of that choice. It remains the single most important career decision of my life.</p>
<p>Two hours spent wisely over the next couple of weeks could easily improve the quality of your life over the 8760 hours of the next year–and perhaps far beyond. After all,&nbsp;<em>if we don’t design our careers, someone else will.</em></p>

	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ancient monuments saluting the winter solstice (115 pts)]]></title>
            <link>https://www.bbc.com/culture/article/20251219-the-ancient-monuments-saluting-the-winter-solstice</link>
            <guid>46352565</guid>
            <pubDate>Mon, 22 Dec 2025 09:30:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/culture/article/20251219-the-ancient-monuments-saluting-the-winter-solstice">https://www.bbc.com/culture/article/20251219-the-ancient-monuments-saluting-the-winter-solstice</a>, See on <a href="https://news.ycombinator.com/item?id=46352565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="layout-block"><p><b id="dozens-of-mysterious-structures-across-the-northern-hemisphere-–-some-nearly-5,000-years-old-–-align-precisely-to-frame-the-rising-and-setting-sun-during-midwinter's-shortest-day.-what-motivated-people-to-construct-these-solar-calibrated-masterpieces?">Dozens of mysterious structures across the Northern Hemisphere – some nearly 5,000 years old – align precisely to frame the rising and setting Sun during midwinter's shortest day. What motivated people to construct these solar-calibrated masterpieces?</b></p><p>The winter solstice, which usually falls on 21 or 22 December in the Northern Hemisphere each year, marks the moment that one yearly cycle comes to an end and another is born. It is the day with the smallest number of sunlight hours in the calendar, and once it's over, the days lengthen again incrementally until the <a target="_self" href="https://www.bbc.com/culture/article/20230616-gen-z-and-millennials-surprising-midsummer-obsession">summer solstice</a> in June.</p><p>The significance of this day is manifested in ancient monuments that were designed to acknowledge and celebrate its passing. One example is <a target="_self" href="https://www.bbc.com/travel/article/20231221-maeshowe-the-uks-doorway-to-another-world">Maeshowe tomb in Orkney</a>. To the untrained eye this burial cairn, created around 2800BC, looks like a grassy hillock – but it conceals a cuboid, stone-clad sepulchre and a 33ft (10m) long entry corridor oriented to the south-west. During midwinter, three weeks either side of the winter solstice, the setting Sun aims directly down the corridor and emanates its light into the tomb.</p><div><p><span>It's possible to understand the enormous significance of the winter solstice both as the darkest moment in the calendar and the pivot to six future months of greater illumination</span></p></div><p>When the sky is cloudless, the light seems to carve a golden aperture into the tomb's rear wall – a sacrament of pure light. These days of radiance are interrupted by the solstice itself, when blackness temporarily takes over. But daylight reappears soon after, to blaze for another few days as if in celebration of the promise of nature's rejuvenation in spring.</p></div><div data-component="layout-block"><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mpp4n7.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mpp4n7.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mpp4n7.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mpp4n7.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mpp4n7.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mpp4n7.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mpp4n7.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mpp4n7.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mpp4n7.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mpp4n7.jpg.webp" loading="lazy" alt="Alamy Orkney's Maeshowe tomb is a burial cairn created around 2800BC that conceals a stone-clad sepulchre (Credit: Alamy)"><span>Alamy</span></p><figcaption>Orkney's Maeshowe tomb is a burial cairn created around 2800BC that conceals a stone-clad sepulchre (Credit: Alamy)</figcaption></figure><p>We will probably never know the specific beliefs and rituals that inspired Maeshowe tomb. But it's nonetheless possible to understand the enormous significance of the winter solstice as the "year's midnight", both as the darkest moment in the calendar and the pivot to six future months of greater illumination. It was a moment of death and rebirth, and a reminder of the cyclical nature of time.</p><p>In the deep past, understanding the markers of nature's clockwork – including solstices – was a matter of survival. Predicting the recurrent patterns of animal migration, for example, could help successful hunting and fishing. Knowing when the climate was likely to change meant being able to adapt and survive. In pre-agricultural societies, it helped people anticipate the availability and location of edible roots, nuts and plants.</p><p>After the introduction of farming, around 9000BC, it was essential – for successful planting and harvesting – to anticipate the timing of seasonal changes. Monuments that calculated time had practical value, but it's likely that they also embodied spiritual beliefs in Neolithic times too, with the winter solstice being of particular importance. This very ancient recognition of the solstice's significance even echoes through to the modern world. The word "Yule", now associated with the winter holiday period, derives from the historic Norse festival of <i id="jól,">Jól,</i> which was based around the winter solstice. Modern Christmas traditions recall bygone midwinter celebrations like the Roman holiday of Saturnalia, which involved feasting and gift-giving. And the solstice continues to be acknowledged in hundreds of traditions across the world, such as the Inca celebration of Inti Raymi, and the Dōngzhì festival in China.</p><h2><span id="'nature's-sublime-power'"><b id="'nature's-sublime-power'">'Nature's sublime power'</b></span></h2><p>Alongside Maeshowe tomb, archaeologists have discovered dozens of Neolithic monuments that stare directly at the Sun on the winter solstice. There's Stonehenge (England), whose tallest trilithon once framed the setting sun; Newgrange (Ireland), which has a passageway aligned to sunrise on this auspicious day; and the standing stones at Callanish (Outer Hebrides) which create similar solar sightlines. In Brittany, north-western France, is <i id="la-roche-aux-fées:">La Roche aux Fées:</i> a megalithic passageway constructed from 41 blocks of stone, some of which weigh over 40 tonnes (40,000kg). At sunrise on the winter solstice, it breathes in its annual dose of restorative midwinter light. Legends once told that fairies constructed it over the course of one night, but it is actually a dolmen (tomb) created by Neolithic architects around 2750BC.</p></div><div data-component="layout-block"><p>Another masterpiece of Land Art, James Turrell's Roden Crater (begun 1979), does this on an even more epic scale than Sun Tunnels. It occupies a volcanic cinder cone in the Painted Desert region of northern Arizona and houses multiple spaces from which to watch celestial phenomena. One of them is a 900ft- (274m) long tunnel drilled through the volcanic cone. It acts like a camera obscura, focusing an image of the midwinter sun (via a glass lens halfway down the passage) on to a slab of white marble in a central chamber. Like Maeshowe tomb's passage, it aligns with the Sun's position <a target="_blank" href="https://rodencrater.com/celestial-events/">around 21 December each year</a>, and drinks down the Sun's light from 10 days before the solstice to the 10th day after it.</p><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mpp6tg.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mpp6tg.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mpp6tg.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mpp6tg.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mpp6tg.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mpp6tg.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mpp6tg.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mpp6tg.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mpp6tg.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mpp6tg.jpg.webp" loading="lazy" alt="Getty Images The epic Roden Crater in Northern Arizona houses multiple spaces from which to watch celestial phenomena (Credit: Getty Images)"><span>Getty Images</span></p><figcaption>The epic Roden Crater in Northern Arizona houses multiple spaces from which to watch celestial phenomena (Credit: Getty Images)</figcaption></figure><p>Enoura Observatory in Kanagawa Prefecture, Japan (completed 2017) was designed by photographer and architect Hiroshi Sugimoto. Its various buildings are all calibrated towards the movement of the Sun, to create what the artist <a target="_blank" href="https://www.mariangoodman.com/usr/documents/press/download_url/151/the-new-york-times-style-magazine-april-3-2017-.pdf?utm">describes as</a> a "new Neolithic aesthetic". He wanted to correct what he saw as a lack of purpose in contemporary art by exploring the primal concerns of our ancient ancestors – our status within the infinite wilderness of the cosmos, our sense of time, and our notion of a human identity within the natural order.</p><p>One of its structures, the "Winter Solstice Light-Worship Tunnel", points directly at the spot on the horizon where the Sun rises at about 06:48 local time on 21 December each year. The solstice sunlight floods this 230ft-(70m) long chamber made of Corten steel and illuminates a stone medieval wellhead that is situated halfway along its length. It passes underneath another structure which aligns with the Sun on the summer solstice. The entire site, which took a decade to build, was intended by Sugimoto to act like a living clock, and to make <a target="_blank" href="https://www.odawara-af.com/en/enoura/">an artwork</a> with the ancient function of helping humans "identify their place within the vastness of the universe".</p><figure><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0mpp7qg.jpg.webp 160w, https://ichef.bbci.co.uk/images/ic/240xn/p0mpp7qg.jpg.webp 240w, https://ichef.bbci.co.uk/images/ic/320xn/p0mpp7qg.jpg.webp 320w, https://ichef.bbci.co.uk/images/ic/480xn/p0mpp7qg.jpg.webp 480w, https://ichef.bbci.co.uk/images/ic/640xn/p0mpp7qg.jpg.webp 640w, https://ichef.bbci.co.uk/images/ic/800xn/p0mpp7qg.jpg.webp 800w, https://ichef.bbci.co.uk/images/ic/1024xn/p0mpp7qg.jpg.webp 1024w, https://ichef.bbci.co.uk/images/ic/1376xn/p0mpp7qg.jpg.webp 1376w, https://ichef.bbci.co.uk/images/ic/1920xn/p0mpp7qg.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0mpp7qg.jpg.webp" loading="lazy" alt="Odawara Art Foundation Hiroshi Sugimoto created the Enoura Observatory in Japan, whose buildings are all calibrated towards the movement of the Sun (Credit: Odawara Art Foundation)"><span>Odawara Art Foundation</span></p><figcaption>Hiroshi Sugimoto created the Enoura Observatory in Japan, whose buildings are all calibrated towards the movement of the Sun (Credit: Odawara Art Foundation)</figcaption></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I announced my divorce on Instagram and then AI impersonated me (125 pts)]]></title>
            <link>https://eiratansey.com/2025/12/20/i-announced-my-divorce-on-instagram-and-then-ai-impersonated-me/</link>
            <guid>46352004</guid>
            <pubDate>Mon, 22 Dec 2025 07:13:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eiratansey.com/2025/12/20/i-announced-my-divorce-on-instagram-and-then-ai-impersonated-me/">https://eiratansey.com/2025/12/20/i-announced-my-divorce-on-instagram-and-then-ai-impersonated-me/</a>, See on <a href="https://news.ycombinator.com/item?id=46352004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">
	
  		<header>
  			
  			<!---->
  		</header>

  		<section>
	
			<article id="post-2412">
				
				<h2>I announced my divorce on Instagram and then AI impersonated me</h2>
				<time>December 20, 2025</time>
		
				
<p>After maintaining a total stance of public silence for months, I recently publicly announced my unexpected divorce on Instagram. I shared a picture of the divorce cake that my friends got for me, and shared a brief essay I had drafted the day before about the news. I had to edit it down slightly from my original draft to fit Instagram’s character limits. You can see the <a href="https://www.instagram.com/p/DSQwb-hkTPb/">Instagram post</a> here. And for the record, here is the photo of the cake and what I wrote:</p>



<figure><a href="https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322.jpeg"><img fetchpriority="high" decoding="async" width="1024" height="928" src="https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322-1024x928.jpeg" alt="Image of a round cake with red and black decorations and piped frosting text that says &quot;Hot Divorcee Era&quot;" srcset="https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322-1024x928.jpeg 1024w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322-300x272.jpeg 300w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322-768x696.jpeg 768w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322-1536x1391.jpeg 1536w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3322.jpeg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<blockquote>
<div><p>Two weeks before our ninth anniversary in April my husband told me he wanted a divorce. I was completely blindsided. Going through an unexpected divorce has been the most brutal, humiliating, and traumatizing process I’ve ever experienced. In an instant my life trajectory changed. The markers of security that I had clung to following the previous year’s burst appendix hospitalization and my Dad dying in the same hospital where I was being treated suddenly vanished. Being forced to sell our house, the address I lived in longer than any other place in my life because of my complicated childhood, was one of the most devastating parts of this hellish timeline.</p><p>I don’t know if I would have survived the last several months were it not for my friends, those from home or around the world, those I’ve known for decades and those I’ve only recently met in the process of beginning to rebuild my life. Having friends from all ages, backgrounds, and circumstances has been a lifesaver because it turns out a lot of people have also gone through similar traumas that turned their lives upside down, who have a lot of counsel and camaraderie to offer.</p><p>Enough people have received the memo about divorce etiquette to ask whether a person’s divorce is a “congratulations” or “sorry” situation. Until recently the latter has been more applicable, but now that the state has decreed that my divorce is official I mostly feel a sense of overwhelming relief tempering my deep grief, and trying to stay focused on what’s next.</p><p>A divorce you don’t see coming really does a number on your sense of worth and identity. I did not choose to end my marriage, but I am humbled and struck by how many choices I get to make about my future. I am spending a lot of time thinking about the people and places, principles and pleasures that I want to prioritize in this next phase of my life.</p><p>Over the last several months I’ve been grateful for my friends’ unwavering belief that things will be better on the other side of this journey. And who I am to argue with my friends? They are smart and I think they love me and I definitely love them. So with that, I’m officially in my hot divorcee era. WATCH THIS SPACE.</p></div>
</blockquote>



<p>I hit post, and cross-posted it to Facebook. I also shared the Instagram link on <a href="http://glammr.us/">glammr.us</a> (the main Mastodon server that caters to GLAM workers), which is where I took a lot of my <a href="https://eiratansey.com/2018/01/07/this-machine-coddles-fascists/">post-Twitter energy</a> a few years ago.&nbsp;</p>



<p>A few days after posting, I was looking at the Mastodon app, Toot, on my phone (yes, all of the terminology around Mastodon is that embarrassing). And I noticed something I definitely did not remember writing. In the app, all the original text from my post was pulled in along with some additional text following my sign-off line of WATCH THIS SPACE that I certainly didn’t author. It also sounded very AI-generated. Here’s what I saw:</p>



<figure><a href="https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363.png"><img decoding="async" width="992" height="2146" src="https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited.png" alt="" srcset="https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited.png 992w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited-139x300.png 139w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited-473x1024.png 473w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited-768x1661.png 768w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited-710x1536.png 710w, https://eiratansey.com/wp-content/uploads/2025/12/IMG_3363-edited-947x2048.png 947w" sizes="(max-width: 992px) 100vw, 992px"></a></figure>



<p>Here’s the AI-generated text I didn’t write:</p>



<blockquote>
<p>Going through a divorce can be a life-altering experience that leaves you feeling lost and uncertain about your future. In this post, I share my personal journey of self-discovery and rebuilding after a sudden divorce.<br>From finding support from friends to prioritizing what matters most, I explore the themes of identity, worth, and finding happiness after divorce.<br>Whether you’re navigating a divorce or simply looking for inspiration, this post is for you. Follow along as I share my story and insights on rebuilding, self-care, and finding happiness after divorce.</p>
</blockquote>



<p>After talking with my friend Ruth (one of the <a href="http://glammr.us/">glammr.us</a> admins and a much smarter tech person than myself) and finding this <a href="https://www.404media.co/instagram-is-generating-inaccurate-seo-bait-for-your-posts/">recent story from 404 Media</a>, I realized that somehow my post had been targeted for Instagram’s AI-generated SEO bait. If you go to the original Instagram post and view the page source, you’ll see that there is both the description text from above, along with a whole bevvy of keywords like “self-discovery” and “finding happiness.”</p>



<figure><a href="https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM.png"><img decoding="async" width="1024" height="394" src="https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM-1024x394.png" alt="A screenshot of Chrome's developer tools showing the AI-generated slop with a pink &quot;WTF&quot; drawn over the top" srcset="https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM-1024x394.png 1024w, https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM-300x115.png 300w, https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM-768x295.png 768w, https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM-1536x590.png 1536w, https://eiratansey.com/wp-content/uploads/2025/12/Screenshot-2025-12-20-at-7.13.15-PM.png 1912w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>While I am sure buried deep in some <a href="https://en.wikipedia.org/wiki/End-user_license_agreement">EULA</a> there is some bullshit allowing Meta to get away with this, I was never explicitly informed about this possibility or asked to review this AI-generated text. Meanwhile, as of today when I look at Instagram’s posting settings, I see that there is a button that requires you to label any AI-generated content you post as the end user, but obviously there is no such obligation in the other direction.</p>



<p>One of the things I gave up long ago about being an extremely online woman who is a <a href="https://tressiemc.com/essays-2/academic-outrage-when-the-culture-wars-go-digital/">micro-public figure</a> with a weird name are any assumptions about privacy. It’s why I drafted the brief essay for my announcement in advance, because I know that once I hit post on anything on the internet I can never unring that bell. I have been writing in the public sphere long enough to know that things that I write are routinely misrepresented by others (a lesson I relearn every time I check some of the citations of my work via Google Scholar updates).&nbsp;</p>



<p>But what I vehemently object to in this situation is the use of the first-person voice without my review or permission. The language used in the description makes it sound as if I wrote it (“In this post, I share my personal journey…”). Because I have fiercely protected my authorship throughout my life and what my name is attached to, any generative AI writing that purports to be in my voice without my informed consent is a profound violation of my authorial voice, agency, and frankly it feels like fraud or impersonation. As an archivist who has spent almost twenty years thinking about accuracy in information, it makes my skin crawl that there is a metadata field with the sole purpose of generating SEO-engagement purporting to be my voice that doesn’t disclose the authorship was actually non-consensual AI.</p>



<p>In recent days I’ve noticed this kind of “some people are saying” text showing up in my search results, especially related to Reddit threads. For example, a list of Google search results showing Reddit threads might show some description like “Some users are discussing the challenges of making a mug cake that doesn’t taste rubbery.” But even if this description had been in the third-person voice such as “This woman discusses her divorce and its impact on her….” my anger would remain.</p>



<p>Because what this AI-generated SEO slop formed from an extremely vulnerable and honest place shows is that women’s pain is still not taken seriously. The tone of the post is laden in the absolute worst therapy/wellness culture cliches (“Follow along as I share my story and insights on rebuilding, self-care, and finding happiness after divorce”) which is language that I abhor since it often trivializes women’s pain. I also object to the flattening of the contours of my particular divorce. There are really important distinctions between the experiences of women who initiate their own divorces versus women who come to a mutual agreement with their spouses to end the marriage versus women, like me, who are completely blindsided by their husbands’ decisions to suddenly end the marriage. All divorces do involve self-discovery and rebuilding your life, but the ways in which you begin down that path often involve dramatically different circumstances.&nbsp;</p>



<p>My story is absolutely layered through with trauma, humiliation, and sudden financial insecurity and I truly resent that this AI-generated garbage erases the deliberately uncomfortable and provocative words I chose to include in my original framing. If my story ends up becoming inspiring to others, that’s great, but that’s not why I share my pain on the internet. I share my pain publicly as a gesture of solidarity with other people, but especially women, who have been profoundly traumatized by those they thought they could love and trust. Having my pain witnessed and acknowledged is part of how I am healing while also letting other people know they are not alone. I have to externalize my pain in regular intervals right now because if I internalize it any more it might actually destroy me. Forcing an “inspiration” vibe onto what I posted feels tone deaf at best, and like toxic positivity at worst.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>We already know that in a patriarchal society, women’s pain is dismissed, belittled, and ignored. This kind of AI-generated language also depoliticizes patriarchal power dynamics. Thinkers like <a href="https://safiyaunoble.com/">Dr. Safiya Noble</a> have been warning us for years that the prejudices carried by the people who design and maintain internet infrastructure continually shapes how we view and think about women and girls in society. I already felt immense pain and anger by the decision of my husband to suddenly end our marriage. And now I feel a double sense of violation that the men who design and maintain and profit from the internet have literally impersonated my voice behind the closed doors of hidden metadata to tell a more palatable version of the story they think will sell.&nbsp;</p>

				<hr>

        
        <p><strong>Tagged with:</strong> </p>
        <p><strong>Categorised as:</strong> <a href="https://eiratansey.com/category/uncategorized/" rel="category tag">Uncategorized</a> </p>

								
				<hr>
				
			</article>

			
<!-- You can start editing here. -->


			<!-- If comments are open, but there are no comments. -->

	 

  
		

		
		

		
	
  		</section>
		
  		<nav>
	<ul>
			</ul>
</nav>
  		

  		






		
				
	  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Android apps using Rust and Iced (134 pts)]]></title>
            <link>https://github.com/ibaryshnikov/android-iced-example</link>
            <guid>46350641</guid>
            <pubDate>Mon, 22 Dec 2025 02:14:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ibaryshnikov/android-iced-example">https://github.com/ibaryshnikov/android-iced-example</a>, See on <a href="https://news.ycombinator.com/item?id=46350641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Example of building android app with iced</h2><a id="user-content-example-of-building-android-app-with-iced" aria-label="Permalink: Example of building android app with iced" href="#example-of-building-android-app-with-iced"></a></p>
<p dir="auto">There are <a href="https://github.com/ibaryshnikov/android-iced-example/blob/master/NativeActivity">NativeActivity</a> and <a href="https://github.com/ibaryshnikov/android-iced-example/blob/master/GameActivity">GameActivity</a> examples here.</p>
<p dir="auto">Based on several other examples:</p>
<ul dir="auto">
<li><code>na-mainloop</code> and <code>agdk-mainloop</code> from
<a href="https://github.com/rust-mobile/android-activity/tree/v0.6.0/examples">android-activity</a></li>
<li><a href="https://github.com/rust-mobile/rust-android-examples/tree/main/na-winit-wgpu">na-winit-wgpu</a>
from <code>rust-android-examples</code></li>
<li><a href="https://github.com/iced-rs/iced/tree/0.14/examples/integration">integration</a>
from <code>iced</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Preview</h2><a id="user-content-preview" aria-label="Permalink: Preview" href="#preview"></a></p>
<p dir="auto">iced integration example</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/pixel_1.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/pixel_1.png" alt="Pixel first screenshot"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/pixel_2.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/pixel_2.png" alt="Pixed second screenshot"></a></p>
<p dir="auto">You can also run most of the examples from iced.
For this omit the scene rendering part and set the background of the root container.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Watch</h2><a id="user-content-watch" aria-label="Permalink: Watch" href="#watch"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/watch_1.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/watch_1.png" alt="Watch first"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/watch_2.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/watch_2.png" alt="Watch second"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/watch_3.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/watch_3.png" alt="Watch third"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Text input</h2><a id="user-content-text-input" aria-label="Permalink: Text input" href="#text-input"></a></p>
<p dir="auto">Text input partially works, unresolved issues:</p>
<ul dir="auto">
<li>window doesn't resize on show/hide soft keyboard</li>
<li>how to change input language of soft keyboard</li>
<li>ime is not supported</li>
</ul>
<p dir="auto">Copy/paste and show/hide soft keyboard is implemented by calling Java</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/pixel_3.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/pixel_3.png" alt="Pixel third screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and running</h2><a id="user-content-building-and-running" aria-label="Permalink: Building and running" href="#building-and-running"></a></p>
<p dir="auto">Check <code>android-activity</code> crate for detailed instructions.
During my tests I was running the following command and using android studio afterwards:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export ANDROID_NDK_HOME=&quot;path/to/ndk&quot;
export ANDROID_HOME=&quot;path/to/sdk&quot;

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build"><pre><span>export</span> ANDROID_NDK_HOME=<span><span>"</span>path/to/ndk<span>"</span></span>
<span>export</span> ANDROID_HOME=<span><span>"</span>path/to/sdk<span>"</span></span>

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build</pre></div>
<p dir="auto">My setup is the following:</p>
<ul dir="auto">
<li>archlinux 6.9.6</li>
<li>jdk-openjdk 22</li>
<li>target api 35</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">Thanks to <code>android-activity</code> we can already build android apps in Rust, and
key crates such as <code>winit</code> and <code>wgpu</code> also support building for android.
<code>iced</code> doesn't support android out of the box, but it can be integrated with
existing graphics pipelines, as shown in
<a href="https://github.com/iced-rs/iced/tree/0.13.1/examples/integration">integration</a> example.
As a result, it was possible to convert existing example running <code>winit</code> + <code>wgpu</code> to
use <code>iced</code> on top.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disney Imagineering Debuts Next-Generation Robotic Character, Olaf (241 pts)]]></title>
            <link>https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</link>
            <guid>46348847</guid>
            <pubDate>Sun, 21 Dec 2025 21:46:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/">https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</a>, See on <a href="https://news.ycombinator.com/item?id=46348847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <p>Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ <em>Frozen</em>. </p>

<p>This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&nbsp;</p>

<div>
<figure><img decoding="async" width="1620" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1620x1080.jpg" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1620x1080.jpg 1620w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-647x431.jpg 647w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-212x141.jpg 212w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-768x512.jpg 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1536x1024.jpg 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1024x683.jpg 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3.jpg 1920w" sizes="(max-width: 1620px) 100vw, 1620px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>


<h2>Innovation at the Core: From Screen to Reality</h2>


<p>From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&nbsp;</p>



<p>Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&nbsp;&nbsp;</p>

<div>
<figure><img decoding="async" width="1920" height="1079" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6.png" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6.png 1920w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-767x431.png 767w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-250x141.png 250w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-768x432.png 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-1536x863.png 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-848x477.png 848w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-1024x575.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>


<h2>The Technology Behind the Magic</h2>


<p>Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&nbsp;</p>

<p>Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&nbsp;&nbsp;</p>

<div>
<figure><img loading="lazy" decoding="async" width="1913" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1913x1080.png" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1913x1080.png 1913w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-763x431.png 763w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-250x141.png 250w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-768x434.png 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1536x867.png 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1024x578.png 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4.png 1920w" sizes="(max-width: 1913px) 100vw, 1913px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>

<p>While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists. </p>

<p>It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&nbsp;&nbsp;</p>

<div>
<figure><img loading="lazy" decoding="async" width="1909" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1909x1080.png" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1909x1080.png 1909w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-762x431.png 762w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-250x141.png 250w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-768x434.png 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1536x869.png 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1024x579.png 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5.png 1920w" sizes="(max-width: 1909px) 100vw, 1909px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>

<p>Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&nbsp;</p>

<p>Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&nbsp;&nbsp;</p>

<p>The BDX Droids, <a href="https://disneyparksblog.com/disney-experiences/herbie-from-the-fantastic-four-first-steps-at-disneyland/" target="_blank" rel="noreferrer noopener">self-balancing H.E.R.B.I.E.</a>, and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world. </p>

<div>
<figure><img loading="lazy" decoding="async" width="720" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-720x1080.jpg" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-720x1080.jpg 720w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-287x431.jpg 287w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-94x141.jpg 94w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-768x1152.jpg 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-1024x1536.jpg 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-1365x2048.jpg 1365w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-512x768.jpg 512w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-scaled.jpg 1707w" sizes="(max-width: 720px) 100vw, 720px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>


<h2>Where Guests Can See Olaf</h2>


<p>Olaf will soon venture out into the unknown, eager to see guests at:</p>

<div>
<ul>
<li>Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.</li>



<li>Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.</li>
</ul>
</div>

<p>Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp; Development, came to life at in the latest episode of <a href="https://www.youtube.com/watch?v=EoPN02bmzrE" target="_blank" rel="noreferrer noopener"><em>We Call It Imagineering</em>.</a></p>


            
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The gift card accountability sink (103 pts)]]></title>
            <link>https://www.bitsaboutmoney.com/archive/gift-card-accountability-sink/</link>
            <guid>46348455</guid>
            <pubDate>Sun, 21 Dec 2025 21:07:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsaboutmoney.com/archive/gift-card-accountability-sink/">https://www.bitsaboutmoney.com/archive/gift-card-accountability-sink/</a>, See on <a href="https://news.ycombinator.com/item?id=46348455">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><em>Programming note</em>: <em>Merry Christmas</em>! <em>There will likely be another Bits about Money after the holiday but before New Year. </em></p><p><em>Bits about Money is </em><a href="https://www.bitsaboutmoney.com/memberships/" rel="noreferrer"><em>supported by our readers</em></a><em>. If your education budget or business can underwrite the coming year of public goods in financial-infrastructure education, commentary, and policy analysis, please consider supporting it. I’m told this is particularly helpful for policymakers and others who cannot easily expense a subscription, and who benefit from all issues remaining publicly available with no paywall.</em></p><p>The American Association of Retired People (AARP, an advocacy non-profit for older adults) has paid for ads on podcasts I listen to. The ad made a claim which felt raspberry-worthy (in service of an important public service announcement), which they <a href="https://www.aarp.org/money/scams-fraud/gift-card-payment/"><u>repeat in writing</u></a>: <em>Asking to be paid by gift card is always a scam</em>.</p><p>Of course it isn’t. Gift cards are a payments rail, and an <em>enormous</em> business independently of being a payments rail. Hundreds of firms will indeed ask you to pay them on gift cards! They also exist, and are marketed, explicitly to do the thing that the AARP implicitly asserts no business or government entity will ever do: provide a method for transacting for people who do not have a banked method of transacting. [0]</p><p>Gift card scams are also enormous. The FBI’s Internet Crime Complaint Center received <a href="https://www.ic3.gov/AnnualReport/Reports/2024_IC3Report.pdf"><u>$16.6 billion</u></a> in reports in 2024 across several payment methods; this is just for those consumers who bothered reporting it, in spite of the <em>extremely real</em> received wisdom that reporting is unlikely to improve one’s direct situation.</p><p>The flavor texts of scams vary wildly, but in substance they’ll attempt to convince someone, often someone socially vulnerable, to part with sometimes very large sums of money by buying gift cards and conveying card information (card number and PIN number, both printed on the card) to the scammer. The scammer will then use the <a href="https://www.bitsaboutmoney.com/archive/the-fraud-supply-chain/"><u>fraud supply chain</u></a>, generally to swap the value on the card to another actor in return for value unconnected to the card. This can be delivered in many ways: cash, crypto, products and services in the scamming economy (such as purloined credit cards or even “lead lists” of vulnerable people to run more scams on), or <a href="https://www.bitsaboutmoney.com/archive/kyc-and-aml-beyond-the-acronyms/"><u>laundered</u></a> funds within regulated financial institutions which obscure the link between the crime and the funds (layering, in the parlance of AML professionals). A huge portion of <a href="https://www.bitsaboutmoney.com/archive/gift-card-marketplaces/"><u>running a gift card marketplace</u></a> is trying to prevent yourself from being exploited or made into an instrumentality in exploiting others.</p><p>It surprises many people to learn that the United States <a href="https://www.consumerfinance.gov/rules-policy/regulations/1005/"><u>aggressively defends customers from fraud over some payment methods</u></a>, via a liability transfer to their financial institution, which transfers it to intermediaries, who largely transfer it to payment-accepting businesses. Many people think the U.S. can’t make large, effective, pro-consumer regulatory regimes. They are straightforwardly wrong… some of the time.</p><p>But the AARP, the FBI, and your friendly local payments nerd will all tell you that if you’re abused on your debit card you are quite likely to be made whole, and if you’re abused via purchasing gift cards, it is unlikely any deep pockets will cover for you. The difference in treatment is partially regulatory carveouts, partially organized political pressure, and partly a side effect of an accountability sink specific to the industrial organization of gift cards.</p><h2 id="most-businesses-do-not-run-their-own-gift-card-programs">Most businesses do not run their own gift card programs</h2><p>There exists an ecosystem of gift card program managers, who are essentially financial services businesses with a sideline in software. (I should probably mention that I previously worked for and am currently an advisor to Stripe, whose self conception would not be precisely that, but which a) supports many ways for people to pay money for things and b) does not necessarily endorse what I say in my personal spaces.)</p><p>Why does the program manager exist? Why not simply have the retailer keep some internal database of who the retailer owes money to, updating this when someone buys or loads a gift card and when they spend the balance at the store? Because this implies many capabilities that retailers do not necessarily have, such as e.g. software development teams.</p><p>There is also a large regulatory component to running a gift card program, despite gift cards’ relatively lax regulatory drag (we’ll return to that in a moment). Card programs are regulated at both the federal and state levels. One frequent requirement in several states is escheatment. (Essentially all states have a requirement for escheatment; many but not all <a href="https://www.alston.com/en/insights/publications/2024/11/developments-impacting-gift-card-programs"><u>exempt gift cards from it</u></a>.)</p><p>As <a href="https://www.bitsaboutmoney.com/archive/more-than-you-want-to-know-about-gift-cards/"><u>discussed previously</u></a> in Bits about Money, a major component of the gift card business model is abandonment (“breakage”). Consumer advocates felt this was unfair to consumers, bordering on fraudulent really. They convinced states to take the money that retailers were keeping for themselves. (Many states didn’t take all that much convincing.)&nbsp;</p><p>In theory, and sometimes even in practice, a consumer can convince a state treasurer’s office of unclaimed property (e.g. <a href="https://icash.illinoistreasurer.gov/"><u>Illinois</u></a>’) that the $24.37 that Target remitted as part of its quarterly escheatment payment for an unused gift card 13 years ago was actually theirs. A consumer who succeeds at this, which is neither easy nor particularly inexpensive to do, will receive a $24.37 check in the mail. The state keeps the interest income; call it a fee for service. It also keeps the interest income of the tens of billions of dollars of accumulated unclaimed property, which it generally promises to dutifully custody awaiting a legitimate claim for as long as the United States shall exist.</p><p>And so if you are a regional or national retailer who wants to offer gift cards, you have a choice. You can dedicate a team of internal lawyers and operations specialists to understanding both what the laws of the several states require with respect to gift cards, which are a <em>tiny</em> portion of your total operations, not merely today but as a result of the next legislative session in Honolulu, because you <em>absolutely must </em>order the software written to calculate the payment to remit accurately several quarters in advance of the legal requirement becoming effective. Or you can make the much more common choice, and outsource this to a specialist.</p><p>That specialist, the gift card program manager, will sell you a Solution™ which integrates across all the surfaces you need: your point-of-sale systems, your website, your accounting software, the 1-800 number and website for customers to check balances, ongoing escheatment calculation and remittance, cash flow management, <em>carefully titrated </em>amounts of attention to other legal obligations like AML compliance, etc. Two representative examples: Blackhawk Network and InComm Payments. You’ve likely never heard of them, even if you have their product on your person right now. Their <em>real</em> customer has the title Director of Payments at e.g. a Fortune 500 company.</p><p>And here begins the accountability sink: by standard practice and contract, when an unsophisticated customer is abused by being asked to buy a BigCo gift card, BigCo will say, truthfully and unhelpfully, that BigCo does not issue BigCo gift cards. It sells them. It accepts them. But it does not issue them. Your princess is in another castle.</p><p>BigCo may very well have a large, well-staffed fraud department. But, not due to any sort of malfeasance whatsoever, that fraud department may consider BigCo gift cards <em>entirely out of their own scope</em>. They <em>physically cannot access the database with the cards</em>. Their security teams, sensitive that gift card numbers are dangerous to keep lying around, very likely made it impossible for anyone at BigCo to reconstruct what happened to a particular gift card between checkout and most recent use. “Your privacy is important to us!” they will say, and they are not cynically invoking it in this case.</p><h2 id="gift-cards-are-not-regulated-like-other-electronic-payments-instruments">Gift cards are not regulated like other electronic payments instruments</h2><p>As mentioned above, Regulation E is the primary driver for the private enforcement edifice that makes scarily smart professionals (and their attached balance sheets) swing into action on behalf of consumers. Reg E has a carveout for certain prepaid payments. Per <a href="https://files.consumerfinance.gov/f/documents/cfpb_prepaid_small-entity-compliance-guide.pdf"><u>most recent guidance</u></a>, that includes prepaid gift cards, gift certificates, and similar.</p><p>And so, if you call your bank and say, “I was defrauded! Someone called me and pretended to be the IRS, and I read them my debit card number, and now I’ve lost money,” the state machine obligates the financial institution to have the customer service representative click a very prominent button on their interface. This will restore your funds very quickly and have some side effects you probably care about much less keenly. One of those is an “investigation,” which is not really an investigation in the commanding majority of cases.</p><p>And if you call the program manager and say, “I was defrauded! Someone called me and pretended to be the IRS, and I read them a gift card number, and now I’ve lost money,” there is… no state machine. There is no legal requirement to respond with alacrity, no statutorily imposed deadline, no button for a CS rep to push, and no investigation to launch. You will likely be told by a low-paid employee that this is unfortunate and that you should file a police report. The dominant reason for this is that suggesting a concrete action to you gets you off the phone faster, and the call center aggressively minimizes time to resolution of calls and recidivism, where you call back because your problem is not solved. Filing a police report will, in most cases, not restore your money—but if it causes you not to call the 1-800 number again, then from the card program manager’s perspective <em>this issue has been closed successfully</em>.</p><h2 id="why-do-we-choose-this-difference-in-regulation">Why do we choose this difference in regulation?</h2><p>The people of the United States, through their elected representatives and the civil servants who labor on their behalf, intentionally exempt gift cards from the Reg E regime in the interest of facilitating commerce.</p><p>It is the ordinary and appropriate work of a democracy to include input from citizens in the rulemaking process. The Retail Industry Leaders Association <a href="https://rilastagemedia.blob.core.windows.net/rila-web/rila.web/media/media/pdfs/regulatory%20letters/fincen/2011/comments-on-prepaid-access-regulations.pdf?ext=.pdf"><u>participated</u></a>, explaining to FinCEN that it would be quite burdensome for retailers to fall into KYC scope, etc etc. Many other lobbyists and industry associations made directionally similar comments.</p><p>The Financial Crimes Enforcement Network, for example, has an <a href="https://www.fincen.gov/news/news-releases/fincen-issues-prepaid-access-final-rule"><u>explicit carveout</u></a> in its regulations: while FinCEN will <a href="https://www.bitsaboutmoney.com/archive/debanking-and-debunking/"><u>aggressively police rogue bodegas</u></a>, it has no interest in you if you sell closed-loop gift cards of less than $2,000 face value. This is explicitly to balance the state’s interest in law enforcement against, quote, preserving innovation and the many legitimate uses and societal benefits offered by prepaid access, endquote.</p><p>FinCEN’s rules clarify that higher-value activity—such as selling more than $10,000 in gift cards to a single individual in a day—brings sellers back into scope. Given the relatively lax enforcement environment for selling a $500 gift card, you very likely might not build out systems which will successfully track customer identities and determine that the same customer has purchased twenty-one $500 gift cards in three transactions. That likely doesn’t rate as a hugely important priority for Q3.&nbsp;</p><p>And so the fraud supply chain comes to learn which firms haven’t done that investment, and preferentially suggests those gift cards to their launderers, <a href="https://theintercept.com/2018/11/23/bitcoin-gift-card-trading-scams/"><u>mules</u></a>, <a href="https://globalchinapulse.net/moving-bricks-money-laundering-practices-in-the-online-scam-industry/"><u>brick movers</u></a>, and scam victims.</p><p>And that’s why the AARP tells fibs about gift cards: we have, with largely positive intentions and for good reasons, exposed them to less regulation than most formal payment systems in the United States received. That decision has a cost. Grandma sometimes pays it.</p><p>[0] Indeed, there are entire companies which exist to turn gift cards into an alternate financial services platform, explicitly to give unbanked and underbanked customers a payments rail. <a href="https://www.paysafe.com/us-en/"><u>Paysafe</u></a>, for example, is a publicly traded company with thousands of employees, the constellation of regulatory supervision you’d expect, and a subsidiary <a href="https://www.openbucks.com/company/about/"><u>Openbucks</u></a> which is designed to give businesses the ability to embed Pay Us With A Cash Voucher in their websites/invoices/telephone collection workflows. This is exactly the behavior that “never happens from a legitimate business” except when it does by the tens of billions of dollars.</p><p>As Bits about Money has frequently observed, people who write professionally about money—including professional advocates for financially vulnerable populations—often misunderstand alternative financial services, largely because those services are designed to serve a social class that professionals themselves do not belong to, rarely interact with directly, and do not habitually ask how they pay rent, utilities, or phone bills.</p>

        

        <div>
          <h2>Want more essays in your inbox?</h2>
          <p>I write about the intersection of tech and finance, approximately biweekly. It's free.</p>
                  </div>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A guide to local coding models (528 pts)]]></title>
            <link>https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</link>
            <guid>46348329</guid>
            <pubDate>Sun, 21 Dec 2025 20:55:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude">https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</a>, See on <a href="https://news.ycombinator.com/item?id=46348329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!fARn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!fARn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!fARn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!fARn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:80981,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!fARn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!fARn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em>[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.</em></p><p><em><span>[Edit 2] This hypothesis was </span><strong>actually wrong</strong><span> and thank you to everyone who commented! </span></em></p><p><em>Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.</em></p><p><em><span>I’m </span><strong>not </strong><span>editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below. </span></em></p><p><em>There is one takeaway this article provides that definitely holds true:</em></p><ul><li><p><em>Local models are far more capable than they’re given credit for, even for coding.</em></p></li></ul><p><em>It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.</em></p><p><em><strong>But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.</strong></em></p><p><em>This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:</em></p><ul><li><p><em>While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.</em></p></li><li><p><em><span>I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. </span><strong>I would not recommend running local models as a company</strong><span> instead of giving employees access to a tool like Claude Code.</span></em></p></li><li><p><em>While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.</em></p></li></ul><p><em>So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.</em></p><p>Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).</p><p>So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!msVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!msVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 424w, https://substackcdn.com/image/fetch/$s_!msVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 848w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1272w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!msVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png" width="618" height="394.7759197324415" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:1196,&quot;resizeWidth&quot;:618,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!msVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 424w, https://substackcdn.com/image/fetch/$s_!msVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 848w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1272w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that </span><strong><span>my hypothesis was </span><s>correct, with nuance</s></strong><span>,</span><strong> not correct </strong><span>[see edit 2 above] which I’ll get into later in this article.</span></p><p>In this article, we cover:</p><ul><li><p>Why local models matter and the benefits they provide.</p></li><li><p>How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.</p></li><li><p>Walk through setting up your own local coding model and tool step-by-step.</p></li></ul><p>Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.</p><p>If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.</p><ul><li><p><strong>Local coding models are very capable</strong><span>. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. </span><strong>[Edited to add in this next part]</strong><span> Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.</span></p></li><li><p><strong>Tools matter a lot</strong><span>. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.</span></p></li><li><p><strong>There’s a lot to consider when you’re actually working within hardware constraints.</strong><span> We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.</span></p></li><li><p><strong>Google threw a wrench into my hypothesis</strong><span>. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below. </span></p></li></ul><p>If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.</p><p><span>You might wonder why local models are worth investing in at all. The obvious answer is </span><strong>cost</strong><span>. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!OUrN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!OUrN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!OUrN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:140073,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!OUrN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>First: </span><strong>Reliability</strong><span>. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.</span></p><p><span>Second: </span><strong>Local models can apply to </strong><em><strong>far </strong></em><strong>more applications</strong><span>. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.</span></p><p>With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.</p><p><span>Finally: </span><strong>Availability. </strong><span>Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).</span></p><p>While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LxTS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LxTS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LxTS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:136854,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LxTS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.</p><p>Local AI has two parts that eat up your memory: The model itself and the model’s context window.</p><p>The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).</p><p>The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.</p><p>When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.</p><p>The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.</p><p>This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cVCW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cVCW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cVCW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109124,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cVCW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works </span><a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list" rel="">here</a><span>.</span></p><p><span>The second trick is quantizing the values you’re working with. </span><a href="https://www.byteplus.com/en/what-is/quantization" rel="">Quantization means converting a continuous set of values into a smaller amount of distinct values</a><span>. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.</span></p><p>You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.</p><p><span>We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it </span><a href="https://arxiv.org/pdf/2510.10964" rel="">causes the model to forget details in long reasoning traces</a><span>. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.</span></p><p>In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.</p><p>Here are a few more factors to understand when setting up a local coding model on your hardware:</p><p>Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).</p><p>You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.</p><p>Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.</p><p>MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.</p><p>Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.</p><p>In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.</p><p>This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.</p><p>There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.</p><p>The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.</p><p><span>There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are </span><a href="https://opencode.ai/" rel="">OpenCode</a><span>, </span><a href="https://aider.chat/" rel="">Aider</a><span>, </span><a href="https://github.com/QwenLM/qwen-code" rel="">Qwen Code</a><span>, </span><a href="https://roocode.com/" rel="">Roo Code</a><span>, and </span><a href="https://www.continue.dev/" rel="">Continue</a><span>. Make sure to use a tool compatible with </span><a href="https://bentoml.com/llm/llm-inference-basics/openai-compatible-api" rel="">OpenAI’s API standard</a><span>. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.</span></p><p><span>I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that </span><strong>tooling matters a lot</strong><span>. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.</span></p><p>If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.</p><p>For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.</p><p>For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.</p><p><span>I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with </span><em>a ton</em><span> of RAM. For serving local coding models, more is always better.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!4IEy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!4IEy!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!4IEy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:133148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!4IEy!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Xqrl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Xqrl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:171397,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Xqrl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>I’ve shared my </span><a href="https://github.com/loganthorneloe/modelfiles" rel="">modelfiles repo</a><span> for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.</span></p><ol><li><p><span>Install </span><a href="https://github.com/ml-explore/mlx" rel="">MLX</a><span> or download </span><a href="https://ollama.com/download" rel="">Ollama</a><span> (the rest of this guide will continue with MLX but details for serving on Ollama can be found </span><a href="https://docs.ollama.com/quickstart" rel="">here</a><span>).</span></p></li><li><p>Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.</p></li><li><p>Run pip install -U mlx-lm to install MLX for serving community models.</p></li><li><p>Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).</p></li><li><p><span>Download </span><a href="https://github.com/QwenLM/qwen-code" rel="">Qwen Code</a><span>. You might need to install Node Package Manager for this. I recommend using </span><a href="https://github.com/nvm-sh/nvm" rel="">Node Version Manager</a><span> (nvm) for managing your npm version.</span></p></li><li><p>Set up your tool to access an OpenAI compatible API by entering the following settings:</p><ol><li><p><span>Base URL: </span><a href="http://localhost:8080/v1" rel="">http://localhost:8080/v1</a><span> (should be the default MLX serves your model at)</span></p></li><li><p>API Key: mlx</p></li><li><p>Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).</p></li></ol></li><li><p>Voila! Your coding model tool should be working with your local coding model.</p></li></ol><p><span>I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run </span><strong>very </strong><span>slowly.</span></p><p><strong>One tip I have for using local coding models</strong><span>: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.</span></p><p><span>My original hypothesis was: </span><strong>Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.</strong></p><p><span>I would argue that</span><s>—yes!—</s><strong>no </strong><span>[see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.</span></p><p><span>[This paragraph was added in after initial release of this article] It’s important to note that local models will </span><strong>not</strong><span> reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.</span></p><p><strong>It’s also important to note that local models are only going to get better and smaller</strong><span>. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.</span></p><p>From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wAV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wAV2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wAV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:162037,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wAV2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.</p><p>Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.</p><p><span>However, this is foiled a bit now that </span><a href="https://blog.google/products/gemini/gemini-3-flash/" rel="">Gemini 3 Flash</a><span> was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.</span></p><p>I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.</p><p>Thanks for reading!</p><p><strong>Always be (machine) learning,</strong></p><p><strong>Logan</strong></p><p data-attrs="{&quot;url&quot;:&quot;https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[More on whether useful quantum computing is “imminent” (124 pts)]]></title>
            <link>https://scottaaronson.blog/?p=9425</link>
            <guid>46348318</guid>
            <pubDate>Sun, 21 Dec 2025 20:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scottaaronson.blog/?p=9425">https://scottaaronson.blog/?p=9425</a>, See on <a href="https://news.ycombinator.com/item?id=46348318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-9425">
				
<p>These days, the most common question I get goes something like this:</p>



<blockquote>
<p>A decade ago, you told people that scalable quantum computing wasn’t imminent. Now, though, you claim it plausibly <em>is</em> imminent. Why have you reversed yourself??</p>
</blockquote>



<p>I appreciated the friend of mine who paraphrased this as follows: “A decade ago you said you were 35. Now you say you’re 45. Explain yourself!”</p>



<hr>



<p>A couple weeks ago, I was delighted to attend <a href="https://q2b.qcware.com/conference/2025-silicon-valley">Q2B</a> in Santa Clara, where I gave a keynote talk entitled <a href="https://www.scottaaronson.com/talks/works.pptx">“Why I Think Quantum Computing Works”</a> (link goes to the PowerPoint slides).  This is one of the most optimistic talks I’ve ever given.  But mostly that’s just because, uncharacteristically for me, here I gave short shrift to the challenge of broadening the class of problems that achieve huge quantum speedups, and just focused on the experimental milestones achieved over the past year.  With every experimental milestone, the little voice in my head that asks “but what if Gil Kalai turned out to be right after all? what if scalable QC <em>wasn’t</em> possible?” grows quieter, until now it can barely be heard.</p>



<p>Going to Q2B was extremely helpful in giving me a sense of the current state of the field.  Ryan Babbush gave a <em>superb</em> overview (I couldn’t have improved a word) of the current status of quantum algorithms, while John Preskill’s annual where-we-stand talk was “magisterial” as usual (that’s the word I’ve long used for his talks), making mine look like just a warmup act for his.  Meanwhile, Quantinuum took a victory lap, boasting of their recent successes in a way that I considered basically justified.</p>



<hr>



<p>After returning from Q2B, I then did an hour-long <a href="https://www.youtube.com/watch?si=T9u5MjX9xwCY9zeJ&amp;v=0_7SH3Eons0&amp;feature=youtu.be">podcast</a> with “The Quantum Bull” on the topic “How Close Are We to Fault-Tolerant Quantum Computing?”  You can watch it here:</p>



<figure><p>
<iframe title="Scott Aaronson on the Possibility of Fault-Tolerant Quantum Computing by 2028" width="500" height="281" src="https://www.youtube.com/embed/0_7SH3Eons0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>As far as I remember, this is the first YouTube interview I’ve ever done that concentrates entirely on the current state of the QC race, skipping any attempt to explain amplitudes, interference, and other basic concepts.  Despite (or conceivably because?) of that, I’m happy with how this interview turned out.  Watch if you want to know my detailed current views on hardware—as always, I recommend 2x speed.</p>



<p>Or for those who don’t have the half hour, a quick summary:</p>



<ul>
<li>In quantum computing, there are the large companies and startups that might succeed or might fail, but are at least trying to solve the real technical problems, and some of them are making amazing progress. And then there are the companies that have optimized for doing IPOs, getting astronomical valuations, and selling a narrative to retail investors and governments about how quantum computing is poised to revolutionize optimization and machine learning and finance. Right now, I see these two sets of companies as <strong>almost entirely disjoint from each other</strong>.<br></li>



<li>The interview also contains my most direct condemnation yet of some of the wild misrepresentations that IonQ, in particular, has made to governments about what QC will be good for (“unlike AI, quantum computers won’t hallucinate because they’re deterministic!”)<br></li>



<li>The two approaches that had the most impressive demonstrations in the past year are trapped ions (especially Quantinuum but also Oxford Ionics) and superconducting qubits (especially Google but also IBM), and perhaps also neutral atoms (especially QuEra but also Infleqtion and Atom Computing).<br></li>



<li>Contrary to a misconception that refuses to die, I <em>haven’t</em> dramatically changed my views on any of these matters. As I have for a quarter century, I continue to profess a lot of confidence in the basic principles of quantum computing theory worked out in the mid-1990s, and I <em>also</em> continue to profess ignorance of exactly how many years it will take to realize those principles in the lab, and of which hardware approach will get there first.<br></li>



<li>But yeah, <em>of course</em> I update in response to developments on the ground, because it would be insane not to! And 2025 was clearly a year that met or exceeded my expectations on hardware, with multiple platforms now boasting &gt;99.9% fidelity two-qubit gates, at or above the theoretical threshold for fault-tolerance. This year updated me in favor of taking more seriously the aggressive pronouncements—the “roadmaps”—of Google, Quantinuum, QuEra, PsiQuantum, and other companies about where they could be in 2028 or 2029.<br></li>



<li>One more time for those in the back: the main <em>known</em> applications of quantum computers remain (1) the simulation of quantum physics and chemistry themselves, (2) breaking a lot of currently deployed cryptography, and (3) eventually, achieving some <em>modest</em> benefits for optimization, machine learning, and other areas (but it will probably be a while before those modest benefits win out in practice).  To be sure, the detailed list of quantum speedups expands over time (as new quantum algorithms get discovered) and also contracts over time (as some of the quantum algorithms get dequantized).  But the list of known applications “from 30,000 feet” remains fairly close to what it was a quarter century ago, after you hack away the dense thickets of obfuscation and hype.</li>
</ul>



<hr>



<p>I’m going to close this post with a warning.  When Frisch and Peierls wrote their <a href="https://en.wikipedia.org/wiki/Frisch%E2%80%93Peierls_memorandum">now-famous memo</a> in March 1940, estimating the mass of Uranium-235 that would be needed for a fission bomb, they didn’t publish it in a journal, but communicated the result through military channels only.  As recently as February 1939, Frisch and Meitner had <a href="https://www.nature.com/articles/143239a0">published in <em>Nature</em></a> their theoretical explanation of recent experiments, showing that the uranium nucleus could fission when bombarded by neutrons.  But by 1940, Frisch and Peierls realized that the time for open publication of these matters had passed.</p>



<p>Similarly, at some point, the people doing detailed estimates of how many physical qubits and gates it’ll take to break actually deployed cryptosystems using Shor’s algorithm are going to stop publishing those estimates, if for no other reason than the risk of giving too much information to adversaries. Indeed, for all we know, that point may have been passed already. This is the clearest warning that I can offer in public right now about the urgency of migrating to post-quantum cryptosystems, a process that I’m grateful is already underway.</p>



<hr>



<p><strong><mark>Update:</mark></strong> Someone on Twitter who’s “long $IONQ” says he’ll be <a href="https://x.com/SPAC_Infleqtion/status/2002826241146302651">posting about and investigating me</a> every day, never resting until UT Austin fires me, in order to punish me for slandering IonQ and other “pure play” SPAC IPO quantum companies. And also, because I’ve been anti-Trump and pro-Biden. He confabulates that I must be trying to profit from my stance (eg by shorting the companies I criticize), it being inconceivable to him that anyone would say anything purely because they care about what’s true.</p>

		
				
				<p>
					<small>
						This entry was posted
												on Sunday, December 21st, 2025 at 11:34 am						and is filed under <a href="https://scottaaronson.blog/?cat=10" rel="category">Adventures in Meatspace</a>, <a href="https://scottaaronson.blog/?cat=4" rel="category">Quantum</a>, <a href="https://scottaaronson.blog/?cat=17" rel="category">Speaking Truth to Parallelism</a>.
						You can follow any responses to this entry through the <a href="https://scottaaronson.blog/?feed=rss2&amp;p=9425">RSS 2.0</a> feed.

													You can <a href="#respond">leave a response</a>, or <a href="https://scottaaronson.blog/wp-trackback.php?p=9425" rel="trackback">trackback</a> from your own site.

						
					</small>
				</p>

			</div><p>You can use rich HTML in comments!  You can also use basic TeX, by enclosing it within <span>$$ $$</span> for displayed equations or <span>\( \)</span> for inline equations.</p><p>
	After two decades of mostly-open comments, in July 2024 <i>Shtetl-Optimized</i> transitioned to the following policy:
	
</p><p>All comments are treated, by default, as personal missives to me, Scott Aaronson---with no expectation either that they'll appear on the blog or that I'll reply to them.

</p><p>At my leisure and discretion, and in consultation with the <a href="https://scottaaronson.blog/?p=6576"><i>Shtetl-Optimized</i> Committee of Guardians</a>, I'll put on the blog a curated selection of comments that I judge to be particularly interesting or to move the topic forward, and I'll do my best to answer those.  But it will be more like Letters to the Editor.  Anyone who feels unjustly censored is welcome to the rest of the Internet.

</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rue: Higher level than Rust, lower level than Go (209 pts)]]></title>
            <link>https://rue-lang.dev/</link>
            <guid>46348262</guid>
            <pubDate>Sun, 21 Dec 2025 20:46:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rue-lang.dev/">https://rue-lang.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46348262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Higher level than Rust, lower level than Go</p></div><div><div><h2>Memory Safe</h2><p>No garbage collector, no manual memory management. A work in progress, though.</p></div><div><h2>Simple Syntax</h2><p>Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.</p></div><div><h2>Fast Compilation</h2><p>Direct compilation to native code.</p></div></div><div><h2>Hello, Rue</h2><div><pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> It's a classic for a reason
</span></span><span><span><span><span>fn</span> </span><span>fib</span></span><span><span><span>(</span><span>n</span><span>:</span> <span>i32</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>i32</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        n
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span> <span>else</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        <span>fib</span><span><span>(</span>n <span>-</span> <span>1</span></span><span><span>)</span></span> <span>+</span> <span>fib</span><span><span>(</span>n <span>-</span> <span>2</span></span><span><span>)</span></span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>i32</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span>//</span> Print the first 20 Fibonacci numbers
</span></span></span></span><span><span><span>    <span>let</span> <span>mut</span> i <span>=</span> <span>0</span><span>;</span>
</span></span></span><span><span><span>    <span>while</span> i <span>&lt;</span> <span>20</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        <span>@</span><span>dbg</span><span><span>(</span><span>fib</span><span><span>(</span>i</span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span><span><span><span><span>        i <span>=</span> i <span>+</span> <span>1</span><span>;</span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span><span>//</span> Return fib(10) = 55
</span></span></span></span><span><span><span>    <span>fib</span><span><span>(</span><span>10</span></span><span><span>)</span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I can't upgrade to Windows 11, now leave me alone (501 pts)]]></title>
            <link>https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone</link>
            <guid>46347108</guid>
            <pubDate>Sun, 21 Dec 2025 18:43:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone">https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone</a>, See on <a href="https://news.ycombinator.com/item?id=46347108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="articleBody">
			<h2>Microsoft won't let you dismiss the upgrade notification</h2>

<p>So support for Windows 10 has ended. Yes, millions of users are still on it. One of my main laptops runs Windows 10. I can't update to Windows 11 because of the hardware requirements. It's not that I don't have enough RAM, storage, or CPU power. The hardware limitation is specifically TPM 2.0.</p>

<p>What is TPM 2.0, you say? It stands for Trusted Platform Module. It's basically a security chip on the motherboard that enables some security features. It's good and all, but Windows says my laptop doesn't support it. Great! Now leave me alone.</p>

<p>Well, every time I turn on my computer, I get a reminder that I need to update to Windows 11. OK, at this point a Windows machine only belongs to you in name. Microsoft can run arbitrary code on it. They already ran the code to decide that my computer doesn't support Windows 11. So why do they keep bothering me?</p>

<p><a href="https://cdn.idiallo.com/images/assets/daily/92/eol_big.jpg"><img src="https://cdn.idiallo.com/images/assets/daily/92/eol.jpg" alt="Windows 10 end of life announcement"></a>
</p>

<p>Fine, I'm frustrated. That's why I'm complaining. I've accepted the fact that my powerful, yet 10-year-old laptop won't get the latest update. But if Microsoft's own systems have determined my hardware is incompatible, why are they harassing? I'll just have to dismiss this notification and call it a day.</p>

<p>But wait a minute. How do I dismiss it?</p>

<p><img src="https://cdn.idiallo.com/images/assets/daily/92/buttons.jpg" alt="remind me later or learn more">
</p>

<p>I cannot dismiss it. I can only be reminded later or... I have to learn more. If I click "remind me later," I'm basically telling Microsoft that I consent to being shown the same message again whenever they feel like it. If I click "learn more"? I'm taken to the <a href="https://www.microsoft.com/en-us/windows/laptop-buying-guide">Windows Store</a>, where I'm shown ads for different laptops I can buy instead. Apparently, I'm also probably giving them consent to show me this ad the next time I log in.</p>

<p><img src="https://cdn.idiallo.com/images/assets/daily/92/buy.jpg" alt="windows laptop buying guide">
</p>

<p>It's one thing to be at the forefront of enshittification, but Microsoft is now <a href="https://idiallo.com/blog/hostile-not-enshittification">actively hostile to its users</a>. I've written about this <a href="https://idiallo.com/byte-size/say-no-to-onedrive-backup">passive-aggressive illusion of choice</a> before. They are basically asking "Do you want to buy a new laptop?" And the options they are presenting are "Yes" and "OK."</p>

<p>This isn't a bug. This is intentional design. Microsoft has deliberately removed the ability to decline.</p>

<h2>Dear Microsoft</h2>

<p>Listen. You said my device doesn't support Windows 11. You're right. Now leave me alone. I have another device running Windows 11. It's festered with ads, and you're trying everything in your power to get me to create a Microsoft account.</p>

<p>I paid for that computer. I also paid for a pro version of the OS. I don't want OneDrive. I don't want to sign up with my Microsoft account. Whether I use my computer online or offline is none of your business. In fact, if you want me to create an account on your servers, you are first required to register your OS on my own website. The terms and conditions are simple. Every time you perform any network access, you have to send a copy of the payload and response back to my server. Either that, or you're in breach of my terms.</p>

<p><strong>Notes:</strong></p>

<p>By the way, the application showing this notification is called <strong>Reusable UX Interaction Manager</strong> sometimes. Other times it appears as <strong>Campaign Manager</strong>.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mullvad VPN: "This is a Chat Control 3.0 attempt." (613 pts)]]></title>
            <link>https://mastodon.online/@mullvadnet/115742530333573065</link>
            <guid>46347080</guid>
            <pubDate>Sun, 21 Dec 2025 18:39:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.online/@mullvadnet/115742530333573065">https://mastodon.online/@mullvadnet/115742530333573065</a>, See on <a href="https://news.ycombinator.com/item?id=46347080">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[You’re not burnt out, you’re existentially starving (315 pts)]]></title>
            <link>https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/</link>
            <guid>46346958</guid>
            <pubDate>Sun, 21 Dec 2025 18:28:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/">https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/</a>, See on <a href="https://news.ycombinator.com/item?id=46346958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p><strong><em><mark>“Those who have a ‘Why’ to live, can bear with almost any ‘How’.”</mark></em></strong></p>



<p><em>― Viktor Frankl quoting Friedrich Nietzsche, <a href="https://en.wikipedia.org/wiki/Man's_Search_for_Meaning" target="_blank" rel="noreferrer noopener">Man’s Search for Meaning</a></em></p>



<p><strong><mark>Let me guess:</mark></strong></p>



<ol>
<li><strong>Your life is going pretty darn well by any objective metric.</strong>
<ul>
<li>Nice place to live. More than enough stuff. Family and friends who love you.</li>
</ul>
</li>



<li><strong>But you’re tired, burnt out, and more.</strong>
<ul>
<li>It feels like you’re stuck in the ordinary when all you want to do is chase greatness.</li>
</ul>
</li>
</ol>



<p><strong><mark>Viktor Frankl calls this feeling the “existential vacuum”</mark></strong> in his famous book <em>Man’s Search for Meaning</em>. Frankl was a psychologist who survived the Holocaust, and in this book he explains that the inmates who survived with him found and focused on a higher purpose in life, like caring for other inmates and promising to stay alive to reconnect with loved ones outside the camps. But these survivors also struggled in their new lives after the war, desperately searching for meaning when every decision was no longer life or death.</p>



<p><strong>Frankl realized that this existential anxiety is not a nuisance to eliminate</strong>, but actually an important signal pointing us towards our need for meaning. Similarly, while Friedrich Nietzsche would argue that life inherently lacks meaning, he’d also implore us to zoom out and find our highest purpose now:</p>



<p><em>“</em><strong><em>This is the most effective way: to let the youthful soul look back on life with the question, ‘What have you up to now truly loved, what has drawn your soul upward, mastered it and blessed it too?’… <mark>for your true being lies not deeply hidden within you, but an infinite height above you, or at least above that which you commonly take to be yourself.</mark></em></strong><em>“</em></p>



<p><em>— Friedrich Nietzsche, <a href="https://en.wikipedia.org/wiki/Untimely_Meditations" target="_blank" rel="noreferrer noopener">Untimely Meditations</a>, 1874</em></p>



<p><strong><mark>Nihilists get both Nietzsche and YOLO wrong.</mark> Neither mean that you give up. Instead, both mean that your efforts are everything.</strong></p>



<p>So when you get those Sunday Scaries, the existential anxiety that <em>your</em> time is ending and the rest of your life is spent working for someone else, the answer isn’t escapism.</p>



<p>Instead, visualize your ideal self, the truest childhood dream of who you wanted to be when you grew up. What would that person be doing now? Go do that thing!</p>



<p><strong><mark>When facing the existential vacuum, there’s only one way out —&nbsp;up, towards your highest purpose.</mark></strong></p>



<hr>



<p><strong><mark>On a 0-10 scale, how happy did you feel when you started working this Monday?</mark></strong></p>



<p><strong><mark>Why wasn’t your answer a 10?</mark></strong></p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="900" height="600" data-attachment-id="10027" data-permalink="https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/mondays-happiness-scale/" data-orig-file="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?fit=1536%2C1024&amp;ssl=1" data-orig-size="1536,1024" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Mondays Happiness Scale" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?fit=300%2C200&amp;ssl=1" data-large-file="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?fit=900%2C600&amp;ssl=1" src="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=900%2C600&amp;ssl=1" alt="" srcset="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?w=1536&amp;ssl=1 1536w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=1024%2C683&amp;ssl=1 1024w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=768%2C512&amp;ssl=1 768w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=1200%2C800&amp;ssl=1 1200w" sizes="(max-width: 900px) 100vw, 900px"></figure>



<p><strong>You got the great job. You built the startup. You took the vacations. But that’s not what you really needed.</strong> You kept coming back Monday after Monday realizing you were doing the same job again.</p>



<p><strong>So you tried to improve yourself.</strong> You optimized your morning routine. You perfected your productivity system. You bought a sleep mask and mouth tape. Yet you’re still dragging yourself out of bed each Monday morning tired and unmotivated.</p>



<p><strong><mark>We’re optimizing for less suffering instead of more meaning.</mark></strong> We’ve confused comfort with fulfillment. And we’re getting really, really good at it. Millennials are the first generation in history to expect our jobs to provide a higher meaning beyond survival. That’s a good thing. It means that the essentials of life are nearly universally available now.</p>



<p><strong>But, as I write in my book <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em>:</strong></p>



<p><em>“</em><strong><em>The last two hundred years of progress pulled most of the world’s population over the poverty line. The next hundred years is about lifting everyone above the abundance line… <mark>Positive Politics seeks to democratize this abundance.</mark></em></strong><em>“</em></p>



<p><strong>Those of us who have already achieved abundance in our own lives now have two responsibilities:</strong></p>



<ol>
<li>Spread that abundance to as many other people as possible.</li>



<li>Find something more meaningful to do than chase more stuff.</li>
</ol>



<hr>



<p><em><mark>“</mark></em><strong><mark><em>The existential vacuum is a widespread phenomenon of the twentieth century</em></mark></strong><em><mark>“</mark></em></p>



<p><em>― Viktor Frankl, Man’s Search for Meaning</em></p>



<p><strong>When I was a kid, I knew exactly what I wanted to do&nbsp;— <a href="https://neilthanedar.com/the-most-important-job-in-the-world/" target="_blank" rel="noreferrer noopener">the most important job in the world</a>.</strong> And I wasn’t afraid to tell you either. At five years old, I would talk your ear off about training to be goalie for the St. Louis Blues. By seven, it was astronaut for NASA. By eleven, it was President of the United States. Then middle school hit, I got made fun of more than a few times, and that voice went silent.</p>



<p>After three startups, three nonprofits, and especially three kids knocked the imposter syndrome out of me, I spent a lot of time training my inner voice to get loud again. And what I heard reinforced what I knew all along —&nbsp;that my highest purpose is way above where I commonly take myself now.</p>



<p><strong>Imposter syndrome can be a good thing.</strong> That external voice saying “this is not you” may actually be telling you the truth. I got into the testing lab industry to save our family business. Fifteen years and three startups later, I had become “the lab expert” to the world. But I cringed at that label. First, there was no room to grow. I had already done it. I didn’t want to be eighty and still running labs. Second, and most importantly, I knew that my skills could be used for much more than money.</p>



<p>I’d love to say I transformed overnight, but really it took 5+ years from 2020 to 2025 for me to fully embody my new identity. You can see it in my writing, which became much more ambitious in 2020, when I relaunched this site and started <a href="https://neilthanedar.com/ideas/" target="_blank" rel="noreferrer noopener">blogging</a> consistently. That led to my <em><a href="https://worldsbiggestproblems.com/" target="_blank" rel="noreferrer noopener">World’s Biggest Problems</a></em> project, which convinced me that <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em> is the #1 solution we need now!</p>



<p><strong><mark>There are two key components to my highest mission now:</mark></strong></p>



<ol>
<li>Help people find their highest purpose.</li>



<li>Be a model for <a href="https://share.evernote.com/note/ab3c0eec-663b-716e-6fa9-35efc848b3ee">the pursuit of greatness</a>.</li>
</ol>



<p><strong>That means consistently chasing my highest purpose —&nbsp;helping ambitious optimists get into politics!</strong> After nearly a decade of doing this behind the scenes as a political volunteer and advisor, 2025 was the first year where I went full-time in politics. Leading <a href="https://mcfn.org/" target="_blank" rel="noreferrer noopener">MCFN</a> and publishing <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em> at the same time was a ton of work. But <strong>nothing energizes me more than fighting two of the biggest battles in the world now&nbsp;—&nbsp;anticorruption and Positive Politics!</strong></p>



<p><strong>I love politics because it’s full of meta solutions —&nbsp;solutions that create more solutions.</strong> My Positive Politics Accelerator is a classic example —&nbsp;recruiting and training more ambitious optimists into politics will lead to them making positive political change at all levels of government. But I’ve also tackled challenges like independent testing with startups and led a nonprofit to drive investigative journalism.</p>



<p>There are so many paths to positive impact, including politics, startups, nonprofits, medicine, law, education, science, engineering, journalism, art, faith, parenting, mentorship, and more! Choose the path that both best fits you now and is pointed towards your long-term highest purpose.</p>



<hr>



<p><strong><mark>I woke up today so excited to get to work thinking it was Monday morning already.</mark></strong> Instead of jumping right into it, I spent all morning making breakfast and playing with my kids, then wrote this post. When I’m writing about something personal, 1,000+ words can easily flow for me in an afternoon. This part will be done just in time to go to a nerf battle birthday party with my boys and their friends.</p>



<p><strong>Both the hustle and anti-hustle cultures get it wrong.</strong> Working long hours isn’t inherently good or bad. If I really had to count how much I’m “on” vs. doing whatever I want, it’s easy 100+ hours per week. But that includes everything from investigative journalism and operations work for MCFN, social media and speaking events for <em>Positive Politics</em>, reading and writing for my site, and 40+ hours every week with my kids.</p>



<p><strong><mark>I want to help more ambitious optimists chase your highest potential!</mark></strong> Whether the best solution is in startups, politics, nonprofits, science, crypto, or some new technology that’s yet to be invented, I’m happy to point you where I think you’ll be most powerful. I’ve thought, written, and worked on many of these ideas in my 15+ year career.</p>



<p>Now with 10+ years of writing for <a href="https://neilthanedar.com/ideas" target="_blank" rel="noreferrer noopener">my site</a> and my <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em> and <em><a href="https://worldsbiggestproblems.com/" target="_blank" rel="noreferrer noopener">World’s Biggest Problems</a></em> projects, I’ve focused on publicly inspiring more people to take on these challenges too. We should be flexible on how we solve the problems but firm in our resolve to consistently organize people and launch solutions.</p>



<p>As Steve Jobs said, “Life can be much broader once you discover one simple fact, and that is <strong><mark>everything around you that you call ‘life’ was made up by people that were no smarter than you… You can change it, you can mold it</mark></strong>… the most important thing…is to shake off this erroneous notion that life is there and you’re just going to live in it, versus embrace it, change it, improve it, make your mark upon it… <strong>Once you learn that, you’ll never be the same again.</strong>”</p>



<p><strong>Remember how it felt as a young child to openly tell the world about your dream job?</strong> Find the work that makes you feel this way and jump on whatever rung of that career ladder you can start now. The pay may be a little lower, but the existential payoff will be exponentially higher for the rest of your life.</p>



<p><strong><mark>You don’t have to go all-in right away!</mark></strong> In fact, after a long diet of low existential work, it’s probably best to ease into public work. You can even volunteer one hour or less per week for a political campaign or nonprofit to get started. Pick the smallest first step, and do it. Not in January, now. Do it before the end of the year. And see how different you feel when 2026 starts!</p>



<p><strong>And you don’t have to choose politics like me!</strong> Do you have the next great ambitious optimistic science fiction novel in your head? That book could spark movies and movements that positively change millions of lives! Choose the path will inspire and energize you for decades!</p>



<p><strong><mark>What matters most is you go straight towards your highest potential right now.</mark></strong> Pause once a month to make sure you’re still on the right track. Stop once a year to triple-check you’re on the right track. But never get off this path towards your highest potential. Anything else will starve you existentially.</p>



<p><strong><mark>When you truly chase your highest potential, everything you thought was burnout will melt away.</mark></strong> Because you weren’t suffering from too much work, you were suffering from too little truly important work. Like a boy who thought he was full until dessert arrives, you’ll suddenly find your hunger return!</p>



<hr>



<p><strong><em><mark>If you’re sick of politics as usual and ready to change the system, join Positive Politics!</mark></em></strong></p>



<ul>
<li><strong><em>Buy the book: <a href="https://positivepoliticsbook.com/"><span>positivepoliticsbook.com</span></a></em></strong></li>



<li><strong><em>Join the accelerator: <a href="https://positivepolitics.org/apply"><span>positivepolitics.org/apply</span></a></em></strong></li>
</ul>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Logging Sucks (543 pts)]]></title>
            <link>https://loggingsucks.com/</link>
            <guid>46346796</guid>
            <pubDate>Sun, 21 Dec 2025 18:09:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://loggingsucks.com/">https://loggingsucks.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46346796">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
        <div>
          
          <p>And here's how to make it better.</p>
          
        </div>
        <div>
              <p><span>2024-12-20T03:14:22.847Z</span> <span>INFO</span> <span>HttpServer started successfully binding=0.0.0.0:3000 pid=28471 env=production version=2.4.1 node_env=production cluster_mode=enabled workers=4</span></p>
              <p><span>2024-12-20T03:14:22.912Z</span> <span>debug</span> <span>PostgreSQL connection pool initialized host=db.internal:5432 database=main pool_size=20 ssl_mode=require idle_timeout=10000ms max_lifetime=1800000ms</span></p>
              <p><span>2024-12-20T03:14:23.156Z</span> <span>INFO</span> <span>Incoming request method=GET path=/api/v1/users/me ip=192.168.1.42 user_agent="Mozilla/5.0" request_id=req_8f7a2b3c trace_id=abc123def456</span></p>
              <p><span>2024-12-20T03:14:23.201Z</span> <span>debug</span> <span>JWT token validation started issuer=auth.company.com audience=api.company.com exp=1703044800 iat=1703041200 sub=user_abc123 scope="read write"</span></p>
              <p><span>2024-12-20T03:14:23.445Z</span> <span>WARN</span> <span>Slow database query detected duration_ms=847 query="SELECT u.*, o.name FROM users u JOIN orgs o ON u.org_id = o.id WHERE u.org_id = $1 AND u.deleted_at IS NULL" rows_returned=2847</span></p>
              <p><span>2024-12-20T03:14:23.892Z</span> <span>debug</span> <span>Redis cache lookup failed key=users:org_12345:list:v2 ttl_seconds=3600 fallback_strategy=database cache_cluster=redis-prod-01 latency_ms=2</span></p>
              <p><span>2024-12-20T03:14:24.156Z</span> <span>info</span> <span>Request completed successfully status=200 duration_ms=1247 bytes_sent=48291 request_id=req_8f7a2b3c cache_hit=false db_queries=3 external_calls=1</span></p>
              <p><span>2024-12-20T03:14:24.312Z</span> <span>ERROR</span> <span>Database connection pool exhausted active_connections=20 waiting_requests=147 timeout_ms=30000 service=postgres suggestion="Consider increasing pool_size or optimizing queries"</span></p>
              <p><span>2024-12-20T03:14:24.445Z</span> <span>warn</span> <span>Retrying failed HTTP request attempt=1 max_attempts=3 backoff_ms=100 error_code=ETIMEDOUT target_service=payment-gateway endpoint=/v1/charges circuit_state=closed</span></p>
              <p><span>2024-12-20T03:14:25.112Z</span> <span>INFO</span> <span>Circuit breaker state transition service=payment-api previous_state=closed current_state=open failure_count=5 failure_threshold=5 reset_timeout_ms=30000</span></p>
              <p><span>2024-12-20T03:14:25.445Z</span> <span>debug</span> <span>Background job executed successfully job_id=job_9x8w7v6u type=weekly_email_digest duration_ms=2341 emails_sent=1847 failures=3 queue=default priority=low</span></p>
              <p><span>2024-12-20T03:14:26.201Z</span> <span>ERROR</span> <span>Memory pressure critical heap_used_bytes=1932735283 heap_limit_bytes=2147483648 gc_pause_ms=847 gc_type=major rss_bytes=2415919104 external_bytes=8847291</span></p>
              <p><span>2024-12-20T03:14:26.556Z</span> <span>WARN</span> <span>Rate limit threshold approaching user_id=user_abc123 current_requests=890 limit=1000 window_seconds=60 remaining=110 reset_at=2024-12-20T03:15:00Z</span></p>
              <p><span>2024-12-20T03:14:27.112Z</span> <span>info</span> <span>WebSocket connection established client_id=ws_7f8g9h2j protocol=wss rooms=["team_updates","notifications","presence"] user_id=user_abc123 ip=192.168.1.42</span></p>
              <p><span>2024-12-20T03:14:27.445Z</span> <span>debug</span> <span>Kafka message consumed successfully topic=user-events partition=3 offset=1847291 key=user_abc123 consumer_group=api-consumers lag=12 processing_time_ms=45</span></p>
              <p><span>2024-12-20T03:14:28.112Z</span> <span>INFO</span> <span>Health check passed service=api-gateway uptime_seconds=847291 active_connections=142 memory_usage_percent=73 cpu_usage_percent=45 status=healthy version=2.4.1</span></p>
              <p><span>2024-12-20T03:14:28.556Z</span> <span>debug</span> <span>S3 upload completed bucket=company-uploads key=avatars/user_abc123/profile.jpg size_bytes=245891 content_type=image/jpeg duration_ms=892 region=us-east-1</span></p>
              <p><span>2024-12-20T03:14:29.112Z</span> <span>warn</span> <span>Deprecated API version detected endpoint=/api/v1/legacy/users version=v1 recommended_version=v3 deprecation_date=2025-01-15 client_id=mobile-app-ios</span></p>
              <!-- Duplicate for seamless loop -->
              <p><span>2024-12-20T03:14:22.847Z</span> <span>INFO</span> <span>HttpServer started successfully binding=0.0.0.0:3000 pid=28471 env=production version=2.4.1 node_env=production cluster_mode=enabled workers=4</span></p>
              <p><span>2024-12-20T03:14:22.912Z</span> <span>debug</span> <span>PostgreSQL connection pool initialized host=db.internal:5432 database=main pool_size=20 ssl_mode=require idle_timeout=10000ms max_lifetime=1800000ms</span></p>
              <p><span>2024-12-20T03:14:23.156Z</span> <span>INFO</span> <span>Incoming request method=GET path=/api/v1/users/me ip=192.168.1.42 user_agent="Mozilla/5.0" request_id=req_8f7a2b3c trace_id=abc123def456</span></p>
              <p><span>2024-12-20T03:14:23.201Z</span> <span>debug</span> <span>JWT token validation started issuer=auth.company.com audience=api.company.com exp=1703044800 iat=1703041200 sub=user_abc123 scope="read write"</span></p>
              <p><span>2024-12-20T03:14:23.445Z</span> <span>WARN</span> <span>Slow database query detected duration_ms=847 query="SELECT u.*, o.name FROM users u JOIN orgs o ON u.org_id = o.id WHERE u.org_id = $1 AND u.deleted_at IS NULL" rows_returned=2847</span></p>
              <p><span>2024-12-20T03:14:23.892Z</span> <span>debug</span> <span>Redis cache lookup failed key=users:org_12345:list:v2 ttl_seconds=3600 fallback_strategy=database cache_cluster=redis-prod-01 latency_ms=2</span></p>
              <p><span>2024-12-20T03:14:24.156Z</span> <span>info</span> <span>Request completed successfully status=200 duration_ms=1247 bytes_sent=48291 request_id=req_8f7a2b3c cache_hit=false db_queries=3 external_calls=1</span></p>
              <p><span>2024-12-20T03:14:24.312Z</span> <span>ERROR</span> <span>Database connection pool exhausted active_connections=20 waiting_requests=147 timeout_ms=30000 service=postgres suggestion="Consider increasing pool_size or optimizing queries"</span></p>
              <p><span>2024-12-20T03:14:24.445Z</span> <span>warn</span> <span>Retrying failed HTTP request attempt=1 max_attempts=3 backoff_ms=100 error_code=ETIMEDOUT target_service=payment-gateway endpoint=/v1/charges circuit_state=closed</span></p>
              <p><span>2024-12-20T03:14:25.112Z</span> <span>INFO</span> <span>Circuit breaker state transition service=payment-api previous_state=closed current_state=open failure_count=5 failure_threshold=5 reset_timeout_ms=30000</span></p>
              <p><span>2024-12-20T03:14:25.445Z</span> <span>debug</span> <span>Background job executed successfully job_id=job_9x8w7v6u type=weekly_email_digest duration_ms=2341 emails_sent=1847 failures=3 queue=default priority=low</span></p>
              <p><span>2024-12-20T03:14:26.201Z</span> <span>ERROR</span> <span>Memory pressure critical heap_used_bytes=1932735283 heap_limit_bytes=2147483648 gc_pause_ms=847 gc_type=major rss_bytes=2415919104 external_bytes=8847291</span></p>
              <p><span>2024-12-20T03:14:26.556Z</span> <span>WARN</span> <span>Rate limit threshold approaching user_id=user_abc123 current_requests=890 limit=1000 window_seconds=60 remaining=110 reset_at=2024-12-20T03:15:00Z</span></p>
              <p><span>2024-12-20T03:14:27.112Z</span> <span>info</span> <span>WebSocket connection established client_id=ws_7f8g9h2j protocol=wss rooms=["team_updates","notifications","presence"] user_id=user_abc123 ip=192.168.1.42</span></p>
              <p><span>2024-12-20T03:14:27.445Z</span> <span>debug</span> <span>Kafka message consumed successfully topic=user-events partition=3 offset=1847291 key=user_abc123 consumer_group=api-consumers lag=12 processing_time_ms=45</span></p>
              <p><span>2024-12-20T03:14:28.112Z</span> <span>INFO</span> <span>Health check passed service=api-gateway uptime_seconds=847291 active_connections=142 memory_usage_percent=73 cpu_usage_percent=45 status=healthy version=2.4.1</span></p>
              <p><span>2024-12-20T03:14:28.556Z</span> <span>debug</span> <span>S3 upload completed bucket=company-uploads key=avatars/user_abc123/profile.jpg size_bytes=245891 content_type=image/jpeg duration_ms=892 region=us-east-1</span></p>
              <p><span>2024-12-20T03:14:29.112Z</span> <span>warn</span> <span>Deprecated API version detected endpoint=/api/v1/legacy/users version=v1 recommended_version=v3 deprecation_date=2025-01-15 client_id=mobile-app-ios</span></p>
            </div>
      </div>

    <div>
        <p>Your logs are lying to you. Not maliciously. They're just not equipped to tell the truth.</p>
<p>You've probably spent hours grep-ing through logs trying to understand why a user couldn't check out, why that webhook failed, or why your p99 latency spiked at 3am. You found nothing useful. Just timestamps and vague messages that mock you with their uselessness.</p>
<p>This isn't your fault. <strong>Logging, as it's commonly practiced, is fundamentally broken.</strong> And no, slapping OpenTelemetry on your codebase won't magically fix it.</p>
<p>Let me show you what's wrong, and more importantly, how to fix it.</p>

<h2>The Core Problem</h2>
<p>Logs were designed for a different era. An era of monoliths, single servers, and problems you could reproduce locally. Today, a single user request might touch 15 services, 3 databases, 2 caches, and a message queue. Your logs are still acting like it's 2005.</p>
<p>Here's what a typical logging setup looks like:</p>
<div id="log-chaos-simulator">
    <p>The Log Chaos Simulator</p>
    <p>Loading interactive demo...</p>
  </div>
<p>That's 13 log lines for a single successful request. Now multiply that by 10,000 concurrent users. You've got 130,000 log lines per second. Most of them saying absolutely nothing useful.</p>
<p>But here's the real problem: when something goes wrong, these logs won't help you. They're missing the one thing you need: <strong>context</strong>.</p>

<h2>Why String Search is Broken</h2>
<p>When a user reports "I can't complete my purchase," your first instinct is to search your logs. You type their email, or maybe their user ID, and hit enter.</p>
<div id="futile-search">
    <p>The Futile Search</p>
    <p>Loading interactive demo...</p>
  </div>
<p>String search treats logs as bags of characters. It has no understanding of structure, no concept of relationships, no way to correlate events across services.</p>
<ul>When you search for "user-123", you might find it logged 47 different ways across your codebase:
<li><code>user-123</code></li>
<li><code>user_id=user-123</code></li>
<li><code>{"userId": "user-123"}</code></li>
<li><code>[USER:user-123]</code></li>
<li><code>processing user: user-123</code></li></ul>
<p>And those are just the logs that <em>include</em> the user ID. What about the downstream service that only logged the order ID? Now you need a second search. And a third. You're playing detective with one hand tied behind your back.</p>
<blockquote>The fundamental problem: logs are optimized for <em>writing</em>, not for <em>querying</em>.</blockquote>
<p>Developers write <code>console.log("Payment failed")</code> because it's easy in the moment. Nobody thinks about the poor soul who'll be searching for this at 2am during an outage.</p>

<h2>Let's Define Some Terms</h2>
<p>Before I show you the fix, let me define some terms. These get thrown around a lot, often incorrectly.</p>
<p><strong>Structured Logging</strong>: Logs emitted as key-value pairs (usually JSON) instead of plain strings. <code>{"event": "payment_failed", "user_id": "123"}</code> instead of <code>"Payment failed for user 123"</code>. Structured logging is necessary but not sufficient.</p>
<p><strong>Cardinality</strong>: The number of unique values a field can have. <code>user_id</code> has high cardinality (millions of unique values). <code>http_method</code> has low cardinality (GET, POST, PUT, DELETE, etc.). High cardinality fields are what make logs actually useful for debugging.</p>
<p><strong>Dimensionality</strong>: The number of fields in your log event. A log with 5 fields has low dimensionality. A log with 50 fields has high dimensionality. More dimensions = more questions you can answer.</p>
<p><strong>Wide Event</strong>: A single, context-rich log event emitted per request per service. Instead of 13 log lines for one request, you emit 1 line with 50+ fields containing everything you might need to debug.</p>
<p><strong>Canonical Log Line</strong>: Another term for wide event, popularized by Stripe. One log line per request that serves as the authoritative record of what happened.</p>
<div id="cardinality-explorer">
    <p>Cardinality Explorer</p>
    <p>Loading interactive demo...</p>
  </div>

<h2>OpenTelemetry Won't Save You</h2>
<p>I see this take constantly: "Just use OpenTelemetry and your observability problems are solved."</p>
<p>No. OpenTelemetry is a <strong>protocol and a set of SDKs</strong>. It standardizes how telemetry data (logs, traces, metrics) is collected and exported. This is genuinely useful: it means you're not locked into a specific vendor's format.</p>
<p>But here's what OpenTelemetry does NOT do:</p>
<p>1. <strong>It doesn't decide what to log.</strong> You still have to instrument your code deliberately.<br>2. <strong>It doesn't add business context.</strong> If you don't add the user's subscription tier, their cart value, or the feature flags enabled, OTel won't magically know.<br>3. <strong>It doesn't fix your mental model.</strong> If you're still thinking in terms of "log statements," you'll just emit bad telemetry in a standardized format.</p>
<div id="otel-reality-check">
    <p>The OTel Reality Check</p>
    <p>Loading interactive demo...</p>
  </div>
<p>OpenTelemetry is a delivery mechanism. It doesn't know that <code>user-789</code> is a premium customer who's been with you for 3 years and just tried to spend $160. <strong>You</strong> have to tell it.</p>

<h2>The Fix: Wide Events / Canonical Log Lines</h2>
<p>Here's the mental model shift that changes everything:</p>
<blockquote>Instead of logging <em>what your code is doing</em>, log <em>what happened to this request</em>.</blockquote>
<p>Stop thinking about logs as a debugging diary. Start thinking about them as a structured record of business events.</p>
<p>For each request, emit <strong>one wide event</strong> per service hop. This event should contain every piece of context that might be useful for debugging. Not just what went wrong, but the full picture of the request.</p>
<div id="wide-event-builder">
    <p>Build a Wide Event</p>
    <p>Loading interactive demo...</p>
  </div>
<p>Here's what a wide event looks like in practice:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "timestamp"</span><span>: </span><span>"2025-01-15T10:23:45.612Z"</span><span>,</span></span>
<span><span>  "request_id"</span><span>: </span><span>"req_8bf7ec2d"</span><span>,</span></span>
<span><span>  "trace_id"</span><span>: </span><span>"abc123"</span><span>,</span></span>
<span></span>
<span><span>  "service"</span><span>: </span><span>"checkout-service"</span><span>,</span></span>
<span><span>  "version"</span><span>: </span><span>"2.4.1"</span><span>,</span></span>
<span><span>  "deployment_id"</span><span>: </span><span>"deploy_789"</span><span>,</span></span>
<span><span>  "region"</span><span>: </span><span>"us-east-1"</span><span>,</span></span>
<span></span>
<span><span>  "method"</span><span>: </span><span>"POST"</span><span>,</span></span>
<span><span>  "path"</span><span>: </span><span>"/api/checkout"</span><span>,</span></span>
<span><span>  "status_code"</span><span>: </span><span>500</span><span>,</span></span>
<span><span>  "duration_ms"</span><span>: </span><span>1247</span><span>,</span></span>
<span></span>
<span><span>  "user"</span><span>: {</span></span>
<span><span>    "id"</span><span>: </span><span>"user_456"</span><span>,</span></span>
<span><span>    "subscription"</span><span>: </span><span>"premium"</span><span>,</span></span>
<span><span>    "account_age_days"</span><span>: </span><span>847</span><span>,</span></span>
<span><span>    "lifetime_value_cents"</span><span>: </span><span>284700</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "cart"</span><span>: {</span></span>
<span><span>    "id"</span><span>: </span><span>"cart_xyz"</span><span>,</span></span>
<span><span>    "item_count"</span><span>: </span><span>3</span><span>,</span></span>
<span><span>    "total_cents"</span><span>: </span><span>15999</span><span>,</span></span>
<span><span>    "coupon_applied"</span><span>: </span><span>"SAVE20"</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "payment"</span><span>: {</span></span>
<span><span>    "method"</span><span>: </span><span>"card"</span><span>,</span></span>
<span><span>    "provider"</span><span>: </span><span>"stripe"</span><span>,</span></span>
<span><span>    "latency_ms"</span><span>: </span><span>1089</span><span>,</span></span>
<span><span>    "attempt"</span><span>: </span><span>3</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "error"</span><span>: {</span></span>
<span><span>    "type"</span><span>: </span><span>"PaymentError"</span><span>,</span></span>
<span><span>    "code"</span><span>: </span><span>"card_declined"</span><span>,</span></span>
<span><span>    "message"</span><span>: </span><span>"Card declined by issuer"</span><span>,</span></span>
<span><span>    "retriable"</span><span>: </span><span>false</span><span>,</span></span>
<span><span>    "stripe_decline_code"</span><span>: </span><span>"insufficient_funds"</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "feature_flags"</span><span>: {</span></span>
<span><span>    "new_checkout_flow"</span><span>: </span><span>true</span><span>,</span></span>
<span><span>    "express_payment"</span><span>: </span><span>false</span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></pre>
<ul>One event. Everything you need. When this user complains, you search for <code>user_id = "user_456"</code> and you instantly know:
<li>They're a premium customer (high priority)</li>
<li>They've been with you for over 2 years (very high priority)</li>
<li>The payment failed on the 3rd attempt</li>
<li>The actual reason: insufficient funds</li>
<li>They were using the new checkout flow (potential correlation?)</li></ul>
<p>No grep-ing. No guessing. No second search.</p>

<h2>The Queries You Can Now Run</h2>
<p>With wide events, you're not searching text anymore. You're querying structured data. The difference is night and day.</p>
<div id="query-playground">
    <p>Query Playground</p>
    <p>Loading interactive demo...</p>
  </div>
<p>This is the superpower of wide events combined with high-cardinality, high-dimensionality data. You're not searching logs anymore. You're running analytics on your production traffic.</p>

<h2>Implementing Wide Events</h2>
<p>Here's a practical implementation pattern. The key insight: build the event throughout the request lifecycle, then emit once at the end.</p>
<pre tabindex="0"><code><span><span>// middleware/wideEvent.ts</span></span>
<span><span>export</span><span> function</span><span> wideEventMiddleware</span><span>() {</span></span>
<span><span>  return</span><span> async</span><span> (</span><span>ctx</span><span>, </span><span>next</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>    const</span><span> startTime</span><span> =</span><span> Date.</span><span>now</span><span>();</span></span>
<span></span>
<span><span>    // Initialize the wide event with request context</span></span>
<span><span>    const</span><span> event</span><span>:</span><span> Record</span><span>&lt;</span><span>string</span><span>, </span><span>unknown</span><span>&gt; </span><span>=</span><span> {</span></span>
<span><span>      request_id: ctx.</span><span>get</span><span>(</span><span>'requestId'</span><span>),</span></span>
<span><span>      timestamp: </span><span>new</span><span> Date</span><span>().</span><span>toISOString</span><span>(),</span></span>
<span><span>      method: ctx.req.method,</span></span>
<span><span>      path: ctx.req.path,</span></span>
<span><span>      service: process.env.</span><span>SERVICE_NAME</span><span>,</span></span>
<span><span>      version: process.env.</span><span>SERVICE_VERSION</span><span>,</span></span>
<span><span>      deployment_id: process.env.</span><span>DEPLOYMENT_ID</span><span>,</span></span>
<span><span>      region: process.env.</span><span>REGION</span><span>,</span></span>
<span><span>    };</span></span>
<span></span>
<span><span>    // Make the event accessible to handlers</span></span>
<span><span>    ctx.</span><span>set</span><span>(</span><span>'wideEvent'</span><span>, event);</span></span>
<span></span>
<span><span>    try</span><span> {</span></span>
<span><span>      await</span><span> next</span><span>();</span></span>
<span><span>      event.status_code </span><span>=</span><span> ctx.res.status;</span></span>
<span><span>      event.outcome </span><span>=</span><span> 'success'</span><span>;</span></span>
<span><span>    } </span><span>catch</span><span> (error) {</span></span>
<span><span>      event.status_code </span><span>=</span><span> 500</span><span>;</span></span>
<span><span>      event.outcome </span><span>=</span><span> 'error'</span><span>;</span></span>
<span><span>      event.error </span><span>=</span><span> {</span></span>
<span><span>        type: error.name,</span></span>
<span><span>        message: error.message,</span></span>
<span><span>        code: error.code,</span></span>
<span><span>        retriable: error.retriable </span><span>??</span><span> false</span><span>,</span></span>
<span><span>      };</span></span>
<span><span>      throw</span><span> error;</span></span>
<span><span>    } </span><span>finally</span><span> {</span></span>
<span><span>      event.duration_ms </span><span>=</span><span> Date.</span><span>now</span><span>() </span><span>-</span><span> startTime;</span></span>
<span></span>
<span><span>      // Emit the wide event</span></span>
<span><span>      logger.</span><span>info</span><span>(event);</span></span>
<span><span>    }</span></span>
<span><span>  };</span></span>
<span><span>}</span></span></code></pre>
<p>Then in your handlers, you enrich the event with business context:</p>
<pre tabindex="0"><code><span><span>app.</span><span>post</span><span>(</span><span>'/checkout'</span><span>, </span><span>async</span><span> (</span><span>ctx</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>  const</span><span> event</span><span> =</span><span> ctx.</span><span>get</span><span>(</span><span>'wideEvent'</span><span>);</span></span>
<span><span>  const</span><span> user</span><span> =</span><span> ctx.</span><span>get</span><span>(</span><span>'user'</span><span>);</span></span>
<span></span>
<span><span>  // Add user context</span></span>
<span><span>  event.user </span><span>=</span><span> {</span></span>
<span><span>    id: user.id,</span></span>
<span><span>    subscription: user.subscription,</span></span>
<span><span>    account_age_days: </span><span>daysSince</span><span>(user.createdAt),</span></span>
<span><span>    lifetime_value_cents: user.ltv,</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  // Add business context as you process</span></span>
<span><span>  const</span><span> cart</span><span> =</span><span> await</span><span> getCart</span><span>(user.id);</span></span>
<span><span>  event.cart </span><span>=</span><span> {</span></span>
<span><span>    id: cart.id,</span></span>
<span><span>    item_count: cart.items.</span><span>length</span><span>,</span></span>
<span><span>    total_cents: cart.total,</span></span>
<span><span>    coupon_applied: cart.coupon?.code,</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  // Process payment</span></span>
<span><span>  const</span><span> paymentStart</span><span> =</span><span> Date.</span><span>now</span><span>();</span></span>
<span><span>  const</span><span> payment</span><span> =</span><span> await</span><span> processPayment</span><span>(cart, user);</span></span>
<span></span>
<span><span>  event.payment </span><span>=</span><span> {</span></span>
<span><span>    method: payment.method,</span></span>
<span><span>    provider: payment.provider,</span></span>
<span><span>    latency_ms: Date.</span><span>now</span><span>() </span><span>-</span><span> paymentStart,</span></span>
<span><span>    attempt: payment.attemptNumber,</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  // If payment fails, add error details</span></span>
<span><span>  if</span><span> (payment.error) {</span></span>
<span><span>    event.error </span><span>=</span><span> {</span></span>
<span><span>      type: </span><span>'PaymentError'</span><span>,</span></span>
<span><span>      code: payment.error.code,</span></span>
<span><span>      stripe_decline_code: payment.error.declineCode,</span></span>
<span><span>    };</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  return</span><span> ctx.</span><span>json</span><span>({ orderId: payment.orderId });</span></span>
<span><span>});</span></span></code></pre>
<div id="event-lifecycle">
    <p>Wide Event Builder Simulator</p>
    <p>Loading interactive demo...</p>
  </div>

<h2>Sampling: Keeping Costs Under Control</h2>
<p>"But Boris," I hear you saying, "if I log 50 fields per request at 10,000 requests per second, my observability bill will bankrupt me."</p>
<p>Valid concern. This is where <strong>sampling</strong> comes in.</p>
<p><strong>Sampling</strong> means keeping only a percentage of your events. Instead of storing 100% of traffic, you might store 10% or 1%. At scale, this is the only way to stay sane (and solvent).</p>
<p>But naive sampling is dangerous. If you randomly sample 1% of traffic, you might accidentally drop the one request that explains your outage.</p>
<div id="sampling-trap">
    <p>The Sampling Trap</p>
    <p>Loading interactive demo...</p>
  </div>
<h3>Tail Sampling</h3>
<p><strong>Tail sampling</strong> means you make the sampling decision <em>after</em> the request completes, based on its outcome.</p>
<p>The rules are simple:<br>1. <strong>Always keep errors.</strong> 100% of 500s, exceptions, and failures get stored.<br>2. <strong>Always keep slow requests.</strong> Anything above your p99 latency threshold.<br>3. <strong>Always keep specific users.</strong> VIP customers, internal testing accounts, flagged sessions.<br>4. <strong>Randomly sample the rest.</strong> Happy, fast requests? Keep 1-5%.</p>
<p>This gives you the best of both worlds: manageable costs, but you never lose the events that matter.</p>
<pre tabindex="0"><code><span><span>// Tail sampling decision function</span></span>
<span><span>function</span><span> shouldSample</span><span>(</span><span>event</span><span>:</span><span> WideEvent</span><span>)</span><span>:</span><span> boolean</span><span> {</span></span>
<span><span>  // Always keep errors</span></span>
<span><span>  if</span><span> (event.status_code </span><span>&gt;=</span><span> 500</span><span>) </span><span>return</span><span> true</span><span>;</span></span>
<span><span>  if</span><span> (event.error) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Always keep slow requests (above p99)</span></span>
<span><span>  if</span><span> (event.duration_ms </span><span>&gt;</span><span> 2000</span><span>) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Always keep VIP users</span></span>
<span><span>  if</span><span> (event.user?.subscription </span><span>===</span><span> 'enterprise'</span><span>) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Always keep requests with specific feature flags (debugging rollouts)</span></span>
<span><span>  if</span><span> (event.feature_flags?.new_checkout_flow) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Random sample the rest at 5%</span></span>
<span><span>  return</span><span> Math.</span><span>random</span><span>() </span><span>&lt;</span><span> 0.05</span><span>;</span></span>
<span><span>}</span></span></code></pre>

<h2>Misconceptions</h2>
<h3>"Structured logging is the same as wide events"</h3>
<p>No. Structured logging means your logs are JSON instead of strings. That's table stakes. Wide events are a <em>philosophy</em>: one comprehensive event per request, with all context attached. You can have structured logs that are still useless (5 fields, no user context, scattered across 20 log lines).</p>
<h3>"We already use OpenTelemetry, so we're good"</h3>
<p>You're using a delivery mechanism. OpenTelemetry doesn't decide what to capture. You do. Most OTel implementations I've seen capture the bare minimum: span name, duration, status. That's not enough. You need to deliberately instrument with business context.</p>
<h3>"This is just tracing with extra steps"</h3>
<p>Tracing gives you request flow across services (which service called which). Wide events give you context <em>within</em> a service. They're complementary. <strong>Ideally, your wide events ARE your trace spans, enriched with all the context you need</strong>.</p>
<h3>"Logs are for debugging, metrics are for dashboards"</h3>
<p>This distinction is artificial and harmful. Wide events can power both. Query them for debugging. Aggregate them for dashboards. The data is the same, just different views.</p>
<h3>"High-cardinality data is expensive and slow"</h3>
<p>It's expensive on <em>legacy logging systems</em> built for low-cardinality string search. Modern columnar databases (ClickHouse, BigQuery, etc.) are specifically designed for high-cardinality, high-dimensionality data. The tooling has caught up. Your practices should too.</p>
<h2>The Payoff</h2>
<p>When you implement wide events properly, debugging transforms from archaeology to analytics.</p>
<p>Instead of: <em>"The user said checkout failed. Let me grep through 50 services and hope I find something."</em></p>
<p>You get: <em>"Show me all checkout failures for premium users in the last hour where the new checkout flow was enabled, grouped by error code."</em></p>
<p>One query. Sub-second results. Root cause identified.</p>
<p>Your logs stop lying to you. They start telling the truth. The whole truth.</p>

<p>Complete the form below to get a personalized report on your stack. I'll tell you what's working, what's not, and where you can save money. I genuinely want to hear about your logging nightmares :)</p>
      </div>

    <div id="signup">
        <p>Free in 30 seconds</p>
        <h2>Get your stack roasted.<br>Get a plan to fix it.</h2>
        <p>Answer a few questions and I'll send you a personalized report with:</p>
        <ul>
          <li>Where wide events would have the biggest impact on your stack</li>
          <li>What to log (and what to stop logging)</li>
          <li>Which tools are worth the cost (and which aren't)</li>
          <li>Quick wins you can ship this week</li>
        </ul>
        <p>Questions? Logging horror stories? Drop them in the comments below.</p>

        <div>
          
          
          <p>Your logs are just as empty.</p>
        </div>

        

        <div id="instant-analysis">
          <h3>Your stack has been judged.</h3>
          
          <p>Check your email for a detailed analysis.</p>
        </div>
      </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get an AI code review in 10 seconds (134 pts)]]></title>
            <link>https://oldmanrahul.com/2025/12/19/ai-code-review-trick/</link>
            <guid>46346391</guid>
            <pubDate>Sun, 21 Dec 2025 17:21:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oldmanrahul.com/2025/12/19/ai-code-review-trick/">https://oldmanrahul.com/2025/12/19/ai-code-review-trick/</a>, See on <a href="https://news.ycombinator.com/item?id=46346391">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><article><p>Here’s a trick I don’t see enough people using:</p><blockquote><p>Add <code>.diff</code> to the end of any PR URL and copy&amp;paste into a LLM</p></blockquote><p>You can get an instant feedback on any GitHub PR.</p><p><strong>No Copilot Enterprise. No browser extensions. No special tooling.</strong></p><p>That’s it.</p><h2 id="example">Example</h2><ol><li>PR Link: <code>https://github.com/RahulPrabha/oldmanrahul.com/pull/11</code></li><li>Add <code>.diff</code> to the end: <code>https://github.com/RahulPrabha/oldmanrahul.com/pull/11.diff</code></li><li>Copy the raw diff</li><li>Paste it into Claude, ChatGPT, or any LLM (Maybe add a short instuction like: <code>please review.</code>)</li></ol><h2 id="so-no-more-human-reviewers">So no more human reviewers?</h2><p>This isn’t a replacement for a real code review by a peer. But it’s a great way to get a first pass in short order.</p><p>Before you ping a teammate, run your PR through an LLM. You’ll catch obvious issues, get suggestions for edge cases you missed, and show up to the real review with cleaner code.</p><p>It’ll shorten your cycle times and be <a href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/">a courtesy to others.</a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autoland saves King Air, everyone reported safe (245 pts)]]></title>
            <link>https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/</link>
            <guid>46346214</guid>
            <pubDate>Sun, 21 Dec 2025 16:57:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/">https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/</a>, See on <a href="https://news.ycombinator.com/item?id=46346214">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-td-block-uid="tdi_84">
<p>Garmin has confirmed the first emergency use of its <a href="https://avbrief.com/more-garmin-autothrottle-safety-backstop-for-big-king-airs/">Autoland</a> system occurred on Saturday in Colorado. “Garmin can confirm that an emergency Autoland activation occurred at Rocky Mountain Metropolitan Airport in Broomfield, Colorado,” the company said in a statement Sunday. “The Autoland took place on Sat., Dec. 20, resulting in a successful landing. We look forward to sharing additional details at the appropriate time.” Social media posts from flight tracking hobbyists reported a King Air 200 squawked 7700 about 2 p.m. local time today. The Autoland system was initiated and landed the aircraft at Rocky Mountain Metropolitan Airport near Denver. A recording from <a href="https://www.liveatc.net/archive.php?" rel="nofollow noopener" target="_blank">LiveATC’s feed</a> of the airport’s tower frequency includes a robotic female voice declaring a pilot incapacitation and the intention to land on Runway 30. The tape is below and first mention of the incident by ATC is at about 5:00. The Autoland system announces its intentions at about 11:10. (The time stamps are approximate.) There is no word on the condition of the pilot but social media posts suggest all aboard were safe.</p>



<figure><audio controls="" src="https://avbrief.com/wp-content/uploads/2025/12/autoland.mp3"></audio></figure>



<p>The aircraft, <a href="https://www.flightaware.com/live/flight/N479BR/history/20251220/1910Z/KASE/KBJC" rel="nofollow noopener" target="_blank">N479BR</a>, was being operated by Buffalo River Outfitters from Aspen to Rocky Mountain Metropolitan. It’s not clear how many people were on board. The system appeared to work flawlessly, and the controller at Rocky Mountain Metropolitan seemed to take it in stride, accommodating as many requests as he could before shutting down the airport for the landing. We’ll have more detail on this as it becomes available. </p>



<p>Larry Anglisano recorded this <a href="https://avbrief.com/king-air-emergency-autoland-demo/" data-type="post" data-id="6834">video demonstration of the Autoland system</a> in the Beechcraft King Air.</p>



<p>A reader was at the airport Saturday and shared this video that he had posted to Instagram.</p>



<blockquote data-service="instagram" data-category="marketing" data-placeholder-image="https://avbrief.com/wp-content/plugins/complianz-gdpr-premium/assets/images/placeholders/instagram-minimal.jpg" data-instgrm-captioned="" data-instgrm-permalink="https://www.instagram.com/p/DSgDCLlEfbw/?utm_source=ig_embed&amp;utm_campaign=loading" data-instgrm-version="14"></blockquote>

</div><div data-td-block-uid="tdi_92"><p><a href="https://avbrief.com/author/russ/" title="Russ Niles"><img src="https://avbrief.com/wp-content/uploads/2025/08/russ-niles-150x150.jpg" width="96" height="96" srcset="https://avbrief.com/wp-content/uploads/2025/08/russ-niles-300x300.jpg 2x" alt="Russ Niles"></a></p><div><p><a href="https://avbrief.com/author/russ/">Russ Niles</a></p><p>Russ Niles is Editor-in-Chief of AvBrief.com. He has been a pilot for 30 years and an aviation journalist since 2003. He and his wife Marni live in southern British Columbia where they also operate a small winery.</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>