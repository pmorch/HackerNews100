<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 13 Aug 2024 03:30:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Spice: Fine-grained parallelism with sub-nanosecond overhead in Zig (161 pts)]]></title>
            <link>https://github.com/judofyr/spice</link>
            <guid>41230344</guid>
            <pubDate>Mon, 12 Aug 2024 23:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/judofyr/spice">https://github.com/judofyr/spice</a>, See on <a href="https://news.ycombinator.com/item?id=41230344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Spice: Parallelism with sub-nanosecond overhead</h2><a id="user-content-spice-parallelism-with-sub-nanosecond-overhead" aria-label="Permalink: Spice: Parallelism with sub-nanosecond overhead" href="#spice-parallelism-with-sub-nanosecond-overhead"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/spice-tree-sum-100M.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/spice-tree-sum-100M.svg" alt="Time to calculate sum of binary tree of 100M nodes with Spice"></a></p>
<p dir="auto"><strong>Spice</strong> uses <a href="https://www.andrew.cmu.edu/user/mrainey/heartbeat/heartbeat.html" rel="nofollow"><em>heartbeat scheduling</em></a> to accomplish extremely efficient parallelism in Zig:</p>
<ul dir="auto">
<li><strong>Sub-nanosecond overhead:</strong>
Turning your function into a parallelism-enabled function adds less than a nanosecond of overhead.</li>
<li><strong>Contention-free:</strong>
Threads will never compete (i.e. spin) over the same work.
Adding more threads to the system will not make your program any slower, but the extra threads might be completely idle since there's nothing useful to do.</li>
</ul>
<p dir="auto">The benchmark in the figure above (summing over the nodes in a binary tree) is typically one of the worst cases for parallelism frameworks:
The actual operation is extremely fast so any sort of overhead will have a measurable impact.</p>
<p dir="auto">Here's the exact same benchmark in <a href="https://docs.rs/rayon/latest/rayon/" rel="nofollow">Rayon</a>, an excellent library in Rust which uses work-stealing fork/join:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/rayon-tree-sum-100M.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/rayon-tree-sum-100M.svg" alt="Time to calculate sum of binary tree of 100M nodes with Rayon"></a></p>
<p dir="auto">The overhead here is roughly ~15 ns (from 7.48 ns to 22.99 ns) which means that at 4 threads we're "back" to the sequential performance - just using four times as much CPU.
Luckily we <em>are</em> able to get linear speed-up (in terms of threads) initially.
These benchmarks were ran on a <code>c4-standard-16</code> instance in Google Cloud with 16 cores.
Rayon itself shows a nice ~14x speed-up (from 22.99 ns to 1.64 ns) at 16 threads, but compared to the <em>baseline</em> this ends up only being ~4.5x due to the overhead.</p>
<p dir="auto">In comparison, Spice scales slightly worse:
It only got ~11x speed-up when going from 1 to 16 threads.
However, due its low overhead this is also essentially the speed-up compared to the baseline.</p>
<p dir="auto">(It's not entirely clear why the Zig baseline implementation is twice as fast as the Rust implementation.
The <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AAvPMFJL6yAngGVG6AMKpaAVxYMJAZlIOAMngMmABy7gBGmMQgAGykAA6oCoS2DM5uHt7xickCAUGhLBFRsZaY1ilCBEzEBGnunlw%2BpeUCldUEeSHhkTEWVTV1GY197Z0FRTEAlBaorsTI7BxoDAoEANTBGJhrAKReACJrq8Su1rsA7ABCOxoAgmsPawBuYiBreNHSN/eP9FQEbz2ADEAFSbLC7A5rBiuWi0UjfR5rYhmBAAyGg8HbPaHGFwhF3RGPOKuMJrKgMI7uCCWKhvEHLVYbLaTd6fC7XO5IpEvYjIzAKSGHWkAOhetD2nJ%2B3PeVDWNMwtCoIr%2BBFZO3OjmQCDo6A1jn5gp2ACZrlDtbqRQpqZNJUSZXg5QqlSKUcA0erNRbaHrNYbdqacWtvegrTa7VyZfyCHNKcQBRHpRd9t8NSmvFLviSyRS1iwmIEIKynqg8OgOfa1gB6KtrAAqbuAkTWBAQ22tLAAtFRXAwWpSiGsImtZgQSQRKwB9IXMrBhlgQXtYGhBdC2jOp877DjTWicACsvE83F4qE4AC0zEdZvNscavDxSACOFpJtMANYSACcIq40X3XiSMaQHGvuwEABySNIe4cJIR6aKenC8AoIAaE%2BCHTHAsBIGgLBxHQkTkJQuH4fQUTGAQJwMO%2BfB0AQkQoRAYQIaQYSBNUACenCPmxzDEBxADyYTaGUz6PrhbCCAJDC0FxL68FgYSuMAjhiLQKEnqQWD5kY4jyVpeDxuUTwCixmCqGUrj0dxvCBPRMFaPoeBhMQnHOFgLGUXgLA2aQJnEGEiSYPsmA6cAtCBKA8nTFQBjAAoABqeCYAA7gJcSML5/CCCIYjsFIMiCIoKjqPpujGvohgmGYTlhChkDTKgcQ2AIGmdgJawAEqKpgTBKECfUEKe/kolg9VFhYPX9vYDBOC49R6P4gRdIUPRcFkSQtak81DBtOQMGM3RROtzRbW0Aw7Q0k1WGd/QdMt4xrSMF3pFdqyjA9R0SNMCg3gs336Ae8H6WeHBrKYwAtlR77yrghAkAGD6TLwz6vtMbZMFgUQTZ%2BoEivu5zRBokhfsaX77vunxgYDsHA45oPIah6HRaQWGICgWxw0QZAUNQBHMGwWWyLl4gFdl8hKGoLHlZVRggCcqzXaJKQzXNr0gBVS35F9f57VtgyeBVCSbSkh2rcdJRTbd7QGxrSv9ud93a%2BbEglHdtsVe9NRmxMf4/X9izHKc6xYnsjh1ns2AVpGDziq47D1gSSaqm86VbWHlyoKoYeh144eRwXXjYEnSJumiqfNSkGdZznWxhxHReF8Xm4poSdzZuSlIdhADBbICxrRLnjgfJIkesp2kdspI0fJ5g6wsFZhqwusQa93OceYImSKOms9DrEIqBsBAIbqlCJrRGvmAqpg/wz1GS%2B0CvgZQt3J9b48aaVjve9rAfR8nzOc%2Bl9XSohXlcSspcBTLwDGaYU1I34bhjsmSs8YYzEDjFAx%2BiY0w7hpoeUgx56acE6q4Jkv05gLERsaFGGEPwa2NCKL84FjTgS8F4fcXAuDGg0NENh0FOBwQISxBmFgmao1wWzCAOFD6kUIrzEiBEojEC4OBDQaEaCPwYpQZi%2BleKcV8no/iQkRLWF8hJRgBBpKyRYopZSqk4QaUfNpKqelHL4CMjYEyGlHLmUstZTSdlFQsQii5Nyfd9JeR8ppfygUlAhTChFOWLNYpMHiklVK6VMqaXFiLfK0hxbFSlmVPQBg5YKwILVcajVK6tU4J2A0TUCCdnoCZCUBwvDDUiKNUysABbsEwPgLafkxDx04Nw40PA3z2y2qrW260tYrV9nrFIczlm5E%2Bi7E6VsKju0unoU6OyPrOyWV7WoeyTp3R9mtf2FD2DGl3EDIRINODIlIQQZAawuAilUSKDQsNBkIxNF4e5NDop0OBSKY05wCbnEkOcc4GgEWSGBTTQRhDEIcEZmhcRUyYLUKeUQzFzM0Z%2BQYirSQQA%3D%3D%3D" rel="nofollow">compiled assembly (godbolt)</a> show that Rust saves five registers on the stack while Zig only saves three, but why?
For the purpose of this benchmark it shouldn't matter since we're only comparing against the baseline of each language.)</p>
<p dir="auto">It becomes even more interesting if we're summing the nodes of a much smaller tree:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/rayon-tree-sum-1000.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/rayon-tree-sum-1000.svg" alt="Time to calculate sum of binary tree of 1000 nodes with Rayon"></a></p>
<p dir="auto">In this scenario we have a very short duration of our program:
The baseline implementation takes 1.56 <em>microseconds</em> in total to run.
For some reason the overhead is a bit higher (~19 ns), but more concerningly we see that performance becomes <em>worse</em> the <em>more</em> threads we're adding.
At 32 threads it's in total <strong>60 times slower</strong>.</p>
<p dir="auto">(In this case we're using 32 threads on a machine which only has 16 cores.
It's not given that we would see the same slowdown for a machine with 32 cores.
Nonetheless, this scaling behavior is concerning.)</p>
<p dir="auto">The conventional wisdom for parallelism therefore ends up being "it's not worth it unless you have <em>enough work</em> to parallelize".
The example above is typically presented as a "bad fit for parallelism".
This is understandable and pragmatic, but in practice it makes it a lot more difficult to <em>actually</em> parallelize your code:</p>
<ul dir="auto">
<li>What exactly is "enough work"?
You might need to do a lot of benchmarking with different types of input to understand this.</li>
<li>It might be difficult to detect how much work a certain input does.
For instance, in our binary tree we don't know the full size of it.
There's no obvious way for us to say "if the tree is small enough, don't run the parallelized code" since by only looking at the root we don't the size of it.</li>
<li>As we've seen, the potential slowdown can be extreme.
What if 90% of your workload is like this?</li>
<li>As your program evolves and your code does more (or less) <em>things</em>, the definition of "enough work" will also naturally change.</li>
</ul>
<p dir="auto">The goal of Spice is for you <strong>to never have to worry about your program becoming slower by making it parallel</strong>.
If you're looking to maximize the performance you should of course do elaborate benchmarking, but <em>generally</em> with Spice you can add parallelism and there will be <em>practically</em> no overhead.</p>
<p dir="auto">The last example of summing over 1000 nodes behaves as follows in Spice:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/spice-tree-sum-1000.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/spice-tree-sum-1000.svg" alt="Time to calculate sum of binary tree of 1000 nodes with Spice"></a></p>
<p dir="auto">What's happening here is that it's discovering that the duration is too short so none of the multi-threading kicks in.
All the extra threads here are sleeping, giving the cores time to execute other programs.</p>
<p dir="auto">Spice is <strong>primarily a research project</strong>.
Read along to learn more about it, but if you're considering using it in production you should be aware of its <a href="#limitations">many limitations</a>.</p>
<p dir="auto"><em>(See the <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a> directory for more details about these specific benchmarks.)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#using-spice">Using Spice</a></li>
<li><a href="#work-stealing-and-its-inefficiencies">Work-stealing and its inefficiencies</a></li>
<li><a href="#implementation-details">Implementation details</a>
<ul dir="auto">
<li><a href="#optimizing-for-static-dispatch">Optimizing for static dispatch</a></li>
<li><a href="#low-overhead-heartbeating-signaling">Low-overhead heartbeating signaling</a></li>
<li><a href="#global-mutex-is-fine-when-theres-no-contention">Global mutex is fine when there's no contention</a></li>
<li><a href="#branch-free-doubly-linked-list">Branch-free doubly-linked list</a></li>
<li><a href="#minimizing-the-stack-usage">Minimizing the stack usage</a></li>
<li><a href="#passing-values-around-in-registers">Passing values around in registers</a></li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#faq">FAQ</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Spice</h2><a id="user-content-using-spice" aria-label="Permalink: Using Spice" href="#using-spice"></a></p>
<p dir="auto">The following example demonstrates how Spice works:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const spice = @import(&quot;spice&quot;);

// (1) Add task as a parameter.
fn sum(t: *spice.Task, node: *const Node) i64 {
    var res: i64 = node.val;

    if (node.left) |left_child| {
        if (node.right) |right_child| {
            var fut = spice.Future(*const Node, i64).init();

            // (3) Call `fork` to set up work for another thread.
            fut.fork(t, sum, right_child);

            // (4) Do some work yourself.
            res += t.call(i64, sum, left_child);

            if (fut.join(t)) |val| {
                // (5) Wait for the other thread to complete the work.
                res += val;
            } else {
                // (6) ... or do it yourself.
                res += t.call(i64, sum, right_child);
            }
            return res;
        }

        res += t.call(i64, sum, left_child);
    }

    if (node.right) |right_child| {
        // (2) Recursive calls must use `t.call`
        res += t.call(i64, sum, right_child);
    }

    return res;
}"><pre><span>const</span> <span>spice</span> <span>=</span> <span>@import</span>(<span>"spice"</span>);

<span>// (1) Add task as a parameter.</span>
<span>fn</span> <span>sum</span>(<span>t</span>: <span>*</span><span>spice.Task</span>, <span>node</span>: <span>*</span><span>const</span> <span>Node</span>) <span>i64</span> {
    <span>var</span> <span>res</span>: <span>i64</span> <span>=</span> <span>node</span>.<span>val</span>;

    <span>if</span> (<span>node</span>.<span>left</span>) <span>|</span><span>left_child</span><span>|</span> {
        <span>if</span> (<span>node</span>.<span>right</span>) <span>|</span><span>right_child</span><span>|</span> {
            <span>var</span> <span>fut</span> <span>=</span> <span>spice</span>.<span>Future</span>(<span>*</span><span>const</span> <span>Node</span>, <span>i64</span>).<span>init</span>();

            <span>// (3) Call `fork` to set up work for another thread.</span>
            <span>fut</span>.<span>fork</span>(<span>t</span>, <span>sum</span>, <span>right_child</span>);

            <span>// (4) Do some work yourself.</span>
            <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>left_child</span>);

            <span>if</span> (<span>fut</span>.<span>join</span>(<span>t</span>)) <span>|</span><span>val</span><span>|</span> {
                <span>// (5) Wait for the other thread to complete the work.</span>
                <span>res</span> <span>+=</span> <span>val</span>;
            } <span>else</span> {
                <span>// (6) ... or do it yourself.</span>
                <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>right_child</span>);
            }
            <span>return</span> <span>res</span>;
        }

        <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>left_child</span>);
    }

    <span>if</span> (<span>node</span>.<span>right</span>) <span>|</span><span>right_child</span><span>|</span> {
        <span>// (2) Recursive calls must use `t.call`</span>
        <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>right_child</span>);
    }

    <span>return</span> <span>res</span>;
}</pre></div>
<ol dir="auto">
<li>Every parallel function needs to take a <em>task</em> as a parameter.
This is used to coordinate the work.</li>
<li>You should never call your function directly, but instead use <code>t.call</code> which will call it for you (in the right way).</li>
<li>Call <code>fork</code> to set up a piece of work which can be done by a different thread.
This can be called multiple times to set up multiple pieces of work.</li>
<li>After that your function should do some meaningful work itself.</li>
<li>Call <code>join</code> to wait for the work done by the other thread.</li>
<li><em>However</em>, <code>join</code> might return <code>null</code> and this signals that <em>no other thread picked up the work</em>.
In this case you must do the work yourself.</li>
</ol>
<p dir="auto">Here we repeat ourselves in step 3 and 6:
Both places we refer to <code>sum</code> and <code>right_child</code>.
It's possible to hide this duplication by some helper function, <em>but</em> this example demonstrates a core idea behind Spice:</p>
<p dir="auto"><strong>Not every piece of work comes from the queue.</strong>
You call <code>fork</code> to signal that there's something which <em>can</em> be executed by another thread, but if all the other threads are busy then you fallback to executing it as if the fork never happened.</p>
<p dir="auto">This principle is core to how Spice achieves its low and predictable overhead:
If there's no parallelism possible then all Spice is doing on the hot path is pushing and popping the queue (without ever looking at any of the items).</p>
<p dir="auto">The actually coordination with other threads happens on a <em>fixed heartbeat</em>:
Every 100 microsecond or so a thread will look at its current work queue and dispatch the top-most item to another waiting thread.
Since the heartbeat happens very infrequently (compared to the clock speed) we also don't need to worry so much about what we're doing during the heartbeat.
Even if we spend <em>hundreds</em> of nanoseconds the <em>total</em> overhead becomes small since we do it rarely.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Work-stealing and its inefficiencies</h2><a id="user-content-work-stealing-and-its-inefficiencies" aria-label="Permalink: Work-stealing and its inefficiencies" href="#work-stealing-and-its-inefficiencies"></a></p>
<p dir="auto">Spice provides the <a href="https://en.wikipedia.org/wiki/Fork%E2%80%93join_model" rel="nofollow">fork/join model</a> which has typically been implementing by using <a href="https://en.wikipedia.org/wiki/Work_stealing" rel="nofollow"><strong>work-stealing</strong></a>.
Let's have a look at work-stealing:</p>
<ul dir="auto">
<li>Every thread have their own local <em>work queue</em>.
Every piece of work in the system gets put onto this queue.</li>
<li>The same thread will pick up work from this queue and execute it.
This might lead to more work being added (onto the same queue).</li>
<li>At some point, the local work queue for a thread will become empty.
The thread will then attempt to <em>steal</em> work from another thread:
It takes a chunk of the work from the <em>end</em> of another thread's queue and places it into its own.</li>
<li>Since each thread pulls work from the <em>beginning</em> of its queue and other thread steals from the <em>end</em>, we expect there to be little contention on these queues.</li>
</ul>
<p dir="auto">However, there's three major sources of inefficiencies in this design:</p>
<p dir="auto"><strong>Every piece of work is a <em>dynamic dispatch</em>.</strong>
In compiled languages (such as C) function calls are "practically" free due to the capability of statically knowing everything about the called function.
This is a scenario which compilers and CPUs have been optimized for <em>decades</em> to execute efficiently.
Work-stealing systems <em>don't</em> use this functionality, but instead puts every piece of work into generic "call this dynamic function".
It's a small piece of overhead, but it does add up.</p>
<p dir="auto"><strong>The "local" work queue isn't really local.</strong>
Yes, it's true that every thread have a single queue that they will push work onto, <em>but</em> this is far from a "local" queue as is typically described in concurrent algorithms.
This is a queue in which <em>every</em> thread at <em>every</em> point might steal from.
In reality, work-stealing systems with N threads have N global queues, where each queue only has a single producer, but everyone is a consumer.
Why does this distinction matter?
<em>Because all operations on these queues have to use atomic operations.</em>
Atomic operations, especially stores, are far more expensive than regular, <em>local</em> stores.</p>
<p dir="auto"><strong>Spinning works great … until it doesn't.</strong>
The queues in work-stealing systems are typically implemented using <em>spinning</em>:
Every thread will optimistically try to acquire a single item from the queue, and if there's a contention with another thread it will <em>try again</em> in a loop.
This typically gives great performance … <strong>until it doesn't</strong>.
It can be very hard to reason about this or replicate it since under one set of conditions everything is fine, but <em>suddenly</em> during contention the system will slow down to a halt (i.e. 10x-100x slower).</p>
<p dir="auto">Spice directly tackles all of these inefficiencies:</p>
<ol dir="auto">
<li>The dynamic dispatch of the work queue is only used when work is sent to another thread.
Work done <em>within</em> a single thread will use regular function calls outside of the work queue.</li>
<li>The work queue is truly local:
Pushing to it involves (1) one memory store to a pointer to somewhere on the stack, (2) one memory store to the current stack frame, (3) one register store.
None of these operations need to synchronize with other threads.</li>
<li>There isn't a single <code>while</code>-loop in Spice which doesn't also contain a <code>wait()</code>-call which will suspend the thread.
There is no spinning.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation details</h2><a id="user-content-implementation-details" aria-label="Permalink: Implementation details" href="#implementation-details"></a></p>
<p dir="auto">Let's dive further into how Spice is implemented to achieve its efficient parallelism.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Optimizing for static dispatch</h3><a id="user-content-optimizing-for-static-dispatch" aria-label="Permalink: Optimizing for static dispatch" href="#optimizing-for-static-dispatch"></a></p>
<p dir="auto">A fork/join program has a set of code blocks which are executed in parallel and once they finish the <code>join</code> action completes:</p>
<div data-snippet-clipboard-copy-content="join(
  fork { code1 }
  fork { code2 }
  fork { code3 }
)"><pre><code>join(
  fork { code1 }
  fork { code2 }
  fork { code3 }
)
</code></pre></div>
<p dir="auto">In Spice this is represented as:</p>
<div data-snippet-clipboard-copy-content="job1 = fork { code1 }  // Place on the queue
job2 = fork { code2 }  // Place on the queue

code3 // Run right away

if (job2.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job2.wait()
} else {
  code2
}

if (job1.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job1.wait()
} else {
  code1
}"><pre><code>job1 = fork { code1 }  // Place on the queue
job2 = fork { code2 }  // Place on the queue

code3 // Run right away

if (job2.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job2.wait()
} else {
  code2
}

if (job1.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job1.wait()
} else {
  code1
}
</code></pre></div>
<p dir="auto">Notice that <code>code1</code> and <code>code2</code> has been duplicated_inside the function.
This is actually a <em>good</em> thing.
Most of the time the job will <em>not</em> be picked up by another thread.
In this case, our program nicely turns into the sequential version (although in reverse order) with a few extra branches which are all very predictable.
This is friendly both for the code optimizer (e.g. it can now inline the function call) and the CPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Low-overhead heartbeating signaling</h3><a id="user-content-low-overhead-heartbeating-signaling" aria-label="Permalink: Low-overhead heartbeating signaling" href="#low-overhead-heartbeating-signaling"></a></p>
<p dir="auto">The core idea of heartbeat scheduling is to do scheduling <em>locally</em> and at a <em>low frequency</em>:
Every 100 microsecond or so we'd like every thread to look at it local work queue and send work to a different thread.
The low frequency is key to eliminating overall overhead.
If we're only doing something every 100 microsecond we can actually spend 100 nanoseconds (an eternity!) and still only introduce 0.1% overhead.</p>
<p dir="auto">Operating systems have built-in support for <em>signaling</em>, but these are very hard to reason about.
The user code gets paused at <em>any</em> random point and it's hard to safely continue running.
For this reason, Spice uses a cooperative approach instead:
The user code have to call <code>tick()</code> and this detects whether a heartbeat should happen.
This function call is automatically called for you whenever you use the <code>call</code>-helper.</p>
<p dir="auto">It's critical that this function is efficient when a heartbeat <strong>isn't</strong> happening.
This is after all the common case (as the heartbeat is only happening every ~100 microsecond).</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub inline fn tick(self: *Task) void {
    if (self.worker.heartbeat.load(.monotonic)) {
        self.worker.pool.heartbeat(self.worker);
    }
}"><pre><span>pub</span> <span>inline</span> <span>fn</span> <span>tick</span>(<span>self</span>: <span>*</span><span>Task</span>) <span>void</span> {
    <span>if</span> (<span>self</span>.<span>worker</span>.<span>heartbeat</span>.<span>load</span>(<span>.monotonic</span>)) {
        <span>self</span>.<span>worker</span>.<span>pool</span>.<span>heartbeat</span>(<span>self</span>.<span>worker</span>);
    }
}</pre></div>
<p dir="auto">In Spice we spawn a separate heartbeat thread whose sole purpose is to periodically flip the thread's atomic heartbeat value from <code>false</code> to <code>true</code>.
The <code>tick()</code> function then reads this atomic value and starts its heartbeat code when it's <code>true</code>.</p>
<p dir="auto">A key part of reducing the overhead of the ticking is to make sure the heartbeat function itself is marked as <em>cold</em>.
This causes the presence of this function call to not use up any registers.
Without this the overhead is significantly higher.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Global mutex is fine when there's no contention</h3><a id="user-content-global-mutex-is-fine-when-theres-no-contention" aria-label="Permalink: Global mutex is fine when there's no contention" href="#global-mutex-is-fine-when-theres-no-contention"></a></p>
<p dir="auto">If you look inside the codebase of Spice you will find that each thread pool has a single mutex which is locked all over the place.
An immediate reaction would be "oh no, a global mutex is terrible" and you might be tempted to replace it.</p>
<p dir="auto"><em>However</em>, there's no problem with a global mutex <em>until you're being blocked</em>.
And you can only be blocked if two conditions occur:</p>
<ol dir="auto">
<li>A thread is holding the lock for a <em>long</em> time.</li>
<li>There's concurrent threads trying to acquire the lock at the same time.</li>
</ol>
<p dir="auto"><strong>None</strong> of these are true for Spice.
The heartbeating ensures that typically only a single thread is executing a heartbeat.
In addition, no user code is executed while the lock is held.
We're only protecting trivial simple memory reads/writes which will complete in constant time.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Branch-free doubly-linked list</h3><a id="user-content-branch-free-doubly-linked-list" aria-label="Permalink: Branch-free doubly-linked list" href="#branch-free-doubly-linked-list"></a></p>
<p dir="auto">We're using a doubly-linked list to keep track of the work queue:
<code>fork()</code> appends to the end, <code>join()</code> pops from the end (if it's still there), and we pop from the <em>beginning</em> when we want to send work to a background worker.</p>
<p dir="auto"><a href="https://github.com/ziglang/zig/blob/cb308ba3ac2d7e3735d1cb42ef085edb1e6db723/lib/std/linked_list.zig#L267-L275">Appending into a doubly-linked list</a> typically looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub fn append(list: *Self, new_node: *Node) void {
    if (list.last) |last| {
        // Insert after last.
        list.insertAfter(last, new_node);
    } else {
        // Empty list.
        list.prepend(new_node);
    }
}"><pre><span>pub</span> <span>fn</span> <span>append</span>(<span>list</span>: <span>*</span><span>Self</span>, <span>new_node</span>: <span>*</span><span>Node</span>) <span>void</span> {
    <span>if</span> (<span>list</span>.<span>last</span>) <span>|</span><span>last</span><span>|</span> {
        <span>// Insert after last.</span>
        <span>list</span>.<span>insertAfter</span>(<span>last</span>, <span>new_node</span>);
    } <span>else</span> {
        <span>// Empty list.</span>
        <span>list</span>.<span>prepend</span>(<span>new_node</span>);
    }
}</pre></div>
<p dir="auto">Notice that there's a conditional here: If the list is empty we need to do something special.
Most of the time the list will of course <em>not</em> be empty.
To eliminate the branch we can make sure that the list is <em>never</em> empty.
We define a sentinel node (the "head") which always represents the beginning of the list.
The tail pointer will start by pointing to this head node.</p>
<p dir="auto">This means that both pushing and popping is completely branch-free and these are operations we do at <em>every</em> recursive function call.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Minimizing the stack usage</h3><a id="user-content-minimizing-the-stack-usage" aria-label="Permalink: Minimizing the stack usage" href="#minimizing-the-stack-usage"></a></p>
<p dir="auto">A <code>Future</code> in Spice has two possible states: It's either <em>queued</em> or <em>executing</em>.
The heartbeat is responsible for taking a <em>queued</em> future and start <em>executing</em> it.
And as we already know: Heartbeating happens rarely so we expect many futures to be queued without executing.</p>
<p dir="auto">An early prototype of Spice used a <em>tagged union</em> to store the future on the stack.
This turns out to be suboptimal because (1) stack usage matters for performance (at least in this benchmark) and (2) there's quite a lot of additional state needed to keep track of futures which are <em>executing</em>.</p>
<p dir="auto">To minimize stack usage Spice therefore uses two techniques:</p>
<ol dir="auto">
<li>Execution state is placed in a separate (pool-allocated) struct.
The queued (but not executed) futures therefore does not need to consume any of this space.</li>
<li>We manually create a tagged union where we use the fact that the <em>executing</em> state only needs a single pointer while the <em>queued</em> state is guaranteed to have a <code>prev</code> pointer.
Whether the first field is <code>null</code> therefore decides which of these it is.
(Maybe a smart enough compiler would be able to this optimization for us.)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="const Future = struct {
    prev_or_null: ?*anyopaque,
    next_or_state: ?*anyopaque,
}

// A future which is _queued_ has:
//   prev_or_null = pointer to prev future
//   next_or_state = pointer to next future

// A future which is _executing_ has:
//   prev_or_null = null
//   next_or_state = ExecuteState

const ExecuteState = struct {
    requester: *Worker,
    done: std.Thread.ResetEvent = .{},
    result: ResultType,
    // Any number of fields.
}"><pre><span>const</span> <span>Future</span> <span>=</span> <span>struct</span> {
    <span>prev_or_null</span>: <span>?</span><span>*</span><span>anyopaque</span>,
    <span>next_or_state</span>: <span>?</span><span>*</span><span>anyopaque</span>,
}

<span>// A future which is _queued_ has:</span>
<span>//   prev_or_null = pointer to prev future</span>
<span>//   next_or_state = pointer to next future</span>

<span>// A future which is _executing_ has:</span>
<span>//   prev_or_null = null</span>
<span>//   next_or_state = ExecuteState</span>

<span>const</span> <span>ExecuteState</span> <span>=</span> <span>struct</span> {
    <span>requester</span>: <span>*</span><span>Worker</span>,
    <span>done</span>: <span>std.Thread.ResetEvent</span> <span>=</span> .{},
    <span>result</span>: <span>ResultType</span>,
    <span>// Any number of fields.</span>
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Passing values around in registers</h3><a id="user-content-passing-values-around-in-registers" aria-label="Permalink: Passing values around in registers" href="#passing-values-around-in-registers"></a></p>
<p dir="auto">Spice works with a <code>Task</code> struct which has two fields:
A pointer to the owning worker and a pointer to tail of the work queue.
For optimal performance these should be passed as registers across all function boundaries.
However, with LLVM, passing a struct will very often cause it be passed on the stack.</p>
<p dir="auto">To work around this we define a <em>separate</em> function where <code>worker</code> and <code>job_tail</code> are actual parameters.
We place the parameters into a struct and pass a pointer to this into the user-defined function.
This function call we make sure is always being inlined:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fn callWithContext(
    worker: *Worker,
    job_tail: *Job,
    comptime T: type,
    func: anytype,
    arg: anytype,
) T {
    var t = Task{
        .worker = worker,
        .job_tail = job_tail,
    };
    return @call(.always_inline, func, .{
        &amp;t,
        arg,
    });
}"><pre><span>fn</span> <span>callWithContext</span>(
    <span>worker</span>: <span>*</span><span>Worker</span>,
    <span>job_tail</span>: <span>*</span><span>Job</span>,
    <span>comptime</span> <span>T</span>: <span>type</span>,
    <span>func</span>: <span>anytype</span>,
    <span>arg</span>: <span>anytype</span>,
) <span>T</span> {
    <span>var</span> <span>t</span> <span>=</span> <span>Task</span>{
        .<span>worker</span> <span>=</span> <span>worker</span>,
        .<span>job_tail</span> <span>=</span> <span>job_tail</span>,
    };
    <span>return</span> <span>@call</span>(<span>.always_inline</span>, <span>func</span>, .{
        <span>&amp;</span><span>t</span>,
        <span>arg</span>,
    });
}</pre></div>
<p dir="auto">This causes the <code>callWithContext</code>-function to be the <em>actual</em> function which LLVM works on, and since this has pointers are parameters it will happily pass these directly into registers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">The initial development of Spice has been focused around a single benchmark which is described in detail in <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<p dir="auto">Spice was made possible thanks to the research into <em>heartbeat scheduling</em>:</p>
<p dir="auto"><a href="https://arxiv.org/abs/2307.10556" rel="nofollow">"The best multicore-parallelization refactoring you've never heard of"</a> gives an <em>excellent</em> introduction into the concepts of heartbeat scheduling.
It's a very short paper which focuses entirely on a single use case, but describes everything in a manner which can be generalized.
The solution presented in this paper is based around turning all the code into continuation-passing style which enables switching between sequential and parallel execution.
Spice started out as an experiment of this approach, but this turned out to have quite high overhead (&gt;10 nanosecond).</p>
<p dir="auto">Going backwards in time, <a href="https://www.chargueraud.org/research/2018/heartbeat/heartbeat.pdf" rel="nofollow">"Heartbeat scheduling: provable efficiency for nested parallelism"</a> was the first paper introducing "heartbeat scheduling".
This paper provides excellent information about the concepts, but the implementation is based around integrating this into an interpreter and focus is primarily on the theoretical guarantees as opposed to raw performance.</p>
<p dir="auto"><a href="https://paragon.cs.northwestern.edu/papers/2021-PLDI-TPAL-Rainey.pdf" rel="nofollow">"Task parallel assembly language for uncompromising parallelism"</a> is a follow-up paper which improves the performance by defining a custom assembly language and using OS signaling for heartbeats.
This is a fascinating line of research, but it's difficult to integrate into an existing language.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Limitations</h2><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<p dir="auto">There's <em>many</em> limitations of the current implementation of Spice:</p>
<ul dir="auto">
<li><strong>Rough edges when you're using it wrong:</strong> Spice is quite peculiar about how it should be used (most notably about <code>fork</code> and <code>join</code>).
If you're using it wrong now then weird things could happen.
This should be improved by adding more compile-time checking, debug-mode assertions, or changing the overall API.</li>
<li><strong>Lack of tests:</strong> Spice contains a lot of gnarly concurrent code, but has zero testing coverage.
This would have be improved before Spice can be responsibly used for critical tasks.</li>
<li><strong>Lack of support for arrays/slices:</strong> Probably <em>the</em> most common use case for fine-grained parallelism is to do something for every element of an array/slice.
There should be native, efficient support for this use case.</li>
<li><strong>Lack of documentation:</strong> There's no good documentation of how to use it.</li>
<li><strong>Lack of further benchmarks:</strong> This has only been tested on a single small benchmark.
This benchmark <em>should</em> be quite representative (see <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a> for more details), but further benchmarks are needed to validate these findings.</li>
<li><strong>@panic-heavy:</strong> Spice is quite optimistic in its error handling and uses <code>@panic</code> extensively.
To be considered a proper Zig library there needs to be way more consideration of how error cases are handled.</li>
<li><strong>Lack of testing with ReleaseSafe:</strong>
<code>ReleaseSafe</code> is an extremely nice feature of Zig.
Further benchmarking and testing is needed to understand how well Spice can work here.</li>
</ul>
<p dir="auto">Luckily the whole codebase is ~500 lines so it shouldn't be <em>too</em> difficult to make progress on these areas.</p>
<p dir="auto">There's currently no plans of doing any active development on Spice to improve this (as the original author don't have the time).
Any improvements in forks and/or re-implementations in other languages are highly encouraged!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><strong>Question: Why is it called "Spice"?</strong></p>
<p dir="auto">Answer: This project enables <em>fine-grained</em> parallelism. Sand is extremely fine-grained. Sand forms in dunes. <a href="https://en.wikipedia.org/wiki/Melange_(fictional_drug)" rel="nofollow">Spice</a>.
Also: It's a hot take on parallelism.</p>
<p dir="auto"><strong>Question: Why is it implemented in Zig?</strong></p>
<p dir="auto">Answer: Why not?
This describes a <em>generic approach</em> to parallelism that should be possible to implement in multiple languages.
Maybe I'll end up implementing something similar in another language as well?
I don't know yet.
If you think this is interesting for <em>your</em> language of choice I would encourage you to explore this area.</p>
<p dir="auto"><strong>Question: But if you did it in Rust we could have <em>safe</em> parallelism?</strong></p>
<p dir="auto">Answer:
Yeah, that sounds very cool.
I'm not at all opposed to it.
<em>That said</em>, I've been exploring many different techniques and variants while developing Spice.
Many of my initial ideas were definitely not "safe" by any means, but I was able to express these ideas in Zig, look at the assembly and measure the performance in benchmarks.
I'd probably only be able to explore a fraction of the ideas if I was limited by Rust's strict semantics in the <em>initial</em> phase of this project.
If I have to turn this into a production-ready system I might decide to use Rust.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workers are stuck in place because everyone is too afraid of a recession to quit (124 pts)]]></title>
            <link>https://boredbat.com/american-workers-are-stuck-in-place-because-everyone-is-too-afraid-of-a-recession-to-quit/#google_vignette</link>
            <guid>41229600</guid>
            <pubDate>Mon, 12 Aug 2024 21:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boredbat.com/american-workers-are-stuck-in-place-because-everyone-is-too-afraid-of-a-recession-to-quit/#google_vignette">https://boredbat.com/american-workers-are-stuck-in-place-because-everyone-is-too-afraid-of-a-recession-to-quit/#google_vignette</a>, See on <a href="https://news.ycombinator.com/item?id=41229600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>America’s job market is in a bind.</p>
<p>That’s probably no surprise to current&nbsp;job-seekers, who are having an&nbsp;increasingly tough time&nbsp;landing a new gig as&nbsp;hiring slows&nbsp;and job boards run dry.</p>
<p>The stagnation has resulted in a rise in “stuck” workers — frustrated employees who say they want to quit a job, but are staying put as the fear of a potential recession looms in the backs of their minds.</p>
<p>A 24-year-old employee working in histology named Amanda, who spoke with Business Insider, is one such worker who feels that way. She’s choosing to stay in her current role as there are limited offerings in her field, and switching employers would likely lead to her pay being cut by at least a third.</p>
<p>“I feel trapped here,” Amanda said. “I’m financially screwed if I leave, and that’s why I don’t, or can’t leave.”</p>
<p>Americans have long grumbled about their&nbsp;feelings of being stuck in an unsatisfying role, but the feeling appears to be growing: Americans are quitting their jobs at the slowest pace since the pandemic, with the quits falling to just 2.1% in July, according to the&nbsp;Bureau of Labor Statistics.</p>
<p>Yet,&nbsp;job satisfaction&nbsp;fell across 26 measures in the past year, per an annual survey from the Conference Board.</p>
<p>Google search interest for the search phrase “quitting job” is down 11% over the last year, according to data accessed from the search analytics tool Glimpse.</p>
<figure><img decoding="async" width="800" height="414" src="https://boredbat.com/wp-content/uploads/2024/08/image-12.jpeg" alt="A graph showing search interest for the phrase &quot;quitting job&quot;" srcset="https://boredbat.com/wp-content/uploads/2024/08/image-12.jpeg 800w, https://boredbat.com/wp-content/uploads/2024/08/image-12-300x155.jpeg 300w, https://boredbat.com/wp-content/uploads/2024/08/image-12-768x397.jpeg 768w, https://boredbat.com/wp-content/uploads/2024/08/image-12-463x240.jpeg 463w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Search interest for the phrase “quitting job” is down 11% over the past year.&nbsp;Google Trends/Glimpse</figcaption></figure>
<p>“Stuck at work,” meanwhile, is becoming a more common search term, with interest rising 9% in the past year.</p>
<figure><img decoding="async" width="800" height="414" src="https://boredbat.com/wp-content/uploads/2024/08/image-13.jpeg" alt="Graph showing search interest for the query &quot;stuck at work&quot;" srcset="https://boredbat.com/wp-content/uploads/2024/08/image-13.jpeg 800w, https://boredbat.com/wp-content/uploads/2024/08/image-13-300x155.jpeg 300w, https://boredbat.com/wp-content/uploads/2024/08/image-13-768x397.jpeg 768w, https://boredbat.com/wp-content/uploads/2024/08/image-13-463x240.jpeg 463w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Google searches for “stuck at work,” meanwhile, have climbed 9% over the past year.&nbsp;Google Trends/Glimpse</figcaption></figure>
<p>Membership on the subreddit r/hatemyjob has more than doubled over the past two years, with users on the community growing 30,000-strong as of August, up from 14,7000 in 2022, according to historical data from the analytics site&nbsp;SubredditStats.</p>
<p>“Stuck at a job,” one&nbsp;<a target="_blank" href="https://www.reddit.com/r/hatemyjob/comments/1c61gqh/stuck_at_a_job/" rel="noreferrer noopener">user</a>&nbsp;on the subreddit posted. “I’m no longer fond of the work I do. I feel stuck because of the money. It’s a good problem to have, I suppose.”</p>

<p>“I’m just so done with this job. I’ve tried everything to stick it out but now I just can’t do it anymore,” another&nbsp;user&nbsp;wrote, adding that they had been looking for a job related to their degree for over a year. The search hasn’t been successful, they said, citing “tough” conditions in the job market.</p>
<p>“I want to quit this job so badly but I can’t afford it.”</p>
<h2 id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">Recession fears loom large</h2>
<p id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">Workers have typically hunkered down when the economy slows, with recessions often tied to plunges in the quits rate, historical data from the Fed shows.</p>
<p id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">The economy hasn’t fallen into a recession but fears of a coming downturn are growing. In markets, investors panicked last week, sparking a huge sell-off after&nbsp;July payrolls were lower than expected, with the unemployment rate ticking up to 4.3%.</p>
<p id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">Most Americans now believe the&nbsp;economy is in a recession, a recent Affirm survey found, despite&nbsp;GDP continuing to grow&nbsp;over the second quarter.</p>
<p>Google search interest in the term “recession” has exploded 230% over the past month, Glimpse data shows.</p>
<figure><img loading="lazy" decoding="async" width="800" height="413" src="https://boredbat.com/wp-content/uploads/2024/08/image-11.jpeg" alt="Graph showing search interest for the term &quot;recession&quot;" srcset="https://boredbat.com/wp-content/uploads/2024/08/image-11.jpeg 800w, https://boredbat.com/wp-content/uploads/2024/08/image-11-300x155.jpeg 300w, https://boredbat.com/wp-content/uploads/2024/08/image-11-768x396.jpeg 768w, https://boredbat.com/wp-content/uploads/2024/08/image-11-463x239.jpeg 463w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Google search interest in “recession” has more than doubled in the past month.&nbsp;Google Trends/Glimpse</figcaption></figure>
<p>“I wouldn’t say that we’re in a recession or anything,” Raymond Lee, the CEO of the career outplacement firm Careerminds told BI. “I would say, though, that, just from my perspective, I think a lot of people are staying put in their jobs because I think that there is a lot of uncertainty … People are trying to stay where they are and not make any big moves.”</p>
<p>Korn Ferry, a consultancy that offers career transitioning and outplacement services, said it had seen an increase in inbound calls from job seekers. That’s the opposite of what the firm saw during the post-pandemic hiring boom — and it’s a solid sign the “engine is slowing down,” according to Radhika Papandreou, the president of the Korn Ferry’s North American arm.</p>
<p>In general, clients are taking longer to secure new roles and appear to be prioritizing job security, Papandreou said.</p>
<p>“People are also hesitant to leave their jobs to look at other jobs unless they feel like they’re going to get something that’s secure and for a long time,” she added. “There’s a little bit of, ‘I don’t want to be last in, first out.'”</p>
<p>Job market forecasters say the&nbsp;slowdown&nbsp;in&nbsp;hiring&nbsp;looks poised to continue,&nbsp;even if the Fed begins to loosen monetary policy. Only 15% of small businesses said they were planning on adding new jobs in July, according to the latest survey from the&nbsp;National Federation of Independent Businesses, down from a peak of over 30% recorded several years ago.</p>
<p>Visited 1,566 times, 987 visit(s) today</p>


<p itemprop="dateModified">Last modified: August 11, 2024</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA investigation finds Boeing hindering Americans' return to moon (204 pts)]]></title>
            <link>https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/</link>
            <guid>41229049</guid>
            <pubDate>Mon, 12 Aug 2024 20:38:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/">https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/</a>, See on <a href="https://news.ycombinator.com/item?id=41229049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="page-content" data-og-area="article-blocks"><p>Mismanagement and inexperience on the part of Boeing are creating severe delays and expenditures for NASA’s efforts to return Americans to the moon, according to a new report from the agency’s office of the inspector general (OIG).</p><p>The 38-page document, released Wednesday, paints the manufacturer’s quality control practices as inadequate and its workforce as insufficiently trained, blaming it for cost increases and schedule delays in the development of NASA’s Space Launch System (SLS) Block 1B. Yet the space agency has neglected to punish Boeing financially for these flaws, arguing that doing so would run contrary to the terms of its contract.</p><p>The heavy-lift rocket, a more powerful configuration of NASA’s existing SLS Block 1, is intended to make its maiden voyage in 2028 on the Artemis IV mission, a crewed lunar landing. It has been under development since 2014. Boeing is <a href="https://www.nasa.gov/humans-in-space/nasa-commits-to-future-artemis-moon-rocket-production/#:~:text=As%20part%20of%20the%20contract,John%20Honeycutt%2C%20SLS%20Program%20manager" target="_blank" rel="noreferrer noopener">under contract</a> to build Block 1B’s Exploration Upper Stage (EUS)—which will increase the SLS’ cargo capacity by about 40 percent—as well as the core stages for Block 1 on Artemis I and the upcoming Artemis II. Other SLS contractors include Aerojet Rocketdyne and Northrop Grumman.</p><h2 data-og-block-area="article-blocks" data-og-block-nth="1" data-og-block-type="core/heading" data-rawhtml="1" id="a_day_late_a_dollar_short">A Day Late, A Dollar Short</h2><p>Originally, the EUS was allocated a budget of $962 million and intended to fly on Artemis II, which in January was <a href="https://www.flyingmag.com/nasa-delays-first-crewed-us-moon-landing-in-half-a-century-to-2026/" target="_blank" rel="noreferrer noopener">pushed</a> to no earlier than September 2025. But by the OIG’s estimate, EUS costs are expected to balloon to $2 billion through 2025 and reach $2.8 billion by the time Artemis IV lifts off in 2028.</p><p>The office projects total SLS Block 1B costs will hit $5.7 billion before then—that’s more than $700 million over the Agency Baseline Commitment (ABC) NASA made last year. The EUS, at nearly triple its original budget, would account for close to half of those costs.</p><p>Add to that an expected six-year delay in the delivery of the system, and the OIG predicts Artemis IV’s launch could be postponed.</p><p>“NASA’s fiscal year 2024 SLS Program budget projections do not account for the additional funds needed for EUS development in fiscal years 2024 through 2027,” the report says. “Without additional funding, scheduled work will continue to be pushed into subsequent years as has been the case for the EUS over the last decade, leading to further cost increases and schedule delays.”</p><p>For example, the OIG says, NASA is evaluating potential risks to the EUS stage controller and avionics that could delay its delivery by another 14 months. NASA officials disagreed with the analysis.</p><h2 data-og-block-area="article-blocks" data-og-block-nth="2" data-og-block-type="core/heading" data-rawhtml="1" id="mismanaged_and_inexperienced">Mismanaged and Inexperienced</h2><p>The OIG interviewed officials at NASA headquarters, Marshall Space Flight Center, Michoud Assembly Facility, the Defense Contract Management Agency (DCMA), and Boeing. It also reviewed NASA and its contractors’ budgets, contract obligations, and quality control documents, among other materials.</p><p>In short, the office found that Boeing’s quality management system at Michoud does not adhere to NASA or international standards.</p><p>For example, Boeing Defense’s Earned Value Management System (EVMS)—which NASA uses to measure contract cost and schedule progress and is required on all projects with a lifecycle cost greater than $250M—has been disapproved by the Department of Defense since 2020. Officials claim this precludes Boeing from reliably predicting an EUS delivery date.</p><p>“Boeing’s process for addressing contractual noncompliance has been ineffective, and the company has generally been nonresponsive in taking corrective actions when the same quality control issues reoccur,” the OIG says.</p><p>The DCMA has issued several corrective action requests (CARs), handed down when quality control issues are identified, for the EVMS. Between September 2021 and September 2023, the agency issued Boeing a whopping 71 CARs after identifying quality control issues in the manufacturing of core and upper stages at Michoud. According to officials, that’s a massive number for a system that has been in development for so long.</p><p>“Boeing officials incorrectly approved hardware processing under unacceptable environmental conditions, accepted and presented damaged seals to NASA for inspection, and used outdated versions of work orders,” the report says. “DCMA also found that Boeing personnel made numerous administrative errors through changes to certified work order data without proper documentation.”</p><p>According to Safety and Mission Assurance officials at NASA and DCMA officials at Michoud, Boeing’s quality control issues stem from a workforce that is, by and large, unqualified.</p><p>During a visit to Michoud in 2023, for example, inspectors discovered that welding on a component of the SLS Core Stage 3 did not meet NASA standards. Per the report, unsatisfactory welding performed on a set of fuel tanks led directly to a seven-month delay in EUS completion.</p><p>“According to NASA officials, the welding issues arose due to Boeing’s inexperienced technicians and inadequate work order planning and supervision,” the OIG says. “The lack of a trained and qualified workforce increases the risk that Boeing will continue to manufacture parts and components that do not adhere to NASA requirements and industry standards.”</p><p>Complicating matters further is the relocation of SLS core stage production for Artemis III from Michoud to Kennedy, which will require Boeing to transition a decade of production processes developed at the former site to the latter.</p><p>The OIG said the manufacturer is developing a more robust, hands-on training program that could revamp its workforce but is long overdue.</p><p>“Some technicians reported they had to hunt through layers of documentation to identify required instructions and documentation of work history and key decisions related to the hardware,” the report says.</p><p>Further, maintaining that workforce may be difficult—the OIG predicts Boeing will spend an average of $26 million per month on EUS personnel through 2027. That was the norm for the company from February to August 2023.</p><p>Boeing management has also dropped the ball at higher levels. For instance, in the leadup to Artemis I, Boeing underestimated the complexity of building the SLS core stage, and EUS funding had to be redirected to that project.</p><p>“This ultimately led to a nearly one-year delay in EUS work and an additional $4 billion in funding to Boeing to cover the costs for the core stage development work,” according to the OIG.</p><p>In addition, NASA officials believe Boeing’s supply chain woes are of its own making, stemming from late negotiations and contract agreements.</p><h2 data-og-block-area="article-blocks" data-og-block-nth="3" data-og-block-type="core/heading" data-rawhtml="1" id="next_steps_for_nasa">Next Steps for NASA</h2><p>The OIG report paints the picture of a company in disarray from top to bottom.</p><p>The office did not pin the blame entirely on Boeing. It criticized NASA, for example, for spending more than $3 billion over ten years without submitting an ABC to Congress and the Office of Budget and Management. The ABC is the only official cost and schedule baseline used to measure project performance against expectations.</p><p>The office’s four recommendations, however, center around the manufacturer.</p><p>First, the OIG calls on the associate administrator of NASA’s Exploration Systems Development Mission Directorate (ESDMD), alongside the agency’s assistant administrator for procurement and chief of safety and mission assurance, to collaborate with Boeing on a more robust, NASA-approved quality management system. It also recommends officials penalize the company financially for its previous violations.</p><p>The OIG further directs the ESDMD to conduct a cost overrun analysis of Boeing’s EUS contract to minimize the impact to Artemis missions. Finally, it asks the associate administrator to coordinate with the DCMA to ensure Boeing’s compliance with EVMS requirements.</p><p>NASA agreed with three of the four recommendations and proposed actions to take. Interestingly, though, it rejected the suggestion of fining Boeing.</p><p>“NASA interprets this recommendation to be directing NASA to institute penalties outside the bounds of the contract,” said Catherine Koerner, deputy associate administrator of the ESDMD, in NASA’s response to the report. “There are already authorities in the contract, such as award fee provisions, which enable financial ramifications for noncompliance with quality control standards.”</p><p>Essentially, the agency believes it can keep Boeing in check by rewarding good behavior rather than penalizing mismanagement. The OIG, predictably, disagrees, characterizing NASA as “unresponsive” to what it considers significant safety concerns.</p><p>“In the end, failure to address these issues may not only hinder the Block 1B’s readiness for Artemis IV but also have a cascading impact on the overall sustainability of the Artemis campaign and NASA’s deep space human exploration efforts,” the report says.</p><p>Boeing will look to improve some of its quality control issues under the leadership of new CEO Kelly Ortberg, the ex-boss of Rockwell Collins who took over after the ousting of former CEO Dave Calhoun.</p><p>Calhoun’s departure this month comes as the company continues to be grilled over the <a href="https://www.flyingmag.com/boeing-subcontractor-scrutinized-over-door-plug-failure/" target="_blank" rel="noreferrer noopener">loss of a door plug</a> on a Boeing 737 Max 9 in January as well as <a href="https://www.flyingmag.com/modern/nasa-starliner-astronauts-may-not-return-until-february/" target="_blank" rel="noreferrer noopener">persistent issues</a> with Starliner, its semireusable spacecraft under contract with NASA for astronaut rotation missions to the&nbsp; International Space Station. Astronauts Butch Wilmore and Suni Williams may end up spending eight months on the orbital laboratory, rather than eight days as intended.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal appeals court finds geofence warrants "categorically" unconstitutional (384 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional</link>
            <guid>41228630</guid>
            <pubDate>Mon, 12 Aug 2024 19:57:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional">https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional</a>, See on <a href="https://news.ycombinator.com/item?id=41228630">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p>In a major decision on Friday, the federal Fifth Circuit Court of Appeals <a href="https://www.ca5.uscourts.gov/opinions/pub/23/23-60321-CR0.pdf">held</a> that <a href="https://www.eff.org/deeplinks/2019/04/googles-sensorvault-can-tell-police-where-youve-been">geofence warrants</a> are “categorically prohibited by the Fourth Amendment.” Closely following arguments EFF has made in a <a href="https://www.eff.org/deeplinks/2023/04/first-us-appellate-court-decide-finds-geofence-warrant-unconstitutional">number</a> <a href="https://www.eff.org/deeplinks/2022/10/california-court-suppresses-evidence-overbroad-geofence-warrant">of</a> <a href="https://www.eff.org/deeplinks/2023/01/eff-files-amicus-briefs-two-important-geofence-search-warrant-cases">cases</a>, the court found that geofence warrants constitute the sort of “general, exploratory rummaging” that the drafters of the Fourth Amendment intended to outlaw. EFF applauds this decision because it is essential that every person feels like they can simply take their cell phone out into the world without the fear that they might end up a criminal suspect because their location data was swept up in open-ended digital dragnet.</p>
<p>The new Fifth Circuit case, <em>United States v. Smith</em>, involved an armed robbery and assault of a US Postal Service worker at a post office in Mississippi in 2018. After several months of investigation, police had no identifiable suspects, so they obtained a geofence warrant covering a large geographic area around the post office for the hour surrounding the crime. Google responded to the warrant with information on several devices, ultimately leading police to the two defendants.</p>
<p>On appeal, the Fifth Circuit reached several important holdings.</p>
<p>First, it determined that under the Supreme Court’s landmark ruling in <em>Carpenter v. United States</em>, individuals have a reasonable expectation of privacy in the location data implicated by geofence warrants. As a result, the court broke from the <a href="https://www.eff.org/deeplinks/2024/07/eff-tells-minnesota-supreme-court-strike-down-geofence-warrant-fourth-circuit">Fourth Circuit’s deeply flawed decision last month</a> in <em>United States v. Chatrie</em>, noting that although geofence warrants can be more “limited temporally” than the data sought in <em>Carpenter</em>, geofence location data is still highly invasive because it can expose sensitive information about a person’s associations and allow police to “follow” them into private spaces.</p>
<p>Second, the court found that even though investigators seek warrants for geofence location data, these searches are inherently unconstitutional. As the court noted, geofence warrants require a provider, almost always Google, to search “the entirety” of its reserve of location data “while law enforcement officials have no idea who they are looking for, or whether the search will even turn up a result.” Therefore, “the quintessential problem with these warrants is that they <em>never</em> include a specific user to be identified, only a temporal and geographic location where any given user <em>may</em> turn up post-search. That is constitutionally insufficient.”</p>
<p>Unsurprisingly, however, the court found that in 2018, police could have relied on such a warrant in “good faith,” because geofence technology was novel, and police reached out to other agencies with more experience for guidance. This means that the evidence they obtained will not be suppressed in this case.</p>
<p>Nevertheless, it is gratifying to see an appeals court recognize the fundamental invasions of privacy created by these warrants and uphold our constitutional tradition prohibiting general searches. Police around the country have increasingly relied on geofence warrants and other reverse warrants, and this opinion should act as a warning against narrow applications of Fourth Amendment precedent in these cases.</p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built an animated 3D bookshelf for ebooks (167 pts)]]></title>
            <link>https://github.com/mawise/bookshelf</link>
            <guid>41227350</guid>
            <pubDate>Mon, 12 Aug 2024 17:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mawise/bookshelf">https://github.com/mawise/bookshelf</a>, See on <a href="https://news.ycombinator.com/item?id=41227350">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">3D Bookshelf</h2><a id="user-content-3d-bookshelf" aria-label="Permalink: 3D Bookshelf" href="#3d-bookshelf"></a></p>
<p dir="auto">Inspired by [<a href="https://scastiel.dev/animated-3d-book-css" rel="nofollow">https://scastiel.dev/animated-3d-book-css</a>] and [<a href="https://github.com/janeczku/calibre-web">https://github.com/janeczku/calibre-web</a>]</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mawise/bookshelf/master/demo.gif"><img src="https://raw.githubusercontent.com/mawise/bookshelf/master/demo.gif" alt="" data-animated-image=""></a></p>
<p dir="auto">This is a 3D bookshelf to browse ebooks. It pulls ebook metadata and cover art from a <a href="https://calibre-ebook.com/" rel="nofollow">Calibre</a> library.  It uses the cover image aspect ratio to determine book height, all books are the same width.  It uses page-count data (if available) to determine the thickness of the book.  The Calibre <code>comment</code> metadata shows up as the back-cover text along with a book download link and page count.</p>
<p dir="auto">Special thanks to <a href="https://www.brandonsanderson.com/" rel="nofollow">Brandon Sanderson</a> and <a href="https://pluralistic.net/" rel="nofollow">Cory Doctorow</a> who publish their books without DRM, and to <a href="https://standardebooks.org/" rel="nofollow">Standard Ebooks</a> and <a href="https://www.planetebook.com/" rel="nofollow">Planet Ebook</a> for beautifully typeset public domain ebooks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="bundle install
ruby app.rb <path-to-calibre-library>"><pre>bundle install
ruby app.rb <span>&lt;</span>path-to-calibre-library<span>&gt;</span></pre></div>
<p dir="auto">Optionally: Install the <a href="https://github.com/kiwidude68/calibre_plugins/tree/main/count_pages">count pages</a> Calibre plugin to make the books variable-width based on estimated page counts.  Configure the plugin with a custom column named <code>pagecount</code> and the app will automatically discover any page-count data.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC seek comments on NextNav petition for rulemaking on lower 900MHz ISM band (123 pts)]]></title>
            <link>https://docs.fcc.gov/public/attachments/DA-24-776A1.txt</link>
            <guid>41226802</guid>
            <pubDate>Mon, 12 Aug 2024 17:08:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.fcc.gov/public/attachments/DA-24-776A1.txt">https://docs.fcc.gov/public/attachments/DA-24-776A1.txt</a>, See on <a href="https://news.ycombinator.com/item?id=41226802">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Tim Sweeney: " Now Apple is demanding a 30% cut of all Patreon DONATIONS (178 pts)]]></title>
            <link>https://twitter.com/TimSweeneyEpic/status/1823027135784558842</link>
            <guid>41226754</guid>
            <pubDate>Mon, 12 Aug 2024 17:04:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/TimSweeneyEpic/status/1823027135784558842">https://twitter.com/TimSweeneyEpic/status/1823027135784558842</a>, See on <a href="https://news.ycombinator.com/item?id=41226754">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Repair and Remain: How to do the slow, hard, good work of staying put (452 pts)]]></title>
            <link>https://comment.org/repair-and-remain/</link>
            <guid>41226039</guid>
            <pubDate>Mon, 12 Aug 2024 16:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://comment.org/repair-and-remain/">https://comment.org/repair-and-remain/</a>, See on <a href="https://news.ycombinator.com/item?id=41226039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<div>
				
				
				
				
				<p><h2>I</h2></p>
			</div><div><p>I’ve never had anything like a real career, only a long and varied string of jobs. I grew up working on the family farm, and then had jobs as a roofer, a groundskeeper at a rural hospital, and a mineral-bagging-machine operator in an unheated feed mill one frigid Manitoba winter. I spent a year as a photographer and store manager in a tiny portrait studio just as digital cameras were beginning to consign film cameras to obsolescence. I worked for three years as a barista at one of Vancouver’s top-rated independent coffee shops. I’ve been a magazine editor, a sessional lecturer in a couple of liberal arts schools, a glazier’s assistant, a mason tender, a plumber’s labourer, and a daycare worker. One winter I lived in a simple little cabin—no plumbing, no electricity—and I made homemade soap over a wood stove and sold it at craft sales. In my twenties and thirties I spent many of my summers planting close to half a million trees on countless logging clear-cuts between Hyder, Alaska, and Dryden, Ontario.</p>
<p>And for twelve years now I’ve had a hybrid operation, juggling a one-man autodidact home-repair business and part-time lay ministry at a little Anglican church in Winnipeg. My basic MO in both roles is simple: repair and remain.</p>
<p>I don’t have the know-how to build you a brand-new house, but I can help fix pretty much anything in your old one. If you do, in fact, need a new house, I’ll send you to Francesco or Myron, or James and Fiona, all of them trustworthy builders and fine people. Odds are the house you’re in right now needs a few updates and minor upgrades, and I’d be happy to help with whatever you need done: add some new windows, open up some walls, replace the old basement stairs, tile the backsplash. Repair and remain.</p>
<p>Same with pastoring: no point thinking you need a brand-new life, but, well, let’s not kid around—you could use some serious updates and upgrades yourself.</p>
<p>Let’s say time comes to gut and renovate your bathroom: I can help you with that—demolition, framing, reworking the plumbing, moving some electrical, installing some mould-resistant drywall, maybe some nice tile for the floor and some classic glazed ceramic three-by-six subway tile for the tub surround. Should take a month or two, depending on what all’s involved. And as for you, hey, for the sake of your wife and kids, I think you better quit the flurry of furtive late-night texts to the sexy young co-worker and cut back a bit on your recreational drinking because wine is a mocker, so goes the proverb, as if those Facebook posts of you at the bar last week weren’t proof enough.</p>
<p>Repair and remain. Work with what you’ve got. Sit still for a moment, take stock, make some changes. Big changes, if necessary.</p></div>
			</div><div>
				
				
				
				
				<p><h3>Repair and remain. Work with what you’ve got. Sit still for a moment, take stock, make some changes. Big changes, if necessary.</h3></p>
			</div><div><p>David and Ruth called me once to unclog their bathroom sink. Someone had dropped a nail clipper into it a decade ago, but now the drain was rusted and when I went to loosen the nut, the steel sink cracked and split, but it was an old sink so I couldn’t find a matching one to replace it with, so that meant the old vanity had to go too, but that left an odd footprint on the curled, old linoleum, so then the flooring had to go too, and, well, if you’re going that far, you might as well put in a new tub. And so on. You get the picture. Renominoes. In the end, a house call to help deal with a bathroom sink with a nail clipper jammed in it led to six weeks’ work and a bill in the teens with three zeros.</p>
<p>Last year, in the middle of the pandemic, a man I haven’t seen in more than fifteen years called me up to weep on the phone because he was having a difficult time loving his kids. He had started to feel resentment toward them because, he said, the kids had taken so much away from him he barely knew who he was anymore. “I called you because I knew you’d be gentle with me,” he said. That I can do.</p>
<p>Six years ago a guy I barely knew cornered me after church and asked if I could meet him for breakfast, and when we met, he told me he was <em>this </em>close to walking out on his wife and kids. Since then, he and I have been meeting up every couple of months or so. Last year he told me he wouldn’t still be married if it weren’t for all those conversations over greasy bacon and eggs over easy. Well and good, I say, but the truth is he’s the one doing the hard work. He’s the one who’s got to live his life. All I have to do is buy breakfast, and sit, and listen.</p>
<p>Repair and remain.</p>
<p>That’s how I work, and it’s what I advise. I don’t know how things are going to turn out in your life or in your marriage or with your kids. Nobody does. Maybe it will all get a whole lot worse, who’s to say. But a brand-new house won’t fix your troubles any more than a fresh start with a fascinating new somebody will. Don’t tell me; I already know it would be easier to just cut and run, because I know how hard it is to live with other people, four of whom are also stuck having to live with brooding, melancholy me. I have planted spruce saplings on the steep, thorny, overgrown slopes of the Rocky Mountains in snowstorms in June. I once heaved a three-hundred-pound cast-iron tub up and out the second-story window of an old house. And when I worked for a bricklayer, he and I took down a concrete-and-rebar-reinforced cinderblock wall with sledgehammers. But this—doing my best to be a loving husband and father in the trauma and tedium of the day-to-day—is without question the hardest thing I’ve ever done.</p></div><div><p>Over the past dozen years I have had hundreds of pastoral conversations, mostly with young men, about the challenges of family life. They tell me it’s exhausting, that there’s no more free time, that they’re having a hard time setting aside their dreams and wishes, that kids can be unbearably frustrating. I get it. They tell me that the marriage isn’t what it used to be, that they don’t really have anything in common anymore, that the passion’s gone, that she isn’t who she used to be, that the sex isn’t what it used to be, that they’re tired of all of it. I sip my coffee and nod in agreement with every word. I understand. I feel it too. It’s the same at my house. Marriage is hard.</p>
<p>But when they say, “I’m thinking of leaving,” I think, Now hang on a sec. You had me right up to that last bit. Fine: you’ve changed; she’s changed; life has changed. And the kids—well, they’ve disrupted, interrupted, confronted, confounded, and otherwise fundamentally altered <em>everything</em>. All very, very hard. And yes, sometimes it feels impossible. I know what it’s like to feel trapped, and my wife undoubtedly knows what it’s like to feel trapped, because she’s stuck with me, <a href="https://comment.org/varieties-of-the-noonday-demon/">the more irritable and moody ingredient</a> in our marriage. But you’re thinking of leaving? What is that going to fix?</p>
<p>We have, all of us and to varying degrees, been duped by the sales pitches, the flashing cascade of advertisements traipsing through the sidebar. That jam-packed flow of ads is full of shiny new things, new techniques, new experiences that promise to finally alleviate the so-far insatiable, burning, lonely, primordial ache. Bono laments, “I still haven’t found what I’m looking for.” Springsteen cries out, “Everybody’s got a hungry heart.” k.d. lang bemoans the “constant craving.” Augustine says, “Our hearts are restless.”</p>
<p>I used to blame advertisers for that restlessness and dissatisfaction, but I don’t think that’s right. We were already restless; we always have been. The advertisers just figured out how to nurture, tend, exacerbate, and capitalize on the pre-existing condition, that innate restlessness, promising that something new is going to set all to rights. When the flashing sidebar connects that hand lotion, those hiking boots, a beach vacation, or some rugged SUV with satisfaction, joy, and inner peace, it sure <em>feels</em> like we’d be suckers <em>not </em>to buy it. And when that thing inevitably disappoints, we hardly even notice. There’s always something new to buy. That narrative of elusive satisfaction isn’t just something we’re repeatedly being told; it is a story we’re literally buying into all the time. No surprise, then, that when our beloved to whom we once upon a time “pledged our troth” inevitably disappoints, we start thinking it might be time to get a new beloved.</p></div><div>
				
				
				
				
				<p><h3>That narrative of elusive satisfaction isn’t just something we’re repeatedly being told; it is a story we’re literally buying into all the time.</h3></p>
			</div><div><p>I have come to think that renovation work is not inherently a sign of fashion-driven, bourgeois, consumerist excess; that beauty is not superfluous; and that a good renovation is a good investment. Taking care of your house is a wise and pragmatic thing to do. The integrity of a house means that all the parts and systems work as a whole, from ridge cap to footing and everything in between. Roof trusses, studs, joists, shiplap, plumbing supply and waste, eaves, windows, flooring, faucets, switches: your house will function as a house when it is well built and well maintained. Integrity of form, function, usability, and beauty. If it’s poorly made, or when it starts to fall apart, the integrity of the whole thing suffers. Give it enough time and a leak in the roof or a leak from a drain will ruin the whole thing.</p>
<p>If you ignore the little things long enough, something as small as a nail clipper can make for two days of demolition and a trailer filled with an old sink, outdated vanity, faded linoleum, some lath and plaster, old plumbing, a thirsty old toilet, and so on. I can haul those few thousand pounds of junk to the landfill and rebuild your bathroom. But in the end, when it’s all put back together again, what you have will still be the spot to do the same basic grooming and human-waste disposal. Pay attention and mind the details and you save yourself a lot of hassle and money. That slow corrosion that comes if you ignore the small, nagging troubles of your life has the potential to wreck a family the way a nail clipper can wreck a bathroom. And somebody’s going to pay for it, even if it isn’t you. Mostly it will be the kids, plus the ongoing emotional and spiritual costs divvied up among the friends, family, and community who witnessed your vows, who backed you as you struggled along, who loved you then and still love you now.</p>
<p>Because however it may sometimes seem that circumstance, fortune, and your exasperating spouse are conspiring to sabotage your happiness and peace of mind, the one certain, irrefutable common factor in all your circumstances is you. You are the bearer and carrier of grief, disappointment, frustration, and heartache, just as you are also the source of much of the same. So it goes. I’ve said it more than once to some guy across the table who tells me he’s planning to leave his marriage: You should stay. Sit in the awful, agonizing sorrow of it all, and figure some things out. Your life is very hard. I know you’ve thought it through more than I can imagine; I know you’ve calculated the cost-benefit, weighed your options; and all that is fine and good. There is no way of knowing how this will play out in your very real life. Nobody can predict the future. Something has to give, yes. But it doesn’t need to be this. <a href="https://ifstudies.org/blog/should-couples-in-unhappy-marriages-stay-together" target="_blank" rel="noopener">I think you should stay</a>.</p>
<p>It’s a tough sell. I understand, because my undisciplined imagination, formed like everyone else’s by countless half-minute ads and building-sized billboards, frolics among fantastic, glamorous possibilities of something other than what I’ve already got. It’s a cornucopia of options, with countless cathedrals and priests promising salvation at the marketplace, be it a new app, new phone, new car, new house, new job, new city, or new spouse. The promise is always the same: <em>this </em>thing will make you happy. Never mind trying to fix what you’ve got. Just get a new one and start over.</p>
<p>Repair and remain sounds simple because it is. But simple is not the same as easy. “For better, for worse,” we say, and everyone likes to stay when it’s the better. But staying through the worse—that’s the whole point of the vow, for Christ’s sake.</p></div><div>
				
				
				
				
				<p><h3>Repair and remain sounds simple because it is. But simple is not the same as easy.</h3></p>
			</div><div><p>Mostly they do what they’ve already decided to do, and they leave. My track record for counselling couples to stick it out is pretty poor. I still think the better part of wisdom says stay. Endure. Wrestle. Suffer. Struggle. Keep working. Your heart is restless, my heart is restless, all our hearts are restless, “until they find their rest in Thee”—a rest that may well be found in full only after our death. So be it. Until then: stay.</p>
<p>Repair and remain.</p>
<p>Repair and remain.</p>
<p>Repair and remain.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple's requirements are about to hit creators and fans on Patreon (793 pts)]]></title>
            <link>https://news.patreon.com/articles/understanding-apple-requirements-for-patreon</link>
            <guid>41224853</guid>
            <pubDate>Mon, 12 Aug 2024 14:34:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.patreon.com/articles/understanding-apple-requirements-for-patreon">https://news.patreon.com/articles/understanding-apple-requirements-for-patreon</a>, See on <a href="https://news.ycombinator.com/item?id=41224853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article data-fetch-key="data-v-0d6194c9:0" data-v-0d6194c9=""><section data-v-0accbd0e="" data-v-0d6194c9=""><h2 data-v-0accbd0e="">Apple’s requirements are about to hit creators and fans on Patreon. Here’s what you need to know.</h2> <p data-v-0accbd0e="">
    August 12, 2024
  </p> <!---->  <p data-sz="md" data-v-0accbd0e="">Apple is requiring that Patreon switch to their iOS in-app purchase system starting this November, or risk being removed from the App Store. Here’s what’s coming, and what you can do about it.</p> <hr data-v-0accbd0e=""></section> <div data-v-0d6194c9=""><div block-id="528" id="article-blade-528"><p>Patreon exists to let creators make what they love for their biggest fans, and earn an income doing it on their own terms. Everything we build is to make those two things easier for creators. For <strong><em>you</em></strong>.</p>
<p>Unfortunately, Apple is requiring us to switch over to their in-app purchase system for all iOS transactions or else risk being kicked out of the App Store altogether – and their in-app purchase system is not built with our same level of creator-first flexibility.<br>
In situations like this — where a partner is forcing us into a change that we don't believe is best for creators — we use these three principles to guide our decisions:</p>
<ol>
<li><strong>Transparency</strong> – Communicating to you early and clearly to help you understand exactly what is happening and what your options are.</li>
<li><strong>Control</strong> – Giving you a clear understanding of the tradeoffs, so that you can decide what’s right for your business.</li>
<li><strong>Stability</strong> – Making sure that you’re able to maintain your earnings and continue to grow your business throughout this process.</li>
</ol>
</div><div block-id="529" id="article-blade-529"><h4><strong>Apple’s requirements</strong></h4>
<p>As we<a href="https://news.patreon.com/articles/an-update-on-commerce-purchases-in-the-ios-app/" target="_blank" rel="noreferrer noopener"> first announced last year</a>, Apple is requiring that Patreon use their in-app purchasing system and remove all other billing systems from the Patreon iOS app by November 2024.</p>
<p>This has two major consequences for creators:</p>
<ol>
<li>Apple will be applying their <strong>30% App Store fee to all new memberships purchased in the Patreon iOS app</strong>, in addition to anything bought in your Patreon shop.</li>
<li>Any creator currently on first-of-the-month or per-creation billing plans will have to <strong>switch over to subscription billing to continue earning in the iOS app</strong>, because that’s the only billing type Apple’s in-app purchase system supports.</li>
</ol>
<p>Before we go any further, we want to be crystal clear about one thing: <strong>Apple’s fee will not impact your existing members</strong>. It will only affect new memberships purchased in the iOS app from November onward.</p>
<p>Here’s everything you need to know to navigate this change in the least disruptive way possible.</p>
</div><div block-id="530" id="article-blade-530"><h4><strong>Navigating Apple’s fee</strong></h4>
<p>We’ve been working closely with creators to figure out the best way to help you avoid earnings disruptions stemming from Apple’s 30% App Store fee.</p>
<p>Based on creator feedback, we've built <strong>an optional tool that can automatically increase your prices — only in the iOS app — to offset the cost of Apple's fee</strong>. This way, you’ll continue to earn at least the same amount per membership as you do on all other platforms.</p>
<p>That said, you deserve the chance to decide whether that’s something you want. So, while the automatic price increase is the default option, you also have the choice to keep your prices the same and pay the 30% fee from your earnings. We don’t recommend this, because it means you’d earn less per membership on in-app iOS transactions – but ultimately we believe it's important to give you agency to make your own decisions.</p>
<p>Obviously, neither of these solutions are ideal. But remember, Apple’s fees are only in the iOS app. Your prices on the web and the Android app will remain completely unaffected. You can always send your fans to <a href="https://support.patreon.com/hc/articles/27992185537037-How-iOS-in-app-purchases-works-for-fans" target="_blank" rel="noreferrer noopener">this Help Center article</a> which explains the iOS in-app fees relative to other platforms, so they can better understand the implications of where they choose to make their purchases.</p>
<p>In any case, the decision you end up making doesn’t have to be final: you can change how you apply the fee at any time. For a breakdown of fees and earnings check out our <a href="https://support.patreon.com/hc/en-us/articles/27991664769677-How-iOS-in-app-purchases-work-on-Patreon" target="_blank" rel="noreferrer noopener">Help Center article</a>.</p>
</div><div block-id="531" id="article-blade-531"><h4><strong>Navigating Apple’s billing requirement</strong></h4>
<p>Patreon is home to an incredible range of creators, all with unique circumstances and billing needs. Apple’s in-app purchase system, on the other hand, <strong>only supports Patreon’s subscription billing model</strong>. Apple has also made clear that if creators on Patreon continue to use unsupported billing models or disable transactions in the iOS app, we will be at risk of having the entire app removed from their App Store.</p>
<p>Apple’s billing mandates mean that if you’re on a first-of-the-month or per-creation billing model, your membership will no longer be available to fans in the iOS app starting this November until you switch over to subscription billing.</p>
<p>As a result of Apple’s mandates and in order to make sure that you can continue getting new members in the iOS app, <strong>we've started a 16-month-long migration process to bring all creators onto subscription billing by November 2025</strong>, supported by a <a href="https://support.patreon.com/hc/articles/27992241435533-Patreon-will-support-with-migration-to-subscription-billing" target="_blank" rel="noreferrer noopener">roadmap</a> of new features and tools to make sure the billing model works for you, your community, and your business. To be clear, this means that first-of-the-month and per-creation billing models will be discontinued in November 2025.</p>
<p>Most creators on Patreon use subscription billing. Over the past few years, we’ve slowly rolled it out, tackling each hurdle that has come up to ensure that the migration is not disruptive for creators. That’s the way we like to roll out products. Unfortunately, because of Apple’s timelines and constraints, we can’t continue to do it this way. Instead of helping creators move to subscription billing if and when they feel like it’s right for them, we’re now forced to migrate all creators on Apple’s timeline.</p>
<p>How this change will affect you depends on your current billing model. If you’re not sure what billing model you’re on, you can find out in <a href="https://www.patreon.com/settings-creator/payout" target="_blank" rel="noreferrer noopener">your settings</a>.</p>
<ul>
<li><strong>Subscription billing</strong> – You do not have to make any changes to your billing model and can adjust how you handle Apple's fees at any time.</li>
<li><strong>First-of-the-month billing</strong> – In order to let you keep earning in the iOS app, we are going to automatically switch you over to subscription billing beginning in November 2024. However, if you’d like more time, you can delay your migration in your settings to keep your current billing model until November 2025. To be clear: even if you delay your migration, Apple’s requirements still apply. If you have not switched to subscription billing by this November, your fans will not be able to purchase new memberships in the iOS app. If you delay the migration but then change your mind, you can migrate to subscription billing at any time in <a href="https://www.patreon.com/settings-creator/payout" target="_blank" rel="noreferrer noopener">your settings</a>.</li>
<li><strong>Per-creation billing</strong> – Because this is such a big change to how you run your business, our Product Support will offer 1-1 support to help you migrate, which takes more time. You will be able to keep your current billing model until November 2025, at which point you’ll be migrated to subscription billing automatically. However, Apple’s requirements still take effect in November 2024. If you have not switched to subscription billing by this November, your fans will not be able to purchase new memberships in the iOS app. You can start the migration process in <a href="https://www.patreon.com/settings-creator/payout" target="_blank" rel="noreferrer noopener">your settings</a>.</li>
</ul>
</div></div>  <!----></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postgres.new (174 pts)]]></title>
            <link>https://postgres.new/</link>
            <guid>41224749</guid>
            <pubDate>Mon, 12 Aug 2024 14:25:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://postgres.new/">https://postgres.new/</a>, See on <a href="https://news.ycombinator.com/item?id=41224749">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><div><div><div data-state="active" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-:Rbjtaja:-trigger-diagram" id="radix-:Rbjtaja:-content-diagram" tabindex="0" dir="ltr"><div><div data-testid="rf__wrapper"><svg style="position:absolute;width:100%;height:100%;top:0;left:0" data-testid="rf__background"><pattern id="pattern-1undefined" x="0" y="0" width="32" height="32" patternUnits="userSpaceOnUse" patternTransform="translate(-0.5,-0.5)"><circle cx="0.5" cy="0.5" r="0.5" fill="hsl(var(--muted-foreground)/.5)"></circle></pattern><rect x="0" y="0" width="100%" height="100%" fill="url(#pattern-1undefined)"></rect></svg><div><svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 2v4"></path><path d="m16.2 7.8 2.9-2.9"></path><path d="M18 12h4"></path><path d="m16.2 16.2 2.9 2.9"></path><path d="M12 18v4"></path><path d="m4.9 19.1 2.9-2.9"></path><path d="M2 12h4"></path><path d="m4.9 4.9 2.9 2.9"></path></svg><p>Loading schema...</p></div></div><div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="m15.5 7.5 2.3 2.3a1 1 0 0 0 1.4 0l2.1-2.1a1 1 0 0 0 0-1.4L19 4"></path><path d="m21 2-9.6 9.6"></path><circle cx="7.5" cy="15.5" r="5.5"></circle></svg>Primary key</li><li><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg>Identity</li><li><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M12 10a2 2 0 0 0-2 2c0 1.02-.1 2.51-.26 4"></path><path d="M14 13.12c0 2.38 0 6.38-1 8.88"></path><path d="M17.29 21.02c.12-.6.43-2.3.5-3.02"></path><path d="M2 12a10 10 0 0 1 18-6"></path><path d="M2 16h.01"></path><path d="M21.8 16c.2-2 .131-5.354 0-6"></path><path d="M5 19.5C5.5 18 6 15 6 12a6 6 0 0 1 .34-2"></path><path d="M8.65 22c.21-.66.45-1.32.57-2"></path><path d="M9 6.8a6 6 0 0 1 9 5.2v2"></path></svg>Unique</li><li><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M2.7 10.3a2.41 2.41 0 0 0 0 3.41l7.59 7.59a2.41 2.41 0 0 0 3.41 0l7.59-7.59a2.41 2.41 0 0 0 0-3.41l-7.59-7.59a2.41 2.41 0 0 0-3.41 0Z"></path></svg>Nullable</li><li><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="currentColor" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"><path d="M2.7 10.3a2.41 2.41 0 0 0 0 3.41l7.59 7.59a2.41 2.41 0 0 0 3.41 0l7.59-7.59a2.41 2.41 0 0 0 0-3.41l-7.59-7.59a2.41 2.41 0 0 0-3.41 0Z"></path></svg>Non-Nullable</li></ul></div></div><div><p><a href="https://github.com/supabase-community/postgres-new" target="_blank" rel="noopener noreferrer">Learn about postgres.new</a></p></div></div><div><div><form></form><p>AI can make mistakes. Check important information.</p></div></div></div><p>Please connect from a laptop or desktop to use postgres.new.</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: PGlite – in-browser WASM Postgres with pgvector and live sync (353 pts)]]></title>
            <link>https://pglite.dev/</link>
            <guid>41224689</guid>
            <pubDate>Mon, 12 Aug 2024 14:20:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pglite.dev/">https://pglite.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=41224689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-4d2e0cc9="" data-v-b447a6fc="" id="VPContent" data-v-52b0c991=""><!--[--><!--]--><div data-v-70d6fb81="" data-v-b447a6fc=""><div data-v-70d6fb81=""><!--[--><!--]--><!--[--><p data-v-70d6fb81="">Embeddable Postgres</p><p data-v-70d6fb81="">Run a full Postgres database locally in your app with reactivity and server sync</p><!--]--><!--[--><!--]--><div data-v-70d6fb81=""><!--[--><div data-v-70d6fb81=""><p><a href="https://pglite.dev/docs/" data-v-70d6fb81="" data-v-321d5de2="">Get Started</a></p></div><div data-v-70d6fb81=""><p><a href="https://github.com/electric-sql/pglite" target="_blank" rel="noreferrer" data-v-70d6fb81="" data-v-321d5de2="">Star on GitHub</a></p></div><!--]--></div><!--[--><!--]--></div><div data-v-f0e325c8="" data-v-70d6fb81=""><p><img src="https://pglite.dev/img/brand/icon-light.svg" data-v-f0e325c8=""></p><div data-v-f0e325c8=""><!--[--><p><img src="https://pglite.dev/img/brand/icon-light.svg" data-v-f0e325c8=""></p><p><img src="https://pglite.dev/img/brand/icon-light.svg" data-v-f0e325c8=""></p><p><img src="https://pglite.dev/img/brand/icon-light.svg" data-v-f0e325c8=""></p><p><img src="https://pglite.dev/img/brand/icon-light.svg" data-v-f0e325c8=""></p><p><img src="https://pglite.dev/img/brand/icon-light.svg" data-v-f0e325c8=""></p><!--]--></div></div></div><!--[--><!--]--><!--[--><!--]--><div data-v-97e2a943="" data-v-b447a6fc=""><!--[--><div data-v-97e2a943="" data-v-b5aa6769=""><!--[--><article data-v-b5aa6769=""><!----><h2 data-v-b5aa6769="">Lightweight</h2><p data-v-b5aa6769="">A complete WASM build of Postgres that's under 3MB Gzipped.</p><!----></article><!--]--></div><div data-v-97e2a943="" data-v-b5aa6769=""><!--[--><article data-v-b5aa6769=""><!----><h2 data-v-b5aa6769="">Extendable</h2><p data-v-b5aa6769="">Dynamic extension loading mechanism, including support for pgvector and PostGIS.</p><!----></article><!--]--></div><div data-v-97e2a943="" data-v-b5aa6769=""><!--[--><article data-v-b5aa6769=""><!----><h2 data-v-b5aa6769="">Reactive</h2><p data-v-b5aa6769="">Built in support for data loading, synchronisation and live query primitives.</p><!----></article><!--]--></div><!--]--></div><!--[--><!--]--><div data-v-29672710="" data-v-b447a6fc="" data-v-45370b1f=""><div data-v-29672710=""><div data-v-29672710=""><h3 data-v-29672710="">Experience PGlite with <a href="https://postgres.new/" data-v-29672710="">postgres.new</a></h3><p data-v-29672710=""> Create and publish a Postgres database using AI<br data-v-29672710=""> build on PGlite by <a href="https://supabase.com/" data-v-29672710="">Supabase</a></p></div><video controls="" poster="https://static.pglite.dev/videos/postgres-new-showcase-loop.png" data-v-29672710=""><source src="https://static.pglite.dev/videos/postgres-new-showcase-loop-1080p.mp4" type="video/mp4" data-v-29672710=""></video><p><a href="https://postgres.new/" data-v-29672710="">What would you like to create?</a></p></div><div data-v-29672710=""><div data-v-29672710=""><h3 data-v-29672710="">Try PGlite Now</h3><p data-v-29672710=""> This is a full PGlite Postgres running in your browser<br data-v-29672710=""> It even includes <a href="https://pglite.dev/extensions/#pgvector" data-v-29672710="">pgvector</a>!</p></div><!----><p><a href="https://pglite.dev/repl" data-v-29672710="">Try more extensions in the playground</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postgres.new: In-browser Postgres with an AI interface (192 pts)]]></title>
            <link>https://supabase.com/blog/postgres-new</link>
            <guid>41224286</guid>
            <pubDate>Mon, 12 Aug 2024 13:43:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://supabase.com/blog/postgres-new">https://supabase.com/blog/postgres-new</a>, See on <a href="https://news.ycombinator.com/item?id=41224286">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p><img alt="postgres.new: In-browser Postgres with an AI interface" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 100vw, (max-width: 1200px) 50vw, 33vw" srcset="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=256&amp;q=100 256w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=384&amp;q=100 384w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=640&amp;q=100 640w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=750&amp;q=100 750w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=828&amp;q=100 828w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=1080&amp;q=100 1080w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=1200&amp;q=100 1200w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=1920&amp;q=100 1920w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=2048&amp;q=100 2048w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=3840&amp;q=100 3840w" src="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flw12%2Fday-1%2Fpostgres-new-thumb.png&amp;w=3840&amp;q=100"></p><p>Introducing <a href="https://postgres.new/">postgres.new</a>, the in-browser Postgres sandbox with AI assistance. With postgres.new, you can instantly spin up an unlimited number of Postgres databases that run directly in your browser (and soon, deploy them to S3).</p>
<p>Each database is paired with a large language model (LLM) which opens the door to some interesting use cases:</p>
<ul>
<li>Drag-and-drop CSV import (generate table on the fly)</li>
<li>Generate and export reports</li>
<li>Generate charts</li>
<li>Build database diagrams</li>
</ul>
<p>All while staying completely local to your browser. It's a bit like having Postgres and ChatGPT combined into a single interface:</p>
<p><iframe src="https://www.youtube-nocookie.com/embed/ooWaPVvljlU" title="I gave AI full control over my database (postgres.new)" allow="accelerometer; autoplay; clipboard-write; encrypted-media; fullscreen; gyroscope; picture-in-picture; web-share"></iframe></p>
<p>In this demo we cover several interesting use-cases:</p>
<ul>
<li>You have a CSV file that you want to quickly query and visualize. You could load it into Excel, but you're SQL-savvy and really just wish you could query it like a database.</li>
<li>You're asking ChatGPT to help you write some SQL, but ideally you'd like to run it against a real database to know if it's correct. LLMs aren't perfect after all.</li>
<li>You're building your next side project and it's time to plan your database. You've done this 1000 times before and wish you could just “describe” what you want and have AI do the work creating ER diagrams and SQL migrations.</li>
</ul>

<p>All queries in postgres.new run directly in your browser. There's no remote Postgres container or WebSocket proxy.</p>
<p>How is this possible? The star of the show is <a href="https://pglite.dev/">PGlite</a>, a WASM version of Postgres that can run directly in your browser. Our friends at <a href="https://electric-sql.com/">ElectricSQL</a> released PGlite a few months ago after discovering a way to compile the real Postgres source to Web Assembly (more on this later).</p>

<p>There are a few things we wanted to achieve with postgres.new:</p>
<ol>
<li><strong>AI-driven development:</strong> We wanted to re-imagine the interaction between Postgres and AI. This gives a lot of leniency for making mistakes, which AI sometimes make (and let's face it: developers too).</li>
<li><strong>Postgres sandboxing:</strong> we're big fans of sandboxes and notebooks. Since PGlite runs in the browser, it feels fast and disposable. You can spin up “a million little elephants” for doing data analysis using the same Postgres interface that we're familiar with in our daily development.</li>
<li><strong>Extremely cheap databases:</strong> we're always looking for ways to offer developers <em>more databases for cheaper</em>. PGlite is still nascent, but we can see its potential for spinning up millions of cheap databases that can be stored and read from S3. This covers a variety of use cases that we see in the Supabase community.</li>
</ol>

<p>So what exactly can you do with postgres.new? How do these work under the hood?</p>
<h3 id="ai-assistant">AI assistant</h3>
<p>We pair PGlite with a large language model (currently GPT-4o) and give it full reign over the database with no restricted permissions or confirmations required from the user. This is actually an important detail - and has opened new doors that other AI + Postgres tools struggle with.</p>
<p>As an analogy, the most helpful team members are those that can do their work without constant micromanagement. They only come ask for help when they're really stuck or need a second opinion.</p>
<p>Giving an AI model full autonomy over the database means that it can run multiple operations back-to-back without delay. It makes AI feel even more human-like and useful. A disposable in-browser database is what really makes this possible since there's no need to worry about data loss.</p>
<h3 id="csv-imports-and-exports">CSV imports and exports</h3>
<p>Drag-and-drop a CSV file directly onto the chat to instantly receive a new table with the data automatically imported into it. The language model will scan the CSV's header and a few sample rows to decide which data types to use for each column:</p>

<p>Just like humans though, AI won't always get this right. There could have been a row of data it missed that didn't conform to the same data types that it expected, causing the import to fail. To solve this, we added the ability for AI to self-heal. Any SQL errors from Postgres are fed back to the language model so that it can try a few more attempts at solving the problem. This behaviour is triggered anytime AI executes SQL, not just CSV imports.</p>
<p>In addition to imports, you can ask AI to export any query to a CSV. This is useful if you wanted to quickly generate a few reports on a dataset then continue using that data in another program.</p>
<h3 id="charts">Charts</h3>
<p>Charts are a first-class feature within the chat. By simply adding the word “chart” (or similar) to your message, AI will execute the appropriate query using SQL then build a chart representing that data:</p>

<p>The goal is to make data visualization as fast as possible. You can generate everything you need from a single chat request rather than the usual steps of loading your CSV into Excel, tweaking the data, then navigating through the chart tools.</p>
<p>Under the hood we render these charts using <a href="https://github.com/chartjs/Chart.js">Chart.js</a>, one of the more mature charting libraries available in JavaScript. The choice a Chart.js was largely influenced by the language model (GPT-4o) which has a pretty good understanding of its syntax and configuration. The model will simply translate the SQL output to the equivalent Chart.js syntax, then render it onto the page. A nice side-effect from this is that you can ask AI to adjust the chart's type, colors, axises, title, or anything else you want to get it to render exactly as you wish, as long as Chart.js supports the feature you are requesting.</p>
<p>It's worth noting that Chart.js sometimes expects an inputs that can be a bit verbose, adding to cost and latency. In the future we'd like to experiment with other charting options that take a more terse input.</p>
<h3 id="er-diagrams-and-migrations">ER diagrams and migrations</h3>
<p>Usually ER diagrams are created before you write any SQL. After all, why get caught up in SQL syntax when you really only care about capturing your app's data and relationship requirements?</p>
<p>But with AI, this workflow shifts a bit. It's trivial for a language model to generate quality <code>CREATE</code> and <code>ALTER</code> statements in a matter of seconds. So why not let the model perform real DDL against a Postgres sandbox and simply generate the ER diagram based on these tables?</p>

<p>With this workflow, we can guarantee from the very beginning that the columns and relationships that we come up with can actually be implemented in a real database. If they can't, the database will just throw an error and AI will fix it. We then have the added bonus of accessing the real SQL code available when we're done, which can be copied over to our new app when we're ready:</p>

<p>Under the hood we use a browser-compatible version of <a href="https://github.com/supabase/postgres-meta">postgres-meta</a> to load PGlite tables into JavaScript, then render them using the <a href="https://supabase.com/blog/supabase-studio-3-0#schema-visualizer">schema visualizer</a>. For migrations, we scan through the chat history and concatenate all DDL-related SQL queries into a single view.</p>
<p>In the future, we would also like to support a “seeds” section that outputs <code>INSERT</code> statements for sample data created by the language model. Unfortunately we can't simply concatenate these queries together like we do with migrations, since table and column structure can change over time and break earlier seeds. To make this work properly we'll need something like a WASM version of <code>pg_dump</code> that can dump all data at the end (stay tuned - the ElectricSQL team is working on it!)</p>
<h3 id="semantic-search-and-rag">Semantic search and RAG</h3>
<p>ElectricSQL has been working hard to support real Postgres extensions in PGlite (compiled to WASM). One extension that was high on the priority list was <a href="https://github.com/pgvector/pgvector">pgvector</a> which enables in-browser vector search.</p>
<p>pgvector is enabled by default in postgres.new, meaning you can create and query vector columns on any table immediately. Of course, this is only useful if you have real embeddings to work with - so we gave AI access to <a href="https://github.com/xenova/transformers.js">Transformers.js</a> which allows you to generate text embeddings directly in the browser, then store/query them in PGlite.</p>

<p>Under the hood, we store the embeddings in a <code>meta.embeddings</code> table then pass back to AI the resulting IDs for each embedding. We do this because embedding vectors are big, and sending these back and forth to the model is not only expensive, but also error prone. Instead, the language model is aware of the <code>meta.embeddings</code> table and simply subqueries to it when it needs access to an embedding.</p>
<p>We're excited to see what people do with this tool. We've found it has provided a perfect sandbox for experimenting with semantic search and RAG in a low risk environment.</p>
<h3 id="deployments">Deployments</h3>
<p>With postgres.new we expect to have read-only deployments by the end of the week. This is important for one main reason: it's incredibly cheap to host a PGLite database in S3. While running a full Postgres database isn't <em>that</em> expensive, there are use-cases where developers would love <strong>most</strong> of the features of Postgres but without the cost of running a full database. PGLite, served via S3, will open the floodgates to many use-cases: a replicated database per user; read-only <a href="https://github.com/supabase/pg_replicate">materialized</a> databases for faster reads; search features hosted on the edge; maybe even a trimmed-down version of Supabase.</p>
<p>By itself PGlite is an embedded database which means you can't connect to it like a normal Postgres database via TCP connection. To support PGlite-backed deployments, we needed a way to recreate the TCP server component of Postgres and also parse real wire protocol messages so that people could connect to their database via any regular Postgres client.</p>
<p>This is how <a href="https://github.com/supabase-community/pg-gateway">pg-gateway</a> was born: a TypeScript library that implements the Postgres wire protocol from the server-side. It provides APIs you can hook into to handle authentication requests, queries, and other client messages yourself.</p>

<p>Under the hood, PGlite <em>does</em> support wire protocol messages (see more below), but only messages that you would see after the startup/auth handshake. So we designed pg-gateway handle the startup/TLS/authentication messages, then simply pass off future messages to PGlite.</p>
<p>There's a lot more juicy features we built into pg-gateway, but those will have to wait for its own blog post.</p>

<p>None of this would be possible without <a href="https://pglite.dev/">PGlite</a>, developed by our friends at <a href="https://electric-sql.com/">ElectricSQL</a>.</p>

<h3 id="what-is-pglite">What is PGlite?</h3>
<p><a href="https://pglite.dev/">PGlite</a> is a WASM (web assembly) build of Postgres packaged into a TypeScript/JavaScript client library. You can use it to run Postgres in the browser, Node.js, and Bun with no additional dependencies. This provides a number of use cases where it might be better than a full Postgres database:</p>
<ul>
<li><strong>Unit and CI testing:</strong> PGlite is very fast to start and tear down. It's perfect for unit tests - you can have a unique fresh Postgres for each test.</li>
<li><strong>Local development:</strong> You can use PGlite as an alternative to a full local Postgres for development; simplifying your development environments.</li>
<li><strong>Remote development, or local web containers:</strong> As PGlite is so lightweight it can be easily embedded into remote containerised development environments, or in-browser web containers</li>
</ul>
<h3 id="data-persistance">Data persistance</h3>
<p>PGlite supports multiple backends for data persistence:</p>
<ul>
<li>the native file system when used in Node</li>
<li>IndexedDB and OPFS when used in the browser</li>
</ul>
<p>It's fast, with CRUD style queries executing in under 0.3 ms. It's ideal storing local state in a web app.</p>
<h3 id="extension-support">Extension support</h3>
<p>PGlite supports a large <a href="https://pglite.dev/extensions/">catalog</a> of Postgres extensions. Here are two notable extensions that are useful in an embedded enviroment:</p>
<p><code>pgvector</code></p>
<p><a href="https://pglite.dev/extensions/#pgvector">pgvector</a> can be used for indexing and searching embeddings, typically as part of a AI workflow (retrieval augmented generation). As AI moves towards the user's device, performing vector searches close to the model is essential for reducing latency.</p>
<p><code>live</code></p>
<p>This is a new extension developed by <a href="https://electric-sql.com/">ElectricSQL</a> as a client for their sync engine. It can synchronize a subset of your Postgres database in realtime to a user's device or an edge service.</p>
<h3 id="a-technical-overview-of-pglite">A technical overview of PGlite</h3>
<p>Postgres normally runs under a multi-process forking model, each client connection is handed off to a child process by the postmaster process. However, in WASM there is no support for forking processes, and limited support for threads.</p>
<p>Fortunately, Postgres has a relatively unknown built-in <a href="https://www.postgresql.org/docs/current/app-postgres.html#APP-POSTGRES-SINGLE-USER">“single user mode”</a> that is both single-process, and single-threaded. This is primarily designed to enable bootstrapping a new database, or for disaster recovery.</p>
<p>PGlite builds on the single user mode by adding Postgres wire protocol support, as standard Postgres only supports a minimal basic cancel REPL in single user mode, this enables parametrised queries and converting between Postgres types and the host languages types.</p>
<p>There are a number of of other things in Postgres that PGlite modifies to enable its use with WASM. These include:</p>
<ul>
<li>Support for <a href="https://pglite.dev/docs/api#options">specifying a user</a> for the connection when starting under single-user mode, allowing permissions and RLS to be applied.</li>
<li>Support for <code>COPY</code>, for loading CSVs or Postgres binary copy formats into the database. This was achieved by creating a virtual device on the VFS, <a href="https://pglite.dev/docs/api#dev-blob"><code>/dev/blob</code></a>, which you can read and write to with the <code>COPY</code> command, and representing a JavaScript Blob object on the query.</li>
<li>The addition of <a href="https://pglite.dev/docs/api#dumpdatadir"><code>dumpDataDir</code></a> and <code>loadDataDir</code> enabling the dumping and loading a tarball of the Postgres datadir.</li>
<li><code>pg_notify</code> has been modified to run in single-process mode, creating the foundations for a <a href="https://pglite.dev/docs/live-queries">live reactive query interface</a> on top of <code>NOTIFY</code> and <code>LISTEN</code>.</li>
<li>A new <a href="https://pglite.dev/docs/filesystems#opfs-ahp-fs">OPFS (origin private filesystem) VFS</a> for browsers, providing better performance and support for databases significantly larger than can fit in memory.</li>
</ul>

<p>We like to ship early and often at Supabase, so there are a number of features in the backlog:</p>
<ul>
<li><strong>Deploy your database:</strong> we're adding the ability to deploy your database to S3 and access it from anywhere on the internet (read-only to start).</li>
<li><strong>Support for more file types:</strong> we plan to add support for Word docs, images (via image embeddings), and more.</li>
<li><strong>Database sharing:</strong> Just like CodeSandbox, you'll soon be able to share databases with others using a unique URL.</li>
<li><strong>PGlite OPFS support:</strong> databases are currently stored in IndexedDB and we'll move this to <a href="https://developer.mozilla.org/en-US/docs/Web/API/File_System_API/Origin_private_file_system">OPFS</a> to store files directly on the host filesystem.</li>
<li><strong>Database exporting:</strong> soon you'll be able to run a pg_dump of your database and restore it to any Postgres database.</li>
</ul>

<p>As always, the work that we've done is open source and permissively licensed (and is the work by the Electric team. Here are all the open source repos if you want to give them a star, add an issue, or cntribute some code yourself:</p>
<ul>
<li><a href="https://github.com/electric-sql/pglite">PGlite</a> (Apache 2.0): A WASM build of Postgres.</li>
<li><a href="https://github.com/supabase-community/pg-gateway">pg-gateway</a> (MIT): Postgres wire protocol for the server-side.</li>
<li><a href="https://github.com/supabase-community/postgres-new">postgres-new</a> (Apache 2.0): The frontend for <a href="https://postgres.new/">postgres.new</a>.</li>
<li><a href="https://github.com/xenova/transformers.js">transformers.js</a>: Run Transformers directly in your browser</li>
</ul></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD records its highest server market share in decades (230 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/cpus/amd-records-its-highest-server-market-share-in-decades-but-intel-fights-back-in-client-pcs</link>
            <guid>41224253</guid>
            <pubDate>Mon, 12 Aug 2024 13:40:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/cpus/amd-records-its-highest-server-market-share-in-decades-but-intel-fights-back-in-client-pcs">https://www.tomshardware.com/pc-components/cpus/amd-records-its-highest-server-market-share-in-decades-but-intel-fights-back-in-client-pcs</a>, See on <a href="https://news.ycombinator.com/item?id=41224253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>

<div id="article-body">
<p>AMD enjoyed another great quarter in Q2 2024 as it gained share in data center and laptop CPU markets, according to a new report from CPU market tracker&nbsp;<a data-analytics-id="inline-link" href="http://www.mercuryresearch.com/" data-url="http://www.mercuryresearch.com/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Mercury Research</a>. Still, Intel gained share in desktops and continues to lead in terms of units in general.&nbsp;</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-320-80.png" alt="Mercury Research" srcset="https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/GYTqQczjjDFR8H6pTrC3RK.png"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: AMD/Mercury Research)</span></figcaption></figure><p>Intel continued to dominate the client PC market in the second quarter of 2024, securing a 78.9% market share, while AMD held 21.1%. This outcome is expected, considering the strength and variety of Intel's client product lineup. Still, AMD managed to increase its unit share by 0.5% sequentially and by 3.8% year-over-year. Despite AMD's ongoing success, it will likely take the company years to achieve the sales growth needed to fully shift the market in its favor, not only because Intel dominates corporate PC sales but also because of Intel's access to vast production capacity.&nbsp;</p><div id="slice-container-table-PNonTt2eYea2mu8LE47VfS-4"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><caption>Desktop CPUs via Mercury Research</caption><tbody><tr><td colspan="1"><span>Row 0 - Cell 0 </span></td><td colspan="1"><strong>2Q24</strong></td><td colspan="1"><strong>1Q24</strong></td><td colspan="1"><strong>4Q23</strong></td><td colspan="1"><strong>3Q23</strong></td><td colspan="1"><strong>2Q23</strong></td><td colspan="1"><strong>1Q23</strong></td><td colspan="1"><strong>4Q22</strong></td><td colspan="1"><strong>3Q22</strong></td><td colspan="1"><strong>2Q22</strong></td><td colspan="1"><strong>1Q22</strong></td><td colspan="1"><strong>4Q21</strong></td><td colspan="1"><strong>3Q21</strong></td><td colspan="1"><strong>2Q21</strong></td><td colspan="1"><strong>1Q21</strong></td><td colspan="1"><strong>4Q20</strong></td><td colspan="1"><strong>3Q20</strong></td><td colspan="1"><strong>2Q20</strong></td><td colspan="1"><strong>1Q20</strong></td><td colspan="1"><strong>4Q19</strong></td><td colspan="1"><strong>3Q19</strong></td><td colspan="1"><strong>2Q19</strong></td><td colspan="1"><strong>1Q2019</strong></td><td colspan="1"><strong>4Q18</strong></td><td colspan="1"><strong>3Q18</strong></td><td colspan="1"><strong>2Q18</strong></td><td colspan="1"><strong>1Q18</strong></td><td colspan="1"><strong>4Q17</strong></td><td colspan="1"><strong>3Q17</strong></td><td colspan="1"><strong>2Q17</strong></td><td colspan="1"><strong>1Q17</strong></td><td colspan="1"><strong>4Q16</strong></td><td colspan="1"><strong>3Q16</strong></td></tr><tr><td colspan="1"><strong>AMD Desktop Unit Share</strong></td><td colspan="1">23.0%</td><td colspan="1">23.9%</td><td colspan="1">19.8%</td><td colspan="1">19.2%</td><td colspan="1">19.4%</td><td colspan="1">19.2%</td><td colspan="1">18.6%</td><td colspan="1">13.9%</td><td colspan="1">20.5%</td><td colspan="1">18.3%</td><td colspan="1">16.2%</td><td colspan="1">17.0%</td><td colspan="1">17.1%</td><td colspan="1">19.3%</td><td colspan="1">19.3%</td><td colspan="1">20.1%</td><td colspan="1">19.2%</td><td colspan="1">18.6%</td><td colspan="1">18.3%</td><td colspan="1">18%</td><td colspan="1">17.1%</td><td colspan="1">17.1%</td><td colspan="1">15.8%</td><td colspan="1">13%</td><td colspan="1">12.3%</td><td colspan="1">12.2%</td><td colspan="1">12.0%</td><td colspan="1">10.9%</td><td colspan="1">11.1%</td><td colspan="1">11.4%</td><td colspan="1">9.9%</td><td colspan="1">9.1%</td></tr><tr><td colspan="1"><strong>Quarter over Quarter / Year over Year (pp)</strong></td><td colspan="1">-1% / +3.6</td><td colspan="1">+4.1 / +4.7</td><td colspan="1">+0.6 / +1.2</td><td colspan="1">-0.2 / +0.5</td><td colspan="1">+0.1 / -1.02</td><td colspan="1">+0.6 / +0.9</td><td colspan="1">+4.7 / +2.4</td><td colspan="1">-6.6 / -3.1</td><td colspan="1">+2.2 / +3.4</td><td colspan="1">+2.1 / -1.0</td><td colspan="1">-0.8 / -3.1</td><td colspan="1">-0.1 / -3.1</td><td colspan="1">-2.3 / -2.1</td><td colspan="1">+0.1 / +0.7</td><td colspan="1">-0.8 / +1.0</td><td colspan="1">+0.9 / +2.1</td><td colspan="1">+0.6 / +2.1</td><td colspan="1">+0.3 / +1.5</td><td colspan="1">+0.3 / +2.4</td><td colspan="1">+0.9 / +5</td><td colspan="1">Flat / +4.8</td><td colspan="1">+1.3 / +4.9</td><td colspan="1">+2.8 / +3.8</td><td colspan="1">+0.7 / +2.1</td><td colspan="1">+0.1 / +1.2</td><td colspan="1">+0.2 / +0.8</td><td colspan="1">+1.1 / +2.1</td><td colspan="1">-0.2 / +1.8</td><td colspan="1">-0.3 / -</td><td colspan="1">+1.5 / -</td><td colspan="1">+0.8 / -</td><td colspan="1">-</td></tr></tbody></table></div></div><p>AMD lost 1% of market share to Intel in desktop PCs in the second quarter of 2024 and now controls 23%, leaving 77% to Intel. Considering that AMD was preparing to release its all-new Zen 5-based CPUs for desktops in August, we doubt the company was too aggressive with stuffing the channel with its previous-generation Zen 4-based offerings, which might be one of the reasons why the company lost a small chunk of the market to its rival. Then again, when compared to the second quarter of 2023, AMD gained a 3.6% share in Q2 2024, which is quite a good result.&nbsp;</p><div id="slice-container-table-PNonTt2eYea2mu8LE47VfS-8"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><caption>Mobile CPUs via Mercury Research</caption><tbody><tr><td colspan="1"><span>Row 0 - Cell 0 </span></td><td colspan="1"><strong>2Q24</strong></td><td colspan="1"><strong>1Q24</strong></td><td colspan="1"><strong>4Q23</strong></td><td colspan="1"><strong>3Q23</strong></td><td colspan="1"><strong>2Q23</strong></td><td colspan="1"><strong>1Q23</strong></td><td colspan="1"><strong>4Q22</strong></td><td colspan="1"><strong>3Q22</strong></td><td colspan="1"><strong>2Q22</strong></td><td colspan="1"><strong>1Q22</strong></td><td colspan="1"><strong>4Q21</strong></td><td colspan="1"><strong>3Q21</strong></td><td colspan="1"><strong>2Q21</strong></td><td colspan="1"><strong>1Q21</strong></td><td colspan="1"><strong>4Q20</strong></td><td colspan="1"><strong>3Q20</strong></td><td colspan="1"><strong>2Q20</strong></td><td colspan="1"><strong>1Q20</strong></td><td colspan="1"><strong>Q419</strong></td><td colspan="1"><strong>3Q19</strong></td><td colspan="1"><strong>2Q19</strong></td><td colspan="1"><strong>1Q2019</strong></td><td colspan="1"><strong>4Q18</strong></td><td colspan="1"><strong>3Q18</strong></td><td colspan="1"><strong>2Q18</strong></td></tr><tr><td colspan="1"><strong>AMD Mobile Unit Share</strong></td><td colspan="1">20.3%</td><td colspan="1">19.3%</td><td colspan="1">20.3%</td><td colspan="1">19.5%</td><td colspan="1">16.5%</td><td colspan="1">16.2%</td><td colspan="1">16.4%</td><td colspan="1">15.7%</td><td colspan="1">24.8%</td><td colspan="1">22.5%</td><td colspan="1">21.6%</td><td colspan="1">22.0%</td><td colspan="1">20.0%</td><td colspan="1">18.0%</td><td colspan="1">19%</td><td colspan="1">20.2%</td><td colspan="1">19.9%</td><td colspan="1">17.1%</td><td colspan="1">16.2%</td><td colspan="1">14.7%</td><td colspan="1">14.1%</td><td colspan="1">13.1%</td><td colspan="1">12.2%</td><td colspan="1">10.9%</td><td colspan="1">8.8%</td></tr><tr><td colspan="1"><strong>Quarter over Quarter / Year over Year (pp)</strong></td><td colspan="1">+1 / +3.8</td><td colspan="1">-1 / +3.1</td><td colspan="1">0.8 / 3.9</td><td colspan="1">2.9 / 3.8</td><td colspan="1">0.3 / -8.3</td><td colspan="1">-0.2 / -6.3</td><td colspan="1">+0.8 / -5.1</td><td colspan="1">-9.1 / -6.4</td><td colspan="1">+2.3 / +4.8</td><td colspan="1">+0.9 / +4.4</td><td colspan="1">-0.4 / +2.6</td><td colspan="1">+2.0 / +1.8</td><td colspan="1">+1.9 / +0.01</td><td colspan="1">-1.0 / +1.1</td><td colspan="1">-1.2 / +2.8 </td><td colspan="1">+0.3 / +5.5</td><td colspan="1">+2.9 / +5.8</td><td colspan="1">+0.9 / +3.2</td><td colspan="1">+1.5 / +4.0</td><td colspan="1">+0.7 / +3.8</td><td colspan="1">+1.0 / +5.3</td><td colspan="1">+0.9 / ?</td><td colspan="1"><span>Row 2 - Cell 23 </span></td><td colspan="1"><span>Row 2 - Cell 24 </span></td><td colspan="1"><span>Row 2 - Cell 25 </span></td></tr></tbody></table></div></div><p>On the laptop front, AMD made gains both sequentially and year-over-year. The company commanded 20.3% of x86 processors for laptops in Q2 2024: this is 1% higher than in the first quarter of this year and 3.8% higher than in the same quarter a year ago.&nbsp;</p><div><p>Apparently, even the upcoming launch of Zen 5-based Ryzen AI and Copilot+ and the AI PC frenzy are not expected to lower demand for AMD's existing offerings for <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/notebooks" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/notebooks">notebooks</a>, which is why PC makers accelerated purchases of these products. Another reason for AMD's success could be Intel's issues with supplying enough Meteor Lake PCs.&nbsp;</p><p>

Still, AMD does not seem to have sold many expensive ('expensive' does not mean highest-end, though) Ryzen CPUs. Its laptop CPU revenue share is 17.7%, which is well below its 20.3% unit market share. Again, AMD's position improved as it increased its revenue share by 2.8% compared to the second quarter of this year and by a rather noticeable 4.5% compared to the second quarter of 2023.</p></div><div id="slice-container-table-PNonTt2eYea2mu8LE47VfS-11"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><caption>Server CPUs via Mercury Research</caption><tbody><tr><td colspan="1"><span>Row 0 - Cell 0 </span></td><td colspan="1"><strong>1Q24</strong></td><td colspan="1"><strong>1Q24</strong></td><td colspan="1"><strong>4Q23</strong></td><td colspan="1"><strong>3Q23</strong></td><td colspan="1"><strong>2Q23</strong></td><td colspan="1"><strong>1Q23</strong></td><td colspan="1"><strong>4Q22</strong></td><td colspan="1"><strong>3Q22</strong></td><td colspan="1"><strong>2Q22</strong></td><td colspan="1"><strong>1Q22</strong></td><td colspan="1"><strong>4Q21</strong></td><td colspan="1"><strong>3Q21</strong></td><td colspan="1"><strong>2Q21</strong></td><td colspan="1"><strong>1Q21</strong></td><td colspan="1"><strong>4Q20</strong></td><td colspan="1"><strong>3Q20</strong></td><td colspan="1"><strong>2Q20</strong></td><td colspan="1"><strong>1Q20</strong></td><td colspan="1"><strong>4Q19</strong></td><td colspan="1"><strong>3Q19</strong></td><td colspan="1"><strong>2Q19</strong></td><td colspan="1"><strong>1Q2019</strong></td><td colspan="1"><strong>4Q18</strong></td><td colspan="1"><strong>3Q18</strong></td><td colspan="1"><strong>2Q18</strong></td><td colspan="1"><strong>4Q17</strong></td></tr><tr><td colspan="1"><strong>AMD Server Unit Share</strong></td><td colspan="1">24.1%</td><td colspan="1">23.6%</td><td colspan="1">23.1%</td><td colspan="1">23.3%</td><td colspan="1">18.6%</td><td colspan="1">18%</td><td colspan="1">17.6%</td><td colspan="1">17.5%</td><td colspan="1">13.9%</td><td colspan="1">11.6%</td><td colspan="1">10.7%</td><td colspan="1">10.2%</td><td colspan="1">9.5%</td><td colspan="1">8.9%</td><td colspan="1">7.1%</td><td colspan="1">6.6%</td><td colspan="1">5.8%</td><td colspan="1">5.1%</td><td colspan="1">4.5%</td><td colspan="1">4.3%</td><td colspan="1">3.4%</td><td colspan="1">2.9%</td><td colspan="1">3.2%</td><td colspan="1">1.6%</td><td colspan="1">1.4%</td><td colspan="1">0.8%</td></tr><tr><td colspan="1"><strong>Quarter over Quarter / Year over Year (pp)</strong></td><td colspan="1">+0.5 / + 5.6</td><td colspan="1">+0.5 / +5.6</td><td colspan="1">-0.2 / 5.5</td><td colspan="1">4.7 / 5.8</td><td colspan="1">0.6 / 4.7</td><td colspan="1">+0.4 / +6.3</td><td colspan="1">+0.1 / +6.9</td><td colspan="1">+3.6 / +7.3</td><td colspan="1">+2.3 / +4.4</td><td colspan="1">+0.9 / +2.7</td><td colspan="1">+0.5% / +3.6</td><td colspan="1">+0.7 / +3.6</td><td colspan="1">+0.6 / +3.7</td><td colspan="1">+1.8 / +3.8</td><td colspan="1">+0.5 / +2.6 </td><td colspan="1">+0.8 / +2.3</td><td colspan="1">+0.7 / +2.4</td><td colspan="1">+0.6 / 2.2</td><td colspan="1">+0.2 / +1.4</td><td colspan="1">+0.9 / +2.7</td><td colspan="1">+0.5 / +2.0</td><td colspan="1">-0.3 / -</td><td colspan="1">+1.6 / 2.4</td><td colspan="1">+0.2 / -</td><td colspan="1"><span>Row 2 - Cell 25 </span></td><td colspan="1"><span>Row 2 - Cell 26 </span></td></tr></tbody></table></div></div><p>AMD's biggest success for the second quarter probably lies in the server space. The company has managed to capture another 0.5% share away from Intel and now controls 24.1% of the data center CPU market with its EPYC CPUs. When compared to the second quarter of 2024, AMD's gain looks even more impressive, as the company grabbed 5.6% from Intel.&nbsp;</p><p>Although Intel is an indisputable leader when it comes to volumes, as it still controlled some 75.9% of datacenter CPU shipments in the second quarter, it is necessary to note that AMD seems to lead in high-end crème-de-la-crème machines that require the most powerful and expensive processors, as we can conclude from the financial results of the two companies in Q2 2024. While&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-loses-dollar16-billion-as-data-center-cpus-and-foundry-struggles" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intel-loses-dollar16-billion-as-data-center-cpus-and-foundry-struggles">Intel earned $3.0 billion selling 75.9% of data center CPUs</a>&nbsp;(in terms of units),&nbsp;<a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amds-gaming-revenue-falls-59-but-company-still-posts-a-9-year-over-year-revenue-increase" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amds-gaming-revenue-falls-59-but-company-still-posts-a-9-year-over-year-revenue-increase">AMD earned $2.8 billion selling 24.1% of server CPUs</a>&nbsp;(in terms of units), which signals that the average selling price of an AMD EPYC is considerably higher than the ASP of an Intel Xeon.&nbsp;&nbsp;</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-PNonTt2eYea2mu8LE47VfS"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>Indeed, AMD's revenue share of the server market in Q2 2024 was 33.7%, as calculated by AMD itself based on data from Mercury Research. That is up 0.7% QoQ and 6.6% YoY, an impressive result. Then again, Intel has nothing to offer against AMD's 96-core and 128-core processors for now, so AMD controls the market for high-end servers.</p>
</div>
<div id="slice-container-authorBio-PNonTt2eYea2mu8LE47VfS"><p>Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.</p></div>



<!-- Drop in a standard article here maybe? -->



</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There Is No Antimemetics Division (2018) (534 pts)]]></title>
            <link>https://qntm.org/scp</link>
            <guid>41224225</guid>
            <pubDate>Mon, 12 Aug 2024 13:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qntm.org/scp">https://qntm.org/scp</a>, See on <a href="https://news.ycombinator.com/item?id=41224225">Hacker News</a></p>
Couldn't get https://qntm.org/scp: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA["Does astrology work? We tested the ability of 152 astrologers" (132 pts)]]></title>
            <link>https://threadreaderapp.com/thread/1822663687145972105.html</link>
            <guid>41224173</guid>
            <pubDate>Mon, 12 Aug 2024 13:32:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threadreaderapp.com/thread/1822663687145972105.html">https://threadreaderapp.com/thread/1822663687145972105.html</a>, See on <a href="https://news.ycombinator.com/item?id=41224173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-controller="mentions">

<div>
<p><a href="https://threadreaderapp.com/user/SpencrGreenberg"><img src="https://pbs.twimg.com/profile_images/1524915487657844736/nyy8e3Yy_bigger.jpg" alt="Spencer Greenberg 🔍 Profile picture" data-controller="twitter-profile" data-twtrid="440506882" data-action="error->twitter-profile#error"></a>
</p>

</div> 
<div id="tweet_1" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663687145972105" dir="auto"><p>
Does astrology work? We tested the ability of 152 astrologers to see if they could demonstrate genuine astrological skill.</p><p>

Here is how the study was designed and what we found (including a result that really surprised me):</p><p>

🧵 <span><a href="https://pbs.twimg.com/media/GUtlavoWgAE_SrX.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtlavoWgAE_SrX.jpg"></a></span>
<sup><i></i></sup></p></div>
<p>
Back in January, we ran a study trying to predict 37 facts about people's lives using their astrological sun signs (whether they are Pisces, Aries, etc.) While personality tests were able to predict these facts decently well, sun signs couldn't predict even a single 1 of them... <span><a href="https://pbs.twimg.com/media/GUtlewTX0AAXspJ.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtlewTX0AAXspJ.jpg"></a></span>
<sup><i></i></sup>
</p>
<div id="tweet_3" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663692892188890" dir="auto"><p>
Some astrologers criticized us for this, saying that sun signs/zodiac signs are just tabloid astrology - real astrologers use a person's entire astrological chart. </p><p>

And they're right! </p><p>

Taking into account this criticism, we got the help of 6 astrologers to design a new study. <span><a href="https://pbs.twimg.com/media/GUtlkzKXgAAYnMu.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtlkzKXgAAYnMu.jpg"></a></span>
<sup><i></i></sup></p></div>
<p>
Here's how the study worked to test astrologers:<br>
• in each round, each astrologer gets LOTS of information about a real person (answers to 43 questions) along with 5 full astrological charts<br>
• they then predict which is the person's real natal chart (the other 4 are decoys) <span><a href="https://pbs.twimg.com/media/GUtlnpvXwAAxqj_.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtlnpvXwAAxqj_.jpg"></a></span>
<sup><i></i></sup>
</p>
<div id="tweet_5" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663698378281055" dir="auto"><p>
Why this study design?</p><p>

One of the most fundamental claims of astrology is that a person's natal chart contains information about that person's life and character. </p><p>

If true, astrologers should be able to correctly choose a person's chart at a rate well above random guessing.
<sup><i></i></sup></p></div>
<p>
Each astrologer tries to match people to their correct chart 12 times. If they're guessing completely at random (e.g., they have no skill because astrology doesn't actually work), then they'll get about 20% of questions right, or about 2.4 questions right (on average) out of 12.
<sup><i></i></sup>
</p>
<p>
Neat aspects of this study design are that (1) if astrology doesn't work, it's impossible for astrologers to do better than random guessing at this task, while (2) for the study to come out in support of astrology, astrologers only need to do slightly better than random guessing
<sup><i></i></sup>
</p>
<p>
But this is only a fair test if astrologers believe they can do this task - so we limit our analyses only to participants with prior astrological experience who predicted they would do better than random guessing at the task. Our results are based on 152 such astrologers. <span><a href="https://pbs.twimg.com/media/GUtmg8mXgAAB0vD.png" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtmg8mXgAAB0vD.png"></a></span>
<sup><i></i></sup>
</p>
<p>
These astrologers were quite confident in their ability to match people to charts. Those with the least experience believed (after they had completed participation) that they'd gotten 5 out of 12 right, and those with the most experience thought they'd gotten 10 out of 12 right. <span><a href="https://pbs.twimg.com/media/GUtly4lWQAANEmC.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtly4lWQAANEmC.jpg"></a></span>
<sup><i></i></sup>
</p>
<p>
So, how did astrologers do overall? If they'd gotten even 23% of questions right (slightly above the 20% of random guessing), the study would have come out in favor of astrology. But astrologers as a group performed indistinguishable from random guessing, getting &lt; 21% right. <span><a href="https://pbs.twimg.com/media/GUtl1yEWUAAiTx-.png" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtl1yEWUAAiTx-.png"></a></span>
<sup><i></i></sup>
</p>
<div id="tweet_11" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663714056626636" dir="auto"><p>
We can compare how frequently astrologers got different numbers of questions correct to how often we'd expect them to get different numbers correct if they were all guessing totally at random with no skill. </p><p>

The two distributions match very closely. <span><a href="https://pbs.twimg.com/media/GUtl3RpWUAAIHsz.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtl3RpWUAAIHsz.jpg"></a></span>
<sup><i></i></sup></p></div>
<div id="tweet_12" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663716669739369" dir="auto"><p>
But perhaps the less experienced astrologers were just dragging down the performance of the group? </p><p>

We looked at how performance varied based on astrological experience. More experienced astrologers did not do better than less experienced ones despite being far more confident. <span><a href="https://pbs.twimg.com/media/GUtl6j7XMAAOHDo.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtl6j7XMAAOHDo.jpg"></a></span>
<sup><i></i></sup></p></div>
<p>
Even if most astrologers have no skill, there's another way astrology could prove itself. If even 1 of the 152 astrologers performed exceptionally well, that could provide meaningful evidence for astrology. We offered a $1000 prize for anyone getting at least 11 out of 12.
<sup><i></i></sup>
</p>
<p>
Unfortunately, despite more than half of the astrologers believing that they had gotten 6 or more questions right (after completing the task), in actual fact, not a single astrologer got more than 5 right. <span><a href="https://pbs.twimg.com/media/GUtmlDpW8AAaydi.png" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtmlDpW8AAaydi.png"></a></span>
<sup><i></i></sup>
</p>
<p>
Okay, so despite them believing they could do this task, astrologers seemed to have no ability to match people to their astrological charts. But, even if they aren't getting the answers right, do they at least agree with each other on what the right answers are?
<sup><i></i></sup>
</p>
<p>
Much to my surprise, astrologers had very low agreement with each other on the chart for each person. If astrologers picked charts at random, they would agree with each other 20% of the time. In our study, even the most experienced astrologers only agreed 28% of the time. <span><a href="https://pbs.twimg.com/media/GUtmqoJXIAAEizC.jpg" target="_blank"><img alt="Image" src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/GUtmqoJXIAAEizC.jpg"></a></span>
<sup><i></i></sup>
</p>
<div id="tweet_17" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663729407791572" dir="auto"><p>
In conclusion, despite believing they could do it, the 152 astrologers seemed to lack any ability to match people to their astrological charts.</p><p>

You can learn a lot more about the study (including its limitations and how we sought to address them) here:</p><p>

<a data-preview="true" href="https://www.clearerthinking.org/post/can-astrologers-use-astrological-charts-to-understand-people-s-character-and-lives-our-new-study-pu">clearerthinking.org/post/can-astro…</a>
<sup><i></i></sup></p></div>
<div id="tweet_18" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822663731660341572" dir="auto"><p>
If you believe you have astrological skill, you can try the same questions that we used in the study (and find out the right answers at the end) in order to test yourself:</p><p>



We also open-sourced the data from the study if you want to analyze it.<a data-preview="true" href="https://programs.clearerthinking.org/astrology_challenge.html">programs.clearerthinking.org/astrology_chal…</a>
<sup><i></i></sup></p></div>
<div id="tweet_19" data-controller="thread" data-action="click->thread#showTweet" data-screenname="SpencrGreenberg" data-tweet="1822715786605281443" dir="auto"><p>
If you found this interesting, I'd appreciate a follow at @SpencrGreenberg !</p><p>

You may also enjoy my newsletter (One Helpful Idea) - where I send out one idea weekly (a 30 sec read) about psychology, philosophy, or society:</p><p>

<a data-preview="true" href="http://www.bit.ly/onehelpfulidea">bit.ly/onehelpfulidea</a>
<sup><i></i></sup></p></div>
<p>• • •</p>
<p><span>
Missing some Tweet in this thread? You can try to
<a id="force-click" href="#" data-category="refresh" data-action="1822663687145972105">force a refresh</a>
</span>
</p>
　
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Okay, I Like WezTerm (410 pts)]]></title>
            <link>https://alexplescan.com/posts/2024/08/10/wezterm/</link>
            <guid>41223934</guid>
            <pubDate>Mon, 12 Aug 2024 13:02:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexplescan.com/posts/2024/08/10/wezterm/">https://alexplescan.com/posts/2024/08/10/wezterm/</a>, See on <a href="https://news.ycombinator.com/item?id=41223934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
        <article role="article">
  

  <div><p>A while back <a href="https://blog.lambo.land/">my friend</a> recommended that I try <a href="https://wezfurlong.org/wezterm/">WezTerm</a>. I’d been an iTerm 2 stalwart for the better part of a decade, but not to be <em>too</em> narrow-minded I conceded, started it up, and saw this:</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-720-7c9bad525.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-1440-7c9bad525.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-1520-7c9bad525.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-720-7c9bad525.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-1440-7c9bad525.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-1520-7c9bad525.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/01_default_look-800-dd3151c15.png" alt="screenshot of WezTerm's default look"></picture>

<p>Does the job, sure, but doesn’t feel quite right. Okay then, experiment over. Back to iTerm…</p>

<p>Fast forward a couple of months and I got the itch to try a new terminal again. I wanted to use one whose config was entirely text based so I could pop it in to my dotfiles and share it across my work and personal machines. A few terminals already do this, but whispers of WezTerm’s powerful API and Lua config got me particularly interested.</p>

<p>I tried it again with a bit more patience and I’m glad I did. My terminal is prettier than it’s ever been, more functional, and I can finally justify my mechanical keyboard purchase with all the keybindings I’ve configured.</p>

<p>This post is an introduction to configuring WezTerm based on the setup that I eventually landed on. I’d consider it relatively low-frills. Most of what I talk about here can already be found in WezTerm’s <a href="https://wezfurlong.org/wezterm/config/lua/general.html">docs</a>, but as they’ve got a large surface area, I’m hoping this post will be a useful jumping off point for WezTerm beginners.</p>

<p>We <em>won’t</em> be looking at some of WezTerm’s key features, like custom hyperlinks highlighting rules, searchable scrollback, quick copy mode, and image support (you can find <a href="https://wezfurlong.org/wezterm/features.html">more details here</a>).</p>

<p>The feature I find most exciting about WezTerm is the flexibility of its Lua config, so we’ll be focusing on that. This includes configuring appearance, keybindings, multiplexing, workspace navigation, status bar setup, and dynamic theming. By the end of it all, we’ll have a terminal that looks like this:</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-720-7c6c65470.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-1440-7c6c65470.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-1518-7c6c65470.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-720-7c6c65470.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-1440-7c6c65470.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-1518-7c6c65470.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/02_pretty_look-800-df2e5e6df.png" alt="screenshot of the WezTerm look we'll end up with at the end of this post"></picture>

<p>Subtly prettier than the default, and with some great features to boot.</p>

<p>I use macOS, so what follows is focused on ergonomics that make WezTerm great there. I haven’t tested my config on other systems, but I’m not doing anything too bespoke so things should be portable (WezTerm works pretty much everywhere).</p>

<p><strong>tl;dr</strong>? Here’s <a href="https://gist.github.com/alexpls/83d7af23426c8928402d6d79e72f9401">a gist</a> containing the config we’ll end up with.</p>

<h2 id="pre-flight-checks">Pre-flight checks</h2>

<p>Start by installing WezTerm. Instructions for this are on <a href="https://wezfurlong.org/wezterm/installation.html">WezTerm’s site</a>. If you’re on macOS and reading this you probably have Homebrew installed, so <code>$ brew install wezterm</code> will do the trick.</p>

<p>Now launch WezTerm, and you’re already winning.</p>

<h3 id="a-note-on-lua">A note on Lua</h3>

<p>My favourite WezTerm feature is its use of Lua for defining config. Unlike terminals where your settings are adjusted via the UI (iTerm 2), your WezTerm config lives in your dotfiles and is portable across all your machines.</p>

<p>And unlike other terminals where your configuration is written using a data serialization format like YAML or TOML (Alacritty, kitty), with Lua you can more easily achieve complex configs by leveraging dynamic scripts.</p>

<p>Granted, Lua is a programming language so it is trickier to learn than YAML or TOML, but it’s still remarkably simple. If you’ve used another dynamic programming language (e.g. Ruby, Python, JavaScript) - you should be able to read the Lua code in this post easily. For achieving more complex configs, I’d recommend diving deeper into the language. Its <a href="https://www.lua.org/start.html">Getting Started guide</a> is a good place to… get started.</p>

<h3 id="config-files-and-the-best-feedback-loop-in-town">Config files, and the best feedback loop in town</h3>

<p>WezTerm supports loading in its config from all the usual places on your system (<a href="https://wezfurlong.org/wezterm/config/files.html#configuration-files">docs</a>). For this guide we’re going to be creating our config in <code>$XDG_CONFIG_HOME/wezterm/wezterm.lua</code>. On most systems (including macOS) this resolves to <code>~/.config/wezterm/wezterm.lua</code>. Using a directory to store our config instead of dumping it in <code>~/.wezterm.lua</code> will let us keep our config logically grouped as we split some of it out into different files.</p>

<p>Create the <code>wezterm.lua</code> file on that path, and add this boilerplate to it:</p>

<div><pre><code><span>-- Import the wezterm module</span>
<span>local</span> <span>wezterm</span> <span>=</span> <span>require</span> <span>'wezterm'</span>
<span>-- Creates a config object which we will be adding our config to</span>
<span>local</span> <span>config</span> <span>=</span> <span>wezterm</span><span>.</span><span>config_builder</span><span>()</span>

<span>-- (This is where our config will go)</span>

<span>-- Returns our config to be evaluated. We must always do this at the bottom of this file</span>
<span>return</span> <span>config</span>
</code></pre></div>

<p>Save the file and all going well… nothing will happen. Well, at least nothing <em>appeared</em> to happen, but what WezTerm did behind the scenes is quite magical. It watched your config file, and when it changed it auto-reloaded instantly. This feature makes for a wonderfully tight feedback loop where you don’t need to restart your terminal to see the effects of your new config.</p>

<p>We can quickly test this auto-reload by adding some invalid syntax and seeing what happens. Replace the call to <code>wezterm.config_builder()</code> with <code>wezterm.config_builderZ()</code>, save, and you should immediately see a window pop-up with:</p>

<div><pre><code>runtime error: [string "/Users/alex/.config/wezterm/wezterm.lua"]:2: attempt
to call a nil value (field 'config_builderZ')
stack traceback:
        [string "/Users/alex/.config/wezterm/wezterm.lua"]:2: in main chunk
</code></pre></div>

<p>How’s that for a feedback loop? Fix the error and save the file again.</p>

<p>This time, have your config log something:</p>

<div><pre><code><span>wezterm</span><span>.</span><span>log_info</span><span>(</span><span>"hello world! my name is "</span> <span>..</span> <span>wezterm</span><span>.</span><span>hostname</span><span>())</span>
</code></pre></div>

<p>Save. Now… where did that log go? Press <code>CTRL + SHIFT + L</code> to bring up the debug overlay (<a href="https://wezfurlong.org/wezterm/troubleshooting.html#debug-overlay">docs</a>) and lo and behold, your beautiful log was waiting for you all along. Not only that but what you’re looking at is a full Lua REPL. Enter <code>1 + 1</code> and you’ll see the result. Enter <code>wezterm.home_dir</code> and you’ll see the result of accessing the <code>home_dir</code> entry on the <code>wezterm</code> module (<a href="https://wezfurlong.org/wezterm/config/lua/wezterm/home_dir.html">docs</a>).</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-720-5d57e63cc.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-1440-5d57e63cc.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-1520-5d57e63cc.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-720-5d57e63cc.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-1440-5d57e63cc.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-1520-5d57e63cc.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/03_debug_overlay-800-31f78c8d4.png" alt="screenshot of the WezTerm's debug overlay"></picture>

<p>The combination of hot reloading and the debug overlay makes experimenting with WezTerm configs extremely low friction and low consequence. The feedback loop is so tight now it’s more like a feedback lp.</p>

<h2 id="configuring-appearance">Configuring appearance</h2>

<p>Okay enough gushing - let’s cut to the chase and make this thing prettier. Add a few lines to the config to start customising the look of the terminal. We’ll start with a colour scheme (<a href="https://wezfurlong.org/wezterm/config/appearance.html">docs</a>):</p>

<div><pre><code><span>-- Pick a colour scheme. WezTerm ships with more than 1,000!</span>
<span>-- Find them here: https://wezfurlong.org/wezterm/colorschemes/index.html</span>
<span>config</span><span>.</span><span>color_scheme</span> <span>=</span> <span>'Tokyo Night'</span>
</code></pre></div>

<p>Save, and you should immediately see it update. Thanks Wez!</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-720-5ed5e3bf1.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-1440-5ed5e3bf1.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-1520-5ed5e3bf1.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-720-5ed5e3bf1.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-1440-5ed5e3bf1.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-1520-5ed5e3bf1.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/04_colour_scheme-800-b30942129.png" alt="screenshot of applying a colour scheme to WezTerm"></picture>

<p>(if the hot config reload doesn’t work for whatever reason, you can manually reload it by pressing <code>CMD + R</code>).</p>

<h3 id="many-colours-all-at-once">Many colours, all at once</h3>

<p>With over 1,000 colour choices to choose from, it’s tough to decide on your favourite. Why not outsource that work to your computer? Let’s explore the power of WezTerm’s dynamic config by randomly assigning a colour scheme for each new window you open:</p>

<div><pre><code><span>-- Creates a lua table containing the name of every color scheme WezTerm</span>
<span>-- ships with.</span>
<span>local</span> <span>scheme_names</span> <span>=</span> <span>{}</span>
<span>for</span> <span>name</span><span>,</span> <span>scheme</span> <span>in</span> <span>pairs</span><span>(</span><span>wezterm</span><span>.</span><span>color</span><span>.</span><span>get_builtin_schemes</span><span>())</span> <span>do</span>
  <span>table.insert</span><span>(</span><span>scheme_names</span><span>,</span> <span>name</span><span>)</span>
<span>end</span>

<span>-- When the config for a window is reloaded (i.e. when you save this file</span>
<span>-- or open a new window)...</span>
<span>wezterm</span><span>.</span><span>on</span><span>(</span><span>'window-config-reloaded'</span><span>,</span> <span>function</span><span>(</span><span>window</span><span>,</span> <span>pane</span><span>)</span>
  <span>-- Don't proceed if the config has already been overriden, otherwise</span>
  <span>-- we'll enter an infinite loop of neverending colour scheme changes.</span>
  <span>-- If that sounds like your kinda thing, then remove this line ;) - but</span>
  <span>-- don't say you haven't been warned.</span>
  <span>if</span> <span>window</span><span>:</span><span>get_config_overrides</span><span>()</span> <span>then</span> <span>return</span> <span>end</span>
  <span>-- Pick a random colour scheme name.</span>
  <span>local</span> <span>scheme</span> <span>=</span> <span>scheme_names</span><span>[</span><span>math.random</span><span>(</span><span>#</span><span>scheme_names</span><span>)]</span>
  <span>-- Assign it as an override for this window.</span>
  <span>window</span><span>:</span><span>set_config_overrides</span> <span>{</span> <span>color_scheme</span> <span>=</span> <span>scheme</span> <span>}</span>
  <span>-- And log it for good measure</span>
  <span>wezterm</span><span>.</span><span>log_info</span><span>(</span><span>"Your colour scheme is now: "</span> <span>..</span> <span>scheme</span><span>)</span>
<span>end</span><span>)</span>
</code></pre></div>

<p>Open up a few windows (<code>CMD + N</code> on macOS) and each one will have a different colour scheme. A cornucopia of terminals, each more surprising than the last. We, my friends, are truly innovating now.</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-720-bfc375d64.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-1440-bfc375d64.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-1520-bfc375d64.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-720-bfc375d64.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-1440-bfc375d64.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-1520-bfc375d64.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/05_cornucopia-800-06950f034.png" alt="screenshot of many WezTerm terminal windows, each with a distinctive colour scheme"></picture>

<p>But really, that was kind of a dumb idea meant to prove a point. Now that you’ve gotten a taste for dynamic config, you probably wanna remove those lines and stick to a colour scheme you <em>do</em> like.</p>

<p>(You may find that after you remove that code and add your static <code>color_scheme</code> config back in, it doesn’t hot reload. That’s because our script set an <em>override</em> on the config specific to each window. To clear your overrides, you can go to your debug terminal and type <code>window:set_config_overrides({})</code> - or you can just close and reopen your WezTerm window).</p>

<h3 id="respecting-the-systems-appearance">Respecting the system’s appearance</h3>

<p>Light themes, dark themes… why not both? Let’s have the terminal’s colour scheme automatically change when the operating system’s appearance changes. While we’re at it, we’ll learn how to split up WezTerm config into different modules.</p>

<p>Create a new file alongside <code>wezterm.lua</code> and call it <code>appearance.lua</code>. Add this to it:</p>

<div><pre><code><span>-- We almost always start by importing the wezterm module</span>
<span>local</span> <span>wezterm</span> <span>=</span> <span>require</span> <span>'wezterm'</span>
<span>-- Define a lua table to hold _our_ module's functions</span>
<span>local</span> <span>module</span> <span>=</span> <span>{}</span>

<span>-- Returns a bool based on whether the host operating system's</span>
<span>-- appearance is light or dark.</span>
<span>function</span> <span>module</span><span>.</span><span>is_dark</span><span>()</span>
  <span>-- wezterm.gui is not always available, depending on what</span>
  <span>-- environment wezterm is operating in. Just return true</span>
  <span>-- if it's not defined.</span>
  <span>if</span> <span>wezterm</span><span>.</span><span>gui</span> <span>then</span>
    <span>-- Some systems report appearance like "Dark High Contrast"</span>
    <span>-- so let's just look for the string "Dark" and if we find</span>
    <span>-- it assume appearance is dark.</span>
    <span>return</span> <span>wezterm</span><span>.</span><span>gui</span><span>.</span><span>get_appearance</span><span>():</span><span>find</span><span>(</span><span>"Dark"</span><span>)</span>
  <span>end</span>
  <span>return</span> <span>true</span>
<span>end</span>

<span>return</span> <span>module</span>
</code></pre></div>

<p>Back in <code>wezterm.lua</code>:</p>

<div><pre><code><span>-- Import our new module (put this near the top of your wezterm.lua)</span>
<span>local</span> <span>appearance</span> <span>=</span> <span>require</span> <span>'appearance'</span>

<span>-- Use it!</span>
<span>if</span> <span>appearance</span><span>.</span><span>is_dark</span><span>()</span> <span>then</span>
  <span>config</span><span>.</span><span>color_scheme</span> <span>=</span> <span>'Tokyo Night'</span>
<span>else</span>
  <span>config</span><span>.</span><span>color_scheme</span> <span>=</span> <span>'Tokyo Night Day'</span>
<span>end</span>
</code></pre></div>

<p>Toggle your system appearance between dark mode and light mode, and watch your theme change right before your eyes.</p>

<!-- hackily putting this into a p to have the margin to next heading look good. need to review
     stylesheet to fix this properly -->
<p><picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-720-807cf36b1.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-1440-807cf36b1.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-1520-807cf36b1.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-720-807cf36b1.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-1440-807cf36b1.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-1520-807cf36b1.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/06_light_v_dark-800-1e54dcccb.png" alt="screenshot of WezTerm in light and dark mode"></picture>
</p>

<h3 id="fonts">Fonts</h3>

<p>Next up let’s look at fonts. WezTerm ships with the lovely JetBrains Mono, and Nerd Font Symbols (<a href="https://wezfurlong.org/wezterm/config/fonts.html">docs</a>) so there’s nothing to complain about there. I do prefer Berkeley Mono at 13 points though, so:</p>

<div><pre><code><span>-- Choose your favourite font, make sure it's installed on your machine</span>
<span>config</span><span>.</span><span>font</span> <span>=</span> <span>wezterm</span><span>.</span><span>font</span><span>({</span> <span>family</span> <span>=</span> <span>'Berkeley Mono'</span> <span>})</span>
<span>-- And a font size that won't have you squinting</span>
<span>config</span><span>.</span><span>font_size</span> <span>=</span> <span>13</span>
</code></pre></div>

<p>There’s good support for ligatures and other fancy font settings if you’re into that (<a href="https://wezfurlong.org/wezterm/config/font-shaping.html">docs</a>), but I’m not so let’s move on.</p>

<h3 id="window-styling">Window styling</h3>

<p>Let’s style our terminal’s window. This controls the chrome that appears around it, and can vary between operating systems. On macOS, I like the below:</p>

<div><pre><code><span>-- Slightly transparent and blurred background</span>
<span>config</span><span>.</span><span>window_background_opacity</span> <span>=</span> <span>0</span><span>.</span><span>9</span>
<span>config</span><span>.</span><span>macos_window_background_blur</span> <span>=</span> <span>30</span>
<span>-- Removes the title bar, leaving only the tab bar. Keeps</span>
<span>-- the ability to resize by dragging the window's edges.</span>
<span>-- On macOS, 'RESIZE|INTEGRATED_BUTTONS' also looks nice if</span>
<span>-- you want to keep the window controls visible and integrate</span>
<span>-- them into the tab bar.</span>
<span>config</span><span>.</span><span>window_decorations</span> <span>=</span> <span>'RESIZE'</span>
<span>-- Sets the font for the window frame (tab bar)</span>
<span>config</span><span>.</span><span>window_frame</span> <span>=</span> <span>{</span>
  <span>-- Berkeley Mono for me again, though an idea could be to try a</span>
  <span>-- serif font here instead of monospace for a nicer look?</span>
  <span>font</span> <span>=</span> <span>wezterm</span><span>.</span><span>font</span><span>({</span> <span>family</span> <span>=</span> <span>'Berkeley Mono'</span><span>,</span> <span>weight</span> <span>=</span> <span>'Bold'</span> <span>}),</span>
  <span>font_size</span> <span>=</span> <span>11</span><span>,</span>
<span>}</span>
</code></pre></div>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-720-87031f878.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-1440-87031f878.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-1520-87031f878.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-720-87031f878.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-1440-87031f878.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-1520-87031f878.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/07_window_styling-800-4b12b9e39.png" alt="screenshot of WezTerm after we've styled its window"></picture>

<p>Now, let’s do something a little kitsch. See that empty space to the right of our terminal’s tab bar? Let’s fill it with a powerline looking status bar. We’ll add an <code>update-status</code> callback:</p>

<div><pre><code><span>wezterm</span><span>.</span><span>on</span><span>(</span><span>'update-status'</span><span>,</span> <span>function</span><span>(</span><span>window</span><span>)</span>
  <span>-- Grab the utf8 character for the "powerline" left facing</span>
  <span>-- solid arrow.</span>
  <span>local</span> <span>SOLID_LEFT_ARROW</span> <span>=</span> <span>utf8.char</span><span>(</span><span>0xe0b2</span><span>)</span>

  <span>-- Grab the current window's configuration, and from it the</span>
  <span>-- palette (this is the combination of your chosen colour scheme</span>
  <span>-- including any overrides).</span>
  <span>local</span> <span>color_scheme</span> <span>=</span> <span>window</span><span>:</span><span>effective_config</span><span>().</span><span>resolved_palette</span>
  <span>local</span> <span>bg</span> <span>=</span> <span>color_scheme</span><span>.</span><span>background</span>
  <span>local</span> <span>fg</span> <span>=</span> <span>color_scheme</span><span>.</span><span>foreground</span>

  <span>window</span><span>:</span><span>set_right_status</span><span>(</span><span>wezterm</span><span>.</span><span>format</span><span>({</span>
    <span>-- First, we draw the arrow...</span>
    <span>{</span> <span>Background</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>'none'</span> <span>}</span> <span>},</span>
    <span>{</span> <span>Foreground</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>bg</span> <span>}</span> <span>},</span>
    <span>{</span> <span>Text</span> <span>=</span> <span>SOLID_LEFT_ARROW</span> <span>},</span>
    <span>-- Then we draw our text</span>
    <span>{</span> <span>Background</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>bg</span> <span>}</span> <span>},</span>
    <span>{</span> <span>Foreground</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>fg</span> <span>}</span> <span>},</span>
    <span>{</span> <span>Text</span> <span>=</span> <span>' '</span> <span>..</span> <span>wezterm</span><span>.</span><span>hostname</span><span>()</span> <span>..</span> <span>' '</span> <span>},</span>
  <span>}))</span>
<span>end</span><span>)</span>
</code></pre></div>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-720-716b8e09a.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-1440-716b8e09a.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-1520-716b8e09a.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-720-716b8e09a.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-1440-716b8e09a.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-1520-716b8e09a.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/08_status_bar-800-7eee51cfe.png" alt="screenshot of WezTerm with a right status bar showing the system's hostname"></picture>

<p>A few interesting things happening here:</p>
<ol>
  <li>We just used WezTerm’s events API with <code>wezterm.on</code>. Events are things that happen to the terminal (e.g. window resize) that we can define callbacks for. The <code>update-status</code> event is emitted periodically when the terminal is ready to have its status updated. WezTerm manages this cleverly to ensure that only one such update can run at any given time, and if your code takes too long to execute, a timeout will be hit and your handler will be abandoned… protecting your terminal from bogging down.</li>
  <li>We’re grabbing the <code>effective_config()</code> of the window to get the “effective” configuration, which is the config with any overrides applied. From this we can get the <code>resolved_palette</code>, which is the currently active colour scheme. To see what this data looks like you can enter the debug overlay (<code>CTRL + SHIFT + L</code>) and execute <code>window:effective_config().resolved_palette</code>.</li>
  <li>We’re using the <code>wezterm.format</code> function (<a href="https://wezfurlong.org/wezterm/config/lua/wezterm/format.html">docs</a>) to style our string with colours. Other ways you could format text include setting font weight, underlining text, and more.</li>
  <li>Finally, the <code>wezterm.hostname()</code> function (<a href="https://wezfurlong.org/wezterm/config/lua/wezterm/hostname.html">docs</a>) gives us the hostname of the machine we’re running on. WezTerm ships with a bunch of useful functions for getting the state of your system, and also… we’re doing stuff in Lua - so you have full access to your file system, are able to make network requests, etc.</li>
</ol>

<p>Altogether this gives us a powerline…ish. It’s a bit sad with only one segment isn’t it? Don’t you worry, we’ll be adding more soon…</p>

<h2 id="keys">Keys</h2>

<p>Here’s the part where we justify our mechanical keyboard purchases. Let’s set up some key assignments. During this section we’ll look at WezTerm’s deep key handling capabilities and ability to take action based on your input.</p>

<p>By default, WezTerm defines some standard key assignments (<a href="https://wezfurlong.org/wezterm/config/default-keys.html">docs</a>). I leave them on because they’re very sensible, but if you wanna <em>really</em> wrest total control of your config, you can turn them off with <code>config.disable_default_key_bindings = true</code>.</p>

<p>Our first key assignment will be a humble start for us macOS users… you might be used to <code>Option + Left Arrow</code> and <code>Option + Right Arrow</code> jumping between words on your terminal. That’s the default in iTerm 2 and Terminal.app, but not in WezTerm. However, we can map it!</p>

<p>We do this by adding a <code>keys</code> table to our <code>config</code>:</p>

<div><pre><code><span>-- Table mapping keypresses to actions</span>
<span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- Sends ESC + b and ESC + f sequence, which is used</span>
  <span>-- for telling your shell to jump back/forward.</span>
  <span>{</span>
    <span>-- When the left arrow is pressed</span>
    <span>key</span> <span>=</span> <span>'LeftArrow'</span><span>,</span>
    <span>-- With the "Option" key modifier held down</span>
    <span>mods</span> <span>=</span> <span>'OPT'</span><span>,</span>
    <span>-- Perform this action, in this case - sending ESC + B</span>
    <span>-- to the terminal</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SendString</span> <span>'\x1bb'</span><span>,</span>
  <span>},</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'RightArrow'</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'OPT'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SendString</span> <span>'\x1bf'</span><span>,</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>By now you’ve probably figured out that you’re gonna be spending more time configuring WezTerm than doing actual work. There’s no shame in admitting this reality, so let’s encode it into our config. On macOS, the default shortcut for opening an application’s preferences is <code>CMD + ,</code> - let’s make it so when we press this, our favourite editor opens up the WezTerm config. I’m using neovim, but feel free to substitute with your own:</p>

<div><pre><code><span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... add these new entries to your config.keys table</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>','</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'SUPER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SpawnCommandInNewTab</span> <span>{</span>
      <span>cwd</span> <span>=</span> <span>wezterm</span><span>.</span><span>home_dir</span><span>,</span>
      <span>args</span> <span>=</span> <span>{</span> <span>'nvim'</span><span>,</span> <span>wezterm</span><span>.</span><span>config_file</span> <span>},</span>
    <span>},</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>Try that out, but you may see an error along the lines of:</p>

<div><pre><code>Unable to spawn nvim because:
No viable candidates found in PATH "/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin"
</code></pre></div>

<p>If that error showed up, it’s typically because the process that launched WezTerm didn’t include a <code>PATH</code> environment variable that led to your editor’s binary (e.g. on macOS, Finder is usually WezTerm’s parent). We can work around this by specifying the full path to your editor in the <code>SpawnCommandInNewTab</code> properties (<a href="https://wezfurlong.org/wezterm/config/lua/SpawnCommand.html">docs</a>), or by updating the default environment variables WezTerm spawns commands with. I prefer the latter, since it means that any other places in our config where we might spawn new commands will also inherit the same env vars:</p>

<div><pre><code><span>config</span><span>.</span><span>set_environment_variables</span> <span>=</span> <span>{</span>
  <span>PATH</span> <span>=</span> <span>'/opt/homebrew/bin:'</span> <span>..</span> <span>os.getenv</span><span>(</span><span>'PATH'</span><span>)</span>
<span>}</span>
</code></pre></div>

<p>Try that again, and it should work.</p>

<p>We really are just scratching the surface of all the commands available (<a href="https://wezfurlong.org/wezterm/config/lua/keyassignment/index.html">WezTerm supports a lot</a>). In the next section, we’ll be growing our key bindings further.</p>

<h2 id="multiplexing-terminals-levelling-up-key-assignments">Multiplexing terminals, levelling up key assignments</h2>

<p>Let’s move on to WezTerm’s multiplexing capabilities. If you make use of a multiplexer (i.e. tmux) then you may consider using WezTerm’s builtin capabilities instead. They’ll generally give you a more integrated experience, with individual scrollback buffers per pane, better mouse control, easier selection functionality, and generally faster performance.</p>

<p>Hit <code>CTRL + SHIFT + P</code> to bring up WezTerm’s command palette. (Yes, WezTerm has a command palette. Yes, it’s as customisable as everything else we’ve seen so far. No, we won’t dwell on it here). Type <code>split horizontally</code> until the “Shell: Split Horizontally” option is selected and hit <code>ENTER</code>. Ta-da! Your shell split horizontally! Do the same for <code>split vertically</code> and… you get the idea.</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-720-5e12352ea.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-1440-5e12352ea.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-1520-5e12352ea.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-720-5e12352ea.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-1440-5e12352ea.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-1520-5e12352ea.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/09_command_palette-800-74d2ff2ad.png" alt="screenshot of WezTerm's command palette"></picture>

<p>You may have noticed that the command palette displays the keyboard shortcut assigned to each action. The ones for splitting are quite a fingerful, e.g. <code>SHIFT + CTRL + OPTION + "</code>. I get why they’re this complicated - because they’re trying not to clash with any other shortcuts you may have on your system, but we can do a lot better - and WezTerm gives us the tools do so easily!</p>

<h3 id="splitting-panes-leader-key">Splitting panes, leader key</h3>

<p>A leader key (<a href="https://wezfurlong.org/wezterm/config/keys.html#leader-key">docs</a>) is a special key combination that you press first, followed by another key combination, to perform a specific action. It can help you create complex shortcuts without needing to push a lot of keys all at once.</p>

<p>Sounds like a perfect fit for splitting panes, right? We’ll bind our leader to <code>CTRL + A</code>, and in case you accidentally type the leader without following it up with another key, we’ll have it automatically deactivate after 1,000 milliseconds.</p>

<div><pre><code><span>-- If you're using emacs you probably wanna choose a different leader here,</span>
<span>-- since we're gonna be making it a bit harder to CTRL + A for jumping to </span>
<span>-- the start of a line</span>
<span>config</span><span>.</span><span>leader</span> <span>=</span> <span>{</span> <span>key</span> <span>=</span> <span>'a'</span><span>,</span> <span>mods</span> <span>=</span> <span>'CTRL'</span><span>,</span> <span>timeout_milliseconds</span> <span>=</span> <span>1000</span> <span>}</span>
</code></pre></div>

<p>Next let’s define some key assignments for splitting panes:</p>

<div><pre><code><span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... add these new entries to your config.keys table</span>
  <span>{</span>
    <span>-- I'm used to tmux bindings, so am using the quotes (") key to</span>
    <span>-- split horizontally, and the percent (%) key to split vertically.</span>
    <span>key</span> <span>=</span> <span>'"'</span><span>,</span>
    <span>-- Note that instead of a key modifier mapped to a key on your keyboard</span>
    <span>-- like CTRL or ALT, we can use the LEADER modifier instead.</span>
    <span>-- This means that this binding will be invoked when you press the leader</span>
    <span>-- (CTRL + A), quickly followed by quotes (").</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SplitHorizontal</span> <span>{</span> <span>domain</span> <span>=</span> <span>'CurrentPaneDomain'</span> <span>},</span>
  <span>},</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'%'</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SplitVertical</span> <span>{</span> <span>domain</span> <span>=</span> <span>'CurrentPaneDomain'</span> <span>},</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>Give it a go now. Press <code>CTRL + A</code>, quickly followed by <code>"</code>, and you’ll get a horizontal split. Use the other assignment and you’ll get a vertical split.</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/10_splits-720-79750a1ee.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/10_splits-1440-79750a1ee.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/10_splits-1520-79750a1ee.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/10_splits-720-79750a1ee.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/10_splits-1440-79750a1ee.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/10_splits-1520-79750a1ee.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/10_splits-800-8e0e76173.png" alt="screenshot of WezTerm's with split panes"></picture>

<p>Before we move on - you might be wondering what happens if you actually want to send the <code>CTRL + A</code> keypress <em>without</em> invoking the leader? <code>CTRL + A</code> is useful in and of its own as pressing it jumps to the start of a line on your shell (and on operating systems like Emacs).</p>

<p>Well there’s a solution for that. We can map <code>CTRL + A</code> quickly followed by <code>CTRL + A</code> to send a <code>CTRL + A</code> to our terminal. That’s a confusing sentence! It’ll be simpler to just look at the config:</p>

<div><pre><code><span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... add these new entries to your config.keys table</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'a'</span><span>,</span>
    <span>-- When we're in leader mode _and_ CTRL + A is pressed...</span>
    <span>mods</span> <span>=</span> <span>'LEADER|CTRL'</span><span>,</span>
    <span>-- Actually send CTRL + A key to the terminal</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SendKey</span> <span>{</span> <span>key</span> <span>=</span> <span>'a'</span><span>,</span> <span>mods</span> <span>=</span> <span>'CTRL'</span> <span>},</span>
  <span>},</span>
<span>},</span>
</code></pre></div>

<h3 id="moving-around-panes">Moving around panes</h3>

<p>Okay with that done, let’s get back to multiplexing. Next up, navigating our splits. I like to use vim direction keybindings, but feel free to replace with arrow keys instead.</p>

<div><pre><code><span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... add these new entries to your config.keys table</span>
  <span>{</span>
    <span>-- I like to use vim direction keybindings, but feel free to replace</span>
    <span>-- with directional arrows instead.</span>
    <span>key</span> <span>=</span> <span>'j'</span><span>,</span> <span>-- or DownArrow</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ActivatePaneDirection</span><span>(</span><span>'Down'</span><span>),</span>
  <span>},</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'k'</span><span>,</span> <span>-- or UpArrow</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ActivatePaneDirection</span><span>(</span><span>'Up'</span><span>),</span>
  <span>},</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'h'</span><span>,</span> <span>-- or LeftArrow</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ActivatePaneDirection</span><span>(</span><span>'Left'</span><span>),</span>
  <span>},</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'l'</span><span>,</span> <span>-- or RightArrow</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ActivatePaneDirection</span><span>(</span><span>'Right'</span><span>),</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>Look at all that duplication - We’re using a dynamic language for our config here, we don’t need to stand for that! Let’s go on a little side quest and see if we can extract it to a function.</p>

<div><pre><code><span>local</span> <span>function</span> <span>move_pane</span><span>(</span><span>key</span><span>,</span> <span>direction</span><span>)</span>
  <span>return</span> <span>{</span>
    <span>key</span> <span>=</span> <span>key</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ActivatePaneDirection</span><span>(</span><span>direction</span><span>),</span>
  <span>}</span>
<span>end</span>

<span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... remove the previous move bindings, and replace with</span>
  <span>move_pane</span><span>(</span><span>'j'</span><span>,</span> <span>'Down'</span><span>),</span>
  <span>move_pane</span><span>(</span><span>'k'</span><span>,</span> <span>'Up'</span><span>),</span>
  <span>move_pane</span><span>(</span><span>'h'</span><span>,</span> <span>'Left'</span><span>),</span>
  <span>move_pane</span><span>(</span><span>'l'</span><span>,</span> <span>'Right'</span><span>),</span>
<span>}</span>
</code></pre></div>

<p>Ooh so much smaller, but it could be smaller still. I dare you to keep code golfing this down to 6 lines. Go on - I believe in you!</p>

<h3 id="resizing-panes-and-introducing-key-tables">Resizing panes, and introducing key tables</h3>

<p>You might’ve figured out that you can resize panes by dragging the edge of one with your mouse, but we’re developers here, not olympic athletes. What’re we expected to <em>move</em> our hands away from the safety of our keyboard and over to the mouse?! No! I won’t stand for it and neither should you!</p>

<p>It’d be really nice to use the same keys that we use for moving between the panes for resizing (h, j, k, l)… but they’ve already been mapped… we <em>could</em> add another key modifier that needs to be held down when we want to resize vs. move between the panes:</p>

<div><pre><code><span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... add this new entry to your config.keys table</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'h'</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'LEADER|CTRL'</span><span>,</span>
    <span>-- "3" here is the amount of cells we wish to resize</span>
    <span>-- the terminal by</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>AdjustPaneSize</span> <span>{</span> <span>'Left'</span><span>,</span> <span>3</span> <span>},</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>But that’s no good really. We have to first push our leader <code>CTRL + A</code>, then push <code>CTRL + H</code>, and keep repeating that each time we wanna resize the pane to the left. Fingers getting sore. Send help. Oh, here comes WezTerm with the antidote: <a href="https://wezfurlong.org/wezterm/config/key-tables.html">key tables</a>.</p>

<p>When you activate a key table you’re entering a different mode with its own set of assignments for whatever you’re doing. This allows you to have multiple layers of assignments that are context specific.</p>

<p>It’s a similar kind of concept to the leader key, but unlike it, our key table will not automatically deactivate after an action is invoked, so it’ll be a good fit for resizing, where we want to keep pressing the same button over and over again until we’re happy with our pane’s new size.</p>

<p>With all that… this is easier <em>done</em> that said, so let’s check out the code:</p>

<div><pre><code><span>local</span> <span>function</span> <span>resize_pane</span><span>(</span><span>key</span><span>,</span> <span>direction</span><span>)</span>
  <span>return</span> <span>{</span>
    <span>key</span> <span>=</span> <span>key</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>AdjustPaneSize</span> <span>{</span> <span>direction</span><span>,</span> <span>3</span> <span>}</span>
  <span>}</span>
<span>end</span>

<span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... remove the yucky keybinding from above and replace it with this</span>
  <span>{</span>
    <span>-- When we push LEADER + R...</span>
    <span>key</span> <span>=</span> <span>'r'</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>-- Activate the `resize_panes` keytable</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ActivateKeyTable</span> <span>{</span>
      <span>name</span> <span>=</span> <span>'resize_panes'</span><span>,</span>
      <span>-- Ensures the keytable stays active after it handles its</span>
      <span>-- first keypress.</span>
      <span>one_shot</span> <span>=</span> <span>false</span><span>,</span>
      <span>-- Deactivate the keytable after a timeout.</span>
      <span>timeout_milliseconds</span> <span>=</span> <span>1000</span><span>,</span>
    <span>}</span>
  <span>},</span>
<span>}</span>

<span>config</span><span>.</span><span>key_tables</span> <span>=</span> <span>{</span>
  <span>resize_panes</span> <span>=</span> <span>{</span>
    <span>resize_pane</span><span>(</span><span>'j'</span><span>,</span> <span>'Down'</span><span>),</span>
    <span>resize_pane</span><span>(</span><span>'k'</span><span>,</span> <span>'Up'</span><span>),</span>
    <span>resize_pane</span><span>(</span><span>'h'</span><span>,</span> <span>'Left'</span><span>),</span>
    <span>resize_pane</span><span>(</span><span>'l'</span><span>,</span> <span>'Right'</span><span>),</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>Now you can push <code>CTRL + A</code> to activate leader, then <code>R</code> to activate the resizing layer… and movement keys to resize to your heart’s content. When 1,000 milliseconds have elapsed, you’ll automatically exit the resizing layer and be back to the default keytable.</p>

<p>WezTerm intensifies…</p>

<p>(While we’re on multiplexing, if you’re using neovim, I’d recommend checking out <a href="https://github.com/mrjones2014/smart-splits.nvim">smart-splits.nvim</a> - that’ll let you jump between your vim panes and your WezTerm ones).</p>

<h2 id="project-workspaces">Project workspaces</h2>

<p>Okay let’s graduate from WezTerm university with one final assignment… project workspaces.</p>

<p>I’m often working across a few different projects at a time, and need to be able to quickly switch between them. I want each project to maintain its own multiplexer instance with its own windows, panes, and tabs. In tmux you might achieve this with different sessions. In WezTerm we’ll do it with <a href="https://wezfurlong.org/wezterm/recipes/workspaces.html">workspaces</a>.</p>

<h3 id="creating-and-switching-between-workspaces">Creating and switching between workspaces</h3>

<p>Create a new file in your config directory and call it <code>projects.lua</code>. We’ll use this to provide some project switching functions to our main config file.</p>

<div><pre><code><span>local</span> <span>wezterm</span> <span>=</span> <span>require</span> <span>'wezterm'</span>
<span>local</span> <span>module</span> <span>=</span> <span>{}</span>

<span>local</span> <span>function</span> <span>project_dirs</span><span>()</span>
  <span>return</span> <span>{</span>
    <span>'~/Projects/mailgrip'</span><span>,</span>
    <span>'~/Projects/alexplescan.com'</span><span>,</span>
    <span>'~/Projects/wezterm_love_letters'</span><span>,</span>
    <span>-- ... keep going, list all your projects</span>
    <span>-- (or don't if you value your time. we'll improve on this soon)</span>
  <span>}</span>
<span>end</span>

<span>function</span> <span>module</span><span>.</span><span>choose_project</span><span>()</span>
  <span>local</span> <span>choices</span> <span>=</span> <span>{}</span>
  <span>for</span> <span>_</span><span>,</span> <span>value</span> <span>in</span> <span>ipairs</span><span>(</span><span>project_dirs</span><span>())</span> <span>do</span>
    <span>table.insert</span><span>(</span><span>choices</span><span>,</span> <span>{</span> <span>label</span> <span>=</span> <span>value</span> <span>})</span>
  <span>end</span>

  <span>-- The InputSelector action presents a modal UI for choosing between a set of options</span>
  <span>-- within WezTerm.</span>
  <span>return</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>InputSelector</span> <span>{</span>
    <span>title</span> <span>=</span> <span>'Projects'</span><span>,</span>
    <span>-- The options we wish to choose from</span>
    <span>choices</span> <span>=</span> <span>choices</span><span>,</span>
    <span>-- Yes, we wanna fuzzy search (so typing "alex" will filter down to</span>
    <span>-- "~/Projects/alexplescan.com")</span>
    <span>fuzzy</span> <span>=</span> <span>true</span><span>,</span>
    <span>-- The action we want to perform. Note that this doesn't have to be a</span>
    <span>-- static definition as we've done before, but can be a callback that</span>
    <span>-- evaluates any arbitrary code.</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action_callback</span><span>(</span><span>function</span><span>(</span><span>child_window</span><span>,</span> <span>child_pane</span><span>,</span> <span>id</span><span>,</span> <span>label</span><span>)</span>
      <span>-- As a placeholder, we'll log the name of what you picked</span>
      <span>wezterm</span><span>.</span><span>log_info</span><span>(</span><span>"you chose "</span> <span>..</span> <span>label</span><span>)</span>
    <span>end</span><span>),</span>
  <span>}</span>
<span>end</span>

<span>return</span> <span>module</span>
</code></pre></div>

<p>… and in your <code>wezterm.lua</code>:</p>

<div><pre><code><span>local</span> <span>projects</span> <span>=</span> <span>require</span> <span>'projects'</span>

<span>config</span><span>.</span><span>keys</span> <span>=</span> <span>{</span>
  <span>-- ... add these new entries to your config.keys table</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'p'</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>-- Present in to our project picker</span>
    <span>action</span> <span>=</span> <span>projects</span><span>.</span><span>choose_project</span><span>(),</span>
  <span>},</span>
  <span>{</span>
    <span>key</span> <span>=</span> <span>'f'</span><span>,</span>
    <span>mods</span> <span>=</span> <span>'LEADER'</span><span>,</span>
    <span>-- Present a list of existing workspaces</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>ShowLauncherArgs</span> <span>{</span> <span>flags</span> <span>=</span> <span>'FUZZY|WORKSPACES'</span> <span>},</span>
  <span>},</span>
<span>}</span>
</code></pre></div>

<p>Lots going on here, take your time to read it and the comments. And give it a go! Push <code>LEADER + P</code>, and you’ll see the project input selector come up. Pick a project by highlighting one and pushing <code>ENTER</code>, or push <code>CTRL + C</code> to close the picker. Once you’ve picked a project you’ll see its directory logged to your debug overlay (<code>CTRL + SHIFT + L</code>).</p>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-720-835177f65.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-1440-835177f65.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-1520-835177f65.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-720-835177f65.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-1440-835177f65.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-1520-835177f65.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/11_workspace_switcher-800-86fb3a529.png" alt="screenshot of WezTerm's with the workspace switcher we've configured"></picture>

<p>Still a couple of issues though… it’s really annoying to type out all your projects by hand in that file, and, uh, what was the other issue? Oh yeah! When you pick a project nothing happens. Okay, let’s fix these. Back in <code>projects.lua</code>, we’ll start by having the list of projects automatically populate.</p>

<div><pre><code><span>-- The directory that contains all your projects.</span>
<span>local</span> <span>project_dir</span> <span>=</span> <span>wezterm</span><span>.</span><span>home_dir</span> <span>..</span> <span>"/Projects"</span>

<span>local</span> <span>function</span> <span>project_dirs</span><span>()</span>
  <span>-- Start with your home directory as a project, 'cause you might want</span>
  <span>-- to jump straight to it sometimes.</span>
  <span>local</span> <span>projects</span> <span>=</span> <span>{</span> <span>wezterm</span><span>.</span><span>home_dir</span> <span>}</span>

  <span>-- WezTerm comes with a glob function! Let's use it to get a lua table</span>
  <span>-- containing all subdirectories of your project folder.</span>
  <span>for</span> <span>_</span><span>,</span> <span>dir</span> <span>in</span> <span>ipairs</span><span>(</span><span>wezterm</span><span>.</span><span>glob</span><span>(</span><span>project_dir</span> <span>..</span> <span>'/*'</span><span>))</span> <span>do</span>
    <span>-- ... and add them to the projects table.</span>
    <span>table.insert</span><span>(</span><span>projects</span><span>,</span> <span>dir</span><span>)</span>
  <span>end</span>

  <span>return</span> <span>projects</span>
<span>end</span>
</code></pre></div>

<p>(This all assumes that you like to keep your projects grouped together in a folder, if not… well you’ve got Lua at your fingertips to implement whatever you want!)</p>

<p>Now launch the project picker, and what do you see? All those projects staring back at thee.</p>

<p>One thing left to do, let’s add the functionality that opens your project in a new WezTerm workspace. Still in <code>projects.lua</code> let’s change up <code>choose_project</code>:</p>

<div><pre><code><span>function</span> <span>module</span><span>.</span><span>choose_project</span><span>()</span>
  <span>local</span> <span>choices</span> <span>=</span> <span>{}</span>
  <span>for</span> <span>_</span><span>,</span> <span>value</span> <span>in</span> <span>ipairs</span><span>(</span><span>project_dirs</span><span>())</span> <span>do</span>
    <span>table.insert</span><span>(</span><span>choices</span><span>,</span> <span>{</span> <span>label</span> <span>=</span> <span>value</span> <span>})</span>
  <span>end</span>

  <span>return</span> <span>wezterm</span><span>.</span><span>action</span><span>.</span><span>InputSelector</span> <span>{</span>
    <span>title</span> <span>=</span> <span>"Projects"</span><span>,</span>
    <span>choices</span> <span>=</span> <span>choices</span><span>,</span>
    <span>fuzzy</span> <span>=</span> <span>true</span><span>,</span>
    <span>action</span> <span>=</span> <span>wezterm</span><span>.</span><span>action_callback</span><span>(</span><span>function</span><span>(</span><span>child_window</span><span>,</span> <span>child_pane</span><span>,</span> <span>id</span><span>,</span> <span>label</span><span>)</span>
      <span>-- "label" may be empty if nothing was selected. Don't bother doing anything</span>
      <span>-- when that happens.</span>
      <span>if</span> <span>not</span> <span>label</span> <span>then</span> <span>return</span> <span>end</span>

      <span>-- The SwitchToWorkspace action will switch us to a workspace if it already exists,</span>
      <span>-- otherwise it will create it for us.</span>
      <span>child_window</span><span>:</span><span>perform_action</span><span>(</span><span>wezterm</span><span>.</span><span>action</span><span>.</span><span>SwitchToWorkspace</span> <span>{</span>
        <span>-- We'll give our new workspace a nice name, like the last path segment</span>
        <span>-- of the directory we're opening up.</span>
        <span>name</span> <span>=</span> <span>label</span><span>:</span><span>match</span><span>(</span><span>"([^/]+)$"</span><span>),</span>
        <span>-- Here's the meat. We'll spawn a new terminal with the current working</span>
        <span>-- directory set to the directory that was picked.</span>
        <span>spawn</span> <span>=</span> <span>{</span> <span>cwd</span> <span>=</span> <span>label</span> <span>},</span>
      <span>},</span> <span>child_pane</span><span>)</span>
    <span>end</span><span>),</span>
  <span>}</span>
<span>end</span>
</code></pre></div>

<p>Try that out, select a new project, and you’ll see a workspace get created for it. Switch back to your default workspace (we bound so <code>LEADER, CTRL + F</code>  to show you a list of active workspaces) and you’ll see everything is right where you left it.</p>

<h3 id="bonus-improving-the-powerline-and-more-colour-stuff">Bonus: improving the powerline, and more colour stuff</h3>

<p>Let’s add a couple of polishing touches to this workflow and then I promise we’ll be done…</p>

<p>Remember that sad powerline we set up earlier? Let’s make it happier by adding another segment to it which contains the name of the current workspace. In true powerline fashion, each subsequent segment on the powerline will display in a different colour. We’ll explore some of WezTerm’s colour maths support and do this all dynamically based on our theme. Back in <code>wezterm.lua</code>:</p>

<div><pre><code><span>-- Replace the old wezterm.on('update-status', ... function with this:</span>

<span>local</span> <span>function</span> <span>segments_for_right_status</span><span>(</span><span>window</span><span>)</span>
  <span>return</span> <span>{</span>
    <span>window</span><span>:</span><span>active_workspace</span><span>(),</span>
    <span>wezterm</span><span>.</span><span>strftime</span><span>(</span><span>'%a %b %-d %H:%M'</span><span>),</span>
    <span>wezterm</span><span>.</span><span>hostname</span><span>(),</span>
  <span>}</span>
<span>end</span>

<span>wezterm</span><span>.</span><span>on</span><span>(</span><span>'update-status'</span><span>,</span> <span>function</span><span>(</span><span>window</span><span>,</span> <span>_</span><span>)</span>
  <span>local</span> <span>SOLID_LEFT_ARROW</span> <span>=</span> <span>utf8.char</span><span>(</span><span>0xe0b2</span><span>)</span>
  <span>local</span> <span>segments</span> <span>=</span> <span>segments_for_right_status</span><span>(</span><span>window</span><span>)</span>

  <span>local</span> <span>color_scheme</span> <span>=</span> <span>window</span><span>:</span><span>effective_config</span><span>().</span><span>resolved_palette</span>
  <span>-- Note the use of wezterm.color.parse here, this returns</span>
  <span>-- a Color object, which comes with functionality for lightening</span>
  <span>-- or darkening the colour (amongst other things).</span>
  <span>local</span> <span>bg</span> <span>=</span> <span>wezterm</span><span>.</span><span>color</span><span>.</span><span>parse</span><span>(</span><span>color_scheme</span><span>.</span><span>background</span><span>)</span>
  <span>local</span> <span>fg</span> <span>=</span> <span>color_scheme</span><span>.</span><span>foreground</span>

  <span>-- Each powerline segment is going to be coloured progressively</span>
  <span>-- darker/lighter depending on whether we're on a dark/light colour</span>
  <span>-- scheme. Let's establish the "from" and "to" bounds of our gradient.</span>
  <span>local</span> <span>gradient_to</span><span>,</span> <span>gradient_from</span> <span>=</span> <span>bg</span>
  <span>if</span> <span>appearance</span><span>.</span><span>is_dark</span><span>()</span> <span>then</span>
    <span>gradient_from</span> <span>=</span> <span>gradient_to</span><span>:</span><span>lighten</span><span>(</span><span>0</span><span>.</span><span>2</span><span>)</span>
  <span>else</span>
    <span>gradient_from</span> <span>=</span> <span>gradient_to</span><span>:</span><span>darken</span><span>(</span><span>0</span><span>.</span><span>2</span><span>)</span>
  <span>end</span>

  <span>-- Yes, WezTerm supports creating gradients, because why not?! Although</span>
  <span>-- they'd usually be used for setting high fidelity gradients on your terminal's</span>
  <span>-- background, we'll use them here to give us a sample of the powerline segment</span>
  <span>-- colours we need.</span>
  <span>local</span> <span>gradient</span> <span>=</span> <span>wezterm</span><span>.</span><span>color</span><span>.</span><span>gradient</span><span>(</span>
    <span>{</span>
      <span>orientation</span> <span>=</span> <span>'Horizontal'</span><span>,</span>
      <span>colors</span> <span>=</span> <span>{</span> <span>gradient_from</span><span>,</span> <span>gradient_to</span> <span>},</span>
    <span>},</span>
    <span>#</span><span>segments</span> <span>-- only gives us as many colours as we have segments.</span>
  <span>)</span>

  <span>-- We'll build up the elements to send to wezterm.format in this table.</span>
  <span>local</span> <span>elements</span> <span>=</span> <span>{}</span>

  <span>for</span> <span>i</span><span>,</span> <span>seg</span> <span>in</span> <span>ipairs</span><span>(</span><span>segments</span><span>)</span> <span>do</span>
    <span>local</span> <span>is_first</span> <span>=</span> <span>i</span> <span>==</span> <span>1</span>

    <span>if</span> <span>is_first</span> <span>then</span>
      <span>table.insert</span><span>(</span><span>elements</span><span>,</span> <span>{</span> <span>Background</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>'none'</span> <span>}</span> <span>})</span>
    <span>end</span>
    <span>table.insert</span><span>(</span><span>elements</span><span>,</span> <span>{</span> <span>Foreground</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>gradient</span><span>[</span><span>i</span><span>]</span> <span>}</span> <span>})</span>
    <span>table.insert</span><span>(</span><span>elements</span><span>,</span> <span>{</span> <span>Text</span> <span>=</span> <span>SOLID_LEFT_ARROW</span> <span>})</span>

    <span>table.insert</span><span>(</span><span>elements</span><span>,</span> <span>{</span> <span>Foreground</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>fg</span> <span>}</span> <span>})</span>
    <span>table.insert</span><span>(</span><span>elements</span><span>,</span> <span>{</span> <span>Background</span> <span>=</span> <span>{</span> <span>Color</span> <span>=</span> <span>gradient</span><span>[</span><span>i</span><span>]</span> <span>}</span> <span>})</span>
    <span>table.insert</span><span>(</span><span>elements</span><span>,</span> <span>{</span> <span>Text</span> <span>=</span> <span>' '</span> <span>..</span> <span>seg</span> <span>..</span> <span>' '</span> <span>})</span>
  <span>end</span>

  <span>window</span><span>:</span><span>set_right_status</span><span>(</span><span>wezterm</span><span>.</span><span>format</span><span>(</span><span>elements</span><span>))</span>
<span>end</span><span>)</span>
</code></pre></div>

<picture><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-720-d88a78af8.webp 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-1440-d88a78af8.webp 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-1520-d88a78af8.webp 2.11x" type="image/webp"><source srcset="https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-720-d88a78af8.jpg 1.0x, https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-1440-d88a78af8.jpg 2.0x, https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-1520-d88a78af8.jpg 2.11x" type="image/jpeg"><img src="https://alexplescan.com/generated/assets/posts/wezterm/12_status_bar_enhanced-800-df09b5aaf.png" alt="screenshot of WezTerm with an enhanced status line, showing multiple segments in different colours"></picture>

<p>WezTerm delivers yet again. This updated callback supports arbitrary numbers of segments for its powerline. We’ve specified 3 but you could add way more. All this without needing to manually configure what colour we want on each segment, but rather have WezTerm do it for us by creating a gradient based on the currently active theme. Some highlights:</p>

<ul>
  <li>We use <code>wezterm.color.parse</code> to convert a string containing a hex colour code into a <code>Color</code> object (<a href="https://wezfurlong.org/wezterm/config/lua/color/index.html">docs</a>) - this lets us perform more advanced operations on the color.</li>
  <li>The colour scheme’s background colour is still what we want to use as the value that our gradient draws <em>to</em>, but to figure out where the gradient should start, we use either <code>color:darken</code> (<a href="https://wezfurlong.org/wezterm/config/lua/color/darken.html">docs</a>) or <code>color:lighten</code> to create a new colour.</li>
  <li>The gradient itself is made with <code>wezterm.color.gradient</code> (<a href="https://wezfurlong.org/wezterm/config/lua/wezterm.color/gradient.html">docs</a>), which returns a table containing a evenly spaced colours between our <code>gradient_to</code> and <code>gradient_from</code>.</li>
  <li>We then iterate over our powerline segments to create the items required for <code>wezterm.format</code>.</li>
</ul>

<h2 id="where-to-from-here">Where to from here?</h2>

<p>There’s a <a href="https://wezfurlong.org/wezterm/features.html">lot more that WezTerm does</a> and that <a href="https://wezfurlong.org/wezterm/config/lua/general.html">you can do with WezTerm</a>. By now you’ll have a good understanding of WezTerm config fundamentals, but I encourage you to keep exploring!</p>

<p>If you’ve followed this guide step by step, I’d recommend pruning the config down to things that you’ll actually use, rewriting it in your own style, then start sprinkling in your own stuff. Take ownership of this thing! Make your own beautiful WezTerm snowflake!</p>

<p>When you want some inspiration for what you could do next, browse through the <a href="https://wezfurlong.org/wezterm/config/lua/general.html">WezTerm API docs</a> to see what’s possible.</p>

<p>And if you find that you too really like WezTerm, please consider <a href="https://wezfurlong.org/sponsor/">supporting Wez</a> for his great open-source work.</p>
</div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go is my hammer, and everything is a nail (241 pts)]]></title>
            <link>https://www.maragu.dev/blog/go-is-my-hammer-and-everything-is-a-nail</link>
            <guid>41223902</guid>
            <pubDate>Mon, 12 Aug 2024 12:59:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.maragu.dev/blog/go-is-my-hammer-and-everything-is-a-nail">https://www.maragu.dev/blog/go-is-my-hammer-and-everything-is-a-nail</a>, See on <a href="https://news.ycombinator.com/item?id=41223902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div hx-boost="true"><p>You know that old saying:</p>

<blockquote>
Always choose the right tool for the job.<cite>Shakespeare, or someone like that</cite>
</blockquote>

<p>Well, I’ve turned that one on its head. I’m using one tool for all possible tasks, for what I believe are good reasons. But I’m getting ahead of myself.</p>

<p>Let’s start somewhere else. I’m Markus, a solo developer and digital product builder. It’s just me. I have no team or subcontractors, and I intend to keep it that way. I’ve pursued a lifestyle that includes having my own business, because it gives me the most flexibility of how and when I want to work. To structure my work around the rest of my life, so to speak. I want to collaborate, but not manage other people.</p>

<p>As a solo developer, I believe it’s a good idea to choose your tools wisely. Even more so than when in a team setting, having a tech stack that is too complex can bury you in unnecessary busy-work; you’ll end up building infrastructure and nursing your layers of technology all day, instead of, you know, talking to customers and building what they want. Delivering value to someone, to anyone, to the world.</p>

<p>In 2016, I started building stuff in Go. I originally didn’t like it, but I started “getting it”, and it stuck with me. So much that I eventually turned my back on other programming languages. I was writing a lot of Python and Javascript at the time, but I dropped that. I ended up <a href="https://www.golang.dk/">making a course for Go developers</a>, as well as sort-of kind-of <a href="https://www.gomponents.com/">reinventing React for backend Go HTML generation</a>. I now build basically all my software in Go, from tiny one-off tools, to web services, CLIs, and everything in between.</p>

<p>But why? When the common wisdom is to always take the problem at hand, analyze it, and then choose the tools, why would I ignore that and go: “nah, I’ll just use Go again”?</p>

<p>Well, I present to you: REASONS.</p>

<h2>Reason 1: Go can do basically anything</h2>

<p>Yeah, Go can do basically anything. Actually, I’ll widen that statement to <em>all popular programming languages can do basically anything</em>. Yes, most are known for one thing or another, and they definitely have their strengths and weaknesses. But I think at the end of the day, it basically comes down to opinion and taste.</p>

<p>I like Go. It’s simple enough, easy to read, consistent, and a lot of other good things that align well with how I want to build software. There are certainly also things I don’t like, but I don’t care enough to let it bother me. But you probably know all of those things already.</p>

<p>Go is good for the obvious things: building CLIs, cloud infrastructure, HTTP servers, network stuff.</p>

<p>But people also <a href="https://ebitengine.org/">build games for the Nintendo Switch in Go</a>. They build <a href="https://wails.io/">GUI</a> <a href="https://fyne.io/">apps</a> using Go. I’ve even tried a <a href="https://github.com/crnbaker/gostringsynth">string synthesizer in the terminal</a> built in Go, which I thought was beyond cool. You get the point: it’s often possible to do what you want, regardless of the tool you use.</p>

<h2>Reason 2: Less context switching</h2>

<p>Do you know the feeling when you start writing syntax for one programming language, realize that, nope, you’re supposed to be using a different one, and try to switch your brain into another mode while feeling slightly dumb? I know I used to.</p>

<p>But I don’t anymore, because I only use one programming language! Ha! Joke’s on past me.</p>

<p>But it goes much deeper than that. A programming language isn’t just a bunch of syntax to build stuff. It’s <em>also</em> a compiler, an IDE, a toolchain. A community to engage with. A never-ending news machine to keep up with. An ecosystem of libraries, tools, programs, idioms, styles. It can be quite a mouthful on its own, so I found it overwhelming to try to keep up with several. Is it possible? Sure. Will it be more superficial? Also yep.</p>

<p>Which brings me to my third reason.</p>

<h2>Reason 3: Depth of knowledge</h2>

<p>When I don’t have to keep up with everything around several languages, and build everything in just one language, I can go deep. Learn stuff I wouldn’t have gone into otherwise. Investigate nooks and crannies I wouldn’t have given a thought. Learn esoteric language features, history, read obscure blog posts on details I wouldn’t necessarily have bothered with.</p>

<p>That doesn’t mean I ignore the rest of the world, obviously. You and I both know there’s always more to learn in this field. Cutting away programming languages doesn’t make software development less complex in a multitude of other aspects, but it does so in one very concrete setting, and one which I spend a lot of time on.</p>

<h2>So, what, I’m going to limit my career options?</h2>

<p>The world is laaaarge. The number of projects are basically infinite. Even if I carve out a tiny subset of infinite, that’s still infinite. Good enough for me and my career ambitions.</p>

<p>And I’ll be <em>really</em> good at at least this one thing. I’m way more productive in Go than when I was just starting out (obviously, otherwise I should probably have switched to plumbing), and I hope to stay on that path, and maybe even increase the slope. EXPONENTIALLY. Okay, less will do as well.</p>

<p>Anyhoo, these are my thoughts. Bring on the nails, and I’ll beat them with a bloody, determined gopher who is screaming “IS THAT ALL YOU GOT?!?”.</p>

<p>(Also see the <a href="https://news.ycombinator.com/item?id=41223902">Hacker News</a> and <a href="https://www.reddit.com/r/golang/comments/1eqah5h/go_is_my_hammer_and_everything_is_a_nail/">Reddit</a> discussions about this post.)</p><div><p><img src="https://www.maragu.dev/images/markus.jpg" alt="A picture of me, Markus." title="A picture of me, Markus."></p><p>I’m Markus, a professional software consultant and developer. 🤓✨ You can reach me at <a href="mailto:markus@maragu.dk">markus@maragu.dk</a>, or check social links in the page footer.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Government wants to make it easier for you to click the 'unsubscribe' button (427 pts)]]></title>
            <link>https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da</link>
            <guid>41223774</guid>
            <pubDate>Mon, 12 Aug 2024 12:46:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da">https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da</a>, See on <a href="https://news.ycombinator.com/item?id=41223774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>WASHINGTON (AP) — In the name of consumer protection, a slew of U.S. federal agencies are working to make it easier for Americans to click the unsubscribe button for unwanted memberships and recurring payment services. </p><p>A broad new government initiative, dubbed “Time Is Money,” includes a rollout of new regulations and the promise of more for industries spanning from healthcare and fitness memberships to media subscriptions.</p><p>“The administration is cracking down on all the ways that companies, through paperwork, hold times and general aggravation waste people’s money and waste people’s time and really hold onto their money,” Neera Tanden, White House domestic policy adviser, told reporters Friday in advance of the announcement.</p><p>“Essentially in all of these practices, companies are delaying services to you or really trying to make it so difficult for you to cancel the service that they get to hold onto your money for longer and longer,” Tanden said. “These seemingly small inconveniences don’t happen by accident — they have huge financial consequences.”</p>
    

<p>Efforts being rolled out Monday include a new Federal Communications Commission inquiry into whether to impose requirements on communications companies that would make it as easy to cancel a subscription or service as it was to sign up for one. </p>

<p>The Federal Trade Commission in <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.ftc.gov/news-events/news/press-releases/2023/03/federal-trade-commission-proposes-rule-provision-making-it-easier-consumers-click-cancel-recurring" target="_blank" rel="noopener">March 2023 initiated</a></span> “click to cancel” rulemaking requiring companies to let customers end subscriptions as easily as they started them. </p>
    
<p>Also Monday, the heads of the departments of Labor and of Health and Human Services are asking health insurance companies and group health plans to make improvements to customer interactions with their health coverage, and “in the coming months will identify additional opportunities to improve consumers’ interactions with the health care system,” according to a White House summary.</p>
    

<p>The government already has launched several initiatives aimed at improving the consumer experience.</p><p>In October, the FTC announced a proposed rule to <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/biden-junk-fees-ftc-hidden-bogus-80e5c17dc7607b99aa92c6c816b96e52">ban hidden and bogus junk fees</a></span>, which can mask the total cost of concert tickets, hotel rooms and utility bills.</p><p>In April, the Transportation Department finalized rules that would require airlines to automatically issue cash refunds for things like delayed flights and to better disclose <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/american-airlines-points-raising-bag-fees-1530a7ba48a0c6685ce2ef7da58a983a">fees for baggage</a></span> or reservation cancellations.</p><p>The department also has taken actions against individual companies accused of misleading customers. </p><p>In June, the Justice Department, referred by the FTC, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.ftc.gov/system/files/ftc_gov/pdf/040-UnredactedComplaint.pdf" target="_blank" rel="noopener">filed a lawsuit against software maker Adobe</a></span> and two of its executives, Maninder Sawhney and David Wadhwani, for allegedly pushing consumers toward the firm’s “annual paid monthly” subscription without properly disclosing that canceling the plan in the first year could cost hundreds of dollars. </p><p>Dana Rao, Adobe’s general counsel, said in an emailed statement that Adobe disagrees with the lawsuit’s characterization of its business and “we will refute the FTC’s claims in court.”</p><p> “The early termination fees equate to minimal impact to our revenue, accounting for less than half a percent of our total revenue globally, but is an important part of our ability to offer customers a choice in plans that balance cost and commitment,” Rao said.</p>
    

<p>Some business advocates are not a fan of the government’s overall efforts to crack down on junk fees. </p><p>Sean Heather, senior vice president of international regulatory affairs and antitrust at the U.S. Chamber of Commerce, said the initiative is “nothing more than an attempt to micromanage businesses’ pricing structures, often undermining businesses’ ability to give consumers options at different price points.”</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interstellar movie is implemented with Einstein's equations in 40k lines C++ (129 pts)]]></title>
            <link>https://twitter.com/bitfield/status/1020632237493112833</link>
            <guid>41222862</guid>
            <pubDate>Mon, 12 Aug 2024 10:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/bitfield/status/1020632237493112833">https://twitter.com/bitfield/status/1020632237493112833</a>, See on <a href="https://news.ycombinator.com/item?id=41222862">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[GIL Become Optional in Python 3.13 (103 pts)]]></title>
            <link>https://geekpython.in/gil-become-optional-in-python</link>
            <guid>41221292</guid>
            <pubDate>Mon, 12 Aug 2024 05:07:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://geekpython.in/gil-become-optional-in-python">https://geekpython.in/gil-become-optional-in-python</a>, See on <a href="https://news.ycombinator.com/item?id=41221292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrap">

			
			
<!-- #site-header -->


			
			<main id="main" role="main">

				
	
	<div id="content-wrap">

				
				
<article id="post-1758">

	
<p><img width="1600" height="840" src="https://geekpython.in/wp-content/uploads/2024/08/nogil.png" alt="You are currently viewing GIL Become Optional in Python 3.13" itemprop="image" decoding="async" srcset="https://geekpython.in/wp-content/uploads/2024/08/nogil.png 1600w, https://geekpython.in/wp-content/uploads/2024/08/nogil-300x158.png 300w, https://geekpython.in/wp-content/uploads/2024/08/nogil-1024x538.png 1024w, https://geekpython.in/wp-content/uploads/2024/08/nogil-768x403.png 768w, https://geekpython.in/wp-content/uploads/2024/08/nogil-1536x806.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px">
</p><!-- .thumbnail -->


<!-- .entry-header -->






<div itemprop="text">
	
<p><strong>GIL</strong> or <strong>Global Interpreter Lock</strong> can be disabled in Python version 3.13. This is currently experimental.</p>



<p>What is GIL? It is a mechanism used by the CPython interpreter to ensure that only one thread executes the Python bytecode at a time.</p>



<h2>An Experimental Feature</h2>



<p>Python 3.13 brings major new features compared to Python 3.12 and one of them is free-threaded mode, which disables the Global Interpreter Lock, allowing threads to run more concurrently.</p>



<p>This is an experimental feature and if you want to try it, you can download the beta version of Python 3.13 from <a target="_blank" rel="noreferrer noopener nofollow" href="https://www.python.org/downloads/release/python-3130b3/">here</a>.</p>



<p>At the time of installation, check the option “free threaded binaries(experimental)” to get the feature in Python.</p>







<p>The GIL will be disabled when you configure the Python with the <code>--disable-gil</code> option which is nothing but a build configuration (free threading build) at the time of installation.</p>



<p>This will allow optionally enabling and disabling GIL using the environment variable <code>PYTHON_GIL</code> which can be set to <code>1</code> and <code>0</code> respectively.</p>



<p>It will also provide a command-line option <code>-X gil</code> which can also be set to <code>0</code> (disable) and <code>1</code> (enable).</p>



<div id="urvanov-syntax-highlighter-66ba71861e070590410074" data-settings=" no-popup minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="hide">
					
				</td>
						<td><div><p><span># v3.13</span></p><p><span># GIL disabled</span></p><p><span>python3</span><span> </span><span>-</span><span>X</span><span> </span><span>gil</span><span>=</span><span>0</span><span> </span><span>sample</span><span>.</span><span>py</span></p><p><span># GIL enabled</span></p><p><span>python3</span><span> </span><span>-</span><span>X</span><span> </span><span>gil</span><span>=</span><span>1</span><span> </span><span>sample</span><span>.</span><span>py</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>We can also check if the current interpreter is configured with the free threading build (<code>--disable-gil</code>) by using the following code.</p>



<div id="urvanov-syntax-highlighter-66ba71861e076400061923" data-settings=" no-popup minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="hide">
					
				</td>
						<td><div><p><span>import</span><span> </span><span>sysconfig</span></p><p><span>print</span><span>(</span><span>sysconfig</span><span>.</span><span>get_config_var</span><span>(</span><span>"Py_GIL_DISABLED"</span><span>)</span><span>)</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>If we run this, we’ll get either <code>0</code> which means GIL is enabled, or <code>1</code> which means GIL is disabled.</p>



<p>With this, we’ll also get a function that can be used to check if GIL is disabled in the running process.</p>



<div id="urvanov-syntax-highlighter-66ba71861e077401157639" data-settings=" no-popup minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="hide">
					
				</td>
						<td><div><p><span>import</span><span> </span><span>sys</span></p><p><span>sys</span><span>.</span><span>_is_gil_enabled</span><span>(</span><span>)</span><span>&nbsp;&nbsp;</span><span># returns a boolean value</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<h2>GIL Vs No GIL</h2>



<p>Let’s see how the performance of multi-threaded programs will be affected when GIL is enabled and disabled.</p>



<p>We have a simple Python program (<code>gil.py</code>) that computes the factorial of numbers and compares the execution time taken by single-threaded, multi-thread, and multi-process tasks. We’ll run this Python program first <strong>with GIL</strong> and then <strong>without GIL</strong>.</p>



<div id="urvanov-syntax-highlighter-66ba71861e078894044374" data-settings=" no-popup minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="hide">
					<div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p><p>54</p><p>55</p><p>56</p><p>57</p><p>58</p><p>59</p><p>60</p><p>61</p><p>62</p><p>63</p><p>64</p><p>65</p><p>66</p><p>67</p><p>68</p><p>69</p><p>70</p><p>71</p><p>72</p><p>73</p><p>74</p><p>75</p><p>76</p><p>77</p><p>78</p><p>79</p><p>80</p><p>81</p><p>82</p></div>
				</td>
						<td><div><p><span>import</span><span> </span><span>sys</span></p><p><span>import</span><span> </span><span>sysconfig</span></p><p><span>import</span><span> </span><span>math</span></p><p><span>import</span><span> </span><span>time</span></p><p><span>import</span><span> </span><span>threading</span></p><p><span>import</span><span> </span><span>multiprocessing</span></p><p><span>def</span><span> </span><span>compute_factorial</span><span>(</span><span>n</span><span>)</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>return</span><span> </span><span>math</span><span>.</span><span>factorial</span><span>(</span><span>n</span><span>)</span></p><p><span># Single-threaded</span></p><p><span>def</span><span> </span><span>single_threaded_compute</span><span>(</span><span>n</span><span>)</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>for</span><span> </span><span>num </span><span>in</span><span> </span><span>n</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>compute_factorial</span><span>(</span><span>num</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>"Single-threaded: Factorial Computed."</span><span>)</span></p><p><span># Multi-threaded</span></p><p><span>def</span><span> </span><span>multi_threaded_compute</span><span>(</span><span>n</span><span>)</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>threads</span><span> </span><span>=</span><span> </span><span>[</span><span>]</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Create 5 threads</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>for</span><span> </span><span>num </span><span>in</span><span> </span><span>n</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>thread</span><span> </span><span>=</span><span> </span><span>threading</span><span>.</span><span>Thread</span><span>(</span><span>target</span><span>=</span><span>compute_factorial</span><span>,</span><span> </span><span>args</span><span>=</span><span>(</span><span>num</span><span>,</span><span>)</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>threads</span><span>.</span><span>append</span><span>(</span><span>thread</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>thread</span><span>.</span><span>start</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Wait for all threads to complete</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>for</span><span> </span><span>thread</span><span> </span><span>in</span><span> </span><span>threads</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>thread</span><span>.</span><span>join</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>"Multi-threaded: Factorial Computed."</span><span>)</span></p><p><span># Multi-process</span></p><p><span>def</span><span> </span><span>multi_processing_compute</span><span>(</span><span>n</span><span>)</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>processes</span><span> </span><span>=</span><span> </span><span>[</span><span>]</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Create a process for each number in the list</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>for</span><span> </span><span>num </span><span>in</span><span> </span><span>n</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>process</span><span> </span><span>=</span><span> </span><span>multiprocessing</span><span>.</span><span>Process</span><span>(</span><span>target</span><span>=</span><span>compute_factorial</span><span>,</span><span> </span><span>args</span><span>=</span><span>(</span><span>num</span><span>,</span><span>)</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>processes</span><span>.</span><span>append</span><span>(</span><span>process</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>process</span><span>.</span><span>start</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Wait for all processes to complete</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>for</span><span> </span><span>process </span><span>in</span><span> </span><span>processes</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>process</span><span>.</span><span>join</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>"Multi-process: Factorial Computed."</span><span>)</span></p><p><span>def</span><span> </span><span>main</span><span>(</span><span>)</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Checking Version</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>f</span><span>"Python version: {sys.version}"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># GIL Status</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>status</span><span> </span><span>=</span><span> </span><span>sysconfig</span><span>.</span><span>get_config_var</span><span>(</span><span>"Py_GIL_DISABLED"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>if</span><span> </span><span>status </span><span>is</span><span> </span><span>None</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>"GIL cannot be disabled"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>if</span><span> </span><span>status</span><span> </span><span>==</span><span> </span><span>0</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>"GIL is active"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>if</span><span> </span><span>status</span><span> </span><span>==</span><span> </span><span>1</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>"GIL is disabled"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>numlist</span><span> </span><span>=</span><span> </span><span>[</span><span>100000</span><span>,</span><span> </span><span>200000</span><span>,</span><span> </span><span>300000</span><span>,</span><span> </span><span>400000</span><span>,</span><span> </span><span>500000</span><span>]</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Single-threaded Execution</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>start</span><span> </span><span>=</span><span> </span><span>time</span><span>.</span><span>time</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>single_threaded_compute</span><span>(</span><span>numlist</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>end</span><span> </span><span>=</span><span> </span><span>time</span><span>.</span><span>time</span><span>(</span><span>)</span><span> </span><span>-</span><span> </span><span>start</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>f</span><span>"Single-threaded time taken: {end:.2f} seconds"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Multi-threaded Execution</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>start</span><span> </span><span>=</span><span> </span><span>time</span><span>.</span><span>time</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>multi_threaded_compute</span><span>(</span><span>numlist</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>end</span><span> </span><span>=</span><span> </span><span>time</span><span>.</span><span>time</span><span>(</span><span>)</span><span> </span><span>-</span><span> </span><span>start</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>f</span><span>"Multi-threaded time taken : {end:.2f} seconds"</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span># Multi-process Execution</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>start</span><span> </span><span>=</span><span> </span><span>time</span><span>.</span><span>time</span><span>(</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>multi_processing_compute</span><span>(</span><span>numlist</span><span>)</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>end</span><span> </span><span>=</span><span> </span><span>time</span><span>.</span><span>time</span><span>(</span><span>)</span><span> </span><span>-</span><span> </span><span>start</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>print</span><span>(</span><span>f</span><span>"Multi-process time taken&nbsp;&nbsp;: {end:.2f} seconds"</span><span>)</span></p><p><span>if</span><span> </span><span>__name__</span><span> </span><span>==</span><span> </span><span>"__main__"</span><span>:</span></p><p><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>main</span><span>(</span><span>)</span></p></div></td>
					</tr>
				</tbody></table>
			</div>



<h3>Running <code>gil.py</code> with GIL</h3>



<div id="urvanov-syntax-highlighter-66ba71861e07b253258893" data-settings=" no-popup minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="hide">
					
				</td>
						<td><div><p>python<span> </span>gil<span>.</span>py</p><p>Python<span> </span>version<span>:</span><span> </span>3<span>.</span>12<span>.</span>2<span> </span><span>(</span>tags/v3<span>.</span>12<span>.</span>2<span>:</span>6abddd9<span>,</span><span> </span>Feb<span>&nbsp;&nbsp;</span>6<span> </span>2024<span>,</span><span> </span>21<span>:</span>26<span>:</span>36<span>)</span><span> </span><span>[</span>MSC<span> </span>v<span>.</span>1937<span> </span>64<span> </span>bit<span> </span><span>(</span>AMD64<span>)</span><span>]</span></p><p>GIL<span> </span>cannot<span> </span>be<span> </span>disabled</p><p>Single-threaded<span>:</span><span> </span>Factorial<span> </span>Computed<span>.</span></p><p>Single-threaded<span> </span>time<span> </span>taken<span>:</span><span> </span>9<span>.</span>04<span> </span>seconds</p><p>Multi-threaded<span>:</span><span> </span>Factorial<span> </span>Computed<span>.</span></p><p>Multi-threaded<span> </span>time<span> </span>taken<span> </span><span>:</span><span> </span>8<span>.</span>21<span> </span>seconds</p><p>Multi-process<span>:</span><span> </span>Factorial<span> </span>Computed<span>.</span></p><p>Multi-process<span> </span>time<span> </span>taken<span>&nbsp;&nbsp;</span><span>:</span><span> </span>5<span>.</span>64<span> </span>seconds</p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>We have Python v3.12 in which there is no option to check the GIL status, so we got “GIL cannot be disabled”.</p>



<p>The difference is not that much between the single-threaded and multi-threaded but we can see a pretty decent difference in the case of multi-process task.</p>



<h3>Running <code>gil.py</code> without GIL</h3>



<div id="urvanov-syntax-highlighter-66ba71861e07c801887581" data-settings=" no-popup minimize scroll-mouseover">
				<table>
					<tbody><tr>
				<td data-settings="hide">
					
				</td>
						<td><div><p>D<span>:</span>/SACHIN/Python13/python3<span>.</span>13t<span> </span>gil<span>.</span>py</p><p>Python<span> </span>version<span>:</span><span> </span>3<span>.</span>13<span>.</span>0b3<span> </span>experimental<span> </span>free-threading<span> </span>build<span> </span><span>(</span>tags/v3<span>.</span>13<span>.</span>0b3<span>:</span>7b41395<span>,</span><span> </span>Jun<span> </span>27<span> </span>2024<span>,</span><span> </span>16<span>:</span>17<span>:</span>17<span>)</span><span> </span><span>[</span>MSC<span> </span>v<span>.</span>1940<span> </span>64<span> </span>bit<span> </span><span>(</span>AMD64<span>)</span><span>]</span></p><p>GIL<span> </span>is<span> </span>disabled</p><p>Single-threaded<span>:</span><span> </span>Factorial<span> </span>Computed<span>.</span></p><p>Single-threaded<span> </span>time<span> </span>taken<span>:</span><span> </span>9<span>.</span>28<span> </span>seconds</p><p>Multi-threaded<span>:</span><span> </span>Factorial<span> </span>Computed<span>.</span></p><p>Multi-threaded<span> </span>time<span> </span>taken<span> </span><span>:</span><span> </span>4<span>.</span>86<span> </span>seconds</p><p>Multi-process<span>:</span><span> </span>Factorial<span> </span>Computed<span>.</span></p><p>Multi-process<span> </span>time<span> </span>taken<span>&nbsp;&nbsp;</span><span>:</span><span> </span>6<span>.</span>14<span> </span>seconds</p></div></td>
					</tr>
				</tbody></table>
			</div>



<p>This time we have a third beta version of Python 3.13 configured with free threading build and as we can see the GIL is disabled.</p>



<p>But the most important part is we can see a massive difference in the performance of multi-threaded task and on the other side, some degradation can be seen in the performance of multi-process and single-threaded task.</p>



<hr>



<p>🏆<strong>Other articles you might be interested in if you liked this one</strong></p>



<p>✅<a target="_blank" rel="noreferrer noopener" href="https://geekpython.in/threading-module-to-create-threads-in-python">Create multi-threaded Python programs using a threading module</a>.</p>



<p>✅<a target="_blank" rel="noreferrer noopener" href="https://geekpython.in/template-inheritance-in-flask">Template inheritance in Flask</a>.</p>



<p>✅<a target="_blank" rel="noreferrer noopener" href="https://geekpython.in/exec-and-eval">Difference between exec and eval in Python</a>.</p>



<p>✅<a target="_blank" rel="noreferrer noopener" href="https://geekpython.in/tempfile-in-python">Create temporary files and directories using tempfile module in Python</a>.</p>



<p>✅<a target="_blank" rel="noreferrer noopener" href="https://geekpython.in/render-images-from-flask">Upload and display images on the frontend using Flask</a>.</p>



<p>✅<a target="_blank" rel="noreferrer noopener" href="https://geekpython.in/impact-of-learning-rates-on-ml-and-dl-models">How does the learning rate affect the ML and DL models</a>?</p>



<hr>



<p><strong>That’s all for now</strong></p>



<p><strong>Keep Coding✌✌</strong></p>

</div><!-- .entry -->




<!-- .entry-share -->
	<!-- .related-posts -->




</article>

				
			</div><!-- #content-wrap -->

	

	</main><!-- #main -->

	
	
	
		
<!-- #footer -->

	
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blitz: A lightweight, modular, extensible web renderer (405 pts)]]></title>
            <link>https://github.com/DioxusLabs/blitz</link>
            <guid>41221252</guid>
            <pubDate>Mon, 12 Aug 2024 04:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/DioxusLabs/blitz">https://github.com/DioxusLabs/blitz</a>, See on <a href="https://news.ycombinator.com/item?id=41221252">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Blitz: A lightweight, modular, extensible web renderer</h2><a id="user-content-blitz-a-lightweight-modular-extensible-web-renderer" aria-label="Permalink: Blitz: A lightweight, modular, extensible web renderer" href="#blitz-a-lightweight-modular-extensible-web-renderer"></a></p>
<p dir="auto">Blitz is a "native" HTML/CSS renderer built to support the "Dioxus Native" project. It is effectively a lightweight webview except that the JavaScript engine is replaced with a native Rust API which allows Rust reactivity / state management libraries like Dioxus to interface with it directly.</p>
<p dir="auto">Talk to us in: the #native channel in the <a href="https://discord.gg/v4mwT25E" rel="nofollow">Dioxus Discord</a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">It currenly has two frontends:</h4><a id="user-content-it-currenly-has-two-frontends" aria-label="Permalink: It currenly has two frontends:" href="#it-currenly-has-two-frontends"></a></p>
<ul dir="auto">
<li>An HTML/markdown frontend that can render an HTML string. This is useful for previewing HTML and/or markdown files but currently lacks interactivity.</li>
<li>A Dioxus frontend that can render a Dioxus VirtualDom. This has full interactivity support via Dioxus's event handling.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Blitz builds upon:</h4><a id="user-content-blitz-builds-upon" aria-label="Permalink: Blitz builds upon:" href="#blitz-builds-upon"></a></p>
<ul dir="auto">
<li><a href="https://github.com/servo/stylo">Stylo</a> (Firefox's parallel browser-grade CSS engine) for CSS resolution</li>
<li><a href="https://github.com/linebender/vello">Vello</a> + <a href="https://github.com/gfx-rs/wgpu">WGPU</a> for rendering</li>
<li><a href="https://github.com/DioxusLabs/taffy">Taffy</a> for box-level layout</li>
<li><a href="https://github.com/linebender/parley">Parley</a> for text/inline-level layout</li>
<li><a href="https://github.com/AccessKit/accesskit">AccessKit</a> for accessibility</li>
<li><a href="https://github.com/rust-windowing/winit">Winit</a> for windowing and input handling</li>
</ul>
<blockquote>
<p dir="auto">Note: This repo contains a new version of Blitz which uses Stylo. The source code for the old version is still available on the <a href="https://github.com/DioxusLabs/blitz/tree/legacy">legacy</a> branch but is not under active development.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto">The Dioxus renderer:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DioxusLabs/blitz/blob/main/examples/screenshot.png"><img src="https://github.com/DioxusLabs/blitz/raw/main/examples/screenshot.png" alt="screenshot"></a></p>
<p dir="auto">The HTML renderer (rendering google.com):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DioxusLabs/blitz/blob/main/examples/google.png"><img src="https://github.com/DioxusLabs/blitz/raw/main/examples/google.png" alt="screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trying it out</h2><a id="user-content-trying-it-out" aria-label="Permalink: Trying it out" href="#trying-it-out"></a></p>
<ol dir="auto">
<li>Clone this repo</li>
<li>Run an example:
<ul dir="auto">
<li><code>cargo run --example google</code></li>
<li><code>cargo run --example url https://myurl.com</code></li>
<li>Other example available</li>
<li>Add <code>--release</code> for better runtime performance</li>
</ul>
</li>
<li>Press <code>Ctrl/Cmd + +</code> / <code>Ctrl/Cmd + -</code> to change the scaling, press F1 to show layout rectangles</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goals</h2><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<p dir="auto">Blitz is designed to render HTML and CSS - we <em>don't</em> want to support the entirety of browser features (or at least we want to make all such "extra" features opt-in). In our opinion, the browser is bloated for the basic usecase of rendering HTML/CSS.</p>
<p dir="auto">We do intend to support:</p>
<ul dir="auto">
<li>Modern HTML layout (flexbox, grid, table, block, inline, absolute/fixed, etc).</li>
<li>Advanced CSS (complex selectors, media queries, css variables)</li>
<li>HTML Form controls</li>
<li>Accessibility using AccessKit</li>
<li>Extensibility via custom widgets</li>
</ul>
<p dir="auto">Notably we <em>don't</em> provide features like webrtc, websockets, bluetooth, localstorage, etc. In a native app, much of this functionality can be fulfilled using regular Rust crates and doesn't need to be coupled with the renderer.</p>
<p dir="auto">We don't yet have Blitz bindings for other languages (JavaScript, Python, etc) but would accept contributions along those lines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">Blitz is split into several pieces:</p>
<ul dir="auto">
<li><code>blitz-dom</code>: The core DOM abstraction that includes style resolution and layout but not drawing/painting. Combines the best of Stylo and Taffy that allows you to build extendable dom-like structures.</li>
<li><code>blitz</code>: Adds a Vello/WGPU based renderer to <code>blitz-dom</code></li>
<li><code>dioxus-blitz</code>: A dioxus integration layer for blitz. Render your Dioxus app using Blitz. Currently <code>dioxus-blitz</code> also contains the HTML renderer but this will likely be split out into it's own package in future.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<p dir="auto">Blitz is currently <strong>experimental</strong>. We are actively working on bringing into a usable state but we would not yet recommend building apps with it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TODO</h3><a id="user-content-todo" aria-label="Permalink: TODO" href="#todo"></a></p>
<ul>
<li> Core DOM tree abstraction</li>
<li> Parse styles using html5ever</li>
<li> Compute styles for html5ever document</li>
<li> Compute layout with Taffy</li>
<li> Render using WGPU</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering</h3><a id="user-content-rendering" aria-label="Permalink: Rendering" href="#rendering"></a></p>
<ul>
<li> Render to window</li>
<li> Render to image</li>
<li> Gradients</li>
<li> Border/Outline</li>
<li> Raster images (png, jpeg, etc)</li>
<li> Zoom</li>
<li> SVG
<ul>
<li> External SVGs (basic support)</li>
<li> Inline SVGs</li>
</ul>
</li>
<li> Shadows</li>
<li> Animations/Transitions</li>
<li> Standard form controls (Checkbox/Dropdown/slider/etc)</li>
<li> Custom widgets</li>
<li> Shadow elements</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Layout</h3><a id="user-content-layout" aria-label="Permalink: Layout" href="#layout"></a></p>
<ul>
<li> Inline (partial support - implementation still immature)</li>
<li> Block</li>
<li> Flexbox</li>
<li> Grid
<ul>
<li> Named grid lines</li>
<li> Subgrid</li>
</ul>
</li>
<li> Table</li>
<li> Z-index</li>
<li> Additional CSS features
<ul>
<li> <code>box-sizing: content-box</code></li>
<li> <code>calc()</code></li>
<li> <code>position: static</code></li>
<li> <code>direction: rtl</code></li>
<li> <code>transform</code></li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text</h3><a id="user-content-text" aria-label="Permalink: Text" href="#text"></a></p>
<ul>
<li> Font loading
<ul>
<li> System font loading</li>
<li> Web font loading</li>
</ul>
</li>
<li> Text
<ul>
<li> Shaping / Bidi</li>
<li> Layout / line breaking</li>
<li> Font size / line height</li>
<li> Text color</li>
<li> Bold / Italic</li>
<li> Underline / Strikethrough</li>
</ul>
</li>
<li> Text selection</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Input</h3><a id="user-content-input" aria-label="Permalink: Input" href="#input"></a></p>
<ul>
<li> Scrolling
<ul>
<li> The root view</li>
<li> Any <code>overflow: scroll</code> element</li>
</ul>
</li>
<li> Hover detection</li>
<li> Click handling</li>
<li> Text input</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance</h3><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<ul>
<li> Hot reloading</li>
<li> Scrolling without re-resolving style and layout</li>
<li> Style caching</li>
<li> Layout caching</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other</h3><a id="user-content-other" aria-label="Permalink: Other" href="#other"></a></p>
<ul>
<li> Multiwindow</li>
<li> Accessibility tree</li>
<li> Focus</li>
<li> Devtools</li>
<li> Hooks for context menu</li>
<li> use_wgpu_context() to grab an element as an arbitrary render surface</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is dual licensed under the Apache 2.0 and MIT licenses</p>
<p dir="auto">Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in stylo-dioxus by you, shall be licensed as MIT, without any additional terms or conditions.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>