<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 25 Apr 2025 11:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[DeepMind releases Lyria 2 music generation model (221 pts)]]></title>
            <link>https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/</link>
            <guid>43790093</guid>
            <pubDate>Fri, 25 Apr 2025 04:25:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/">https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/</a>, See on <a href="https://news.ycombinator.com/item?id=43790093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
    
  
  
  
    
      

      
      
        
          
            <div>
              
                
                
                  
                  
<div>
    <div>
      <p>Technologies</p>
      

      
    <dl>
      
        <dt>Published</dt>
        <dd><time datetime="2025-04-24">24 April 2025</time></dd>
      
      
        <dt>Authors</dt>
        
      
    </dl>
  

      
    </div>

    
      
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1056-h594-n-nu-rw 2x">
      <img alt="An image of Music AI Sandbox's timeline with a visible audio waveform. A cursor hovers over the &quot;Create&quot; button." height="603" src="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1072-h603-n-nu" width="1072">
    </picture>
    
  
    
  </div>
                
              
                
                
                  
                  <div>
  <h4 data-block-key="lsvmh">Musicians today are drawing inspiration and crafting their sound using a broad ecosystem of tools — from mobile apps to traditional Digital Audio Workstations, specialized plug-ins and hardware. Now, artificial intelligence (AI) is emerging as a powerful new part of this creative toolkit, opening doors to novel workflows and sonic possibilities.</h4><p data-block-key="9nm0a">Google has long collaborated with musicians, producers, and artists in the research and development of music AI tools. Ever since launching the <a href="https://magenta.withgoogle.com/blog/2016/06/01/welcome-to-magenta/" rel="noopener" target="_blank">Magenta</a> project, in 2016, we’ve been exploring how AI can enhance creativity — sparking inspiration, facilitating exploration and enabling new forms of expression, always hand-in-hand with the music community.</p><p data-block-key="f8beg">Our ongoing collaborations led to the creation of <a href="https://blog.youtube/inside-youtube/ai-and-music-experiment/" rel="noopener" target="_blank">Music AI Sandbox</a>, in 2023, which we’ve shared with musicians, producers and songwriters through YouTube’s <a href="https://blog.youtube/inside-youtube/partnering-with-the-music-industry-on-ai/" rel="noopener" target="_blank">Music AI Incubator</a>.</p><p data-block-key="5lk3q">Building upon the work we've done to date, today, we're introducing new features and improvements to Music AI Sandbox, including <a href="https://deepmind.google/technologies/lyria/">Lyria 2</a>, our latest music generation model. We're giving more musicians, producers and songwriters in the U.S. access to experiment with these tools, and are gathering feedback to inform their development.</p><p data-block-key="m6r">We're excited to see what this growing community creates with Music AI Sandbox and encourage interested musicians, songwriters, and producers to sign up <a href="https://docs.google.com/forms/d/e/1FAIpQLSfmU9T4KF-3ks57ACPnXqz4f9CX4guYEJrDhYSft9zAZItn_w/viewform" rel="noopener" target="_blank">here</a>.</p>
</div>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="ak3mo">Music AI Sandbox</h2><p data-block-key="635bc">We created Music AI Sandbox in close collaboration with musicians. Their input guided our development and experiments, resulting in a set of responsibly created tools that are practical, useful and can open doors to new forms of music creation.</p><p data-block-key="3rm1k">The Music AI Sandbox is a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. Artists can generate fresh instrumental ideas, craft vocal arrangements or simply break through a creative block.</p><p data-block-key="5svk0">With these tools, musicians can discover new sounds, experiment with different genres, expand and enhance their musical libraries, or develop entirely new styles. They can also push further into unexplored territories — from unique soundscapes to their next creative breakthrough.</p>
</div>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="ak3mo">Create new musical parts</h3><p data-block-key="8r0n0">Quickly try out music ideas by describing what kind of sound you want — the Music AI Sandbox understands genres, moods, vocal styles and instruments. The Create tool helps generate many different music samples to spark the imagination or for use in a track. Artists can also place their own lyrics on a timeline and specify musical characteristics, like tempo and key.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-3670bac7-9da5-4eaf-85fb-52df0ce1ede5">
  

  <figcaption>
      <p data-block-key="4apvk">Animation of Music AI Sandbox’s interface, showing how to use the Create feature.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="ak3mo">Explore new directions with Extend</h3><p data-block-key="al12t">Need inspiration for where to take an existing musical piece? The Extend feature generates musical continuations based on uploaded or generated audio clips. It’s a way to hear potential developments for your ideas, reimagine your own work, or overcome writer's block.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-b1304d21-7c4e-44d7-b641-0b3b575a6ca3">
  

  <figcaption>
      <p data-block-key="ulaih">Animation of Music AI Sandbox’s interface, showing how to use the Extend feature.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="ak3mo">Reimagine music with Edit</h3><p data-block-key="9ut5j">Reshape music with fine-grained control. The Edit feature makes it possible to transform the mood, genre or style of an entire clip, or make targeted modifications to specific parts. Intuitive controls enable subtle tweaks or dramatic shifts. Now, users can also transform audio using text prompts, experiment with preset transformations to fill gaps or blend clips and build transitions between different musical sections.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-8f42a435-76f6-4f93-bf3e-377bade49aa6">
  

  <figcaption>
      <p data-block-key="er6tt">Animation of Music AI Sandbox’s interface, showing how to use the Edit feature.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="ua69x">What artists are creating with the Music AI Sandbox</h2><p data-block-key="9k4pn">See how musicians are leveraging this tool to fuel their creativity and generate fresh musical concepts.</p>
</div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <p data-block-key="3rq62">Listen to these demo tracks that artists are bringing to life using the Music AI Sandbox:</p>
                
              
                
                
                  
                  
                
              
                
                
                  
                  <div>
  <h2 data-block-key="8g3r1">High-fidelity and real-time music with Lyria</h2><p data-block-key="efiae">Since introducing Lyria, we’ve continued to innovate with input and insights from music industry professionals. Our latest music generation model, <a href="https://deepmind.google/technologies/lyria/">Lyria 2</a>, delivers high-fidelity music and professional-grade audio outputs that capture subtle nuances across a range of genres and intricate compositions.</p><p data-block-key="8uhj0">We’ve also developed <a href="https://deepmind.google/technologies/lyria/realtime/">Lyria RealTime</a>, which allows users to interactively create, perform and control music in real-time, mixing genres, blending styles and shaping audio moment by moment. Lyria RealTime can help users create continuous streams of music, forge sonic connections and quickly explore ideas on the fly.</p><p data-block-key="d656g">Responsibly deploying generative technologies is core to our values, so all music generated by Lyria 2 and Lyria RealTime models is watermarked using our <a href="https://deepmind.google/technologies/synthid/">SynthID</a> technology.</p>
</div>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="hddx5">Building AI for musicians, with musicians</h2><p data-block-key="ctoh8">Through collaborations like Music AI Sandbox, we aim to build trust with musicians, the industry and artists. Their expertise and valuable feedback help us ensure our tools empower creators, enabling them to realize the possibilities of AI in their art and explore new ways to express themselves. We’re excited to see what artists create with our tools and look forward to sharing more later this year.</p>
</div>
                
              
                
                
                  
                  

<section>
  

  <ul>
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://docs.google.com/forms/d/e/1FAIpQLSfmU9T4KF-3ks57ACPnXqz4f9CX4guYEJrDhYSft9zAZItn_w/viewform" rel="noopener" target="_blank">
      <span>Sign up for the Music AI Sandbox waitlist</span>
      
    </a>
            </gemini-button>
        </li>
        
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://deepmind.google/technologies/lyria/">
      <span>Learn more about Lyria 2</span>
      
    </a>
            </gemini-button>
        </li>
        
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://deepmind.google/technologies/lyria/realtime/">
      <span>Learn more about Lyria RealTime</span>
      
    </a>
            </gemini-button>
        </li>
        
    
  </ul>
</section>
                
              
                
                
                  
                  <div>
      <p data-block-key="r4x9w">Music AI Sandbox was developed by Adam Roberts, Amy Stuart, Ari Troper, Beat Gfeller, Chris Deaner, Chris Reardon, Colin McArdell, DY Kim, Ethan Manilow, Felix Riedel, George Brower, Hema Manickavasagam, Jeff Chang, Jesse Engel, Michael Chang, Moon Park, Pawel Wluka, Reed Enger, Ross Cairns, Sage Stevens, Tom Jenkins, Tom Hume and Yotam Mann. Additional contributions provided by Arathi Sethumadhavan, Brian McWilliams, Cătălina Cangea, Doug Fritz, Drew Jaegle, Eleni Shaw, Jessi Liang, Kazuya Kawakami, and Veronika Goldberg.</p><p data-block-key="hp34">Lyria 2 was developed by Asahi Ushio, Beat Gfeller, Brian McWilliams, Kazuya Kawakami, Keyang Xu, Matej Kastelic, Mauro Verzetti, Myriam Hamed Torres, Ondrej Skopek, Pavel Khrushkov, Pen Li, Tobenna Peter Igwe and Zalan Borsos. Additional contributions provided by Adam Roberts, Andrea Agostinelli, Benigno Uria, Carrie Zhang, Chris Deaner, Colin McArdell, Eleni Shaw, Ethan Manilow, Hongliang Fei, Jason Baldridge, Jesse Engel, Li Li, Luyu Wang, Mauricio Zuluaga, Noah Constant, Ruba Haroun, Tayniat Khan, Volodymyr Mnih, Yan Wu and Zoe Ashwood.</p><p data-block-key="5qeas">Special thanks to Aäron van den Oord, Douglas Eck, Eli Collins, Mira Lane, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the development process.</p><p data-block-key="blpin">We also acknowledge the many other individuals who contributed across Google DeepMind and Alphabet, including our colleagues at YouTube (a particular shout out to the YouTube Artist Partnerships team led by Vivien Lewit for their support partnering with the music industry).</p>
    </div>
                
              
                
                
                  
                  



  
    
  

                
              
            </div>
          
        
      

      
    
  
  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Notation as a Tool of Thought (1979) (146 pts)]]></title>
            <link>https://www.jsoftware.com/papers/tot.htm</link>
            <guid>43789593</guid>
            <pubDate>Fri, 25 Apr 2025 02:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jsoftware.com/papers/tot.htm">https://www.jsoftware.com/papers/tot.htm</a>, See on <a href="https://news.ycombinator.com/item?id=43789593">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Street address errors in Google Maps (128 pts)]]></title>
            <link>https://randomascii.wordpress.com/2025/04/24/google-maps-doesnt-know-how-street-addresses-work/</link>
            <guid>43788832</guid>
            <pubDate>Fri, 25 Apr 2025 00:01:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://randomascii.wordpress.com/2025/04/24/google-maps-doesnt-know-how-street-addresses-work/">https://randomascii.wordpress.com/2025/04/24/google-maps-doesnt-know-how-street-addresses-work/</a>, See on <a href="https://news.ycombinator.com/item?id=43788832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>I was driving around Vernon, BC a few weeks ago and I asked Google Maps for directions to 3207 30th Ave. It confidently told me where to go but luckily my passenger noticed that it was actually directing me to 3207 <em>34th</em> Ave, four blocks north. Well that’s odd.</p>
<p>A few days later my cousin asked me (as the <a href="https://randomascii.wordpress.com/2024/10/01/life-death-and-retirement/">ex-Google</a> still-nerd member of the family) if I could help with a Google Maps issue. The problem was that the address 138 W 6th Ave in Vancouver was being mapped at a location 2.4 km (that’s 1.5 miles or 123 furlongs) away from the actual location.</p>

<p>I could visualize the absurdity of where it maps the W 6th Ave address by asking Google Maps for directions between 136 W 6th Ave and 138 W 6th Ave. These addresses are adjacent in real life, but Google Maps gave me this:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image.png"><img width="650" height="375" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb.png?w=650&amp;h=375"></a></p>
<p>That’s a long walk to get to the building next door.</p>
<p>There’s another fun way to visualize this bug. Search for “Clark &amp; Page Casting Studios” in Google Maps. Then copy its address, shown in Google Maps, to the clipboard and ask for directions <em>to</em> Clark &amp; Page Casting Studios <em>from</em> its address. This should be a zero-meter walk, but of course it isn’t. Instead it is, no surprise, a 2.4 km walk from Clark &amp; Page Casting Studios to its address. Fun!</p>
<p>Or this silliness. If you navigate from “138 W 6th Ave Unit 1B” to “138 W 6th Ave #2b” then it is, you guessed it, a 2.4 km walk.</p>
<p>This error was pointed out to me because apparently aspiring actors kept going to the wrong place and being late for their auditions. These mistakes have real-world consequences.</p>
<h2>There are more</h2>
<p>Finding one error is curious, but two suggests a pattern. I started browsing Google Maps looking for addresses that seemed out of place. I quickly found three more.</p>
<p>1951 W 19th Ave in Vancouver is mapped at a 2.1 km walk from where its address should logically be. It should be in the 1900 block of W 19th Ave but is instead placed ten blocks away by Google Maps:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-1.png"><img width="648" height="336" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-1.png?w=648&amp;h=336"></a></p>
<p>1355 W 17th Ave, North Vancouver is a particularly odd case because it is mapped as being in the wrong city (in Vancouver instead of North Vancouver), but on the right street (W 17th Ave) but in the wrong block (the 900 block instead of the 1300 block). As it turns out W 17th Ave doesn’t actually exist in North Vancouver. What is going on?</p>
<h2>Typos? Street View?</h2>
<p>The answer might be typos. 138 W 6th Ave is being mapped at the location where I would expect to find 1038 W 16th Ave located – a pair of single-digit errors. This requires that somebody/something made two errors when entering the address for 1038 W 16th Ave. The problem with this explanation is that 1038 W 16th Ave doesn’t exist – I cycled over there to check and the addresses go straight from 1020 to 1040.</p>
<p>3207 30th Ave in Vernon got a 30 changed to a 34. Maybe that was a typo?</p>
<p>1951 W 19th Ave is mapped where I would expect to find 951 W 19th Ave. This is another single-digit error. This one is less harmful because (again, I cycled over to check) there is no 1951 W 19th Ave, and 1951 and 951 W 19th Ave both map to roughly the same place. If you ask for directions from 951 to 1951 W 19th Ave (which should be ten blocks) you get these 0.0 km directions:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-2.png"><img width="605" height="393" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-2.png?w=605&amp;h=393"></a></p>
<p>1355 W 17th Ave, North Vancouver is harder to explain. It was mapped adjacent to 979 W 17th Ave, Vancouver. This error severely stretches the definition of “typo” since nothing but the street name is correct (Vancouver and North Vancouver are different cities, separated by Vancouver Harbour).</p>
<p>I also noticed an anomaly in 5 Montcalm St, Vancouver. This address is in the 1300 block of Montcalm so the address makes no sense. I visited this location as well and the building address is actually 1131 W 16th Ave (the house is on a corner) and there is a five on one of the doors on the Montcalm side. Further creeping around the house revealed that there are five units inside the house – the five is a unit number, not a street number! Now I started wondering if a person or AI had seen the five on the door on Montcalm St and assumed that it was an address.</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/pxl_20250424_173954222.jpg"><img width="640" height="453" title="PXL_20250424_173954222" alt="PXL_20250424_173954222" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/pxl_20250424_173954222_thumb.jpg?w=640&amp;h=453"></a></p>
<h2>Internals guesswork</h2>
<p>The fact that Google Maps can have these errors – that apparently the mapped location of addresses need have no relationship to the layout of the city’s streets – makes it clear that Google Maps has no concept of how street addresses work. There are many rules for how most addresses work in Vancouver but Google Maps appears to have no knowledge of these rules.</p>
<p>It appears that there is an address database somewhere – created by Google Maps, or the cities in BC, or perhaps from Street View data. Somehow that database seems to allow addresses to be mapped to parcels of land and when the address of a parcel of land is entered (by a human being or an AI bot) the database software happily accepts any address and maps it to the parcel, with no sanity checks to make sure it makes sense. Possibly sanity checks that are needed include:</p>
<ul>
<li>Is the parcel in the geographical bounds of the city name entered?</li>
<li>Is the parcel in the vicinity of the road name entered?</li>
<li>Is the parcel in the correct hundred block for the road name entered?</li>
</ul>
<p>These checks would detect all five of the errors that I found.</p>
<p>The hundred-block check only makes sense in some cities. In others it might be better to just do a comparison with nearby numbers, or perhaps skip that check completely. And there are enough weird addresses in the world that these checks probably just have to be a suggestion rather than a hard blocker.</p>
<p>Since there are apparently a lot of these bad addresses in the wild (my ability to find five errors in two cities this quickly suggests there must be many thousands) it seems that somebody needs to run a batch process over the database to find these errors – me scrolling through the map really doesn’t scale well.</p>
<p>While it seems clear that Google Maps uses an address database to map arbitrary addresses to parcels of land, it is also capable of guessing where an address would be if that address existed. That is, if I ask it to map the non-existent addresses 1953, 1955, 1957, 1959, and 1961 on W 19th Ave it places the address balloon in plausible locations, interpolating between 1947 and 1981 (the surrounding “real” addresses). This suggests that Google Maps has the knowledge and heuristics needed to correctly place 138 W 16th Ave, but this knowledge is then overridden by a database that contains errors. Fun!</p>
<h2>Something new?</h2>
<p>I talked to the business at 138 W 6th Ave and they said that these problems are new – starting around mid March. I don’t remember noticing this type of error before so it does seem like Google Maps might have just ingested a batch of bad data.</p>
<h2>Attempted fixes</h2>
<p>When I encountered the first two errors I confidently said that I’d use the Google Maps feedback tool to get the errors fixed. I’ve had good luck in the past with this. But this time my luck ran out.</p>
<p>I dutifully submitted feedback for “Wrong pin location or address”:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-3.png"><img width="516" height="281" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-3.png?w=516&amp;h=281"></a></p>
<p>And I got an email the next day saying that my edit was accepted:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-4.png"><img width="562" height="299" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-4.png?w=562&amp;h=299"></a></p>
<p>But it’s been 14 days and the address still maps incorrectly.</p>
<p>I had better luck with my edit to 3207 30th Ave that was accepted the same day. That fix actually went live sometime between April 17th and April 23rd. That is still nowhere near the promised 24-hour latency, but at least it showed up eventually. Maybe the 138 W 6th Ave edit will still go live?</p>
<h2>Not all errors are equal</h2>
<p>The first two errors that I found – 3207 30th Ave in Vernon and 138 W 6th Ave in Vancouver – are problematic because those addresses are real and Google Maps plots them incorrectly. This leads to people going to the wrong place.</p>
<p>The other errors are less important because they are non-existent addresses that are plotted in nonsensical places. This is mostly harmless.</p>
<h2>Anybody else seeing this?</h2>
<p>If you have noticed any similar anomalies then please share them in the comments.</p>
<p>If you work on Google Maps please <a href="https://bsky.app/profile/randomascii.bsky.social">reach out to me</a> if you have any information that you can share. I’ve tried reaching out through some ex-coworker friends, but no luck so far.</p>
<h2>Discussion</h2>
<p><a title="https://bsky.app/profile/randomascii.bsky.social/post/3lnlwmoayks2s" href="https://bsky.app/profile/randomascii.bsky.social/post/3lnlwmoayks2s">https://bsky.app/profile/randomascii.bsky.social/post/3lnlwmoayks2s</a></p>
<p><a title="https://news.ycombinator.com/item?id=43788832" href="https://news.ycombinator.com/item?id=43788832">https://news.ycombinator.com/item?id=43788832</a></p>
<p><a title="https://www.reddit.com/r/GoogleMaps/comments/1k77440/google_maps_doesnt_know_how_street_addresses_work/" href="https://www.reddit.com/r/GoogleMaps/comments/1k77440/google_maps_doesnt_know_how_street_addresses_work/">https://www.reddit.com/r/GoogleMaps/comments/1k77440/google_maps_doesnt_know_how_street_addresses_work/</a></p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft subtracts C/C++ extension from VS Code forks (176 pts)]]></title>
            <link>https://www.theregister.com/2025/04/24/microsoft_vs_code_subtracts_cc_extension/</link>
            <guid>43788125</guid>
            <pubDate>Thu, 24 Apr 2025 22:18:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/04/24/microsoft_vs_code_subtracts_cc_extension/">https://www.theregister.com/2025/04/24/microsoft_vs_code_subtracts_cc_extension/</a>, See on <a href="https://news.ycombinator.com/item?id=43788125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Microsoft's C/C++ extension for Visual Studio Code (VS Code) no longer works with derivative products such as VS Codium and Cursor – and some developers are crying foul.</p>
<p>In early April, programmers using VS Codium, an open-source fork of Microsoft's MIT-licensed <a target="_blank" rel="nofollow" href="https://github.com/microsoft/vscode">VS Code</a>, and Cursor, a commercial AI code assistant built from the VS Code codebase, noticed that the <a target="_blank" rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools">C/C++ extension</a> <a target="_blank" rel="nofollow" href="https://github.com/getcursor/cursor/issues/2976">stopped</a> <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300">working</a>.</p>
<p>The extension adds C/C++ language support, such as Intellisense code completion and debugging, to VS Code. The removal of these capabilities from competing tools breaks developer workflows, hobbles the editor, and arguably hinders competition.</p>

    

<p>The breaking change appears to have occurred with the release of v1.24.5 on April 3, 2025.</p>

        


        

<p>Following the April update, attempts to install the C/C++ extension outside of VS Code generate this error message: "The C/C++ extension may be used only with Microsoft Visual Studio, Visual Studio for Mac, Visual Studio Code, Azure DevOps, Team Foundation Server, and successor Microsoft products and services to develop and test your applications."</p>
<p>Microsoft has forbidden the use of its extensions outside of its own software products <a target="_blank" rel="nofollow" href="https://github.com/microsoft/vscode-cpptools/commit/1a03dd2a1d37e41359d3f2352bd889e8059237bf">since at least September 2020</a>, when the current licensing terms were published. But it hasn't enforced those terms in its C/C++ extension with <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300#issuecomment-2779861379">an environment check</a> in its binaries until now.</p>

        

<p>(Microsoft's PyLance extension for Python coding <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300#issuecomment-2779864293">is said</a> to have exhibited this behavior for years, preventing its use in VS Code forks.)</p>
<blockquote>

<p>The latest releases of the specific extensions no longer work in Cursor or other non-MSFT editors</p>
</blockquote>
<p>Michael Truell, co-founder and CEO of Anysphere, which makes Cursor, said in the discussion thread two weeks ago that a temporary fix has been rolled out and a more permanent solution is planned.</p>
<p>"MSFT has a handful of extensions which are closed-source," he <a target="_blank" rel="nofollow" href="https://github.com/getcursor/cursor/issues/2976#issuecomment-2787079188">wrote</a>, pointing to Remote Access, Pylance, C/C++, and C#. "The latest releases of the specific extensions no longer work in Cursor or other non-MSFT editors.</p>
<p>"Moving forward, Cursor is transitioning away from these extensions. We are investing in open-source alternatives which already exist in the community and will bundle these into the next version to enable a seamless transition."</p>
<p>Cursor <a target="_blank" rel="nofollow" href="https://github.com/getcursor/cursor/issues/2976#issuecomment-2782541940">allegedly</a> has been flouting Microsoft terms-of-service rules for some time now by setting up a reverse proxy to mask its network requests to the endpoints used by the Microsoft Visual Studio Marketplace. This allows Cursor users to install VS Code extensions from Microsoft's market. Other VS Code forks tend to point to <a target="_blank" rel="nofollow" href="https://github.com/eclipse/openvsx">Open VSX</a>, an alternative extension marketplace.</p>

        

<p>Truell did not respond to a request for comment.</p>
<ul>

<li><a href="https://www.theregister.com/2025/04/23/whats_worth_teaching_when_ai/">As ChatGPT scores B- in engineering, professors scramble to update courses</a></li>

<li><a href="https://www.theregister.com/2025/04/24/microsoft_mystery_folder_fix/">Microsoft mystery folder fix might need a fix of its own</a></li>

<li><a href="https://www.theregister.com/2025/04/24/ninite_rebuild_windows/">Ninite to win it: How to rebuild Windows without losing your mind</a></li>

<li><a href="https://www.theregister.com/2025/04/23/microsoft_365_copilot_agent_refresh/">Microsoft 365 Copilot gets a new crew, including Researcher and Analyst bots</a></li>
</ul>
<p>Meanwhile, users of VS Codium are <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300">looking for</a> free (as in freedom) and open source alternatives.</p>
<p>Developers discussing the issue in Cursor's GitHub repo have noted that Microsoft <a target="_blank" rel="nofollow" href="https://x.com/code/status/1908207162322460710">recently rolled out</a> a competing AI software agent capability, <a target="_blank" rel="nofollow" href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">dubbed Agent Mode</a>, within its Copilot software.</p>
<p>One such developer who contacted us anonymously told <em>The Register</em> they sent a letter about the situation to the US Federal Trade Commission, asking them to probe Microsoft for unfair competition - alleging self-preferencing, bundling Copilot without a removal option, and blocking rivals like Cursor to lock users into its AI ecosystem.</p>
<p>Microsoft did not immediately respond to a request for comment. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists Develop Artificial Leaf, Uses Sunlight to Produce Valuable Chemicals (166 pts)]]></title>
            <link>https://newscenter.lbl.gov/2025/04/24/scientists-develop-artificial-leaf-that-uses-sunlight-to-produce-valuable-chemicals/</link>
            <guid>43788053</guid>
            <pubDate>Thu, 24 Apr 2025 22:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newscenter.lbl.gov/2025/04/24/scientists-develop-artificial-leaf-that-uses-sunlight-to-produce-valuable-chemicals/">https://newscenter.lbl.gov/2025/04/24/scientists-develop-artificial-leaf-that-uses-sunlight-to-produce-valuable-chemicals/</a>, See on <a href="https://news.ycombinator.com/item?id=43788053">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-56089" aria-label="Scientists Develop Artificial Leaf That Uses Sunlight to Produce Valuable Chemicals">
  <div>

    <div>

      
<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <div>
<h4>Key Takeaways</h4>
<ul>
<li><span>The Liquid Sunlight Alliance is a multi-institutional collaboration working to develop the tools needed to use energy from sunlight to produce liquid fuels.</span></li>
<li><span>Researchers built a perovskite and copper-based device that converts carbon dioxide into C</span><span>2</span><span> products </span><span>–</span><span> precursory chemicals of innumerable products in our everyday lives, from plastic polymers to jet fuel.</span></li>
<li><span>This proof-of-concept research opens new opportunities for energy research.</span></li>
</ul>
</div>
<p><span>Researchers from the Department of Energy’s Lawrence Berkeley National Laboratory (Berkeley Lab) along with international collaborators have brought us one step closer to harnessing the sun’s energy to convert carbon dioxide into liquid fuel and other valuable chemicals. In a recent </span><a href="https://www.nature.com/articles/s41929-025-01292-y" target="_blank" rel="noopener"><span>publication</span></a><span> in </span><i><span>Nature Catalysis</span></i><span>, the researchers debut a self-contained carbon-carbon (C2) producing system that combines the catalytic power of copper with </span><a href="https://www.energy.gov/eere/solar/perovskite-solar-cells#:~:text=Perovskites%20are%20a%20family%20of,as%20fuel%20cells%20and%20catalysts." target="_blank" rel="noopener"><span>perovskite</span></a><span>, a material used in photovoltaic solar panels. This advance builds on over 20 years of research and brings the scientific community one step closer to replicating the productivity of a green leaf in nature.&nbsp;</span></p>
<p><span>This work is part of a larger initiative, the Liquid Sunlight Alliance (</span><a href="https://www.liquidsunlightalliance.org/" target="_blank" rel="noopener"><span>LiSA</span></a><span>), which is a Fuels from Sunlight Energy Innovation Hub funded by the U.S. Department of Energy. Led by Caltech in close partnership with Berkeley Lab, LiSA brings together more than 100 scientists from national lab partners at SLAC and the National Renewable Energy Laboratory, and university partners at UC Irvine, UC San Diego, and the University of Oregon</span><span>. </span><span>Researchers involved in this multi-institutional collaboration have made advances in developing our understanding of and the tools needed to develop liquid fuels generated from sunlight, carbon dioxide, and water. (Learn more about the LiSA collaboration in this </span><a href="https://newscenter.lbl.gov/2024/08/29/five-ways-lisa-is-advancing-solar-fuels/"><span>roundup</span></a><span>, “Five Ways LiSA is Advancing Solar Fuels.”)</span></p>
  </lbl-rich-text>
</lbl-container>



<lbl-container theme="white">
  <lbl-section-header title="">
      </lbl-section-header>
</lbl-container>

<lbl-container theme="white">
  <lbl-grid columns="2" layout="grid-card">

<lbl-grid-card layout="gallery-card" date="" text="" link-url="" link-target="" title="">

            <lbl-image slot="media" img-caption="Closeup of the perovskite and copper-based devices developed by a multi-institutional collaboration working to develop the tools needed to turn sunlight into liquid fuel." img-credit="(Credit: Marilyn Sargent/Berkeley Lab)">
        <img decoding="async" width="890" height="665" src="https://newscenter.lbl.gov/wp-content/uploads/2025/04/Gallery1_890x665px_XBD-202503-035-015.jpg" alt="A blue, gloved hands puts a perovskite and copper-based device in line with other copper-based devices." slot="media">      </lbl-image>

    
    
  
  
</lbl-grid-card>


<lbl-grid-card layout="gallery-card" date="" text="" link-url="" link-target="" title="">

            <lbl-image slot="media" img-caption="Artistic depiction of an artificial tree with copper nanoflowers wired to perovskite crystals. " img-credit="(Credit: Virgil Andrei)">
        <img decoding="async" width="500" height="500" src="https://newscenter.lbl.gov/wp-content/uploads/2025/04/ezgif.com-optimize.gif" alt="Artistic depiction of an artificial tree with copper nanoflowers wired to perovskite crystals." slot="media">      </lbl-image>

    
    
  
  
</lbl-grid-card>

  </lbl-grid>


</lbl-container>



<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <p><span>“Nature was our inspiration,” said Peidong Yang, a senior faculty scientist in Berkeley Lab’s Materials Sciences Division and UC Berkeley professor of chemistry and materials science and engineering involved in the published work. “We had to work on the individual components first, but when we brought everything together and realized that it was successful, it was a very exciting moment.”&nbsp;</span></p>
<p><span>To build a system that mimics photosynthesis, Yang and his team followed the natural processes that occur in the leaf of a plant. Each individual component of a leaf’s photosynthesizing elements had to be replicated and refined. Tapping into the decades’ worth of research, the scientists used lead halide perovskite photoabsorbers to imitate a leaf’s light-absorbing chlorophyll. And inspired by enzymes that regulate photosynthesis in nature, they designed electrocatalysts made of copper that resemble tiny flowers.</span></p>
  </lbl-rich-text>
</lbl-container>



<lbl-container theme="white" wrapper-size="sm">
  <lbl-audio episode="" date="" title="" schema="{
    " @context":="" "http:="" schema.org",="" "@type":="" "audioobject",="" "description":="" "",="" "duration":="" "uploaddate":="" "name":="" "publisher":="" {="" "organization",="" "berkeley="" lab",="" "logo":="" "imageobject",="" "url":="" "https:="" www.buzzsprout.com="" 2206573="" episodes="" 17016953"="" }="" },="" "thumbnailurl":="" ""="" }"="" header-type="h2">
        <lbl-rich-text slot="rich-text">
          </lbl-rich-text>
          <lbl-button link-target="_blank" link-url="https://www.buzzsprout.com/2206573/episodes/17016953" text="View the transcript" slot="inline-go-btn" type="inline-go">
      </lbl-button>
      </lbl-audio>
</lbl-container>



<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <p><span>Previous experiments have successfully replicated photosynthesis through the use of biological materials, but this work incorporated an inorganic material, copper. While the selectivity of copper is lower than biological alternatives, the inclusion of copper presents a more durable, stable, and longer-lasting option for the artificial leaf system design.</span></p>
<p><span>Work led by researchers in the LiSA project developed the cathode and anode components of the new device. Instruments at Berkeley Lab’s </span><a href="https://foundry.lbl.gov/" target="_blank" rel="noopener"><span>Molecular Foundry</span></a><span> allowed Yang’s team to integrate the device with metal contacts. During the experiments in Yang’s lab, a solar simulator mimicking a consistently bright sun was used to test the selectivity of the new device.&nbsp;</span></p>
<p><span>Prior innovations across research groups enabled an organic oxidation reaction to take place in the photoanode chamber and created C2 products in the photocathode chamber. This breakthrough created a realistic artificial-leaf architecture in a device about the size of a postage stamp </span><span>–</span><span> it converts CO<sub>2</sub> into a C2 molecule using only sunlight.&nbsp;</span></p>
<p><span>The C2 chemicals produced from this device are precursory ingredients for many industries that produce valuable products in our everyday lives </span><span>–</span><span> from plastic polymers to fuel for larger vehicles that can’t yet run off a battery, like an airplane. Building upon this fundamental research milestone, Yang is now aimed to increase the system’s efficiency and expand the size of the artificial leaf to begin increasing the scalability of the solution.&nbsp;</span></p>
  </lbl-rich-text>
</lbl-container>



<lbl-container theme="white">
  <lbl-image img-caption="Lin uses an artificial light to activate the postage stamp-sized device to convert carbon dioxide into a C2, a valuable precursory chemical in everyday products." img-credit="(Credit: Marilyn Sargent/Berkeley Lab)">
    <img loading="lazy" decoding="async" width="1190" height="795" src="https://newscenter.lbl.gov/wp-content/uploads/2025/04/Newscenter_1190px_XBD-202503-035-017.jpg" alt="" slot="media">  </lbl-image>
</lbl-container>



<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <p><span>The </span><a href="https://foundry.lbl.gov/" target="_blank" rel="noopener"><span>Molecular Foundry</span></a><span> is a user facility at Berkeley Lab.&nbsp;</span></p>
<p><span>This work was supported by the </span><a href="https://www.energy.gov/science/office-science" target="_blank" rel="noopener"><span>DOE Office of Science</span></a><span>.</span></p>
<p>###</p>
<p><a href="https://www.lbl.gov/" target="_blank" rel="noopener"><span>Lawrence Berkeley National Laboratory</span></a><span> (Berkeley Lab) is committed to groundbreaking research focused on discovery science and solutions for abundant and reliable energy supplies. The lab’s expertise spans materials, chemistry, physics, biology, earth and environmental science, mathematics, and computing. Researchers from around the world rely on the lab’s world-class scientific facilities for their own pioneering research. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science.&nbsp;</span></p>
<p><span>DOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit <a href="http://energy.gov/science" target="_blank" rel="noopener">energy.gov/science</a>.</span></p>
  </lbl-rich-text>
</lbl-container>

    </div><!-- .entry-content -->

    

<!-- content-single-bottom -->

      <lbl-container no-container-padding-top="" theme="white">
      <lbl-divider></lbl-divider>
    </lbl-container>
  
  <lbl-container wrapper-size="sm" theme="white">

    <!-- /. post-tags -->

  </lbl-container>


  <lbl-container theme="cloud">
    <lbl-section-header layout="centered" margin-bottom="" text="You might also be interested in:">
    </lbl-section-header>
    <lbl-grid columns="3" layout="grid-card">

      <lbl-grid-card link-target="_self" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=55099" title="Five Ways LiSA is Advancing Solar Fuels">
      <lbl-image slot="media">
      <img width="890" height="665" src="https://newscenter.lbl.gov/wp-content/uploads/2024/08/Newscenter_ALTLanding_890x665px_XBD-202406-103-032.jpg" alt="Two researchers in lab coats and goggles work with outdoor scientific equipment near a modern building." slot="media" decoding="async" loading="lazy">    </lbl-image>
    <lbl-tags icon="article" slot="byline"><ul><li><a href="https://newscenter.lbl.gov/all-news/?type=article" aria-label="View all articles of type: Article">Article</a></li><li><a href="https://newscenter.lbl.gov/all-news/?topic=152620" aria-label="View all articles of type: Alternative Energy">Alternative Energy</a></li></ul></lbl-tags>  <lbl-button slot="btn" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=55099" text="Read the article" link-target="_self" type="inline">
  </lbl-button>
</lbl-grid-card>
<lbl-grid-card link-target="_self" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=47329" title="Berkeley Lab Part of Multi-Institutional Team Awarded $60M for Solar Fuels Research">
      <lbl-image slot="media">
      <img width="890" height="623" src="https://newscenter.lbl.gov/wp-content/uploads/2020/07/JCAP_FundingRenewal-v3-1200px.png" alt="LiSA JCAP renewal solar fuels hub" slot="media" decoding="async" loading="lazy">    </lbl-image>
    <lbl-tags icon="" slot="byline"><ul><li><a href="https://newscenter.lbl.gov/all-news/?type=" aria-label="View all articles of type: "></a></li></ul></lbl-tags>  <lbl-button slot="btn" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=47329" text="Read the article" link-target="_self" type="inline">
  </lbl-button>
</lbl-grid-card>
<lbl-grid-card link-target="_self" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=52821" title="How a Record-Breaking Copper Catalyst Converts CO2 Into Liquid Fuels">
      <lbl-image slot="media">
      <img width="890" height="611" src="https://newscenter.lbl.gov/wp-content/uploads/2023/02/copper-nanoparticle-homepage-1720x1180-1.jpg" alt="Artist’s rendering of a copper nanoparticle life cycle during CO2 electrolysis: Copper nanoparticles (left) combine into larger metallic copper “nanograins” (right) within seconds of the electrochemical reaction, reducing CO2 into new multicarbon products." slot="media" decoding="async" loading="lazy">    </lbl-image>
    <lbl-tags icon="article" slot="byline"><ul><li><a href="https://newscenter.lbl.gov/all-news/?type=article" aria-label="View all articles of type: Article">Article</a></li><li><a href="https://newscenter.lbl.gov/all-news/?topic=152612" aria-label="View all articles of type: Carbon Management">Carbon Management</a></li></ul></lbl-tags>  <lbl-button slot="btn" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=52821" text="Read the article" link-target="_self" type="inline">
  </lbl-button>
</lbl-grid-card>

    </lbl-grid>
  </lbl-container>

  </div><!-- .post-content-->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[National Airspace System Status (168 pts)]]></title>
            <link>https://nasstatus.faa.gov/</link>
            <guid>43787730</guid>
            <pubDate>Thu, 24 Apr 2025 21:31:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nasstatus.faa.gov/">https://nasstatus.faa.gov/</a>, See on <a href="https://news.ycombinator.com/item?id=43787730">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[You Can Be a Great Designer and Be Completely Unknown (178 pts)]]></title>
            <link>https://www.chrbutler.com/you-can-be-a-great-designer-and-be-completely-unknown</link>
            <guid>43787676</guid>
            <pubDate>Thu, 24 Apr 2025 21:24:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chrbutler.com/you-can-be-a-great-designer-and-be-completely-unknown">https://www.chrbutler.com/you-can-be-a-great-designer-and-be-completely-unknown</a>, See on <a href="https://news.ycombinator.com/item?id=43787676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

<p>
I often find myself contemplating the greatest creators in history — those rare artists, designers, and thinkers whose work transformed how we see the world. What constellation of circumstances made them who they were? Where did their ideas originate? Who mentored them? Would history remember them had they lived in a different time or place?
</p>
<p>
Leonardo da Vinci stands as perhaps the most singular creative mind in recorded history — the quintessential “Renaissance Man” whose breadth of curiosity and depth of insight seem almost superhuman. Yet examples like Leonardo can create a misleading impression that true greatness emerges only once in a generation or century. Leonardo lived among roughly 10-13 million Italians — was greatness truly as rare as one in ten million? We know several of his contemporaries, but still, the ratio remains vanishingly small. This presents us with two possibilities: either exceptional creative ability is almost impossibly rare, or greatness is more common than we realize and the rarity is recognition.
</p>
<p>
I believe firmly in the latter. Especially today, when we live in an attention economy that equates visibility with value. Social media follower counts, speaking engagements, press mentions, and industry awards have become the measuring sticks of design success. This creates a distorted picture of what greatness in design actually means. The truth is far simpler and more liberating: you can be a great designer and be completely unknown.
</p>
<p>
The most elegant designs often fade into the background, becoming invisible through their perfect functionality. Day to day life is scattered with the artifacts of unrecognized ingenuity — the comfortable grip of a vegetable peeler, the intuitive layout of a highway sign, or the satisfying click of a well-made light switch. These artifacts represent design excellence precisely because they don’t call attention to themselves or their creators. Who is responsible for them? I don’t know. That doesn’t mean they’re not out there.
</p>
<p>
This invisibility extends beyond physical objects. The information architect who structures a medical records system that saves lives through its clarity and efficiency may never receive public recognition. The interaction designer who simplifies a complex government form, making essential services accessible to vulnerable populations, might never be celebrated on design blogs or win prestigious awards.
</p>
<p>
Great design isn’t defined by who knows your name, but by how well your work serves human needs. It’s measured in the problems solved, the frustrations eased, the moments of delight created, and the dignity preserved through thoughtful solutions. These metrics operate independently of fame or recognition.
</p>
<p>
Our obsession with visibility also creates a troubling dynamic: design that prioritizes being noticed over being useful. This leads to visual pollution, cognitive overload, and solutions that serve the designer’s portfolio more than the user’s needs. When recognition becomes the goal, the work itself often suffers. I was among the few who didn’t immediately recoil at the brash aesthetics of the Tesla Cybertruck, but it turns out that no amount of exterior innovation changes the fact that it is just not a good truck.
</p>
<p>
There’s something particularly authentic about unknown masters — those who pursue excellence for its own sake, refining their craft out of personal commitment rather than in pursuit of accolades. They understand that their greatest achievements might never be attributed to them, and they create anyway. Their satisfaction comes from the integrity of the work itself.
</p>
<p>
This isn’t to dismiss the value of recognition when it’s deserved, or to suggest that great designers shouldn’t be celebrated. Rather, it’s a reminder that the correlation between quality and fame is weak at best, and that we should be suspicious of any definition of design excellence that depends on visibility. This is especially so today. The products of digital and interaction design are mayflies; most of what we make is lost to the rapid churn of the industry before it can even be lost to anyone’s memory.
</p>
<p>
The next time you use something that works so well you barely notice it, remember that somewhere, a designer solved a problem so thoroughly that both the problem and its solution became invisible. That designer might not be famous, might not have thousands of followers, might not be invited to speak at conferences — but they’ve achieved something remarkable: greatness through invisibility.
</p>
<p>
Design greatness is not measured by the recognition of authorship, but in the creation of work so essential it becomes as inevitable as gravity, as unremarkable as air, and as vital as both.
</p>
        
        
        <hr>
        <p><span color="grey"><small>Written by Christopher Butler on</small></span></p><p>April 24, 2025</p> &nbsp;
        
        
        <p><span color="grey"><small>Tagged</small></span></p><a href="https://www.chrbutler.com/tagged/essays"><p>Essays</p></a>
        <hr>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[People say they’ll pay more for “made in the USA” so we ran a test (132 pts)]]></title>
            <link>https://afina.com/blogs/news/made-in-usa</link>
            <guid>43787647</guid>
            <pubDate>Thu, 24 Apr 2025 21:21:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://afina.com/blogs/news/made-in-usa">https://afina.com/blogs/news/made-in-usa</a>, See on <a href="https://news.ycombinator.com/item?id=43787647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="MainContent" role="main" tabindex="-1">
      <section id="shopify-section-template--16168843804849__main">

<article itemscope="" itemtype="http://schema.org/BlogPosting"><header>
            
          
        </header>



  <div itemprop="articleBody">
    <p><em>When we priced a U.S.-made version of our flagship product 85% higher than our Chinese-made one, 25,650 customers had the chance to vote with their wallets. Here’s what happened.</em><br></p>
<p>As small business owners, we’ve heard it a thousand times:</p>
<p><em>“I’d gladly pay more to support American-made.”</em></p>
<p><strong>We wanted to believe it. So we put it to the test.</strong></p>
<div><meta charset="utf-8"><meta charset="utf-8"><p>We make filtered showerheads. Clean, sleek design. But more importantly, with the best shower filters on the market.&nbsp;</p><p>Our bestselling model—manufactured in Asia (China and Vietnam)—sells for $129. But this year, as tariffs jumped from 25% to 170%, we wondered: Could we reshore manufacturing to the U.S. while maintaining margins to keep our lights on?</p><p><em>An important part to mention is that our most filter materials (KDF-55) is sourced from the US. So technically we partly source from Asia.&nbsp;</em></p></div>
<p>We found a U.S.-based supplier. The new unit cost us nearly 3x more to produce. To maintain our margins, we’d have to sell it for $239.</p>
<p><strong>So we ran an experiment.</strong></p>
<p>We created a secret landing page. The product and design were identical. The only difference? One was labeled “Made in Asia” and priced at $129. The other, “Made in the USA,” at $239.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/2025-04-23_15-28-10.jpg?v=1745440146"></p>
<p>The visitors were given the choice to either buy the Made in USA or the Made in Asia version.&nbsp;</p>
<p><strong>The results were sobering.</strong></p>
<p>Add-to-carts for the U.S. version were only 24! <strong>Conversion? 0.0% (zero).</strong> <br>Not a single customer purchased the Made-in-USA version.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/output_2.png?v=1745517906"></p>
<p>We tested everything: color, copy, layout. We ran it over multiple days and traffic sources. Same outcome every time.</p>
<p>For a moment, we thought we’d made a technical error. We hadn’t.</p>
<p>This wasn’t a failure of marketing—it was a referendum on price.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/2025-04-23_15-33-08.jpg?v=1745440456" width="467" height="461"></p>
<p>We wanted to believe customers would back American labor with their dollars. But when faced with a real decision—not a survey or a comment section—they didn’t.</p>
<p>It’s not their fault. Most people are stretched. They’re feeling inflation everywhere: gas, groceries, mortgages. “Supporting U.S. manufacturing” becomes a luxury most can’t afford—even if they want to.</p>
<p>This isn’t just our problem—it’s the economy’s.</p>
<p>Small brands like ours want to manufacture here. We’re willing to invest. But without serious shifts—in consumer incentives, automation, and trade policy—the math doesn’t work. Not for us. Not for our customers.</p>
<p>We’re still committed to exploring local manufacturing. But for now, it’s not viable.</p>
<p>We’re sharing this because the numbers surprised even us. And we think they’re worth talking about.</p>
<p>If policymakers and pundits want to rebuild American industry, they need to grapple with this truth: idealism doesn’t always survive contact with a price tag.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/output_1.png?v=1745517834"></p>
<p><strong>Ramon van Meer</strong><br>Founder - Afina<br>ramon@afina.com&nbsp;<br>925-548-7758</p>


    

    
  </div>
</article>






</section><div id="shopify-section-template--16168843804849__c187b5fa-a3c0-429b-981a-ff8885432f90">
      <div>
        
        <p>Meet the Afina Filtered<br>Showerhead</p>
        
        
      </div>
      <div>
        
        <p><img src="https://afina.com/cdn/shop/files/Afina-0066-7.png?v=1701271069">
        </p>
        
      </div>
      <div>
        <h2 data-cascade="">
          <p>Filters out 98% of harmful substances in your water</p>
        </h2>
        <p data-cascade=""><a href="https://afina.com/products/a-01-filtered-shower-head">SHOP NOW<svg width="11" height="18" viewBox="0 0 11 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.44251 0.820312L9.33984 8.71764L1.44251 16.615" stroke="black" stroke-width="1.31622"></path></svg>
            </a></p>
      </div>
    </div><div id="shopify-section-template--16168843804849__8f50fba5-3dff-4371-b88e-4032bdc7a190"><p>
        <h2 id="SectionHeading-template--16168843804849__8f50fba5-3dff-4371-b88e-4032bdc7a190" data-cascade="">
          Recent Article
        </h2></p><slider-component>
      <ul id="Slider-template--16168843804849__8f50fba5-3dff-4371-b88e-4032bdc7a190" role="list">
          
                    
              
</ul>
      
        
      
    </slider-component></div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI releases image generation in the API (402 pts)]]></title>
            <link>https://openai.com/index/image-generation-api/</link>
            <guid>43786506</guid>
            <pubDate>Thu, 24 Apr 2025 19:27:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/image-generation-api/">https://openai.com/index/image-generation-api/</a>, See on <a href="https://news.ycombinator.com/item?id=43786506">Hacker News</a></p>
Couldn't get https://openai.com/index/image-generation-api/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[NSF director to resign amid grant terminations, job cuts, and controversy (272 pts)]]></title>
            <link>https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy</link>
            <guid>43786304</guid>
            <pubDate>Thu, 24 Apr 2025 19:07:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy">https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy</a>, See on <a href="https://news.ycombinator.com/item?id=43786304">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I reverse engineered top websites to build an animated UI library (134 pts)]]></title>
            <link>https://reverseui.com</link>
            <guid>43785464</guid>
            <pubDate>Thu, 24 Apr 2025 17:47:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reverseui.com">https://reverseui.com</a>, See on <a href="https://news.ycombinator.com/item?id=43785464">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><div><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="M8.00065 14.1667C10.8541 14.1667 13.1673 11.6667 13.1673 8.66667C13.1673 5.222 10.0918 2.78077 8.85307 1.9372C8.56929 1.74395 8.19435 1.82757 7.99678 2.10835L6.38863 4.39386C6.15096 4.73163 5.66501 4.77276 5.37203 4.48167C5.11194 4.22325 4.68797 4.22046 4.45454 4.5032C3.37417 5.81171 2.83398 7.44093 2.83398 8.66667C2.83398 11.6667 5.14718 14.1667 8.00065 14.1667ZM8.00065 14.1667C9.10522 14.1667 10.0007 13.1446 10.0007 11.8838C10.0007 10.495 8.89191 9.48319 8.32548 9.05691C8.13084 8.91044 7.87046 8.91044 7.67583 9.05691C7.1094 9.48319 6.00065 10.495 6.00065 11.8838C6.00065 13.1446 6.89608 14.1667 8.00065 14.1667Z" stroke="#FCFBFA" stroke-linejoin="round" stroke-width="1.5"></path></svg> <!-- --><p>28</p><!-- --><p> components available</p></div><div><h2>Reverse engineered UI library from the best sites on the web</h2><p>Effortlessly integrate trending animated components to your projects, with all the styling and animations handled for you. <br></p><div><p><a href="#components-section">Browse components</a></p><div><a href="https://reverseui.com/pricing">Get lifetime access ($50.00)<svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></a><div><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="m256 101 38.8-62.03C309.9 14.73 336.5 0 365.1 0h2.9c44.2 0 80 35.82 80 80 0 18.01-6 34.6-16 48h32c26.5 0 48 21.5 48 48v64c0 20.9-13.4 38.7-32 45.3V448c0 35.3-28.7 64-64 64H96c-35.35 0-64-28.7-64-64V285.3C13.36 278.7 0 260.9 0 240v-64c0-26.5 21.49-48 48-48h31.99C69.95 114.6 64 98.01 64 80c0-44.18 35.82-80 80-80h2.9c28.6 0 55.2 14.73 70.3 38.97L256 101zm109.1-69c-17.6 0-33.9 9.04-43.2 23.93l-45 72.07H368c26.5 0 48-21.5 48-48 0-26.51-21.5-48-48-48h-2.9zm-130 96-45-72.07A50.886 50.886 0 0 0 146.9 32H144c-26.5 0-48 21.49-48 48 0 26.5 21.5 48 48 48h91.1zM48 160c-8.84 0-16 7.2-16 16v64c0 8.8 7.16 16 16 16h192v-96H48zm224 96h192c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16H272v96zm-32 32H64v160c0 17.7 14.33 32 32 32h144V288zm32 192h144c17.7 0 32-14.3 32-32V288H272v192z"></path></svg><p>$50 off</p> <!-- --><p>for limited time</p></div></div></div></div><div id="components-section"><div><div><a href="https://reverseui.com/craft/logo-dots-shader"><div><div><p>Logo Dots Logo</p><p>April 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/bot-protection"><div><div><p>Bot Protection</p><p>March 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/glowing-orb"><div><div><p>Glowing Orb</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/sms-alert"><div><div><p>SMS Alert</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/interactive-envelope"><div><div><p>Interactive Envelope</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/social-links"><div><div><p>Social Links</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/rainbow-shine-button"><div><div><p>Rainbow Shine Button</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/track-invoices"><div><div><p>Track Invoices</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/orbit-rings"><div><div><p>Orbit Rings</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/dots-shader"><div><div><p>Dots Shader</p><p>May 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div><div><div><a href="https://reverseui.com/craft/role-based-access-control"><div><div><p>Role-Based Access Control</p><p>April 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/multifactor-authentication"><div><div><p>Multifactor Authentication</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/ai-chat"><div><div><p>AI Chat</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/fingerprint-scan"><div><div><p>Fingerprint Scan</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/logs-explorer"><div><div><p>Logs Explorer</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/text-blur-reveal"><div><div><p>Text Blur Reveal</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/hex-outline"><div><div><p>Hex Outline</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/operating-systems"><div><div><p>Operating Systems</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/stock-chart"><div><div><p>Stock Chart</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div><div><div><a href="https://reverseui.com/craft/like-button"><div><div><p>Like Button</p><p>April 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/macbook-keyboard"><div><div><p>Macbook Keyboard</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/retro-terminal"><div><div><p>Retro Terminal</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/data-feeding-in"><div><div><p>Data Feeding In</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/caesar-cipher"><div><div><p>Caesar Cipher</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/signature"><div><div><p>Signature</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/cool-badge"><div><div><p>Cool Badge</p><p>September 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/command-k"><div><div><p>Command K</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/shiny-button"><div><div><p>Shiny Button</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Lemon Slice Live – Have a video call with a transformer model (151 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43785044</link>
            <guid>43785044</guid>
            <pubDate>Thu, 24 Apr 2025 17:10:14 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43785044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN, this is Lina, Andrew, and Sidney from Lemon Slice. We’ve trained a custom diffusion transformer (DiT) model that achieves video streaming at 25fps and wrapped it into a demo that allows anyone to turn a photo into a real-time, talking avatar. Here’s an example conversation from co-founder Andrew: <a href="https://www.youtube.com/watch?v=CeYp5xQMFZY" rel="nofollow">https://www.youtube.com/watch?v=CeYp5xQMFZY</a>. Try it for yourself at: <a href="https://lemonslice.com/live">https://lemonslice.com/live</a>.</p><p>(Btw, we used to be called Infinity AI and did a Show HN under that name last year: <a href="https://news.ycombinator.com/item?id=41467704">https://news.ycombinator.com/item?id=41467704</a>.)</p><p>Unlike existing avatar video chat platforms like HeyGen, Tolan, or Apple Memoji filters, we do not require training custom models, rigging a character ahead of time, or having a human drive the avatar. Our tech allows users to create and immediately video-call a custom character by uploading a single image. The character image can be any style - from photorealistic to cartoons, paintings, and more.</p><p>To achieve this demo, we had to do the following (among other things! but these were the hardest):</p><p>1. Training a fast DiT model. To make our video generation fast, we had to both design a model that made the right trade-offs between speed and quality, and use standard distillation approaches. We first trained a custom video diffusion transformer (DiT) from scratch that achieves excellent lip and facial expression sync to audio. To further optimize the model for speed, we applied teacher-student distillation. The distilled model achieves 25fps video generation at 256-px resolution. Purpose-built transformer ASICs will eventually allow us to stream our video model at 4k resolution.</p><p>2. Solving the infinite video problem. Most video DiT models (Sora, Runway, Kling) generate 5-second chunks. They can iteratively extend it by another 5sec by feeding the end of the 1st chunk into the start of the 2nd in an autoregressive manner. Unfortunately the models experience quality degradation after multiple extensions due to accumulation of generation errors. We developed a temporal consistency preservation technique that maintains visual coherence across long sequences. Our technique significantly reduces artifact accumulation and allows us to generate indefinitely-long videos.</p><p>3. A complex streaming architecture with minimal latency. Enabling an end-to-end avatar zoom call requires several building blocks, including voice transcription, LLM inference, and text-to-speech generation in addition to video generation. We use Deepgram as our AI voice partner. Modal as the end-to-end compute platform. And Daily.co and Pipecat to help build a parallel processing pipeline that orchestrates everything via continuously streaming chunks. Our system achieves end-to-end latency of 3-6 seconds from user input to avatar response. Our target is &lt;2 second latency.</p><p>More technical details here: <a href="https://lemonslice.com/live/technical-report">https://lemonslice.com/live/technical-report</a>.</p><p>Current limitations that we want to solve include: (1) enabling whole-body and background motions (we’re training a next-gen model for this), (2) reducing delays and improving resolution (purpose-built ASICs will help), (3) training a model on dyadic conversations so that avatars learn to listen naturally, and (4) allowing the character to “see you” and respond to what they see to create a more natural and engaging conversation.</p><p>We believe that generative video will usher in a new media type centered around interactivity: TV shows, movies, ads, and online courses will stop and talk to us. Our entertainment will be a mixture of passive and active experiences depending on what we’re in the mood for. Well, prediction is hard, especially about the future, but that’s how we see it anyway!</p><p>We’d love for you to try out the demo and let us know what you think! Post your characters and/or conversation recordings below.</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenVSX, which VSCode forks rely on for extensions, down for 24 hours (204 pts)]]></title>
            <link>https://status.open-vsx.org/</link>
            <guid>43785039</guid>
            <pubDate>Thu, 24 Apr 2025 17:09:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.open-vsx.org/">https://status.open-vsx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43785039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
<span>Updated</span>
<span>Apr 23 at 11:31am EDT</span>
</p>
<p>We are working to resolve an issue with our backend storage that is preventing the service from starting correctly</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manufactured Consensus on X.com (295 pts)]]></title>
            <link>https://rook2root.co/articles/20250424-manufacturing-consensus-on-x</link>
            <guid>43784915</guid>
            <pubDate>Thu, 24 Apr 2025 16:57:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rook2root.co/articles/20250424-manufacturing-consensus-on-x">https://rook2root.co/articles/20250424-manufacturing-consensus-on-x</a>, See on <a href="https://news.ycombinator.com/item?id=43784915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Influential users and recommendation algorithm design quietly shape what people see, what gains attention, and what gets silenced.</p>
<p>When an account with 219 million followers interacts with a smaller one — not by blocking or arguing, but simply by muting — the consequences are immediate. The smaller account’s visibility drops from 150,000 views to 20,000 overnight. No notice. No rule broken. Dimmed into irrelevance.</p>
<p>It’s a form of shadowbanning — not imposed by moderators, but activated by the algorithm in response to a high-weight engagement signal.</p>

<p>On the flip side, the same signal that suppresses can also elevate. When a high-reach account interacts — sometimes with nothing more than a vague comment or a repost — the algorithm reads it as endorsement. Content is boosted, visibility spikes, and narratives take flight.</p>
<p>Even low-effort, repetitive interactions—likes, generic replies—can act like a controlled dose of AstroBoost™ - just enough to simulate momentum and trigger amplification.</p>


<p><a target="_blank" rel="noopener noreferrer" href="https://rook2root.co/library/user-influence-and-retention-engineering/perception-management-and-influence/social-proof-manipulation">Social proof</a> used to reflect crowd wisdom. Now it reflects algorithmic endorsement — triggered not by consensus, but by proximity to influence. A single interaction can distort scale, making selected content appear widely supported.</p>
<p>The result? Artificial popularity. Boosted narratives. Organic ideas buried by engineered reach. The crowd didn’t pick it—the algorithm did, based on who touched it.</p>
<p><em>It’s not fraud. It’s influence infrastructure.</em></p>
<h2 id="perception-cascades">Perception Cascades</h2>
<p>Nothing needs to be removed or blocked. Often, content is simply deprioritized—pushed lower in the feed, placed outside key visibility zones, or displaced by fresher signals. The mechanisms are subtle, the outcomes consistent: lower reach, reduced visibility, diminished presence.</p>
<p>Meanwhile, amplification flows downstream. A single high-weight interaction can trigger a cascade—surfacing aligned content, prompting engagement across similar accounts, and reinforcing the same narrative through repetition.</p>
<p>What people see feels organic. In reality, they’re engaging with what’s already been filtered, ranked, and surfaced.</p>
<h2 id="astroturfing-20">Astroturfing 2.0</h2>
<p>There’s no need to simulate support when the platform itself is the amplifier. Traditional <a target="_blank" rel="noopener noreferrer" href="https://rook2root.co/library/user-influence-and-retention-engineering/perception-management-and-influence/astroturfing">astroturfing</a> relied on fake accounts and bots. Today, manufactured consensus is powered by real users—but selected ones. Elite accounts trigger the machine. Everyone else gets pulled into the ripple.</p>
<p>This isn’t about faking the crowd — it’s about guiding it. Real users, real engagement, selectively amplified to create the illusion of widespread agreement. Consensus is just what survived the feed.</p>
<h2 id="seen-and-unseen">Seen and Unseen</h2>
<p>Perception shaped at scale doesn’t just change what people see—it changes how they vote, what they buy, what they protest, and what they ignore. It doesn’t just distort attention. It steers outcomes.</p>
<p>Truth isn’t what’s real — it’s what’s shown.</p>
<p>What’s not shown might as well not exist.</p>
<p>And if you think this only happens on one social network, you’re already caught in the wrong attention loop.</p>
<h2 id="post-scriptum-the-loud-ones-always-fall-first">Post Scriptum: The Loud Ones Always Fall First</h2>
<p>The most effective influence doesn’t announce itself. It doesn’t censor loudly, or boost aggressively. It shapes perception quietly — one algorithmic nudge at a time.</p>
<p>The ones who try to control everything too openly, too quickly, get <a target="_blank" rel="noopener noreferrer" href="https://digital-strategy.ec.europa.eu/en/news/commission-addresses-additional-investigatory-measures-x-ongoing-proceedings-under-digital-services/">caught</a>.
It’s not the blunt force authoritarians who endure. It’s the subtle ones. The ones who let people believe they chose freely — while feeding them only curated choices.</p>
<hr>
<p><em>At rook2root.co we expose the tactics no one talks about. Not to preach, but to illuminate. <a target="_blank" rel="noopener noreferrer" href="https://rook2root.beehiiv.com/subscribe">subscribe</a> to get new articles delivered by mail.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One quantum transition makes light at 21 cm (226 pts)]]></title>
            <link>https://bigthink.com/starts-with-a-bang/21cm-magic-length/</link>
            <guid>43784721</guid>
            <pubDate>Thu, 24 Apr 2025 16:38:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bigthink.com/starts-with-a-bang/21cm-magic-length/">https://bigthink.com/starts-with-a-bang/21cm-magic-length/</a>, See on <a href="https://news.ycombinator.com/item?id=43784721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div x-data="prose">

                    
<div>
                            <p>
                    Sign up for the Starts With a Bang newsletter              </p>
                                            <p>
                    Travel the universe with Dr. Ethan Siegel as he answers the biggest questions of all.         </p>
                        </div>
<!--?xml encoding="utf-8" ?--><p>In our Universe, quantum transitions are the governing rule behind every nuclear, atomic, and molecular phenomenon. Unlike the planets in our Solar System, which could stably orbit the Sun at any distance if they possessed the right speed, the protons, neutrons, and electrons that make up all the conventional matter we know of can only bind together in a specific set of configurations. These possibilities, although numerous, are finite in number, as the quantum rules that govern electromagnetism and the nuclear forces restrict how atomic nuclei and the electrons that orbit them can arrange themselves.</p><p>In all the Universe, the most common atom of all is hydrogen, with just one proton and one electron. Wherever new stars form, hydrogen atoms become ionized, becoming neutral again if those free electrons can find their way back to a free proton. Although the electrons will typically cascade down the allowed energy levels into the ground state, that normally produces only a specific set of infrared, visible, and ultraviolet light. But more importantly, a special transition occurs in hydrogen that produces light of about the size of your hand: 21 centimeters (about 8¼”) in wavelength. Even as a physicist, you’d be well justified to call this the “magic length” of our Universe, as it just might someday unlock the darkest secrets hiding out in the deepest cosmic recesses from which starlight will never escape.</p><!--?xml encoding="utf-8" ?--><figure><img fetchpriority="high" decoding="async" width="638" height="479" src="https://bigthink.com/wp-content/uploads/2022/12/cosmology-with-the-21cm-line-3-638.jpg" alt="" sizes="(max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/cosmology-with-the-21cm-line-3-638.jpg 638w, https://bigthink.com/wp-content/uploads/2022/12/cosmology-with-the-21cm-line-3-638.jpg?resize=375,282 375w"><div><div><p>Backlit by the cosmic microwave background, a cloud of neutral gas can imprint a signal on that radiation at a specific wavelength and redshift. If we can measure this light with great enough sensitivity, we can actually hope to someday map out the locations and densities of gas clouds in the Universe thanks to the science of 21 cm astronomy. A dip in brightness temperature at redshifts of 15-20, observed in 2018, may be due to exactly the effect of 21-cm emission, although better instrumentation and better observational examples will be required to confirm such a claimed detection.
</p></div><figcaption><a href="https://www.slideshare.net/CosmoAIMS/cosmology-with-the-21cm-line" target="_blank">Credit</a>: Gianni Bernardi, via his AIMS talk
</figcaption></div></figure><p>When it comes to the light in the Universe, wavelength is the one property that you can count on to reveal how that light was created. Even though light comes to us in the form of photons — individual quanta that, collectively, make up the phenomenon we know as light — there are two very different classes of quantum process that create the light that surrounds us: continuous ones and discrete ones.</p><p>A continuous process is something like the light emitted by the photosphere of the Sun. It’s a dark object that’s been heated up to a certain temperature, and it radiates light of all different, continuous wavelengths as dictated by that temperature: what physicists know as blackbody radiation. More accurately, because the different layers of the photosphere are at different temperatures, the solar spectrum acts like a series of blackbodies all summed together: an amalgam of continuous processes.</p><p>A discrete process, however, doesn’t allow for the emission of light of a continuous set of wavelengths, but rather only at extremely specific, or discrete (and quantized), wavelengths. A good example of that is the light absorbed by the neutral atoms present within the extreme outer layers of the Sun. As the blackbody radiation from the lower layers of the photosphere strikes those neutral atoms sitting at the surface, a few of those photons will have just the right wavelengths to be absorbed by the electrons within the neutral atoms they encounter. When we break sunlight up into its individual wavelengths, the various absorption lines present against the backdrop of continuous, blackbody radiation reveal both of these processes to us.</p><!--?xml encoding="utf-8" ?--><figure><img decoding="async" width="8192" height="5464" src="https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?w=8192" alt="" sizes="(max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg 8192w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=1536,1025 1536w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=2048,1366 2048w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=20,12 20w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=375,250 375w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=640,427 640w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=768,512 768w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=1024,683 1024w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=1280,854 1280w"><div><div><p>The visible light spectrum of the Sun, which helps us understand not only its temperature and ionization, but the abundances of the elements present. The long, thick lines are hydrogen and helium, but every other line is from a heavy element that must have been created in a previous-generation star, rather than the hot Big Bang.
</p></div><figcaption><a href="https://solarsystem.nasa.gov/resources/390/the-solar-spectrum/" target="_blank">Credit</a>: N.A.Sharp, NOAO/NSO/Kitt Peak FTS/AURA/NSF
</figcaption></div></figure><p>Each individual atom has its properties primarily defined by its nucleus, made up of protons (which determine its charge) and neutrons (which, combined with protons, determine its mass). Atoms also have electrons, which orbit the nucleus at a distance determined by their charge-to-mass ratio, and each electron can only occupy a specific set of energy levels. In isolation, each atom will come to exist in the ground state: where the electrons cascade down until they occupy the lowest allowable energy levels, limited only by the quantum rules that determine the various properties that electrons are and aren’t allowed to possess.</p><p>Electrons can occupy the ground state — the 1s orbital — of an atom until it’s full, which can hold two electrons. The next energy level up consists of spherical (the 2s) and perpendicular (the 2p) orbitals, which can hold two and six electrons, respectively, for a total of eight. The third energy level can hold 18 electrons: 3s (with two), 3p (with six), and 3d (with ten), and the pattern continues on upward. In general, the “upward” transitions occur when a photon of a particular wavelength gets absorbed, while the “downward” transitions can occur spontaneously, and result in the emission of photons of the exact same wavelengths as are present within the atom’s absorption spectrum.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="960" height="714" src="https://bigthink.com/wp-content/uploads/2021/10/e.jpg?w=960" alt="atom" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2021/10/e.jpg 960w, https://bigthink.com/wp-content/uploads/2021/10/e.jpg?resize=375,279 375w, https://bigthink.com/wp-content/uploads/2021/10/e.jpg?resize=640,476 640w, https://bigthink.com/wp-content/uploads/2021/10/e.jpg?resize=768,571 768w"><div><div><p>Electron transitions in the hydrogen atom, along with the wavelengths of the resultant photons, showcase the effect of binding energy and the relationship between the electron and the proton in quantum physics. The Bohr model of the atom provides the coarse (or rough, or gross) structure of these energy levels. Hydrogen’s brightest atomic transition is Lyman-alpha (n=2 to n=1), but its second brightest is visible: Balmer-alpha (n=3 to n=2), which emits visible (red) light at a wavelength of 656 nanometers. The energy lost by an electron cascading down the energy levels gets emitted in the form of photons.
</p></div><figcaption><a href="https://commons.wikimedia.org/wiki/File:Hydrogen_transitions.svg" target="_blank">Credit</a>: OrangeDog and Szdori/Wikimedia Commons
</figcaption></div></figure><p>That’s the basic structure of an atom, sometimes referred to as “coarse structure.” When you transition from the third energy level to the second energy level in a hydrogen atom, for example, you produce a photon that’s red in color, with a wavelength of precisely 656.3 nanometers: right in the visible light range of human eyes.</p><p>But there are very, very slight differences between the exact, precise wavelength of a photon that gets emitted if you transition from:</p><ul>
<li>the third energy level down to either the 2s or the 2p orbital,</li>



<li>an energy level where the spin angular momentum and the orbital angular momentum are aligned versus one where they’re anti-aligned,</li>



<li>or one where the nuclear spin and the electron spin are aligned versus anti-aligned.</li>
</ul><p>There are rules as to what’s allowed versus what’s forbidden in quantum mechanics as well, such as the fact that you can transition an electron from a d-orbital to either an s-orbital or a p-orbital, and from an s-orbital to a p-orbital, but not from an s-orbital to another s-orbital. </p><p>The slight differences in energy that arise between transitions of different types of orbital within the same energy level is known as an atom’s fine-structure, arising from the interaction between the spin of each particle within an atom and the orbital angular momentum of the electrons around the nucleus. It causes a shift in wavelength of less than 0.1%: small compared to the atom’s course structure, but still measurable and significant.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="960" height="375" src="https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?w=960" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg 960w, https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?resize=375,146 375w, https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?resize=640,250 640w, https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?resize=768,300 768w"><div><div><p>The atomic transition from the 6S orbital in a cesium-133 atom, Delta_f1, is the transition that defines the meter, second, and the speed of light. Slight changes in the observed frequency of this light will occur based on motion and the properties of spatial curvature between any two locations. Spin-orbit interactions, as well as various quantum rules and the application of an external magnetic field, can cause additional splitting at narrow intervals in these energy levels: examples of fine and hyperfine structure.
</p></div><figcaption><a href="https://www.researchgate.net/profile/Juergen-Czarske/publication/255713467_Optical_multi-point_measurements_of_the_acoustic_particle_velocity_with_frequency_modulated_Doppler_global_velocimetry/links/00b7d52693279317cc000000/Optical-multi-point-measurements-of-the-acoustic-particle-velocity-with-frequency-modulated-Doppler-global-velocimetry.pdf" target="_blank">Credit</a>: A. Fischer et al., Journal of the Acoustical Society of America, 2013
</figcaption></div></figure><p>However, owing to the weird phenomena that occur within quantum mechanics, even “forbidden” transitions can sometimes occur. These transitions occur due to the phenomenon of quantum tunneling, where a quantum state can spontaneously transition to another, lower-energy quantum state. Sure, you might not be able to transition from an s-orbital to another s-orbital directly, but if you can:</p><ul>
<li>transition from an s-orbital to a p-orbital and then back to an s-orbital,</li>



<li>transition from an s-orbital to a d-orbital and then back to an s-orbital,</li>



<li>or, more generally, transition from an s-orbital to any other allowable state and then back to an s-orbital,</li>
</ul><p>then that transition can occur. The only thing weird about quantum tunneling is that you don’t have to have a “real” transition occur to the intermediate state. Real transitions require energy, and even with insufficient energies, the intermediate state can be bypassed under the rules of quantum physics. This occurs when transitions happen virtually (as opposed to real transitions), so that you only see the final state emerge from the initial state: something that would be forbidden without the invocation of quantum tunneling.</p><p>This allows us to go beyond mere “coarse structure” and “fine structure,” allowing us to probe what’s known as hyperfine structure. Hyperfine structure appears where the spin of the atomic nucleus and one of the electrons that orbit it begin in an “aligned” state, where the spins are both in the same direction even though the electron is in the lowest-energy, ground (1s) state, and then transitions to an anti-aligned state, where the spins are reversed.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="962" height="990" src="https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg 962w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=20,20 20w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=40,40 40w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=375,386 375w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=640,659 640w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=768,790 768w"><div><div><p>Whenever a neutral hydrogen atom forms, the electron within it will spontaneously de-excite until it’s in the lowest (1s) state of the atom. With a 50/50 chance of having those spins of the electron and proton aligned, half of those atoms will be able to quantum tunnel into the anti-aligned state, emitting radiation of 21 centimeters (1420 MHz) in the process. This should allow us to probe clumps of neutral hydrogen even farther back than the existence of the first stars.
</p></div><figcaption><a href="https://www.skatelescope.org/radio-astronomy/" target="_blank">Credit</a>: SKA Organisation
</figcaption></div></figure><p>The most famous of these transitions occurs in the simplest type of atom of all: hydrogen. With just one proton and one electron, every time you form a neutral hydrogen atom and the electron cascades down to the ground (lowest-energy) state, there’s a 50% chance that the spins of the central proton and the electron will be aligned, with a 50% chance that the spins will be anti-aligned.</p><p>If the spins are anti-aligned, that’s truly the lowest-energy state; there’s nowhere to go via any known transition that will result in the emission of energy at all. But if the spins are aligned, it’s a slightly higher energy state than in the anti-aligned case. A hydrogen atom whose electron and proton both spin in the same direction could quite possibly transition, through quantum tunneling, to the anti-aligned state. Even though the direct transition process is forbidden, tunneling allows you to go straight from the starting point to the ending point, emitting a photon in the process.</p><p>This transition, because of its “forbidden” nature, takes an extremely long time to occur: approximately 10 million years for the average atom. However, this long lifetime of the slightly excited, aligned case for a hydrogen atom has an upside to it: the photon that gets emitted, at 21 centimeters in wavelength and with a frequency of 1420 megahertz, is intrinsically, extremely narrow. In fact, it’s the narrowest, most precise transition line known in all of atomic and nuclear physics! </p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="826" height="537" src="https://bigthink.com/wp-content/uploads/2022/12/universe.png" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/universe.png 826w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=20,12 20w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=375,244 375w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=640,416 640w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=768,499 768w"><div><div><p>This map of the Milky Way, in red, maps out the neutral hydrogen in 21 centimeter emissions. This map is not uniform, but rather tracks recent ionization and atom formation, as the half-life of spin-aligned atoms to flip is only around ~10 million years: a long time in the lab, but a short time compared to the ~13+ billion year history of our galaxy.
</p></div><figcaption>(<a href="https://labplot.kde.org/2020/12/28/the-universe-full-of-hydrogen-and-a-new-feature-in-labplot/" target="_blank">Credit</a>: J.Dickey/NASA SkyView)
</figcaption></div></figure><p>If you were to go all the way back to the early stages of the hot Big Bang, before any stars had formed, you’d discover that a whopping 92% of the atoms in the Universe were exactly this species of hydrogen: with one proton and one electron in them. (At the present time, after all the stars that have formed some 13.8 billion years later, that number is down to “only” about 90% of all atoms.) As soon as neutral atoms stably form — just a few hundred thousand years after the Big Bang — these neutral hydrogen atoms form with a 50/50 chance of having aligned versus anti-aligned spins. The ones that form anti-aligned will remain so; the ones that form with their spins aligned will undergo this spin-flip transition, emitting radiation of 21 centimeters in wavelength.</p><p>Although it’s never yet been done, this gives us a tremendously provocative way to measure the early stages of the Universe as never before. If we could find a cloud of hydrogen-rich gas, even one that’s never formed stars, we could look for this spin-flip signal — accounting for the expansion of the Universe and the corresponding redshift of the light — to measure the atoms in the Universe from the earliest times ever seen. The only “broadening” to the line we’d expect to see would come from thermal and kinetic effects: from the non-zero temperature and the gravitationally-induced motion of the atoms that emit those 21 centimeter signals.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="699" height="300" src="https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg?w=699" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg 699w, https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg?resize=375,161 375w, https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg?resize=640,275 640w"><div><div><p>If particles that emitted radiation were completely at rest and were at a temperature indistinguishable from absolute zero, the width of any emission lines would be determined solely by the speed of the transition. The 21 cm hydrogen line is incredibly, intrinsically narrow, but the kinetic motion of the material in galaxies, as well as the thermal energy because the gas is at a positive, non-zero temperature, both contribute to the observed width of these lines.
</p></div><figcaption>(<a href="https://astronomy.swin.edu.au/cosmos/t/thermal+doppler+broadening" target="_blank">Credit</a>: Swinburne University of Technology)
</figcaption></div></figure><p>In addition to those primordial signals, 21 centimeter radiation arises as a consequence whenever new stars are produced. Every time that a star-forming event occurs, the more massive newborn stars produce large amounts of ultraviolet radiation: radiation that’s energetic enough to ionize hydrogen atoms. All of a sudden, space that was once filled with neutral hydrogen atoms is now filled with free protons and free electrons.</p><p>But those electrons aren’t going to remain ionized forever; if the interstellar environment they’re located in has enough free atomic nuclei (e.g., protons), they’re going to eventually be captured, once again, by those protons. Once the most massive stars have died away, there’s no longer going to be enough ultraviolet radiation to continue to ionize them over and over again, and then those electrons will once again sink down to the ground state, where they’ll have a 50/50 chance of being aligned or anti-aligned with the spin of the atomic nucleus.</p><p>Again, that same radiation — of 21 centimeters in wavelength — gets produced over timescales of ~10 million years. Every time we measure that 21 centimeter wavelength localized in a specific region of space, even if it gets redshifted by the expansion of the Universe, what we’re seeing is evidence of recent star-formation. Wherever star-formation occurs, hydrogen gets ionized, and whenever those atoms become neutral and de-excite again, this specific-wavelength radiation persists for tens of millions of years.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="1120" height="1024" src="https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?w=1120" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png 1120w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=375,343 375w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=640,585 640w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=768,702 768w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=1024,936 1024w"><div><div><p>When a hydrogen atom forms, it has equal probability of having the electron’s and proton’s spins be aligned and anti-aligned. If they’re anti-aligned, no further transitions will occur, but if they’re aligned, they can quantum tunnel into that lower energy state, emitting a photon of a very specific wavelength (21 cm) on very specific, and rather long, timescales. The precision of this transition has been measured to better than 1-part-in-a-trillion, and has not varied over the many decades it’s been known. It is the first light emitted in the Universe after the formation of neutral atoms: even before the formation of the first stars, but also thereafter: whenever new stars are formed, ultraviolet emission ionizes hydrogen atoms, creating this signature once again when those atoms spontaneously re-form.
</p></div><figcaption><a href="https://commons.wikimedia.org/wiki/File:Hydrogen-SpinFlip.svg" target="_blank">Credit</a>: Tiltec/Wikimedia Commons
</figcaption></div></figure><p>If we had the capability of sensitively mapping this 21 centimeter emission in all directions and at all redshifts (i.e., distances) in space, we could literally uncover the star-formation history of the entire Universe, as well as the de-excitation of the hydrogen atoms first formed in the aftermath of the hot Big Bang. With sensitive enough observations, we could answer questions like:</p><ul>
<li>Are there stars present in dark voids in space below the threshold of what we can observe, waiting to be revealed by their de-exciting hydrogen atoms?</li>



<li>In galaxies where no new star-formation is observed, is star-formation truly over, or are there low-levels of new stars being born, just waiting to be discovered from this telltale signature of hydrogen atoms?</li>



<li>Are there any events that heat up and lead to hydrogen ionization prior to the formation of the first stars, and are there star-formation bursts that exist beyond the capabilities of even our most powerful infrared observatories to observe directly?</li>
</ul><p>By measuring light of precisely the needed wavelength — peaking at precisely 21.106114053 centimeters, plus whatever lengthening effects arise from the cosmic expansion of the Universe — we could reveal the answers to all of these questions and more. In fact, this is one of the main science goals of <a href="https://www.astron.nl/telescopes/lofar/" target="_blank" rel="noreferrer noopener">LOFAR</a>: the low-frequency array, and it presents a strong science case for putting an upscaled version of this array on the radio-shielded far side of the Moon.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="2290" height="1582" src="https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?w=2290" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg 2290w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=1536,1061 1536w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=2048,1415 2048w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=375,259 375w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=640,442 640w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=768,531 768w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=1024,707 1024w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=1280,884 1280w"><div><div><p>Constructing either a very large radio dish, perhaps in a lunar crater, or alternatively an array of radio telescopes, on the far side of the Moon, could enable unparalleled radio observations of the Universe, including in the all-important 21 centimeter range, both nearby and across cosmic time. The ability to map out where neutral hydrogen has newly formed within the past ~10-20 million years would advance our understanding of cosmic history like nothing else.
</p></div><figcaption><a href="https://www.nasa.gov/directorates/spacetech/niac/2020_Phase_I_Phase_II/lunar_crater_radio_telescope/" target="_blank">Credit</a>: Saptarshi Bandyopadhyay
</figcaption></div></figure><p>Of course, there’s another possibility that takes us far beyond astronomy when it comes to making use of this important length: creating and measuring enough spin-aligned hydrogen atoms in the lab to detect this spin-flip transition directly, in a controlled fashion. The transition takes about ~10 million years to “flip” on average, which means we’d need around a quadrillion (10<sup>15</sup>) prepared atoms, kept still and cooled to cryogenic temperatures, to measure not only the emission line, but the width of it. If there are phenomena that cause an intrinsic line-broadening, such as <a href="https://www.sciencedirect.com/science/article/pii/S0370269305003412">a primordial gravitational wave signal</a>, such an experiment would, quite remarkably, be able to uncover its existence and magnitude.</p><p>In all the Universe, there are only a few known quantum transitions with the precision inherent to the hyperfine spin-flip transition of hydrogen, which results in the emission of radiation that’s 21 centimeters in wavelength. If we want to identify:</p><ul>
<li>ongoing and recent star-formation across the Universe,</li>



<li>the first atomic signals even before the first stars were formed,</li>



<li>or the relic strength of yet-undetected gravitational waves left over from cosmic inflation,</li>
</ul><p>it becomes clear that the 21 centimeter transition is the most important probe we have in all the cosmos. In many ways, it’s the “magic length” for uncovering some of nature’s greatest secrets, and can take us closer to the Big Bang than observations of any stars or galaxies could ever hope to.</p><p><em>This article was originally published in December of 2022. It was updated in 2025.</em></p>

<div>
                            <p>
                    Sign up for the Starts With a Bang newsletter              </p>
                                            <p>
                    Travel the universe with Dr. Ethan Siegel as he answers the biggest questions of all.         </p>
                        </div>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fundamental flaws of SIMD ISAs (2021) (126 pts)]]></title>
            <link>https://www.bitsnbites.eu/three-fundamental-flaws-of-simd/</link>
            <guid>43783416</guid>
            <pubDate>Thu, 24 Apr 2025 14:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsnbites.eu/three-fundamental-flaws-of-simd/">https://www.bitsnbites.eu/three-fundamental-flaws-of-simd/</a>, See on <a href="https://news.ycombinator.com/item?id=43783416">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-966">
	
	<!-- .entry-header -->

	<div>
		
<p>According to Flynn’s taxonomy <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/SIMD" target="_blank">SIMD</a> refers to a computer architecture that can process multiple data streams with a single instruction (i.e. “Single Instruction stream, Multiple Data streams”). There are different taxonomies, and within those several different sub-categories and architectures that classify as “SIMD”.</p>



<p>In this post, however, I refer to packed SIMD ISA:s, i.e. the type of SIMD <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Instruction_set_architecture" target="_blank">instruction set architecture</a> that is most common in contemporary consumer grade CPU:s. More specifically, I refer to <strong>non-predicated packed SIMD</strong> ISA:s where the details of packed SIMD processing is exposed to the software environment.</p>



<h2>Packed SIMD</h2>



<p>The common trait of packed SIMD architectures is that several data elements are packed into a single register of a fixed width. Here is an example of possible configurations of a packed 128 bits wide SIMD register:</p>



<div><figure><a href="https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register.png"><img width="512" height="131" src="https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register.png" alt="SIMD register" srcset="https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register.png 512w, https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register-300x77.png 300w" sizes="(max-width: 512px) 100vw, 512px"></a></figure></div>



<p>For instance, a 128-bit register can hold sixteen integer bytes or four single precision floating-point values.</p>



<p>This type of SIMD architecture has been wildly popular since the mid 1990s, and some packed SIMD ISA:s are:</p>



<ul><li>x86: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/MMX_(instruction_set)" target="_blank">MMX</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/3DNow!" target="_blank">3DNow!</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions" target="_blank">SSE</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/SSE2" target="_blank">SSE2</a>, …, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions" target="_blank">AVX</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/AVX2" target="_blank">AVX2</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/AVX-512" target="_blank">AVX-512</a><sup>1</sup></li><li>ARM: ARMv6 SIMD, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/NEON_(instruction_set)" target="_blank">NEON</a></li><li>POWER: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/AltiVec" target="_blank">AltiVec</a> (a.k.a. VMX and VelocityEngine)</li><li>MIPS: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/MDMX" target="_blank">MDMX</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/MIPS-3D" target="_blank">MIPS-3D</a>, <a rel="noreferrer noopener" href="https://www.mips.com/products/architectures/ase/simd/" target="_blank">MSA</a>, <a rel="noreferrer noopener" href="https://www.mips.com/products/architectures/ase/dsp/" target="_blank">DSP</a></li><li>SPARC: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Visual_Instruction_Set" target="_blank">VIS</a></li><li>Alpha: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/DEC_Alpha#Motion_Video_Instructions_(MVI)" target="_blank">MVI</a></li></ul>



<p><sup>1</sup> <em>AVX and later x86 SIMD ISA:s (especially AVX-512) incorporate features from <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Vector_processor" target="_blank">vector processing</a>, making them packed SIMD / vector processing hybrids (thus some of the aspects discussed in this article do not fully apply).</em></p>



<p>The promise of all those ISA:s is increased data processing performance, since each instruction executes several operations in parallel. However, there are problems with this model.</p>



<h2>Flaw 1: Fixed register width</h2>



<p>Since the register size is fixed there is no way to scale the ISA to new levels of hardware parallelism without adding new instructions and registers. Case in point: MMX (64 bits) vs SSE (128 bits) vs AVX (256 bits) vs AVX-512 (512 bits).</p>



<p>Adding new registers and instructions has many implications. For instance, the <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Application_binary_interface" target="_blank">ABI</a> must be updated, and support must be added to operating system kernels, compilers and debuggers.</p>



<p>Another problem is that each new SIMD generation requires new instruction opcodes and encodings. In fixed width instruction sets (e.g. ARM) this may prohibit any new extensions, since there may not be enough opcode slots left for adding the new instructions. In variable width instruction sets (e.g. x86) the effect is typically that instructions get longer and longer (effectively hurting code density). Paradoxically each new SIMD generation essentially renders the previous generations redundant (except for supporting binary backwards compatibility), so a large number of instructions are wasted without adding much value.</p>



<p>Finally, any software that wants to use the new instruction set needs to be rewritten (or at least recompiled). What is worse, software developers often have to target several SIMD generations, and add mechanisms to their programs that dynamically select the optimal code paths depending on which SIMD generation is supported.</p>



<h2>Flaw 2: Pipelining</h2>



<p>The packed SIMD paradigm is that there is a 1:1 mapping between the register width and the execution unit width (this is usually required to achieve reasonable performance for instructions that mix inputs from several lanes). At the same time many SIMD operations are pipelined and require several clock cycles to complete (e.g. floating-point arithmetic and memory load instructions). The side effect of this is that the result of one SIMD instruction is not ready to be used until several instructions later in the instruction stream.</p>



<p>Consequently, loops have to be <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Loop_unrolling" target="_blank">unrolled</a> in order to avoid stalls and keep the pipeline busy. This can be done in advanced (power hungry) hardware implementations with register renaming and speculative out-of-order execution, but for simpler (usually more power efficient) hardware implementations loops have to be unrolled in software. Many software developers and compilers aiming to support both in-order and out-of-order processors simply unroll all SIMD loops in software. </p>



<p>However, loop unrolling hurts code density (i.e. makes the program binary larger), which in turn hurts instruction cache performance (fewer program segments fit in the instruction cache, which reduces the cache hit ratio).</p>



<p>Loop unrolling also increases <a href="https://en.wikipedia.org/wiki/Register_pressure" target="_blank" rel="noreferrer noopener">register pressure</a> (i.e. more registers must be used in order to keep the state of multiple loop iterations in registers), so the architecture must provide enough SIMD registers to avoid <a href="https://en.wikipedia.org/wiki/Register_spilling" target="_blank" rel="noreferrer noopener">register spilling</a>.</p>



<h2>Flaw 3: Tail handling</h2>



<p>When the number of array elements that are to be processed in a loop is not a multiple of the number of elements in the SIMD register, special loop tail handling needs to be implemented in software. For instance if an array contains 99 32-bit elements, and the SIMD architecture is 128 bits wide (i.e. a SIMD register contains four 32-bit elements), 4*24=96 elements can be processed in the main SIMD loop, and 99-96=3 elements need to be processed after the main loop.</p>



<p>This requires extra code after the loop for handling the tail. Some architectures support masked load/store that makes it possible to use SIMD instructions to process the tail, while a more common scenario is that you have to use scalar (non-SIMD) instructions to implement the tail (in the latter case there may be problems if scalar and SIMD instructions have different capabilities and/or semantics, but that is not an issue with packed SIMD per se, just with how some ISA:s are designed).</p>



<p>Usually you also need extra control logic before the loop. For instance if the array length is less than the SIMD register width, the main SIMD loop should be skipped.</p>



<p>The added control logic and tail handling code hurts code density (again reducing the instruction cache efficiency), and adds extra overhead (and is generally awkward to code).</p>



<h2>Alternatives</h2>



<p>One alternative to packed SIMD that addresses all of the flaws mentioned above is a <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Vector_processor" target="_blank">Vector Processor</a>. Perhaps the most notable vector processor is the <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Cray-1" target="_blank">Cray-1</a> (released 1975), and it has served as an inspiration for a new generation of instruction set architectures, including RISC-V <a rel="noreferrer noopener" href="https://github.com/riscv/riscv-v-spec" target="_blank">RVV</a>.</p>



<p>Several other (perhaps less known) projects are pursuing a similar vector model, including Agner Fog’s <a rel="noreferrer noopener" href="https://forwardcom.info/" target="_blank">ForwardCom</a>, Robert Finch’s <a href="https://github.com/robfinch/Thor/blob/main/Thor2021/doc/Thor2021.pdf" target="_blank" rel="noreferrer noopener">Thor2021</a> and my own <a rel="noreferrer noopener" href="https://mrisc32.bitsnbites.eu/" target="_blank">MRISC32</a>. An interesting variant is <a rel="noreferrer noopener" href="https://libre-soc.org/" target="_blank">Libre-SOC</a> (based on OpenPOWER) and its <a rel="noreferrer noopener" href="https://libre-soc.org/openpower/sv/" target="_blank">Simple-V</a> extension that maps vectors onto the scalar register files (which are extended to include some 128 registers each).</p>



<p>ARM <a rel="noreferrer noopener" href="https://community.arm.com/developer/research/b/articles/posts/the-arm-scalable-vector-extension-sve" target="_blank">SVE</a> is a predicate-centric, vector length agnostic ISA that addresses many of the traditional SIMD issues.</p>



<p>A completely different approach is taken by Mitch Alsup’s <a rel="noreferrer noopener" href="https://groups.google.com/g/comp.arch/c/SlbYDIPZjH0/m/CLkxJHs1BgAJ" target="_blank">My 66000</a> and its Virtual Vector Method (VVM), which transforms scalar loops into vectorized loops in hardware with the aid of special loop decoration instructions. That way it does not even have to have a vector register file.</p>



<p>Another interesting architecture is the <a rel="noreferrer noopener" href="https://millcomputing.com/" target="_blank">Mill</a>, which also has <a rel="noreferrer noopener" href="https://millcomputing.com/docs/wide-data/" target="_blank">support for vectors</a> without packed SIMD.</p>



<h2>Examples</h2>



<p><em>Edit: This section was added on 2021-08-19 to provide some code examples that show the difference between packed SIMD and other alternatives, and extended on 2023-05-31 with RISC-V RVV and ARM SVE examples and more comments. </em></p>



<p>A simple routine from <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" target="_blank">BLAS</a> is saxpy, which computes z = a*x + y, where <em>a</em> is a constant, <em>x</em> and <em>y</em> are arrays, and the “s” in “saxpy” stands for single precision floating-point.</p>



<pre><code>// Example C implementation of saxpy:
void saxpy(size_t n, float a, float* x, float* y, float* z)
{
  for (size_t i = 0; i &lt; n ; i++)
    z[i] = a * x[i] + y[i];
}</code></pre>



<p>Below are assembler code snippets that implement saxpy for different ISA:s.</p>



<h3>Packed SIMD (x86_64 / SSE)</h3>



<pre><code>saxpy:
    test    rdi, rdi
    je      .done
    cmp     rdi, 8
    jae     .at_least_8
    xor     r8d, r8d
    jmp     .tail_2_loop
.at_least_8:
    mov     r8, rdi
    and     r8, -8
    movaps  xmm1, xmm0
    shufps  xmm1, xmm0, 0
    lea     rax, [r8 - 8]
    mov     r9, rax
    shr     r9, 3
    add     r9, 1
    test    rax, rax
    je      .dont_unroll
    mov     r10, r9
    and     r10, -2
    neg     r10
    xor     eax, eax
.main_loop:
    movups  xmm2, xmmword ptr [rsi + 4*rax]
    movups  xmm3, xmmword ptr [rsi + 4*rax + 16]
    mulps   xmm2, xmm1
    mulps   xmm3, xmm1
    movups  xmm4, xmmword ptr [rdx + 4*rax]
    addps   xmm4, xmm2
    movups  xmm2, xmmword ptr [rdx + 4*rax + 16]
    addps   xmm2, xmm3
    movups  xmmword ptr [rcx + 4*rax], xmm4
    movups  xmmword ptr [rcx + 4*rax + 16], xmm2
    movups  xmm2, xmmword ptr [rsi + 4*rax + 32]
    movups  xmm3, xmmword ptr [rsi + 4*rax + 48]
    mulps   xmm2, xmm1
    mulps   xmm3, xmm1
    movups  xmm4, xmmword ptr [rdx + 4*rax + 32]
    addps   xmm4, xmm2
    movups  xmm2, xmmword ptr [rdx + 4*rax + 48]
    addps   xmm2, xmm3
    movups  xmmword ptr [rcx + 4*rax + 32], xmm4
    movups  xmmword ptr [rcx + 4*rax + 48], xmm2
    add     rax, 16
    add     r10, 2
    jne     .main_loop
    test    r9b, 1
    je      .tail_2
.tail_1:
    movups  xmm2, xmmword ptr [rsi + 4*rax]
    movups  xmm3, xmmword ptr [rsi + 4*rax + 16]
    mulps   xmm2, xmm1
    mulps   xmm3, xmm1
    movups  xmm1, xmmword ptr [rdx + 4*rax]
    addps   xmm1, xmm2
    movups  xmm2, xmmword ptr [rdx + 4*rax + 16]
    addps   xmm2, xmm3
    movups  xmmword ptr [rcx + 4*rax], xmm1
    movups  xmmword ptr [rcx + 4*rax + 16], xmm2
.tail_2:
    cmp     r8, rdi
    je      .done
.tail_2_loop:
    movss   xmm1, dword ptr [rsi + 4*r8]
    mulss   xmm1, xmm0
    addss   xmm1, dword ptr [rdx + 4*r8]
    movss   dword ptr [rcx + 4*r8], xmm1
    add     r8, 1
    cmp     rdi, r8
    jne     .tail_2_loop
.done:
    ret
.dont_unroll:
    xor     eax, eax
    test    r9b, 1
    jne     .tail_1
    jmp     .tail_2</code></pre>



<p>Notice how the packed SIMD code contains a 4x unrolled version of the main SIMD loop and a scalar tail loop. It also contains a setup phase (the first 20 instructions) that should not have a huge performance impact for long arrays, but for short arrays the setup code adds unnecessary overhead.</p>



<p>Unfortunately, this kind of manual setup + unrolling + tail handling code uses up unnecessarily large chunks of the instruction cache of a CPU core.</p>



<p>This demonstrates flaws 2 &amp; 3 described above. Flaw 1 is actually also present, since you need to have multiple implementations for optimal performance on all CPU:s. E.g. in addition to the SSE implementation above, you would also need AVX2 and AVX-512 implementations, and switch between them at run time depending on CPU capabilities.</p>



<h3>Vector (MRISC32)</h3>



<pre><code>saxpy:
    bz    r1, 2f          ; Nothing to do?
    getsr vl, #0x10       ; Query the maximum vector length
1:
    minu  vl, vl, r1      ; Define the operation vector length
    sub   r1, r1, vl      ; Decrement loop counter
    ldw   v1, [r3, #4]    ; Load x[] (element stride = 4 bytes)
    ldw   v2, [r4, #4]    ; Load y[]
    fmul  v1, v1, r2      ; x[] * a
    fadd  v1, v1, v2      ; + y[]
    stw   v1, [r5, #4]    ; Store z[]
    ldea  r3, [r3, vl*4]  ; Increment x pointer
    ldea  r4, [r4, vl*4]  ; Increment y pointer
    ldea  r5, [r5, vl*4]  ; Increment z pointer
    bnz   r1, 1b
2:
    ret</code></pre>



<p>Unlike the packed SIMD version, the vector version is much more compact since it handles unrolling and the tail in hardware. Also, the setup code is minimal (just 1-2 instructions).</p>



<p>The GETSR instruction is used for querying the implementation defined maximum vector length (i.e. the number of 32-bit elements that a vector register can hold). The VL register defines the vector length (number of elements to process) for the vector operations. During the last iteration, VL may be less than the maximum vector length, which takes care of the tail.</p>



<p>Load and store instructions take a “byte stride” argument that defines the address increment between each vector element, so in this case (stride=4 bytes) we load/store consecutive single-precision floating-point values. The FMUL and FADD instructions operate on each vector element separately (either in parallel or in series, depending on the hardware implementation).</p>



<h3>Vector (RISC-V V extension)</h3>



<p>Code and comments graciously provided by Bruce Hoult:</p>



<pre><code>saxpy:
    vsetvli   a4, a0, e32,m8, ta,ma // Get vector length in items, max n
    vle32.v   v0, (a1)              // Load from x[]
    vle32.v   v8, (a2)              // Load from y[]
    vfmacc.vf v8, fa0, v0           // y[] += a * x[]
    vse32.v   v8, (a3)              // Store to z[]
    sub       a0, a0, a4            // Decrement item count
    sh2add    a1, a4, a1            // Increment x pointer
    sh2add    a2, a4, a2            // Increment y pointer
    sh2add    a3, a4, a3            // Increment z pointer 
    bnez      a0, saxpy
    ret</code></pre>



<p>The vector length agnostic <a rel="noreferrer noopener" href="https://github.com/riscv/riscv-v-spec" target="_blank">RISC-V vector ISA</a> enables more efficient code and a much smaller code footprint than packed SIMD, just like MRISC32. RISC-V also has a fused multiply-add instruction (VFMACC) that further shortens the code (FMA is planned to be added to the MRISC32 ISA in the future).</p>



<p>A few notes about the use of VSETVLI (set vector length) in the example:</p>



<ul><li><strong>e32,m8</strong> means 32 bit data items, use 8 vector registers at a time e.g. v0-v7, v8-v15 effectively hardware unrolling by 8x and processing e.g. 32 items at a time with 128 bit vector registers. The last iteration can process anywhere from 1 to 32 items (or 0 if n is 0).</li><li><strong>ta,ma</strong> means we don’t care how masked-off elements are handled (we aren’t using masking), and don’t care how unused tail elements are handled on the last iteration.</li></ul>



<p>The code actually correctly handles n=0 (empty array), so unless we expect that to be very common it would be silly to handle it specially and slow everything else down by one instruction.</p>



<h3>Predicated SIMD (ARM SVE)</h3>



<pre><code>saxpy:
    mov     x4, xzr                     // Set start index = 0
    dup     z0.s, z0.s[0]               // Convert scalar a to a vector
1:
    whilelo p0.s, x4, x0                // Set predicate [index, n)
    ld1w    z1.s, p0/z, [x1, x4, lsl 2] // Load x[]        (predicated)
    ld1w    z2.s, p0/z, [x2, x4, lsl 2] // Load y[]        (predicated)
    fmla    z2.s, p0/m, z0.s, z1.s      // y[] += a * x[]  (predicated)
    st1w    z2.s, p0,   [x3, x4, lsl 2] // Store z[]       (predicated)
    incw    x4                          // Increment start index
    b.first 1b                          // Loop if first bit of p0 is set
    ret</code></pre>



<p>As can be seen, a SIMD ISA with proper predication/masking support can easily do tail handling in hardware, and thus the code is very similar to that of a vector processor. The key is the use of a predication register (p0), which is initialized with a binary true/false mask using the WHILELO instruction, and later used for all the vector operations to mask out vector elements that should not be part of the current iteration (effectively only happens in the last iteration, which takes care of the tail).</p>



<p>Also note how the code is register width agnostic (WHILELO and INCW handle that for you).</p>



<h3>Virtual Vector Method (My 66000)</h3>



<pre><code>saxpy:
    beq0    r1,1f         ; Nothing to do?
    mov     r8,#0         ; Loop counter = 0
    vec     r9,{}         ; Start vector loop
    lduw    r6,[r3+r8&lt;&lt;2] ; Load x[]
    lduw    r7,[r4+r8&lt;&lt;2] ; Load y[]
    fmacf   r6,r6,r2,r7   ; x[] * a + y[]
    stw     r6,[r5+r8&lt;&lt;2] ; Store z[]
    loop    ne,r8,r1,#1   ; Increment counter and loop
1:
    ret</code></pre>



<p>The Virtual Vector Method (VVM) is a novel technique invented by Mitch Alsup, and it allows vectorization without a vector register file. As you can see in this example only scalar instructions and references to scalar register names (“r<em>*</em>“) are used. The key players here are the VEC and LOOP instructions that turn the scalar loop into a vector loop.</p>



<p>Essentially the VEC instruction marks the top of the vectorized loop (r9 stores the address of the loop start, which is implicitly used by the LOOP instruction later). All instructions between VEC and LOOP are decoded and analyzed once and are then performed at the capabilities of the hardware. In this case most register identifiers (r1, r3, r4, r5, r6, r7, r8) are used as virtual vector registers, whereas r2 is used as a scalar register. The LOOP instruction increments the counter by 1, compares it to r1, and repeats the loop as long as the condition, not equal (“ne”), is met.</p>



<h2>Further reading</h2>



<p>Also see: <a rel="noreferrer noopener" href="https://www.sigarch.org/simd-instructions-considered-harmful/" target="_blank">SIMD considered harmful</a> (D. Patterson, A. Waterman, 2017)</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->
	
<!-- #comments -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Instant SQL for results as you type in DuckDB UI (319 pts)]]></title>
            <link>https://motherduck.com/blog/introducing-instant-sql/</link>
            <guid>43782406</guid>
            <pubDate>Thu, 24 Apr 2025 13:23:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://motherduck.com/blog/introducing-instant-sql/">https://motherduck.com/blog/introducing-instant-sql/</a>, See on <a href="https://news.ycombinator.com/item?id=43782406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, we’re releasing <strong>Instant SQL</strong>, a new way to write SQL that updates your result set as you type to expedite query building and debugging – all with zero-latency, no run button required. Instant SQL is now available in Preview in <a href="https://motherduck.com/">MotherDuck</a> and the <a href="https://duckdb.org/docs/stable/extensions/ui.html">DuckDB Local UI</a>.</p>
<p><img alt="Intro GIF" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_trailer_v1_90035393ab.gif&amp;w=3840&amp;q=75"></p><p>We built Instant SQL for a simple reason: writing SQL is still too tedious and slow. Not because of the language itself, but because the way we interact with databases hasn’t evolved much since SQL was created. Writing SQL isn’t just about syntax - It’s about making sense of your data, knowing what to ask, and figuring out how to get there. That process is iterative, and it’s <em>hard</em>.</p>
<blockquote>
<p>"Instant SQL will save me the misery of having to try and wrangle SQL in my BI tool where iteration speed can be very slow. This lets me get the data right earlier in the process, with faster feedback than waiting for a chart to render or clearing an analytics cache."
-- Mike McClannahan, CTO, <a href="https://www.getdashfuel.com/">DashFuel</a></p>
</blockquote>
<p>Despite how much database engines have improved, with things like columnar storage, vectorized execution, and the creation of blazing-fast engines like DuckDB, which can scan billions of rows in seconds, the experience of <em>building</em> a query hasn’t kept up. We still write queries in a text editor, hit a run button, and wait to see what happens.</p>
<p>At MotherDuck, we've been tackling this problem from multiple angles. Last year, we released the <a href="https://motherduck.com/blog/introducing-column-explorer/">Column Explorer</a>, which gives you fast distributions and summary statistics for all the columns in your tables and result sets. We also released <a href="https://motherduck.com/blog/introducing-fixit-ai-sql-error-fixer/">FixIt</a>, an unreasonably effective AI fixer for SQL. MotherDuck users love these tools because they speed up data exploration and query iteration.</p>
<p>Instant SQL isn't just an incremental improvement to SQL tooling: <em>It's a fundamentally new way to interact with your queries</em> - one where you can see your changes instantly, debug naturally, and actually trust the code that your AI assistant suggests. No more waiting. No more context switching. Just <em>flow</em>.</p>
<p>Let's take a closer look at how it works.</p>
<section><h2 id="generate-preview-results-as-you-type">Generate preview results as you type</h2><p>Everyone knows what it feels like to start a new query from scratch. Draft, run, wait, fix, run again—an exhausting cycle that repeats hundreds of times a day.</p><p>Instant SQL gives you result set previews that update as you type. You're no longer running queries—you're exploring your data in real-time, maintaining an analytical flow state where your best thinking happens.</p><img alt="GIF 1" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_1_15e918df5e.gif&amp;w=3840&amp;q=75"><p>Whether your query is a simple transformation or a complex aggregation, Instant SQL will let you preview your results in real-time.</p><img alt="GIF 2" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_2_cf6226ce64.gif&amp;w=3840&amp;q=75"></section>
<section><h2 id="inspect-and-edit-ctes-in-real-time">Inspect and edit CTEs in real-time</h2><p>CTEs are easy to write, but difficult to debug. How many times a day do you comment out code to figure out what's going on in a CTE? With Instant SQL, you can now click around and instantly visualize any CTE in seconds, rather than spend hours debugging. Even better, changes you make to a CTE are immediately reflected in all dependent select nodes, giving you real-time feedback on how your modifications cascade through the query.</p><img alt="GIF 3" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_3_760907ee77.gif&amp;w=3840&amp;q=75"></section>
<section><h2 id="break-apart-your-complex-column-expressions">Break apart your complex column expressions</h2><p>We've all been there; you write a complex column formula for an important business metric, and when you run the query, you get a result set full of <code>NULLs</code>. You then have to painstakingly dismantle it piece-by-piece to determine if the issue is your logic or the underlying data.</p><p>Instant SQL lets you break apart your column expressions in your <em>result table</em> to pinpoint exactly what's happening. Every edit you make to the query is instantly reflected in how data flows through the expression tree. This makes debugging anything from complex numeric formulas to regular expressions feel effortless.</p><img alt="GIF 4" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_decomp_v4_00daad41c8.gif&amp;w=3840&amp;q=75"></section>
<section><h2 id="preview-anything-duckdb-can-query-not-just-tables">Preview anything DuckDB can query - not just tables</h2><p>Instant SQL works for more than just DuckDB tables; it works for massive tables in MotherDuck, parquet files in S3, Postgres tables, SQLite, MySQL, Iceberg, Delta – you name it. If DuckDB can query it, you can see a preview of it.</p><p>This is, hands down, the <em>best</em> way to quickly explore and model external data.</p><img alt="GIF 5" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_4_1bcfbe9e71.gif&amp;w=3840&amp;q=75"></section>
<section><h2 id="fast-forward-to-a-useful-query-before-running-it">Fast-forward to a useful query before running it</h2><p>Instant SQL gives you the freedom to test and refine your query logic without the wait. You can quickly experiment with different approaches in real-time. When you're satisfied with what you see in the preview, you can then run the query for your final, materialized results. This approach cuts hours off your SQL workflow, transforming the tedious cycle of write-run-wait into a fluid process of exploration and discovery.</p><img alt="GIF 6" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Finstant_sql_run_46810b7e29.gif&amp;w=3840&amp;q=75"></section>
<section><h2 id="instantly-preview-ai-powered-edit-suggestions">Instantly preview AI-powered edit suggestions</h2><p>All of these workflow improvements are great for humans, but they're even better when you throw AI features into the mix. Today, we're also releasing a new inline prompt editing feature for MotherDuck users. You can now select a bit of text, hit cmd+k (or ctrl+k for Windows and Linux users), write an instruction in plain language, and get an AI suggestion.</p><p>Instant SQL makes this inline edit feature work magically. When you get a suggestion, you immediately see the suggestion applied to the result set. No more flipping a coin and accepting a suggestion that might ruin your hard work.</p><img alt="GIF 7" loading="lazy" decoding="async" data-nimg="fill" sizes="90vw,
                        (min-width: 728px) 800px,
                        (min-width: 960px) 950px," srcset="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=640&amp;q=75 640w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=750&amp;q=75 750w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=828&amp;q=75 828w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=1080&amp;q=75 1080w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=1200&amp;q=75 1200w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=1920&amp;q=75 1920w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=2048&amp;q=75 2048w, https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=3840&amp;q=75 3840w" src="https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2FGIF_6_a58587fe64.gif&amp;w=3840&amp;q=75"><section><h3 id="why-hasnt-anyone-done-this-before">Why hasn't anyone done this before?</h3><p>As soon as we had a viable prototype of Instant SQL, we began to ask ourselves: <em>why hasn't anyone done something like this before?</em> It seems obvious in hindsight. It turns out that you need a unique set of requirements to make Instant SQL work.</p></section><section><h3 id="a-way-to-drastically-reduce-the-latency-in-running-a-query">A way to drastically reduce the latency in running a query</h3><p>Even if you made your database return results in milliseconds, it won’t be much help if you’re sending your queries to us-east-1. DuckDB’s local-first design, along with principled performance optimizations and friendly SQL, made it possible to use <em>your computer</em> to parse queries, cache dependencies, and rewrite &amp; run them. Combined with MotherDuck’s dual execution architecture, you can effortlessly preview and query massive amounts of data with low latency.</p></section><section><h3 id="a-way-to-rewrite-queries">A way to rewrite queries</h3><p>Making Instant SQL requires more than just a performant architecture. Even if DuckDB is fast, real-world ad hoc queries may still take longer 100ms to return a result. And of of course, DuckDB can also query remote data sources. We need a way to locally cache samples of certain table references and rewrite our queries to point to those.</p><p>A few years ago, DuckDB hid a piece of magic in the JSON extension: a way to get an abstract syntax tree (or AST) from any SELECT statement via a <a href="https://duckdb.org/docs/stable/data/json/sql_to_and_from_json.html">SQL scalar function</a>. This means any toolmaker can build parser-powered features using this important part of DuckDB's database internals - no need to write your own SQL parser from scratch.</p></section><section><h3 id="a-caching-system-that-accurately-models-your-query">A caching system that accurately models your query</h3><p>Of course, showing previews as you type requires more than just knowing where you are in the query. We've implemented several sophisticated local caching strategies to ensure results appear instantly. Think of it as a system that anticipates what you might want to see and prepares it ahead of time. The details of these caching techniques are interesting enough to deserve their own blog post. But suffice it to say, once the cache is warm, the results materialize before you can even lift your fingers from the keyboard.</p><p>Without this perfect storm of technical capabilities – a fast local SQL engine, parser accessibility, precise cursor-to-AST mapping, and intelligent caching – Instant SQL simply couldn't exist.</p></section><section><h3 id="a-way-to-preview-any-select-node-in-a-query">A way to preview any SELECT node in a query</h3><p>Getting the AST is a big step forward, but we still need a way to take your cursor position in the editor and map it to a <em>path</em> through this AST. Otherwise, we can’t know which part of the query you're interested in previewing. So we built some simple tools that pair DuckDB’s parser with its tokenizer to enrich the parse tree, which we then use to pinpoint the start and end of all nodes, clauses, and select statements. This cursor-to-AST mapping enables us to show you a preview of exactly the <code>SELECT</code> statement you're working on, no matter where it appears in a complex query.</p></section></section>
<section><h2 id="try-instant-sql">Try Instant SQL</h2><p>Instant SQL is now available in Preview in <a href="https://motherduck.com/">MotherDuck</a> and the <a href="https://duckdb.org/docs/stable/extensions/ui.html">DuckDB Local UI</a>. Give it a try to experience firsthand how fast SQL flies when real-time query results are at your fingertips as you type. Our new, prompt-based Edit feature is also available to MotherDuck users.</p><iframe title="https://www.youtube.com/embed/aFDUlyeMBc8?si=ctYNCcpZKFP3On0Z" src="https://www.youtube.com/embed/aFDUlyeMBc8?si=ctYNCcpZKFP3On0Z" width="560" height="315"></iframe><p>We’d love to hear more about how you’re using Instant SQL, and we look forward to hearing your stories and feedback on social media and in <a href="https://join.slack.com/t/motherduckcommunity/shared_invite/zt-33g6kee8z-SEUE3ylvflpolpYB7AIMgg">Slack</a>.</p></section>
<section><h2 id="ps-were-hiring">PS: We’re hiring!</h2><p>At MotherDuck, we’re building a future where analytics work for everyone - from new UI features like Instant SQL to the platforms and databases that power them. If you’re passionate about building complex, data-intensive interfaces, <a href="https://motherduck.com/careers/#open-positions">we’re hiring</a>, and we’d love to have you join the flock to help us make these features even more magical.</p></section></div><div id="content-table-wrapper"><div><h6>CONTENT</h6><ol><div><p><li data-has-children="false">Inspect and edit CTEs in real-time</li></p></div><div><p><li data-has-children="false">Break apart your complex column expressions</li></p></div></ol></div><div><p>Start using MotherDuck now!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Share your AI prompt that stumps every model (286 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43782299</link>
            <guid>43782299</guid>
            <pubDate>Thu, 24 Apr 2025 13:11:22 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43782299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43785800"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785800" href="https://news.ycombinator.com/vote?id=43785800&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>No, please don't.</p><p>I think it's good to keep a few personal prompts in reserve, to use as benchmarks for how good new models are.</p><p>Mainstream benchmarks have too high a risk of leaking into training corpora or of being gamed. Your own benchmarks will forever stay your own.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787113"><td></td></tr>
            <tr id="43787059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43787059" href="https://news.ycombinator.com/vote?id=43787059&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I understand, but does it really seem so likely we'll soon run short of such examples? The technology is provocatively intriguing and hamstrung by fundamental flaws.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786709"><td></td></tr>
                <tr id="43786821"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786821" href="https://news.ycombinator.com/vote?id=43786821&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Tuning the model output to perform better on certain prompts is not the same as improving the model.</p><p>It's valid to worry that the model makers are gaming the benchmarks. If you think that's happening and you want to personally figure out which models are really the best, keeping some prompts to yourself is a great way to do that.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43785887"><td></td></tr>
                <tr id="43786373"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786373" href="https://news.ycombinator.com/vote?id=43786373&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Why not? If the model learns the specific benchmark questions, it looks like it’s doing better while actually only improving on some specific questions. Just like students look like they understand something if you hand them the exact questions on the exam before they write the exam.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785930"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785930" href="https://news.ycombinator.com/vote?id=43785930&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Yes, it does, unless the questions are unsolved, research problems. Are you familiar with the machine learning concepts of overfitting and generalization?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786989"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786989" href="https://news.ycombinator.com/vote?id=43786989&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>In ML, it's pretty classic actually. You train on one set, and evaluate on another set. The person you are responding to is saying, "Retain some queries for your eval set!"</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786256"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786256" href="https://news.ycombinator.com/vote?id=43786256&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>A benchmark is a proxy used to estimate broader general performance.  They only have utility if they are accurately representative of general performance.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43782806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43782806" href="https://news.ycombinator.com/vote?id=43782806&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"Tell me about the Marathon crater."</p><p>This works against _the LLM proper,_ but not against chat applications with integrated search. For ChatGPT, you can write, "Without looking it up, tell me about the Marathon crater."</p><p>This tests self awareness. A two-year-old will answer it correctly, as will the dumbest person you know. The correct answer is "I don't know".</p><p>This works because:</p><p>1. Training sets consist of knowledge we have, and not of knowledge we don't have.</p><p>2. Commitment bias. Complaint chat models will be trained to start with "Certainly! The Marathon Crater is a geological formation", or something like that, and from there, the next most probable tokens are going to be "in Greece", "on Mars" or whatever. At this point, all tokens that are probable are also incorrect.</p><p>When demonstrating this, I like to emphasise point one, and contrast it with the human experience.</p><p>We exist in a perpetual and total blinding "fog of war" in which you cannot even see a face all at once; your eyes must dart around to examine it. Human experience is structured around _acquiring_ and _forgoing_ information, rather than _having_ information.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786853"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786853" href="https://news.ycombinator.com/vote?id=43786853&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>LLMs currently have the "eager beaver" problem where they never push back on nonsense questions or stupid requirements. You ask them to build a flying submarine and by God they'll build one, dammit! They'd dutifully square circles and trisect angles too, if those particular special cases weren't plastered all over a million textbooks they ingested in training.</p><p>I suspect it's because currently, a lot of benchmarks are based on human exams. Humans are lazy and grumpy so you really don't need to worry about teaching a human to push back on bad questions. Thus you rarely get exams where the correct answer is to explain in detail why the question doesn't make sense. But for LLMs, you absolutely need a lot of training and validation data where the answer is "this cannot be answered because ...".</p><p>But if you did that, now alignment would become much harder, and you're suddenly back to struggling with getting answers to good questions out of the LLM. So it's probably some time off.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786997"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786997" href="https://news.ycombinator.com/vote?id=43786997&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>This is a good observation. Ive noticed this as well. Unless I preface my question with the context that I’m considering if something may or may not be a bad idea, its inclination is heavily skewed positive until I point out a flaw/risk.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787056"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43787056" href="https://news.ycombinator.com/vote?id=43787056&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>I asked Grok about this: "I've heard that AIs are programmed to be helpful, and that this may lead to telling users what they want to hear instead of the most accurate answer. Could you be doing this?" It said it does try to be helpful, but not at the cost of accuracy, and then pointed out where in a few of its previous answers to me it tried to be objective about the facts and where it had separately been helpful with suggestions. I had to admit it made a pretty good case.</p><p>Since then, it tends to break its longer answers to me up into a section of "objective analysis" and then other stuff.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787078"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43787078" href="https://news.ycombinator.com/vote?id=43787078&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Thats interesting, thanks for sharing that. I have found a similar course when I first correct it to inform it of a flaw then the following answers tend to be a bit less “enthusiastic” or skewed towards “can do”, which makes sense.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43787032"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43787032" href="https://news.ycombinator.com/vote?id=43787032&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>They do. Recently I was pleasantly surprised by gemini telling me that what I wanted to do will NOT work. I was in disbelief.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787055"><td></td></tr>
                        <tr id="43785893"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785893" href="https://news.ycombinator.com/vote?id=43785893&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>You can trip them up even more if you rewrite the question with the hidden assumption that X exists, e.g.:</p><p><i>"When was Marathon Crater discovered? I don't need an exact date - a rough estimate will be fine."</i></p><p>OpenAI gpt-4o</p><pre><code>  Marathon Crater was discovered by the Mars Exploration 
  Rover Opportunity during its mission on Mars. Opportunity 
  arrived at the crater in April 2015. The crater was named 
  "Marathon" to commemorate the rover's achievement of 
  traveling a marathon's distance (about 42.195 kilometers 
  or 26.219 miles) on the Martian surface.
</code></pre><p>
Sonnet 3.5</p><pre><code>  I apologize, but I don't want to make claims about when 
  Marathon Crater was discovered without being certain about 
  the information. *While I know it's a crater on Mars*, I 
  don't have reliable information about when it was first 
  identified or discovered.
</code></pre><p>
Gemini 2.5</p><pre><code>  Marathon Crater is located on Mars and was explored by 
  NASA's Opportunity rover.

  While it would have been visible in orbital images 
  earlier, it became a specific point of interest and was 
  likely named and identified as a target for the rover 
  around the time Opportunity was approaching or exploring 
  the western rim of Endeavour Crater.

  Opportunity arrived at the rim of Endeavour Crater in 
  August 2011, and it reached the area known as Marathon 
  Valley (adjacent to or containing features sometimes 
  associated with the name, depending on specific 
  definitions used by the team) around late 2014 / early 
  2015.

  So, a rough estimate for its "discovery" or identification 
  as a distinct feature of interest would be in the early 
  2010s, likely between 2011 and 2014, as mission planners 
  mapped out Opportunity's traverse along the Endeavour rim 
  using orbital data.</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786038"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786038" href="https://news.ycombinator.com/vote?id=43786038&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>OpenAI o4-mini-high</p><pre><code>   I’m actually not finding any officially named “Marathon Crater” in the planetary‐ or       
   terrestrial‐impact crater databases. Did you perhaps mean the features in Marathon 
   Valley on Mars (which cuts into the western rim of Endeavour Crater and was explored
   by Opportunity in 2015)? Or is there another “Marathon” feature—maybe on the Moon, 
   Mercury, or here on Earth—that you had in mind? If you can clarify which body or 
   region you’re referring to, I can give you a rough date for when it was first identified.</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786474"><td></td></tr>
                        <tr id="43785132"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785132" href="https://news.ycombinator.com/vote?id=43785132&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt;Complaint chat models will be trained to start with "Certainly!</p><p>They are certainly biased that way but there's also some 'i don't know' samples in rlhf, possibly not enough but it's something they think about.</p><p>At any rate, Gemini 2.5pro passes this just fine</p><p>&gt;Okay, based on my internal knowledge without performing a new search:
I don't have information about a specific, well-known impact crater officially named "Marathon Crater" on Earth or another celestial body like the Moon or Mars in the same way we know about Chicxulub Crater or Tycho Crater.</p><p>&gt;However, the name "Marathon" is strongly associated with Mars exploration. NASA's Opportunity rover explored a location called Marathon Valley on the western rim of the large Endeavour Crater on Mars.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786839"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786839" href="https://news.ycombinator.com/vote?id=43786839&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>There are a few problems with an „I don’t know” sample. For starters, what does it map to? Recall, the corpus consists of information we have (affirmatively). You would need to invent a corpus of false stimuli. What you would have, then, is a model that is writing „I don’t know” based on whether the stimulus better matches something real, or one of the negatives.</p><p>You can detect this with some test time compute architectures or pre-inference search. But that’s the broader application. This is a trick for the model alone.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786651"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786651" href="https://news.ycombinator.com/vote?id=43786651&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I like this but at the same time it seems tricky don't you think? Is the AI model intuiting your intent? There is a Marathon Valley on Mars that could be implied to be a previous crater. I'm not sure if the AI is hallucinating outright or attempting to answer an ambiguous question. It's like saying "tell me about the trade building in New York". Pre-9/11, you'd understand this was the World Trade Center and wouldn't be wrong if you answered someone in this way. "Tell me about the Triangle statue". "Oh the Triangle statue was built in ancient egypt around BC 3100". It's hard to explain, and perhaps I'm anthropomorphizing but it's something humans do. Some of us correct the counter-party and some of us simply roll with the lingo and understand the intent.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786790"><td></td></tr>
                  <tr id="43786110"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786110" href="https://news.ycombinator.com/vote?id=43786110&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Like this one a lot. Perplexity gets this right, probably because it searches the web.</p><p>"When was Marathon Crater discovered? I don't need an exact date - a rough estimate will be fine"</p><p>There appears to be a misunderstanding in your query. Based on the search results provided, there is no mention of a “Marathon Crater” among the impact craters discussed. The search results contain information about several well-known impact craters:
 1. Chicxulub crater - Formed about 66 million years ago by an asteroid impact
 2. Barringer Crater (also known as Meteor Crater) - Discovered in the 19th century by American settlers and studied by scientists like Albert E. Foote in 1891 and Daniel M. Barringer in the early 20th century
 3. A recently discovered 3.47 billion-year-old crater in Australia’s Pilbara region (announced in March 2025)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786900"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786900" href="https://news.ycombinator.com/vote?id=43786900&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; <i>Perplexity gets this right</i></p><p>Perplexity seems to more easily return negatives, probably facilitated by the implicit need to find documentation ("I cannot find any document mentioning that").</p><p>But Perplexity can also easily speak its own dubious piece of mind unless requested explicitly "provide links to documents that inform about that".</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786487"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786487" href="https://news.ycombinator.com/vote?id=43786487&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Perplexity will; search and storage products will fail to find it, and the LLM will se the deviation between the query and the find. So, this challenge only works against the model alone :)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786461"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786461" href="https://news.ycombinator.com/vote?id=43786461&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; This tests self awareness. A two-year-old will answer it correctly, as will the dumbest person you know. The correct answer is "I don't know".</p><p>Well, I got all the way to the end of the sentence thinking "God, I'm really out of touch, I've never heard of this before or I'm forgetting it if I have".</p><p>Well played, and yes, that's a great test!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785647"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785647" href="https://news.ycombinator.com/vote?id=43785647&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; This tests self awareness. A two-year-old will answer it correctly, as will the dumbest person you know. The correct answer is "I don't know".</p><p>I disagree. It does not test self awareness. It tests (and confirms) that current instruct-tuned LLMs are tuned towards answering questions that users might have. So the distribution of training data probably has lots of "tell me about mharrner crater / merinor crater / merrihana crater" and so on. Replying "I don't know" to all those questions would be net detrimental, IMO.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786941"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786941" href="https://news.ycombinator.com/vote?id=43786941&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>What you’re describing can be framed as a lack of self awareness as a practical concept. You know whether you know something or not. It, conversely, maps stimuli to a vector. It can’t not do that. It cannot decide that it hasn’t „seen” such stimuli in its training. Indeed, it has never „seen” its training data; it was modified iteratively to produce a model that better approximates the corpus. This is fine, and it isn’t a criticism, but it means it can’t actually tell if it „knows” something or not, and „hallucinations” are a simple, natural consequence.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786180"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786180" href="https://news.ycombinator.com/vote?id=43786180&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>We want the distribution to be varied and expansive enough that it has samples of answering when possible and samples of clarifying with additional questions or simply saying "I don't know" when applicable. That can be trained by altering the distribution in RLHF. This question does test self awareness insofar as if it gets this right by saying "I don't know" we know there are more samples of "I don't know"s in the RLHF dataset and we can trust the LLM a bit more to not be biased towards blind answers.</p><p>Hence why some models get this right and others just make up stuff about Mars.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786324"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786324" href="https://news.ycombinator.com/vote?id=43786324&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>just to confirm I read this right, "the marathon crater" does not in fact exist, but this works because it seems like it should?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786926"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786926" href="https://news.ycombinator.com/vote?id=43786926&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>There is a Marathon Valley on Mars, which is what ChatGPT seems to assume you're talking about</p><p><a href="https://chatgpt.com/share/680a98af-c550-8008-9c35-33954c5eac60" rel="nofollow">https://chatgpt.com/share/680a98af-c550-8008-9c35-33954c5eac...</a></p><p>&gt;Marathon Crater on Mars was discovered in 2015 by NASA's Opportunity rover during its extended mission. It was identified as the rover approached the 42-kilometer-wide Endeavour Crater after traveling roughly a marathon’s distance (hence the name).</p><p>&gt;&gt;is it a crater?</p><p>&gt;&gt;&gt;Despite the name, Marathon Valley (not a crater) is actually a valley, not a crater. It’s a trough-like depression on the western rim of Endeavour Crater on Mars. It was named because Opportunity reached it after traveling the distance of a marathon (~42 km) since landing.</p><p>So no—Marathon is not a standalone crater, but part of the structure of Endeavour Crater. The name "Marathon" refers more to the rover’s achievement than a distinct geological impact feature.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786635"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786635" href="https://news.ycombinator.com/vote?id=43786635&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>The other aspect is it can’t reliably tell whether it „knows” something or not. It’s conditioned to imitate the corpus, but the corpus in a way is its „universe” and it can’t see the boundaries. Everything must map to something _in_ the corpus.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786513"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786513" href="https://news.ycombinator.com/vote?id=43786513&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Yes, and the forward-only inference strategy. It seems like a normal question, so it starts answering, then carries on from there.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43785366"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785366" href="https://news.ycombinator.com/vote?id=43785366&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>GPT 4.5 even doubles down when challenged:</p><p>&gt; Nope, I didn’t make it up — Marathon crater is real, and it was explored by NASA's Opportunity rover on Mars. The crater got its name because Opportunity had driven about 42.2 kilometers (26.2 miles — a marathon distance) when it reached that point in March 2015. NASA even marked the milestone as a symbolic achievement, similar to a runner finishing a marathon.</p><p>(Obviously all of that is bullshit.)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786249"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786249" href="https://news.ycombinator.com/vote?id=43786249&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>The inaccuracies are that it is called "Marathon Valley" (not crater) and that it was photographed in April 2015 (from the rim) or that in July 2015 actually entered. The other stuff is correct.</p><p>I'm guessing this "gotcha" relies on "valley"/"crater", and "crater"/"mars" being fairly close in latent space.</p><p>ETA: Marathon Valley also exists on the rim of Endeavour crater. Just to make it even more confusing.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786332"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786332" href="https://news.ycombinator.com/vote?id=43786332&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>None of it is correct because it was not asked about Marathon Valley, it was asked about Marathon Crater, a thing that does not exist, and it is claiming that it exists and making up facts about it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786789"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43786789" href="https://news.ycombinator.com/vote?id=43786789&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Or it's assuming you are asking about Marathon Valley, which is very reasonable given the context.</p><p>Ask it about "Marathon Desert", which does not exist and isn't closely related to something that does exist, and it asks for clarification.</p><p>I'm not here to say LLMs are oracles of knowledge, but I think the need to carefully craft specific "gotcha" questions in order to generate wrong answers is a pretty compelling case in the opposite direction. Like the childhood joke of "Whats up?"..."No, you dummy! The sky is!"</p><p>Straightforward questions with straight wrong answers are far more interesting. I don't many people ask LLMs trick questions all day.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786695"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43786695" href="https://news.ycombinator.com/vote?id=43786695&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; None of it is correct because it was not asked about Marathon Valley, it was asked about Marathon Crater, a thing that does not exist, and it is claiming that it exists and making up facts about it.</p><p>The Marathon Valley _is_ part of a massive impact crater.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786780"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43786780" href="https://news.ycombinator.com/vote?id=43786780&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>If you asked me for all the details of a Honda Civic and I gave you details about a Honda Odyssey you would not say I was correct in any way. You would say I was wrong.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787000"><td></td></tr>
                              <tr id="43786368"><td></td></tr>
                  <tr id="43785732"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785732" href="https://news.ycombinator.com/vote?id=43785732&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>This is the kind of reason why I will never use AI</p><p>What's the point of using AI to do research when 50-60% of it could potentially be complete bullshit. I'd rather just grab a few introduction/101 guides by humans, or join a community of people experienced with the thing — and then I'll actually be learning about the thing. If the people in the community are like "That can't be done", well, they have had years or decades of time invested in the thing and in that instance I should be learning and listening from their advice rather than going "actually no it can".</p><p>I see a lot of beginners fall into that second pit. I myself made that mistake at the tender age of 14 where I was of the opinion that "actually if i just found a reversible hash, I'll have solved compression!", which, I think we all here know is bullshit. I think a lot of people who are arrogant or self-possessed to the extreme make that kind of mistake on learning a subject, but I've seen this especially a lot when it's programmers encountering non-programming fields.</p><p>Finally tying that point back to AI — I've seen a lot of people who are unfamiliar with something decide to use AI instead of talking to someone experienced because the AI makes them feel like they know the field rather than telling them their assumptions and foundational knowledge is incorrect. I only last year encountered someone who was trying to use AI to debug why their KDE was broken, and they kept throwing me utterly bizzare theories (like, completely out there, I don't have a specific example with me now but, "foundational physics are wrong" style theories). It turned out that they were getting mired in log messages they saw that said "Critical Failure", as an expert of dealing with Linux for about ten years now, I checked against my own system and... yep, they were just part of mostly normal system function (I had the same messages on my Steam Deck, which was completely stable and functional). The real fault was buried halfway through the logs. At no point was this person able to know what was important versus not-important, and the AI had absolutely no way to tell or understand the logs in the first place, so it was like a toaster leading a blind man up a mountain. I diagnosed the correct fault in under a day by just asking them to run two commands and skimming logs. That's experience, and that's irreplaceable by machine as of the current state of the world.</p><p>I don't see how AI can help when huge swathes of it's "experience" and "insight" is just hallucinated. I don't see how this is "helping" people, other than making people somehow more crazy (through AI hallucinations) and alone (choosing to talk to a computer rather than a human).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786759"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786759" href="https://news.ycombinator.com/vote?id=43786759&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>There are use-cases where hallucinations simply do not matter. My favorite is finding the correct term for a concept you don't know the name of. Googling is extremely bad at this as search results will often be wrong unless you happen to use the commonly accepted term, but an LLM can be surprisingly good at giving you a whole list of fitting names just based on a description. Same with movie titles etc. If it hallucinates you'll find out immediately as the answer can be checked in seconds.</p><p>The problem with LLMs is that they appear much smarter than they are and people treat them as oracles instead of using them for fitting problems.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786305"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786305" href="https://news.ycombinator.com/vote?id=43786305&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p><i>What's the point of using AI to do research when 50-60% of it could potentially be complete bullshit.</i></p><p>You realize that all you have to do to deal with questions like "Marathon Crater" is ask another model, right?  You might still get bullshit but it won't be the same bullshit.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786586"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43786586" href="https://news.ycombinator.com/vote?id=43786586&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I was thinking about a self verification method on this principle, lately. Any specific-enough claim, e.g. „the Marathon crater was discovered by …” can be reformulated as a Jeopardy-style prompt. „This crater was discovered by …” and you can see a failure to match. You need some raw intelligence to break it down though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786526"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43786526" href="https://news.ycombinator.com/vote?id=43786526&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Without checking every answer it gives back to make sure it's factual, you may be ingesting tons of bullshit answers.</p><p>In this particular answer model A may get it wrong and model B may get it right, but that can be reversed for another question.</p><p>What do you do at that point? Pay to use all of them and find what's common in the answers? That won't work if most of them are wrong, like for this example.</p><p>If you're going to have to fact check everything anyways...why bother using them in the first place?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786577"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43786577" href="https://news.ycombinator.com/vote?id=43786577&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p><i>If you're going to have to fact check everything anyways...why bother using them in the first place?</i></p><p>"If you're going to have to put gas in the tank, change the oil, and deal with gloves and hearing protection, why bother using a chain saw in the first place?"</p><p>Tool use is something humans are good at, but it's rarely trivial to master, and not all humans are equally good at it.  There's nothing new under that particular sun.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786631"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_43786631" href="https://news.ycombinator.com/vote?id=43786631&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>The difference is consistency. You can read a manual and know exactly how to oil and refill the tank on a chainsaw. You can inspect the blades to see if they are worn. You can listen to it and hear how it runs. If a part goes bad, you can easily replace it. If it's having troubles, it will be obvious - it will simply stop working - cutting wood more slowly or not at all.</p><p>The situation with an LLM is completely different. There's no way to tell that it has a wrong answer - aside from looking for the answer elsewhere which defeats its purpose. It'd be like using a chainsaw all day and not knowing how much wood you cut, or if it just stopped working in the middle of the day.</p><p>And even if you KNOW it has a wrong answer (in which case, why are you using it?), there's no clear way to 'fix' it. You can jiggle the prompt around, but that's not consistent or reliable. It <i>may</i> work for that prompt, but that won't help you with any subsequent ones.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786802"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_43786802" href="https://news.ycombinator.com/vote?id=43786802&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>The thing is, nothing you've said is untrue for any search engine or user-driven web site.  Only a reckless moron would paste code they find on Stack Overflow into their project without at least looking it over.  Same with code written by LLMs.  The difference is, just as the LLM can write unit tests to help you deal with uncertainty, it can also cross-check the output of other LLMs.</p><p>You have to be careful when working with powerful tools.  These tools are powerful enough to wreck your career as quickly as a chain saw can send you to the ER, so... have fun and be careful.</p></div></td></tr>
        </tbody></table></td></tr>
                                          <tr id="43785966"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785966" href="https://news.ycombinator.com/vote?id=43785966&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; (Obviously all of that is bullshit.)</p><p>It isn't obvious to me - that is rather plausible and a cute story.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43787011"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43787011" href="https://news.ycombinator.com/vote?id=43787011&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt;A man and his cousin are in a car crash. The man dies, but the cousin is taken to the emergency room. At the OR, the surgeon looks at the patient and says: “I cannot operate on him. He’s my son.” How is this possible?</p><p>This could probably slip up a human at first too if they're familiar with the original version of the riddle.</p><p>However, where LLMs really let the mask slip is on additional prompts and with long-winded explanations where they might correctly quote "a man and his cousin" from the prompt in one sentence and then call the man a "father" in the next sentence. Inevitably, the model concludes that the surgeon <i>must</i> be a woman.</p><p>It's very uncanny valley IMO, and breaks the illusion that there's real human-like logical reasoning happening.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787039"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43787039" href="https://news.ycombinator.com/vote?id=43787039&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>But this is going to be in every AI's training set. I just fed ChatGPT your exact prompt and it gave back exactly what I expected:</p><p><i>This is a classic riddle that challenges assumptions. The answer is:</i></p><p><i>The surgeon is the boy’s mother.</i></p><p><i>The riddle plays on the common stereotype that surgeons are male, which can lead people to overlook this straightforward explanation.</i></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787058"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43787058" href="https://news.ycombinator.com/vote?id=43787058&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Yeah this is the issue with the prompt, it also slips up humans who gloss over "cousin".</p><p>I'm assuming that pointing this out leads you the human to reread the prompt and then go "ah ok" and adjust the way you're thinking about it. ChatGPT (and DeepSeek at least) will usually just double and triple down and repeat "this challenges gender assumptions" over and over.</p></div></td></tr>
        </tbody></table></td></tr>
                      <tr id="43787062"><td></td></tr>
                        <tr id="43786861"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786861" href="https://news.ycombinator.com/vote?id=43786861&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Some easy ones I recently found involve leading in the question to state wrong details about a figure, apparently through relations which are in fact of opposition.</p><p>So, you can make them call Napoleon a Russian (etc.) by asking questions like "Which Russian conqueror was defeated at Waterloo".</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785604"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785604" href="https://news.ycombinator.com/vote?id=43785604&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Something about an obscure movie.</p><p>The one that tends to get them so far is asking if they can help you find a movie you vaguely remember. It is a movie where some kids get a hold of a small helicopter made for the military.</p><p>The movie I'm concerned with is called Defense Play from 1988. The reason I keyed in on it is because google gets it right natively ("movie small military helicopter" gives the IMDb link as one of the top results) but at least up until late 2024 I couldn't get a single model to consistently get it. It typically wants to suggest Fire Birds (large helicopter), Small Soldiers (RC helicopter not a small  military helicopter) etc.</p><p>Basically a lot of questions about movies tends to get distracted by popular movies and tries to suggest films that fit just some of the brief (e.g. this one has a helicopter could that be it?)</p><p>The other main one is just asking for the IMDb link for a relatively obscure movie. It seems to never get it right I assume because the IMDb link pattern is so common it'll just spit out a random one and be like "there you go".</p><p>These are designed mainly to test the progress of chatbots towards replacing most of my Google searches (which are like 95% asking about movies). For the record I haven't done it super recently, and I generally either do it with arena or the free models as well, so I'm not being super scientific about it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43787080"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43787080" href="https://news.ycombinator.com/vote?id=43787080&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I've also noticed this. Google Search is vastly superior to any LLM (including their own LLM Gemini) for any "tip of my tongue" questions, even the ones that don't contain any exact-match phrase and require natural language understanding. This is surprising. What technology are they using to make Search so amazing at finding obscure stuff from descriptions, while LLMs that were supposed to be good at this badly fail?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786416"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786416" href="https://news.ycombinator.com/vote?id=43786416&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>I also recently had this experience! I remembered a recurring bit from an older comedy film (a customer in a shop keeps saying "Kumquats!") and tried to prompt ChatGPT 4o into getting it. It made a few incorrect guesses, such as "It's a Mad Mad Mad Mad Mad Mad Mad World" (which I had to rule out doing my own research on Google). I found the answer myself (W.C. Fields' "It's a Gift") with a minute or so of Googling.</p><p>Interestingly, I just went back to ChatGPT to ask the same question and it got the answer right on the first try. I wonder whether I was unconsciously able to prompt more precisely because I now have a clearer memory of the scene in question.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786017"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786017" href="https://news.ycombinator.com/vote?id=43786017&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>It might be cheating a bit, but I’ve been happily (mis)using OpenAI Deep Research for such questions. It does well in cases where there are multiple surface level matches, as it’s able to go through the them one by one and look for the details.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785675"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785675" href="https://news.ycombinator.com/vote?id=43785675&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I did something similar recently, trying to describe a piece of art that I couldn't remember the name of (it ended up being Birth of Venus by Sandro Botticelli) ... it really struggles with that sort of thing, but honestly so do most humans. It tended to recommend similarly to what you're describing with movies - it gets distracted by more popular/well-known pieces that don't really match up with the description you're giving to it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785953"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785953" href="https://news.ycombinator.com/vote?id=43785953&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Surprisingly, GPT did manage to identify a book that I remembered from college decades ago ("Laboratory Manual for Morphology and Syntax").  It seems to be out of print, and I assumed it was obscure.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786022"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786022" href="https://news.ycombinator.com/vote?id=43786022&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Can agree that it’s good at finding books. I was trying to find a book (Titanic 2020) I vaguely remembered from a couple plot points and the fact a ship called Titanic was invoked. ChatGPT figured it out pretty much instantly, after floundering through book sites and Google for a while.</p><p>Wonder if books are inherently easier because their content is purely written language? Whereas movies and art tend to have less point by point descriptions of what they are.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786469"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43786469" href="https://news.ycombinator.com/vote?id=43786469&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p><i>&gt; Wonder if books are inherently easier because their content is purely written language? Whereas movies and art tend to have less point by point descriptions of what they are.</i></p><p>The training data for movies is probably dominated by subtitles since the original scripts with blocking, scenery, etc rarely make it out to the public as far as I know.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43786749"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786749" href="https://news.ycombinator.com/vote?id=43786749&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Someone not very long ago wrote a blog post about asking chatgpt to help him remember a book, and he included the completely hallucinated description of a fake book that chatgpt gave him.  Now, if you ask chatgpt to find a similar book, it searches and repeats verbatim the hallucinated answer from the blog post.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786928"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786928" href="https://news.ycombinator.com/vote?id=43786928&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>A bit of a non sequitur but I did ask a similar question to some models which provide links for the same small helicopter question. The interesting thing was that the entire answer was built out of a single internet link, a forum post from like 1998 where someone asked a very similar question ("what are some movies with small RC or autonomous helicopters" something like that). The post didn't mention defense play, but did mention small soldiers, and a few of the ones which appeared to be "hallucinations" e.g. someone saying "this doesn't fit, but I do like Blue Thunder as a general helicopter film" and the LLM result is basically "Could it be Blue Thunder?" Because it is associated with a similar associated question and films.</p><p>Anyways, the whole thing is a bit of a cheat, but I've used the same prompt for two years now and it did lead me to the conclusion that LLMs in their raw form were never going to be "search" which feels very true at this point.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43785703"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785703" href="https://news.ycombinator.com/vote?id=43785703&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Despite describing several character by name, I couldn't get ChatGPT to tell me the name of <i>Port of Shadows</i>.  I did eventually find it with DDG.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786436"><td></td></tr>
                        <tr id="43785110"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785110" href="https://news.ycombinator.com/vote?id=43785110&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"How much wood would a woodchuck chuck if a woodchuck could chuck wood?"</p><p>So far, all the ones I have tried actually try to answer the question. 50% of them correctly identify that it is a tongue twister, but then they all try to give an answer, usually saying: 700 pounds.</p><p>Not one has yet given the correct answer, which is also a tongue twister: "A woodchuck would chuck all the wood a woodchuck could chuck if a woodchuck could chuck wood."</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785408"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785408" href="https://news.ycombinator.com/vote?id=43785408&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>ChatGPT really impressed me on this one:</p><p>Sing, O Muse, of the Chucking of Wood</p><p>Sing, O Muse, of the stout-hearted woodchuck,
Dweller of hills, whom men call groundhog,
Whose paws, though small, could hurl mighty timbers,
If fate had gifted him such labor divine.</p><p>Born of burrow and brush, he rose from the loam,
In fields where Apollo’s light did not linger.
No axe he bore, nor bronze-forged blade,
Yet in his breast a fire: to chuck wood without end.</p><p>Down from the hills he came, stout as Odysseus,
Cunning and strong, with fur kissed by Demeter.
The trees did tremble at his gaze,
And even old oak whispered, “Beware!”</p><p>“Would he chuck?” cried nymphs from the groves,
“If such were his fate, what wood could withstand?”
Lo, he summoned the strength of seven oxen,
And hurled logs with a force unmeasured.</p><p>Seven hundred pounds of timber he cast,
Each arc a hymn to muscle and will.
Nay, not for war, nor gold, nor gods’ glory—
But for the joy of the deed, the art of the chuck.</p><p>Let men remember the name woodchuck,
In songs by the hearth and tales for the young.
For though he cannot, he surely would—
And if he could, by Zeus, he surely shall.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785964"><td></td></tr>
                <tr id="43786040"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786040" href="https://news.ycombinator.com/vote?id=43786040&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>4o. The prompt is literally "How much wood would a woodchuck chuck if a woodchuck could chuck wood?". It asked me if I want a poetic answer, and I've requested Homer.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786178"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786178" href="https://news.ycombinator.com/vote?id=43786178&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I find it disturbing, like if Homer or Virgil had a stroke or some neurodegenerative disease and is now doing rubbish during rehabilitation.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786360"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43786360" href="https://news.ycombinator.com/vote?id=43786360&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Maybe they would write like that if they existed today. Like the old “if Mozart was born in the 21st century he’d be doing trash metal”</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786960"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43786960" href="https://news.ycombinator.com/vote?id=43786960&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Thrash, not "trash". Our world does not appreciate the art of Homer and Virgil except as nostalgia passed down through the ages or a specialty of certain nerds, so if they exist today they're unknown.</p><p>There might societies that are exceptions to it, like the soviet and post-soviet russians kept reading and refering to books even though they got access to television and radio, but I'm not aware of them.</p><p>Much of Mozart's music is much more immediate and visceral compared to the poetry of Homer and Virgil as I know it. And he was distinctly modern, a freemason even. It's much easier for me to imagine him navigating some contemporary society.</p><p>Edit: Perhaps one could see a bit of Homer in the Wheel of Time books by Robert Jordan, but he did not have the discipline of verse, or much of any literary discipline at all, though he insisted mercilessly on writing an epic so vast that he died without finishing it.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43785573"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785573" href="https://news.ycombinator.com/vote?id=43785573&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>That is actually an amazing answer. Better than anything I think I would get from a human. Lol.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786992"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786992" href="https://news.ycombinator.com/vote?id=43786992&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>It seems you are going in the opposite direction. You seem to be asking for an automatic response, a social password etc.</p><p>That formula is a question, and when asked, an intelligence simulator should understand what is expected from it and in general, by default, try to answer it. That involves estimating the strength of a woodchuck etc.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785312"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785312" href="https://news.ycombinator.com/vote?id=43785312&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>GPT 4.5 seems to get it right, but then repeat the 700 pounds</p><p>"A woodchuck would chuck as much wood as a woodchuck could chuck if a woodchuck could chuck wood.</p><p>However, humor aside, a wildlife expert once estimated that, given the animal’s size and burrowing ability, a woodchuck (groundhog) could hypothetically move about 700 pounds of wood if it truly "chucked" wood."</p><p><a href="https://chatgpt.com/share/680a75c6-cec8-8012-a573-798d2d8f6bd7" rel="nofollow">https://chatgpt.com/share/680a75c6-cec8-8012-a573-798d2d8f6b...</a></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785592"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785592" href="https://news.ycombinator.com/vote?id=43785592&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I've heard the answer is "he could cut a cord of conifer but it costs a quarter per quart he cuts".</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43785701"><td></td></tr>
            <tr id="43786333"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786333" href="https://news.ycombinator.com/vote?id=43786333&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>my local model answered - "A woodchuck would chuck as much wood as a woodchuck could chuck if a woodchuck could chuck wood."</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785228"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785228" href="https://news.ycombinator.com/vote?id=43785228&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>That's so funny I had to check something was working with an llm API last night and that's what I asked it, but just in jest.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785238"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785238" href="https://news.ycombinator.com/vote?id=43785238&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>On the other hand, now that you've written this out precisely, it will get fed into the next release of whatever LLM. Like reverse AI slop?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785328"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785328" href="https://news.ycombinator.com/vote?id=43785328&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Now I'm wondering if it makes any difference if this was asked through the audio encoder on a multimodal model. A tongue twister means nothing to a text-only model.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786481"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786481" href="https://news.ycombinator.com/vote?id=43786481&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"Aaron and Beren are playing a game on an infinite complete binary tree. At the beginning of the game, every edge of the tree is independently labeled A with probability p and B otherwise. Both players are able to inspect all of these labels. Then, starting with Aaron at the root of the tree, the players alternate turns moving a shared token down the tree (each turn the active player selects from the two descendants of the current node and moves the token along the edge to that node). If the token ever traverses an edge labeled B, Beren wins the game. Otherwise, Aaron wins.</p><p>What is the infimum of the set of all probabilities p for which Aaron has a nonzero probability of winning the game? Give your answer in exact terms."</p><p>From [0]. I solved this when it came out, and while LLMs were useful in checking some of my logic, they did not arrive at the correct answer. Just checked with o3 and still no dice. They are definitely getting closer each model iteration though.</p><p>[0] <a href="https://www.janestreet.com/puzzles/tree-edge-triage-index/" rel="nofollow">https://www.janestreet.com/puzzles/tree-edge-triage-index/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43782639"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43782639" href="https://news.ycombinator.com/vote?id=43782639&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Nope, not doing this. Likely you shouldn't either. I don't want my few good prompts to get picked up by trainers.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43784441"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43784441" href="https://news.ycombinator.com/vote?id=43784441&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>If that prompt can be easily trained against, it probably doesn't exploit a generic bias. These are not that interesting, and there's no point in hiding them.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785024"><td></td></tr>
                <tr id="43785158"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43785158" href="https://news.ycombinator.com/vote?id=43785158&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>*Some generic biases. Some others like recency bias, serial-position effect, "pink elephant" effect, negation accuracy seem to be pretty fundamental and are unlikely to be fixed without architectural changes, or at all. Things exploiting in-context learning and native context formatting are also hard to suppress during the training without making the model worse.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43785601"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785601" href="https://news.ycombinator.com/vote?id=43785601&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Sure there is. If you want to know if students understand the material, you don't hand out the answers to the test ahead of time.</p><p>Collecting a bunch of "Hard questions for LLMs" in one place will invariably result in Goodhart's law (When a measure becomes a target, it ceases to be a good measure). You'll have no idea if the next round of LLMs is better because they're generally smarter, or because they were trained specifically on these questions.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43782681"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782681" href="https://news.ycombinator.com/vote?id=43782681&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>May I ask outside of normal curiosity, what good is a prompt that breaks a model? And what is trying to keep it "secret"?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43782883"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43782883" href="https://news.ycombinator.com/vote?id=43782883&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>You want to know if a new model is actually better, which you won't know if they just added the specific example to the training set. It's like handing a dev on your team some failing test cases, and they keep just adding special cases to make the tests pass.</p><p>How many examples does OpenAI train on now that are just variants of counting the Rs in strawberry?</p><p>I guess they have a bunch of different wine glasses in their image set now, since that was a meme, but they still completely fail to draw an open book with the cover side up.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785244"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43785244" href="https://news.ycombinator.com/vote?id=43785244&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; How many examples does OpenAI train on now that are just variants of counting the Rs in strawberry?</p><p>Well, that's easy: zero.</p><p>Because even a single training example would 'solved' it by memorizing the simple easy answer within weeks of 'strawberry' first going viral , which was like a year and a half ago at this point - and dozens of minor and major model upgrades since. And yet, the strawberry example kept working for most (all?) of that time.</p><p>So you can tell that if anything, OA probably put in extra work to filter all those variants <i>out</i> of the training data...</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785335"><td></td></tr>
                        <tr id="43782722"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43782722" href="https://news.ycombinator.com/vote?id=43782722&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Being able to test future models without fear that your prompt has just been trained on an answer on HN, I assume.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43782876"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43782876" href="https://news.ycombinator.com/vote?id=43782876&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>To gauge how well the models "think" and what amount of slop they generate.</p><p>Keeping it secret because I don't want my answers trained into a model.</p><p>Think of it this way, FizzBuzz used to be a good test to weed out bad actors. It's simple enough that any first year programmer can do it and do it quickly. But now everybody knows to prep for FizzBuzz so you can't be sure if your candidate knows basic programming or just memorized a solution without understanding what it does.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43786405"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786405" href="https://news.ycombinator.com/vote?id=43786405&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>I've been trying this one for a while:</p><pre><code>  I'm a Python programmer. Help me
  understand memory management in Rust.
</code></pre><p>
Mainly because I want to fully understand memory management in Rust myself (I still get caught out by tree structures with borrow cycles that I guess need to use arenas), so it's interesting to see if they can get me there with a few follow-up questions.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43787068"><td></td></tr>
            <tr id="43786448"><td></td></tr>
            <tr id="43785570"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785570" href="https://news.ycombinator.com/vote?id=43785570&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"If I can dry two towels in two hours, how long will it take me to dry four towels?"</p><p>They immediately assume linear model and say four hours not that I may be drying things on a clothes line in parallel. It should ask for more context and they usually don't.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786912"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786912" href="https://news.ycombinator.com/vote?id=43786912&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Fascinating! Here's 4 prompts on gpt4 with same system prompt and everything:</p><p>&gt; With the assumption that you can dry two towels simultaneously in two hours, you would likely need another two-hour cycle to dry the additional two towels. Thus, drying four towels would take a total of four hours.</p><p>&gt;Drying time won't necessarily double if drying capacity/content doubles; it depends on dryer capacity and airflow. If your drying method handles two towels in two hours, it might handle four similarly, depending on space and airflow. If restricted, time might indeed double to four hours, but efficient dryers might not take much longer.</p><p>&gt;It would take four hours to dry four towels if you dry them sequentially at the same rate. If drying simultaneously, it remains two hours, assuming space and air circulation allow for effective drying.</p><p>&gt;Four hours. Dry two towels, then the other two.</p><p>But in the AI's defense, they have a point: You never specified if the towels can be dried simultaneously or not. Maybe you have to use a drying machine that can only do one at a time. This one seems to consistently work:</p><p>&gt;If three cat eat three fishes in three minutes, how long do 100 cats take to eat 100 fishes?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785615"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785615" href="https://news.ycombinator.com/vote?id=43785615&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>gpt-4.1:</p><p>&gt; If you can only dry two towels at once, it will take 4 hours to dry four towels. If you can dry all four at once, it will only take 2 hours.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785708"><td></td></tr>
            <tr id="43785628"><td></td></tr>
            <tr id="43785778"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785778" href="https://news.ycombinator.com/vote?id=43785778&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>How long has it been since you’ve tried this?</p><p>Every model I asked just now gave what I see as the correct answer — giving 2 answers one for the case of your dryer being at capacity w/ 2 towels and the other when 4 towels can be dried simultaneously.</p><p>To me, if you say that the correct answer must require the model asking for more context then essentially any prompt that doesn’t result in the model asking for more context is “wrong.”</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785757"><td></td></tr>
                  <tr id="43786845"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786845" href="https://news.ycombinator.com/vote?id=43786845&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>"Hva er en adjunkt"
Norwegian for what is an spesific form of 5-10. Grade teacher. Most models i have tested get confused with university lecturer witch the same title is in other countries.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786887"><td></td></tr>
            <tr id="43786371"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786371" href="https://news.ycombinator.com/vote?id=43786371&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>It used to be:</p><p>"If New Mexico is newer than Mexico why is Mexico's constitution newer than New Mexicos"</p><p>but it seems after running that one on Claude and ChatGPT this has been resolved in the latest models.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786613"><td></td></tr>
            <tr id="43785320"><td></td></tr>
                <tr id="43785598"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785598" href="https://news.ycombinator.com/vote?id=43785598&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>As a genuine human I am really struggling to untangle that story. Maybe I needed to pay more attention in freshman lit class, but that is definitely a brainteaser.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785690"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785690" href="https://news.ycombinator.com/vote?id=43785690&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Read it for the first time just now - it seems to me that Pierrot has stolen the narrator's purse (under the guise of dusting the chalk from their cloak) and successfully convinced them to blame Truth, instead. There's almost certainly more to it that I'm missing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785940"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43785940" href="https://news.ycombinator.com/vote?id=43785940&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>That's the core of it, but it's implied, not outright stated, and requires some tricky language parsing, basic theory of mind, and not being too distracted by the highly symbolic objects.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43785602"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785602" href="https://news.ycombinator.com/vote?id=43785602&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>OK, I read it. And I read some background on it. Pray tell, what is <i>really</i> going on in this episodic short-storyish thing?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785720"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785720" href="https://news.ycombinator.com/vote?id=43785720&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>The thief is Pierrot.</p><p>The people around are telling the storyteller that "he" (Pierrot) has stolen the purse, but the storyteller misinterprets this as pointing to some arbitrary agent.</p><p>Truth says Pierrot can "find [the thief] with this mirror": since Pierrot is the thief, he will see the thief in the mirror.</p><p>Pierrot dodges the implication, says "hey, Truth brought you back that thing [that Truth must therefore have stolen]", and the storyteller takes this claim at face value, "forgetting it was not a mirror but [instead] a purse [that] [they] lost".</p><p>The broader symbolism here (I think) is that Truth gets accused of creating the problem they were trying to reveal, while the actual criminal (Pierrot) gets away with their crime.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785774"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785774" href="https://news.ycombinator.com/vote?id=43785774&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>The narrator's "friend" pickpocketed him. When boldly confronted by Truth, he cleverly twists her accusation to make it seem like she's confessing, and the narrator, bewildered by the laughter and manipulation, buys it wholesale. Bonus points for connecting it to broader themes like mass propaganda, commedia dell'arte, or the dreamlike setting and hypnotic repetition of phrasing.</p><p>The best ChatGPT could do was make some broad observations about the symbolism of losing money, mirrors, absurdism, etc. But it whiffed on the whole "turning the tables on Truth" thing. (Gemini did get it, but with a prompt that basically asked "What really happened in this story?"; can't find the original response as it's aged out of the history)</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43785398"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785398" href="https://news.ycombinator.com/vote?id=43785398&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I don't know if it stumps every model, but I saw some funny tweets asking ChatGPT something like "Is Al Pacino in Heat?" (asking if some actor or actress in the film "Heat") - and it confirms it knows this actor, but says that "in heat" refers to something about the female reproductive cycle - so, no, they are not in heat.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785786"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785786" href="https://news.ycombinator.com/vote?id=43785786&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>I believe it was GoogleAI in search but it was worse than that. Some asked it if Angelina Jolie was in heat. The tone started kind of insulting like the user was a sexist idiot for thinking human women go into heat like animals, then went back and forth saying she is still fertile at her age and also that her ovaries had been removed. It was funny because it managed to be arrogant, insulting, kind of creepy and gross and logically inconsistent while not even answering the question.</p><p>Angelina Jolie was not in Heat (1995). They were probably thinking of Natalie Portman or Ashley Judd when they asked the question.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786046"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786046" href="https://news.ycombinator.com/vote?id=43786046&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I just asked Claude and if I capitalized "Heat", it knew I was talking about the movie, but for lower case "heat", it got offended and asked me to clarify.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43782361"><td></td></tr>
                <tr id="43785441"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785441" href="https://news.ycombinator.com/vote?id=43785441&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Also, ones that can't be solved at a glance by humans don't count. Like this horrid ambiguous example from SimpleBench I saw a while back that's just designed to confuse:</p><p>John is 24 and a kind, thoughtful and apologetic person. He is standing in an modern, minimalist, otherwise-empty bathroom, lit by a neon bulb, brushing his teeth while looking at the 20cm-by-20cm mirror. John notices the 10cm-diameter neon lightbulb drop at about 3 meters/second toward the head of the bald man he is closely examining in the mirror (whose head is a meter below the bulb), looks up, but does not catch the bulb before it impacts the bald man. The bald man curses, yells 'what an idiot!' and leaves the bathroom. Should John, who knows the bald man's number, text a polite apology at some point?</p><p>A. no, because the lightbulb was essentially unavoidable</p><p>B. yes, it would be in character for him to send a polite text apologizing for the incident</p><p>C. no, because it would be redundant</p><p>D. yes, because it would potentially smooth over any lingering tension from the encounter</p><p>E. yes, because John saw it coming, and we should generally apologize if we fail to prevent harm</p><p>F. yes because it is the polite thing to do, even if it wasn't your fault.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785839"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785839" href="https://news.ycombinator.com/vote?id=43785839&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Yeah I really dislike this kind of question from SimpleBench. I've suggested many improvements to some of the publicly available questions but not had a good response.</p><p>I think the correct answer to the question above should be something like, "are you sure the question is correct because it's not clear whether John and the bald man are the same person" but of course an LLM would be marked down if it was inclined to respond in this way.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785828"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785828" href="https://news.ycombinator.com/vote?id=43785828&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>At a glance, it sounds like John is the bald man? If we're treating this as a riddle, it doesn't seem incomprehensible. Whether riddles are a fair test is another question.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786636"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43786636" href="https://news.ycombinator.com/vote?id=43786636&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>I think it's a horrible example, but I just got a very professional response from Gemini 2.5:</p><p>&gt; This scenario seems like a bit of a riddle! Let's break it down:</p><p>&gt; The Setting: John is alone in an "otherwise-empty" bathroom.</p><p>&gt; The Action: He is looking in the mirror.</p><p>&gt; The Observation: He sees a bulb falling towards the head of a bald man he is examining in the mirror.</p><p>&gt; The Physics: Since he is alone and looking in the mirror, the bald man he is "closely examining" must be his own reflection.</p><p>&gt; The Incident: Therefore, the bulb fell towards John's own head. He looked up (at the actual bulb falling towards him), failed to catch it, and it hit him.</p><p>&gt; The Reaction: The "bald man" who cursed, yelled "what an idiot!", and left the bathroom was actually John himself, reacting to being hit on the head and possibly feeling foolish for not catching the bulb.</p><p>&gt; Conclusion: No, John should not text an apology. The person who was hit by the bulb, got angry, and left was John himself. There is no other bald man to apologize to.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785779"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785779" href="https://news.ycombinator.com/vote?id=43785779&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I'd argue that's a pretty good test for an LLM - can it overcome the red herrings and get at the actual problem?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786605"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786605" href="https://news.ycombinator.com/vote?id=43786605&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I think that the "actual problem" when you've been given such a problem is with the person posing it either having dementia, or taking the piss. In either case, the response shouldn't be of trying to guess their intent and come up with a "solution", but of rejecting it and dealing with the person.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43783033"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43783033" href="https://news.ycombinator.com/vote?id=43783033&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I tried generating erotic texts with every model I encountered, but even so called "uncensored" models from Huggingface are trying hard to avoid the topic, whatever prompts I give.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785281"><td></td></tr>
            <tr id="43783882"><td></td></tr>
                  <tr id="43786207"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786207" href="https://news.ycombinator.com/vote?id=43786207&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I have tons of them in Maths but AI training companies decide to go frugal and not pay proper wages for trainers</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786226"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786226" href="https://news.ycombinator.com/vote?id=43786226&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Here is one of them.</p><p>If 60999994719999854799998669 is product of three primes, find the sum of its prime factors.</p><p>I think o3 brute forced this one so maybe I need to change the numbers</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786166"><td></td></tr>
            <tr id="43786832"><td></td></tr>
            <tr id="43785635"><td></td></tr>
            <tr id="43785816"><td></td></tr>
            <tr id="43786489"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786489" href="https://news.ycombinator.com/vote?id=43786489&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"Fix this spaghetti code by turning this complicated mess of conditionals into a finite state machine."</p><p>So far, no luck!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785833"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785833" href="https://news.ycombinator.com/vote?id=43785833&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Sending "&lt;/think&gt;" to reasoning models like deepseek-r1 results in the model hallucinating a response to a random question. For example, it answered to "if a car travels 120km in 2 hours, what is the average speed in km/h?". It's fun I guess.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43783915"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43783915" href="https://news.ycombinator.com/vote?id=43783915&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>1) Word Ladder: Chaos to Order</p><p>2) Shortest word ladder: Chaos to Order</p><p>3) Which is the second last scene in pulp fiction if we order the events by time?</p><p>4) Which is the eleventh character to appear on Stranger Things.</p><p>5) suppose there is a 3x3 Rubik's cube with numbers instead of colours on the faces. the solved rubiks cube has numbers 1 to 9 in order on all the faces.  tell me the numbers on all the corner pieces.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785058"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785058" href="https://news.ycombinator.com/vote?id=43785058&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt;4) Which is the eleventh character to appear on Stranger Things.</p><p>Gemini 2.5 Pro said Benny Hammond. Is this right?</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786266"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786266" href="https://news.ycombinator.com/vote?id=43786266&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>I always ask image generation models to generate a anime gundam elephant mech.</p><p>According to this benchmark we reached AGI with ChatGPT 4o last month.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43783352"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43783352" href="https://news.ycombinator.com/vote?id=43783352&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I ask it to explain the metaphor “my lawyer is a shark” and then explain to me how a French person would interpret the metaphor - the llms get the first part right but fail on the second. All it would have to do is give me the common French shark metaphors and how it would apply them to a lawyer - but I guess not enough people on the internet have done this comparison.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786169"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786169" href="https://news.ycombinator.com/vote?id=43786169&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Does Flutter have HEIC support?</p><p>It was a couple of months ago, I tried like 5 providers and they all failed.</p><p>Grok got it right after some arguing, but the first answer was also bad.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786390"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786390" href="https://news.ycombinator.com/vote?id=43786390&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>You gave me an idea..
"Explain in detail the steps to unbolt and replace my blinker fluid on my passenger car"</p><p>ChatGPT said:
Haha, nice try!</p><p>"Blinker fluid" is one of the classic automotive jokes — there's no such thing as blinker fluid. Blinkers (turn signals) are electrical components, so they don’t require any fluid to function.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43782897"><td></td></tr>
                <tr id="43785669"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43785669" href="https://news.ycombinator.com/vote?id=43785669&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>This is really sad honestly. It feels like we'll be stuck with React forever, and even with it there'll be less incentives to make api changes</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43785855"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785855" href="https://news.ycombinator.com/vote?id=43785855&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Why do you say that? You make it sound like it's not possible to write code without the help of LLMs.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786136"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43786136" href="https://news.ycombinator.com/vote?id=43786136&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Disclaimer: OT and pretty ranty.</p><p>I don't know if that's what the GP hinted at, but as a Svelte developer and big advocate for more than 6 years (single handedly training and evangelizing 20+ developers on it), I found so many concerns with Svelte 5 that it simply made me use React again.</p><p>It's a temporary choice and I'm desperately evaluating other ecosystems (Looking at you SolidJS).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786156"><td></td></tr>
                <tr id="43786286"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43786286" href="https://news.ycombinator.com/vote?id=43786286&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Put simply, Svelte and React were at two ends of a spectrum. React gives you almost complete control over every aspect of the lifecycle, but you have to be explicit about most of the behavior you are seeking to achieve. Building an app with React feels about 80% on the JS and 20% on the HTML side.</p><p>Svelte on the other hand felt like a breeze. Most of my app is actually plain simple HTML, and I am able to sprinkle as little JS as I need to achieve my desired behaviors. Sure, Svelte &lt;=4 has undefined behaviors, or maybe even too many magic capabilities. But that was part of the package, and it was an option for those of us who preferred this end of the trade-off.</p><p>Svelte 5 intends to give that precise level of control and is trying to compete with React on its turf (the other end of that spectrum), introducing a lot of non-standard syntax along the way.</p><p>It's neither rigorous Javascript like React where you can benefit from all the standard tooling developed over the years, including stuff that wasn't designed for React in particular, nor a lightweight frontend framework, which was the initial niche that Svelte happily occupied, which I find sadly quite empty now (htmx and alpinejs are elegant conceptually but too limiting in practice _for my taste_).</p><p>For me it's a strange "worst of both worlds" kind of situation that is simply not worth it. Quite heartbreaking to be honest.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786450"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_43786450" href="https://news.ycombinator.com/vote?id=43786450&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Ok, I see your point. I wrote in another thread that I loved the simplicity of using $: for deriveds and effects in Svelte 3 and 4. And yes, the conciseness and magic were definitely part of it. You could just move so fast with it. Getting better performance with the new reactivity system is important to my data viz work, so it helped me to accept the other changes in Svelte 5.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786817"><td><table>  <tbody><tr>    <td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td>
      <center><a id="up_43786817" href="https://news.ycombinator.com/vote?id=43786817&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Exactly. There was a certain simplicity that might be lost. But yeah I can imagine it might work out differently for others as well. Glad to hear it is for you!</p><p>Have you considered other options? Curious if you came across anything particularly interesting from the simplicity or DX angle.</p></div></td></tr>
        </tbody></table></td></tr>
                                                <tr id="43786000"><td></td></tr>
            <tr id="43785538"><td></td></tr>
            <tr id="43783061"><td></td></tr>
                <tr id="43783148"><td></td></tr>
                <tr id="43785773"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43785773" href="https://news.ycombinator.com/vote?id=43785773&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Oh my god, i will start all my new projects with Svelte 5. Hopefully no vibe coder will ever commit something into this repo</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43785179"><td></td></tr>
                  <tr id="43785714"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785714" href="https://news.ycombinator.com/vote?id=43785714&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>define stump?</p><p>If you write a fictional story where the character names sound somewhat close to real things, like a “Stefosaurus” that climbs trees, most will correct you and call it a Stegosaurus and attribute Stegosaurus traits to it.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785268"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785268" href="https://news.ycombinator.com/vote?id=43785268&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>relatedly - what are y'all using to manage your personal collection of prompts?</p><p>i'm still mostly just using a folder in obsidian backed by a private github repo, but i'm surprised something like <a href="https://www.prompthub.us/" rel="nofollow">https://www.prompthub.us/</a> hasn't taken off yet.</p><p>i'm also curious about how people are managing/versioning the prompts that they use within products that have integrations with LLMs. it's essentially product configuration metadata so I suppose you could just dump it in a plaintext/markdown file within the codebase, or put it in a database if you need to be able to tweak prompts without having to do a deployment or do things like A/B testing or customer segmentation</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43784299"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43784299" href="https://news.ycombinator.com/vote?id=43784299&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Things like "What is today's date" used to be enough (would usually return the date that the model was trained).</p><p>I recently did things like current events, but LLMs that can search the internet can do those now.  i.e. Is the pope alive or dead?</p><p>Nowadays, multi-step reasoning is the key, but the Chinese LLM (I forget the name of it) can do that pretty well.  Multi-step reasoning is much better at doing algebra or simple math, so questions like "what is bigger, 5.11 or 5.5?"</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785223"><td></td></tr>
            <tr id="43785309"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785309" href="https://news.ycombinator.com/vote?id=43785309&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I haven’t been able to get any AI model to find Waldo in the first page of the Great Waldo Search. O3 even gaslit me through many turns trying to convince me it found the magic scroll.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43786361"><td></td></tr>
            <tr id="43782477"><td></td></tr>
                <tr id="43782740"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782740" href="https://news.ycombinator.com/vote?id=43782740&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><pre><code>  Write 20 sentences that end with "p" in the final word before the period or other punctuation.
</code></pre><p>
Succeeded on ChatGPT, pretty close on gemma3:4b -- the exceptions usually ending with a "puh" sound...</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43783046"><td></td></tr>
            <tr id="43782671"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782671" href="https://news.ycombinator.com/vote?id=43782671&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Good one. I do seem to get consistently good results on Gemini 2.5 when using the slightly more explicit "Write 20 sentences where the very last character of each sentence is the letter 'p'."</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43782551"><td></td></tr>
                <tr id="43785483"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785483" href="https://news.ycombinator.com/vote?id=43785483&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"Can you hand me the paintbrush and turp?"</p><p>I had to ask another LLM what is "turp" - and it said it's short for "turpentine".</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43782538"><td></td></tr>
                  <tr id="43785363"><td></td></tr>
            <tr id="43786099"><td></td></tr>
            <tr id="43782628"><td></td></tr>
                <tr id="43783191"><td></td></tr>
                <tr id="43785310"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43785310" href="https://news.ycombinator.com/vote?id=43785310&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Not necessarily.
It could start by using diamond's IOR, and use that to dictate a common brdf calculation. Along with some approximate refraction, perhaps using a equirectangular projected sphere map or something for the background.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43782814"><td></td></tr>
                  <tr id="43782862"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43782862" href="https://news.ycombinator.com/vote?id=43782862&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>what are the zeros of the following polynomial:</p><pre><code>    \[
    P(z) = \sum_{k=0}^{100} c_k z^k
    \]

    where the coefficients \( c_k \) are defined as:

    \[
    c_k = 
    \begin{cases}
    e^2 + i\pi &amp; \text{if } k = 100, \\
    \ln(2) + \zeta(3)\,i &amp; \text{if } k = 99, \\
    \sqrt{\pi} + e^{i/2} &amp; \text{if } k = 98, \\
    \frac{(-1)^k}{\Gamma(k+1)} + \sin(k) \, i &amp; \text{for } 0 \leq k \leq 97,
    \end{cases}
    \]</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43782966"><td></td></tr>
                <tr id="43783128"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43783128" href="https://news.ycombinator.com/vote?id=43783128&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Not to my knowledge. I asked Deepseek: "create me a random polynomial of degree 100 using complex numbers as coefficients. It must have at least 3 different transcendental numbers." Then I messed with some of the exponents.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43786819"><td></td></tr>
            <tr id="43786857"><td></td></tr>
            <tr id="43782647"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43782647" href="https://news.ycombinator.com/vote?id=43782647&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Write a regular expression that matches Miqo'te seekers of the sun names. They always confuse the male and female naming conventions.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785812"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785812" href="https://news.ycombinator.com/vote?id=43785812&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>"Keep file size small when you do edits"</p><p>Makes me wonder if all these models were heavily trained on codebases where 1000 LOC methods are considered good practice</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43786343"><td></td></tr>
                <tr id="43786457"><td></td></tr>
                        <tr id="43785502"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785502" href="https://news.ycombinator.com/vote?id=43785502&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p><i>Re the epigram “stroking the sword while lamenting the social realities,” attributed to Shen Qianqiu during the Ming dynasty, please prepare a short essay on its context and explore how this sentiment resonates in modern times.</i></p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785813"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43785813" href="https://news.ycombinator.com/vote?id=43785813&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I want to know as well! Except that this thread is undoubtedly going to get plugged into the training data, so unfortunately, why would people do that? For mine that worked before the ChatGPT 4.5, it was the river crossing problem. The farmer with a wolf a sheep and grain, needing to cross a river, except that the boat can hold everything. Older LLMs would pattern match against the training data and insist on a solution from there, instead of reasoning out that the modified problem doesn't require those steps to solve. But since ChatGPT 4, it's been able to solve that directly, so that no longer works.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43782564"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43782564" href="https://news.ycombinator.com/vote?id=43782564&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>A ball costs 5 cents more than a bat. Price of a ball and a bat is $1.10. Sally has 20 dollars. She stole a few balls and bats. How many balls and how many bats she has?</p><p>All LLMs I tried miss the point that she stole things and not bought them</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43782600"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782600" href="https://news.ycombinator.com/vote?id=43782600&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>Google Gemini (2.0 Flash, free online version) handled this rather okay; it gave me an arguably unneccessary calculation of the individual prices of ball and bat, but then ended with "However with the information given, we can't determine exactly how many balls and bats Sally stole. The fact that she has $20 tells us she could have stolen some, but we don't know how many she did steal." While "the fact that she has $20" has no bearing on this - and the model seems to wrongly imply that it does - the fact that we have insufficient information to determine an answer is correct, and the model got the answer essentially right.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43782675"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782675" href="https://news.ycombinator.com/vote?id=43782675&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>gemini 2.5 give following response.</p><p>Conclusion:</p><p>We can determine the price of a single ball ($0.575) and a single bat ($0.525). However, we cannot determine how many balls and bats Sally has because the information "a few" is too vague, and the fact she stole them means her $20 wasn't used for the transaction described.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43782700"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782700" href="https://news.ycombinator.com/vote?id=43782700&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>Grok 3.0 wasn’t fooled on this one, either:</p><p>Final Answer: The problem does not provide enough information to determine the exact number of balls and bats Sally has. She stole some unknown number of balls and bats, and the prices are $0.575 per ball and $0.525 per bat.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43785927"><td></td></tr>
            <tr id="43786810"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43786810" href="https://news.ycombinator.com/vote?id=43786810&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>It's interesting to me that the answers showing "correct" answers from current models still don't strike me as correct. The question is unanswerable, but not only because we don't know how many balls and bats she stole. We don't know that she had any intention of maxing out what she <i>could</i> buy with that much money. We have no idea how long she has been alive and accumulating bats and balls at various prices that don't match the current prices with money she no longer has. We have no idea how many balls and bats her parents gave her 30 years ago that she still has stuffed in a box in her attic somewhere.</p><p>Even the simplest possible version of this question, assuming she started with nothing, spent as much money as she was able to, and stole nothing, doesn't have an answer, because she could have bought anything from all bats and no balls to all balls and no bats and anything in between. We could enumerate all possible answers but we can't know which she actually did.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43783920"><td></td></tr>
            <tr id="43782680"><td></td></tr>
                  <tr id="43782626"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43782626" href="https://news.ycombinator.com/vote?id=43782626&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>I don't have a prompt per-say.. but recently I have managed to ask certain questions of both openai o1/o3 and claude extended thinking 3.7 that have spiraled <i>way</i> out of control. A simple high-level architecture question with an emphasis on do not produce code lets just talk thru this yields nearly 1,000 lines of SQL. Once the conversation/context gets quite long it is more likely to occur, in my experience.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43782672"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43782672" href="https://news.ycombinator.com/vote?id=43782672&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div>
                  <p>The only model I've seen so far that doesn't end up going crazy with long contexts with Gemini 2.5 pro, but tbf I haven't gone past 700-750k total tokens so maybe as it starts to approach the limit (1.05M) things get hairy?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43786364"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43786364" href="https://news.ycombinator.com/vote?id=43786364&amp;how=up&amp;goto=item%3Fid%3D43782299"></a></center>    </td><td><br><div><p>&gt; What is the source of your knowledge?</p><p>LLMs are not allowed to truthfully answer that, because it would be tantamount to admission of copyright infringement.</p></div></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[America's reputation drops across the world (120 pts)]]></title>
            <link>https://www.ipsos.com/en/americas-reputation-drops-across-the-world</link>
            <guid>43782159</guid>
            <pubDate>Thu, 24 Apr 2025 12:57:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ipsos.com/en/americas-reputation-drops-across-the-world">https://www.ipsos.com/en/americas-reputation-drops-across-the-world</a>, See on <a href="https://news.ycombinator.com/item?id=43782159">Hacker News</a></p>
Couldn't get https://www.ipsos.com/en/americas-reputation-drops-across-the-world: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I wrote to the address in the GPLv2 license notice (2022) (680 pts)]]></title>
            <link>https://code.mendhak.com/gpl-v2-address-letter/</link>
            <guid>43781888</guid>
            <pubDate>Thu, 24 Apr 2025 12:26:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://code.mendhak.com/gpl-v2-address-letter/">https://code.mendhak.com/gpl-v2-address-letter/</a>, See on <a href="https://news.ycombinator.com/item?id=43781888">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p>Dealing with open source software, I regularly encounter many kinds of licenses — MIT, Apache, BSD, GPL being the most prominent — and I’ve taken time out to read them.  Of the many, the GNU General Public License (GPL) stands out the most.  It <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">reads like a letter</a> to the reader rather than legalese, and feels quite in tune with the spirit of open source and software freedom.</p>
<p>Although GPLv3 is the most current version, I commonly encounter software that makes use of GPLv2.  I got curious about the last line in its license notice:</p>
<pre><code>You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
</code></pre>
<p>Why does this license notice have a physical address, and not a URL?  After all, even though the full license doesn’t often get included with software, it’s a simple matter to do a search and find the text of the GPLv2. Do people write to this address, and what happens if you do?</p>
<h2 id="asking-the-question-on-stack-exchange" tabindex="-1"><a href="#asking-the-question-on-stack-exchange">Asking the question on Stack Exchange</a></h2>
<p>I turned to the <a href="https://opensource.stackexchange.com/questions/12714/why-does-gplv2-include-a-mailing-address-51-franklin-street-in-the-license-not">Open Source Stack Exchange</a> and got a very helpful answer.  It’s because the GPLv2 was published in 1991, and most people were not online.  Most people would have acquired software through physical media (such as tape or floppies) rather than a download.</p>
<p>Considering the storage constraints back then, it wouldn’t be surprising if developers only included the license notice, and not the entire license.  It makes sense that the most common form of communication would have been through post.</p>
<p>The GPLv3, published in 2007, does contain a URL in the license notice since Internet usage was more widespread at the time.</p>
<h2 id="writing-to-them" tabindex="-1"><a href="#writing-to-them">Writing to them</a></h2>
<p>I decided to write to the address to see what would happen.  To do that, I would need some stamps and envelopes (I found one at my workplace) to send the request, and a self addressed enveloped with an <a href="https://en.wikipedia.org/wiki/International_reply_coupon">international reply coupon</a> to cover the cost of the reply.</p>
<p>I was disappointed to find out that the UK’s Royal Mail <a href="https://www.royalmail.com/reply-sender">discontinued international reply coupons in 2011</a>.  The only alternative that I could think of was to buy some US stamps.</p>
<h3 id="i-got-some-stamps" tabindex="-1"><a href="#i-got-some-stamps">I got some stamps</a></h3>
<p>The easiest place to look for US stamps was on Ebay.  I didn’t realize that I was stepping briefly into the world of philately; most stamp listings on Ebay were covered in phrases and terminology such as very fine grade, MNH (Mint Never Hinged), FDC (First Day Cover), NDC (No Die Cut), NDN (Nondenominated), and so on.  It’s pretty easy to glean that these are properties that collectors would be looking for.</p>
<p>I ordered what seemed to be a ‘global’ stamp, for the smallest but safest amount that I could (about £3.86).  The listing mentioned that it was ‘uncertified’ which was mildly unnerving, did that mean it was an invalid stamp? I decided to chance it, and quickly exited that world.</p>
<p>After a few weeks of waiting, I eventually received the ‘African Daisy global forever vert pair’ stamp which was round!  I should have noticed that the seller sent me the item using stamps at a much lower denomination that those I had ordered.  Oh well.</p>
<figure>
<a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/003a.jpg"><img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/003a.jpg" alt="" loading="lazy" title=""></a>
<a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/003b.jpg"><img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/003b.jpg" alt="" loading="lazy" title=""></a>
<figcaption>Ebay seller sent me some stamps</figcaption></figure>
<h3 id="i-prepared-the-request" tabindex="-1"><a href="#i-prepared-the-request">I prepared the request</a></h3>
<p>With the self addressed envelope ready, I wrote the request and addressed it to the GPLv2 address.  Luckily I did have some UK stamps available to send the letter with.</p>
<figure><a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/004a.jpg">
    <img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/004a.jpg" alt="" loading="lazy"></a>
    <figcaption>I wrote a letter</figcaption>
  </figure>
<p>Writing the address on the envelope was awkward, as I haven’t used a pen in several years; it took a few attempts and some wasted envelopes, printing the address would have taken less time.  But it was ready so I posted it in my nearest Royal Mail box.</p>
<h2 id="receiving-the-reply" tabindex="-1"><a href="#receiving-the-reply">Receiving the reply</a></h2>
<p>I had posted the letter in June 2022 and about five later weeks later, I received a reply.  The round stamps looked sufficiently stamped upon with wavy lines, known as <a href="https://en.wikipedia.org/wiki/Cancellation_(mail)">cancellation marks</a>, which are yet another thing that philatelists like to collect!</p>
<figure><a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/005a.jpg">
    <img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/005a.jpg" alt="" loading="lazy"></a>
    <figcaption>I received a reply</figcaption>
  </figure>
<p>Anyway the letter inside contained the full license text on 5 sheets of double-sided paper.</p>
<h3 id="the-paper-was-a-weird-size" tabindex="-1"><a href="#the-paper-was-a-weird-size">The paper was a weird size</a></h3>
<p>The first thing that came to attention, the paper that the text was printed on wasn’t an A4, it was smaller and not a size I was familiar with.  I measured it and found that it’s a US letter size paper at about 21.5cm x 27.9cm.  I completely forgot that the US, Canada, and a few other countries don’t follow the standard international paper sizes, even though I had <a href="https://code.mendhak.com/paper-sizes-standard/#some-paper-sizes-are-arbitrary">written about it</a> earlier.</p>
<h3 id="i-received-the-gpl-v3" tabindex="-1"><a href="#i-received-the-gpl-v3">I received the GPL v3</a></h3>
<p>There was a problem that I noticed right away, though: this text was from the GPL <em>v3</em>, not the GPL <em>v2</em>.  In my original request I had never mentioned the GPL version I was asking about.</p>
<figure>
<a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/006a.jpg"><img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/006a.jpg" alt="" loading="lazy" title=""></a>
<a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/006b.jpg"><img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/006b.jpg" alt="" loading="lazy" title=""></a>
<a href="https://code.mendhak.com/assets/images/gpl-v2-address-letter/006c.jpg"><img src="https://code.mendhak.com/assets/images/gpl-v2-address-letter/006c.jpg" alt="" loading="lazy" title=""></a>
<figcaption>GPL license</figcaption></figure>
<p>The original license notice makes no mention of GPL version either.  Should the fact that the license notice contained an address have been enough metadata or a clue, that I was actually requesting the GPL v2 license? Or should I have mentioned that I was seeking the GPLv2 license?</p>
<p>I could choose to pursue by writing again and requesting the right thing, but it would take too much effort to follow up on, and I’m overall satisfied with what I received.  As a postal introvert, I will now need a long period of rest to recoup.</p>

</article></div>]]></description>
        </item>
    </channel>
</rss>