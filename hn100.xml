<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 05 Mar 2024 13:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I spend £8,500 a year to live on a train (154 pts)]]></title>
            <link>https://metro.co.uk/2024/03/03/spend-8-500-a-year-live-a-train-20388001/</link>
            <guid>39601538</guid>
            <pubDate>Tue, 05 Mar 2024 10:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://metro.co.uk/2024/03/03/spend-8-500-a-year-live-a-train-20388001/">https://metro.co.uk/2024/03/03/spend-8-500-a-year-live-a-train-20388001/</a>, See on <a href="https://news.ycombinator.com/item?id=39601538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<figure><div>

<p><img loading="lazy" decoding="sync" width="644" height="338" data-rsz="shrink" src="https://metro.co.uk/wp-content/uploads/2024/03/SEI_194337327-bc53.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=644%2C338" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard." fetchpriority="high">
</p></div>
<figcaption>‘I have a lot of freedom and can decide every day where I want to go’ (Picture: Getty Images / Facebook)</figcaption></figure><p><em>Uch</em>. TRAINS. They’re a necessary evil in many of our lives. Horrible big tin cans full of smelly people that never turn up on time and make you late for everything. The less time spent on them the better. At least for most of us in the UK, anyway.</p>
<p>Not so for digital nomad Lasse Stolley. This <a href="https://metro.co.uk/tag/germany/?ico=auto_link_lifestyle_P2_LNK1" data-track="inline-tag-auto-link_article">German</a> teenager can’t get enough of them. He’s not a trainspotter, though. He’s more of a train<em>squatter</em>.</p>
<p>Okay, ‘squatter’ isn’t really accurate. While the 17-year-old does indeed live on trains, he does so entirely legally. And with a surprising amount of comfort.</p>
<p>Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.</p>
<p>The self-employed coder technically has no fixed abode and appears to really enjoy his unusual way of life, something which he chronicles regularly on his blog, <a href="https://leben-im-zug.de/">Life on the Train</a>.</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="360" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/GettyImages-1948505416-3b2c.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C360" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>Lasse travels a whopping 600 miles each and every day (Picture: Getty Images)</figcaption></figure><h2>Embarking on an unusual journey</h2>
<p>‘I’ve been living on the train as a digital nomad for a year and a half now,’ Lasse told <a href="https://www.businessinsider.de/leben/bahncard100-17-jaehriger-lebt-seit-2022-in-zuegen-der-bahn/">Business Insider</a> recently. ‘At night I sleep on the moving Intercity Express (ICE) train and during the day I sit in a seat, at a table and work as a programmer, surrounded by many other commuters and passengers. I travel from one end of the country to the other. I’m exploring the whole of Germany.’</p>
<p>‘I decided to live on a train when I was 16 years old. My school days were behind me and the whole world was open to me. So in the summer of 2022, I decided to give in to my wanderlust, leave my parents’ house in Schleswig-Holstein behind and embark on a huge adventure.’</p>
<p>‘If I feel like travelling to the sea, I take the train north in the morning. If I long for the hustle and bustle of the big city, then I look for a connection to Berlin or Munich. Or I take the express train to the Alps for a hiking trip.’</p>
<p>‘I use the app to organise the next connection in the evening and sleep while I race along the tracks towards my destination. I don’t have a place to retreat to. My home <em>is</em> the train.’</p>
<p>‘The early months were tough and I had to learn a lot about how it all worked. Everything was different than how I’d imagined.’</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="675" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/362276901_142677792192526_8628661540473484597_n-32dd.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C675" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>The 17 year-old calls himself a ‘digital nomad’ and really takes it quite literally (Picture: Facebook)</figcaption></figure><h2>Costs, overnights and The Parent Question</h2>
<p>Lasse says that, all things considered, it costs him around €10,000 (£8,500) a year to live the way he does.</p>
<p>‘I have a lot of freedom and can decide every day where I want to go, whether it’s to the Alps, to a big city or to the sea. I’m completely flexible.’</p>
<p>He’s forced to keep on the ball, though. You know how it is with trains. Even the unsurprisingly much more efficient German rail system. ‘Every night I have to make sure that I catch the night train and sometimes I have to reschedule very quickly because it suddenly doesn’t arrive.’</p>
<p>What do Lasse’s mum and dad think of his decision? ‘I had to do a lot of convincing,’ he says. Once he’d done that convincing, his parents checked out the legal side of it and agreed. They helped him sell off the majority of his possessions and now fully back their son’s decision.</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="360" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/GettyImages-1833760864-c460.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C360" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>The unlimited pass means that Lasse has now seen every inch of his homeland (Picture: Getty Images)</figcaption></figure><h2>Keeping luggage to a minimum</h2>
<p>Luggage is, obviously, something of an issue. Lasse has to travel light.</p>
<p>‘The most important thing is my laptop and my noise-cancelling headphones, which at least give me a little privacy on the train.’</p>
<p>‘An important aspect of minimalism on the train is the reduction of material possessions,’ Lasse says. ‘Since the available space is very limited, you have to choose carefully what you really need. It means getting rid of unnecessary items and limiting yourself to the bare essentials.’</p>
<p>‘The challenge of not accumulating more and more things is a central component of minimalist living. Especially with a backpack, you quickly reach a space limit.’</p>
<figure><div>

<p><img loading="lazy" decoding="async" width="540" height="525" data-rsz="shrink" src="https://metro.co.uk/wp-content/themes/metro-parent/img/fallback.png" data-src="https://metro.co.uk/wp-content/uploads/2024/03/354036290_111526388641000_2911877444056073821_n-f1ad-e1709471979145.jpg?quality=90&amp;strip=all&amp;zoom=1&amp;resize=540%2C525" alt="Lasse travels 600 miles a day throughout Germany aboard Deutsche Bahn trains. He travels first class, sleeps on night trains, has breakfast in DB lounges and takes showers in public swimming pools and leisure centres, all using his unlimited annual railcard.">
</p></div>
<figcaption>Lasse’s parents took some convincing (Picture: Lasse Stolley’s Facebook)</figcaption></figure><h2>Reflecting on an hectic 18 months</h2>
<p>‘This life means a pretty restless existence. To switch off, I just look out the window and watching the scenery. That calms me down a lot. Then I just let my thoughts wander.’</p>
<p>‘My favourite route leads through the Middle Rhine Valley between Mainz and Bonn. Here the trains always travel very slowly along the river. It’s a beautifully picturesque route that stretches at the foot of the vineyards. The view outside is wonderful.’</p>
<p>‘I’ve travelled a total of over 500,000 kilometres (310,000 miles) since I started living on the train. I don’t know how much longer I want to travel through Germany and wake up somewhere different every day, though.’</p>
<p>‘My Bahncard 100 is still valid for six months. I haven’t seen enough yet.’</p>
<p>
	<span data-track-module="mor-link_article">
	MORE : <a href="https://metro.co.uk/2024/03/02/benidorm-travel-warning-uk-tourists-face-1-000-fines-new-ban-20384776/?ico=more_text_links">Benidorm travel warning as UK tourists face £1,000 fines over new beach rules</a>
	</span>
	</p>
<p>
	<span data-track-module="mor-link_article">
	MORE : <a href="https://metro.co.uk/2024/03/01/mum-of-two-dies-suddenly-falling-sick-flight-20382396/?ico=more_text_links">Mum-of-two dies after suddenly falling sick on flight</a>
	</span>
	</p>
<p>
	<span data-track-module="mor-link_article">
	MORE : <a href="https://metro.co.uk/2024/03/01/tube-journeys-will-cheaper-certain-times-next-week-20377984/?ico=more_text_links">Tube journeys to be cheaper at certain times from next week</a>
	</span>
	</p>						
					
					<!-- Article below content widget area -->
					

					<div data-track-module="email-signup-shortcode_travel"><p><h3>Get need-to-know travel news, inspiration and advice from Metro every week.</h3></p><h4>Sign up here...</h4></div><p><span>This site is protected by reCAPTCHA and the Google <a href="https://policies.google.com/privacy">Privacy Policy</a> and <a href="https://policies.google.com/terms">Terms of Service</a> apply.</span>
				</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radicle: Open-Source, Peer-to-Peer, GitHub Alternative (243 pts)]]></title>
            <link>https://app.radicle.xyz/nodes/seed.radicle.garden/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5</link>
            <guid>39600810</guid>
            <pubDate>Tue, 05 Mar 2024 08:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.radicle.xyz/nodes/seed.radicle.garden/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5">https://app.radicle.xyz/nodes/seed.radicle.garden/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5</a>, See on <a href="https://news.ycombinator.com/item?id=39600810">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Linux Crosses 4% Market Share Worldwide (155 pts)]]></title>
            <link>https://linuxiac.com/linux-crosses-four-percent-market-share-worldwide/</link>
            <guid>39600172</guid>
            <pubDate>Tue, 05 Mar 2024 06:49:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxiac.com/linux-crosses-four-percent-market-share-worldwide/">https://linuxiac.com/linux-crosses-four-percent-market-share-worldwide/</a>, See on <a href="https://news.ycombinator.com/item?id=39600172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Linux has surpassed a 4% share in the desktop operating system market as of the end of February 2024. According to the <a href="https://gs.statcounter.com/os-market-share/desktop/worldwide" target="_blank" rel="noreferrer noopener">latest data from StatCounter</a>, a leading web traffic analysis tool, Linux’s market share has reached 4.03%.</p>



<p>At first glance, the number might seem modest, but it represents a significant leap. Let’s break it down. <a href="https://linuxiac.com/linux-hits-3-percent-market-share/">It took Linux 30 years to secure a 3% share of desktop operating systems</a>, a milestone reached last June.</p>



<p>Impressively, the open-source operating system has surged by an additional 1% in the last eight months.</p>



<figure><a href="https://linuxiac.b-cdn.net/wp-content/uploads/2024/03/linux-marketshare-february-2024.jpg"><img decoding="async" width="1024" height="802" src="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-1024x802.jpg" data-spai-egr="1" alt="Linux desktop market share, February 2024" srcset="https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-1024x802.jpg 1024w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-380x297.jpg 380w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024-768x601.jpg 768w, https://cdn.shortpixel.ai/spai/q_lossy+ret_img+to_auto/linuxiac.com/wp-content/uploads/2024/03/linux-marketshare-february-2024.jpg 1183w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Linux desktop market share, February 2024</figcaption></figure>



<p>Now, we’re all curious about the journey Linux is on and where it’ll end up by the year’s end. Will we be celebrating a milestone of surpassing 5% market share? It’s a goal many of us who champion open source are eagerly hoping to achieve.</p>



<p>The rise in Linux’s popularity can be attributed to several factors. Firstly, the open-source nature of Linux has made it a favored choice among developers, IT professionals, and tech enthusiasts who appreciate the flexibility and control it offers.</p>



<p>Additionally, the security and stability of Linux have been key selling points, making it an attractive option for both personal and professional use.</p>



<p>However, while having great features is important, an attractive presentation often captures attention first, something both Windows and macOS understand well. This is precisely where the top Linux desktop distros have made remarkable strides, significantly enhancing their appearance and user-friendliness in recent years.</p>



<p>With the continuous improvement and user-friendly designs of distributions such as <a href="https://linuxiac.com/ubuntu/">Ubuntu</a>, Fedora, <a href="https://linuxiac.com/linux-mint/">Mint</a>, and many others, Linux has become more accessible to a broader audience, including those who may not be as technically inclined.</p>



<p>Is the much-anticipated “Linux on the Desktop” year upon us? Well, not exactly. The truth is, seeing Linux dominate desktops any time soon is quite unlikely, but then again, achieving widespread desktop dominance was never the primary aim of Linux. It’s more of an ongoing, lighthearted debate among enthusiasts than a serious expectation.</p>



<p>However, it’s worth noting and celebrating that Linux’s desktop usage has surpassed 4% and even saw a growth of 1% in just the last eight months – a feat that was beyond the expectations of many. So, let’s take a moment to appreciate this achievement. It may seem small to some, but it’s a significant stride forward for those who hold Linux dear.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stable Diffusion 3: Research Paper (137 pts)]]></title>
            <link>https://stability.ai/news/stable-diffusion-3-research-paper</link>
            <guid>39599958</guid>
            <pubDate>Tue, 05 Mar 2024 06:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/stable-diffusion-3-research-paper">https://stability.ai/news/stable-diffusion-3-research-paper</a>, See on <a href="https://news.ycombinator.com/item?id=39599958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="62f2452bc121595f4d87c71c">
  
  
    
    


  


<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="bright-inverse" data-section-id="62f2452bc121595f4d87c71e" data-controller="SectionWrapperController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;sectionTheme&quot;: &quot;bright-inverse&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-current-context="{
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0,
&quot;videoSourceProvider&quot;: &quot;none&quot;
},
&quot;backgroundImageId&quot;: null,
&quot;backgroundMediaEffect&quot;: null,
&quot;divider&quot;: null,
&quot;typeName&quot;: &quot;blog-side-by-side&quot;
}" data-animation="none">
  <article id="article-">
  
    
    
    
    <div data-layout-label="Post Body" data-type="item" id="item-65e63d2437be6e755f2ef692"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-4eb8968a0fbd2de4a226">
  <h2>Key Takeaways:</h2><ul data-rte-list="default"><li><p>Today, we’re publishing our <span data-text-attribute-id="aea29532-822b-4f7e-a716-d7f164d6abb5"><a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf">research paper</a></span> that dives into the underlying technology powering Stable Diffusion 3.</p></li><li><p>Stable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL·E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations.&nbsp;</p></li><li><p>Our new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities&nbsp;compared to previous versions of SD3.</p></li></ul>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_3258">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png" data-image-dimensions="1920x1235" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png" width="1920" height="1235" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/50a6aa56-9b9c-4dec-9efc-8a159c7c8422/Blog+SD3+Research+Paper.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_2549">
  <p>Following our announcement of the <a href="https://stability.ai/news/stable-diffusion-3" target=""><span>early preview of Stable Diffusion 3</span></a>, today we are publishing the <a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf" target=""><span>research paper</span></a> which outlines the technical details of our upcoming model release. The paper will be accessible on arXiv soon, and we invite you to sign up for <a href="https://stability.ai/stablediffusion3"><span>the waitlist</span> </a>to participate in the early preview.&nbsp;</p><h2>Performance</h2>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_15593">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg" data-image-dimensions="640x480" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg" width="640" height="480" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/19acc961-3e42-413f-b495-450803aba582/baseline_comp.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>With SD3 as a baseline, this chart outlines the areas it wins against competing models based on human evaluations of Visual Aesthetics, Prompt Following, and Typography. </em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_15946">
  <p>We have compared output images from Stable Diffusion 3 with various other open models including <a href="https://stability.ai/news/stable-diffusion-sdxl-1-announcement"><span>SDXL</span></a>, <a href="https://stability.ai/news/stability-ai-sdxl-turbo"><span>SDXL Turbo</span></a>, <a href="https://stability.ai/news/introducing-stable-cascade"><span>Stable Cascade</span></a>, Playground v2.5 and Pixart-α as well as closed-source systems such as DALL·E 3, Midjourney v6 and Ideogram v1 to evaluate performance based on human feedback. During these tests, human evaluators were provided with example outputs from each model and asked to select the best results based on how closely the model outputs follow the context of the prompt it was given (“prompt following”), how well text was rendered based on the prompt (“typography”) and, which image is of higher aesthetic quality (“visual aesthetics”).&nbsp;</p><p>From the results of our testing, we have found that Stable Diffusion 3 is equal to or outperforms current state-of-the-art text-to-image generation systems in all of the above areas.&nbsp;</p><p>In early, unoptimized inference tests on consumer hardware our largest SD3 model with 8B parameters fits into the 24GB VRAM of a RTX 4090 and takes 34 seconds to generate an image of resolution 1024x1024 when using 50 sampling steps. Additionally, there will be multiple variations of Stable Diffusion 3 during the initial release, ranging from 800m to 8B parameter models to further eliminate hardware barriers.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_4412">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png" data-image-dimensions="1920x1310" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png" width="1920" height="1310" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/12973f20-3043-4184-b4f7-a0f182022fb6/Blog+SD3+GPUs+go+brrrrrrr.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_5284">
  <h2>Architecture Details&nbsp;</h2><p>For text-to-image generation, our model has to take both modalities, text and images, into account. This is why we call this new architecture MMDiT, a reference to its ability to process multiple modalities. As in previous versions of Stable Diffusion, we use pretrained models to derive suitable text and image representations. Specifically, we use three different text embedders - two CLIP models and T5 - to encode text representations, and an improved autoencoding model to encode image tokens.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_5773">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png" data-image-dimensions="596x520" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png" width="596" height="520" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/2b5df5af-5b84-40d5-8fc0-c8618c22488c/simplemmdit2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Conceptual visualization of a block of our modified multimodal diffusion transformer: MMDiT.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_6570">

<p>The SD3 architecture builds upon the <a href="https://arxiv.org/abs/2212.09748"><span>Diffusion Transformer (“DiT”, Peebles &amp; Xie, 2023)</span></a>. Since text and image embeddings are conceptually quite different, we use two separate sets of weights for the two modalities. As shown in the above figure, this is equivalent to having two independent transformers for each modality, but joining the sequences of the two modalities for the attention operation, such that both representations can work in their own space yet take the other one into account.&nbsp;</p>




















  
  



</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_7981">

<p>By using this approach, information is allowed to flow between image and text tokens to improve overall comprehension and typography within the outputs generated. This architecture is also easily extendable to multiple modalities such as video, as we discuss in our <a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf" target=""><span>paper</span>.</a></p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_8435">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png" data-image-dimensions="1920x659" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png" width="1920" height="659" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/96d35c17-a76d-4f0d-a4ba-454955dbd87f/Blog+SD3+-+Wizard+and+Frog.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_9295">

<p>Thanks to Stable Diffusion 3’s improved prompt following, our model has the ability to create images that focus on various different subjects and qualities while also remaining highly flexible with the style of the image itself.</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_9722">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png" data-image-dimensions="1920x1592" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png" width="1920" height="1592" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/10401b22-e408-4ce6-9883-893569ebaa65/Blog+SD3.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_10076">
  <h2>Improving Rectified Flows by Reweighting</h2><p>Stable Diffusion 3 employs a Rectified Flow (RF) formulation (<a href="https://arxiv.org/abs/2209.03003"><span>Liu et al., 2022</span></a>; <a href="https://arxiv.org/abs/2209.15571"><span>Albergo &amp; Vanden-Eijnden,2022</span></a>; <a href="https://arxiv.org/abs/2210.02747"><span>Lipman et al., 2023</span></a>), where data and noise are connected on a linear trajectory during training. This results in straighter inference paths, which then allow sampling with fewer steps. Furthermore, we introduce a novel trajectory sampling schedule into the training process. This schedule gives more weight to the middle parts of the trajectory, as we hypothesize that these parts result in more challenging prediction tasks. We test our approach against 60 other diffusion trajectories such as <a href="https://arxiv.org/abs/2112.10752"><span>LDM</span></a>, <a href="https://arxiv.org/abs/2206.00364"><span>EDM</span></a> and <a href="https://arxiv.org/abs/2105.05233"><span>ADM</span></a>, using multiple datasets, metrics, and sampler settings for comparison. The results indicate that while previous RF formulations show improved performance in few step sampling regimes, their relative performance declines with more steps. In contrast, our re-weighted RF variant consistently improves performance.</p><h2>Scaling Rectified Flow Transformer Models</h2>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_11203">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png" data-image-dimensions="1600x1139" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png" width="1600" height="1139" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/b62062fe-e5b0-4d89-8316-ca938e31d55c/Scalming_Rectified_Flow_Transformer_Models.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_12066">
  <p>We conduct a scaling study for text-to-image synthesis with our reweighted Rectified Flow formulation and MMDiT backbone. We train models ranging from 15 blocks with 450M parameters to 38 blocks with 8B parameters and observe a smooth decrease in the validation loss as a function of both model size and training steps (top row). To test whether this translates into meaningful improvements of the model outputs, we also evaluate automatic image-alignment metrics (<a href="https://arxiv.org/abs/2310.11513"><span>GenEval</span></a>) as well as human preference scores (ELO) (bottom row). Our results demonstrate a strong correlation between these metrics and the validation loss, indicating that the latter is a strong predictor of overall model performance. Furthermore, the scaling trend shows no signs of saturation, which makes us optimistic that we can continue to improve the performance of our models in the future.</p><h2>Flexible Text Encoders</h2><p>By removing the memory-intensive 4.7B parameter T5 text encoder for inference, SD3’s memory requirements can be significantly decreased with only small performance loss. Removing this text encoder does not affect visual aesthetics (win rate w/o T5: 50%) and results only in slightly reduced text adherence (win rate 46%) as seen in the above image under the “Performance” section. However, we recommend including T5 for using SD3’s full power in generating written text, since we observe larger performance drops in typography generation without it (win rate 38%) as seen in the examples below:</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1709587747088_13449">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png" data-image-dimensions="1596x1268" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png" width="1596" height="1268" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/05ce9c95-78ff-47f7-8fdc-c7772c0996af/text-encoders.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Removing T5 for inference only results in significant performance drops when rendering very complex prompts involving many details or large amounts of written text. The above figure shows three random samples per example.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709587747088_13802">
  <p>To learn more about MMDiT, Rectified Flows, and the research behind Stable Diffusion 3, read our full research paper <a href="https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf" target=""><span>here</span></a>.</p><p>To stay updated on our progress follow us on<a href="https://twitter.com/stabilityai"> <span>Twitter</span></a>,<a href="https://www.instagram.com/stability.ai/"> <span>Instagram</span></a>,<a href="https://www.linkedin.com/company/stability-ai"> <span>LinkedIn</span></a>, and join our<a href="https://discord.gg/stablediffusion"> <span>Discord Community</span></a>.</p>
</div></div>
  
</article>

</div>

  
</article>


          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sell for half a billion and get nothing (2021) (241 pts)]]></title>
            <link>https://www.fundablestartups.com/blog/half-a-billion</link>
            <guid>39598903</guid>
            <pubDate>Tue, 05 Mar 2024 02:47:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fundablestartups.com/blog/half-a-billion">https://www.fundablestartups.com/blog/half-a-billion</a>, See on <a href="https://news.ycombinator.com/item?id=39598903">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>In July 2018, Paddy Power Betfair (now known as Flutter) acquired FanDuel for $465M in cash. On the surface, this looks like a great win for the FanDuel founders and employees. However, because the two lead investors held strong liquidation preference rights, <strong>the FanDuel founders and most employees received nothing</strong> in this massive deal.</p>

<h3>What is Liquidation Preference?</h3>
<p>Liquidation preference is one of the most important terms in a term sheet. Liquidation preference determines who gets paid first and how much they get paid when there’s an acquisition.</p>
<p>Because they take on significant risks, investors expect to get “VIP” head-of-line privileges to be paid upon a liquidation event such as an acquisition. Employees wait in line and collect proceeds only after all of the preferred investors take their share.</p>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/3gJxOCS6J821JWbd67gE_liquidation_preference_investor_priority_tiny.png"></p>

<h3>What Are Liquidation Preference Terms?</h3>
<p>There are two components to liquidation preference. The first is the preference multiple, which basically says the investor gets a certain multiple of their investment amount. If the investor invested $5M and got a 2x preference, they would get paid $10M before the common shareholders got paid anything. For founders, obviously a 1x multiple is better than a 3x multiple.</p>
<p>The second component of liquidation preference is participation, which determines whether the investor can take additional proceeds after the preference multiple is paid out. Capped or full participation rights would allow the investor to “double dip” in their payout.</p>
<p>The simplified 3x3 matrix below provides a simplified view of how liquidation preference works. Healthy startups can get terms more in the lower left corner. Marginal startups may find funding, but liquidation preference terms will likely trend towards the upper right corner.</p>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/1FyTFXR2Sj6aBk9f8Q5v_liquidation_preference_3x3_matrix_tiny.png"></p>
<h3><br>How Liquidation Preference Hurt FanDuel Founders</h3>
<p>When the FanDuel founders raised funds, two key investors received a liquidation preference that entitled them to the first $559M in an acquisition. Founders and employees would be paid only if the acquisition exceeded $559M. Because the Paddy Power Betfair was for just $465M, the founders received nothing.</p>
<p>To help founders visualize how liquidation preference could affect their startup, we have a liquidation preference scenario calculator available in our <a title="Free Training and Tools for Startup Founders" href="https://www.fundablestartups.com/resources">free members area</a>.</p>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/ZFIz7jxRnKpQ0Eijz1tG_liquidation_preference_model_tiny.jpg">&nbsp;</p>
<h3>Why Founders Couldn’t Stop the Deal</h3>
<p>But wait. If this was such a horrible deal for the founders, why did they do the deal? The reality was the founders couldn’t stop the deal because they also granted the same two lead investors drag along rights. This drag along right forced the other shareholders to accept the decisions made by these two investors. Imagine how the founders felt when they received the notice below that the drag along right was being exercised and <strong>they could do nothing to stop getting short-changed</strong>.</p>
<blockquote>
<p><img src="https://kajabi-storefronts-production.global.ssl.fastly.net/kajabi-storefronts-production/blogs/25302/images/M6v7cIqTbGD5rkronJ6C_FanDuel_drag_along_notice_tiny.png"></p>
</blockquote>

<h3>Lessons Learned: Build a Very Fundable Startup</h3>
<p>Every founder should learn from this disastrous scenario the importance of building a very healthy, fundable startup. A healthy, vibrant startup draws more investors during fundraising. The competition gives founders the leverage to negotiate for more founder-friendly terms. Healthy startups get better valuations, better terms, and raise funds with much less effort.</p>
<p>Building a healthy startup requires great execution. Great execution involves doing dozens of tasks and processes the right way. Learn how to execute well with our <a title="Premium Training and Tools for Startup Founders" href="https://www.fundablestartups.com/training">premium startup training</a>. If you are new to our startup training, we recommend starting by watching the 7 Keys to Triple Your Startup Payout, located in our <a href="https://www.fundablestartups.com/resource_redirect/offers/yFLUCuzb">Basic Tier</a>.</p>
<p>For more detail on liquidation preferences, refer to chapter 11 in our <a title="21 Secrets of Successful Startups" href="https://www.fundablestartups.com/resources#section-1703194520014">book</a>. Good luck building your own healthy, fundable startup!</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Screen time robs average toddler of hearing 1,000 words spoken by adult a day (101 pts)]]></title>
            <link>https://www.theguardian.com/australia-news/2024/mar/04/does-children-toddlers-kids-watching-tv-impact-development-learning</link>
            <guid>39598776</guid>
            <pubDate>Tue, 05 Mar 2024 02:30:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/australia-news/2024/mar/04/does-children-toddlers-kids-watching-tv-impact-development-learning">https://www.theguardian.com/australia-news/2024/mar/04/does-children-toddlers-kids-watching-tv-impact-development-learning</a>, See on <a href="https://news.ycombinator.com/item?id=39598776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The average toddler is missing out on hearing more than 1,000 words spoken by an adult each day due to screen time, setting back their language skills, a first-of-its kind study has found.</p><p>The research, <a href="https://jamanetwork.com/journals/jamapediatrics/fullarticle/10.1001/jamapediatrics.2023.6790?guestAccessKey=af1b82f5-2ff4-4cc9-a88c-2720ef541470&amp;utm_source=For_The_Media&amp;utm_medium=referral&amp;utm_campaign=ftm_links&amp;utm_content=tfl&amp;utm_term=030424" data-link-name="in body link">published on Tuesday</a> in the Journal of the American Medical Association (Jama) Pediatrics, tracked 220 Australian families over two years to measure the relationship between family screen use and children’s language environment.</p><p>Families recorded all the audio around their child using advanced speech recognition technology over a 16-hour period on an average day at home. They repeated this process every six months between the ages of 12 and 36 months.</p><figure id="6e513fda-4428-4ad3-be44-6e21418fd35e" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;My kid had too much screen time – so I tested out these alternatives&quot;,&quot;elementId&quot;:&quot;6e513fda-4428-4ad3-be44-6e21418fd35e&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/lifeandstyle/2024/jan/19/smartphone-alternatives-kids-toys&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The lead researcher, Dr Mary Brushe from the Telethon Kids Institute, said: “The technology we use is essentially like a Fitbit, but instead of counting the number of steps, this device counts the number of words spoken by, to and around the child.”</p><p>The device also picked up electronic noise, which the researchers analysed to calculate screen time.</p><p>The researchers found young children’s exposure to screens including TVs and phones was interfering with their language opportunities, with the association most pronounced at three years of age.</p><p>For every extra minute of screen time, the three-year-olds in the study were hearing seven fewer words, speaking five fewer words themselves and engaging in one less conversation.</p><ul>
 <li><p><strong><a href="https://www.theguardian.com/email-newsletters?CMP=copyembed" data-link-name="in body link">Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup</a></strong></p></li>
</ul><p>The study found the average three-year-old in the study was exposed to two hours and 52 minutes of screen time a day. Researchers estimated this led to those children being exposed to 1,139 fewer adult words, 843 fewer child words and 194 fewer conversations.</p><p>Because the study couldn’t capture parents’ silent phone use, including reading emails, texting or quietly scrolling through websites or social media, Brushe said they might have underestimated how much screen usage is affecting children.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-11">skip past newsletter promotion</a><p id="EmailSignup-skip-link-11" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><figure id="9c4e1ade-b6d4-4517-9383-bf5abbca3c3f" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:12,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Kids are on their phones more than ever. We asked parents what they’re doing about it&quot;,&quot;elementId&quot;:&quot;9c4e1ade-b6d4-4517-9383-bf5abbca3c3f&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/lifeandstyle/2024/feb/01/screen-time-phones-kids-limit&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>A language-rich home environment was critical in supporting infants and toddlers’ language development, Brushe said. While some educational children’s shows were designed to help children’s language skills, very young kids in the age group of the study could struggle to translate television shows into their own life, she said.</p><p>This study did not differentiate between whether children were watching high- or low-quality screen content.</p><p>Previous research in the area had relied on parents self-reporting their own and their child’s screen time, and only studied short periods of time.</p><p>“To our knowledge, no studies conducted since the rapid uptake of mobile phones and tablets have actually tracked children’s screen time and their early language experiences over an extended period of time,” Brushe said.</p><p>Prof Angela Morgan, the leader of the speech and language group at the Murdoch Children’s Research Institute, which was not involved in the study, said: “To my knowledge it’s the most robust examination of looking at screen time and interactions between parents and children that we’ve had available.</p><p>“For all children, the biggest opportunities for language learning are of course in those first few years of life … we know that early predictors do predict your later language outcomes, so it is really important that they’ve been looking at this question in the early years.”</p><figure id="826fe14b-8c71-4076-a366-80553c782dad" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:19,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;As parents, can we all agree that a bit of screen time for children is actually a good thing? | Rhiannon Lucy Cosslett&quot;,&quot;elementId&quot;:&quot;826fe14b-8c71-4076-a366-80553c782dad&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/commentisfree/2023/mar/15/parents-screen-time-children-tv-cbeebies&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>Amber Flohm, the vice-president of the NSW Teachers Federation, said members who taught in early education and primary school had said how children were affected significantly by the increased amount of time spent on screen.</p><p>Flohm said teachers had noted language skills going backwards, both in conversation between children themselves and teachers and in reading and writing skills. The pandemic exacerbated the situation, but teachers had noted the trends around the increased used of screen time “at least the last five or six years pre-Covid”, she said.</p><p>The research in the study was carried out between 2018 and 2021, with some families undertaking their 30- or 36-month recording day early in the pandemic. However, researchers said participants’ average screen times did not appear to have increased substantially compared with those who completed their recordings prior to the pandemic.</p><p>Due to the advanced speech recognition technology only being able to code for English, only English-speaking households were part of the study.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European crash tester says carmakers must bring back physical controls (685 pts)]]></title>
            <link>https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/</link>
            <guid>39598189</guid>
            <pubDate>Tue, 05 Mar 2024 01:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/">https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/</a>, See on <a href="https://news.ycombinator.com/item?id=39598189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      do that here, too    —
</h4>
            
            <h2 itemprop="description">In 2026, Euro NCAP points will be deducted if some controls aren't physical.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/03/GettyImages-1491147785-800x533.jpg" alt="man pushing red triangle warning car button">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/03/GettyImages-1491147785-scaled.jpg" data-height="1707" data-width="2560">Enlarge</a> <span>/</span> A car's hazard warning lights will need a physical control to get a five-star EuroNCAP score in 2026.</p></figcaption>  </figure>

  




<!-- cache hit 104:single/related:09847d295e1e25abd2a94d9739516e83 --><!-- empty -->
<p>Some progress in the automotive industry is laudable. Cars are safer than ever and more efficient, too. But there are other changes we'd happily leave by the side of the road. That glossy "piano black" trim that's been overused the last few years, for starters. And the industry's overreliance on touchscreens for functions that used to be discrete controls. Well, the automotive safety organization European New Car Assessment Programme (Euro NCAP) feels the same way about that last one, and it says the controls ought to change in 2026.</p>
<p>"The overuse of touchscreens is an industry-wide problem, with almost every vehicle-maker moving key controls onto central touchscreens, obliging drivers to take their eyes off the road and raising the risk of distraction crashes," said Matthew Avery, Euro NCAP's director of strategic development.</p>                                            
                                                        
<p>"New Euro NCAP tests due in 2026 will encourage manufacturers to use separate, physical controls for basic functions in an intuitive manner, limiting eyes-off-road time and therefore promoting safer driving," he said.</p>
<p>Now, Euro NCAP is not insisting on everything being its own button or switch. But the organization wants to see physical controls for turn signals, hazard lights, windshield wipers, the horn, and any SOS features like the European Union's <a href="https://europa.eu/youreurope/citizens/travel/security-and-emergencies/emergency-assistance-vehicles-ecall/index_en.htm">eCall feature</a>.</p>
<p>Tesla is probably at greatest risk here, having recently ditched physical stalks that instead move the turn signal functions to haptic buttons on the steering wheel. (Ferrari also has its <a href="https://arstechnica.com/cars/2022/12/the-2023-ferrari-296-gts-we-drive-ferraris-plug-in-hybrid-convertible/">turn signals on the steering wheel</a>, but Ferrari does not appear in Euro NCAP's database so probably doesn't care.)</p>
<p>Euro NCAP is not a government regulator, so it has no power to mandate carmakers use physical controls for those functions. But a five-star safety score from Euro NCAP is a strong selling point, similar to the Insurance Institute for Highway Safety's coveted Top Safety Pick program here in the US, and it's likely this pressure will be effective. Perhaps someone should start bugging IIHS to do the same.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Based: Simple linear attention language models (131 pts)]]></title>
            <link>https://www.together.ai/blog/based</link>
            <guid>39597847</guid>
            <pubDate>Tue, 05 Mar 2024 00:15:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.together.ai/blog/based">https://www.together.ai/blog/based</a>, See on <a href="https://news.ycombinator.com/item?id=39597847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Introducing Based, a simple efficient architecture that combines two familiar primitives – sliding window attention and linear attention – to offer high-quality language modeling with strong associative recall capabilities! At inference time, Based decodes without a KV-cache, enabling a 24x throughput improvement over Transformers with Flash-Attention 2!</p><h2><strong>Overview</strong></h2><p>In an <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">ICLR paper</a> (and <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">blogpost</a>) we posted towards the end of last year, we share the finding that many efficient architectures (e.g. <a href="https://arxiv.org/abs/2312.00752">Mamba</a>, <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a>, <a href="https://arxiv.org/abs/2302.10866">Hyena</a>, <a href="https://arxiv.org/abs/2307.08621">RetNet</a>) underperform Transformers on recall, the ability to ground generations on information seen in-context, which is critical for in-context learning and copying. We used this analysis to design a new Based architecture (previewed in this <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based">blogpost</a>). We’re excited to share the latest progress in this line of work.&nbsp;</p><p>Our recent work digs deeper into the recall challenge. We begin by illustrating a fundamental tradeoff between a model’s recall abilities and the size of its recurrent state during generation. This analysis informs the design of Based, a simple recurrent architecture that outperforms prior sub-quadratic models on real-world recall-intensive tasks (information extraction, reading comprehension) and in-context learning (few-shot natural language understanding on SuperGLUE). At the same time, Based offers fast&nbsp; generation speeds: Based is 56% and 44% faster at processing prompts than FlashAttention-2 and Mamba respectively (4k sequence length, 1.3Bn parameters). Based also offers 24x higher throughput than FlashAttention-2 in next token prediction (generating 1024 tokens, 128 batch size, 1.3Bn parameters).&nbsp;</p><p>We’re particularly excited about the <em>simplicity</em> of Based. Using just two well-known, familiar, attention-like building blocks, sliding window attention (with <em>tiny</em> window sizes) and linear attention (with Taylor series approximation of exp(QK^T)), we can outperform the strongest sub-quadratic architectures on language modeling and achieve massive speedups over optimized Transformers!&nbsp;</p><p>This blogpost provides an overview of our 1) analysis on recall in sub-quadratic architectures that leads to the Based architecture’s design and 2) how we make Based go brrrr!&nbsp;</p><h2><strong>Motivating analysis: the recall-memory tradeoff</strong></h2><p><strong>‍</strong>The main question driving our exploration is: <em>can we drastically improve the real-world speed and memory consumption of language models without compromising on recall and in-context learning capability?</em>&nbsp;</p><p>To begin answering this question, we had to first think about what slows architectures down. Efficient architectures (<em>e.g.</em> Mamba) are much faster than Transformers at inference time (<em>e.g. </em>5x higher throughput) in large part because they have a reduced memory footprint. Smaller memory footprint means larger batch sizes and less I/O. However, it also makes intuitive sense that reducing memory footprint too much could hurt a model’s capacity to recall information seen earlier in the sequence. This looked to us like a classic “<em>no free lunch”</em> situation, so we took a number of popular architectures, varied the hyper-parameters that affected the memory footprint, and evaluated performance on a challenging synthetic associative recall task.</p><p><em>The recall-memory tradeoff. </em><strong>&nbsp;</strong>We found that all architectures obeyed a fundamental tradeoff: the less memory the model consumed during inference, the worse it did on associative recall. We focused on the <em>recurrent state size, </em>the number of bytes used to represent previously seen tokens when generating tokens one-by-one (<em>i.e.</em> recurrently).&nbsp;</p><p>In attention, the <em>state </em>is commonly referred to as the KV-cache, and it grows with the length of the sequence. In the top right of Figure 1, we can see that attention performs recall perfectly, albeit at the cost of a huge recurrent state. Sliding window attention provides a way to cap the size of the KV-cache, but we found (unsurprisingly) that recall performance drops off rapidly as we reduce the size of the recurrent state (<em>e.g.</em> from 100% with 1MB recurrent state to 50% with a 65 KB recurrent state) (Figure 1, light blue).&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557e1f75f13323664294a_blogpost-01.png" loading="lazy" alt=""></p></figure><p>Excitingly, we found that Mamba expands the pareto frontier of the recall-memory tradeoff curve beyond sliding window attention. This means it is making <strong>better use of limited recurrent state size</strong> than approaches like sliding window attention.&nbsp;</p><p>The natural question is: <em>are there other, perhaps simpler, models that can also expand the pareto frontier?</em></p><h2>‍<strong>Based<em>: </em>a simple model at the pareto frontier</strong></h2><p><strong>‍</strong>To answer this question, we started studying why the simplest alternatives to softmax attention fail to strike a favorable tradeoff. As a further design principle, we searched for primitives that could scale well on current and future hardware. For instance, it would be nice if our primitives could leverage GPU Tensor Cores, specialized hardware on modern GPUs that can perform matrix multiplications (GEMMs) 16x faster for 16x16 matrices than the default (CUDA cores)!</p><p>In our <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">ICLR paper</a>, we did a deep dive on why any model with a convolutional view (<em>e.g. </em>H3 or Hyena) will struggle on recall. Next, we considered two of the simplest efficient attention techniques out there: (1) <a href="https://arxiv.org/abs/2004.05150">sliding</a> <a href="https://arxiv.org/abs/2007.14062">window</a> <a href="https://mistral.ai/news/announcing-mistral-7b/">attention</a> and (2) <a href="https://arxiv.org/abs/2006.16236">linear attention</a> (<em>i.e.</em> attention without softmax).</p><p>Our experiments on real-world language modeling (up to 1.4bn parameters) and synthetic associative recall suggested to us that neither primitive alone would suffice to navigate the pareto frontier.</p><ol role="list"><li>We found that pure linear attention models struggled to perform precise local token shifts and token comparisons, skills important in recall (Fu et al., 2023; Arora et al., 2023a), as well as dense attention. Expanding on our findings, we do find that our pure linear attention model improves over earlier sub-quadratic architectures. Focusing on the recall-intensive slice of the Pile test set (i.e. next token predictions that force the model to use the prior context vs. memorized knowledge), the 355M pure linear attention model outperforms RWKV-v5 by 0.1 ppl and H3 by 2.6 ppl (Table 1, paper). Pure linear attention is even comparable to the Mamba architecture on this recall-slice – 2.21 ppl for Mamba vs. 2.29 for pure linear attention! However, we observe a sizeable gap to Transformers, which achieve 1.87 ppl on the recall slice.&nbsp;</li><li>In sliding window attention, models can only recall tokens within the sliding window (Figure 2, center). As we increase the window size, the recurrent state grows linearly and has a non-linear effect on speed during parallel training and inference (Figure 2, left).</li></ol><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e556927f9f5016be3204c8_figure-01.png" loading="lazy" alt=""></p></figure><p>However, we find the two primitives are complementary – linear attention for modeling long-range token interactions and sliding window for local token interactions in the sequence. We combined them into a single architecture, called Based (Figure 2, right).&nbsp;</p><ol role="list"><li>Sliding window attention can perform the precise <em>local </em>shifts needed for associative recall. We use <em>tiny </em>window sizes (e.g. 64 in experiments) contrasting the larger window sizes in architectures like <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> and the recently proposed <a href="https://arxiv.org/abs/2402.19427">Griffin</a>. Intuitively more attention (larger window sizes) is nice from a quality perspective, but we’d like to balance quality and wall-clock speed. To balance these objectives, let’s take a look at the left plot in the above figure. Observe that the latency of matrix multiplication for 16x16 vs. 64x64 matrices are roughly equal, and beyond 64, latency grows non-linearly with the window size.&nbsp; Note that the rough similarity between 16x16 and 64x64 is because the latter keeps the GPU tensor core occupancy high enough to saturate!</li><li>Linear attention enables <em>global </em>token interactions, while maintaining a fixed size recurrent state. Unlike softmax attention, the size of linear attention’s recurrent state is a function of hyperparameters (<em>e.g.</em> choice of feature map) and not sequence length. This allows us to traverse the tradeoff space smoothly. We use a <strong>Taylor approximation of the exponential function as the feature map</strong>, that was first used in <a href="https://arxiv.org/abs/2402.04347">our prior work</a> on linear attentions!</li></ol><p>Critically, the recurrent state size in Based does not grow with the sequence length, as it does in attention. Instead, it is determined by the linear attention feature dimension and the window size. <strong>By dialing these hyperparameters, we can tradeoff recall for throughput and navigate the pareto frontier in Figure 1.&nbsp;&nbsp;</strong></p><p>‍</p><p>Despite its simplicity, on real language modeling experiments (up to at least 1.3 billion parameters), Based is competitive with Mamba in terms of overall Pile perplexity and standard zero-shot benchmarks from the <a href="https://github.com/EleutherAI/lm-evaluation-harness">LM eval harness</a> (shown under Question Answering - Common).&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5522e1c637fb50e3155b9_oIYg-xYDMbP9eAfBzRAVAfObwm4UrEQo_3fvq2ToPW8kGV722Si8b_h6oYgY6oPB-52hlPHe_gF5od4BQMrG440RNL7Dfwk_V1iSNOgafKsPQQaPRFaOnV4r5ix0zhLMh3hnI5lL4yi8wIJgRiDAN2A.png" alt=""></p></figure><p>These commonly-used zero-shot benchmarks are limited to extremely short text, so they don’t stress test models’ recall capabilities. To address this shortcoming, we <a href="https://arxiv.org/abs/2304.09433">curated a small suite of <em>real world recall-intensive</em> benchmarks</a> that require recalling information from long documents (<em>e.g. </em>information extraction from <a href="https://pubmed.ncbi.nlm.nih.gov/21321283/https://pubmed.ncbi.nlm.nih.gov/21321283/">FDA documents</a> and <a href="https://paperswithcode.com/dataset/swde">raw HTML</a>, and reading comprehension).<em> </em>Based is the strongest sub-quadratic architecture on these tasks, outperforming Mamba by 6.22 accuracy points on average. However, both Based and Mamba still underperform the strongest Transformer baseline, sometimes by large margins. This is consistent with our “no free lunch” observation above.&nbsp;</p><p>It’s important to note that we don’t believe Based is the only architecture that can operate at this point on the tradeoff curve. For example, we show in our paper that we can replace the sliding window attention with short-convolutions (filter size 3) and achieve similar performance within 0.1 perplexity points. We suspect that there are lots of other architectures that can also match this pareto frontier and we’re hopeful there are even others that can even expand beyond it!&nbsp;</p><h2>‍<strong>How we use our fixed-size recurrent state matters too!&nbsp;</strong></h2><p>There are many recurrent architectures that might have the same hidden state size, but our work highlights how the featurization (e.g. linear attention feature map, state update mechanism) matters as well. Our choice for the map in Based is surprisingly simple (<em>high-school calculus is all you need)</em>: approximating the exponential with a Taylor series. We compute $\phi$ such that $\phi(q) \phi(k)^T \approx \exp (q k^T)$. We use just the second-order Taylor series as in our prior work, where $\hat{\exp}(x) = 1 + x + x^2 / 2$! Note that if $x$ has dimension $d’$ then the&nbsp; $x^2$ term will have dimension $d’^2$! The result of the key-value outer product (step 1 above) grows quickly in $d’$, expanding the state size for Based.&nbsp;</p><p><em>How much does our choice of featurization vs. the expanded state size matter when leading to the quality of Based?</em><strong><em> </em></strong>The model’s ability to <em>use the state effectively</em> is key. Shown in the accuracy vs. recurrent state size tradeoff curves, several alternatives to the Taylor map fall <em>below</em> the pareto frontier. Below we compare to models that expand the state size using learned projections and then apply popular feature maps (<a href="https://arxiv.org/abs/2009.14794">Performer</a>, <a href="https://arxiv.org/abs/2202.08791">CosFormer</a>, <a href="https://proceedings.mlr.press/v119/katharopoulos20a.html">PosELU</a>) from the literature. We train these models on the <a href="https://github.com/HazyResearch/zoology">MQAR synthetic</a> test for associative recall and sweep hyperparameters (learning rate) for all points shown in the plot below, finding that the Taylor map is most effective. This trend carries to real world experiments on the Pile language modeling corpus (see our paper for more).</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5fa9f5c6f4e90a0584448_Horizontal%20Figure.png" loading="lazy" alt=""></p></figure><h2>IO and dataflow-aware implementation</h2><p>The next key question is how to make Based competitive in wall clock efficiency. Linear attention is theoretically more efficient than standard attention as a function of sequence length. However, existing implementations of linear attention methods are often <em>slower</em> than well-optimized attention implementations like <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>.&nbsp;</p><p>In Based, we use the 2nd degree Taylor approximation, which expands the dimension of the keys, leading to large state sizes and large memory consumption O(Nd’<sup>2</sup>d), in sequence length N, key dimension d’, and value dimension d (discussed above). The large resulting key-value state makes naïve implementations of Taylor linear attention quite slow.&nbsp;</p><p>First let’s revisit a bit of context on how the hardware works. GPUs have small amounts of fast-to-access memory (thread-specific registers, shared memory at the warp/32-threads level using SRAM) and large amounts of slow-to-access memory (HBM). It is important to reduce the number of reads-and-writes between slow HBM and SRAM as well as SRAM and registers to unlock efficiency. We present new IO-aware algorithms for the Taylor linear attention forward pass and inference that reduce the HBM to SRAM data movement by $O(Nd'^2)$ bytes and the SRAM to register data movement by $O(Nd^{2}d')$ bytes. Our algorithm allows holding the KV state <em>in-thread-register</em> at feature dimension d’ = 16, which we use in experiments.&nbsp;</p><p>Below we include a comparison between the naive Taylor attention forward pass, an implementation that leverages the popular linear attention kernels from <a href="https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention.py">Fast Transformers</a>, and our custom kernels are shown below across batch size (sequence length 1024).&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e5ffb985fe463472153665_Screenshot%202024-03-04%20at%209.06.47%E2%80%AFAM.png" loading="lazy" alt=""></p></figure><p>We then compare the end-to-end generation speeds of FlashAttention-2, Mamba, and Based 360M and 1.3Bn parameter models using our IO-aware algorithms. We hold the batch size to 2 for prefill, and generate 1024 tokens for next token prediction. Strikingly, Based achieves up to 24x higher throughput than FlashAttention-2!&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557d27b21a0953533a862_Figure%20from%20DM.png" loading="lazy" alt=""></p></figure><p><em>Stay tuned! </em>These algorithms are implemented in an exciting new CUDA DSL called ThunderKittens, that’s being developed by our lab. Stay tuned for more on this soon – we hope the DSL improves the accessibility of CUDA development! In contrast to frameworks like Triton, which makes opinionated decisions about the supported scope of operations the user can perform, our DSL is <em>embedded</em> in C++. We’re really excited to share it and get your feedback! We’re cooking up more model artifacts alongside in the coming weeks, motivated by the question: <em>What models does the hardware want?&nbsp;</em></p><p>‍</p><p>You can play with our checkpoints and evaluations on <a href="https://huggingface.co/collections/hazyresearch/based-65d77fb7a6f9c813c8b94339c">Hugging Face</a> and in this code repository: <strong><em>&nbsp;</em></strong><a href="https://github.com/HazyResearch/based">https://github.com/HazyResearch/based</a>! Huge thank you to<a href="https://www.together.ai/"> Together AI</a>, <a href="https://hai.stanford.edu/">Stanford HAI</a>, and <a href="https://crfm.stanford.edu/">Stanford CRFM</a> for supporting this work! Please send your feedback and questions to: Simran Arora (<a href="mailto:simarora@stanford.edu">simarora@stanford.edu</a>), Sabri Eyuboglu (<a href="mailto:eyuboglu@stanford.edu">eyuboglu@stanford.edu</a>), Michael Zhang (<a href="mailto:mzhang@stanford.edu">mzhang@stanford.edu</a>).&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miles Davis and the recording of Kind of Blue (200 pts)]]></title>
            <link>https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/</link>
            <guid>39597525</guid>
            <pubDate>Mon, 04 Mar 2024 23:22:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/">https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/</a>, See on <a href="https://news.ycombinator.com/item?id=39597525">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Every product was carefully curated by an Esquire editor. We may earn a commission from these links.</span></p><div data-journey-body="standard-article"><p data-journey-content="true" data-node-id="0"><em>Jazz was at the apex of its artistic power and commercial popularity when, in 1959, some of the music's greatest innovators gathered to record in New York City. In this excerpt from the new book </em><em></em>3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool<em> (Penguin Press, March 5, 2024), author James Kaplan puts us in the room as Davis and his collaborators record "So What," the track that leads off what is often hailed as the greatest jazz album ever</em><em>. </em></p><hr data-node-id="1"><p data-journey-content="true" data-node-id="2">March 2, 1959—a late-winter Monday in the second-to-last year of the Eisenhower administration. Fair and mild in Manhattan. Among the top stories in <em>The New York</em> <em>Times</em> that morning: a fogbound collision between the American Export liner <em>Constitution </em>and an oil tanker; the “commuter crisis” caused by ever-rising automobile use in the metropolitan area; tensions between white colonials and Black natives in East Africa. This last article quotes a British banker alleging “the vast unreadiness of the great majority of Africans for self-government.”</p><p data-journey-content="true" data-node-id="3">An older, staider world. On the first page of the second section, a story by the young reporter Gay Talese about “Crazy Couple Clubs”—groups of jaded suburbanites seeking unusual amusements in the city: visits to yoga clubs, night court, Bowery restaurants. And deeper into the section, on what was still called the Theatres page, a review (glowing) by the paper’s jazz critic, John S. Wilson, of a Thelonious Monk concert at Town Hall. “He has carried apparent uncertainty to a high and refined art,” Wilson wrote. “He makes each performance a fresh and provocative experience.”</p><p data-journey-content="true" data-node-id="4">If the Crazy Couple Club of Manhasset—by the evidence of the photograph in the <em>Times</em> piece a prosperous and cheerfully self-satisfied group—had dared to extend their Bowery slumming beyond ethnic restaurants, they might have wandered into the cozy, smoky, messy confines of the Five Spot Café, at 5 Cooper Square, whose owners, two Italian American brothers and ex-GIs named Joe and Iggy Termini, had for the past three years been booking some of the greatest jazz musicians of the day, including Cecil Taylor, Cannonball Adderley, and, most notably, Thelonious Monk, whom the Terminis had helped regain his New York City cabaret card—a conditional ID issued by the police department as a (legally and practically questionable) method of discouraging narcotics use—six years after Monk had lost his card, and with it the right to play in clubs that served alcohol, in a mistaken 1951 drug bust.</p><div size="medium" data-embed="body-image" data-lazy-id="P0-7" data-node-id="5"><p><img alt="new york circa 1959 jazz trumpeter miles davis records at 30th street studios in circa 1959 in new york city, new york photo by vernon l smith photo by michael ochs archivesgetty images" title="Recording At 30th Street Studios" loading="lazy" width="2400" height="3479" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/vernon-l-smith-gettyimages-74259964-65d8bbfd0e9f4.jpeg?resize=980:*"></p><div><figcaption><span>Michael Ochs Archives</span><span>//</span><span>Getty Images</span></figcaption><p>Miles Davis at 30th Street Studio in New York City circa 1959. </p></div></div> <p data-journey-content="true" data-node-id="7">Amid the tobacco and reefer fumes and beer reek of that tiny, dark saloon (a glass of gin cost fifty cents; a pitcher of beer, a dollar), the members of the Crazy Couple Club of Manhasset might have found themselves sitting shoulder to shoulder with (though they almost certainly would have failed to recognize) such Five Spot habitués as the painters Willem de Kooning, Joan Mitchell, and Mark Rothko; the writers Jack Kerouac, Allen Ginsberg, and Frank O’Hara; and the young jazz titans Miles Davis, John Coltrane, and Bill Evans.</p><p data-journey-content="true" data-node-id="8">The Five Spot was closed on Mondays, but on that March Monday Davis, Coltrane, and Evans had other business anyway: in Columbia Records’ 30th Street Studio, they were joining the alto saxophonist Cannonball Adderley, bassist Paul Chambers, and drummer Jimmy Cobb to begin making, under Miles’s leadership, what would become the bestselling, and arguably most beloved, jazz album of all time, Miles’s <em>Kind of Blue</em>. March 2 and April 22: three tunes recorded on the first date (“So What,” “Freddie Freeloader,” and “Blue in Green”), two on the second (“All Blues” and “Flamenco Sketches”). Every complete take but one (“Flamenco Sketches”) was a first take, the process similar, as Evans later wrote in the LP’s liner notes, to a genre of Japanese visual art in which black watercolor is applied spontaneously to a thin stretched parchment, with no unnatural or interrupted strokes possible, Miles’s cherished ideal of spontaneity achieved.</p><p data-journey-content="true" data-node-id="9">The quiet and enigmatic majesty of the resulting record both epitomizes jazz and transcends the genre. The album’s powerful and enduring mystique has made it widely beloved among musicians and music lovers of every category: jazz, rock, classical, rap. This is the story of the three geniuses who joined forces to create one of the great classics in Western music—how they rose up in the world, came together like a chance collision of particles in deep space, produced a brilliant flash of light, and then went on their separate ways to jazz immortality.</p><hr data-node-id="10"><div size="medium" data-embed="body-image" data-lazy-id="P0-8" data-node-id="11"><p><img alt="american jazz pianist bill evans 1929 1980 at the piano during a performance filmed for the bbc television music series 'jazz 625' at bbc television theatre in shepherd's bush, london on 19th march 1965 photo by david redfernredferns" title="Bill Evans Trio On BBC Jazz 625 TV Series" loading="lazy" width="2400" height="2396" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/david-redferns-gettyimages-84893751-65d8bceb30b5d.jpeg?resize=980:*"></p><div><figcaption><span>David Redfern</span><span>//</span><span>Getty Images</span></figcaption><p>Bill Evans at the piano during a performance filmed for the BBC in London in March 1965.</p></div></div><p data-journey-content="true" data-node-id="12">No musician ever goes into a record date expecting to make history; every man in Miles’s band had recorded dozens of times before. “Professionals,” Bill Evans said, “have to go in at 10 o’clock on a Wednesday and make a record and hope to catch a really good day.” On the face of it, there was nothing remarkable about Project B 43079. </p><p data-journey-content="true" data-node-id="13">The control booth at 30th Street was up a flight of stairs from the studio floor, in what had once been the balcony of the old church: producer Irving Townsend, recording engineer Fred Plaut, and Plaut’s assistant Bob Waller looked down from above as Miles talked to the musicians, who were placed around the open floor much as they’d stand onstage in a concert. On some recording sessions, Columbia producers used rolling baffles to isolate musicians or singers and eliminate sound leakage; at Davis’s direction, this session would proceed baffle-free, all musicians constantly aware of, and inspired by, each other’s playing. Sound leakage from one player’s mike to another’s was not only expected but essential. Each man had his own Telefunken U-49 microphone, except for Cobb, who had two, one pointed at the snare and one overhead to pick up the cymbals. </p><p data-journey-content="true" data-node-id="14">The state of studio recording in 1959 was such that the musicians rather than the engineer were responsible for regulating the loudness or softness of their instruments, by dynamics or distance from the mike. As Davis picked up his horn, Waller started the tapes rolling—one master and one safety—on the Ampex reel-to-reel recorders, and Townsend pushed the intercom button. </p><p data-journey-content="true" data-node-id="15"> “Miles, where are you gonna work now?” he asked. The producer was referring to Davis’s position in relation to the microphone, from which he had apparently stepped back momentarily.</p><p data-journey-content="true" data-node-id="16">“Right here,” Miles said.</p><p data-journey-content="true" data-node-id="17">“When I play it I’m gonna raise my horn a little bit,” Miles said. His customary playing stance, onstage or in the recording studio, was to point his trumpet straight at the floor as he played, a position that communicated contemplation and moodiness, though it was primarily a way of regulating his tone. “Can I move this down a little bit?” He indicated the mike.</p><p data-journey-content="true" data-node-id="18">“It’s against policy to move a microphone,” Townsend said, deadpan. The old church echoed with laughter.</p><hr data-node-id="19"><p data-journey-content="true" data-node-id="20">Outside the 30th Street Studio, Manhattan was Manhattaning: rounded buses and big yellow cabs grinding up and down the avenues; car horns and scraps of radio music and pedestrians’ voices echoing in the deep-shadowed side streets. Outside, the everyday clamor and clash of a city afternoon in late-winter 1959; inside, the densest quiet as a passage outside of time proceeded: the recording of CO 62291, the number that would come to be titled “So What,” leading off the album soon to be known as <em>Kind of Blue</em>.</p><p data-journey-content="true" data-node-id="21">The first take began. There was a false start of four seconds, followed by an incomplete take of forty-nine seconds. Townsend interrupted from the booth: something was interfering with the song’s profound hush. “Hold it,” the producer said. “Sorry—listen, we gotta watch it because, ah, there’s noises all the way through this. This is so quiet to begin with, and every click—watch the snare too, we’re picking up some of the vibrations on it—”</p><p data-journey-content="true" data-node-id="22">Miles, ever on the lookout for meaningful accidentals, demurred. “Well, that goes with it,” he said. “<em>All </em>that goes with it.”</p><p data-journey-content="true" data-node-id="23">“All right,” Townsend allowed. “Not all the other noises, though . . .”</p><p data-journey-content="true" data-node-id="24">Another false start, seventeen seconds. An incomplete take, a minute eleven. A telephone rang in the control booth. Once quiet was restored, three more false starts, of sixteen, seven, and fifteen seconds.</p><p data-journey-content="true" data-node-id="25">Then, history.</p><section data-embed="pullquote" data-lazy-id="P0-9" data-node-id="26"><blockquote><blockquote>The full Take 3 was nine minutes and thirty-five seconds of musical transcendence.</blockquote></blockquote></section><p data-journey-content="true" data-node-id="27">Someone—some say it was Gil Evans; Bill Evans’s biographer Peter Pettinger and the trumpeter Wallace Roney asserted it was Bill Evans—had sketched out a single-line introduction to the piece, a hushed dialogue between piano and bass, proceeding at its own dreamy pace and built on meditative, European art song–esque chords (the fourth, with two white piano keys between the lower and upper notes, being an interval which, enigmatically, presents as neither major nor minor), then on skipping single notes played in unison by the two instruments. Paul Chambers then set the rhythm, plucking the eight-note figure that was to become immortal, the call that began the rhythmic call-and-response of “So What.” Evans then answered, followed by the rest of the sextet.</p><p data-journey-content="true" data-node-id="28">“The piano’s (and then the band’s) answering ‘amen’ (or ‘so what’) riffs,” Pettinger writes, “were built up largely in fourths, as opposed to the thirds that are basic to the tonal system, as exemplified by Bobby Timmons’s comparable composition ‘Moanin’,’ recorded by him some four months earlier.”</p><p data-journey-content="true" data-node-id="29">Timmons, the pianist for Art Blakey’s Jazz Messengers, was all of twenty-two when he wrote “Moanin’,” a call-and-response number which, unlike Miles’s, had a strong, foursquare gospel feeling. Call-and-response was an ancient form, with roots in African ritual, civics, and music; it traveled to America and underlay African American work songs and religious rituals from 1619 on. Timmons’s song, like Blakey’s quintet and Horace Silver’s compositions and bands, was hugely influential in pointing jazz in a more soulful direction. Miles would have known the tune well—would he have enjoyed its old-fashioned wholeheartedness? been impatient with it? It didn’t matter: He was proceeding on his own musical path, channeling strong emotions through the prisms and filters of his biting intelligence and contrary spirit. Some people called this Cool; under the surface it was anything but.</p><div size="medium" data-ad-exclude="(min-width: 90rem)" data-embed="embed-product" data-node-id="30"><p><h2>3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool</h2></p><a target="_blank" rel="nofollow noopener" href="https://www.amazon.com/dp/0525561005" aria-label="$35 at Amazon for 3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool" data-href="https://www.amazon.com/dp/0525561005" data-product-url="https://www.amazon.com/dp/0525561005" data-affiliate="true" data-affiliate-url="https://www.amazon.com/dp/0525561005" data-affiliate-network="{&quot;id&quot;:&quot;6d6f68a9-010c-4169-97b0-14dd8a217bdb&quot;,&quot;site_id&quot;:&quot;8b6b0f67-bcde-4d9d-86c6-dedff1abae12&quot;,&quot;is_active&quot;:true,&quot;details&quot;:null,&quot;metadata&quot;:{},&quot;network&quot;:{&quot;id&quot;:&quot;469ce69f-4798-416d-9432-eaa9954b4053&quot;,&quot;name&quot;:&quot;Amazon&quot;,&quot;is_active&quot;:true,&quot;business_unit_id&quot;:&quot;ad046b46-538b-42cb-aa54-c3d158875ed6&quot;,&quot;details&quot;:&quot;&quot;,&quot;metadata&quot;:{},&quot;created_at&quot;:&quot;2021-07-28T16:03:03.241365+00:00&quot;,&quot;last_updated_at&quot;:&quot;2021-07-28T16:03:03.241381+00:00&quot;}}" data-vars-ga-call-to-action="$35 at Amazon" data-vars-ga-media-role="" data-vars-ga-media-type="Single Product Embed" data-vars-ga-outbound-link="https://www.amazon.com/dp/0525561005" data-vars-ga-product-id="e3aca9e6-8276-4b48-b1ed-ea713fd7e0e8" data-vars-ga-product-price="$35.00" data-vars-ga-product-retailer-id="ea986cbc-1ff3-48eb-b7a9-52cb72928f2c" data-vars-ga-link-treatment="(not set) | (not set)" data-vars-ga-sku="0525561005" data-vars-ga-magento-tracking="1"><div><p><img srcset="https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1706194460-713WQ70oeyL.jpg?crop=1xw:1xh;center,top&amp;resize=640:* 640w, https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1706194460-713WQ70oeyL.jpg?crop=1xw:1xh;center,top&amp;resize=980:* 980w" alt="3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool" title="3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool" src="https://hips.hearstapps.com/vader-prod.s3.amazonaws.com/1706194460-713WQ70oeyL.jpg?crop=1xw:1xh;center,top&amp;resize=980:*" width="1684" height="2560" decoding="async" loading="lazy"></p></div></a><div size="medium"><p><h2>3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool</h2></p></div></div><p data-journey-content="true" data-node-id="31">The full Take 3 was nine minutes and thirty-five seconds of musical transcendence. Miles’s solo, an impromptu composition in itself, would gain its own immortality: generations of musicians would memorize it note for note. Miles is talking to you in that solo, playing in the middle sonic range of the human voice, and he’s got all kinds of things to say, in brief and at length. He starts and stops; he starts again and goes on. And we’re freshly astonished at how very much he can express, in so few notes, <em>in the moment</em>. </p><p data-journey-content="true" data-node-id="32">The richness each of the soloists was able to create improvising over just two chords, D and E♭ Dorian, vindicates Miles’s modal concept. Coltrane was in exploratory rather than loud and fast form, traveling up and down each scale to find astringent delights. Cannonball was no less seeking, but lush toned as always, and unable not to find melodies and tuneful fillips, even in this minimalist frame. And Evans’s solo was perhaps most in sync with the tune’s hushed simplicity: playing quiet arpeggios and complex chords a little shyly at first, but then growing more assertive—and surprising: “I’m thinking of the end of Bill’s solo on ‘So What,’” Herbie Hancock told the writer Ashley Kahn. “He plays these phrases, a second apart. He plays seconds.” Still filled with wonderment forty years after the fact, Hancock was talking about an interval on the piano that’s barely an interval—two adjacent keys played simultaneously. By itself, the sound is dissonant; in this context it’s startlingly expressive. “I had never heard anybody do that before,” Hancock said. “He’s following the modal concept maybe more than anybody else. That just opened up a whole vista for me.”</p><p data-journey-content="true" data-node-id="33">CO 62291 wasn’t yet officially named on the day it was recorded, but in the years after <em>Kind of Blue</em>’s release, more than one person would take credit for its title: John Szwed writes that it “may have been suggested to [Davis] by Beverly Bentley [a girlfriend of Miles’s, later Norman Mailer’s fourth wife], who said it sounded like his favorite dismissive remark, but folks in East St. Louis were more likely to believe that it came from Miles’s brother-in-law’s retort when Miles told him in 1944 that he was leaving for New York: ‘So what?’” And the actor Dennis Hopper, who said he was a close friend of Davis’s, recalled that the phrase was a comeback he, Hopper, used to deploy, jab-like, when Miles ran his mouth while the two of them sparred together: “Oh come on, Miles, so what?”</p><p data-journey-content="true" data-node-id="34">“So one time I came into the [jazz] club,” Hopper remembered, “and he said, ‘I wrote a little song for you’—and he played ‘So What.’”</p><hr data-node-id="35"><div size="medium" data-embed="body-image" data-lazy-id="P0-10" data-node-id="36"><p><img alt="jazz great john coltrane 1926 1967 playing his tenor saxophone during an impulse records recording session in new york, early 1960s photo by robert abbott sengstackegetty images" title="John Coltrane" loading="lazy" width="2400" height="3439" decoding="async" data-nimg="1" sizes="100vw" srcset="https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=640:* 640w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=768:* 980w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:* 1120w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:* 1200w, https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:* 1920w" src="https://hips.hearstapps.com/hmg-prod/images/robert-abbott-sengstacke-gettyimages-107695082-65d8bf243729d.jpeg?resize=980:*"></p><div><figcaption><span>Robert Abbott Sengstacke</span><span>//</span><span>Getty Images</span></figcaption><p>John Coltrane playing tenor saxophone during a recording session in New York in the early 1960s.</p></div></div><p data-journey-content="true" data-node-id="37">The word “timeless” has become a cliché, a selling tool for luxury goods. And yet <em>Kind of Blue</em> is a timeless album, and “So What” arguably its signature number. What is this about? For sixty years and more, jazz and popular music had consisted of songs that told stories, either explicitly—in lyrics—or in their construction. The most common song framework in both genres was known as AABA: two choruses followed by a bridge (aka channel, release, or middle eight), followed by an out-chorus. (Popular songs of the first half of the twentieth century also typically began with a verse: a brief, explanatory introduction that might or might not be included in performance or on recordings.) The sound of tunes made this way was a satisfying blend of exposition and resolution. </p><p data-journey-content="true" data-node-id="38">Popular songs, which often became the explicit or implicit basis for jazz tunes, were written in a given key, and while they might wander chordally—see Oscar Hammerstein and Jerome Kern’s “All the Things You Are” or the bridge of Richard Rodgers and Lorenz Hart’s “Have You Met Miss Jones?”—they tended, satisfyingly, to come back home to that first chord. This was even truer of the blues, with its intrinsic I-IV-V format, a structure that was restrictive but deeply pleasing. A story was told, and you learned the outcome, even if it was sad. (See: “Moanin’.”) You knew how it turned out—maybe you knew before the song started—but hearing about it could make you forget your troubles for a while or identify with the singer’s or the musician’s troubles.</p><p data-journey-content="true" data-node-id="39">But with Miles, in life and in art, it was always the thing withheld. And the essence of modal music—the essence of “So What”—was that you had no idea how it turned out, or if it turned out. Which was pretty much the way the world was looking at that moment, and maybe the way (you had to think) it was going to look from then on.</p><p data-journey-content="true" data-node-id="40">It was 1959; the world jostled and rocked. American automobiles sprouted double headlights and fins. Batista fled Havana; Castro entered. Khrushchev met Mao, visited Disneyland, debated Nixon in a model kitchen at the American National Exhibition in Moscow. Alaska and Hawaii joined the Union; the flag gained two stars. The crashes of commercial airliners were a depressingly regular event. The music died in Clear Lake, Iowa; the Clutters died in Holcomb, Kansas. Johnny and the Moondogs, a British guitar trio led by the eighteen-year-old art student John Lennon (his bandmates were the seventeen-year-old Paul McCartney and sixteen-year-old George Harrison), played gigs around Liverpool whenever they could find a drummer. <em>Rocky and Bullwinkle</em> and <em>Bonanza</em>—in color—debuted. As did the Xerox machine. As did the Barbie doll. NASA named the seven original astronauts, and the Space Age began. <em>Gypsy</em> and <em>The Sound of Music</em> and <em>A Raisin in the Sun</em> premiered on Broadway. The Boeing 707 and the ICBM were introduced: travelers could now fly to far-off destinations at unprecedented speeds, as could nuclear bombs.</p><p data-journey-content="true" data-node-id="41">So what.</p><hr data-node-id="42"><p data-journey-content="true" data-node-id="43"><em>From </em>3 SHADES OF BLUE: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool<em> by James Kaplan, to be published on March 5, 2024, by Penguin Press, an imprint of Penguin Publishing Group, a division of Penguin Random House, LLC. Copyright © 2024 by James Kaplan. </em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS acquires Talen's 960MW nuclear data center campus in Pennsylvania (180 pts)]]></title>
            <link>https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</link>
            <guid>39597184</guid>
            <pubDate>Mon, 04 Mar 2024 22:47:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/">https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/</a>, See on <a href="https://news.ycombinator.com/item?id=39597184">Hacker News</a></p>
Couldn't get https://www.datacenterdynamics.com/en/news/aws-acquires-talens-nuclear-data-center-campus-in-pennsylvania/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[How I keep myself alive using Golang (507 pts)]]></title>
            <link>https://www.bytesizego.com/blog/keeping-alive-with-go</link>
            <guid>39597131</guid>
            <pubDate>Mon, 04 Mar 2024 22:42:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bytesizego.com/blog/keeping-alive-with-go">https://www.bytesizego.com/blog/keeping-alive-with-go</a>, See on <a href="https://news.ycombinator.com/item?id=39597131">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: 3 years and 1M users later, I just open-sourced my "Internet OS" (911 pts)]]></title>
            <link>https://github.com/HeyPuter/puter</link>
            <guid>39597030</guid>
            <pubDate>Mon, 04 Mar 2024 22:31:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/HeyPuter/puter">https://github.com/HeyPuter/puter</a>, See on <a href="https://news.ycombinator.com/item?id=39597030">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h3 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/45728aadda95f178ec0e4f26999b5f7cc4cd74882d80872ecb46142258177f66/68747470733a2f2f6173736574732e70757465722e736974652f70757465722d6c6f676f2e706e67"><img width="80" alt="Puter.com, The Personal Cloud Computer: All your files, apps, and games in one place accessible from anywhere at any time." src="https://camo.githubusercontent.com/45728aadda95f178ec0e4f26999b5f7cc4cd74882d80872ecb46142258177f66/68747470733a2f2f6173736574732e70757465722e736974652f70757465722d6c6f676f2e706e67" data-canonical-src="https://assets.puter.site/puter-logo.png"></a></h3><a id="" aria-label="Permalink: " href="#"></a></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Desktop Environment in the Browser!</h3><a id="user-content-desktop-environment-in-the-browser" aria-label="Permalink: Desktop Environment in the Browser!" href="#desktop-environment-in-the-browser"></a></p>

<div dir="auto"><h3 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3069986234ddeb463c411dc613d2d62d64cf7f485d2b76c15946ab02607c174c/68747470733a2f2f6173736574732e70757465722e736974652f70757465722e636f6d2d73637265656e73686f742d322e77656270"><img width="700" alt="screenshot" src="https://camo.githubusercontent.com/3069986234ddeb463c411dc613d2d62d64cf7f485d2b76c15946ab02607c174c/68747470733a2f2f6173736574732e70757465722e736974652f70757465722e636f6d2d73637265656e73686f742d322e77656270" data-canonical-src="https://assets.puter.site/puter.com-screenshot-2.webp"></a></h3><a id="user-content--1" aria-label="Permalink: " href="#-1"></a></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Puter</h2><a id="user-content-puter" aria-label="Permalink: Puter" href="#puter"></a></p>
<p dir="auto">Puter is an advanced open-source desktop environment in the browser, designed to be feature-rich, exceptionally fast, and highly extensible. It can be used to build remote desktop environments or serve as an interface for cloud storage services, remote servers, web hosting platforms, and more.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/HeyPuter/puter
cd puter
npm install
npm start"><pre>git clone https://github.com/HeyPuter/puter
<span>cd</span> puter
npm install
npm start</pre></div>
<p dir="auto">This will launch Puter at <a href="http://localhost:4000/" rel="nofollow">http://localhost:4000</a> (or the next available port).</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Deploy to Production</h2><a id="user-content-deploy-to-production" aria-label="Permalink: Deploy to Production" href="#deploy-to-production"></a></p>
<p dir="auto">Detailed guide on how to deploy Puter in production: <a href="https://github.com/HeyPuter/puter/blob/master/docs/prod.md">docs/prod.md</a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">❓ What's the use case for Puter?</h3><a id="user-content--whats-the-use-case-for-puter" aria-label="Permalink: ❓ What's the use case for Puter?" href="#-whats-the-use-case-for-puter"></a></p>
<p dir="auto">Puter can be used as:</p>
<ul dir="auto">
<li>An alternative to Dropbox, Google Drive, OneDrive, etc. with a fresh interface and powerful features.</li>
<li>Remote desktop environment for servers and workstations.</li>
<li>A platform for building and hosting websites, web apps, and games.</li>
<li>A friendly, open-source project and community to learn about web development, cloud computing, distributed systems, and much more!</li>
</ul>

<p dir="auto"><h3 tabindex="-1" dir="auto">❓ Why isn't Puter built with React, Angular, Vue, etc.?</h3><a id="user-content--why-isnt-puter-built-with-react-angular-vue-etc" aria-label="Permalink: ❓ Why isn't Puter built with React, Angular, Vue, etc.?" href="#-why-isnt-puter-built-with-react-angular-vue-etc"></a></p>
<p dir="auto">For performance reasons, Puter is built with vanilla JavaScript and jQuery. Additionally, we'd like to avoid complex abstractions and to remain in control of the entire stack, as much as possible.</p>
<p dir="auto">Also partly inspired by some of our favorite projects that are not built with frameworks: <a href="https://github.com/microsoft/vscode">VSCode</a>, <a href="https://www.photopea.com/" rel="nofollow">Photopea</a>, and <a href="https://www.onlyoffice.com/" rel="nofollow">OnlyOffice</a>.</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">❓ Why jQuery?</h3><a id="user-content--why-jquery" aria-label="Permalink: ❓ Why jQuery?" href="#-why-jquery"></a></p>
<p dir="auto">Puter interacts directly with the DOM and jQuery provides an elegant yet powerful API to manipulate the DOM, handle events, and much more. It's also fast, mature, and battle-tested.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">The default wallpaper is created by <a href="https://unsplash.com/photos/blue-orange-and-yellow-wallpaper-E8Ufcyxz514" rel="nofollow">Milad Fakurian</a> and published on <a href="https://unsplash.com/" rel="nofollow">Unsplash</a>.</p>
<p dir="auto">Icons by <a href="https://github.com/PapirusDevelopmentTeam/papirus-icon-theme">Papirus</a> under GPL-3.0 license.</p>
<p dir="auto">Icons by <a href="https://iconoir.com/" rel="nofollow">Iconoir</a> under MIT license.</p>
<p dir="auto">Icons by <a href="https://github.com/elementary/icons">Elementary Icons</a> under GPL-3.0 license.</p>
<p dir="auto">Icons by <a href="https://tabler.io/" rel="nofollow">Tabler Icons</a> under MIT license.</p>
<p dir="auto">Icons by <a href="https://icons.getbootstrap.com/" rel="nofollow">bootstrap-icons</a> under MIT license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Borrow checking without lifetimes (105 pts)]]></title>
            <link>https://smallcultfollowing.com/babysteps/blog/2024/03/04/borrow-checking-without-lifetimes/</link>
            <guid>39594362</guid>
            <pubDate>Mon, 04 Mar 2024 18:52:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smallcultfollowing.com/babysteps/blog/2024/03/04/borrow-checking-without-lifetimes/">https://smallcultfollowing.com/babysteps/blog/2024/03/04/borrow-checking-without-lifetimes/</a>, See on <a href="https://news.ycombinator.com/item?id=39594362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This blog post explores an alternative formulation of Rust’s type system that eschews <em>lifetimes</em> in favor of <em>places</em>. The TL;DR is that instead of having <code>'a</code> represent a <em>lifetime</em> in the code, it can represent a set of <em>loans</em>, like <code>shared(a.b.c)</code> or <code>mut(x)</code>. If this sounds familiar, it should, it’s the basis for <a href="https://smallcultfollowing.com/babysteps/blog/2023/09/22/polonius-part-1/">polonius</a>, but reformulated as a type system instead of a static analysis. This blog post is just going to give the high-level ideas. In follow-up posts I’ll dig into how we can use this to support interior references and other advanced borrowing patterns. In terms of implementation, I’ve mocked this up a bit, but I intend to start extending <a href="https://github.com/rust-lang/a-mir-formality">a-mir-formality</a> to include this analysis.</p><h2 id="why-would-you-want-to-replace-lifetimes">Why would you want to replace lifetimes?</h2><p>Lifetimes are the best and worst part of Rust. The best in that they let you express very cool patterns, like returning a pointer into some data in the middle of your data structure. But they’ve got some serious issues. For one, the idea of what a lifetime is rather abstract, and hard for people to grasp (“what does <code>'a</code> actually represent?”). But also Rust is not able to express some important patterns, most notably interior references, where one field of a struct refers to data owned by another field.</p><h2 id="so-what-is-a-lifetime-exactly">So what <em>is</em> a lifetime exactly?</h2><p>Here is the definition of a lifetime from the RFC on non-lexical lifetimes:</p><blockquote><p>Whenever you create a borrow, the compiler assigns the resulting reference a lifetime. This lifetime corresponds to the span of the code where the reference may be used. The compiler will infer this lifetime to be the smallest lifetime that it can have that still encompasses all the uses of the reference.</p></blockquote><p><a href="https://rust-lang.github.io/rfcs/2094-nll.html#what-is-a-lifetime">Read the RFC for more details.</a></p><h2 id="replacing-a-lifetime-with-an-origin">Replacing a <em>lifetime</em> with an <em>origin</em></h2><p>Under this formulation, <code>'a</code> no longer represents a <em>lifetime</em> but rather an <strong>origin</strong> – i.e., it explains where the reference may have come from. We define an origin as a <strong>set of loans</strong>. Each loan captures some <strong>place expression</strong> (e.g. <code>a</code> or <code>a.b.c</code>), that has been borrowed along with the mode in which it was borrowed (<code>shared</code> or <code>mut</code>).</p><pre tabindex="0"><code>Origin = { Loan }

Loan = shared(Place)
     | mut(Place)

Place = variable(.field)*  // e.g., a.b.c
</code></pre><h2 id="defining-types">Defining types</h2><p>Using origins, we can define Rust types roughly like this (obviously I’m ignoring a bunch of complexity here…):</p><pre tabindex="0"><code>Type = TypeName &lt; Generic* &gt;
     | &amp; Origin Type
     | &amp; Origin mut Type
     
TypeName = u32 (for now I'll ignore the rest of the scalars)
         | ()  (unit type, don't worry about tuples)
         | StructName
         | EnumName
         | UnionName

Generic = Type | Origin
</code></pre><p>Here is the first interesting thing to note: there is no <code>'a</code> notation here! This is because I’ve not introduced generics yet. Unlike Rust proper, this formulation of the type system has a concrete syntax (<code>Origin</code>) for what <code>'a</code> represents.</p><h2 id="explicit-types-for-a-simple-program">Explicit types for a simple program</h2><p>Having a fully explicit type system also means we can easily write out example programs where all types are fully specified. This used to be rather challenging because we had no notation for lifetimes. Let’s look at a simple example, a program that ought to get an error:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>let</span><span> </span><span>mut</span><span> </span><span>counter</span>: <span>u32</span> <span>=</span><span> </span><span>22_</span><span>u32</span><span>;</span><span>
</span></span></span><span><span><span></span><span>let</span><span> </span><span>p</span>: <span>&amp;</span> <span>/*{shared(counter)}*/</span><span> </span><span>u32</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>counter</span><span>;</span><span>
</span></span></span><span><span><span></span><span>//       ---------------------
</span></span></span><span><span><span>//       no syntax for this today!
</span></span></span><span><span><span></span><span>counter</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span> </span><span>// Error: cannot mutate `counter` while `p` is live
</span></span></span><span><span><span></span><span>println!</span><span>(</span><span>"</span><span>{p}</span><span>"</span><span>);</span><span>
</span></span></span></code></pre></div><p>Apart from the type of <code>p</code>, this is valid Rust. Of course, it won’t compile, because we can’t modify <code>counter</code> while there is a live shared reference <code>p</code> (<a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=1a05f0a4aad12c33345ca4adc1cd9bb2">playground</a>). As we continue, you will see how the new type system formulation arrives at the same conclusion.</p><h2 id="basic-typing-judgments">Basic typing judgments</h2><p>Typing judgments are the standard way to describe a type system. We’re going to phase in the typing judgments for our system iteratively. We’ll start with a simple, fairly standard formulation that doesn’t include borrow checking, and then show how we introduce borrow checking. For this first version, the typing judgment we are defining has the form</p><pre tabindex="0"><code>Env |- Expr : Type
</code></pre><p>This says, “in the environment <code>Env</code>, the expression <code>Expr</code> is legal and has the type <code>Type</code>”. The <em>environment</em> <code>Env</code> here defines the local variables in scope. The Rust expressions we are looking at for our <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2021&amp;gist=1a05f0a4aad12c33345ca4adc1cd9bb2">sample program</a> are pretty simple:</p><pre tabindex="0"><code>Expr = integer literal (e.g., 22_u32)
     | &amp; Place
     | Expr + Expr
     | Place (read the value of a place)
     | Place = Expr (overwrite the value of a place)
     | ...
</code></pre><p>Since we only support one scalar type (<code>u32</code>), the typing judgment for <code>Expr + Expr</code> is as simple as:</p><pre tabindex="0"><code>Env |- Expr1 : u32
Env |- Expr2 : u32
----------------------------------------- addition
Env |- Expr1 + Expr2 : u32
</code></pre><p>The rule for <code>Place = Expr</code> assignments is based on subtyping:</p><pre tabindex="0"><code>Env |- Expr : Type1
Env |- Place : Type2
Env |- Type1 &lt;: Type2
----------------------------------------- assignment
Env |- Place = Expr : ()
</code></pre><p>The rule for <code>&amp;Place</code> is somewhat more interesting:</p><pre tabindex="0"><code>Env |- Place : Type
----------------------------------------- shared references
Env |- &amp; Place : &amp; {shared(Place)} Type
</code></pre><p>The rule just says that we figure out the type of the place <code>Place</code> being borrowed (here, the place is <code>counter</code> and its type will be <code>u32</code>) and then we have a resulting reference to that type. The origin of that reference will be <code>{shared(Place)}</code>, indicating that the reference came from <code>Place</code>:</p><pre tabindex="0"><code>&amp;{shared(Place)} Type
</code></pre><h2 id="computing-liveness">Computing liveness</h2><p>To introduce borrow checking, we need to phase in the idea of <strong>liveness</strong>.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> If you’re not familiar with the concept, the NLL RFC has a <a href="https://rust-lang.github.io/rfcs/2094-nll.html#liveness">nice introduction</a>:</p><blockquote><p>The term “liveness” derives from compiler analysis, but it’s fairly intuitive. We say that a variable is live if the current value that it holds may be used later.</p></blockquote><p>Unlike with NLL, where we just computed live <strong>variables</strong>, we’re going to compute <strong>live places</strong>:</p><pre tabindex="0"><code>LivePlaces = { Place }
</code></pre><p>To compute the set of live places, we’ll introduce a helper function <code>LiveBefore(Env, LivePlaces, Expr): LivePlaces</code>. <code>LiveBefore()</code> returns the set of places that are live before <code>Expr</code> is evaluated, given the environment <code>Env</code> and the set of places live after expression. I won’t define this function in detail, but it looks roughly like this:</p><pre tabindex="0"><code>// `&amp;Place` reads `Place`, so add it to `LivePlaces`
LiveBefore(Env, LivePlaces, &amp;Place) =
    LivePlaces ∪ {Place}

// `Place = Expr` overwrites `Place`, so remove it from `LivePlaces`
LiveBefore(Env, LivePlaces, Place = Expr) =
    LiveBefore(Env, (LivePlaces - {Place}), Expr)

// `Expr1` is evaluated first, then `Expr2`, so the set of places
// live after expr1 is the set that are live *before* expr2
LiveBefore(Env, LivePlaces, Expr1 + Expr2) =
    LiveBefore(Env, LiveBefore(Env, LivePlaces, Expr2), Expr1)
    
... etc ...
</code></pre><h2 id="integrating-liveness-into-our-typing-judgments">Integrating liveness into our typing judgments</h2><p>To detect borrow check errors, we need to adjust our typing judgment to include liveness. The result will be as follows:</p><pre tabindex="0"><code>(Env, LivePlaces) |- Expr : Type
</code></pre><p>This judgment says, “in the environment <code>Env</code>, and given that the function will access <code>LivePlaces</code> in the future, <code>Expr</code> is valid and has type <code>Type</code>”. Integrating liveness in this way gives us some idea of what accesses will happen in the future.</p><p>For compound expressions, like <code>Expr1 + Expr2</code>, we have to adjust the set of live places to reflect control flow:</p><pre tabindex="0"><code>LiveAfter1 = LiveBefore(Env, LiveAfter2, Expr2)
(Env, LiveAfter1) |- Expr1 : u32
(Env, LiveAfter2) |- Expr2 : u32
----------------------------------------- addition
(Env, LiveAfter2) |- Expr1 + Expr2 : u32
</code></pre><p>We start out with <code>LiveAfter2</code>, i.e., the places that are live after the entire expression. These are also the same as the places live after expression 2 is evaluated, since this expression doesn’t itself reference or overwrite any places. We then compute <code>LiveAfter1</code> – i.e., the places live after <code>Expr1</code> is evaluated – by looking at the places that are live <em>before</em> <code>Expr2</code>. This is a bit mind-bending and took me a bit of time to see. The tricky bit here is that liveness is computed <em>backwards</em>, but most of our typing rules (and intution) tends to flow <em>forwards</em>. If it helps, think of the “fully desugared” version of <code>+</code>:</p><pre tabindex="0"><code>let tmp0 = &lt;Expr1&gt;
    // &lt;-- the set LiveAfter1 is live here (ignoring tmp0, tmp1)
let tmp1 = &lt;Expr2&gt;
    // &lt;-- the set LiveAfter2 is live here (ignoring tmp0, tmp1)
tmp0 + tmp1
    // &lt;-- the set LiveAfter2 is live here
</code></pre><h2 id="borrow-checking-with-liveness">Borrow checking with liveness</h2><p>Now that we know liveness information, we can use it to do borrow checking. We’ll introduce a “permits” judgment:</p><pre tabindex="0"><code>(Env, LiveAfter) permits Loan
</code></pre><p>that indicates that “taking the loan Loan would be allowed given the environment and the live places”. Here is the rule for assignments, modified to include liveness and the new “permits” judgment:</p><pre tabindex="0"><code>(Env, LiveAfter - {Place}) |- Expr : Type1
(Env, LiveAfter) |- Place : Type2
(Env, LiveAfter) |- Type1 &lt;: Type2
(Env, LiveAfter) permits mut(Place)
----------------------------------------- assignment
(Env, LiveAfter) |- Place = Expr : ()
</code></pre><p>Before I dive into how we define “permits”, let’s go back to our example and get an intution for what is going on here. We want to declare an error on this assigment:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>let</span><span> </span><span>mut</span><span> </span><span>counter</span>: <span>u32</span> <span>=</span><span> </span><span>22_</span><span>u32</span><span>;</span><span>
</span></span></span><span><span><span></span><span>let</span><span> </span><span>p</span>: <span>&amp;</span><span>{</span><span>shared</span><span>(</span><span>counter</span><span>)}</span><span> </span><span>u32</span><span> </span><span>=</span><span> </span><span>&amp;</span><span>counter</span><span>;</span><span>
</span></span></span><span><span><span></span><span>counter</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span> </span><span>// &lt;-- Error
</span></span></span><span><span><span></span><span>println!</span><span>(</span><span>"</span><span>{p}</span><span>"</span><span>);</span><span> </span><span>// &lt;-- p is live
</span></span></span></code></pre></div><p>Note that, because of the <code>println!</code> on the next line, <code>p</code> will be in our <code>LiveAfter</code> set. Looking at the type of <code>p</code>, we see that it includes the loan <code>shared(counter)</code>. The idea then is that mutating counter is illegal because there is a live loan <code>shared(counter)</code>, which implies that <code>counter</code> must be immutable.</p><p>Restating that intution:</p><blockquote><p>A set <code>Live</code> of live places <em>permits</em> a loan <code>Loan1</code> if, for every live place <code>Place</code> in <code>Live</code>, the loans in the type of <code>Place</code> are compatible with <code>Loan1</code>.</p></blockquote><p>Written more formally:</p><pre tabindex="0"><code>∀ Place ∈ Live {
    (Env, Live) |- Place : Type
    ∀ Loan2 ∈ Loans(Type) { Compatible(Loan1, Loan2) }
}
-----------------------------------------
(Env, Live) permits Loan1
</code></pre><p>This definition makes use of two helper functions:</p><ul><li><code>Loans(Type)</code> – the set of loans that appear in the type</li><li><code>Compatible(Loan1, Loan2)</code> – defines if two loans are compatible. Two shared loans are always compatible. A mutable loan is only compatible with another loan if the places are disjoint.</li></ul><h2 id="conclusion">Conclusion</h2><p>The goal of this post was to give a high-level intution. I wrote it from memory, so I’ve probably overlooked a thing or two. In follow-up posts though I want to go deeper into how the system I’ve been playing with works and what new things it can support. Some high-level examples:</p><ul><li>How to define subtyping, and in particular the role of liveness in subtyping</li><li>Important borrow patterns that we use today and how they work in the new system</li><li>Interior references that point at data owned by other struct fields and how it can be supported</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There are only 12 binaries in Talos Linux (137 pts)]]></title>
            <link>https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/</link>
            <guid>39594355</guid>
            <pubDate>Mon, 04 Mar 2024 18:52:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/">https://www.siderolabs.com/blog/there-are-only-12-binaries-in-talos-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=39594355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="60b29bb" data-element_type="widget" data-widget_type="theme-post-content.default">
			
<figure><img fetchpriority="high" decoding="async" width="1024" height="538" src="https://mlxz2ksifyzh.i.optimole.com/w:1024/h:538/q:mauto/ig:avif/f:best/https://www.siderolabs.com/wp-content/uploads/2024/03/12-binaries-banner-1.png" alt=""></figure>



<p>Linux is a core component of your Kubernetes cluster. The distribution you choose will have a big impact on how quickly you can create a cluster, the stability of your workloads, and how much maintenance you’ll need to perform.</p>



<p>When creating a version of Linux for containers or Kubernetes, for many companies and distributions the common practice is to start with a general-purpose Linux and strip away things you don’t need. This results in a smaller footprint variation of the main distribution—e.g. <a href="https://wiki.ubuntu.com/Minimal">Ubuntu minimal</a>—but it always starts from a big, general purpose Linux and tries to make it smaller.</p>



<p>Talos Linux takes the opposite approach. What if the distribution only had to run Kubernetes? What is the minimal set of tooling and executables needed?</p>



<p>There are <strong>12 unique binaries</strong> in /bin and /sbin in Talos 1.7.0. This greatly reduces the size of installation, the maintenance needed for the operating system, and the possible security vulnerabilities of the system.</p>



<p>For reference here are some other popular distributions and how many binaries they include by default. This also counts executables that are symlinked or hard linked to another file (e.g. <code>lvm</code> is often symlinked multiple times for <code>lvs</code> and <code>vgs</code>).</p>



<figure><table><tbody><tr><td>Talos Linux</td><td>29</td></tr><tr><td>Ubuntu Server 22.04</td><td>2780</td></tr><tr><td>Amazon Linux 2</td><td>1382</td></tr><tr><td>Flatcar Container Linux</td><td>2391</td></tr></tbody></table></figure>



<p>All distros were set up with default installation options. No additional packages were installed and binaries were counted with:</p>



<pre><code>ls -1 $(echo $PATH | tr ':' '\n') | wc -l</code></pre>



<p>Talos doesn’t provide a shell, <code>ls</code>, <code>tr</code>, or <code>wc</code>. Files were counted via the API and we didn’t count directories:</p>



<pre><code>talosctl list -n $NODE_IP /sbin | wc -l
talosctl list -n $NODE_IP /bin | wc -l</code></pre>



<p>With only 12 unique files on the system we can tell you what each one does.</p>



<h2>/sbin/init</h2>



<p>The init binary is one of the biggest strengths of Talos Linux. Talos doesn’t ship with a general purpose init system like systemd. Talos’ init system is purpose built for running the Kubelet and a container runtime. The init system exposes a declarative API which is how the system is configured and maintained.</p>



<p>The init binary is <a href="https://github.com/siderolabs/talos/blob/main/internal/app/machined/main.go">called machined</a> and is written in go. It’s less than 400 lines of code and can be understood by a go developer in less than a day. As <a href="https://github.com/systemd/systemd/blob/main/src/core/main.c">opposed to systemd</a> which is over 3000 lines of C code I’ll never comprehend.</p>



<p>The <code>/sbin/init</code> binary is hard linked to <code>/sbin/dashboard</code>, <code>/sbin/poweroff</code>, <code>/sbin/shutdown</code>, and <code>/sbin/wrapperd</code>. While this technically is 5 files, it’s a single file hard linked 4 times to provide convenience commands.</p>



<p>The dashboard is used for providing local and remote information about the node. You can see an overview of how it works on YouTube.</p>



<figure><p>
<iframe title="Talos dashboard overview" width="800" height="450" src="https://www.youtube.com/embed/52Wmkb0H-98?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>The binaries <code>poweroff</code> and <code>shutdown</code> are commands to cleanly shut down the node. These are used by the kernel and external tools, but Talos uses the system API to shutdown.</p>



<p>The <a href="https://github.com/siderolabs/talos/blob/main/internal/app/wrapperd/main.go">wrapperd</a> binary is used during init to fork processes with reduced privledges. Because a child process will inherit a lot from the parent process wrapperd is used to remove kernel capabilities like CAP_SYSADMIN.</p>



<p>All of the other binaries on the system are included from other packages we build from source. You can see how they get built <a href="https://github.com/siderolabs/pkgs">from GitHub</a> and we will review each binary below.</p>



<h2>/bin/containerd*</h2>



<p>This is the container runtime that ships with Talos. It is commonly used with Kubernetes clusters and is the default container runtime option for the majority of providers.</p>



<p>This also includes <code>/bin/containerd-shim-runc-v2</code> and <code>/bin/containerd-shim</code>. Both of these shims provide the same function (executing a container under <code>runc</code>) but <code>containerd-shim</code> was originally used with docker and <code>containerd-shim-runc-v2</code> is used from containerd.</p>



<h2>/bin/runc</h2>



<p>This is the true <a href="https://github.com/opencontainers/runc">parent process of your containers</a>. It is daemonless so the containerd service can restart if needed without stopping all your containers.</p>



<h2>/sbin/modprobe</h2>



<p>The command <a contenteditable="false" href="https://linux.die.net/man/8/modprobe">modprobe</a> is for managing kernel modules to add or remove functionality from your kernel. This is often for adding support for special hardware (e.g. GPUs) but is also used for additional kernel tooling.</p>



<p>Talos doesn’t use modprob directly but some modules require the binary to load other modules. You can add kernel modules to Talos via <a href="https://www.talos.dev/latest/talos-guides/configuration/system-extensions/">system extensions</a> and use pre-built extensions from the <a href="https://www.talos.dev/latest/learn-more/image-factory/">Image Factory</a>.</p>



<h2>/sbin/lvm</h2>



<p>The <code>lvm</code> binary is used for managing logical volumes in Linux. This is provided for services that run in Kubernetes that may need or expect a logical volume to be present on the host (e.g. <a href="https://rook.io/docs/rook/latest-release/Getting-Started/Prerequisites/prerequisites/#lvm-package">rook</a>).</p>



<h2>/sbin/dmsetup</h2>



<p>This is used for managing logical volumes that use the device-mapper driver. It’s similar to <code>lvs</code> commands but a separate binary for more complex disk configuration.</p>



<h2>/sbin/udevd</h2>



<p>The udevd daemon takes kernel messages and passes the messages to other systems to read the messages. It can be configured as part of the <a href="https://www.talos.dev/latest/reference/configuration/v1alpha1/config/#Config.machine.udev">Talos machine config</a>.</p>



<h2>/sbin/mkfs.xfs</h2>



<p>This will create an <a href="https://en.wikipedia.org/wiki/XFS">XFS file system</a> on a disk or logical volume.</p>



<h2>/sbin/xfs_repair</h2>



<p>This is used to repair an XFS file system if it becomes corrupted.</p>



<h2>/sbin/xtables-legacy-multi</h2>



<p>This binary is symlinked by <code>iptables*</code> and <code>ip6tables*</code> to configure IP tables on the host. Container network interface (CNI) providers often mount directories from the host and expect these commands to exist because they cannot easily be run from within a container.</p>



<p>These symlinks account for 12 total files in the system but they all perform common iptables commands.</p>



<h2>Conclusion</h2>



<p>It may seem impossible but that’s the entire system. Every binary is required to bootstrap a Kubernetes cluster or run a node as part of the cluster. This is why we call Talos Linux the <a href="https://www.siderolabs.com/platform/talos-os-for-kubernetes/">Kubernetes Operating System</a>.</p>



<p>There are more executable files in /lib and /usr but those are Shared Object (.so) files and Kernel modules (.ko). These are necessary to run the system for drivers and various programs but are not called directly.</p>



<p>If you would like to download and install Talos on your system of choice you can get started at <a href="https://talos.dev/" target="_blank" rel="noreferrer noopener">https://talos.dev</a>.</p>



<p>To get an even easier interface to managing Kubernetes clusters on-prem or in a cloud provider check out Omni at <a href="https://www.siderolabs.com/platform/saas-for-kubernetes/" target="_blank" rel="noreferrer noopener">https://www.siderolabs.com/platform/saas-for-kubernetes/</a>.</p>



<p>If you have questions or want to get started come join the <a href="https://slack.dev.talos-systems.io/">Talos community Slack</a>.</p>



<figure><p>
<iframe title="Welcome to Omni" width="800" height="450" src="https://www.youtube.com/embed/0gPF0_fLins?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yuzu emulator developers settle Nintendo lawsuit, pay $2.4M in damages (493 pts)]]></title>
            <link>https://twitter.com/oatmealdome/status/1764704580724576465</link>
            <guid>39593647</guid>
            <pubDate>Mon, 04 Mar 2024 18:00:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/oatmealdome/status/1764704580724576465">https://twitter.com/oatmealdome/status/1764704580724576465</a>, See on <a href="https://news.ycombinator.com/item?id=39593647">Hacker News</a></p>
Couldn't get https://twitter.com/oatmealdome/status/1764704580724576465: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Opus 1.5 released: Opus gets a machine learning upgrade (357 pts)]]></title>
            <link>https://opus-codec.org/demo/opus-1.5/</link>
            <guid>39593256</guid>
            <pubDate>Mon, 04 Mar 2024 17:36:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opus-codec.org/demo/opus-1.5/">https://opus-codec.org/demo/opus-1.5/</a>, See on <a href="https://news.ycombinator.com/item?id=39593256">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="xiphlogo">
      <p><a href="https://www.xiph.org/"><img src="https://www.xiph.org/images/logos/fish_xiph_org.png" alt="Fish Logo and Xiph.org"></a></p>
      
    </div>


    
    <a href="https://opus-codec.org/">
    <img src="https://opus-codec.org/demo/opus-1.5/opus-1.5_logo.png" alt="Banner"></a>
    <p>
    Opus gets another major update with the release of version 1.5. This release brings quality improvements, including
    ML-based ones, while remaining fully compatible with RFC&nbsp;6716. Here are some of the most noteworthy upgrades.
    </p>

    <h2>Opus Gets a Serious Machine Learning Upgrade</h2>

    <p>This <a href="https://opus-codec.org/downloads/">1.5 release</a> is unlike any of the previous ones. It brings many new features
    that can improve quality and the general audio experience.
    That is achieved through machine learning. Although Opus has
    included machine learning — and even deep learning — before
    (e.g. for <a href="https://jmvalin.ca/opus/opus-1.3/">speech/music detection</a>),
    this is the first time it has used deep learning techniques to process or generate the signals
    themselves.</p>

    <p> Instead of designing a new ML-based codec
    from scratch, we prefer to improve Opus in a fully-compatible way.
    That is an important design goal for ML in Opus.
    Not only does that ensure Opus
    keeps working on older/slower devices, but it also provides an easy upgrade path. Deploying
    a new codec can be a long, painful process. Compatibility means that older and newer
    versions of Opus can coexist, while still providing the benefits of the new version
    when available.</p>

    <p>Deep learning also often gets associated with powerful GPUs, but
    in Opus, we have optimized everything such that it easily runs on most
    CPUs, including phones. We have been careful to avoid huge models (unlike LLMs with
    their hundreds of billions of parameters!). In the end, most users should not notice the extra cost,
    but people using older (5+ years) phones or microcontrollers might. For that reason, all new
    ML-based features are disabled by default in Opus 1.5. They require both a compile-time
    switch (for size reasons) and then a run-time switch (for CPU reasons).</p>

    <p>The following sections describe the new features enabled by ML.</p>

    <h2>Dealing with Packet Loss</h2>

    <p>Packet loss is one of the main annoyances one can encounter during a call. It does not
    matter how good the codec is if the packets do not get through.
    That's why most codecs have <i>packet loss concealment</i> (PLC) that can fill in for missing
    packets with plausible audio that just extrapolates what was being said and avoids leaving
    a hole in the audio (a common thing to hear with Bluetooth headsets). PLC is a place
    where ML can help a lot. Instead of using carefully hand-tuned concealment heuristics, we can just
    let a Deep Neural Network (DNN) do it. The technical details are in our
    <a href="https://arxiv.org/pdf/2205.05785.pdf">Interspeech 2022 paper</a>, for which we got the
    second place in the <a href="https://www.microsoft.com/en-us/research/academic-program/audio-deep-packet-loss-concealment-challenge-interspeech-2022/results/">Audio Deep Packet Loss Concealment Challenge</a>.</p>

    <p>When building Opus, using --enable-deep-plc will compile in the deep PLC code at a cost of
    about 1 MB in binary size.
    To actually enable it at run time, you will need to set the decoder complexity to 5 or more.
    Previously, only the encoder had a complexity knob, but the decoder is now getting one too.
    It can be set with the -dec_complexity option to opus_demo, or OPUS_SET_COMPLEXITY() in the
    API (like for the encoder).
    The extra complexity from running PLC at a high loss rate is about 1% of a laptop CPU core.
    Because deep PLC only affects the decoder, turning it on does not have any compatibility
    implications.
    </p>

    <h3>Deep REDundancy (DRED)</h3>

    <p>PLC is great for filling up occasional missing packets, but unfortunately
    packets often go missing in bursts. When that happens, entire phonemes or words are lost. Of course,
    new generative models could easily be used to seamlessly fill any gap with very plausible words, but
    we believe it is good to have the listener hear the <i>same</i> words that were spoken.
    The way to achieve that is through redundancy. Opus already includes
    a low-bitrate redundancy (LBRR) mechanism to transmit every speech frame twice, but only twice.
    While this helps reduce the impact
    of loss, there's only so much it can do for long bursts.
    </p>

    <p>That is where ML can help. We were certainly not the first to think about using
    ML to make a very low bitrate speech codec. However (we think) we are the first to
    design one that is optimized solely for transmitting redundancy. A regular codec needs to
    have short packets (typically 20 ms) to keep the latency low
    and it has to limit its use of prediction specifically to avoid making the packet
    loss problem even worse. For redundancy, we don't have these problems.
    Each packet will contain a large (up to 1 second) chunk of redundant audio
    that will be transmitted all at once.
    Taking advantage of that, the Opus Deep REDundancy (DRED) uses a rate-distortion-optimized
    variational autoencoder (RDO-VAE) to efficiently compress acoustic parameters in such a way that it can
    transmit one second of redundancy with about 12-32 kb/s overhead.
    Every 20-ms packet is effectively transmitted <i>50 times</i> at a cost similar
    to the existing LBRR.
    See this <a href="https://www.amazon.science/blog/neural-encoding-enables-more-efficient-recovery-of-lost-audio-packets">
    demo</a> for a high-level overview of the science behind DRED, or read the
    <a href="https://arxiv.org/pdf/2212.04453.pdf">ICASSP 2023 paper</a> for all the details and math
    behind it.
    </p>

    <a href="https://opus-codec.org/demo/opus-1.5/dred_results.png"><img src="https://opus-codec.org/demo/opus-1.5/dred_results.png" alt="recurrent units"></a>
    <p>
    Subjective testing (MOS) results measuring the improvement provided by DRED with one second
    redundancy for a range of
    realistic packet loss conditions.
    The results show that DRED achieves much higher quality than what either neural PLC alone,
    or LBRR with neural PLC can achieve.
    When DRED is combined with LBRR, the quality approaches that of the no-loss case.
    In these tests, we used 24&nbsp;kb/s for the <em>base</em> Opus layer, 16&nbsp;kb/s extra for LBRR,
    and 32&nbsp;kb/s extra for DRED.
    </p>

    <p>Use the --enable-dred configure option (which automatically turns on --enable-deep-plc) to
    enable DRED.
    Doing so increases the binary size by about 2&nbsp;MB, with a run-time cost around 1% like for deep PLC.
    Beware that DRED is not yet standardized and the version included in Opus&nbsp;1.5 will
    not be compatible with the final version.
    That being said, it is still safe to experiment with it in applications since the bitstream
    carries an experiment version number and any version incompatibility will be detected and simply cause
    the DRED payload to be ignored (no erroneous decoding or loud noises).</p>


    <h3>Neural Vocoder</h3>

    <p>The very low complexity of deep PLC and DRED is made possible by new neural vocoder technology
    we created specifically for this project. The original papers linked above used a
    <a href="https://arxiv.org/pdf/2202.11169.pdf">highly-optimized</a> version of the original
    <a href="https://jmvalin.ca/demo/lpcnet/">LPCNet vocoder</a>, but even that was not quite
    fast enough. So we came up with a new framewise autoregressive generative
    adversarial network (FARGAN) vocoder that uses pitch prediction to achieve
    a complexity of 600 MFLOPS: 1/5 of LPCNet. That
    allows it to run with less than 1% of a CPU core on laptops or even recent phones.
    We don't yet have a paper or writeup on FARGAN, but we are working on fixing that.</p>

    <h2>Low-Bitrate Speech Quality Enhancement</h2>

    <p>Given enough bits, most speech codecs — including Opus — are able to reach a quality
      level close to transparency.
      Unfortunately, the real world sometimes doesn't give us "enough bits". Suddenly, the coding
      artifacts can become audible, or even annoying.
      The classical approach to mitigate this problem is to apply simple, handcrafted
      postfilters that reshape the coding noise to make it less noticeable.
      While those postfilters usually provide a noticeable improvement, their effectiveness is limited. They
      can't work wonders.
    </p>

    <p>
      The rise of ML and DNNs has produced a number of new and much more powerful enhancement methods,
      but these are typically large, high in complexity, and cause additional decoder delay.
      Instead, we went for a different approach: start with the tried-and-true postfilter idea
      and sprinkle just enough DNN magic on top of it.
      Opus 1.5 includes two enhancement methods: the Linear Adaptive Coding Enhancer (LACE) and a
      Non-Linear variation (NoLACE).
      From the signal point of view, LACE is very similar to a classical postfilter.
      The difference comes from a DNN that
      optimizes the postfilter coefficients on-the-fly based on all the data available to the decoder.
      The audio itself never goes through the DNN.
      The result is a small and very-low-complexity model (by DNN standards) that can run even
      on older phones. An explanation of the internals of LACE is given in this short
      <a href="https://www.youtube.com/watch?v=W47qh9Wp9E0">video presentation</a> and more technical
      details can be found in the corresponding <a href="https://arxiv.org/abs/2307.06610">WASPAA 2023 paper</a>.
      NoLACE is an extension of LACE that requires more computation but is
      also much more powerful due to extra non-linear signal processing.
      It still runs without significant overhead on
      recent laptop and smartphone CPUs. Technical details about NoLACE are given in the corresponding
      <a href="https://arxiv.org/abs/2309.14521">ICASSP 2024 paper</a>.
    </p>

    <a href="https://opus-codec.org/demo/opus-1.5/nolace_results.png"><img src="https://opus-codec.org/demo/opus-1.5/nolace_results.png" alt="recurrent units"></a>
    <p>
    Subjective testing (MOS) results comparing the speech decoded from the default
    decoder to the enhanced speech produced by LACE and NoLACE from that same decoder.
    The uncompressed speech has a MOS of 4.06.
    The results show that using NoLACE, Opus is now perfectly usable down to 6 kb/s.
    At 9 kb/s, NoLACE-enhanced speech is already close to transparency, and better than
    the non-enhanced 12 kb/s.
    </p>

    <p>
      To try LACE and NoLACE, just add the --enable-osce configure flag when building Opus.
      Then, to enable LACE at run-time, set the decoder complexity to 6.
      Set it to 7 or higher to enable NoLACE instead of LACE. Building with --enable-osce increases
      the binary size by about 1.6 MB, roughly 0.5 MB for LACE and 1.1 MB for NoLACE. The LACE model has a
      complexity of 100 MFLOPS which leads to a run-time cost of ~0.15% CPU usage. The NoLACE model has a complexity
      of 400 MFLOPS which corresponds to a run-time cost of ~0.75% CPU usage.
      LACE and NoLACE are currently only applied when the frame size is 20 ms (the default) and the bandwidth
      is at least wideband.
      Although LACE and NoLACE have not yet been standardized, turning them on does not have
      compatibility implications since the enhancements are independent of the encoder.
    </p>

    <h3>Samples</h3>

    <p>OK, nice graphs, but how does it actually sound? The following samples demonstrate
    the effect of LACE or NoLACE on Opus wideband speech quality at different bitrates. We recommend listening with good headphones,
    especially for higher bitrates. </p>

    <div>
      <p><audio controls="" id="speech_player" src="https://opus-codec.org/demo/opus-1.5/samples/female_nolace_12k.wav">
        Your browser does not support the audio tag.
      </audio></p><div>
      <p>Select sample</p>
      <ul>
        <li onclick="setSpeechSample(0, this);" id="speech_default_sample">Female</li>
        <li onclick="setSpeechSample(1, this);">Male</li>
      </ul>
      </div>
      <div>
      <p>Select enhancement</p>
      <ul>
        <li onclick="setSpeechCodec(1, this);">None</li>
        <li onclick="setSpeechCodec(2, this);">LACE</li>
        <li onclick="setSpeechCodec(3, this);" id="speech_default_codec">NoLACE</li>
        <li onclick="setSpeechCodec(4, this);">Uncompressed</li>
      </ul>
      </div>
      <div>
      <p>Select bitrate</p>
      <ul id="speech_bitrate_selector">
        <li onclick="setSpeechRate(0, this);">6 kb/s</li>
        <li onclick="setSpeechRate(1, this);" id="speech_default_rate">9 kb/s</li>
        <li onclick="setSpeechRate(2, this);">12 kb/s</li>
      </ul>
      </div>
      <p>Select where to start playing when selecting a new sample</p>
      <p id="speech_restart_string">Player will <b>continue</b> when changing sample.</p>
    </div>
    <p>Demonstrating the effect of LACE and NoLACE on speech quality at 6, 9, and 12 kb/s.
    </p>

    <h2>WebRTC Integration</h2>

    <p>Using the deep PLC or the quality enhancements should typically require only minor
    code changes. DRED is an entirely different story. It requires closer integration with
    the jitter buffer to ensure that redundancy gets used.</p>
    <p>In a real-time communications system, the size of the jitter buffer determines the
    maximum amount of packet arrival lateness that can be tolerated without producing
    an audible gap in audio playout.
    In the case of packet loss, we can treat the DRED data similarly to
    late arriving audio packets. We take care to only insert this data into the jitter
    buffer if we have observed prior loss. In ideal circumstances, an adaptive jitter
    buffer (like NetEq used in WebRTC) will try to minimize its size in order to preserve
    interactive latency. If data arrives too late for playback, there will be an audible
    gap, but the buffer will then grow to accommodate the new nominal lateness. If network
    conditions improve the buffer can shrink back down, using time scaling to play the
    audio at a slightly faster rate. In the case of DRED, there will always be a loss vs.
    latency tradeoff. In order to make use of the DRED data and cover prior lost packets,
    we will need to tolerate a larger jitter buffer.
    But because we treat DRED similarly to late packet arrival, we can take advantage
    of the existing adaptation in NetEq to provide a reasonable compromise in loss vs. latency.
    </p>

    <p>You can try out DRED using the patches in our <a href="https://github.com/xiph/webrtc-opus-ng/tree/opus-ng">
    webrtc-opus-ng</a> fork of the Google WebRTC repository.
    Using these patches, we were able to evaluate how DRED compares to other approaches.
    And yes, it still works well even with 90% loss.
    See the results below.
    </p>

    <a href="https://opus-codec.org/demo/opus-1.5/dred_plcmos.png"><img src="https://opus-codec.org/demo/opus-1.5/dred_plcmos.png" alt="DRED PLCMOSv2 results"></a>
    <p>
    Objective evaluation of different redundancy schemes under simulated realistic
    packet loss (see <a href="#lossgen">Realistic Loss Simulator</a> below) using Microsoft's
    <a href="https://github.com/microsoft/PLC-Challenge/tree/main/PLCMOS">PLCMOS v2</a> (higher is better).
    All conditions use 48 kb/s, except for DRED+LBRR which uses 64 kb/s to fully take advantage
    of both forms of redundancy.
    Results show that even under extremely lossy conditions, DRED is able to maintain
    acceptable quality.
    It may look strange that the DRED quality increases past 60% loss, but that can be explained by
    the reduced amount of switching between regular packets and DRED redundancy.
    </p>

    <h3>Samples</h3>

    <p>Of course, hearing is believing, so here are some samples produced with the WebRTC patches.
    These should be close to what one might experience during a meeting when packets start to drop.
    Notice some gaps at the beginning as the jitter buffer adapts and is then able to take full advantage
    of DRED. </p>

    <div>
      <p><audio controls="" id="dred_player" src="https://opus-codec.org/demo/opus-1.5/samples/dred/spe48_loss_50_dred_48kbps.wav">
        Your browser does not support the audio tag.
      </audio></p><div>
      <p>Select loss rate</p>
      <ul>
        <li onclick="setDREDPercent(0, this);">0%</li>
        <li onclick="setDREDPercent(1, this);">10%</li>
        <li onclick="setDREDPercent(2, this);">20%</li>
        <li onclick="setDREDPercent(3, this);">30%</li>
        <li onclick="setDREDPercent(4, this);">40%</li>
        <li onclick="setDREDPercent(5, this);" id="dred_default_percent">50%</li>
        <li onclick="setDREDPercent(6, this);">60%</li>
        <li onclick="setDREDPercent(7, this);">70%</li>
        <li onclick="setDREDPercent(8, this);">80%</li>
        <li onclick="setDREDPercent(9, this);">90%</li>
      </ul>
      </div>
      <div>
      <p>Select redundancy</p>
      <ul id="dred_bitrate_selector">
        <li onclick="setDREDSample(0, this);">None (48 kb/s)</li>
        <li onclick="setDREDSample(1, this);">LBRR (48 kb/s)</li>
        <li onclick="setDREDSample(2, this);" id="dred_default_sample">DRED (48 kb/s)</li>
        <li onclick="setDREDSample(3, this);">DRED+LBRR (64 kb/s)</li>
      </ul>
      </div>
      <p>Select where to start playing when selecting a new sample</p>
      <p id="dred_restart_string">Player will <b>continue</b> when changing sample.</p>
    </div>
    <p>Evaluating the effectiveness of the different redundancy options. These audio samples are generated
      using real packet loss traces with the entire WebRTC stack.
    </p>

    <h2>IETF and Standardization</h2>

    <p>To ensure compatibility with the existing standard and future extensions of Opus,
    this work is being conducted within the newly-created IETF
    <a href="https://datatracker.ietf.org/wg/mlcodec/">mlcodec</a> working group.
    This effort is currently focused on three topics:
    a generic extension mechanism for Opus, deep redundancy, and speech coding enhancement.
    </p>

    <h3>Extension Format</h3>

    <p>The new DRED mechanism requires adding extra information to Opus packets while
    allowing an older decoder that does not know about DRED to still decode the regular Opus data.
    We found that the best way to achieve that was through the Opus padding mechanism.
    In the original specification, padding was added to make it possible to make a packet
    bigger if needed (e.g., to meet a constant bitrate even when the encoder produced fewer
    bits than the target).
    Thanks to padding, we can transmit extra information in a packet in a way that an
    older decoder will just not see (so it won't get confused).
    Of course, if we're going to all that trouble, we might as well make sure we're also
    able to handle any future extensions.
    Our <a href="https://datatracker.ietf.org/doc/draft-ietf-mlcodec-opus-extension/">
    Opus extension Internet-Draft</a> defines a format <i>within</i> the Opus padding
    that can be used to transmit both deep redundancy, but also any future extension
    that may become useful. See our
    <a href="https://datatracker.ietf.org/meeting/118/materials/slides-118-mlcodec-opus-extension-mechanism-00.pdf">presentation at IETF&nbsp;118</a>
    for diagrams of how the extensions fit within an Opus packet.
    </p>

    <h3>DRED Bitstream</h3>

    <p>We are also working on standardizing DRED. Standardizing an ML algorithm is
    challenging because of the tradeoff between compatibility and extensibility.
    That's why our <a href="https://datatracker.ietf.org/doc/draft-valin-opus-dred/">
    DRED Internet-Draft</a> describes how to decode extension bits into acoustic features,
    but leaves implementers free to make both better encoders and also better
    vocoders that may further improve on the quality and/or complexity.</p>

    <h3>Enhancement</h3>
    <p>
      For enhancement, we also follow the general strategy to standardize as little as
      possible, since we also expect future research to produce better methods than we
      currently have. That's why we will specify requirements an enhancement method like
      LACE or NoLACE should satisfy in order to be allowed in an opus decoder rather than
      specifying the methods themselves.
      A corresponding <a href="https://datatracker.ietf.org/doc/draft-buethe-opus-speech-coding-enhancement/">
      enhancement Internet-Draft</a>
      has already been created for that purpose.
    </p>

    <h2>Other Improvements</h2>

    <p>Here are briefly some other changes in this release.</p>

    <h3>AVX2 Support</h3>

    <p>Opus now has support and run-time detection for AVX2.
    On machines that support AVX2/FMA (from around 2015 or newer), both the new DNN
    code and the SILK encoder will be significantly faster thanks to the use of
    256-bit SIMD.</p>

    <h3>More NEON Optimizations</h3>

    <p>Existing ARMv7 Neon optimization were re-enabled for AArch64, resulting
    in more efficient encoding.
    The new DNN code can now take advantage of the Arm <i>dot product</i> extensions
    that significantly speed up 8-bit integer dot products on a Cortex-A75 or newer
    (~5 year old phones). Support is detected at run-time, so
    these optimizations are safe on all Arm CPUs.</p>

    <a name="lossgen"></a><h3>Realistic Loss Simulator</h3>
    <p>As a side effect of trying to tune the DRED encoder to maximize quality, we realized
    we needed a better way of simulating packet loss.
    For some purposes, testing with random loss patterns (like tossing a coin repeatedly)
    can be good enough, but since DRED is specifically designed to handle bust loss (which
    is rare with independent random losses) we needed something better.
    As part of the Audio Deep Packet Loss Concealment Challenge, Microsoft
    <a href="https://github.com/microsoft/PLC-Challenge">made available</a>
    some more realistic recorded packet loss traces.
    A drawback of such real data is that one cannot control the percentage of loss
    or generate sequences longer than those in the dataset.
    So we trained a generative packet loss model that can simulate realistic losses with
    a certain target overall percentage of loss.
    Packet loss traces are quite simple and our generative
    model fits in fewer than 10,000 parameters.
    To simulate loss with opus_demo, you need to build with --enable-lossgen.
    Then add -sim-loss &lt;percentage&gt; to the opus_demo command line.
    Note that the loss generator is just an initial design, so feedback is welcome.
    </p>

    <p>
    Because we believe that this loss generator can be useful to other applications,
    we have made it easy to extract it from Opus and use it in other applications.
    The main source file for the generator is <a href="https://gitlab.xiph.org/xiph/opus/-/blob/main/dnn/lossgen.c">
    dnn/lossgen.c</a>. Comments in the file contain information about the other
    dependencies needed for the loss generator.</p>

    <h2> Conclusion </h2>
    <p>
      We hope we demonstrated how our new ML-based tools substantially
      improve error robustness and speech quality with a very modest
      performance impact and without sacrificing compatibility.
      And we're only getting started. There's still more to come.
      We encourage everyone to try out these new features for themselves.
      Please <a href="https://www.opus-codec.org/contact/">let us know</a> about your experience (good or bad)
      so we can continue to improve them.
      Enjoy!
    </p>

    <address>—The Opus development team
      <br>March 4th, 2024
    </address>


    <h2>Additional Resources</h2>
    <ol>
      <li>First and foremost: <a href="https://www.opus-codec.org/">The Opus Project Homepage</a></li>
      <li>The basic Opus techniques for music coding are described in the AES paper:
      <a href="https://jmvalin.ca/papers/aes135_opus_celt.pdf">High-Quality, Low-Delay Music Coding in
      the Opus Codec</a></li>
      <li>The basic Opus techniques for speech coding are described in this other AES paper:
      <a href="https://jmvalin.ca/papers/aes135_opus_silk.pdf">Voice Coding with Opus</a></li>

      <li>Join our development discussion in <a href="irc://irc.libera.chat/opus">#opus at irc.libera.chat</a> (→<a href="https://web.libera.chat/#opus" onclick="document.getElementById('chatbox').innerHTML='<iframe src=\'https://web.libera.chat/#opus\' width=800 height=600/>';return false;">web interface</a>)</li>
    </ol>
      
    <hr>

    <div>
        <p>
          (C) Copyright 2024 Xiph.Org Foundation
        </p>
      </div>

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The women who coined the expression 'surfing the internet' (2019) (110 pts)]]></title>
            <link>https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet</link>
            <guid>39592993</guid>
            <pubDate>Mon, 04 Mar 2024 17:20:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet">https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet</a>, See on <a href="https://news.ycombinator.com/item?id=39592993">Hacker News</a></p>
Couldn't get https://www.surfertoday.com/surfing/the-woman-who-coined-the-expression-surfing-the-internet: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Improving Network Performance with Linux Flowtables (148 pts)]]></title>
            <link>https://www.ubicloud.com/blog/improving-network-performance-with-linux-flowtables</link>
            <guid>39592771</guid>
            <pubDate>Mon, 04 Mar 2024 17:05:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ubicloud.com/blog/improving-network-performance-with-linux-flowtables">https://www.ubicloud.com/blog/improving-network-performance-with-linux-flowtables</a>, See on <a href="https://news.ycombinator.com/item?id=39592771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><p><a href="https://www.ubicloud.com/"><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/64fe48116c52fe1a51e17279_ubicolud%20logo.png" loading="lazy" alt=""></a></p></div><div id="w-node-decdb48f-56e8-4c35-c577-932285e9b439-32a26126"><p>March 4, 2024 · 5 min read</p><div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e39641837eb115bbff4328_Furkan%20Sahin%20Picture.jpeg" loading="lazy" sizes="40px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e39641837eb115bbff4328_Furkan%20Sahin%20Picture-p-500.jpeg 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e39641837eb115bbff4328_Furkan%20Sahin%20Picture.jpeg 792w" alt=""></p><div><p>Furkan Sahin</p><p>Senior Software Engineer</p></div></div><div><p>We’re building an opensource alternative to AWS. Among other things, that means running a ton of VMs,which we do on Linux. We rely on Linux KVM for virtualization, and keep each VM in a separate namespace for isolation.</p><p>In a setup like this, the networking stack has to provide encryption in transit, dynamically assign public IPv4 addresses to VMs, and allow flexible firewall rules. For encryption, we can offload logic to the underlying network card, if the card supports it. This saves CPU cycles on the machine and improves VM performance. For IPv4 assignment and firewall rules, we use Linux’s <a href="https://netfilter.org/">Netfilter / Nftables</a>. This subsystem provides a powerful way to handle packets addressed to the host.</p><p>We started to wonder if we could offload some of the packet processing logic to the network card, similarly to what we can do with encryption, in order to save even more CPU cycles. While investigating, we came across <a href="https://docs.kernel.org/networking/nf_flowtable.html">flowtables</a>—a network acceleration feature in the kernel that works like a routing cache. That is, the kernel remembers the routing decisions for packets that belong to a particular connection.</p><p>When we introduced flowtables into our stack, it reduced network latencies by 7.5%, and all it took was a 7-line change. We thought that was remarkable and worth sharing! So in this post, we describe how we use Netfilter and flowtables for packet processing, and include a simple benchmark that shows flowtables’ benefits.</p></div><div id="sign-up-and-sign-in"><h3>Background</h3><div><p>Ubicloud uses an established pattern in building public cloud services. A control plane manages a data plane, where the data plane usually uses open source software. The control plane holds the data model, responds to web requests, and coordinates changes to the data plane (nodes).</p><p>For example, when the user wants to update firewall rules on a VM, we register this change with the control plane. The control plane then finds the corresponding bare metal instance running the VM and pushes changes to the data plane. Ubicloud’s data plane then reprograms the Linux hosts’ networking rules for the firewall changes to take effect.</p></div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design.jpg" loading="lazy" sizes="(max-width: 479px) 65vw, (max-width: 767px) 64vw, (max-width: 991px) 65vw, (max-width: 1439px) 50vw, 604.796875px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-500.jpg 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-800.jpg 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-1080.jpg 1080w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design-p-1600.jpg 1600w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6508570535d13621d3449e84_architecture%20design.jpg 2286w" alt=""></p><p>To reprogram the host OS, Ubicloud uses the Netfilter project. Netfilter is a framework inside the Linux kernel that provides hooks for features like packet filtering, connection tracking, and network address translation (NAT). For firewall rules, we use Netfilter’s packet filtering feature. (For assigning IPv4 addresses to VMs, we use the NAT feature.) Let’s look at how firewall rules work in a bit more detail.<br></p></div><div id="sign-up-and-sign-in"><h3>An Example: Implementing Firewall Rules</h3><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e384bbde896d9f2c34c235_Linux%20kernel%20packet%20forwarding%20path.png 964w" alt=""></p><p>Classic forwarding path for a packet. Please see Acknowledgements, CC&nbsp;BY-SA&nbsp;4.0 license<br>‍</p><div><p>The above diagram displays the classic packet forwarding path within the Linux kernel, also identifying the Netfilter hooks. To make things more concrete, let’s consider an example where we implement firewall rules in Ubicloud. In this example, on a VM, we want to allow incoming TCP connections on port 5432 for all IP addresses, and reject other traffic.</p><p>The VM has an IPv4 address of 12.12.12.12. A packet then comes to the host with the following packet contents:</p></div><ol role="list"><li>Source address: 11.11.11.11</li><li>Destination address: 12.12.12.12</li><li>Destination port: 5432</li><li>Protocol: TCP</li></ol></div><p>The routing lookup on the host detects &nbsp;that the packet isn’t for itself and needs to be forwarded to the VM. As a result, the packet traverses the Netfilter forward hook. The forward stage then has the following filter (which we configured) that needs to be applied:</p><div><pre><code>
  ip saddr 0.0.0.0/0 tcp dport 5432 ip daddr 12.12.12.12 accept
</code>
</pre></div><div><p>This rule says if the packet is coming from any address, using the TCP protocol, with destination port 5432 and destination IPv4 address 12.12.12.12, accept it. So, the forward hook simply passes the packet to the post-routing hook to be sent to the destination address. If the destination port or any of these fields doesn’t match this rule, Nftables checks for any other rules in the chain. If no other rules exist, Nftables follows the table policy to take the appropriate action.</p><p>What’s interesting here is that a lot of the work is done in the pre-routing, routing, forward, and post-routing stages.</p></div><div id="enter-billing"><h3><strong>Flowtables: Optimizing Network Traffic Handling</strong></h3><p>Flowtables is an optimization to improve network packet throughput. By remembering and reusing the connection based packet processing decisions, flowtables reduces the number of repetitive processing steps for each packet. This further reduces CPU utilization and improves network latency and throughput.</p><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38bc2b0108aa9f3d8ea2c_Linux%20kernel%20forwarding%20with%20flowtables.png 964w" alt=""></p><p>Adding flowtables into a packet's forwarding path. Please see Acknowledgements, CC&nbsp;BY-SA&nbsp;4.0 license<br>‍<br></p></div><p>The previous diagram shows how Netfilter / Nftables work when flowtables are applied. Further, since flowtables integrate nicely with Netfilter, enabling them is straightforward. In Ubicloud’s case, enabling flowtables just took <a href="https://github.com/ubicloud/ubicloud/pull/1009">seven lines of code!</a></p><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-1080.png 1080w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-1600.png 1600w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code-p-2000.png 2000w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/65e38cc8b0108aa9f3d96722_Ubicloud%20networking%20flowtables%20code.png 2200w" alt=""></p><div id="enter-billing"><h3><strong>Simple Latency Benchmarks with PostgreSQL</strong></h3><div><p>We designed a simple benchmark to measure our networking stack’s latency at the host level. For this, we created a <a href="https://www.ubicloud.com/docs/managed-postgresql/quickstart">Ubicloud PostgreSQL</a> instance and installed pgbench on the host machine. pgbench is a simple benchmarking tool provided by PostgreSQL; it’s a nice fit for simple benchmarking because we can tweak pgbench’s parameters to focus on the networking overhead.</p><p>We first initialized pgbench using its initialization option (-i) and then ran pgbench for benchmarking:</p></div><div><pre><code>
  pgbench -i -s 100 demo-pg.postgresql.ubicloud.com

  pgbench -c 1 -j 1 -T 60 -P 1 -S demo-pg.postgresql.ubicloud.com
</code>
</pre></div><div><p>By keeping the client and thread counts at one, we could better isolate flowtable optimization’s impact. Additionally, we ran pgbench directly from the host against Ubicloud’s managed PostgreSQL. This way, we could remove any variance associated with taking an actual network hope; and only measure the end-to-end latency for one pgbench SELECT query.</p><p>Our observations were clear and consistent:</p></div><ul role="list"><li>Without flowtables (our original Netfilter / Nftables implementation), the average latency was 0.127ms.<br></li><li>With flowtables, the average latency decreased to 0.118ms. This showed a latency improvement of 7.5%.</li><li>There are two ways to think about these improvements. First, we didn’t take network latency into account in this benchmark. So, we’d expect real life latency benefits to be lower.</li><li>Second, and on the flip side, most of the end-to-end latency was associated with the pgbench client sending a query to PostgreSQL and receiving the reply. We didn’t work to measure this latency. Intuitively, we’d expect flowtable’s throughput benefits (shaved off CPU cycles) to be more important than its latency benefits.</li></ul></div><div id="create-vm"><h3>Conclusion</h3><div><p>We use Netfilter / Nftables on our data plane bare metal instances to provide cloud networking services. These Linux kernel features are reliable and portable. Recently, we introduced flowtables into our networking setup at Ubicloud. The change took seven lines of code and improved latency by 7.5% in a simple application benchmark.</p><p>As we work on Ubicloud, we’re actively learning more about and making improvements to our networking layer. If you have any questions or feedback for us, we’d love to hear from you. Please feel free to drop us a line at <a href="#">info@ubicloud.com</a>.</p></div></div><div id="create-vm"><h3>Acknowledgements</h3><p>As we worked to introduce flowtables into Ubicloud’s networking stack, a significant portion of our understanding came from the blog post, <a href="https://thermalcircle.de/doku.php?id=blog:linux:flowtables_1_a_netfilter_nftables_fastpath">"Flowtables: A Netfilter nftables Fastpath"</a>. Andrej's post provides an in-depth look into flowtables and their benefits. We’d like to thank its author Andrej Stender for that comprehensive work!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia bans using translation layers for CUDA software to run on other chips (279 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/gpus/nvidia-bans-using-translation-layers-for-cuda-software-to-run-on-other-chips-new-restriction-apparently-targets-zluda-and-some-chinese-gpu-makers</link>
            <guid>39592689</guid>
            <pubDate>Mon, 04 Mar 2024 16:58:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/gpus/nvidia-bans-using-translation-layers-for-cuda-software-to-run-on-other-chips-new-restriction-apparently-targets-zluda-and-some-chinese-gpu-makers">https://www.tomshardware.com/pc-components/gpus/nvidia-bans-using-translation-layers-for-cuda-software-to-run-on-other-chips-new-restriction-apparently-targets-zluda-and-some-chinese-gpu-makers</a>, See on <a href="https://news.ycombinator.com/item?id=39592689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="AMD" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg"><source type="image/jpeg" alt="AMD" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg"><img src="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-320-80.jpg" alt="AMD" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/RR98zH9VCjMREBqoxLUAuT.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: AMD)</span>
</figcaption>
</div>

<div id="article-body"><p>Nvidia has now banned running CUDA-based software on other hardware platforms using translation layers in its updated licensing terms. This appears to be designed to prevent both the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/zluda-project-cuda-intel-gpus" data-before-rewrite-localise="https://www.tomshardware.com/news/zluda-project-cuda-intel-gpus">ZLUDA initiative</a> and, perhaps more critically, some Chinese GPU makers from utilizing CUDA code with translation layers. We've pinged Nvidia for comment and will update you with additional details or clarifications when we get a response.</p><p>

<a data-analytics-id="inline-link" href="https://twitter.com/never_released/status/1758946808183525702" data-url="https://twitter.com/never_released/status/1758946808183525702">Longhorn</a>, a software engineer, noticed the updated terms. "You may not reverse engineer, decompile or disassemble any portion of the output generated using Software elements for the purpose of translating such output artifacts to target a non-Nvidia&nbsp;platform," a new clause in CUDA 11.5 reads.</p><p>

Being a leader has a good side and a bad side. On the one hand, everyone depends on you; on the other hand, everyone wants to stand on your shoulders. The latter is apparently what has happened with CUDA. Because the combination of CUDA and Nvidia hardware has proven to be incredibly efficient, tons of programs rely on it. However, as more competitive hardware enters the market, more users are inclined to run their CUDA programs on competing platforms. There are two ways to do it: recompile the code (available to developers of the respective programs) or use a translation layer.</p><p>

For obvious reasons, using a translation layer like <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/zluda-project-cuda-intel-gpus" data-before-rewrite-localise="https://www.tomshardware.com/news/zluda-project-cuda-intel-gpus">ZLUDA</a> is the easiest way to run a CUDA program on non-Nvidia hardware. All one has to do is take already-compiled binaries and run them using ZLUDA or other translation layers. ZLUDA appears to be floundering now, with both <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/software-allows-cuda-code-to-run-on-amd-and-intel-gpus-without-changes-zluda-is-back-but-both-companies-ditched-it-nixing-future-updates" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/gpus/software-allows-cuda-code-to-run-on-amd-and-intel-gpus-without-changes-zluda-is-back-but-both-companies-ditched-it-nixing-future-updates">AMD and Intel having passed on the opportunity to develop it further</a>, but that doesn't mean translation isn't viable.</p><p>

Several Chinese GPU makers, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/chinese-gpu-developer-gets-government-funds" data-before-rewrite-localise="https://www.tomshardware.com/news/chinese-gpu-developer-gets-government-funds">including one funded by the Chinese government,</a> claim to run CUDA code. Denglin Technology designs processors featuring a "computing architecture compatible with programming models like CUDA/OpenCL." Given that reverse engineering of an Nvidia GPU is hard (unless one already somehow has all the low-level details about Nvidia GPU architectures), we are probably dealing with some sort of translation layer here, too.</p><p>

One of the largest Chinese GPU makers, Moore Threads, also has a <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/gpus/nvidias-biggest-chinese-competitor-unveils-cutting-edge-new-ai-gpus-moore-threads-s4000-ai-gpu-and-intelligent-computing-center-server-clusters-using-1000-of-the-new-ai-gpus" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/gpus/nvidias-biggest-chinese-competitor-unveils-cutting-edge-new-ai-gpus-moore-threads-s4000-ai-gpu-and-intelligent-computing-center-server-clusters-using-1000-of-the-new-ai-gpus">MUSIFY translation tool designed to allow CUDA code to work with its GPUs</a>. However, whether or not MUSIFY falls under the classification of a complete translation layer remains to be seen (some of the aspects of MUSIFY could involve porting code). As such, it isn't entirely clear if the Nvidia ban on translation layers is a direct response to these initiatives or a pre-emptive strike against future developments.</p><p>

For obvious reasons, using translation layers threatens Nvidia's hegemony in the accelerated computing space, particularly with AI applications. This is probably the impetus behind Nvidia's decision to ban running their CUDA applications on other hardware platforms using translation layers starting from CUDA 11.5.</p><p>

The clause was absent in the CUDA 11.4 release, so it looks like running applications compiled using CUDA 11.4 and earlier compilers on non-Nvidia processors using translation layers is still fine. To that end, Nvidia won't achieve its goal of preventing everyone from running software developed for its hardware on other hardware platforms using layers like ZLUDA in the short term. Longer-term, the company will certainly place legal barriers for running CUDA programs via translation layers on third-party hardware, which could have a positive effect for Nvidia and a negative one for AMD, Intel, Biren, and other developers of AI compute hardware.</p><p>

Recompiling existing CUDA programs remains perfectly legal. To simplify this, both AMD and Intel have tools to port CUDA programs to their&nbsp;<a data-analytics-id="inline-link" href="https://www.amd.com/system/files/documents/porting-cuda-to-hip.pdf" data-url="https://www.amd.com/system/files/documents/porting-cuda-to-hip.pdf">ROCm</a>&nbsp;(<a data-analytics-id="inline-link" href="https://github.com/ROCm/HIPIFY" data-url="https://github.com/ROCm/HIPIFY">1</a>) and&nbsp;<a data-analytics-id="inline-link" href="https://www.intel.com/content/www/us/en/developer/articles/technical/migrate-cuda-applications-to-oneapi-based-on-sycl.html" data-url="https://www.intel.com/content/www/us/en/developer/articles/technical/migrate-cuda-applications-to-oneapi-based-on-sycl.html">OpenAPI</a> platforms, respectively.</p><p>

As AMD, Intel, Tenstorrent, and other companies develop better hardware, more software developers will be inclined to design for these platforms, Nvidia's CUDA dominance could ease over time. Furthermore, programs specifically developed and compiled for particular processors will inevitably work better than software run via translation layers, which means better competitive positioning for AMD, Intel, Tenstorrent, and others against Nvidia — if they can get software developers on board. GPGPU remains an important and highly competitive arena, and we'll be keeping an eye on how the situation progresses in the future.</p></div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-HvbF9tUczupknDXDUAqK9o"><section><p>Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.</p></section></div>
<div id="slice-container-authorBio-HvbF9tUczupknDXDUAqK9o"><p>Anton Shilov is a Freelance News Writer at Tom’s Hardware US. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.</p></div>



<!-- Drop in a standard article here maybe? -->



</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>









</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Air Force Evaluating Pivotal's Blackfly Ultralight EVTOL Aircraft (123 pts)]]></title>
            <link>https://www.futureflight.aero/news-article/2024-02-27/us-air-force-evaluating-pivotals-blackfly-ultralight-evtol-aircraft</link>
            <guid>39592499</guid>
            <pubDate>Mon, 04 Mar 2024 16:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.futureflight.aero/news-article/2024-02-27/us-air-force-evaluating-pivotals-blackfly-ultralight-evtol-aircraft">https://www.futureflight.aero/news-article/2024-02-27/us-air-force-evaluating-pivotals-blackfly-ultralight-evtol-aircraft</a>, See on <a href="https://news.ycombinator.com/item?id=39592499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main"><p>Shortly after Pivotal officially <a href="https://futureflight.aero/news-article/2024-01-17/pivotal-launches-sales-helix-personal-evtol-aircraft-private-flyers" rel="nofollow" target="_blank">launched sales</a> of its single-seat personal aerial vehicle—a recreational eVTOL aircraft that requires no license to fly—the company’s&nbsp;“tilt-aircraft” architecture has garnered some attention from the U.S. Air Force.&nbsp;</p>
<p>Pivotal (formerly known as Opener Aero) announced on February 21 that it had delivered the first four of eight Blackfly aircraft—the pre-production version of its flagship <a href="https://futureflight.aero/news-article/2023-10-10/opener-rebrands-pivotal-unveils-helix-personal-evtol-aircraft" rel="nofollow" target="_blank">Helix</a> vehicle—to Modern Technology Solutions (MTSI), an engineering firm contracted by the U.S. Air Force’s Afwerx innovation unit to assess the tilt-aircraft’s performance for potential defense applications.&nbsp;</p>
<p>As a partner in the Afwerx Agility Prime program, Pivotal is providing pilot training and support services to MTSI and the USAF throughout the eight-month flight testing campaign, which will take place at New Braunfels National Airport in Texas as well as the National Advanced Air Mobility Center of Excellence (NAAMCE) at Springfield-Beckley Municipal Airport in Ohio. In addition to leasing eight aircraft to MTSI, Pivotal is also leasing two of its flight simulators to support training at those locations. Those two simulators were delivered along with the first batch of Blackflys, according to Pivotal.&nbsp;</p>
<p>“Over the next eight months, we will fly eight BlackFly eVTOL aircraft in different environments to test their mission effectiveness and suitability in military uses,” said Vance Drenkhahn, executive v-p of MTSI’s defense services division. “Pivotal's amazing light eVTOL platform offers an opportunity to support Afwerx and solve real-world challenges.”</p>
<article><div>
      
            <p><iframe src="https://cncms.aero/media/oembed?url=https%3A//www.youtube.com/watch%3Fv%3DtvXHWSwWvSg&amp;max_width=0&amp;max_height=0&amp;hash=bNrOAc8puMx9cym4khvvStlXEn2aVPMFovY6TKYT79E" frameborder="0" allowtransparency="" width="200" height="113" title="Pivotal | BlackFly 2023 Highlights"></iframe>
</p>
      
    </div>

  </article><p>Small, light eVTOL aircraft such as the <a href="https://www.futureflight.aero/news-article/2021-07-28/openers-blackfly-makes-rare-appearance-during-oshkosh-show" rel="nofollow" target="_blank">Blackfly</a> “have potential for several missions including special operations, surveillance, and disaster and emergency response with local command and control at a much lower price point than traditional helicopters,” said Agility Prime program lead John Tekell. The Agility Prime program has worked with various other eVTOL developers in recent years, including air taxi manufacturers such as Archer and Joby as well as other personal aerial vehicles such as <a href="https://futureflight.aero/news-article/2021-07-16/jetpack-starts-taking-preorders-speeder-vtol-vehicle" rel="nofollow" target="_blank">JetPack Aviation's Speeder</a>.&nbsp;</p>
<p>“Afwerx's and MTSI's decision to partner with Pivotal is a strong endorsement of our platform's maturity and a milestone on our journey to mission relevance,” said Pivotal CEO Ken Karklin. "With over 12 years at the forefront of light eVTOL aircraft development, it is an honor to join forces with Afwerx and MTSI to demonstrate the utility and versatility of Pivotal's patented eVTOL architecture."</p>
<p>Recreational pilots (licensed or not) can order the Pivotal Helix aircraft online at a starting price of $190,000. Because the aircraft complies with the FAA’s Part 103 rules for ultralight aircraft, operators do not need a pilot’s license to legally fly it, but Pivotal is still requiring customers to complete a training course before they can receive their deliveries. Pivotal said it aims to begin shipping the Helix to customers on June 10. Meanwhile, about a dozen customers participating in the company’s early access program have already begun receiving deliveries of the pre-production Blackfly aircraft.</p>
<p>The Helix's design features&nbsp;eight fixed rotors built into two cantilevered tandem wings. With a fully battery-electric powertrain, it can fly at a&nbsp;cruise speed of 55 knots to a range of about 20 miles (32 kilometers) with 20 percent battery reserves left, and it can fully recharge in 75 minutes with a Level 2 EV charger.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The hunt for the missing data type (571 pts)]]></title>
            <link>https://www.hillelwayne.com/post/graph-types/</link>
            <guid>39592444</guid>
            <pubDate>Mon, 04 Mar 2024 16:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hillelwayne.com/post/graph-types/">https://www.hillelwayne.com/post/graph-types/</a>, See on <a href="https://news.ycombinator.com/item?id=39592444">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en">
    

    
    

    <div>
  

<p>A (directed) <dfn>graph</dfn> is a set of nodes, connected by arrows (<dfn>edges</dfn>). The nodes and edges may contain data. Here are some graphs:</p>

<center>
<figure> 
  <img src="https://www.hillelwayne.com/post/graph-types/img/graph-examples.gv.png" title="Four graphs, with circles for nodes and arrows for edges."> 

  <figcaption>
    All graphs made with graphviz 

  <a href="https://www.hillelwayne.com/post/graph-types/img/graph-examples.gv">(source)</a>

  </figcaption>
</figure>
</center>


<p>Graphs are ubiquitous in software engineering:</p>

<ol>
<li>Package dependencies form directed graphs, as do module imports.</li>
<li>The internet is a graph of links between webpages.</li>
<li>Model checkers analyze software by exploring the “state space” of all possible configurations. Nodes are states, edges are valid transitions between states.</li>
<li>Relational databases are graphs where the nodes are records and the edges are foreign keys.</li>
<li>Graphs are a generalization of linked lists, binary trees, and hash tables.<sup id="fnref:hash-tables"><a href="#fn:hash-tables">1</a></sup></li>
</ol>

<p>Graphs are also widespread in business logic. Whitepapers with references form graphs of citations. Transportation networks are graphs of routes. Social networks are graphs of connections. If you work in software development long enough, you will end up encountering graphs <em>somewhere</em>.</p>

<p>I see graphs everywhere and use them to analyze all sorts of systems. At the same time, I dread actually using graphs in my code. There is almost no graph support in any mainstream language. None have it as a built-in type, very few have them in the standard library, and many don’t have a robust third-party library in the ecosystem. Most of the time, I have to roll graphs from scratch. There’s a gap between how often software engineers could use graphs and how little our programming ecosystems support them. Where are all the graph types?</p>

<p>As I ran into more and more graphs in my work, this question became more and more intriguing to me. So late last year I finally looked for an answer. I put a <a href="https://buttondown.email/hillelwayne/archive/if-you-work-on-a-big-language-id-like-to-talk/">call out</a> on <a href="https://buttondown.email/hillelwayne/">my newsletter</a> asking for people with relevant expertise— graph algorithm inventors, language committee members, graph library maintainers— to reach out. I expected to interview a dozen people, but in the end I only needed to talk to four:</p>

<ol>
<li><strong>Zayenz</strong>: Former core developer of the <a href="https://www.gecode.org/">Gecode constraint solver</a>, and who has “implemented every graph algorithm there is”</li>
<li><strong>Bradford</strong>: Author of the <a href="https://github.com/praetorian-inc/noseyparker/">Nosey Parker</a> security library and inventor of several new graph algorithms</li>
<li><strong><a href="https://ntietz.com/">Nicole</a></strong>: Former graph database engineer</li>
<li><strong>Kelly</strong>: Maintainer on the <a href="https://networkx.org/">NetworkX</a> python graph library and <a href="https://github.com/boothby/repiet">compiler developer</a>.</li>
</ol>

<p>After these four people all gave similar answers, I stopped interviewing and start writing.</p>

<h2 id="the-reasons">The reasons</h2>

<h3 id="there-are-too-many-design-choices">There are too many design choices</h3>

<p>So far I’ve been describing <em>directed</em> graphs. There are also <em>undirected</em> graphs, where edges don’t have a direction. Both directed and undirected graphs can either be <dfn>simple graphs</dfn>, where there is a maximum of one edge between two nodes, or <dfn>multigraphs</dfn>, where there can be many edges. And then for each of <em>those</em> types we have hypergraphs, where an edge can connect three or more nodes, and ubergraphs, where edges can point to other edges. For each possible variation you have more choices to make: do you assign ids to edges or just to nodes? What data can be stored in a node, and what can be stored in an edge? That’s a lot of decisions for a library to make!</p>

<p>But wait, do these distinctions matter at all? A simple graph is just a degenerate multigraph, and and undirected edge can be losslessly transformed into two directed edges. A language could just provide directed hyperubermultigraphs and let users restrict it however they want.</p>

<p>There are two problems with this. First of all, it changes the interface, like whether various operations return single values or lists. Second, as I’ll discuss later, graph algorithm performance is a serious consideration and the special cases <em>really matter</em>. Kelly raised the example of <a href="https://en.wikipedia.org/wiki/Maximum_weight_matching">maximum weight matching</a>. If you know that your graph is “bipartite”, you can use a particular fast algorithm to find a matching, while for other graphs you need to use a slow, more general algorithm.</p>

<center>
<figure> 
  <img src="https://www.hillelwayne.com/post/graph-types/img/bipartite.gv.png" title="A bipartite graph"> 

  <figcaption>
    A bipartite graph 

  <a href="https://www.hillelwayne.com/post/graph-types/img/bipartite.gv">(source)</a>

  </figcaption>
</figure>
</center>


<blockquote>
<p>[It] ties back to the “algorithm dispatch problem.”  Given a Problem P, a Graph G, and Algorithms A, B, C to solve P on G… which one do you run?  If we don’t know that G is bipartite, and Algorithm C only works on bipartite graphs, how much time can we afford to determine whether or not G is bipartite? — <em>Kelly</em></p>
</blockquote>

<p>The perfect graph library would support a lot of different kinds of graphs. But that takes time away from supporting what people want to <em>do</em> with graphs. Graph algorithms are notoriously hard to get right. In <a href="https://www.python.org/doc/essays/graphs/">this essay</a>, the inventor of Python implemented his own <code>find_shortest_path</code> algorithm. It had to be updated with corrections five times!</p>

<blockquote>
<p>Every single implementation of pagerank that I compared to was wrong. — <em>Nicole</em></p>
</blockquote>

<p>So which algorithms should come with the library? “The amount of things people want to do with graphs is absurd,” Kelly told me. That matches my experience, and the experiences of all my interviewees. It sometimes seems like graphs are <em>too powerful</em>, that all their possibilities are beyond my understanding. “The question is,” Kelly said, “where do you draw the line?”</p>

<p>For NetworkX, “the line” is approximately 500 distinct graph algorithms, by themselves making up almost 60,000 lines of code. By comparison, the entire Python standard library, composed of 300 packages, is just under 600,000 lines.<sup id="fnref:derivation"><a href="#fn:derivation">2</a></sup></p>

<p>With all that, it’s unsurprising that you don’t see graphs in standard libraries. The language maintainers would have to decide which types of graphs to support, what topologies to special-case, and what algorithms to include. It makes sense to push this maintenance work onto third parties. This is already the mainstream trend in language development; even Python, famous for being “batteries included”, is <a href="https://peps.python.org/pep-0594/">removing 20 batteries</a>.</p>

<p>Third parties can make opinionated decisions on how to design graphs and what algorithms to include. But then they’re faced with the next problem: once you have a graph interface, how do you represent it?</p>

<h3 id="there-are-too-many-implementation-choices">There are too many implementation choices</h3>

<p>Let’s imagine we’re supporting only barebones simple directed graphs: nodes have identities, edges do not, neither has any associated data. How do we encode this graph?</p>

<center>
<figure> 
  <img src="https://www.hillelwayne.com/post/graph-types/img/many-encodings.gv.png" title="A graph diagram of `a -> b -> c -> {a, b}`"> 

  <figcaption>
     

  <a href="https://www.hillelwayne.com/post/graph-types/img/many-encodings.gv">(source)</a>

  </figcaption>
</figure>
</center>


<p>Here are four possible ways a programming language could internally store it:</p>

<ol>
<li>Edge list: <code>[[a, b], [b, c], [c, a], [c, b]]</code></li>
<li>Adjacency list: <code>[[b], [c], [a, b]]</code></li>
<li>Adjacency matrix: <code>[0 1 0; 0 0 1; 1 1 0]</code></li>
<li>A set of three structs with references to each other</li>
</ol>

<p>Different graph operations have different performance characteristics on different representations. Take a directed graph with 100 nodes and 200 edges. If we use an adjacency matrix representation, we need a 100×100 matrix containing 200 ones and 9,800 zeros.  If we instead use an edge list we need only 200 pairs of nodes. Depending on your PL and level of optimizations that could be a memory difference of 20x or more.</p>

<p>Now instead take a graph with 100 nodes and 8,000 edges and try to find whether an edge exists between node 0 and node 93. In the matrix representation, that’s an O(1) lookup on <code>graph[0][93]</code>. In the edge list representation, that’s an O(|edge|) iteration through all 8,000 edges.<sup id="fnref:variations"><a href="#fn:variations">3</a></sup></p>

<p>Graphs with only a few edges are <dfn>sparse</dfn> and graphs with almost all edges are <dfn>dense</dfn>. The same program may need to do both operations on both kinds of graph topologies: if you’re constructing a graph from external data, you could start out with a sparse graph and later have a dense one. There’s no “good option” for the internal graph representation.</p>

<p>And all this trouble is just for the most barebones directed graph! What about implementing node data? Edge data? Different types of nodes and edges? Most third party libraries roughly fall in one of two categories:</p>

<ol>
<li><p>Offer a single rich datatype that covers all use-cases at the cost of efficiency. NetworkX stores graph as a dict of dicts of dicts, so that both nodes and edges can have arbitrary data.<sup id="fnref:networkx-representations"><a href="#fn:networkx-representations">4</a></sup></p></li>

<li><p>Offer separate graph types for each representation, and rely on the user to store node and edge data separately from the graph type.</p></li>
</ol>

<p>An example of the second case would be <a href="https://docs.rs/petgraph/latest/petgraph/index.html">Petgraph</a>, the most popular graph library for Rust. Petgraph has <code>graph</code>, <code>graphmap</code>, and <code>matrix_graph</code> for different use-cases. Bradford used Petgraph for <a href="https://github.com/praetorian-inc/noseyparker/">Nosey Parker</a>, a security tool that scans for secrets across an entire history of a git repo. His benchmarking graph is CPython, which has 250k commits and 1.3M objects but only a few edges per commit node. He went with an adjacency list.</p>

<p>Supporting many representations has a serious downside: you have to do a lot more work to add algorithms. If you write a separate version of the algorithm for each graph representation, you’re tripling or quadrupling the maintenance burden. If you instead write a generic abstraction over polymorphic types, then your library is less performant. One programmer I talked to estimated that a hand-rolled graph algorithm can be 20x faster or more than a generic algorithm.</p>

<p>And this gets into every interviewee’s major complaint.</p>

<h3 id="performance-is-too-important">Performance is too important</h3>

<blockquote>
<p>A “generic” graph implementation often doesn’t cut it. <em>— Bradford</em></p>
</blockquote>

<p>This is the big one.</p>

<p>Many, many graph algorithms are NP-complete or harder.<sup id="fnref:np"><a href="#fn:np">5</a></sup> While NP-complete is often tractable <a href="https://www.hillelwayne.com/post/np-hard/">for large problems</a>, graphs can be <em>enormous</em> problems. The choice of representation plays a big role in how fast you can complete it, as do the specifics of your algorithm implementation.</p>

<p>Everyone I talked to had stories about this. In Nosey Parker, Bradford needed to reconstruct a snapshot of the filesystem for each commit, which meant traversing the object graph. None of the <a href="https://docs.rs/petgraph/latest/petgraph/visit/index.html">four provided graph walkers</a> scaled to his use case. Instead he had to design a “semi-novel” <a href="https://github.com/praetorian-inc/noseyparker/blob/aaacceaa4baf0fb6a9c98c95b9b063ed74654349/crates/input-enumerator/src/git_metadata_graph.rs#L337">graph traversal algorithm</a> on the fly, which reduced the memory footprint by a factor of a thousand.</p>

<blockquote>
<p>I was able to get working a proof of concept pretty quickly with [petgraph], but then… this is one of those cases where the performance constraints end up meeting reality. <em>— Bradford</em></p>
</blockquote>

<p>Zayenz raised a different problem: what if the graph is simply too big to work with? He gave the example of finding a solution to the <a href="https://en.wikipedia.org/wiki/15_Puzzle">15 puzzle</a>. This is done by running a <a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A* search</a> on the state space. A state space with <a href="https://mediatum.ub.tum.de/doc/1283911/60434.pdf">over 20 trillion states</a>.</p>

<blockquote>
<p>If you generate all the nodes, you’ve lost already. — <em>Zayenz</em></p>
</blockquote>

<p>Zayenz oversaw one research project to add graphs to the Gecode constraint solver. They eventually found that a generic graph type simply couldn’t compete with handpicking the representation for the problem.</p>

<p>Even graph databases, designed entirely around running complex graph algorithms, struggle with this problem. Nicole, the graph database engineer, told me about some of the challenges with optimizing even basic graph operations.</p>

<blockquote>
<p>If you’re doing a traversal, you either have to limit your depth or accept you’re going to visit the entire graph. When you do a depth search, like “go out three steps from this and find the path if it exists”, then you’re just committing to visiting quite a bit of data. <em>— Nicole</em></p>
</blockquote>

<p>After leaving that job, she worked as a graph query performance consultant. This usually meant migrating off the graph database. She told me about one such project: to speed the graph queries up, she left one computation as-is and rewrote the rest as MapReduce procedures. “Which was a lot harder to understand,” she said, “But would actually finish overnight.”</p>

<p>All of this means that if you have graph problems you want to solve, you need a lot of control over the specifics of your data representation and algorithm. You simply cannot afford to leave performance on the table.</p>

<h2 id="it-was-unanimous">It was unanimous</h2>

<p>So, the reasons we don’t have widespread graph support:</p>

<ul>
<li>There are many different kinds of graphs</li>
<li>There are many different representations of each kind of graph</li>
<li>There are many different graph algorithms</li>
<li>Graph algorithm performance is very sensitive to graph representation and implementation details</li>
<li>People run very expensive algorithms on very big graphs.</li>
</ul>

<p>This explains why languages don’t support graphs in their standard libraries: too many design decisions, too many tradeoffs, and too much maintenance burden. It explains why programmers might avoid third party graph libraries, because they’re either too limited or too slow. And it explains why programmers might not want to think about things in terms of graphs except in extreme circumstances: it’s just too hard to work with them.</p>

<p>Since starting this research, I’ve run into several new graph problems in my job. I still appreciate analyzing systems as graphs and dread implementing them. But now I know why everybody else dreads them, too. Thank you for reading!</p>

<p><em>Thanks to <a href="https://predr.ag/">Predrag Gruevski</a> for research help, <a href="https://lars.hupel.info/">Lars Hupel</a>, <a href="https://predr.ag/">Predrag Gruevski</a>, <a href="https://www.danluu.com/">Dan Luu</a>, and <a href="https://medium.com/@bellmar">Marianne Bellotti</a> for feedback, and to all of the people who agreed to do interviews. If you liked this post, come join my <a href="https://buttondown.email/hillelwayne/">newsletter</a>! I write new essays there every week.</em></p>

<p><em>I train companies in formal methods, making software development faster, cheaper, and safer. Learn more <a href="https://www.hillelwayne.com/consulting/">here</a>.</em></p>

<hr>

<h2 id="appendix-languages-with-graph-types">Appendix: Languages with Graph Types</h2>

<h3 id="graph-querying-languages">Graph Querying Languages</h3>

<p>Graph querying languages (GQLs)<sup id="fnref:GQL"><a href="#fn:GQL">6</a></sup> are to graph databases what SQL is to relational databases. There is no widely-used standard, but two of the most popular are <a href="https://www.w3.org/TR/sparql11-query/">SPARQL</a> for querying <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">RDF triples</a> and Neo4j’s <a href="https://neo4j.com/product/cypher-graph-query-language/">cypher</a>. Ironically, <a href="https://graphql.org/">GraphQL</a> is <a href="https://graphql.org/faq/#is-graphql-a-database-language-like-sql"><em>not</em></a> a graph querying language, instead being named for its connection to the <a href="https://en.wikipedia.org/wiki/Facebook_Graph_Search">Facebook Graph Search</a>. I considered graph databases themselves mostly distinct from graphs in programming languages, but their query languages show how graphs could work in a PL.</p>

<p>The main difference between all GQLs and SQL is that the “joins” (relationships) are first-class entities. Imagine a dataset of movies and people, where people act in, direct, or produce movies. In SQL you’d implement each relationship as a many-to-many tables, which makes it easy to query “who acted in movie X” but hard to query “who had any role in movie Y, and what was that role”. In SPARQL relationships are just edges, making the same query easy.</p>
<div><pre><code data-lang="sparql"><span></span><span>PREFIX</span> <span>mv:</span> <span>&lt;your_movie_ontology_URL&gt;</span>
<span>SELECT</span> <span>?person</span> <span>?role</span>
<span>WHERE</span> <span>{</span>
    <span>?person</span> <span>?role</span> <span>mv:</span><span>casablanca</span><span>.</span>
<span>}</span>
</code></pre></div>

<p>Cypher has a similar construct. GQLs can also manipulate edges: reverse them, compose them together, take the transitive closure, etc. If we wanted to find all actors with some degree of separation from Kevin Bacon, we could write</p>
<div><pre><code data-lang="sparql"><span></span><span>PREFIX</span> <span>mv:</span> <span>&lt;your_movie_ontology_URL&gt;</span>
<span>SELECT</span> <span>?a</span>
<span>WHERE</span> <span>{</span>
    <span>mv:</span><span>kbacon</span> <span>(:</span><span>acted_in/</span><span>^:</span><span>acted_in</span><span>)</span><span>+</span> <span>?a.</span>
    <span># a/b = join two lookups</span>
    <span># ^a = reverse a</span>
    <span># a+ = transitive closure</span>
<span>}</span>
</code></pre></div>

<p>SPARQL cannot give the length of the path nor do computation <em>along</em> the path, like collecting the chain of movies linking two actors. GQLs that support this are significantly more complicated.</p>

<p>My main takeaway from looking at GQLs is that there’s a set of useful traversal primitives that a PL with graph support would need to provide. Interestingly, the formal specification language <a href="https://alloytools.org/">Alloy</a> has all of these primitives for its “relation” datatype. For this reason I find working with a graph representation in Alloy much easier than in a proper programming language. That said, these all work with labeled edges and may not work for other graph representations.</p>

<h3 id="mainstream-languages-with-graphs-in-the-standard-library">Mainstream Languages with Graphs in the Standard Library</h3>

<p><strong>Python</strong> added a <a href="https://docs.python.org/3/library/graphlib.html">graphlib</a> in 2020. Based on the discussion <a href="https://bugs.python.org/issue17005">here</a>, it was because topological sorting is a “fundamental algorithm” and it would be useful for “pure Python implementations of MRO [Method Resolution Order] logic”. Graphlib has no other methods besides <code>TopologicalSorter</code>, which only takes graphs represented as node dicts. Unusually, the direction of the node dict is <em>reversed</em>: the graph <code>a -&gt; b</code> is represented as <code>{b: [a]}</code>.</p>

<p>As of 2023, nothing in <a href="https://github.com/search?q=repo%3Apython%2Fcpython%20TopologicalSorter&amp;type=code">CPython uses graphlib</a> and there are <a href="https://github.com/search?q=%2F%28from%7Cimport%29+graphlib%2F+language%3Apython+NOT+is%3Afork&amp;type=code">fewer than 900 files referencing it on Github</a>. By comparison, another package added in 2020, zoneinfo, appears in over 6,000 files, and the term <code>def topological_sort(</code> appears in 4,000. I’d guess a lot of these are from before 2020, though. Some skimming suggests that all of these custom topological sorts take different graph representations than graphlib, so they wouldn’t be convertable regardless. Graph representation matters.</p>

<p>There are two other languages I found with graph types: <a href="https://www.erlang.org/doc/man/digraph.html#">Erlang</a> and <a href="https://www.swi-prolog.org/pldoc/man?section=ugraphs">SWI-Prolog</a>. I don’t know either language and cannot tell when they were added; with Erlang, at least, it was before 2008. I reached out to a person on the Erlang core language committee but did not hear back.</p>

<h3 id="graph-languages">Graph languages</h3>

<p>Programming languages where “everything is a graph” in the same way that everything in bash a string and everything in lisp is a list. Some examples include <a href="https://github.com/UoYCS-plasma/GP2">GP2</a> and <a href="http://jenshweber.github.io/grape/">Grape</a>. Based on some correspondence with people in the field, right now this is still highly academic.</p>

<h3 id="mathematics-software-languages">Mathematics Software Languages</h3>

<p>Mathematica, MATLAB, Maple, etc all have graph libraries of some form or another. I am not paying the thousands of dollars in licensing needed to learn more.</p>


</div>

    



  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MacPad: I created the hybrid Mac-iPad laptop and tablet that Apple won't make (150 pts)]]></title>
            <link>https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/</link>
            <guid>39592288</guid>
            <pubDate>Mon, 04 Mar 2024 16:30:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/">https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/</a>, See on <a href="https://news.ycombinator.com/item?id=39592288">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
<p id="p2">It all started because I wanted a better keyboard for my Vision Pro. I had no idea that, in looking for one, I’d accidentally create the hybrid Apple computer of my dreams.</p>
<p id="p3">As I quickly discovered after working on the Vision Pro daily, you can get by without an external trackpad, but a keyboard is necessary if you want to type something longer than a passcode. That’s where my journey began: if I wanted to write and edit articles on the Vision Pro, what would the best keyboard-trackpad setup be?</p>
<p id="p4">Over the past few weeks, I’ve tested all the options at my disposal. I started with an Apple Magic Trackpad and Keyboard, which I then placed inside a <a href="https://amzn.to/49S9jJw" rel="noopener noreferrer">Twelve South MagicBridge</a> (it was too uncomfortable to put on my lap for longer stretches of time). Next, I tried using <a href="https://amzn.to/3STG9mM" rel="noopener noreferrer">different</a> <a href="https://amzn.to/3TgPTbO" rel="noopener noreferrer">types</a> of “trays” for these two accessories that offered a laptop-like layout (comfort was better, but lack of palm rejection was an issue). I even attempted to revive an <a href="https://amzn.to/3SXlr5j" rel="noopener noreferrer">old Brydge keyboard</a> and use it with the Vision Pro, but, alas, third-party trackpads aren’t supported on visionOS at the moment.</p>
<p id="p5">Eventually, I settled on the solution that I should have known was coming for me all along: the best keyboard and trackpad combo for a Vision Pro is a Mac laptop. So I started using my MacBook Air every day, taking advantage of <a href="https://support.apple.com/en-us/HT213971" rel="noopener noreferrer">Mac Virtual Display</a> and <a href="https://support.apple.com/en-us/102459" rel="noopener noreferrer">Universal Control</a> to get work done with the Vision Pro in a mix of classic desktop apps and new visionOS experiences. I’ll write more about this soon, but, so far, it’s felt powerful and flexible in a way that iPadOS hasn’t made me feel in a while.</p>
<p id="p6">But something kept nagging me.</p>

<p id="p9">Why wasn’t <em>Apple</em> making that kind of combined accessory for the Vision Pro? More importantly for my use case: I had a screen physically attached to my MacBook Air, but I rarely looked at it. If I’m using my MacBook Air with Mac Virtual Display or, when I’m at my desk, in clamshell mode with my Studio Display, do I even need a screen to be permanently attached to a computer that I primarily use as a keyboard and trackpad for other displays?</p>
<p id="p10">Just as I was pondering these thoughts, <a href="https://www.macstories.net/linked/can-you-use-a-headless-macbook-air-with-a-vision-pro/" rel="noopener noreferrer">Luke Miani’s video appeared</a> like a sign from the universe. The concept behind the video was fascinating: if a Vision Pro can provide you with a virtual display for your Mac, can you physically remove the display from a MacBook and continue using it in “headless” mode with a Vision Pro or other external monitors? The answer is <a href="https://www.youtube.com/watch?v=QUa_pPUbpGQ" rel="noopener noreferrer">yes</a>.</p>
<p id="p11">That video planted an idea in my brain that I couldn’t get rid of. I could see myself working with that type of accessory; removing the screen from my MacBook Air would make it even lighter to carry around and put on my lap; it would also mean I could get rid of the standalone Magic Keyboard and Trackpad on my desk and just use the remaining part of the MacBook Air for input with my Studio Display.</p>
<p id="p12">I was tempted, but the more I thought about it, the more I realized there was an obvious missing piece. What about those times when you do want to have a screen attached to your MacBook Air because you can’t or don’t want to wear a Vision Pro?</p>
<p id="p13">And <em>that</em> was the turning point, and why I needed you to follow along with this introduction, so you can understand – I hope – how my brain operates.</p>
<p id="p14">You see, I’d been thinking about creating a headless MacBook Air and relying on Universal Control, but I was only considering one side of the story – the Mac-to-Vision Pro side. It was only when I remembered that Mac-to-iPad Universal Control and Sidecar also existed that everything clicked:</p>
<p id="p15">I didn’t just want to make a headless MacBook anymore. I had to figure out how to combine the MacBook Air and iPad Pro into a single machine.</p>
<p id="p16">For the past three weeks, I’ve been using something I call a “MacPad” as my new laptop. The MacPad is a hybrid device that serves multiple purposes:</p>
<ul id="ul17"><li>It’s a keyboard and trackpad for my Vision Pro;</li>
<li>It’s a Mac with a detachable display;</li>
<li>It’s an iPad Pro with an external keyboard and trackpad.</li>
</ul><p id="p18">You know where this is going. These aren’t three separate devices: it’s <strong>one</strong> computer made of, well, <strong>two</strong> computers working together thanks to the magic of Apple’s ecosystem. It’s a Mac with an iPad display that I can detach and use as a tablet whenever I want; it’s an iPad that transforms into a Mac when docked. And, it’s the ideal keyboard and trackpad accessory for the Vision Pro.</p>
<p id="p19">In researching keyboard options for the Vision Pro, I ended up building the convertible Apple laptop-tablet that I so desperately want the company to make.</p>
<p id="p20">Let me explain how.</p>
<h2>Table of Contents</h2><ul><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#setting-up-a-headless-mac"><span>Setting Up a Headless Mac</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#attaching-the-ipad-pro-to-the-macbook-air"><span>Attaching the iPad Pro to the MacBook Air</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#using-sidecar-to-turn-an-ipad-into-a-mac-display"><span>Using Sidecar to Turn an iPad into a Mac Display</span></a><ul><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#macos-and-ipados-together"><span>macOS and iPadOS, Together</span></a></li></ul></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#questions-tidbits-and-other-setups"><span>Questions, Tidbits, and Other Setups</span></a><ul><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#what-about-sidecar-without-wi-fi"><span>What About Sidecar without Wi-Fi?</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#what-about-unlocking-the-mac"><span>What About Unlocking the Mac?</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#ipados-mode-with-universal-control"><span>iPadOS Mode with Universal Control</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#ipad-as-a-tablet"><span>iPad as a Tablet</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#headless-mac-at-a-desk-with-a-studio-display"><span>Headless Mac at a Desk with a Studio Display</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#headless-mac-as-keyboard-and-trackpad-input-for-the-vision-pro"><span>Headless Mac as Keyboard and Trackpad Input for the Vision Pro</span></a></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#more-questions"><span>More Questions</span></a></li></ul></li><li><a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/#my-future-is-a-macpad"><span>My Future Is a MacPad</span></a></li></ul><h2 id="setting-up-a-headless-mac">Setting Up a Headless Mac</h2>
<p id="p25">Alright, so obviously the first step of the process is to physically remove the screen from a MacBook, right? Well, not quite. Let me share some advice first.</p>
<p id="p26">As I was considering this procedure for my own MacBook Air, I started wondering what would happen if my headless Mac shut down or had to reboot and I didn’t have a monitor to see the login screen and type my password. <a href="https://support.apple.com/guide/ipad/use-ipad-as-a-second-display-for-mac-ipad2b1aa3be/ipados" rel="noopener noreferrer">Sidecar</a>, which is the Apple software that turns an iPad into a Mac display, can only be activated from an unlocked Mac; <a href="https://support.apple.com/en-us/102442" rel="noopener noreferrer">Apple Watch unlock</a> for macOS only works after you’ve logged in with a password at least once after a macOS restart.</p>
<p id="p27">This may be a dealbreaker to some, but to overcome this issue, I created a stronger password for my Mac and disabled <a href="https://support.apple.com/guide/mac-help/protect-data-on-your-mac-with-filevault-mh11785/mac" rel="noopener noreferrer">FileVault encryption</a> so that I could use <a href="https://support.apple.com/guide/remote-desktop/set-up-a-computer-running-vnc-software-apdbed09830/mac" rel="noopener noreferrer">VNC</a> access at the macOS login screen, even after a reboot.<sup id="fnref-74584-loginScreen"><a href="#fn-74584-loginScreen" rel="noopener noreferrer">1</a></sup></p>
<p id="p28">I disabled FileVault in System Settings ⇾ Privacy &amp; Security ⇾ FileVault, made sure that screen sharing was enabled under System Settings ⇾ General ⇾ Sharing, and shut down my MacBook Air before removing the display, as a test.</p>


<p id="p31">I then powered on the MacBook Air again, selected the computer I previously configured in <a href="https://edovia.com/en/screens/" rel="noopener noreferrer">Edovia’s Screens VNC client</a> and the app successfully showed me the macOS login screen even after my Mac had just rebooted:</p>

<p id="p33">That was the first issue I wanted to figure out upfront, and my theory worked. As long as you have FileVault disabled and both a headless Mac and VNC client (which, in my case, is running on an iPad) are connected to the same network, screen sharing is going to work at the login screen to type in your password. For subsequent unlocks, of course, you can rely on Touch ID, Apple Watch unlock, or <a href="https://www.macstories.net/shortcuts/#wake-mac-login" rel="noopener noreferrer">the Shortcuts app with SSH scripts</a>, as we’ll see below.<sup id="fnref-74584-captureCard"><a href="#fn-74584-captureCard" rel="noopener noreferrer">2</a></sup></p>
<p id="p34">Next, you may have heard that to use Sidecar as an external monitor for a headless Mac, you also have to buy a “dummy” HDMI adapter to make macOS believe it still has a main display connected to it. This is only partially true. If you want to use Sidecar with a headless Mac mini that is not connected to an actual monitor, then yes: you have to buy a dummy HDMI plug. But with the method I followed to remove the display from my MacBook Air, it’s not necessary: macOS will continue to think there is a built-in display attached to the computer, even after it’s been physically disconnected from the display connector inside the chassis. If you plan on doing this with a Mac laptop, you won’t need an extra video-out adapter.</p>


<p id="p37">And now for the main event: to make my 13-inch M2 MacBook Air a headless computer, I followed – word for word – <a href="https://www.youtube.com/watch?v=QUa_pPUbpGQ" rel="noopener noreferrer">Luke Miani’s walkthrough on YouTube</a>, which you can watch below:</p>
<p><span><iframe type="text/html" width="640" height="360" src="https://www.youtube.com/embed/QUa_pPUbpGQ?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span></p>
<p id="p39">My advice: before you do this, watch the video twice and make a note of all the equipment you’re going to need. I recommend the <a href="https://amzn.to/4bQgAeN" rel="noopener noreferrer">iFixit toolkit</a> and a separate <a href="https://amzn.to/3UXskpV" rel="noopener noreferrer">set of Torx screwdrivers</a> with all the sizes mentioned in the video. It’s not a hard procedure, but you’ll have to be careful, particularly when removing the shield from the battery connector and unplugging it. If you’re brave enough to do this, make sure you’re working on a clean surface where you can keep track of all the shields and screws you’re removing from the computer; don’t rush the job and watch every step multiple times, even if you think you know what you’re doing.</p>


<p id="p42">Once I successfully removed the display, I then put back some of the shields that were covering all the connectors and hinges inside the Air’s chassis, but – like Miani – I eventually ran out of patience and sealed everything again without re-applying all the shields. (It’s been fine so far.)</p>

<p id="p44">It was finally the moment of truth: I had to turn on my Mac, wait a few seconds, and see if I could log in the first time using VNC on my iPad. I have to be honest: it was a very stressful 30 seconds after I pressed the power button and the Air’s trackpad wasn’t clicking. But then the keyboard lit up, the Air showed up in the list of nearby devices in Screens, and I knew I’d done it:</p>

<p id="p46">I was “remotely” logged into my headless MacBook Air from my iPad, but, of course, using it as an actual display meant I had to set up Sidecar. And not only that, but I also had to find out if I could <strong>initiate a Sidecar connection from the iPad itself</strong> rather than do it from macOS every time.</p>
<p id="p47">I’ll get to that in a minute. First, I had to create an adjustable display mount for my new iPad display.</p>
<h2 id="attaching-the-ipad-pro-to-the-macbook-air">Attaching the iPad Pro to the MacBook Air</h2>
<p id="p48">If I wanted the hybrid laptop-tablet of my dreams that Apple won’t make, I’d need a system to tear my MacBook’s display away, so I could use it as a tablet. That’s the computer I want Apple to make; for now, I’d have to create my own version by relying on some old friends:</p>
<p id="p49">Magnets.</p>
<p id="p51">The idea came naturally as soon as I pictured the iPad Pro as a Sidecar display floating above the MacBook Air’s keyboard: to mimic a laptop configuration, I could rely on adjustable magnetic mounts to provide a sturdy, yet flexible mounting option for the iPad Pro. Even better, if I used enough magnets and positioned them just right, I could even support a wider viewing angle than the MacBook Air typically allows for.</p>
<p id="p52">Once again, I used the <a href="https://amzn.to/3SUCepI" rel="noopener noreferrer">Rolling Square Edge Pro Core</a> mounts, which you may remember from my story about <a href="https://www.macstories.net/ipad/how-i-modded-my-ipad-pro-with-a-screen-protector-iphone-holder-and-magnetic-stereo-speakers/" rel="noopener noreferrer">attaching an iPhone and puck-style speakers</a> to my iPad Pro. First, I attached three adhesive magnetic bases to the bottom of my MacBook Air, like so:</p>

<p id="p54">As you can see, I attached the bases as close to the chassis’ edge as I could. I did that so that the adjustable mounting clips could extend as far as possible both toward the Air’s keyboard <em>and</em> away from it. By doing this, thanks to the Edge Pro Core’s widely adjustable angle, I can even lay the display flat on a table if I want to:</p>



<p id="p58">Then came my next idea: crafting a custom magnetic cover for the iPad Pro that would also attach to the Rolling Square magnets on the MacBook Air.</p>
<p id="p59">While I stuck three magnetic bases to the bottom of the MacBook Air – which is something I never see or touch – I didn’t want to attach three magnets to the back of my iPad Pro. Here’s what I did instead: I took an <a href="https://amzn.to/3UWHonx" rel="noopener noreferrer">ESR smart cover for the iPad Pro</a> (a cheaper alternative to Apple’s Smart Folio that I’ve been using for years now) and used a knife to separate the back cover from the front. I also cut out the Apple Pencil flap since I wasn’t going to use it. Then, I lined up three magnets with the Rolling Square clips, attached them to the smart cover, and <em>voila</em>: I created a lightweight mounting system that would allow me to easily and quickly tear away the MacBook’s display and use it as an iPad.</p>


<p id="p62">When I say “tear away the display”, I mean it. Here’s how simple it is to go from MacBook mode to iPad mode when I’m done working with macOS:</p>

<p id="p64">I can also choose to detach the iPad <em>without</em> its cover:</p>

<p id="p66">I’ve been working with this magnetic attachment system for the past couple of weeks, and it’s been terrific. The more adjustable viewing angles have allowed me to use this computer while lying on the couch or in bed and tilt the display back further than a typical MacBook Air. Three magnets provide a connection strong enough to walk around the house with this computer without the fear of the iPad detaching from the Air’s chassis. The final product, as I’ll explain later, is heavier than a MacBook Air – but it also carries the benefits of two OSes in the same machine.</p>
<p id="p67">Is there a more elegant and practical solution that doesn’t involve sticking magnets to a MacBook Air or cutting a smart cover by hand? I’m sure there is. Perhaps someone with a 3D printer and more skills than yours truly could design a custom mounting system that integrates nicely with the Air’s display hinges and chassis. However, given my lack of expertise and resources, I think the modular system I designed is pretty good, and it does the job well.</p>
<h2 id="using-sidecar-to-turn-an-ipad-into-a-mac-display">Using Sidecar to Turn an iPad into a Mac Display</h2>
<p id="p68">Enough about hardware. It’s time to get to the meaty part of this story: Sidecar.</p>
<p id="p69">We’ve been able to use an iPad as an extended display for macOS for the past five years – since macOS Catalina and iPadOS 13, to be exact. Sidecar has barely changed over the years, so what <a href="https://www.macstories.net/stories/sidecar-in-ios-13-and-macos-catalina-working-seamlessly-between-an-ipad-and-mac/" rel="noopener noreferrer">John covered in his original story</a> still applies today: Sidecar was conceived to use an iPad as a secondary display for a Mac; you can drag windows from the Mac’s main display to the iPad’s, or you can send individual windows from one monitor to another. There are some additional options you can configure (there’s even a virtual Touch Bar, still!), which you can also read about in Apple’s support document <a href="https://support.apple.com/en-us/102597" rel="noopener noreferrer">here</a>.</p>
<p id="p70">There’s only one problem, though: even after five years, you can only initiate a Sidecar connection from a Mac first. Suffice to say, that was a bit of a problem for someone who no longer had a display attached to his MacBook Air. How could I interact with the macOS UI to start Sidecar without a display? Was my only solution to VNC into my Mac first and then manually start Sidecar from there every time?</p>
<p id="p71">See, my dream was more ambitious. I wanted to sit down with my MacPad, press a button on iPadOS, and enter macOS mode; then once I was done, I wanted to be able to press another button on the Mac, and return to iPadOS mode. Essentially, I wanted <strong>a seamless transition between macOS and iPadOS</strong>, designed around Sidecar but faster than what Apple normally allows for.</p>
<p id="p72">And, dear readers, I figured it out.</p>
<p id="p73">During my research, I came across a lot of Reddit and forum threads from people who were wondering if there was a way to use Shortcuts and automation to start a Sidecar connection directly from iPadOS. I tried some of the methods mentioned online, including one that <a href="https://www.reddit.com/r/shortcuts/comments/fzjlil/initiate_sidecar_from_ipad/" rel="noopener noreferrer">involved</a> the often-unreliable GUI automation, and none of them worked as quickly or reliably as I hoped. Then I came across <a href="https://github.com/Ocasio-J/SidecarLauncher" rel="noopener noreferrer">Sidecar Launcher</a>, an open-source project by developer Jovany Ocasio that provided exactly the solution I was looking for.</p>
<p id="p74">Unlike other tools to start a Sidecar connection you may find on the Internet, Sidecar Launcher is not based on AppleScript. Instead, Sidecar Launcher is a little executable program that uses private APIs based on Apple’s ‘SidecarCore’ framework to return a list of all available Sidecar-enabled devices, start a specific Sidecar connection without interacting with macOS at all, or disconnect Sidecar. You can inspect the Sidecar Launcher code <a href="https://github.com/Ocasio-J/SidecarLauncher/blob/main/SidecarLauncher/main.swift" rel="noopener noreferrer">here</a> and, if you want, download the Xcode project and build the executable file yourself.</p>
<p id="p75">To use Sidecar Launcher, you just need to download the executable script, place it somewhere in Finder, and then download a companion shortcut on your iPad. The shortcut created by Jovany Ocasio requires you to enter your Mac’s IP address/hostname, username, and password, as well as the path where you placed the Sidecar Launcher script. That’s all the configuration you need to do and you won’t have to touch Sidecar Launcher at all in the future.<sup id="fnref-74584-someTips"><a href="#fn-74584-someTips" rel="noopener noreferrer">3</a></sup> When you run the shortcut on your iPad, it’ll take care of waking up your Mac, logging in, and enabling the Sidecar connection for you. Run the shortcut on your iPad, wait 10 seconds or so, and the iPad’s screen will turn into a macOS extended display – no further interaction necessary.</p>

<p id="p77">The first time I tried Sidecar Launcher, I knew it was precisely what I needed, but I still wanted to make it faster and easier to use. Take a look at how Jovany Ocasio’s shortcut works by default:</p>

<p id="p79">And here’s <strong>Sidecar Launcher (Ticci’s Version)</strong>, which connects to macOS and enables Sidecar in two seconds:</p>

<p id="p81">I modified the original Sidecar Launcher shortcut in a couple of ways. For starters, I privately got in touch with Jovany Ocasio, who was kind enough to listen to my feedback and implemented a <code>disconnect</code> command that terminates an existing Sidecar session. That allowed me to put together a version of a shortcut that acts as a toggle:</p>
<ul id="ul82"><li>When run from the iPad, the shortcut instantly connects to the Mac;</li>
<li>When run from the Mac, the same shortcut disconnects Sidecar.</li>
</ul><p id="p83">In my version of the shortcut, I removed all the code that dealt with waking a Mac and authenticating the user: I can just unlock my headless MacBook Air with Touch ID and know that the computer is ready to go when I run the shortcut in my iPad’s dock.<sup id="fnref-74584-miniHeadless"><a href="#fn-74584-miniHeadless" rel="noopener noreferrer">4</a></sup> Furthermore, I removed the need to manually choose an iPad from a list of supported Sidecar devices nearby and instead used a ‘Text’ action that gives macOS the name of the iPad it needs to connect to.</p>

<p id="p85">Thanks to these changes, as long as both the Mac and iPad are already unlocked, nearby, and on the same Wi-Fi network, the Sidecar Launcher shortcut in the iPad dock will initiate a Sidecar connection immediately. When I’m done working with macOS, running the same shortcut from the Mac’s menu bar will quit Sidecar instead. Props to Jovany Ocasio for reverse-engineering how Sidecar works and finding the private APIs that make these direct connections possible. You can download <strong>Sidecar Launcher (Ticci’s Version)</strong> below and find it in the <a href="https://www.macstories.net/shortcuts/" rel="noopener noreferrer">MacStories Shortcuts Archive</a>:</p>

<p id="p87">As we saw previously, macOS still thinks there’s a built-in display attached to my computer. And since Sidecar can turn an iPad into an extended display, it’s also possible to make it the main display of the computer – which is a setting that will stick over time, thus defaulting to the Sidecar-enabled iPad as the MacBook’s main display the next time you connect. To do this, you just need to visit System Settings ⇾ Displays, drag the menu bar onto the iPad display, and choose to make it the main one.</p>


<hr id="hr90"><p id="p91">With those setup-related details out of the way, allow me to dig into how I’ve been working with my MacPad for the past three weeks.</p>
<p id="p92">When I want to get some work done (such as editing an entire issue of the <a href="https://club.macstories.net/" rel="noopener noreferrer">newsletter</a>, long writing sessions, or recording <a href="https://appstories.net/" rel="noopener noreferrer">podcasts</a>), I attach my 11” iPad Pro to the MacBook Air, press a button, and I’m in macOS mode thanks to Sidecar. In my experience, Sidecar has proven to be remarkably fast, reliable, and lag- or artifact-free when both the iPad Pro and MacBook Air are on a 5&nbsp;GHz W-Fi network.<sup id="fnref-74584-6Ghz"><a href="#fn-74584-6Ghz" rel="noopener noreferrer">5</a></sup> It’s truly impressive just how <em>good</em> Sidecar is when used as a wireless display and how much better it looks than, say, any VNC client that has to deal with video compression. Thanks to Sidecar, my iPad Pro feels like a native display attached to the MacBook Air – not like a different computer that’s wirelessly connected to the Mac.</p>
<p id="p94">Sidecar never breaks the illusion that you’re working with macOS and, for this reason, writing about how Sidecar works would be like writing about the entire Mac operating system. When I connect to Sidecar, my iPad simply becomes a Mac display, capable of doing all the things any regular Mac can. If there ever was a perfect fit for the <a href="https://www.youtube.com/watch?v=qmPq00jelpc" rel="noopener noreferrer">“It just works” mantra</a>, Sidecar is it.</p>
<p id="p95">That said, there are a couple of details I want to point out about working with macOS using Sidecar.</p>
<p id="p96">I tried setting up Sidecar in portrait mode using <a href="https://github.com/waydabber/BetterDisplay" rel="noopener noreferrer">BetterDisplay</a> with my 11” iPad Pro, and it was a bad idea. At the default resolution scaling option provided by Sidecar, everything displayed on-screen was too tiny to be comfortable; plus, the portrait orientation was just weird to me. I know some folks out there like their vertical monitors for coding and other tasks, but it wasn’t for me. If you want to try, you can <a href="https://github.com/waydabber/BetterDisplay/wiki/Rotated-Sidecar" rel="noopener noreferrer">follow this guide</a>.</p>
<p id="p97">When you’re using Sidecar, some touch input options are still available on the iPad’s display. You can swipe from the edge of the display to invoke Slide Over apps (a <em>lot</em> more on this in the next section) and swipe up with four fingers to enter the iPadOS app switcher, where you’ll notice that Sidecar is shown as a special ‘Continuity’ app that cannot be seen anywhere else in the system:</p>

<p id="p99">You can also scroll any macOS window with two fingers, pinch to zoom, or use the Apple Pencil to click any UI element. This is where we enter the wild hybrid territory of this project that I <em>love</em>, and which I will expand upon in the next section of the story.</p>
<p id="p100">At any point when I’m using macOS on the iPad’s display, I can simply reach out with my fingers and pinch on, say, a specific part of a photo I want to zoom in on. It’s a natural gesture that brings the direct manipulation we’ve gotten used to appreciating on touch-enabled devices to the Mac. I’m not saying that I’d like to control the macOS UI with touch all the time – some UI elements are too small to be tapped comfortably; I’m saying that for the <em>occasional</em> interaction, it’s nice to be able to touch a screen like we do hundreds of times each day for other devices in our lives.</p>
<p id="p101">Another nice use case for occasional touch interactions? Ripping the “display” apart from the MacBook and using the Apple Pencil to navigate and click around macOS.</p>

<p id="p103">Every once in a while, I’m doing something with my MacPad that requires reading for a while, such as editing an article for MacStories. A tablet – which lets you cozy up on a nice chair, put the screen on your lap or in front of your face as you’re lying down – is great for reading in a comfortable position for prolonged periods. A regular MacBook doesn’t let you do this. With my MacPad, I can detach the screen, and – if I want to continue using macOS for the task (the choice is always mine) – keep working with the Mac and use two fingers to scroll and the Pencil to click. Sidecar works just fine within a 10-meter distance when used wirelessly, and I can’t tell you how <em>nice</em> it is to have this kind of freedom for those times when you need it.</p>
<p id="p104">I also want to mention two Mac apps that have helped in this transition from a vanilla MacBook Air to the MacPad. Beyond the usual suspects (<a href="https://www.raycast.com/" rel="noopener noreferrer">Raycast</a>, <a href="https://rogueamoeba.com/audiohijack/" rel="noopener noreferrer">Audio Hijack</a>, and <a href="https://www.macbartender.com/" rel="noopener noreferrer">Bartender</a>), these apps have considerably sped up my MacPad workflow:</p>
<p id="p105"><strong><a href="https://magnet.crowdcafe.com/" rel="noopener noreferrer">Magnet</a>.</strong> There are dozens of tile-based window managers for macOS that aim to replicate the multitasking flow of Windows 11, but after trying a lot of them, I like Magnet for two specific reasons. First, it comes with easy-to-remember hotkeys for tiling windows to the left or right sides of the screen, which is an effective way to create split views without using <a href="https://support.apple.com/guide/mac-help/use-apps-in-split-view-mchl4fbe2921/mac" rel="noopener noreferrer">macOS’ actual Split View mode</a>:</p>

<p id="p107">More importantly, Magnet has a keyboard shortcut for cycling the window of the frontmost app between displays. On the off-chance that a window doesn’t open on my Sidecar display but is instead spawned on the non-existent built-in MacBook Air display, Magnet’s <code>⌃⌥⌘←/→</code> hotkeys let me instantly cycle that window across connected monitors, thus bringing it back to the iPad’s display.</p>
<p id="p108"><strong><a href="https://edovia.com/en/screens/" rel="noopener noreferrer">Screens</a>.</strong> It doesn’t get any better than Edovia’s Screens app when it comes to connecting via VNC to computers on your local network or available remotely. Screens is fundamental to my new MacPad workflow because it’s how I can type in my login password after a macOS reboot, but it’s also what I use to connect to my gaming PC to start streaming games to macOS or visionOS using <a href="https://moonlight-stream.org/" rel="noopener noreferrer">Moonlight</a>.</p>
<h3 id="macos-and-ipados-together">macOS and iPadOS, Together</h3>
<p id="p109">We need to go deeper now.</p>
<p id="p110">What if I told you that, with my MacPad, I can use both macOS and iPadOS together, as in literally <em>at the same time</em>? This is where things get a bit wild, in a way that I wasn’t expecting when this experiment began.</p>
<p id="p111">A few weeks ago when I started using Sidecar, I noticed that – as long as my iPad had successfully authenticated on the Lock Screen with Face ID – I could toggle back and forth between macOS and iPadOS by swiping up on the Home indicator. As I noted above, Sidecar shows up as a special Continuity app in the iPad app switcher; you can use other iPad apps and then return to Sidecar, or you can quit the Sidecar app on the iPad to terminate the connection.</p>
<p id="p113">It was only when I disabled Stage Manager on my iPad Pro that I realized Sidecar would let me use macOS and iPadOS apps on the same display at once. Our <a href="https://www.macstories.net/stories/ios-9-review/10/#slide-over" rel="noopener noreferrer">good old friend Slide Over</a> is one of the iPadOS UI elements to be allowed inside Sidecar’s display while macOS is active. Just swipe from the right edge of the display, bring in Slide Over, and you can use an iPad app while macOS is shown underneath.</p>

<p id="p115">The ability to use macOS and iPad apps in tandem has been a game-changer for me. There are some apps – like Music, Home, or Messages – that I prefer on the iPad as they lend themselves well to quick interactions that only require glancing at them briefly. (For instance, if I want to turn on a light in my office or respond with a Tapback to a message.) Instead of cluttering my Mac workspace with more windows, I can touch the screen, swipe for Slide Over, do what I have to do in an iPad app for a few seconds, and dismiss it.</p>

<p id="p117">I’m not constrained to a single iPad app either: Slide Over has supported multiple apps <a href="https://www.macstories.net/stories/ios-and-ipados-13-the-macstories-review/21/#new-slide-over" rel="noopener noreferrer">for a few years now</a>; when invoked from inside Sidecar, Slide Over lets me cycle between multiple app windows and, if I realize I’m going to need more time with one of them, make it full-screen and leave macOS.</p>

<p id="p119">Another iPadOS UI element<sup id="fnref-74584-UIElements"><a href="#fn-74584-UIElements" rel="noopener noreferrer">6</a></sup> that is allowed to coexist with macOS while in Sidecar is the Picture in Picture player for videos (or, as we’ve seen on MacStories, <a href="https://www.macstories.net/ios/yoink-brings-background-clipboard-monitoring-to-ios-and-ipados-15-via-picture-in-picture-workaround/" rel="noopener noreferrer">other apps that use the same technology</a>):</p>

<p id="p121">This is great for watching videos from an iPad app that doesn’t have a Mac counterpart while you’re still working with macOS, but it also brings me to another, bigger point: when using two OSes at once this way, you can listen to multiple audio sources at the same time – one from iPadOS, another from macOS.</p>

<p id="p123">This, of course, is something that has long been possible on the Mac, but never on the iPad, which still pauses a video if, for example, you start music playback somewhere else. Want to watch a video on mute on the Mac while you’re still listening to a podcast from <a href="https://apps.apple.com/us/app/pocket-casts-podcast-player/id414834813" rel="noopener noreferrer">Pocket Casts</a>, which doesn’t have a native Mac app? Just put Pocket Casts in Slide Over, play the video on the Mac, and both media streams can live together. This has always been the case for Sidecar, but it takes on a new meaning when the iPad itself becomes the primary display for a headless Mac laptop.</p>

<p id="p125">The advantages of Sidecar and Apple’s tightly integrated ecosystem don’t stop at media, however. When I’m using my MacPad, I can start a screen recording on iPadOS, switch over to macOS, and the recording will continue without interruptions. Here, let me show you with a video that I recorded from my MacPad:</p>

<p id="p127">Another integration that Apple built years ago and that also works beautifully with the MacPad is <a href="https://support.apple.com/en-us/102430" rel="noopener noreferrer">Universal Clipboard</a>. Whether it’s some text I copied from the Mac or, say, an image I copied from the iPad’s Photos app, the clipboard will be instantly shared between platforms when Sidecar is active, allowing me to easily copy and paste between different apps and environments as if I was using one device.</p>
<hr id="hr128"><p id="p129">The best part of using macOS and iPadOS together with my MacPad has been the realization that doing this doesn’t feel weird at all. Quite the opposite: thanks to <a href="https://www.macstories.net/stories/macos-big-sur-the-macstories-review/4/#design-optimizing-for-modern-familiarity" rel="noopener noreferrer">Apple’s consistent design language</a> and the common denominator of SwiftUI across modern apps, there are platform-specific differences between apps and OSes, but they are holistically consistent in a way that facilitates hopping between two distinct environments.</p>
<p id="p130">This, I believe, would be Apple’s greatest advantage against its competitors if only they invested in making the MacPad a real product: they already have a vibrant, deep ecosystem of apps based on a shared UI language that is equally optimized for each platform but consistent and familiar at the same time.</p>
<p id="p131">I say this as someone who worked with a Surface Pro 9 last year for six months and recently spent time exploring Android foldables: no other tech company on Earth currently has the same ecosystem strength, developer tooling, design foundation, and user-friendly approach as Apple here. The combination of iPadOS and macOS is, right now, already leagues above what Microsoft is doing with the Surface line; the only problem is that I had to build this hybrid device myself to prove it.</p>
<p id="p132">Not only is Apple well positioned to release a convertible laptop in the future, but their existing OSes and cross-device integrations already make for a fantastic experience with something like a MacPad <em>today</em>.</p>
<h2 id="questions-tidbits-and-other-setups">Questions, Tidbits, and Other Setups</h2>
<p id="p133">Below, I’ve compiled a list of some questions I had to figure out during the MacPad’s creation as well as examples of other setups for the MacPad’s modular system.</p>
<h3 id="what-about-sidecar-without-wi-fi">What About Sidecar without Wi-Fi?</h3>
<p id="p134">In case you’re not familiar with Sidecar, you can use this feature with both an iPad and Mac signed into the same Apple ID on a Wi-Fi network, or by connecting the two with a USB cable.</p>
<p id="p135">In my experience, aside from the occasional glitch that forced me to quit Sidecar and restart it, the USB connection hasn’t been necessary, which enabled me to keep the iPad charged while I was using macOS with it. My theory is that the issues I encountered were due to the iPad switching to the 6&nbsp;GHz band of my Wi-Fi 6E router; the 5&nbsp;GHz band has been more stable on a daily basis. As I wrote above, when using Sidecar over Wi-Fi, I didn’t notice any discernible latency or image artifacts.</p>
<p id="p136">But what about those times when I want to use the MacPad and don’t have a known Wi-Fi network to connect to – like, say, when I’m in my car? I solved that problem with an accessory <a href="https://club.macstories.net/posts/working-on-the-go-with-the-gl-inet-gl-mt3000-wifi-6-travel-router" rel="noopener noreferrer">John first brought to my attention</a>: a portable Wi-Fi router called <a href="https://amzn.to/49RSU85" rel="noopener noreferrer">GL-iNet Beryl AX3000</a>.</p>

<p id="p138">This small gadget, which is powered via USB-C, can create a Wi-Fi 6, 5&nbsp;GHz network by tethering over USB to my iPhone’s cellular plan. When I need to get work done with my MacPad in the car, I can just fire up the AX3000, wait for its Wi-Fi network to come back online, and wait for the iPad and Mac to connect to it automatically (it takes a few seconds). Then, wireless Sidecar will work just fine. I tested this in my car over the past few weeks, and it’s worked out perfectly.</p>
<h3 id="what-about-unlocking-the-mac">What About Unlocking the Mac?</h3>
<p id="p139">Since my MacBook Air no longer has a display lid, the computer’s built-in sensor that determines whether the computer is closed or not doesn’t work anymore. This is not a huge deal – I can always manually log out with ⌃⌘Q or wait for my computer to go to sleep on its own – but it’s caused a specific issue I had to solve.</p>

<p id="p141">With this kind of headless laptop setup, my recommendation is to disable automatic Apple Watch unlock and only rely on Touch ID for login instead. Without a lid always closed on top of it, my Apple Watch would continuously unlock the MacBook, even when it was stored in a bag while I was driving. (I use <a href="https://amzn.to/49xQNGJ" rel="noopener noreferrer">this sleeve</a> to hold both parts of the MacPad.) That made me nervous, so I decided to disable Apple Watch authentication, and I haven’t had any accidental logins since.</p>
<h3 id="ipados-mode-with-universal-control">iPadOS Mode with Universal Control</h3>
<p id="p142">For those times when I want to work with the more focused environment of iPadOS, I can rely on Universal Control to move the pointer and type on my iPad Pro. All I have to do is place my finger on the Mac’s Touch ID sensor to log in, then move the trackpad to the left, and the pointer lands on iPadOS.</p>

<p id="p144">As <a href="https://club.macstories.net/posts/the-macintosh-desktop-experience-my-universal-control-setup" rel="noopener noreferrer">we’ve known for a while</a>, this feature is glorious. But I will also add that, as of today, the best iPad Pro keyboard is a headless MacBook’s keyboard. Ironic, I know. But hear me out: the keyboard is quiet, the keys have nice travel but not too much, they’re backlit, and…there’s a function row. The same keys that control media playback or keys for Spotlight and Do Not Disturb work the same way on iPadOS while in Universal Control mode without having to remap anything. The trackpad is much more comfortable than the iPad’s Magic Keyboard one, and all the gestures for multitasking work too. It feels like using a more pro-level Magic Keyboard (which Apple <a href="https://www.macrumors.com/2023/08/27/new-ipad-pro-and-redesigned-magic-keyboard/" rel="noopener noreferrer">may announce soon</a>), except it’s actually a Mac.</p>
<p id="p145">The only downside here is that Sidecar needs to be turned off for Mac-to-iPad Universal Control to work. In the future, I hope Apple can find a way to let us use Universal Control when the Sidecar/Continuity “app” is running, but not in the foreground.</p>
<h3 id="ipad-as-a-tablet">iPad as a Tablet</h3>
<p id="p146">As for those times when I want to use the iPad as a tablet, well…I can just detach it from the MacPad’s magnetic structure, and it’s an iPad. Since I attached three magnets to the back of the smart cover I modded, I figured they could be useful when I’m using the iPad as a tablet as well. Sure enough, the magnets were a perfect fit for a <a href="https://amzn.to/3wIldaD" rel="noopener noreferrer">magnetic PopSocket</a>, which I can attach to the back of the iPad when I want to read an article or book for longer sessions:</p>

<p id="p148">I should also say that I tried both the 12.9” and 11” iPad Pro for my MacPad, and I prefer the smaller iPad Pro. For a while, I’ve been feeling like I was mostly using the 12.9” iPad Pro in laptop mode (with the Magic Keyboard) because the device was too big and heavy to be enjoyed as a tablet. The MacPad further cemented this idea and pushed me to stop using the 12.9” iPad Pro altogether.</p>
<p id="p149">For starters, while the <a href="https://www.macstories.net/stories/ipad-pro-2021-review/#liquid-retina-xdr-display" rel="noopener noreferrer">12.9” iPad Pro display is better</a> and makes for a more visually balanced MacPad, the whole thing gets too heavy at 1900 grams – that’s 350 grams more than a 14” MacBook Pro. With an 11” iPad Pro, the MacPad weighs around 1650 grams, which is only 100 grams more than a MacBook Pro. The big iPad Pro flavor of the MacPad looked great with the larger screen and XDR display, but it was unwieldy, and I was constantly afraid the magnets wouldn’t hold.</p>

<p id="p151">More importantly, however, the 11” iPad Pro strikes a much better balance between large enough as a Mac display and small enough to be held as a tablet in my hands. Since I have real multitasking on the Mac, I disabled Stage Manager on the iPad Pro, and it reminded me of just how much <em>nicer</em> old-school Split View and Slide Over are on an iPad when you want to do less-intensive iPad things.</p>

<p id="p153">I can hold the 11” iPad Pro with both hands in landscape and type with the software keyboard using my thumbs; in portrait, I see more text than an iPad mini on a single page, and holding the device is not as awkward as doing the same with a 12.9” iPad. Plus, as <a href="https://club.macstories.net/posts/turning-the-11-ipad-pro-into-a-game-streaming-device" rel="noopener noreferrer">I showed in the Monthly Log for Club MacStories members</a>, the 11” iPad Pro also works great as a fake PlayStation Portal, which I created by modding the Galileo G8 game controller.</p>

<p id="p155">The MacPad made me an 11” iPad Pro believer. I can’t wait for this iPad model to carry an <a href="https://www.macrumors.com/2024/02/28/2024-ipad-pro-cad-drawings/" rel="noopener noreferrer">OLED display</a>.</p>
<h3 id="headless-mac-at-a-desk-with-a-studio-display">Headless Mac at a Desk with a Studio Display</h3>
<p id="p156">By making a headless MacBook Air, I was also able to simplify the setup at my desk with my Studio Display, which is where I record all my podcasts. Since my computer is now comprised of a keyboard and trackpad, I no longer need separate input accessories on my desk: I just need a single Thunderbolt cable, which connects the MacBook Air to Ethernet and the Studio Display via the <a href="https://amzn.to/3Iou9Vb" rel="noopener noreferrer">CalDigit TS4 dock</a>.</p>

<p id="p158">Those who have been following me for a while know how much I disliked having a keyboard and trackpad always on my desk only for those times when I needed to record. Now, when I’m not in the office, the desk is empty; and when I’m working, I only have to connect one cable to my “keyboard computer” to extend macOS to the Studio Display and use that as my primary monitor. I even switched to an <a href="https://amzn.to/3wrCKUr" rel="noopener noreferrer">angled USB-C connector</a> to create a straight connection between the cable and the computer:</p>

<p id="p160">Because my desk is now tidied up and clutter-free, I can use the space when I’m not working to put <a href="https://www.macstories.net/news/macstories-unwind-godzilla-vs-ticcis-surprise-part-2/" rel="noopener noreferrer">my Z13 gaming PC</a>, which I use to stream games to macOS, iPadOS, or visionOS using <a href="https://app.lizardbyte.dev/Sunshine/?lng=en-US" rel="noopener noreferrer">Sunshine</a> and <a href="https://moonlight-stream.org/" rel="noopener noreferrer">Moonlight</a>.</p>
<h3 id="headless-mac-as-keyboard-and-trackpad-input-for-the-vision-pro">Headless Mac as Keyboard and Trackpad Input for the Vision Pro</h3>
<p id="p161">I saved the idea that started this whole journey for last.</p>
<p id="p162">As I suspected when I originally linked to Luke Miani’s YouTube video, a headless MacBook Air makes for the ultimate Vision Pro accessory: in a very compact, quiet, and powerful package, you have a Mac that you can use as a virtual display on the Vision Pro <em>and</em> as an input system that supports excellent typing with palm rejection features for its trackpad.</p>


<p id="p165">When I’m not using the MacPad, I use Mac Virtual Display. I love the feature so much, I’m thinking about selling my Studio Display, which is just taking up space on a wall at this point. Sure, the resolution of Mac Virtual Display isn’t as good as a physical Studio Display; however, a Studio Display can’t become a giant monitor in front of my eyes that can coexist with other large visionOS windows. In many ways, I believe that, for my use case, the Vision Pro is the most impressive Mac monitor I own: the trade-off in image quality is balanced by its portability and freedom to use a large Mac interface anywhere I am.</p>
<p id="p166">As is the case with Universal Control on the iPad, you can’t use Sidecar and Mac Virtual Display at the same time. While in pure headless mode, the Vision Pro still recognizes the MacBook Air in front of me and displays a ‘Connect’ button where the physical display used to be. When it doesn’t do that, I can just connect to the Mac from visionOS’ Control Center.</p>

<h3 id="more-questions">More Questions</h3>
<p id="p168">Here are answers to some common questions I received from friends who read the story in advance:</p>
<p id="p169"><strong>Can the MacPad use the iPad’s webcam while in Sidecar?</strong> Sadly, no. <a href="https://www.macstories.net/stories/macos-ventura-the-macstories-review/3/#continuity-camera" rel="noopener noreferrer">Continuity Camera</a> is another feature that is unavailable while Sidecar is active. If I need to be on a video call while using the MacPad in macOS mode, I can plug in my <a href="https://amzn.to/49xqqAX" rel="noopener noreferrer">Opal Tadpole portable webcam</a> and attach it to the iPad Pro’s bezel; otherwise, I can just switch to iPadOS mode with Universal Control and use the iPad’s front-facing camera (which I hope moves to the landscape side in the next iPad Pros).</p>
<p id="p170"><strong>What are the full weight comparisons between the 12.9” and 11” iPad Pro versions?</strong> Here are the numbers:</p>
<ul id="ul171"><li>MacPad with 11” iPad Pro (ESR back cover): 1650 grams (3.63 lbs)</li>
<li>MacPad with 12.9” iPad Pro (<a href="https://nomadgoods.com/products/leather-folio-black-ipad-pro-12-9-inch" rel="noopener noreferrer">Nomad leather back cover</a>): 1900 grams (4.18 lbs)</li>
<li>MacBook Air 13”: 1240 grams (2.73 lbs)</li>
<li>MacBook Pro 14” (M3): 1550 grams (3.41 lbs)</li>
</ul><p id="p172">Essentially, for 100 grams more than a MacBook Pro, I get two computers instead of one with my 11” MacPad.</p>
<p id="p173">If you have more questions, feel free to ask them in the <a href="https://club.macstories.net/faq/discord" rel="noopener noreferrer">Club MacStories Discord</a>, and I’ll try my best to answer them tomorrow in the live Q&amp;A we’re doing with members.</p>
<p id="p174"><strong>How do you charge both devices?</strong> I can plug in a USB-C cable in each device using my <a href="https://amzn.to/3ThJPjA" rel="noopener noreferrer">Anker GaN Prime charger</a> when I’m home, or via the <a href="https://amzn.to/3TjuRcR" rel="noopener noreferrer">Anker Prime battery</a> when I’m out and about. I should note, however, that by not having a built-in display anymore, the MacBook Air’s battery lasts <em>forever</em> now, which is a nice perk of this project.</p>
<h2 id="my-future-is-a-macpad">My Future Is a MacPad</h2>

<p id="p177">It’s funny: the MacPad, a product that doesn’t exist and that I created by modding a MacBook Air, is a testament to the incredible strength of Apple’s ecosystem.</p>
<p id="p178">Apple doesn’t offer a MacPad (<a href="https://www.bloomberg.com/news/articles/2023-01-11/apple-working-on-adding-touch-screens-to-macs-in-major-turnabout" rel="noopener noreferrer">yet</a>), but they’ve been developing the technologies that could make it possible for a long time. It’s only because of the integration between hardware and software in the Apple ecosystem that I was able to turn two computers into one and use it with basically zero issues.</p>
<p id="p180">There are several lessons to be learned from this story. For starters, you shouldn’t have to do this yourself if all you’re looking for is a better input accessory for the Vision Pro: Apple should make that device, and it’s silly that they haven’t updated their standalone keyboard and trackpad in years. I’m fully aware that what I’ve done is absurd, even if it worked out well in the end.</p>
<p id="p181">Second, Sidecar is surprisingly resilient: I’ve gotten so used to working with my MacPad now, I sometimes forget I’m using a wireless display that’s actually an iPad running a different operating system. The teams at Apple working on Sidecar, Universal Control, and Mac Virtual Display are responsible for truly remarkable engineering feats that are fast, secure, powerful, and intuitive. These features are quintessential Apple, and I’m so happy whenever I use them. The fact that I’m now using macOS almost entirely with virtual screens says a lot about the quality of these functionalities.</p>
<p id="p182">Additionally, the setup I created will immediately scale to new iPad Pros as soon as they’re released: I’ll just need to buy another smart cover I can mod. And thanks to the MacPad, I’ll have an OLED MacBook months (<a href="https://www.macrumors.com/2023/10/11/macbook-pro-oled-three-years-away/" rel="noopener noreferrer">years</a>?) before those products are released. OLED is going to be a glorious addition to my MacPad setup.</p>
<p id="p183">Most importantly, however, making the MacPad has shown me that it’s so liberating to use a convertible Apple computer that can be both a laptop and tablet at the same time.</p>
<p id="p184">At a moment’s notice, I can go from working with a laptop to holding a tablet in my hands, and the user experience doesn’t suffer because of the transition. Likewise, by blending macOS and iPadOS apps on the same display, I can have the best of both worlds: the power and versatility of the Mac, and the nimble nature and richer app ecosystem of the iPad.</p>
<p id="p185">It all started because I wanted a better keyboard for my Vision Pro, and I ended up accidentally creating the Apple computer of my dreams: a Mac-iPad hybrid with two form factors I love, and none of the constraints of either of them. After using my MacPad for the past weeks, there’s one key takeaway:</p>
<p id="p186">An official Apple one can’t come soon enough.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do literally anything (116 pts)]]></title>
            <link>https://aaronfrancis.com/2024/do-literally-anything</link>
            <guid>39591910</guid>
            <pubDate>Mon, 04 Mar 2024 15:57:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaronfrancis.com/2024/do-literally-anything">https://aaronfrancis.com/2024/do-literally-anything</a>, See on <a href="https://news.ycombinator.com/item?id=39591910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>There are a lot of things I want to do. I want to create and build and make and write.</p>
<p>And then, of course, there are other things that I just have to do, like bookkeeping and responding to emails.</p>
<p>Sometimes, it all gets to be too much. There are too many things to do! I need to figure out where to start! I get
paralyzed by the sheer enormity of it all.</p>
<p>This has happened enough times that I know the solution: do literally anything.</p>
<p>It doesn't matter what! Write half of an article. Categorize two expenses. Write down all your ideas for videos. Clean
your office. Respond to a single email.</p>
<p>By doing anything, I find momentum. I find motivation! I ease the heavy feeling and find my stride. The mere act of
doing pushes back the tidal wave of thoughts.</p>
<p>The problem is primarily emotional. I feel paralyzed.</p>
<p>The solution is primarily emotional too! Just do one thing.</p>
<p>By doing one thing, I feel like maybe things are doable after all. And if things are doable, then maybe I can do more
things!</p>
<p>Progress leads to progress, motion to motion.</p>
<p>There have been times when I've looked at the amount of stuff that needs to be done and thought: "everything is
impossible. Nothing can ever be finished. There's no way that anything in the universe has ever been done."</p>
<p>And since there's no way that all of this can be done, I'm just gonna sit here and worry about everything that needs to
be done!</p>
<p>Do literally anything.</p>
<p>Make progress on literally anything.</p>
<p>It may not be rational, but it is helpful, so who cares if it's rational? Certainly not me.</p>
<p>It's a solution to overwhelm, but I've found it's also a solution for another issue: underwhelm. Aimlessness!</p>
<p>The complete opposite problem! Having too little to do or not knowing what you want to do.</p>
<p>If I'm stuck thinking, "I don't know what I want the next phase of my career to be," or "I don't have any ideas for
videos," or "I don't know what projects to build," the answer is the same: do literally anything.</p>
<p>Know that it's okay to change course once you've started. It's okay to start something and then realize something might
be better for you. That's the beauty of action; it opens paths you wouldn't have seen earlier.</p>
<p>You'll rarely discover "what's next" by standing still. Start working on <em>something</em>. Start tinkering. That will put you
in motion to figure out what "the thing" is.</p>
<p>Overwhelmed? Underwhelmed?</p>
<p>Do literally anything.</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why it's so challenging to land upright on the moon (158 pts)]]></title>
            <link>https://www.nytimes.com/2024/03/04/science/moon-landing-sideways-gravity.html</link>
            <guid>39591772</guid>
            <pubDate>Mon, 04 Mar 2024 15:46:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/03/04/science/moon-landing-sideways-gravity.html">https://www.nytimes.com/2024/03/04/science/moon-landing-sideways-gravity.html</a>, See on <a href="https://news.ycombinator.com/item?id=39591772">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/03/04/science/moon-landing-sideways-gravity.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Rust for Embedded Systems: Current state, challenges and open problems (166 pts)]]></title>
            <link>https://arxiv.org/abs/2311.05063</link>
            <guid>39591694</guid>
            <pubDate>Mon, 04 Mar 2024 15:38:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2311.05063">https://arxiv.org/abs/2311.05063</a>, See on <a href="https://news.ycombinator.com/item?id=39591694">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2311.05063.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Embedded software is used in safety-critical systems such as medical devices and autonomous vehicles, where software defects, including security vulnerabilities, have severe consequences. Most embedded codebases are developed in unsafe languages, specifically C/C++, and are riddled with memory safety vulnerabilities. To prevent such vulnerabilities, RUST, a performant memory-safe systems language, provides an optimal choice for developing embedded software. RUST interoperability enables developing RUST applications on top of existing C codebases. Despite this, even the most resourceful organizations continue to develop embedded software in C/C++. This paper performs the first systematic study to holistically understand the current state and challenges of using RUST for embedded systems. Our study is organized across three research questions. We collected a dataset of 2,836 RUST embedded software spanning various categories and 5 Static Application Security Testing ( SAST) tools. We performed a systematic analysis of our dataset and surveys with 225 developers to investigate our research questions. We found that existing RUST software support is inadequate, SAST tools cannot handle certain features of RUST embedded software, resulting in failures, and the prevalence of advanced types in existing RUST software makes it challenging to engineer interoperable code. In addition, we found various challenges faced by developers in using RUST for embedded systems development.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Ayushi Sharma [<a href="https://arxiv.org/show-email/d9750174/2311.05063">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 8 Nov 2023 23:59:32 UTC (429 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Astro App (261 pts)]]></title>
            <link>https://astro.sshh.io/</link>
            <guid>39591529</guid>
            <pubDate>Mon, 04 Mar 2024 15:24:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://astro.sshh.io/">https://astro.sshh.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39591529">Hacker News</a></p>
<div id="readability-page-1" class="page"><main><div><div><div><p>Sky</p><p>4:07:36 PM</p></div><div><p>Sky</p><p>4:07:36 PM</p></div></div><div></div></div></main></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Revy – proof-of-concept time-travel debugger for the Bevy game engine (112 pts)]]></title>
            <link>https://github.com/rerun-io/revy</link>
            <guid>39590683</guid>
            <pubDate>Mon, 04 Mar 2024 14:09:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rerun-io/revy">https://github.com/rerun-io/revy</a>, See on <a href="https://news.ycombinator.com/item?id=39590683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Revy</h2><a id="user-content-revy" aria-label="Permalink: Revy" href="#revy"></a></p>
<p dir="auto"><a href="https://crates.io/crates/revy" rel="nofollow"><img src="https://camo.githubusercontent.com/101765999786b9dc34863677264838cb2c824818660c4c7ec6d39454f370e1b3/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f726576792e737667" alt="Latest version" data-canonical-src="https://img.shields.io/crates/v/revy.svg"></a>
<a href="https://docs.rs/revy" rel="nofollow"><img src="https://camo.githubusercontent.com/530af3a9f1571641fa72e2cac95d5bc4ced3e803b17743a2efc0cff65586ad0f/68747470733a2f2f646f63732e72732f726576792f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/revy/badge.svg"></a>
<a href="https://github.com/rerun-io/revy/blob/master/LICENSE-MIT"><img src="https://camo.githubusercontent.com/2bb6ac78e5a9f4f688a6a066cc71b62012101802fcdb478e6e4c6b6ec75dc694/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"></a>
<a href="https://github.com/rerun-io/revy/blob/master/LICENSE-APACHE"><img src="https://camo.githubusercontent.com/d7addb0c76e1634e2335a071640043bf515d94b5c9d88b604fbe77c26920cfb0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4170616368652d626c75652e737667" alt="Apache" data-canonical-src="https://img.shields.io/badge/license-Apache-blue.svg"></a></p>
<p dir="auto">Revy is a proof-of-concept time-travel debugger for the <a href="https://github.com/bevyengine/bevy">Bevy</a> game engine, built using <a href="https://github.com/rerun-io/rerun">Rerun</a>.</p>
<p dir="auto">The general idea is that one would use Revy to investigate gameplay/physics/general-behavior-ish kinds of bugs.<br>
Revy is <em>not</em> a graphics debugger: for that you'd use e.g. <a href="https://github.com/baldurk/renderdoc">RenderDoc</a>.<br>
It is <em>not</em> a performance profiler either: for that, Bevy integrates well with e.g. <a href="https://github.com/wolfpld/tracy">Tracy</a>.</p>
<p dir="auto">Revy works by snapshotting diffs of the Bevy database every frame that are then logged into the Rerun database.<br>
This allows you to inspect and visualize the state of the engine at any point in time, either in real-time or after the fact.<br>
These recordings can then be shared to be replayed or e.g. attached to bug reports.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description revy.mp4">revy.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/2910679/309282832-cd096cbe-5e68-4acf-8010-e6c32c5568dc.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDk2MTE1MDYsIm5iZiI6MTcwOTYxMTIwNiwicGF0aCI6Ii8yOTEwNjc5LzMwOTI4MjgzMi1jZDA5NmNiZS01ZTY4LTRhY2YtODAxMC1lNmMzMmM1NTY4ZGMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMDVUMDQwMDA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTMwMDg0YjIzMjFhOWQ3Y2E0MDcwZDc1MGU1ZTBmOTY2ZWM2Y2QzOTNlNTkyYzIyMDQ1YzJmNWQxZTBhNTM5YSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.oy3KRLXiZTGcOFH4vGnUH6-8ETdPjXcJ89iiO3x7roQ" data-canonical-src="https://private-user-images.githubusercontent.com/2910679/309282832-cd096cbe-5e68-4acf-8010-e6c32c5568dc.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDk2MTE1MDYsIm5iZiI6MTcwOTYxMTIwNiwicGF0aCI6Ii8yOTEwNjc5LzMwOTI4MjgzMi1jZDA5NmNiZS01ZTY4LTRhY2YtODAxMC1lNmMzMmM1NTY4ZGMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMDVUMDQwMDA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTMwMDg0YjIzMjFhOWQ3Y2E0MDcwZDc1MGU1ZTBmOTY2ZWM2Y2QzOTNlNTkyYzIyMDQ1YzJmNWQxZTBhNTM5YSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.oy3KRLXiZTGcOFH4vGnUH6-8ETdPjXcJ89iiO3x7roQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<table>
<thead>
<tr>
<th><a href="https://github.com/bevyengine/bevy/blob/v0.13.0/examples/games/breakout.rs">Breakout</a></th>
<th><a href="https://github.com/bevyengine/bevy/blob/v0.13.0/examples/3d/3d_shapes.rs">3D shapes</a></th>
<th><a href="https://github.com/bevyengine/bevy/blob/v0.13.0/examples/games/alien_cake_addict.rs">Alien Cake Addict</a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://app.rerun.io/version/0.14.1/index.html?url=https://storage.googleapis.com/rerun-example-datasets/revy/breakout_014_001.rrd" rel="nofollow"><em>Live demo</em></a></td>
<td><a href="https://app.rerun.io/version/0.14.1/index.html?url=https://storage.googleapis.com/rerun-example-datasets/revy/3d_shapes_014_001.rrd" rel="nofollow"><em>Live demo</em></a></td>
<td><a href="https://app.rerun.io/version/0.14.1/index.html?url=https://storage.googleapis.com/rerun-example-datasets/revy/alien_014_001.rrd" rel="nofollow"><em>Live demo</em></a></td>
</tr>
<tr>
<td><themed-picture data-catalyst-inline="true"><picture> <img src="https://camo.githubusercontent.com/91b1f899a1ec3edcc3d3ca1871f616126b9b6db00973caa2b3507dd4f23a2d36/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f627265616b6f75745f7469746c652f613835336166343131313535303532313232393638313361306265663233373362313035373537622f66756c6c2e706e67" alt="" data-canonical-src="https://static.rerun.io/revy_breakout_title/a853af41115505212296813a0bef2373b105757b/full.png"> <source media="(max-width: 480px)" srcset="https://camo.githubusercontent.com/84f003e39e3c05464f594f690bc6df08468fed3eb09f41d7fb667a1a0302c9f0/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f627265616b6f75745f7469746c652f613835336166343131313535303532313232393638313361306265663233373362313035373537622f343830772e706e67" data-canonical-src="https://static.rerun.io/revy_breakout_title/a853af41115505212296813a0bef2373b105757b/480w.png"> <source media="(max-width: 768px)" srcset="https://camo.githubusercontent.com/3cea92e0867d3c7351607018f860c128ca772283960d63b023fd075e341adb86/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f627265616b6f75745f7469746c652f613835336166343131313535303532313232393638313361306265663233373362313035373537622f373638772e706e67" data-canonical-src="https://static.rerun.io/revy_breakout_title/a853af41115505212296813a0bef2373b105757b/768w.png"> <source media="(max-width: 1024px)" srcset="https://camo.githubusercontent.com/b0502f8295778a7d853fbba377a74f57fb7fb6e73831ab84ca55bcdaf9a45ca9/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f627265616b6f75745f7469746c652f613835336166343131313535303532313232393638313361306265663233373362313035373537622f31303234772e706e67" data-canonical-src="https://static.rerun.io/revy_breakout_title/a853af41115505212296813a0bef2373b105757b/1024w.png"> <source media="(max-width: 1200px)" srcset="https://camo.githubusercontent.com/416993737f8b1c1dcb6d7de172502086d7b859aa90332b10faf562377d9042e7/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f627265616b6f75745f7469746c652f613835336166343131313535303532313232393638313361306265663233373362313035373537622f31323030772e706e67" data-canonical-src="https://static.rerun.io/revy_breakout_title/a853af41115505212296813a0bef2373b105757b/1200w.png"> </picture></themed-picture></td>
<td><themed-picture data-catalyst-inline="true"><picture> <img src="https://camo.githubusercontent.com/e120baa0383135833c32a850b3ed5b6bba4e2ed61ddd9b483f408f942a6af2c9/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f33647368617065735f7469746c652f393634343436643033663137393262363065333934653863343935653666653136323733393339612f66756c6c2e706e67" alt="" data-canonical-src="https://static.rerun.io/revy_3dshapes_title/964446d03f1792b60e394e8c495e6fe16273939a/full.png"> <source media="(max-width: 480px)" srcset="https://camo.githubusercontent.com/31a9d6ba5a419cb5dcb95dede00acf4028daca140c885d15763bccc47c51123e/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f33647368617065735f7469746c652f393634343436643033663137393262363065333934653863343935653666653136323733393339612f343830772e706e67" data-canonical-src="https://static.rerun.io/revy_3dshapes_title/964446d03f1792b60e394e8c495e6fe16273939a/480w.png"> <source media="(max-width: 768px)" srcset="https://camo.githubusercontent.com/06871f08bf0efbac7c3dc53330665c518de8c7d8e53ab5a8cac49a82a18962a0/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f33647368617065735f7469746c652f393634343436643033663137393262363065333934653863343935653666653136323733393339612f373638772e706e67" data-canonical-src="https://static.rerun.io/revy_3dshapes_title/964446d03f1792b60e394e8c495e6fe16273939a/768w.png"> <source media="(max-width: 1024px)" srcset="https://camo.githubusercontent.com/66dcb8626f98cd1a08fbfab13cab21459f0efe0b4bab5f30c8290d2a06d599eb/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f33647368617065735f7469746c652f393634343436643033663137393262363065333934653863343935653666653136323733393339612f31303234772e706e67" data-canonical-src="https://static.rerun.io/revy_3dshapes_title/964446d03f1792b60e394e8c495e6fe16273939a/1024w.png"> <source media="(max-width: 1200px)" srcset="https://camo.githubusercontent.com/48cf939f775a4f8b1ce48280d0b1180b57c7dec2005a067be96a60d918d59a55/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f33647368617065735f7469746c652f393634343436643033663137393262363065333934653863343935653666653136323733393339612f31323030772e706e67" data-canonical-src="https://static.rerun.io/revy_3dshapes_title/964446d03f1792b60e394e8c495e6fe16273939a/1200w.png"> </picture></themed-picture></td>
<td><themed-picture data-catalyst-inline="true"><picture> <img src="https://camo.githubusercontent.com/f46f07bf67b1564509383f622537936ab2e110ead2895b26b83e2e0b60f8dd3a/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f616c69656e5f7469746c652f336534626134663363666237323839343265636233386261336536313366333439386464613365322f66756c6c2e706e67" alt="" data-canonical-src="https://static.rerun.io/revy_alien_title/3e4ba4f3cfb728942ecb38ba3e613f3498dda3e2/full.png"> <source media="(max-width: 480px)" srcset="https://camo.githubusercontent.com/5e09df5e266052fe2319edcfcc1e1542779e115a449fd2fce23f683d27d71b50/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f616c69656e5f7469746c652f336534626134663363666237323839343265636233386261336536313366333439386464613365322f343830772e706e67" data-canonical-src="https://static.rerun.io/revy_alien_title/3e4ba4f3cfb728942ecb38ba3e613f3498dda3e2/480w.png"> <source media="(max-width: 768px)" srcset="https://camo.githubusercontent.com/74dbc24bb07035b7cc277eddeb190c3d0a107e07115a0d5434286940654ed97c/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f616c69656e5f7469746c652f336534626134663363666237323839343265636233386261336536313366333439386464613365322f373638772e706e67" data-canonical-src="https://static.rerun.io/revy_alien_title/3e4ba4f3cfb728942ecb38ba3e613f3498dda3e2/768w.png"> <source media="(max-width: 1024px)" srcset="https://camo.githubusercontent.com/cd8ce776093a33428a168c92d9aad9787a2005b36bba8e9378bd5bad522815df/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f616c69656e5f7469746c652f336534626134663363666237323839343265636233386261336536313366333439386464613365322f31303234772e706e67" data-canonical-src="https://static.rerun.io/revy_alien_title/3e4ba4f3cfb728942ecb38ba3e613f3498dda3e2/1024w.png"> <source media="(max-width: 1200px)" srcset="https://camo.githubusercontent.com/a86494e8c020c33426e082418afd2d0302bf89b97c17adb4adb9c3238196a6ca/68747470733a2f2f7374617469632e726572756e2e696f2f726576795f616c69656e5f7469746c652f336534626134663363666237323839343265636233386261336536313366333439386464613365322f31323030772e706e67" data-canonical-src="https://static.rerun.io/revy_alien_title/3e4ba4f3cfb728942ecb38ba3e613f3498dda3e2/1200w.png"> </picture></themed-picture></td>
</tr>
</tbody>
</table>
<hr>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <em>This is not an official Rerun project. This is a side-experiment meant to explore the possibilities that a tool like Rerun could open when it comes to gamedev. It is not a full-fledged, properly maintained thing -- nor does it aim to be. It's also probably buggy and slow in many ways, and it certainly is full of code abominations 🙃.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://www.rerun.io/docs/getting-started/installing-viewer" rel="nofollow">Install the Rerun Viewer</a> (<code>0.14</code>).</p>
</li>
<li>
<p dir="auto">Add <code>revy</code> to your dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="revy = &quot;0.14&quot;  # always matches the rerun version"><pre><span>revy</span> = <span><span>"</span>0.14<span>"</span></span>  <span><span>#</span> always matches the rerun version</span></pre></div>
</li>
<li>
<p dir="auto">Initialize the <code>rerun</code> plugin:</p>
<div dir="auto" data-snippet-clipboard-copy-content=".add_plugins({
    let rec = revy::RecordingStreamBuilder::new(&quot;<your_app_name>&quot;).spawn().unwrap();
    revy::RerunPlugin { rec }
})"><pre><span>.</span><span>add_plugins</span><span>(</span><span>{</span>
    <span>let</span> rec = revy<span>::</span><span>RecordingStreamBuilder</span><span>::</span><span>new</span><span>(</span><span>"&lt;your_app_name&gt;"</span><span>)</span><span>.</span><span>spawn</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
    revy<span>::</span><span>RerunPlugin</span> <span>{</span> rec <span>}</span>
<span>}</span><span>)</span><span></span></pre></div>
<p dir="auto">This will start a Rerun Viewer in the background and stream the recording data to it.<br>
Check out the <a href="https://docs.rs/rerun/latest/rerun/struct.RecordingStreamBuilder.html" rel="nofollow"><code>RecordingStreamBuilder</code></a> docs for other options (saving to file, connecting to a remote viewer, etc).</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Custom loggers</h2><a id="user-content-custom-loggers" aria-label="Permalink: Custom loggers" href="#custom-loggers"></a></p>
<p dir="auto">Revy will record every components of every single entity (), either using one of the builtin <a href="https://github.com/rerun-io/revy/blob/main/src/default_loggers.rs">dedicated loggers</a>, or using the generic reflection-based logger.</p>
<p dir="auto">You can also register your own custom loggers by inserting a <code>RerunComponentLoggers</code> resource:</p>
<div dir="auto" data-snippet-clipboard-copy-content=".insert_resource(revy::RerunComponentLoggers::new([
    (
        &quot;bevy_render::view::visibility::ViewVisibility&quot;.into(),
        Some(revy::RerunLogger::new(
            |_world, _all_entities, entity, _component| {
                let suffix = None;

                use revy::external::rerun;
                let data = entity
                    .get::<ViewVisibility>()
                    .map(|vviz| {
                        revy::Aliased::<rerun::components::Text>::new(
                            &quot;ViewVisibility&quot;,
                            rerun::components::Text(
                                if vviz.get() { &quot;:)))&quot; } else { &quot;:'(&quot; }.into(),
                            ),
                        )
                    })
                    .map(|data| Box::new(data) as _);

                (suffix, data)
            },
        )),
    ),
]))"><pre><span>.</span><span>insert_resource</span><span>(</span>revy<span>::</span><span>RerunComponentLoggers</span><span>::</span><span>new</span><span>(</span><span>[</span>
    <span>(</span>
        <span>"bevy_render::view::visibility::ViewVisibility"</span><span>.</span><span>into</span><span>(</span><span>)</span><span>,</span>
        <span>Some</span><span>(</span>revy<span>::</span><span>RerunLogger</span><span>::</span><span>new</span><span>(</span>
            |_world<span>,</span> _all_entities<span>,</span> entity<span>,</span> _component| <span>{</span>
                <span>let</span> suffix = <span>None</span><span>;</span>

                <span>use</span> revy<span>::</span>external<span>::</span>rerun<span>;</span>
                <span>let</span> data = entity
                    <span>.</span><span>get</span><span>::</span><span>&lt;</span><span>ViewVisibility</span><span>&gt;</span><span>(</span><span>)</span>
                    <span>.</span><span>map</span><span>(</span>|vviz| <span>{</span>
                        revy<span>::</span><span>Aliased</span><span>::</span><span>&lt;</span>rerun<span>::</span>components<span>::</span><span>Text</span><span>&gt;</span><span>::</span><span>new</span><span>(</span>
                            <span>"ViewVisibility"</span><span>,</span>
                            rerun<span>::</span>components<span>::</span><span>Text</span><span>(</span>
                                <span>if</span> vviz<span>.</span><span>get</span><span>(</span><span>)</span> <span>{</span> <span>":)))"</span> <span>}</span> <span>else</span> <span>{</span> <span>":'("</span> <span>}</span><span>.</span><span>into</span><span>(</span><span>)</span><span>,</span>
                            <span>)</span><span>,</span>
                        <span>)</span>
                    <span>}</span><span>)</span>
                    <span>.</span><span>map</span><span>(</span>|data| <span>Box</span><span>::</span><span>new</span><span>(</span>data<span>)</span> <span>as</span> <span>_</span><span>)</span><span>;</span>

                <span>(</span>suffix<span>,</span> data<span>)</span>
            <span>}</span><span>,</span>
        <span>)</span><span>)</span><span>,</span>
    <span>)</span><span>,</span>
<span>]</span><span>)</span><span>)</span><span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Compatibility</h2><a id="user-content-compatibility" aria-label="Permalink: Compatibility" href="#compatibility"></a></p>
<table>
<thead>
<tr>
<th>Bevy</th>
<th>Revy</th>
<th>Rerun</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/bevyengine/bevy/releases/tag/v0.13.0">0.13</a></td>
<td><a href="https://github.com/rerun-io/revy/releases/tag/0.14.0">0.14</a></td>
<td><a href="https://github.com/rerun-io/rerun/releases/tag/0.14.0">0.14</a></td>
</tr>
</tbody>
</table>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude 3 model family (881 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-3-family</link>
            <guid>39590666</guid>
            <pubDate>Mon, 04 Mar 2024 14:08:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-3-family">https://www.anthropic.com/news/claude-3-family</a>, See on <a href="https://news.ycombinator.com/item?id=39590666">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img alt="Claude 3 " loading="eager" width="2880" height="1620" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4e78f69ef8d4186fb5691714abe36224483d91b0-2880x1620.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4e78f69ef8d4186fb5691714abe36224483d91b0-2880x1620.png&amp;w=3840&amp;q=75"></figure><p>Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and <a href="https://www.anthropic.com/api#pricing">cost</a> for their specific application.</p><p>Opus and Sonnet are now available to use in claude.ai and the Claude API which is now generally available in <a href="https://www.anthropic.com/supported-countries">159 countries</a>. Haiku will be available soon.</p><h4>Claude 3 model family</h4><figure><img loading="lazy" width="2200" height="1174" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5d20371eeb8d045465bb22cacfd269b5958b004d-2200x1174.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5d20371eeb8d045465bb22cacfd269b5958b004d-2200x1174.png&amp;w=3840&amp;q=75"></figure><h4>A new standard for intelligence</h4><p>Opus, our most intelligent model, outperforms its peers on most of the common evaluation benchmarks for AI systems, including undergraduate level expert knowledge (MMLU), graduate level expert reasoning (GPQA), basic mathematics (GSM8K), and more. It exhibits near-human levels of comprehension and fluency on complex tasks, leading the frontier of general intelligence.</p><p>All <a href="https://www.anthropic.com/claude-3-model-card">Claude 3</a> models show increased capabilities in analysis and forecasting, nuanced content creation, code generation, and conversing in non-English languages like Spanish, Japanese, and French.</p><p>Below is a comparison of the Claude 3 models to those of our peers on multiple benchmarks [1] of capability:</p><figure><img loading="lazy" width="2200" height="1954" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F9ad98d612086fe52b3042f9183414669b4d2a3da-2200x1954.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F9ad98d612086fe52b3042f9183414669b4d2a3da-2200x1954.png&amp;w=3840&amp;q=75"></figure><h4>Near-instant results</h4><p>The Claude 3 models can power live customer chats, auto-completions, and data extraction tasks where responses must be immediate and in real-time.</p><p>Haiku is the fastest and most cost-effective model on the market for its intelligence category. It can read an information and data dense research paper on arXiv (~10k tokens) with charts and graphs in less than three seconds. Following launch, we expect to improve performance even further.</p><p>For the vast majority of workloads, Sonnet is 2x faster than Claude 2 and Claude 2.1 with higher levels of intelligence. It excels at tasks demanding rapid responses, like knowledge retrieval or sales automation. Opus delivers similar speeds to Claude 2 and 2.1, but with much higher levels of intelligence.</p><h4>Strong vision capabilities</h4><p>The Claude 3 models have sophisticated vision capabilities on par with other leading models. They can process a wide range of visual formats, including photos, charts, graphs and technical diagrams. We’re particularly excited to provide this new modality to our enterprise customers, some of whom have up to 50% of their knowledge bases encoded in various formats such as PDFs, flowcharts, or presentation slides.</p><figure><img loading="lazy" width="2200" height="960" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6b66d86ff0c180e95bc6ad2e6e4a1843aa74c80f-2200x960.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6b66d86ff0c180e95bc6ad2e6e4a1843aa74c80f-2200x960.png&amp;w=3840&amp;q=75"></figure><h4>Less refusals</h4><p>Previous Claude models often made unnecessary refusals that suggested a lack of contextual understanding. We’ve made meaningful progress in this area: Opus, Sonnet, and Haiku are significantly less likely to refuse to answer prompts that border on the system’s guardrails than previous generations of models. As shown below, the Claude 3 models show a more nuanced understanding of requests, recognize real harm, and refuse to answer harmless prompts much less often.</p><figure><img loading="lazy" width="2188" height="918" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd1fbcf3d58ebc2dcd2e98aac995d70bf50cb2e9c-2188x918.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd1fbcf3d58ebc2dcd2e98aac995d70bf50cb2e9c-2188x918.png&amp;w=3840&amp;q=75"></figure><h4>Improved accuracy</h4><p>Businesses of all sizes rely on our models to serve their customers, making it imperative for our model outputs to maintain high accuracy at scale. To assess this, we use a large set of complex, factual questions that target known weaknesses in current models. We categorize the responses into correct answers, incorrect answers (or hallucinations), and admissions of uncertainty, where the model says it doesn’t know the answer instead of providing incorrect information. Compared to Claude 2.1, Opus demonstrates a twofold improvement in accuracy (or correct answers) on these challenging open-ended questions while also exhibiting reduced levels of incorrect answers.</p><p>In addition to producing more trustworthy responses, we will soon enable citations in our Claude 3 models so they can point to precise sentences in reference material to verify their answers.<br></p><figure><img loading="lazy" width="2200" height="896" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7cb598c6a9fa58c12b77f67ee2067feaac4a2de0-2200x896.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7cb598c6a9fa58c12b77f67ee2067feaac4a2de0-2200x896.png&amp;w=3840&amp;q=75"></figure><h4>Long context and near-perfect recall</h4><p>The Claude 3 family of models will initially offer a 200K context window upon launch. However, all three models are capable of accepting inputs exceeding 1 million tokens and we may make this available to select customers who need enhanced processing power.</p><p>To process long context prompts effectively, models require robust recall capabilities. The 'Needle In A Haystack' (NIAH) evaluation measures a model's ability to accurately recall information from a vast corpus of data. We enhanced the robustness of this benchmark by using one of 30 random needle/question pairs per prompt and testing on a diverse crowdsourced corpus of documents. Claude 3 Opus not only achieved near-perfect recall, surpassing 99% accuracy, but in some cases, it even identified the limitations of the evaluation itself by recognizing that the "needle" sentence appeared to be artificially inserted into the original text by a human.</p><figure><img loading="lazy" width="2200" height="1088" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2aa12b60e9c57e7057924bd8878d754c7b3d8e7-2200x1088.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2aa12b60e9c57e7057924bd8878d754c7b3d8e7-2200x1088.png&amp;w=3840&amp;q=75"></figure><h4>Responsible design</h4><p>We’ve developed the Claude 3 family of models to be as trustworthy as they are capable. We have several dedicated teams that track and mitigate a broad spectrum of risks, ranging from misinformation and CSAM to biological misuse, election interference, and autonomous replication skills. We continue to develop methods such as <a href="https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback">Constitutional AI</a> that improve the safety and transparency of our models, and have tuned our models to mitigate against privacy issues that could be raised by new modalities.</p><p>Addressing biases in increasingly sophisticated models is an ongoing effort and we’ve made strides with this new release. As shown in the model card, Claude 3 shows less biases than our previous models according to the <a href="https://aclanthology.org/2022.findings-acl.165/">Bias Benchmark for Question Answering (BBQ)</a>. We remain committed to advancing techniques that reduce biases and promote greater neutrality in our models, ensuring they are not skewed towards any particular partisan stance.</p><p>While the Claude 3 model family has advanced on key measures of biological knowledge, cyber-related knowledge, and autonomy compared to previous models, it remains at AI Safety Level 2 (ASL-2) per our <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy</a>. Our <a href="https://www.anthropic.com/news/red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned">red teaming</a> evaluations (performed in line with our <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/">White House commitments</a> and the <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">2023 US Executive Order</a>) have concluded that the models present negligible potential for catastrophic risk at this time. We will continue to carefully monitor future models to assess their proximity to the ASL-3 threshold. Further safety details are available in the <a href="https://www.anthropic.com/claude-3-model-card">Claude 3 model card</a>.<br></p><h4>Easier to use</h4><p>The Claude 3 models are better at following complex, multi-step instructions. They are particularly adept at adhering to brand voice and response guidelines, and developing customer-facing experiences our users can trust. In addition, the Claude 3 models are better at producing popular structured output in formats like JSON—making it simpler to instruct Claude for use cases like natural language classification and sentiment analysis.<br></p><h4>Model details</h4><p><strong>Claude 3 Opus </strong>is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what’s possible with generative AI.</p><div aria-label="data" role="region" tabindex="0"><table><tbody><tr><td><strong>Cost</strong><em> [Input $/million tokens | Output $/million tokens]</em></td><td>$15 | $75</td></tr><tr><td><strong>Context window</strong></td><td>200K*</td></tr><tr><td><strong>Potential uses</strong></td><td><ul><li>Task automation: plan and execute complex actions across APIs and databases, interactive coding</li><li>R&amp;D: research review, brainstorming and hypothesis generation, drug discovery</li><li>Strategy: advanced analysis of charts &amp; graphs, financials and market trends, forecasting</li></ul></td></tr><tr><td><strong>Differentiator</strong></td><td>Higher intelligence than any other model available.</td></tr></tbody></table></div><p><sup><em>*1M tokens available for specific use cases, please inquire.</em></sup></p><figure><img loading="lazy" width="1148" height="56" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08376f135c37fe029e2aea16fa55c4c83ec77b6b-1148x56.png&amp;w=1200&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08376f135c37fe029e2aea16fa55c4c83ec77b6b-1148x56.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08376f135c37fe029e2aea16fa55c4c83ec77b6b-1148x56.png&amp;w=3840&amp;q=75"></figure><p><strong>Claude 3 Sonnet</strong> strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.</p><div aria-label="data" role="region" tabindex="0"><table><tbody><tr><td><strong>Cost</strong><em> [Input $/million tokens | Output $/million tokens]</em></td><td>$3 | $15</td></tr><tr><td><strong>Context window</strong></td><td>200K</td></tr><tr><td><strong>Potential uses</strong></td><td><ul><li>Data processing: RAG or search &amp; retrieval over vast amounts of knowledge</li><li>Sales: product recommendations, forecasting, targeted marketing</li><li>Time-saving tasks: code generation, quality control, parse text from images</li></ul></td></tr><tr><td><strong>Differentiator</strong></td><td>More affordable than other models with similar intelligence; better for scale.</td></tr></tbody></table></div><p><strong>Claude 3 Haiku</strong> is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.</p><div aria-label="data" role="region" tabindex="0"><table><tbody><tr><td><strong>Cost</strong><em> [Input $/million tokens | Output $/million tokens]</em></td><td>$0.25 | $1.25</td></tr><tr><td><strong>Context window</strong></td><td>200K</td></tr><tr><td><strong>Potential uses</strong></td><td><ul><li>Customer interactions: quick and accurate support in live interactions, translations</li><li>Content moderation: catch risky behavior or customer requests</li><li>Cost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured data</li></ul></td></tr><tr><td><strong>Differentiator</strong></td><td>Smarter, faster, and more affordable than other models in its intelligence category.</td></tr></tbody></table></div><h4>Model availability</h4><p>Opus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers.</p><p>Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud’s Vertex AI Model Garden—with Opus and Haiku coming soon to both.</p><h4>Smarter, faster, safer</h4><p>We do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We're also excited to release a series of features to enhance our models' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities.</p><p>As we push the boundaries of AI capabilities, we’re equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes.</p><p>We’re excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit <a href="https://www.anthropic.com/claude">anthropic.com/claude</a>. </p><figure><img loading="lazy" width="1148" height="272" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F9169d7976c74c5da28de0369cfaa85ab4268adc9-1148x272.png&amp;w=1200&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F9169d7976c74c5da28de0369cfaa85ab4268adc9-1148x272.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F9169d7976c74c5da28de0369cfaa85ab4268adc9-1148x272.png&amp;w=3840&amp;q=75"></figure><div><h2>Footnotes</h2><ol><li id="footnote-1">This table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we’d like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. <a href="https://github.com/microsoft/promptbase">Source</a>.</li></ol></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New 13- and 15‑inch MacBook Air with M3 chip (357 pts)]]></title>
            <link>https://www.apple.com/newsroom/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/</link>
            <guid>39589924</guid>
            <pubDate>Mon, 04 Mar 2024 13:00:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/">https://www.apple.com/newsroom/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/</a>, See on <a href="https://news.ycombinator.com/item?id=39589924">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">









    
    
    









    





    <div>
        

        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>March 4, 2024</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple unveils the new 13- and 15‑inch MacBook&nbsp;Air with the powerful M3 chip
    

                    </h2>
                
            </div>

        <div>
                
                
                    The world’s most popular laptop is better than ever with even more performance, faster Wi-Fi, and support for up to two external displays — all in its strikingly thin and light design with up to 18 hours of battery life
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, Two new MacBook Air devices are shown folded and angled from the side.">
        <div>
             
              
              <div>
                The new 13- and 15-inch MacBook Air soars with the powerful M3 chip, featuring a super-portable design, power-efficient performance, and all-day battery life.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-2-up-hero-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-2-up-hero-240304_big" aria-label="Download media, Two new MacBook Air devices are shown folded and angled from the side."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span> </strong>Apple today announced the new <a href="https://www.apple.com/macbook-air/" target="_blank">MacBook Air</a> with the powerful M3 chip, taking its incredible combination of power-efficient performance and portability to a new level. With M3, MacBook Air is up to 60 percent faster than the model with the M1 chip and up to 13x faster than the fastest Intel-based MacBook Air.<sup>1</sup> And with a faster and more efficient Neural Engine in M3, MacBook Air continues to be the world’s best consumer laptop for AI. The 13- and 15-inch MacBook Air both feature a strikingly thin and light design, up to 18 hours of battery life,<sup>1</sup> a stunning Liquid Retina display, and new capabilities, including support for up to two external displays and up to 2x faster Wi-Fi than the previous generation. With its durable aluminum unibody enclosure that’s built to last, the new MacBook Air is available in four gorgeous colors: midnight, which features a breakthrough anodization seal to reduce fingerprints; starlight; space gray; and silver. Combined with its world-class camera, mics, and speakers; MagSafe charging; its silent, fanless design; and macOS, MacBook Air delivers an unrivaled experience — making the 13-inch model the world’s bestselling laptop and the 15-inch model the world’s bestselling 15-inch laptop. Customers can order starting today, with availability beginning Friday, March 8.
</div>
                 
             
                 <div>“MacBook Air is our most popular and loved Mac, with more customers choosing it over any other laptop. And today it gets even better with the M3 chip and new capabilities,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “From college students pursuing their degrees, to business users who need powerful productivity, or anyone who simply wants the unmatched combination of performance, portability, and industry-leading battery life, all in a fanless design, the new MacBook Air continues to be the world’s best thin and light laptop.”
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A person uses the new MacBook Air in a studio space.">
        <div>
             
              
              <div>
                The new MacBook Air with the M3 chip is strikingly thin and fast, so users can work, play, or create anywhere.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-lifestyle-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-lifestyle-240304_big" aria-label="Download media, A person uses the new MacBook Air in a studio space."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Blazing-Fast Performance with M3</strong>
</h2>
                 
             
                 <div>Built using industry-leading 3-nanometer technology, the M3 chip brings even faster performance and more capabilities to MacBook Air. Featuring a powerful 8-core CPU, up to a 10-core GPU, and support for up to 24GB of unified memory, the new MacBook Air is up to 60 percent faster than the model with M1 and up to 13x faster than the fastest Intel-based MacBook Air.<sup>1</sup> It also features up to 18 hours of battery life, which is up to six hours longer than an Intel-based MacBook Air.<sup>1</sup> Users will feel the blazing speed of M3 in everything they do, from everyday productivity, to demanding tasks like photo and video editing, and software development. And with the next-generation GPU of M3, the new MacBook Air supports hardware-accelerated mesh shading and ray tracing, offering more accurate lighting, reflections, and shadows for extremely realistic gaming experiences. It also includes the latest media engine with support for AV1 decode, providing more efficient and higher-quality video experiences from streaming services.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Final Cut Pro, Stage Manager, and Pixelmator are shown on the new MacBook Air.">
        <div>
             
              
              <div>
                The power-efficient M3 chip brings more speed to everything users do, including working on demanding tasks like photo and video editing.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-Final-Cut-Pro-Stage-Manager-Pixelmator-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-Final-Cut-Pro-Stage-Manager-Pixelmator-240304_big" aria-label="Download media, Final Cut Pro, Stage Manager, and Pixelmator are shown on the new MacBook Air."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div>M3 takes MacBook Air performance even further:
</div>
                 
             
                 <div><ul>
<li>Game titles like No Man’s Sky run up to 60 percent faster than the 13-inch MacBook Air with the M1 chip.<sup>1</sup></li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Enhancing an image with AI using Photomator’s Super Resolution feature is up to 40 percent faster than the 13-inch model with the M1 chip, and up to 15x faster for customers who haven’t upgraded to a Mac with Apple silicon.<sup>1</sup></li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Working in Excel spreadsheets is up to 35 percent faster than the 13-inch model with the M1 chip, and up to 3x faster for customers who haven’t upgraded to a Mac with Apple silicon.<sup>1</sup></li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Video editing in Final Cut Pro is up to 60 percent faster than the 13-inch model with the M1 chip, and up to 13x faster for customers who haven’t upgraded to a Mac with Apple silicon.<sup>1</sup></li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Compared to a PC laptop with an Intel Core i7 processor, MacBook Air delivers up to 2x faster performance, up to 50 percent faster web browsing, and up to 40 percent longer battery life.<sup>1</sup></li>
</ul>
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Excel and Slack are shown on the new MacBook Air.">
        <div>
             
              
              <div>
                For everyone from students to business users, MacBook Air with M3 takes productivity to new heights with incredible performance and battery life, making tasks like collaborating on Excel even faster.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-Excel-Slack-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-Excel-Slack-240304_big" aria-label="Download media, Excel and Slack are shown on the new MacBook Air."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>World’s Best Consumer Laptop for AI</strong>
</h2>
                 
             
                 <div>With the transition to Apple silicon, every Mac is a great platform for AI. M3 includes a faster and more efficient 16-core Neural Engine, along with accelerators in the CPU and GPU to boost on-device machine learning, making MacBook Air the world’s best consumer laptop for AI. Leveraging this incredible AI performance, macOS delivers intelligent features that enhance productivity and creativity, so users can enable powerful camera features, real-time speech to text, translation, text predictions, visual understanding, accessibility features, and much more.
</div>
                 
             
                 <div>With a broad ecosystem of apps that deliver advanced AI features, users can do everything from checking their homework with AI Math Assistance in Goodnotes 6, to automatically enhancing photos in Pixelmator Pro, to removing background noise from a video using CapCut. Combined with the unified memory architecture of Apple silicon, MacBook Air can also run optimized AI models, including large language models (LLMs) and diffusion models for image generation locally with great performance. In addition to on-device performance, MacBook Air supports cloud-based solutions, enabling users to run powerful productivity and creative apps that tap into the power of AI, such as Microsoft Copilot for Microsoft 365, Canva, and Adobe Firefly.
</div>
                 
             
                 <h2><strong>World’s Most Popular Laptop</strong>
</h2>
                 
             
                 <div>More people choose MacBook Air over any other laptop, and M3 raises the bar yet again with its phenomenal combination of performance, portability, and capabilities users love:
</div>
                 
             
                 <div><ul>
<li><strong>Two perfect sizes in a super-portable design</strong>: With a durable aluminum enclosure that’s built to last, the 13- and 15‑inch MacBook Air have fantastic battery life, are incredibly light, and are less than half an inch thin, so users can work, play, or create from&nbsp;anywhere. The 13-inch model provides the ultimate in portability, while the 15-inch model offers even more screen real estate for multitasking. There’s a perfect size for everyone, from students on the go to business professionals who prefer a larger screen.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Gorgeous Liquid Retina display</strong>: MacBook Air features a brilliant 13.6- or 15.3-inch Liquid Retina display with up to 500 nits of brightness, support for 1 billion colors, and up to 2x the resolution of comparable PC laptops. Content looks vivid with sharp detail, and text appears super crisp.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Support for up to two external displays</strong>: MacBook Air with M3 now supports up to two external displays when the laptop lid is closed — perfect for business users, or anyone who requires multiple displays for multitasking across apps or spreading out documents at the same time.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Versatile connectivity</strong>: MacBook Air with M3 features Wi-Fi 6E, which delivers download speeds that are up to twice as fast as the previous generation. It also includes MagSafe charging and two Thunderbolt ports for connecting accessories, along with a 3.5mm headphone jack.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Camera, mics, and speakers</strong>: With a 1080p FaceTime HD camera, users will look their best whether they’re connecting with friends and family, or collaborating with coworkers around the&nbsp;world. Users will also sound their best with a three-mic array and enhanced voice clarity on audio and video calls. MacBook Air features an immersive sound system with support for Spatial Audio along with Dolby Atmos, so users can enjoy three-dimensional soundstages for music and movies.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Magic Keyboard and Touch ID</strong>:&nbsp;The comfortable and quiet backlit Magic&nbsp;Keyboard comes with a full-height function row with Touch&nbsp;ID, giving users a fast, easy, and secure way to unlock their Mac; sign in to apps and websites; and make purchases with Apple Pay — all with the touch of a&nbsp;finger.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="new-macbook-air">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-6ed126ae2a3ca9fefd72d54215a07c25" href="#gallery-6ed126ae2a3ca9fefd72d54215a07c25" data-ac-gallery-trigger="gallery-6ed126ae2a3ca9fefd72d54215a07c25"><span>Colorful graphics are shown on two new MacBook Air devices.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-a23819ef1681137fd1c9c759183c567f" href="#gallery-a23819ef1681137fd1c9c759183c567f" data-ac-gallery-trigger="gallery-a23819ef1681137fd1c9c759183c567f"><span>A person wearing a bright red fuzzy coat is shown on the new MacBook Air.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-efd7e9eeee3351ebbacec5ecdc3db30c" href="#gallery-efd7e9eeee3351ebbacec5ecdc3db30c" data-ac-gallery-trigger="gallery-efd7e9eeee3351ebbacec5ecdc3db30c"><span>A person does some work on the new MacBook Air with two external displays.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-5eaa1b4bd58b6137aa14912cdcde552d" href="#gallery-5eaa1b4bd58b6137aa14912cdcde552d" data-ac-gallery-trigger="gallery-5eaa1b4bd58b6137aa14912cdcde552d"><span>A close-up on the keyboard of the new MacBook Air, shown in midnight.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-6ed126ae2a3ca9fefd72d54215a07c25" aria-labelledby="gallery-dotnav-6ed126ae2a3ca9fefd72d54215a07c25" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:two-screens">
                                
                                <div>
                                    <div>Available in two perfect sizes, the 13-inch MacBook Air delivers the ultimate in portability, while the 15-inch model offers even more screen real estate for multitasking in a thin and light design.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-2-up-front-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-2-up-front-240304_big" aria-label="Download media, Colorful graphics are shown on two new MacBook Air devices."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-a23819ef1681137fd1c9c759183c567f" aria-labelledby="gallery-dotnav-a23819ef1681137fd1c9c759183c567f" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:person-in-red-coat">
                                
                                <div>
                                    <div>MacBook Air with M3 features a stunning Liquid Retina display, making photos and movies look incredibly vibrant.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-Liquid-Retina-Display-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-Liquid-Retina-Display-240304_big" aria-label="Download media, A person wearing a bright red fuzzy coat is shown on the new MacBook Air."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-efd7e9eeee3351ebbacec5ecdc3db30c" aria-labelledby="gallery-dotnav-efd7e9eeee3351ebbacec5ecdc3db30c" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:person-working">
                                
                                <div>
                                    <div>With support for up to two external displays when the laptop lid is closed, it’s easy to multitask across apps and spread out across two screens.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-lifestyle-display-support-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-lifestyle-display-support-240304_big" aria-label="Download media, A person does some work on the new MacBook Air with two external displays."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-5eaa1b4bd58b6137aa14912cdcde552d" aria-labelledby="gallery-dotnav-5eaa1b4bd58b6137aa14912cdcde552d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:keyboard">
                                
                                <div>
                                    <div>MacBook Air comes in four great colors, including midnight, which features a breakthrough anodization seal to reduce fingerprints.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-keyboard-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-keyboard-240304_big" aria-label="Download media, A close-up on the keyboard of the new MacBook Air, shown in midnight."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>The Magic of macOS</strong>
</h2>
                 
             
                 <div>Together with macOS, the MacBook Air experience is unrivaled:
</div>
                 
             
                 <div><ul>
<li><strong>macOS Sonoma</strong>: Users can now place widgets right on the desktop, interact with them with just a click, and even access the extensive ecosystem of iPhone widgets on MacBook Air. Video conferencing gets more engaging with great features like Presenter Overlay and Reactions. Profiles in Safari keep browsing separate between multiple topics or projects, while web apps provide faster access to favorite websites. And gaming gets even better with Game Mode.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Enhanced productivity</strong>:<strong> </strong>All users, including business professionals, can take advantage of the expansive display on MacBook Air with Split View, or spread out across screens with support for up to two external displays. Features like Stage Manager also help users like students focus on the task in front of them.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Better with iPhone</strong>:<strong> </strong>With Continuity,<strong> </strong>MacBook Air works seamlessly across iPhone and other Apple devices. Features like AirDrop allow users to share and receive photos, documents,&nbsp;and more&nbsp;across nearby Apple devices. Universal Clipboard lets users easily copy images, video, or text from an app on one Apple device, and effortlessly paste them into another app on a nearby Mac. Continuity Camera makes it easy for users to scan or take a picture of something nearby with their iPhone and have it appear instantly on their Mac. And Handoff lets them start a task like answering an email on one Apple device and easily finish it on another.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Wide array of apps</strong>: MacBook Air comes with powerful apps built in, including FaceTime, Freeform, iMovie, GarageBand, and Photos, as well as productivity apps including Pages, Numbers, and Keynote, making it easy for users to create amazing work. And with thousands of apps optimized for Apple silicon, all of users’ go-to apps run incredibly fast in macOS — including Microsoft 365 and many of their favorite iOS apps.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="macbook-air-macos">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-81f5c6559a2da797a4a213d35c767a1d" href="#gallery-81f5c6559a2da797a4a213d35c767a1d" data-ac-gallery-trigger="gallery-81f5c6559a2da797a4a213d35c767a1d"><span>A person wearing headphones plays a game on the new MacBook Air.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-83e3d84b76bd7ce15b07838ab1aa94ff" href="#gallery-83e3d84b76bd7ce15b07838ab1aa94ff" data-ac-gallery-trigger="gallery-83e3d84b76bd7ce15b07838ab1aa94ff"><span>The Continuity feature is shown across MacBook Air and iPhone 15 Pro, with a user’s PowerPoint project appearing on both screens.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-81f5c6559a2da797a4a213d35c767a1d" aria-labelledby="gallery-dotnav-81f5c6559a2da797a4a213d35c767a1d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:game-mode">
                                
                                <div>
                                    <div>Games get even better on MacBook Air with M3, delivering incredible performance, breathtaking graphics, and features like Game Mode in macOS Sonoma.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-lifestyle-gaming-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-lifestyle-gaming-240304_big" aria-label="Download media, A person wearing headphones plays a game on the new MacBook Air."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-83e3d84b76bd7ce15b07838ab1aa94ff" aria-labelledby="gallery-dotnav-83e3d84b76bd7ce15b07838ab1aa94ff" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:continuity">
                                
                                <div>
                                    <div>Users can work seamlessly across Mac and iPhone with Continuity features like AirDrop, Universal Clipboard, Continuity Camera, and Handoff.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/article/Apple-MacBook-Air-Continuity-PowerPoint-240304.zip" download="" data-analytics-title="download image - Apple-MacBook-Air-Continuity-PowerPoint-240304_big" aria-label="Download media, The Continuity feature is shown across MacBook Air and iPhone 15 Pro, with a user’s PowerPoint project appearing on both screens."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Better for the Environment</strong>
</h2>
                 
             
                 <div>The new MacBook Air is the first Apple product to be made with 50 percent recycled content, including 100 percent recycled aluminum in the enclosure, 100 percent recycled rare earth elements in all magnets and, in another first for Apple, 100 percent recycled copper in the main logic board. MacBook Air meets Apple’s high standards for energy efficiency, and is free of mercury, brominated flame retardants, and PVC. The packaging is 99 percent fiber-based, bringing Apple closer to its goal to remove plastic from all packaging by 2025.
</div>
                 
             
                 <div>Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be carbon neutral across the entire manufacturing supply chain and the life cycle of every product.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>Customers can order the new MacBook Air with M3 starting Monday, March 4, on <a href="https://www.apple.com/store/" target="_blank">apple.com/store</a> and in the Apple Store app in 28 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, starting Friday, March 8.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The 13-inch MacBook Air with M3 starts at <strong>$1,099 </strong>(U.S.) and<strong> $999 </strong>(U.S.) for education, and the 15‑inch MacBook Air with M3 starts at<strong> $1,299</strong> (U.S.) and<strong> $1,199</strong> (U.S.) for education. Both are available in midnight, starlight, silver, and space gray.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The 13-inch MacBook Air with M2, available in midnight, starlight, silver, and space gray, now starts at<strong> $999 </strong>(U.S.) and <strong>$899 </strong>(U.S.) for education.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Additional technical specifications, configure-to-order options, and accessories are available at <a href="https://www.apple.com/mac/" target="_blank">apple.com/mac</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit <a href="https://www.apple.com/shop/trade-in/" target="_blank">apple.com/shop/trade-in</a> to see what their device is worth.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>When customers shop at Apple using Apple Card, they can pay monthly at 0 percent APR for their new MacBook Air when they choose to check out with Apple Card Monthly Installments, and they’ll get 3 percent Daily Cash back — all upfront.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Every customer who buys a Mac from their Apple Store can enjoy a free Online Personal Session with an Apple Specialist, get their product set up — including help with data transfer — and receive guidance on how to get the most out of their new Mac.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Testing was conducted by Apple in January 2024. See <a href="https://www.apple.com/macbook-air/" target="_blank">apple.com/macbook-air</a> for more information.</li>
</ol>

        </div>



    
    
    






    
















	
	
	
		















	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
    </channel>
</rss>