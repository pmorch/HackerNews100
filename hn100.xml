<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 07 Nov 2023 13:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Inshellisense – IDE style shell autocomplete (102 pts)]]></title>
            <link>https://github.com/microsoft/inshellisense</link>
            <guid>38167363</guid>
            <pubDate>Mon, 06 Nov 2023 19:15:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/inshellisense">https://github.com/microsoft/inshellisense</a>, See on <a href="https://news.ycombinator.com/item?id=38167363">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-inshellisense" dir="auto"><a href="#inshellisense">inshellisense</a></h2>
<p dir="auto"><code>inshellisense</code> provides IDE style autocomplete for shells. It's a terminal native runtime for <a href="https://github.com/withfig/autocomplete">autocomplete</a> which has support for 600+ command line tools. <code>inshellisense</code> supports Windows, Linux, &amp; MacOS.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/inshellisense/blob/main/docs/demo.gif"><img alt="demo of inshellisense working" src="https://github.com/microsoft/inshellisense/raw/main/docs/demo.gif" height="450px" data-animated-image=""></a></p>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting Started</a></h2>
<h3 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="npm install -g @microsoft/inshellisense"><pre>npm install -g @microsoft/inshellisense</pre></div>
<h3 tabindex="-1" id="user-content-quickstart" dir="auto"><a href="#quickstart">Quickstart</a></h3>
<p dir="auto">After completing the installation, you can already run <code>inshellisense --shell &lt;shell&gt;</code> to start the autocomplete session for your desired shell. Additionally, you can bind <code>inshellisense</code> to a keybinding of <code>CTRL+a</code> by running the below command. This brings the added advantages of automatically starting the autocomplete session with your current shell and injecting any accepted command into your shell's history.</p>

<p dir="auto">Additionally, inshellisense is also aliased under <code>is</code> after install for convenience.</p>
<h2 tabindex="-1" id="user-content-integrations" dir="auto"><a href="#integrations">Integrations</a></h2>
<p dir="auto">inshellisense supports the following shells:</p>
<ul dir="auto">
<li><a href="https://www.gnu.org/software/bash/" rel="nofollow">bash</a></li>
<li><a href="https://www.zsh.org/" rel="nofollow">zsh</a></li>
<li><a href="https://github.com/fish-shell/fish-shell">fish</a></li>
<li><a href="https://github.com/PowerShell/PowerShell">pwsh</a></li>
<li><a href="https://learn.microsoft.com/en-us/powershell/scripting/windows-powershell/starting-windows-powershell" rel="nofollow">powershell</a> (Windows Powershell)</li>
</ul>
<h2 tabindex="-1" id="user-content-contributing" dir="auto"><a href="#contributing">Contributing</a></h2>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<h2 tabindex="-1" id="user-content-trademarks" dir="auto"><a href="#trademarks">Trademarks</a></h2>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPTs: Custom versions of ChatGPT (302 pts)]]></title>
            <link>https://openai.com/blog/introducing-gpts</link>
            <guid>38166431</guid>
            <pubDate>Mon, 06 Nov 2023 18:18:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/blog/introducing-gpts">https://openai.com/blog/introducing-gpts</a>, See on <a href="https://news.ycombinator.com/item?id=38166431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div><p>We’re rolling out custom versions of ChatGPT that you can create for a specific purpose—called GPTs. GPTs are a new way for anyone to create a tailored version of ChatGPT to be more helpful in their daily life, at specific tasks, at work, or at home—and then share that creation with others. For example, GPTs can help you <a href="https://openai.com/chatgpt#do-more-with-gpts" rel="noopener noreferrer">learn the rules to any board game, help teach your kids math, or design stickers</a>.</p><p>Anyone can easily build their own GPT—no coding is required. You can make them for yourself, just for your company’s internal use, or for everyone. Creating one is as easy as starting a conversation, giving it instructions and extra knowledge, and picking what it can do, like searching the web, making images or analyzing data.</p><p>Example GPTs are available today for ChatGPT Plus and Enterprise users to try out including <a href="https://chat.openai.com/g/g-alKfVrz9K-canva" rel="noopener noreferrer" target="_blank">Canva</a> and <a href="https://zapier.com/blog/gpt-assistant/" rel="noopener noreferrer" target="_blank">Zapier AI Actions</a>. We plan to offer GPTs to more users soon.</p><p><em><br>Learn more about our </em><a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday" rel="noopener noreferrer" target="_blank"><em>OpenAI DevDay announcements for new models and developer products</em></a><em>.</em><br></p></div><!--]--><!--[--><div id="gpts-let-you-customize-chatgpt-for-a-specific-purpose" data-heading=""><p><h3>GPTs let you customize ChatGPT for a specific purpose</h3></p></div><!--]--><!--[--><div><p>Since launching ChatGPT people have been asking for ways to customize ChatGPT to fit specific ways that they use it. We launched Custom Instructions in July that let you set some preferences, but requests for more control kept coming. Many power users maintain a list of carefully crafted prompts and instruction sets, manually copying them into ChatGPT. GPTs now do all of that for you.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p>We believe the most incredible GPTs will come from builders in the community. Whether you’re an educator, coach, or just someone who loves to build helpful tools, you don’t need to know coding to make one and share your expertise. <br></p></div><!--]--><!--[--><div id="the-gpt-store-is-rolling-out-later-this-month" data-heading=""><p><h3>The GPT Store is rolling out later this month</h3></p></div><!--]--><!--[--><div><p>Starting today, you can create GPTs and share them publicly. Later this month, we’re launching the GPT Store, featuring creations by verified builders. Once in the store, GPTs become searchable and may climb the leaderboards. We will also spotlight the most useful and delightful GPTs we come across in categories like productivity, education, and “just for fun”. In the coming months, you’ll also be able to earn money based on how many people are using your GPT.<br></p></div><!--]--><!--[--><div id="we-built-gpts-with-privacy-and-safety-in-mind" data-heading=""><p><h3>We built GPTs with privacy and safety in mind</h3></p></div><!--]--><!--[--><div><p>As always, you are in control of your data with ChatGPT. Your chats with GPTs are not shared with builders. If a GPT uses third party APIs, you choose whether data can be sent to that API. When builders customize their own GPT with actions or knowledge, the builder can choose if user chats with that GPT can be used to improve and train our models. These choices build upon the existing <a href="https://help.openai.com/en/articles/7730893-data-controls-faq" rel="noopener noreferrer" target="_blank">privacy controls</a> users have, including the option to opt your entire account out of model training.&nbsp;</p><p>We’ve set up new systems to help review GPTs against our <a href="https://openai.com/policies/usage-policies" rel="noopener noreferrer" target="_blank">usage policies</a>. These systems stack on top of our existing mitigations and aim to prevent users from sharing harmful GPTs, including those that involve fraudulent activity, hateful content, or adult themes. We’ve also taken steps to build user trust by allowing builders to verify their identity. We'll continue to monitor and learn how people use GPTs and update and strengthen our safety mitigations. If you have concerns with a specific GPT, you can also use our reporting feature on the GPT shared page to notify our team.</p><p>GPTs will continue to get more useful and smarter, and you’ll eventually be able to let them take on real tasks in the real world. In the field of AI, these systems are often discussed as “agents”. We think it’s important to move incrementally towards this future, as it will require careful technical and safety work—and time for society to adapt. We have been thinking deeply about the societal implications and will have more analysis to share soon.<br></p></div><!--]--><!--[--><div id="developers-can-connect-gpts-to-the-real-world" data-heading=""><p><h3>Developers can connect GPTs to the real world</h3></p></div><!--]--><!--[--><div><p>In addition to using our built-in capabilities, you can also define custom actions by making one or more APIs available to the GPT. Like plugins, actions allow GPTs to integrate external data or interact with the real-world. Connect GPTs to databases, plug them into emails, or make them your shopping assistant. For example, you could integrate a travel listings database, connect a user’s email inbox, or facilitate e-commerce orders.</p><p>The design of actions builds upon insights from our plugins beta, granting developers greater control over the model and how their APIs are called. Migrating from the plugins beta is easy with the ability to use your existing plugin manifest to define actions for your GPT.<br></p></div><!--]--><!--[--><div id="enterprise-customers-can-deploy-internal-only-gpts" data-heading=""><p><h3>Enterprise customers can deploy internal-only GPTs</h3></p></div><!--]--><!--[--><div><p>Since we launched ChatGPT Enterprise a few months ago, early customers have expressed the desire for even more customization that aligns with their business. GPTs answer this call by allowing you to create versions of ChatGPT for specific use cases, departments, or proprietary datasets. Early customers like Amgen, Bain, and Square are already leveraging internal GPTs to do things like craft marketing materials embodying their brand, aid support staff with answering customer questions, or help new software engineers with onboarding.</p><p>Enterprises can get started with GPTs on Wednesday. You can now empower users inside your company to design internal-only GPTs without code and securely publish them to your workspace. The admin console lets you choose how GPTs are shared and whether external GPTs may be used inside your business. Like all usage on ChatGPT Enterprise, we do not use your conversations with GPTs to improve our models.<br></p></div><!--]--><!--[--><div id="we-want-more-people-to-shape-how-ai-behaves" data-heading=""><p><h3>We want more people to shape how AI behaves</h3></p></div><!--]--><!--[--><div><p>We designed GPTs so more people can build with us. Involving the community is critical to our mission of building safe AGI that benefits humanity. It allows everyone to see a wide and varied range of useful GPTs and get a more concrete sense of what’s ahead. And by broadening the group of people who decide 'what to build' beyond just those with access to advanced technology it's likely we'll have safer and better aligned AI. The same desire to build with people, not just for them, drove us to launch the OpenAI API and to research methods for incorporating democratic input into AI behavior, which we plan to share more about soon.<br></p></div><!--]--><!--[--><div id="we-ve-made-chatgpt-plus-fresher-and-simpler-to-use" data-heading=""><p><h3>We’ve made ChatGPT Plus fresher and simpler to use</h3></p></div><!--]--><!--[--><div><p>Finally, ChatGPT Plus now includes fresh information up to April 2023. We’ve also heard your feedback about how the model picker is a pain. Starting today, no more hopping between models; everything you need is in one place. You can access DALL·E, browsing, and data analysis all without switching. You can also attach files to let ChatGPT search PDFs and other document types. Find us at <a href="https://chatgpt.com/" rel="noopener noreferrer" target="_blank">chatgpt.com</a>.<br></p></div><!--]--><!--[--><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New models and developer products (331 pts)]]></title>
            <link>https://openai.com/blog/new-models-and-developer-products-announced-at-devday</link>
            <guid>38166420</guid>
            <pubDate>Mon, 06 Nov 2023 18:17:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">https://openai.com/blog/new-models-and-developer-products-announced-at-devday</a>, See on <a href="https://news.ycombinator.com/item?id=38166420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div><p>Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:</p><ul><li>New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window</li><li>New Assistants API that makes it easier for developers to build their own assistive AI apps that have goals and can call models and tools</li><li>New multimodal capabilities in the platform, including vision, image creation (DALL·E 3), and text-to-speech (TTS)</li></ul><p>We’ll begin rolling out new features to OpenAI customers starting at 1pm PT today.</p><p><em><br>Learn more about </em><a href="https://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank"><em>OpenAI DevDay announcements for ChatGPT</em></a><em>.</em><br></p></div><!--]--><!--[--><div id="gpt-4-turbo-with-128k-context" data-heading=""><p><h2>GPT-4 Turbo with 128K context</h2></p></div><!--]--><!--[--><div><p>We released the first version of GPT-4 in March and made GPT-4 generally available to all developers in July. Today we’re launching a preview of the next generation of this model, <a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="noopener noreferrer" target="_blank">GPT-4 Turbo</a>.&nbsp;</p><p>GPT-4 Turbo is more capable and has knowledge of world events up to April 2023. It has a 128k context window so it can fit the equivalent of more than 300 pages of text in a single prompt. We also optimized its performance so we are able to offer GPT-4 Turbo at a <a href="https://openai.com/pricing#gpt-4-turbo" rel="noopener noreferrer" target="_blank">3x cheaper</a> price for input tokens and a 2x cheaper price for output tokens compared to GPT-4.</p><p>GPT-4 Turbo is available for all paying developers to try by passing <code>gpt-4-1106-preview</code> in the API and we plan to release the stable production-ready model in the coming weeks.<br></p></div><!--]--><!--[--><div id="function-calling-updates" data-heading=""><p><h3>Function calling updates</h3></p></div><!--]--><!--[--><div><p><a href="https://platform.openai.com/docs/guides/function-calling" rel="noopener noreferrer" target="_blank">Function calling</a> lets you describe functions of your app or external APIs to models, and have the model intelligently choose to output a JSON object containing arguments to call those functions. We’re releasing several improvements today, including the ability to call multiple functions in a single message: users can send one message requesting multiple actions, such as “open the car window and turn off the A/C”,&nbsp;which would previously require multiple roundtrips with the model (<a href="https://platform.openai.com/docs/guides/function-calling/parallel-function-calling" rel="noopener noreferrer" target="_blank">learn more</a>). We are also improving function calling accuracy: GPT-4 Turbo is more likely to return the right function parameters.<br></p></div><!--]--><!--[--><div id="improved-instruction-following-and-json-mode" data-heading=""><p><h3>Improved instruction following and JSON mode</h3></p></div><!--]--><!--[--><div><p>GPT-4 Turbo performs better than our previous models on tasks that require the careful following of instructions, such as generating specific formats (e.g., “always respond in XML”). It also supports our new <a href="https://platform.openai.com/docs/guides/text-generation/json-mode" rel="noopener noreferrer" target="_blank">JSON mode</a>, which ensures the model will respond with valid JSON. The new API parameter <code>response_format</code> enables the model to constrain its output to generate a syntactically correct JSON object. JSON mode is useful for developers generating JSON in the Chat Completions API outside of function calling.<br></p></div><!--]--><!--[--><div id="reproducible-outputs-and-log-probabilities" data-heading=""><p><h3>Reproducible outputs and log probabilities</h3></p></div><!--]--><!--[--><div><p>The new <code>seed</code> parameter enables <strong>reproducible outputs</strong> by making the model return consistent completions most of the time. This beta feature is useful for use cases such as replaying requests for debugging, writing more comprehensive unit tests, and generally having a higher degree of control over the model behavior. We at OpenAI have been using this feature internally for our own unit tests and have found it invaluable. We’re excited to see how developers will use it. <a href="https://platform.openai.com/docs/guides/text-generation/reproducible-outputs" rel="noopener noreferrer" target="_blank">Learn more</a>.<br></p><p>We’re also launching a feature to return the <strong>log probabilities</strong> for the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo in the next few weeks, which will be useful for building features such as autocomplete in a search experience.<br></p></div><!--]--><!--[--><div id="updated-gpt-3-5-turbo" data-heading=""><p><h3>Updated GPT-3.5 Turbo</h3></p></div><!--]--><!--[--><div><p>In addition to GPT-4 Turbo, we are also releasing a new version of GPT-3.5 Turbo that supports a 16K context window by default. The new 3.5 Turbo supports improved instruction following, JSON mode, and parallel function calling. For instance, our internal evals show a 38% improvement on format following tasks such as generating JSON, XML and YAML. Developers can access this new model by calling <code>gpt-3.5-turbo-1106</code> in the API. Applications using the <code>gpt-3.5-turbo</code> name will automatically be upgraded to the new model on December 11. Older models will continue to be accessible by passing <code>gpt-3.5-turbo-0613</code> in the API until June 13, 2024. <a href="https://platform.openai.com/docs/models/gpt-3-5" rel="noopener noreferrer" target="_blank">Learn more</a>.<br></p></div><!--]--><!--[--><div id="assistants-api-retrieval-and-code-interpreter" data-heading=""><p><h2>Assistants API, Retrieval, and Code Interpreter</h2></p></div><!--]--><!--[--><div><p>Today, we’re releasing the <a href="https://platform.openai.com/docs/assistants/overview" rel="noopener noreferrer" target="_blank">Assistants API</a>, our first step towards helping developers build agent-like experiences within their own applications. An assistant is a purpose-built AI that has specific instructions, leverages extra knowledge, and can call models and tools to perform tasks. The new Assistants API provides new capabilities such as Code Interpreter and Retrieval as well as function calling to handle a lot of the heavy lifting that you previously had to do yourself and enable you to build high-quality AI apps.</p><p>This API is designed for flexibility; use cases range from a natural language-based data analysis app, a coding assistant, an AI-powered vacation planner, a voice-controlled DJ, a smart visual canvas—the list goes on. The Assistants API is built on the same capabilities that enable <a href="http://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank">our new GPTs product</a>: custom instructions and tools such as Code interpreter, Retrieval, and function calling.</p><p>A key change introduced by this API is <strong>persistent and infinitely long threads</strong>, which allow developers to hand off thread state management to OpenAI and work around context window constraints. With the Assistants API, you simply add each new message to an existing <code>thread</code>.</p><p>Assistants also have access to call new tools as needed, including:</p><ul><li><strong>Code Interpreter</strong>: writes and runs Python code in a sandboxed execution environment, and can generate graphs and charts, and process files with diverse data and formatting. It allows your assistants to run code iteratively to solve challenging code and math problems, and more.</li><li><strong>Retrieval</strong>: augments the assistant with knowledge from outside our models, such as proprietary domain data, product information or documents provided by your users. This means you don’t need to compute and store embeddings for your documents, or implement chunking and search algorithms. The Assistants API optimizes what retrieval technique to use based on our experience building knowledge retrieval in ChatGPT.</li><li><strong>Function calling</strong>: enables assistants to invoke functions you define and incorporate the function response in their messages.</li></ul><p>As with the rest of the platform, data and files passed to the OpenAI API are <a href="https://openai.com/enterprise-privacy" rel="noopener noreferrer" target="_blank">never used to train our models</a> and developers can delete the data when they see fit.</p><p>You can try the Assistants API beta without writing any code by heading to the <a href="https://platform.openai.com/playground?mode=assistant" rel="noopener noreferrer" target="_blank">Assistants playground</a>.<br></p></div><!--]--><!--[--><div><video autoplay="" loop="" muted="" playsinline="true" src="https://cdn.openai.com/new-models-and-developer-products-announced-at-devday/assistants-playground.mp4" poster="https://cdn.openai.com/new-models-and-developer-products-announced-at-devday/assistants-playground-poster.jpg"></video><p>Use the Assistants playground to create high quality assistants without code.</p></div><!--]--><!--[--><div><p>The Assistants API is in beta and available to all developers starting today. Please share what you build with us (<a href="https://twitter.com/OpenAI" rel="noopener noreferrer" target="_blank">@OpenAI</a>) along with your feedback which we will incorporate as we continue building over the coming weeks. Pricing for the Assistants APIs and its tools is available <a href="https://openai.com/pricing" rel="noopener noreferrer" target="_blank">on our pricing page</a>.<br></p></div><!--]--><!--[--><div id="new-modalities-in-the-api" data-heading=""><p><h2>New modalities in the API</h2></p></div><!--]--><!--[--><div id="gpt-4-turbo-with-vision" data-heading=""><p><h3>GPT-4 Turbo with vision</h3></p></div><!--]--><!--[--><div><p>GPT-4 Turbo can accept images as inputs in the Chat Completions API, enabling use cases such as generating captions, analyzing real world images in detail, and reading documents with figures. For example, BeMyEyes uses this technology to help people who are blind or have low vision with daily tasks like identifying a product or navigating a store. Developers can access this feature by using <code>gpt-4-vision-preview</code> in the API. We plan to roll out vision support to the main GPT-4 Turbo model as part of its stable release. <a href="https://openai.com/pricing" rel="noopener noreferrer" target="_blank">Pricing</a> depends on the input image size. For instance, passing an image with 1080×1080 pixels to GPT-4 Turbo costs $0.00765. Check out <a href="https://platform.openai.com/docs/guides/vision" rel="noopener noreferrer" target="_blank">our vision guide</a>.<br></p></div><!--]--><!--[--><div id="dall-e-3" data-heading=""><p><h3>DALL·E 3</h3></p></div><!--]--><!--[--><div><p>Developers can integrate DALL·E 3, which we <a href="https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise" rel="noopener noreferrer" target="_blank">recently launched</a> to ChatGPT Plus and Enterprise users, directly into their apps and products through our Images API by specifying <code>dall-e-3</code> as the model. Companies like Snap, Coca-Cola, and Shutterstock have used DALL·E 3 to programmatically generate images and designs for their customers and campaigns. Similar to the previous version of DALL·E, the API incorporates built-in moderation to help developers protect their applications against misuse. We offer different format and quality options, with prices starting at $0.04 per image generated. Check out our <a href="https://platform.openai.com/docs/guides/images" rel="noopener noreferrer" target="_blank">guide to getting started</a> with DALL·E 3 in the API.<br></p></div><!--]--><!--[--><div id="text-to-speech-tts" data-heading=""><p><h3>Text-to-speech (TTS)</h3></p></div><!--]--><!--[--><div><p>Developers can now generate human-quality speech from text via the text-to-speech API. Our new TTS model offers six preset voices to choose from and two model variants, <code>tts-1</code> and <code>tts-1-hd</code>. <code>tts</code> is optimized for real-time use cases and <code>tts-1-hd</code> is optimized for quality. Pricing starts at $0.015 per input 1,000 characters. Check out our <a href="https://platform.openai.com/docs/guides/text-to-speech" rel="noopener noreferrer" target="_blank">TTS guide</a> to get started.<br></p></div><!--]--><!--[--><div><h4>Listen to voice samples</h4><p><label><span>Select text</span></label></p><blockquote><p><span>As the golden sun dips below the horizon, casting long shadows across the tranquil meadow, the world seems to hush, and a sense of calmness envelops the Earth, promising a peaceful night’s rest for all living beings.</span></p></blockquote><p><label><span>Select voice</span></label><audio src="https://cdn.openai.com/new-models-and-developer-products-announced-at-devday/audio/scenic-alloy.mp3" controls=""></audio></p></div><!--]--><!--[--><div id="model-customization" data-heading=""><p><h2>Model customization</h2></p></div><!--]--><!--[--><div id="gpt-4-fine-tuning-experimental-access" data-heading=""><p><h3>GPT-4 fine tuning experimental access</h3></p></div><!--]--><!--[--><div><p>We’re creating an experimental access program for <strong>GPT-4 fine-tuning</strong>. Preliminary results indicate that GPT-4 fine-tuning requires more work to achieve meaningful improvements over the base model compared to the substantial gains realized with GPT-3.5 fine-tuning. As quality and safety for GPT-4 fine-tuning improves, developers actively using GPT-3.5 fine-tuning will be presented with an option to apply to the GPT-4 program within their <a href="https://platform.openai.com/finetune" rel="noopener noreferrer" target="_blank">fine-tuning console</a>.<br></p></div><!--]--><!--[--><div id="custom-models" data-heading=""><p><h3>Custom models</h3></p></div><!--]--><!--[--><div><p>For organizations that need even more customization than fine-tuning can provide (particularly applicable to domains with extremely large proprietary datasets—billions of tokens at minimum), we’re also launching a <strong>Custom Models program</strong>, giving selected organizations an opportunity to work with a dedicated group of OpenAI researchers to train custom GPT-4 to their specific domain. This includes modifying every step of the model training process, from doing additional domain specific pre-training, to running a custom RL post-training process tailored for the specific domain. Organizations will have exclusive access to their custom models. In keeping with our existing enterprise privacy policies, custom models will not be served to or shared with other customers or used to train other models.&nbsp;Also, proprietary data provided to OpenAI to train custom models will not be reused in any other context. This will be a very limited (and expensive) program to start—interested orgs can <a href="https://openai.com/form/custom-models" rel="noopener noreferrer" target="_blank">apply here</a>.<br></p></div><!--]--><!--[--><div id="lower-prices-and-higher-rate-limits" data-heading=""><p><h2>Lower prices and higher rate limits</h2></p></div><!--]--><!--[--><div id="lower-prices" data-heading=""><p><h3>Lower prices</h3></p></div><!--]--><!--[--><div><p>We’re <a href="https://openai.com/pricing" rel="noopener noreferrer" target="_blank">decreasing several prices</a> across the platform to pass on savings to developers (all prices below are expressed per 1,000 tokens):</p><ul><li>GPT-4 Turbo input tokens are 3x cheaper than GPT-4 at $0.01 and output tokens are 2x cheaper at $0.03.</li><li>GPT-3.5 Turbo input tokens are 3x cheaper than the previous 16K model at $0.001 and output tokens are 2x cheaper at $0.002. Developers previously using GPT-3.5 Turbo 4K benefit from a 33% reduction on input tokens at $0.001. Those lower prices only apply to the new GPT-3.5 Turbo introduced today.</li><li>Fine-tuned GPT-3.5 Turbo 4K model input tokens are reduced by 4x at $0.003 and output tokens are 2.7x cheaper at $0.006. Fine-tuning also supports 16K context at the same price as 4K with the new GPT-3.5 Turbo model. These new prices also apply to fine-tuned <code>gpt-3.5-turbo-0613</code> models.<br></li></ul></div><!--]--><!--[--><div><table><tbody><!--[--><!--[--><tr><!--[--><td><!----><!----></td><td><span>Older models</span><!----></td><td><span>New models</span><!----></td><!--]--></tr><!--]--><!--[--><tr><!--[--><td><span>GPT-4 Turbo</span><!----></td><td><span>GPT-4 8K
<span>Input: $0.03
Output: $0.06</span>

GPT-4 32K
<span>Input: $0.06
Output: $0.012</span></span><!----></td><td><span>GPT-4 Turbo 128K
<span>Input: $0.01
Output: $0.03</span></span><!----></td><!--]--></tr><!--]--><!--[--><tr><!--[--><td><span>GPT-3.5 Turbo</span><!----></td><td><span>GPT-3.5 Turbo 4K
<span>Input: $0.0015
Output: $0.002</span>

GPT-3.5 Turbo 16K
<span>Input: $0.003
Output: $0.004</span></span><!----></td><td><span>GPT-3.5 Turbo 16K
<span>Input: $0.001
Output: $0.002</span></span><!----></td><!--]--></tr><!--]--><!--[--><tr><!--[--><td><span>GPT-3.5 Turbo fine-tuning</span><!----></td><td><span>GPT-3.5 Turbo 4K fine-tuning
<span>Training: $0.008
Input: $0.012
Output: $0.016</span></span><!----></td><td><span>GPT-3.5 Turbo 4K and 16K fine-tuning
<span>Training: $0.008
Input: $0.003
Output: $0.006</span></span><!----></td><!--]--></tr><!--]--><!--]--></tbody></table></div><!--]--><!--[--><div id="higher-rate-limits" data-heading=""><p><h3>Higher rate limits</h3></p></div><!--]--><!--[--><div><p>To help you scale your applications, we’re doubling the tokens per minute limit for all our paying GPT-4 customers. You can view your new rate limits in your <a href="https://platform.openai.com/account/rate-limits" rel="noopener noreferrer" target="_blank">rate limit page</a>. We’ve also published our <a href="https://platform.openai.com/docs/guides/rate-limits/usage-tiers" rel="noopener noreferrer" target="_blank">usage tiers</a> that determine automatic rate limits increases, so you know what to expect in how your usage limits will automatically scale. You can now request increases to usage limits from your <a href="https://platform.openai.com/account/rate-limits" rel="noopener noreferrer" target="_blank">account settings</a>.<br></p></div><!--]--><!--[--><div id="copyright-shield" data-heading=""><p><h2>Copyright Shield</h2></p></div><!--]--><!--[--><div><p>OpenAI is committed to protecting our customers with built-in copyright safeguards in our systems. Today, we’re going one step further and introducing Copyright Shield—we will now step in and defend our customers, and pay the costs incurred, if you face legal claims around copyright infringement. This applies to generally available features of ChatGPT Enterprise and our developer platform.<br></p></div><!--]--><!--[--><div id="whisper-v3-and-consistency-decoder" data-heading=""><p><h2>Whisper v3 and Consistency Decoder</h2></p></div><!--]--><!--[--><div><p>We are releasing <a href="https://github.com/openai/whisper" rel="noopener noreferrer" target="_blank">Whisper large-v3,</a> the next version of our open source automatic speech recognition model (ASR) which features improved performance across languages. We also plan to support Whisper v3 in our API in the near future.</p><p>We are also open sourcing the <a href="https://github.com/openai/consistencydecoder" rel="noopener noreferrer" target="_blank">Consistency Decoder</a>, a drop in replacement for the Stable Diffusion VAE decoder. This decoder improves all images compatible with the by Stable Diffusion 1.0+ VAE, with significant improvements in text, faces and straight lines.<br></p></div><!--]--><!--[--><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI DevDay, Opening Keynote Livestream [video] (129 pts)]]></title>
            <link>https://www.youtube.com/watch?v=U9mJuUkhUzk</link>
            <guid>38165090</guid>
            <pubDate>Mon, 06 Nov 2023 16:55:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=U9mJuUkhUzk">https://www.youtube.com/watch?v=U9mJuUkhUzk</a>, See on <a href="https://news.ycombinator.com/item?id=38165090">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[XAI PromptIDE (130 pts)]]></title>
            <link>https://x.ai/prompt-ide/</link>
            <guid>38164886</guid>
            <pubDate>Mon, 06 Nov 2023 16:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x.ai/prompt-ide/">https://x.ai/prompt-ide/</a>, See on <a href="https://news.ycombinator.com/item?id=38164886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Integrated development environment for prompt engineering and interpretability research</p></div><div class="page"><p><small>November 6, 2023</small></p><p><strong>The xAI PromptIDE is an integrated development environment for prompt engineering and interpretability research. It accelerates prompt engineering through an SDK that allows implementing complex prompting techniques and rich analytics that visualize the network's outputs. We use it heavily in our continuous development of <a href="https://x.ai/">Grok</a>.</strong></p><p>We developed the PromptIDE to give transparent access to Grok-1, the model that powers <a href="https://x.ai/">Grok</a>, to engineers and researchers in the community. The IDE is designed to empower users and help them explore the capabilities of our large language models (LLMs) at pace. At the heart of the IDE is a Python code editor that - combined with a new <a href="https://x.ai/ide/docs.html">SDK</a> - allows implementing complex prompting techniques. While executing prompts in the IDE, users see helpful analytics such as the precise tokenization, sampling probabilities, alternative tokens, and aggregated attention masks.</p><p>The IDE also offers quality of life features. It automatically saves all prompts and has built-in versioning. The analytics generated by running a prompt can be stored permanently allowing users to compare the outputs of different prompting techniques. Finally, users can upload small files such as CSV files and read them using a single Python function from the SDK. When combined with the SDK's concurrency features, even somewhat large files can be processed quickly.</p><p>We also hope to build a community around the PromptIDE. Any prompt can be shared publicly at the click of a button. Users can decide if they only want to share a single version of the prompt or the entire tree. It's also possible to include any stored analytics when sharing a prompt.</p><p>The PromptIDE is available to members of our early <a href="https://ide.x.ai/">access program</a>. Below, you find a walkthrough of the main features of the IDE.</p><p>Thank you,<br> the xAI Team</p><h2 id="code-editor-sdk">Code editor &amp; SDK</h2><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42.png"></p><p>At the heart of the PromptIDE is a code editor and a <a href="https://x.ai/ide/docs.html">Python SDK</a>. The SDK provides a new programming paradigm that allows implementing complex prompting techniques elegantly. All Python functions are executed in an implicit context, which is a sequence of tokens. You can manually add tokens to the context using the <code>prompt()</code> function or you can use our models to generate tokens based on the context using the <code>sample()</code> function. When sampling from the model, you have various configuration options that are passed as argument to the function:</p><pre data-lang="python"><code data-lang="python"><span>async def </span><span>sample</span><span>(
</span><span>    </span><span>self</span><span>,
</span><span>    </span><span>max_len</span><span>: int = </span><span>256</span><span>,
</span><span>    </span><span>temperature</span><span>: float = </span><span>1.0</span><span>,
</span><span>    </span><span>nucleus_p</span><span>: float = </span><span>0.7</span><span>,
</span><span>    </span><span>stop_tokens</span><span>: Optional[list[str]] = </span><span>None</span><span>,
</span><span>    </span><span>stop_strings</span><span>: Optional[list[str]] = </span><span>None</span><span>,
</span><span>    </span><span>rng_seed</span><span>: Optional[int] = </span><span>None</span><span>,
</span><span>    </span><span>add_to_context</span><span>: bool = </span><span>True</span><span>,
</span><span>    </span><span>return_attention</span><span>: bool = </span><span>False</span><span>,
</span><span>    </span><span>allowed_tokens</span><span>: Optional[Sequence[Union[int, str]]] = </span><span>None</span><span>,
</span><span>    </span><span>disallowed_tokens</span><span>: Optional[Sequence[Union[int, str]]] = </span><span>None</span><span>,
</span><span>    </span><span>augment_tokens</span><span>: bool = </span><span>True</span><span>,
</span><span>) -&gt; SampleResult:
</span><span>    </span><span>"""Generates a model response based on the current prompt.
</span><span>
</span><span>    The current prompt consists of all text that has been added to the prompt either since the
</span><span>    beginning of the program or since the last call to `clear_prompt`.
</span><span>
</span><span>    Args:
</span><span>        max_len: Maximum number of tokens to generate.
</span><span>        temperature: Temperature of the final softmax operation. The lower the temperature, the
</span><span>            lower the variance of the token distribution. In the limit, the distribution collapses
</span><span>            onto the single token with the highest probability.
</span><span>        nucleus_p: Threshold of the Top-P sampling technique: We rank all tokens by their
</span><span>            probability and then only actually sample from the set of tokens that ranks in the
</span><span>            Top-P percentile of the distribution.
</span><span>        stop_tokens: A list of strings, each of which will be mapped independently to a single
</span><span>            token. If a string does not map cleanly to one token, it will be silently ignored.
</span><span>            If the network samples one of these tokens, sampling is stopped and the stop token
</span><span>            *is not* included in the response.
</span><span>        stop_strings: A list of strings. If any of these strings occurs in the network output,
</span><span>            sampling is stopped but the string that triggered the stop *will be* included in the
</span><span>            response. Note that the response may be longer than the stop string. For example, if
</span><span>            the stop string is "Hel" and the network predicts the single-token response "Hello",
</span><span>            sampling will be stopped but the response will still read "Hello".
</span><span>        rng_seed: See of the random number generator used to sample from the model outputs.
</span><span>        add_to_context: If true, the generated tokens will be added to the context.
</span><span>        return_attention: If true, returns the attention mask. Note that this can significantly
</span><span>            increase the response size for long sequences.
</span><span>        allowed_tokens: If set, only these tokens can be sampled. Invalid input tokens are
</span><span>            ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.
</span><span>        disallowed_tokens: If set, these tokens cannot be sampled. Invalid input tokens are
</span><span>            ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.
</span><span>        augment_tokens: If true, strings passed to `stop_tokens`, `allowed_tokens` and
</span><span>            `disallowed_tokens` will be augmented to include both the passed token and the
</span><span>            version with leading whitespace. This is useful because most words have two
</span><span>            corresponding vocabulary entries: one with leading whitespace and one without.
</span><span>
</span><span>    Returns:
</span><span>        The generated text.
</span><span>    """
</span></code></pre><p>The code is executed locally using an in-browser Python interpreter that runs in a separate web worker. Multiple web workers can run at the same time, which means you can execute many prompts in parallel.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42_completion.png"></p><p>Complex prompting techniques can be implemented using multiple contexts within the same program. If a function is annotated with the <code>@prompt_fn</code> decorator, it is executed in its own, fresh context. The function can perform some operations independently of its parent context and pass the results back to the caller using the <code>return</code> statement. This programming paradigm enables recursive and iterative prompts with arbitrarily nested sub-contexts.</p><h2 id="concurrency">Concurrency</h2><p>The SDK uses Python coroutines that enable processing multiple <code>@prompt_fn</code>-annotated Python functions concurrently. This can significantly speed up the time to completion - especially when working with CSV files.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_concurrency.png"></p><h2 id="user-inputs">User inputs</h2><p>Prompts can be made interactive through the <code>user_input()</code> function, which blocks execution until the user has entered a string into a textbox in the UI. The <code>user_input()</code> function returns the string entered by the user, which cen then, for example, be added to the context via the <code>prompt()</code> function. Using these APIs, a chatbot can be implemented in just four lines of code:</p><pre data-lang="python"><code data-lang="python"><span>await </span><span>prompt</span><span>(</span><span>PREAMBLE</span><span>)
</span><span>while </span><span>text := </span><span>await </span><span>user_input</span><span>("</span><span>Write a message</span><span>"):
</span><span>    </span><span>await </span><span>prompt</span><span>(</span><span>f</span><span>"</span><span>&lt;|separator|&gt;</span><span>\n\n</span><span>Human: </span><span>{text}</span><span>&lt;|separator|&gt;</span><span>\n\n</span><span>Assistant:</span><span>")
</span><span>    </span><span>await </span><span>sample</span><span>(</span><span>max_len</span><span>=</span><span>1024</span><span>, </span><span>stop_tokens</span><span>=["</span><span>&lt;|separator|&gt;</span><span>"], </span><span>return_attention</span><span>=</span><span>True</span><span>)
</span></code></pre><h2 id="files">Files</h2><p>Developers can upload small files to the PromptIDE (up to 5 MiB per file. At most 50 MiB total) and use their uploaded files in the prompt. The <code>read_file()</code> function returns any uploaded file as a byte array. When combined with the concurrency feature mentioned above, this can be used to implement batch processing prompts to evaluate a prompting technique on a variety of problems. The screenshot below shows a prompt that calculates the MMLU evaluation score.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_mmlu2.png"></p><h2 id="analytics">Analytics</h2><p>While executing a prompt, users see detailed per-token analytics to help them better understand the model's output. The completion window shows the precise tokenization of the context alongside the numeric identifiers of each token. When clicking on a token, users also see the top-K tokens after applying top-P thresholding and the aggregated attention mask at the token.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42_completion.png"></p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42_token.png"></p><p>When using the <code>user_input()</code> function, a textbox shows up in the window while the prompt is running that users can enter their response into. The below screenshot shows the result of executing the chatbot code snippet listed above.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_chat_completion.png"></p><p>Finally, the context can also be rendered in markdown to improve legibility when the token visualization features are not required.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_completion_markdown.png"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple developer boycott of Feedback Assistant (254 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2023/11/2.html</link>
            <guid>38164735</guid>
            <pubDate>Mon, 06 Nov 2023 16:30:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2023/11/2.html">https://lapcatsoftware.com/articles/2023/11/2.html</a>, See on <a href="https://news.ycombinator.com/item?id=38164735">Hacker News</a></p>
<div id="readability-page-1" class="page">
<nav>
Previous: <a href="https://lapcatsoftware.com/articles/2023/11/1.html">This Feedback will no longer be monitored, and incoming messages will not be reviewed</a>
<br><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a></nav>
<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>

<h3>November 6 2023</h3>

<p>I'm organizing a boycott of <a href="https://feedbackassistant.apple.com/">Apple's Feedback Assistant</a>, starting immediately, and I encourage all Apple developers to join me. Here's how I propose that each of us can effectively participate in the boycott and let Apple know that we're boycotting Feedback Assistant:</p>
<ol>
<li>File a new Feedback about Feedback Assistant (in Developer Tools &amp; Resources) that lists the issues below and states that you're boycotting Feedback Assistant until the issues are addressed.</li>
<li>Don't file any other new Feedbacks unless and until Apple addresses the issues.</li>
<li>If Apple requests a response to a previously filed Feedback, respond only by saying that you're boycotting Feedback Assistant, and refer to your Feedback number from step 1.</li>
</ol>
<p>Ideally, I think you should make your Feedback from step 1 as unique as possible. The point is to flood Apple with new Feedbacks about the boycott and force Apple to do some work to handle them, to take notice of the boycott, and to recognize that we're serious about it.</p>
<p>Boycotting Feedback Assistant does not preclude talking about your bugs on social media, on your blogs, and on your podcasts. Nor does it preclude filing reports with Apple's other public bug reporting systems, such as those for <a href="https://bugs.webkit.org/">WebKit</a> and various open source projects on <a href="https://github.com/apple">GitHub</a>. Those other bug reporting systems are superior to Feedback Assistant in a number of ways. The primary goal of the boycott is to bring about changes specifically in Feedback Assistant, the most <em>hostile</em> bug reporter I've ever seen.</p>
<p>After consulting with fellow developers, I've composed a list of issues with Feedback Assistant that Apple needs to address in order to end the boycott. I'll number the issues for ease of reference, but the order doesn't necessarily reflect their relative importance.</p>
<ol>
<li>Apple neglects or refuses to say whether or not they can reproduce reported bugs, even when we give them precise steps to reproduce and sample Xcode projects. This is crucial for us to determine whether Apple is taking our Feedbacks seriously or just lazily, bureaucratically stringing us along.</li>
<li>Apple closes Feedbacks with the status "Investigation complete - Unable to diagnose with current information" without asking us for more information or even notifying us that the Feedback has been closed.</li>
<li>Apple closes Feedbacks without the agreement of the person who filed the Feedback, and apparently it's now a "feature" of their bug reporting system that closed Feedbacks cannot be reopened, even by Apple employees. (It wasn't always this way, I believe.)</li>
<li>When Apple mistakenly closes a Feedback for a bug that isn't fixed, Apple demands that we open a new Feedback for the same bug, instead of just opening a new one themselves and giving us the new Feedback number.</li>
<li>Apple demands that developers "verify" Feedbacks with the latest betas despite the fact that Apple has not fixed the bugs, attempted to fix the bugs, or even attempted to reproduce the bugs with the steps given by us. This is a giant waste of our time. And Apple closes the Feedbacks if we don't "verify" them.</li>
<li>Apple doesn't always notify us of changes to the status of the original Feedback when our Feedbacks are closed as duplicates.</li>
<li>Apple constantly demands invasive sysdiagnose reports, often unnecessarily, and refuses to look at Feedbacks without them. Many developers work on their own personal devices, and sysdiagnoses are gross violations of our privacy, which Apple claims is a fundamental human right. Apple has avoided or abandoned creating smaller, more targeted and less intrusive methods of collecting information and diagnosing bugs.</li>
<li>Feedbacks can no longer be filed from the web. Apple now requires that all Feedbacks be filed from the native Feedback Assistant app on macOS or iOS. This is a very recent setback: I've been filing Feedbacks via the web app for years, the last one on October 26. Note the passive-aggressive question "Where would you like to start your feedback?" and the "recommendation" to use the native app, as if there were a choice.<br>
<img src="https://lapcatsoftware.com/articles/2023/11/2.png" width="414" height="300"></li>
<li>We can't search Feedback Assistant for bugs. Apple employees can search the database, but I can see only the Feedbacks that I personally filed. Of course we acknowledge that some Feedbacks need to remain secret, especially for products that haven't yet been announced by Apple, but countless Feedbacks require no such protection, and an opt-in searchable bug database would help external developers immensely, improving the overall quality of the software on Apple's platforms, to the benefit of Apple, developers, and users alike.</li>
</ol>
<p>Below is a screenshot of one of my old reports that epitomizes the absurdity of Apple's bug reporting system. Apple claimed in their response that "much has changed", but to this day, nothing has really changed. I've seen no evidence that Apple sincerely appreciates our input. Apple's Feedback Assistant, formerly known as Radar, has remained unreasonably terrible for a very long time, much too long, so now we're demanding change.</p>
<p><img src="https://lapcatsoftware.com/articles/images/Safari-extension-issues.jpeg" width="638" height="421" alt="Safari-extension-issues"></p>
<p>In defense of Apple, some people assert that Apple doesn't have the time to properly respond to Feedbacks. I don't find this argument convincing, because Apple's priorities, schedules, and staffing are determined by Apple itself, via the decisions of the company's leadership. Apple values its own time over the time of external developers and seems to have no guilt over wasting endless amounts of our time. We are not happy, though, to sacrifice ourselves for a corporation worth trillions of dollars. Needless to say, my net worth and income are microscopic in comparison. If Apple can decide that it doesn't have the time to respond to our Feedbacks, then we can decide that we don't have the time to file them; Apple's problems with lack of time are thereby solved. Frankly, as a longtime Apple user, I could do without the relentless annual OS updates, and many of us look back fondly to the era of Mac OS X Snow Leopard when the updates were around two years apart, leaving more time for bug fixes.</p>
<p>This is not a boycott against individual Apple engineers, many of whom also want Feedback Assistant to be improved. Indeed, the improvement of Feedback Assistant would enhance rather than detract from the relationship between Apple engineers and external developers. This is a boycott against the bug reporting <em>system</em>, intended to force Apple leadership to recognize and respond to the persistent problems with the system.</p>
<p>Although I call it a boycott, it could also be termed a labor strike. Apple utilizes developers for vast amounts of unpaid QA labor. Both Apple and developers know the crucial role that developers play in testing and refining Apple's software and products. Apple <em>needs</em> our bug reports, our labor, often hours or even days of labor for a single Feedback. Nonetheless, Apple acts as if it were entitled to our Feedbacks, treating developers kind of like indentured servants. No respect or basic human courtesy is afforded by Apple to developers in the bug reporting system. We've been indoctrinated into believing that it's simply our <em>duty</em> to file Feedbacks, for the sake of the platforms. However, Apple's platforms are not charity cases. To the contrary, they've made Apple the most profitable  company in the world. We developers are not Apple employees, and our unpaid labor should not be taken for granted. Henceforth, it will not be taken for granted.</p>
<p>In my view the boycott, or strike, has two goals. First, obviously, is to pressure Apple into improving Feedback Assistant by showing Apple that it needs us to file bug reports and would suffer without them. The second goal is to show ourselves that we don't actually need to file bug reports with Apple. My feeling is that Apple has a lot more to lose here than we do. After all, the majority of bugs that I file never get fixed anyway, and even the fixes usually come later rather than sooner, not in time to avoid the consequences of the bug. Do Apple bugs affect our apps? Yes, of course. But we typically have to ship workarounds for the bugs in our apps, because we can't count on Apple to fix our reported bugs in a timely manner. With a workaround for a bug in place, we no longer <em>need</em> Apple to fix the bug, so reporting the bug becomes more an act of charity than urgency.</p>
<p>This situation is often misunderstood by the public and even by Apple employees. Feedback Assistant does not provide customer service to developers. We developers are the ones who are providing the service to Feedback Assistant, and now we're choosing to withhold our services until the system is improved. I hope that Apple chooses to address the problems with Feedback Assistant, but if Apple happens to choose otherwise, and improvements never arrive, then my intention is to boycott forever. Regardless of whether Apple responds positively, I will consider the boycott to be a success if many developers participate, and we show ourselves that Feedback Assistant is not essential to our work and our livelihood.</p>

<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>
<nav><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a><br>
Previous: <a href="https://lapcatsoftware.com/articles/2023/11/1.html">This Feedback will no longer be monitored, and incoming messages will not be reviewed</a>
</nav>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckDB performance improvements with the latest release (173 pts)]]></title>
            <link>https://duckdb.org/2023/11/03/db-benchmark-update.html</link>
            <guid>38164189</guid>
            <pubDate>Mon, 06 Nov 2023 15:51:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duckdb.org/2023/11/03/db-benchmark-update.html">https://duckdb.org/2023/11/03/db-benchmark-update.html</a>, See on <a href="https://news.ycombinator.com/item?id=38164189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p><span>2023-11-03</span><span>Tom Ebergen</span></p>
							
							<p><em>TL;DR: the H2O.ai db-benchmark has been updated with new results. In addition, the AWS EC2 instance used for benchmarking has been changed to a c6id.metal for improved repeatability and fairness across libraries. DuckDB is the fastest library for both join and group by queries at almost every data size.</em></p>

<p><a href="#results">Skip directly to the results</a></p>
      <h2 id="the-benchmark-has-been-updated">
        
        <a href="#the-benchmark-has-been-updated">The Benchmark Has Been Updated!</a>
        
      </h2>
    

<p>In April, DuckDB Labs published a <a href="https://duckdb.org/2023/04/14/h2oai.html">blog post reporting updated H2O.ai db-benchmark results</a>. Since then, the results haven’t been updated. The original plan was to update the results with every DuckDB release. DuckDB 0.9.1 was recently released, and DuckDB Labs has updated the benchmark. While updating the benchmark, however, we noticed that our initial setup did not lend itself to being fair to all solutions. The machine used had network storage and could suffer from noisy neighbors. To avoid these issues, the whole benchmark was re-run on a c6id.metal machine.</p>
      
    

<p>Initially, updating the results to the benchmark showed strange results. Even using the same library versions from the prior update, some solutions regressed and others improved. We believe this variance came from the AWS EC2 instance we chose: an m4.10xlarge. The m4.10xlarge has 40 virtual CPUs and EBS storage. EBS storage is highly available network block storage for EC2 instances. When running compute-heavy benchmarks, a machine like the m4.10xlarge can suffer from the following issues:</p>

<ul>
  <li>
    <p><strong>Network storage</strong> is an issue for benchmarking solutions that interact with storage frequently. For the 500MB and 5GB workloads, network storage was not an issue on the m4.10xlarge since all solutions could execute the queries in memory. For the 50GB workload, however, network storage was an issue for the solutions that could not execute queries in memory. While the m4.10xlarge has dedicated EBS bandwidth, any read/write from storage is still happening over the network, which is usually slower than physically mounted storage. Solutions that frequently read and write to storage for the 50GB queries end up doing this over the network. This network time becomes a chunk of the execution time of the query. If the network has variable performance, the query performance is then also variable.</p>
  </li>
  <li>
    <p><strong>Noisy neighbors</strong> is a common issue when benchmarking on virtual CPUs. The previous machine most likely shared its compute hardware with other (neighboring) AWS EC2 instances. If these neighbors are also running compute heavy workloads, the physical CPU caches are repeatedly invalidated/flushed by the neighboring instance and the benchmark instance. When the CPU cache is shared between two workloads on two instances, both workloads require extra reads from memory for data that would already be in CPU cache on a non-virtual machine.</p>
  </li>
</ul>

<p>In order to be fair to all solutions, we decided to change the instance type to a metal instance with local storage. Metal instance types negate any noisy neighbor problems because the hardware is physical and not shared with any other AWS users/instances. Network storage problems are also fixed because solutions can read and write data to the local instance storage, which is physically mounted on the hardware.</p>

<p>Another benefit of the the c6id.metal box is that it stresses parallel performance. There are 128 cores on the c6id.metal. Performance differences between solutions that can effectively use every core and solutions that cannot are clearly visible.</p>

<p>See the <a href="#updated-settings">updated settings</a> section on how settings were change for each solution when run on the new machine.</p>
      <h2 id="updating-the-benchmark">
        
        <a href="#updating-the-benchmark">Updating the Benchmark</a>
        
      </h2>
    

<p>Moving forward we will update the benchmark when PRs with new performance numbers are provided. The PR should include a description of the changes to a solution script or a version update and new entries in the <code>time.csv</code> and <code>logs.csv</code> files. These entries will be verified using a different c6id.metal instance, and if there is limited variance, the PR will be merged and the results will be updated!</p>
      <h3 id="updated-settings">
        
        <a href="#updated-settings">Updated Settings</a>
        
      </h3>
    

<ol>
  <li>ClickHouse
    <ul>
      <li>Storage: Any data this gets spilled to disk also needs to be on the NVMe drive. This has been changed in the new <code>format_and_mount.sh</code> script and the <code>clickhouse/clickhouse-mount-config.xml</code> file.</li>
    </ul>
  </li>
  <li>Julia (juliadf &amp; juliads)
    <ul>
      <li>Threads: The threads were hardcoded for juliadf/juliads to 20/40 threads. Now the max number of threads are used. No option was given to spill to disk, so this was not changed/researched.</li>
    </ul>
  </li>
  <li>DuckDB
    <ul>
      <li>Storage: The DuckDB database file was specified to run on the NVMe mount.</li>
    </ul>
  </li>
  <li>Spark
    <ul>
      <li>Storage: There is an option to spill to disk. I was unsure of how to modify the storage location so that it was on the NVMe drive. Open to a PR with storage location changes and improved results!</li>
    </ul>
  </li>
</ol>

<p>Many solutions do not spill to disk, so they did not require any modification to use the instance storage. Other solutions use <code>parallel::ncores()</code> or default to a maximum number of cores for parallelism. Solution scripts were run in their current form on <a href="https://github.com/duckdblabs/db-benchmark">github.com/duckdblabs/db-benchmark</a>. Please read the <a href="https://github.com/duckdblabs/db-benchmark#updating-the-benchmark">Updating the Benchmark</a> section on how to re-run your solution.</p>
      <h3 id="results">
        
        <a href="#results">Results</a>
        
      </h3>
    

<p>The first results you see are the 50GB group by results. The benchmark runs every query twice per solution, and both runtimes are reported. The “first time” can be considered a cold run, and the “second time” can be considered a hot run. DuckDB and DuckDB-latest perform very well among all dataset sizes and variations.</p>

<p>The team at DuckDB Labs has been hard at work improving the performance of the out-of-core hash aggregates and joins. The most notable improvement is the performance of query 5 in the advanced group by queries. The cold run is almost an order of magnitude better than every other solution! DuckDB is also one of only two solutions to finish the 50GB join query. Some solutions are experiencing timeouts on the 50GB datasets. Solutions running the 50GB group by queries are killed after running for 180 minutes, meaning all 10 group by queries need to finish within the 180 minutes. Solutions running the 50GB join queries are killed after running for 360 minutes.</p>

<p><a href="https://duckdblabs.github.io/db-benchmark/">Link to result page</a></p>


							<p><a href="https://duckdb.org/news/">back to news archive <span></span></a>
						</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Create a shortcut for even lower phone brightness (154 pts)]]></title>
            <link>https://practicalbetterments.com/create-a-shortcut-for-even-lower-phone-brightness/</link>
            <guid>38164127</guid>
            <pubDate>Mon, 06 Nov 2023 15:47:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practicalbetterments.com/create-a-shortcut-for-even-lower-phone-brightness/">https://practicalbetterments.com/create-a-shortcut-for-even-lower-phone-brightness/</a>, See on <a href="https://news.ycombinator.com/item?id=38164127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><nav></nav><p>This practical betterment presupposes that…</p><ul><li>You use your phone in bed</li><li>You know staying up until 3am scrolling through contextless rage fuel is bad for your sleep, your mental health, and possibly even your eye balls</li><li>You're going to do it anyway</li></ul><p>We're all addicted to our phones and many of us go to sleep with them in our hands<sup><a href="#footnote-1" id="footnote-reference-1">1</a></sup> — so it's odd that manufacturers choose the default minimum brightness to be brighter than the sun.</p><figure><svg viewBox="0 0 566 186" stroke-linejoin="round" stroke-miterlimit="2"><title>A brightness slider ranging from maximum with a dilated pupil to minimum with a less dilated pupil, and an arrow indicating a lower, more comfortable brightness level with a drowsy eye.</title><path d="m57 186 1-2h7l2 1h1l1-1h6l17-2 2 1h7l5-2h3l1 1h1l1-1a143 143 0 0 1 14-2l1 1h2l5-2h5l2 1h3l21-1h7v-1l-5-2-6 2-1-1-1 1-1-1h-7l-1 1v-1h-1l-2 1h-8l-2-1-7 2h-4l-1-1h-2l-9 2-9 1a1130 1130 0 0 1-30 2h-6l-2-1-14 3h-4l-1-1-3 1-2-1H33l-9-2-2-5-3-1h-2l-2-5h-1l-2-4-1-1-3-8-1-2-1-3v-1l2-3v-3l3-3v-3l1-4v-1l-1-1v-1l2-5v-3l-2-3v-2l3-9v-5l-1-2 1-2v-2l1-2v-1l-1-1 1-1h1l2 2h-1l1 1h3l1-1-1-1 1-1v-2l-3-2h-2l-1-3-2-3v-3l1-1 2-1-1-2-1-1-2 2v-1l-2 3-2 2H8l-1 2-2 4-3 9 1 1h1l1-2 2 2 2-1 2 1v4l-1 3 1 2v9l-3 7 2 4v2l-3 8-2 2 1 4-1 6v6l1 2 3 8 2 2v4l1 3 5 3h1l2 1 2 3 3 3 15 2 2-1h6l2 1h4v-1l1 1m186-3 2-1v-1l4-1 1-3 2-1 2-1 4 1 1-1h1l2 2 4-2 2 1 1-1 3 2v-1l3 1h3v-1l1 1 1-1 2 1 1 1 1-1h15l3 1h11l7 1h3l1-1h7l2 1 2-1h5l2 1 9-1h5l4-1 8 1h16l8-1 2 1h4l2-1 2 1 2-1h1l3 1 2-1h4l5 1 8-2 1 1h6l1-1h18l2 1 1-1h3l6 1 44 4 1-1h5l1 1h5l2-1 2-1 2 1h1l4-2 1 1 3-2v-4l-1-1h-8l-19-1-53-2h-4l-2 1h-30v-1h-10l-14 2h-1l-2-1h-11l-1 1h-6l-1-1-2 1h-34l-85-4-3-3-3-1-1-2h-8l-2 2-4 5h-3l-1-1h-6l-7 2-3 2v2l1 1 3-1 3 2 9-1 1-1c1-1 0-1-1-1h-6v-2c3 0 7-2 9 0s1 5 2 8l2 3 5 3m280-6h-3l-23-1-38-3h-22l-15 1h-64l-22 1-42-2h-7l-2 1-3-1h-5l-5-1h-4l-1 1h-4l-6-2v-1l1-1h33l6 1a1445 1445 0 0 0 38 1h1l10 1h27l39-1 5-1h3l14 1h74l9 2h3l2-1h1l1 1 1-1h3l3 1h6l1 1h-1l-2 2-1-1h-2l-2 1-1 1-1-1h-2m-277 1-7-1-3-2-1-2v-2l1-3 1-2 1-1h2l2-1c2-1 3 0 4 1h1l1 1 2 2-1 1h1l1 2v1h-3l-1 1h-1v1l3 1-2 2m-29-5h-1l1-2 2 2M8 138v3-1m228-14h1l1-1 1-1-1-2v-3l1-12v-6h2l3-3-1-1-1-4-2-1 1-1h-1l-1-1-1-1-3-1h-1l-3 5-2 2-3 2v1l1 1h3l1 1 2-1 1-1h1v19l-1 2 1 2-1 1v2m298-5v6h2l2-9v-12l1-8 1-2h4l1-1v-2l-3-3-2-4-1-1-1-4-1 1-8 8-2 1-2 2v1l2 1h9v4l-1 4m-298-5v-1l1-1v1m-2-1h-1l1-1v-1l1 1M12 93h-1v-1l-2-1H6l1-4 2-3c1-1 2-3 3-2 2 2 3 6 3 9l-2 2m518-2h-1l-2-1v-1l2 1 1-1c1-1 0-5 2-4 2 0 1 3 2 4l-1 1M225 46l1-2v-3l2 1 1-1h2l1 1c1-1 2-3 3-2l1 3 2 1 2-2v-2l2-2h2l1 1v-1l3-4v-1h-1l-2 2-3 1-6 1-2 2-3-2-1 2h-2l-3-2h-2l-1 2-5-1-3-3-2-2h-2l-2-1h-1v1l1 3 2 2h1v1l-1 2 1 1h1l1-4h1l2 2 4 1h3l1 5m296-2 2-7v-2l-1 1-2 5v3m11-2v-3l1-1h-2v5h1M19 42l1-3h3l2 2 1-2 3-2 1 1 1 1h3v-2l2-1v-1l2-2v-1l-2-1-1 1a40 40 0 0 1-14 4l-2-1-1 1-2-2-2-2-2 1c-1-1-1-2-3-2H8v3l1 3h1l4 2h3l2 3m521-1v-3l-1-1h-1l-1 1h1l1 3m-27-2 2-4-1-1h-1l-2 4v1h2m36-1 1 1 1-1-2-1 1-2-1-2-1 1v4m-13-3 2-1h2l4-1h3l2-2h3l2-4v-2l-2 2h-2l-2 2h-3l-2 2-3-1h-1l-2 1h-7l-4-1h-1l-7-3h-3l-1 1 1 3 4 3 7 1 1 1 3-2 1 1 1-1 2 2m23-2 1-1-1-1-1 1m-338 1h1v-1c-1-3 0-6-2-8l-1 2v1l1 6M22 34l3-2 1-4 1-1 2 1h3l1 2 3-1 1 1 4-1 2-3-1-1h-2l-1-1-3-2 1-2-1-1-2 1h-1l-2-2v-1l1-4v-2l-1 1-2 5h-2v-2l1-1-1-1h-1v1l-2 2h-1l-2-1h-3l-2 1h-1v-3l1-2-1-1-2 6-3 1-2-3H7v3l-3 2v2l1 1h2l2-2 1-1h7l2 1-1 1-11 2c-2 0-4 1-5 3l1 2c2-1 3-3 5-3l7-2h2v1l-2 5 2 3 1 1 2 1m212-1h2l2-4v-2l-1-3h-1l-3-2h-2l-2 2-3 3 2 2 1 3 2 1m14-1 1-2v-2l-2 3v1m282-3v-1 1m17-1 1-3 2-5-1-1-2 5-2 3v2m-16-3-1-3 2-6v-1l2-2v-1h-1l-3 3-1 3v4l1 3v1m-279 0 2-1-1-4h-1v4m-16 0h1v1m28-2 6-1 1-1-1-2-3-2-1 1-2-1 3-2-1-1h-2l-1 1-1 1h-1l-3-2 1-1 2-2-1-1h-1l-2 3h-3l-1-1v-1l1-2v-1c-2 0-2 3-3 3l-2-1-3-2V9l2-2V5l-4 5h-2l-3-2V6l1-1-1-1V3h-1l-1 7-2-1h-2l-1 1-3 1V3h-1l-1 7-2 2h-2l-2-4h-1v2l2 2-1 1-2 2v1l1 1h1l1-2h2l2-1 5-2 2 1h3l10 4 4 1 4 3h2v1l5 2 2 1m-40-2 1 1v-1l2-4 2-2v-1h-1l-2 2-2 5M3 23v-1l-2-4H0v3l2 2m556-1 7-2 1-1-1-2-2-1-2 1-4-1 2-2v-1l-1-1v-1l-3 4-4-3 1-1 1-1-1-1h-1l-2 1-2-1-2-1 2-3V4l-1-1c-1 1-1 4-3 4-2 1-3-1-5-2h-1l1-2-1-2-1 3-4 1h-2l-1-5-1 1v3c0 1-1 2-3 2h-3l-1-1h1V3l-2-2v1h-1v1l1 1v2l1 1-4 2-1-1-2-3-1-1-1 2c1 1 3 2 2 3 0 2-2 3-3 3h-1l-2-1-1-4-1 1v1l2 4h1l2 3 1-1 2-3 4-1 8-2 3 1 2-1h2l2 2 4 1 2 1h2l6 5h1l1 2M21 22l-1-1h4l-1 1m510-1v-2l-2-2-2 3v1h3m10-3v-3h-2m-334 2 1-1v-2l-1-1h-2l-2-2v3l1 1 3 2M20 14l1-1v-1l-1-3-1 1v4" fill-rule="nonzero"></path></svg></figure><p>It turns out you can go lower than the minimum brightness.</p><p>Save your eyes from strain, increase your phone's battery life<sup><a href="#footnote-2" id="footnote-reference-2">2</a></sup>, and better ease yourself into a slumber by <strong>creating a shortcut to turn your phone brightness lower than the minimum.</strong></p><p>When you go to bed, or are trying not to disturb others on a sleeper train, or just want to save battery, you can quickly enable this shortcut.</p><p>Here's how…</p><h2>How to create a extra low brightness shortcut on iPhone</h2><p>On iOS this setting is called <code>Reduce White Point</code>.</p><p>Here's how you enable it:</p><ol><li>Open <code>Settings</code></li><li>Navigate to <code>Accessibility</code></li><li>Navigate to <code>Accessibility shortcut</code></li><li>From the menu select <code>Reduce White Point</code></li></ol><p>That's it. Once you've set this up you can <strong>triple-click the side button</strong> and your phone will be significantly dimmer.</p><h2>How to create an extra dim shortcut on Android</h2><p>On android phones this feature is called <code>extra dim</code>.</p><p>Android phones are all different from one another so the instructions are not consistent —&nbsp;but you can search for "extra dim shortcut" in settings and follow the instructions from there.</p><section><h3>Footnotes</h3><ol><li id="footnote-1"><a href="#footnote-reference-1">1</a><p>There are lots of studies on this from all over the world. I couldn't pick which one to cite. I do this.</p></li><li id="footnote-2"><a href="#footnote-reference-2">2</a><p><a href="https://endtimes.dev/actually-dark-mode-can-save-the-world/">Actually, dark mode can save the planet.</a> I wrote this</p></li></ol></section><dl><dt>Tags</dt><dd></dd><dt>Published</dt><dd><time datetime="2023-11-06">6 Nov 2023</time></dd><dt>Updated</dt><dd><time datetime="2023-11-06">6 Nov 2023</time></dd></dl></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Git rebase, what can go wrong (191 pts)]]></title>
            <link>https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/</link>
            <guid>38164046</guid>
            <pubDate>Mon, 06 Nov 2023 15:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/">https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/</a>, See on <a href="https://news.ycombinator.com/item?id=38164046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! While talking with folks about Git, I’ve been seeing a comment over and
over to the effect of “I hate rebase”. People seemed to feel pretty strongly
about this, and I was really surprised because I don’t run into a lot of
problems with rebase and I use it all the time.</p>

<p>I’ve found that if many people have a very strong opinion that’s different from
mine, usually it’s because they have different experiences around that thing
from me.</p>

<p>So I asked on <a href="https://social.jvns.ca/@b0rk/111342083852635579">Mastodon</a>:</p>

<blockquote>
<p>today I’m thinking about the tradeoffs of using <code>git rebase</code> a bit. I think
the goal of rebase is to have a nice linear commit history, which is something
I like.</p>

<p>but what are the <em>costs</em> of using rebase? what problems has it caused for you
in practice? I’m really only interested in specific bad experiences you’ve had
here – not opinions or general statements like “rewriting history is bad”</p>
</blockquote>

<p>I got a huge number of incredible answers to this, and I’m going to do my best
to summarize them here. I’ll also mention solutions or workarounds to those
problems in cases where I know of a solution. Here’s the list:</p>

<ul>
<li><a href="#fixing-the-same-conflict-repeatedly-is-annoying">fixing the same conflict repeatedly is annoying</a></li>
<li><a href="#undoing-a-rebase-is-hard">undoing a rebase is hard</a></li>
<li><a href="#force-pushing-to-shared-branches-can-cause-lost-work">force pushing to shared branches can cause lost work</a></li>
<li><a href="#force-pushing-makes-code-reviews-harder">force pushing makes code reviews harder</a></li>
<li><a href="#losing-commit-metadata">losing commit metadata</a></li>
<li><a href="#rebasing-can-break-intermediate-commits">rebasing can break intermediate commits</a></li>
<li><a href="#accidentally-run-git-commit-amend-instead-of-git-rebase-continue">accidentally run git commit –amend instead of git rebase –continue</a></li>
<li><a href="#splitting-commits-in-an-interactive-rebase-is-hard">splitting commits in an interactive rebase is hard</a></li>
<li><a href="#complex-rebases-are-hard">complex rebases are hard</a></li>
<li><a href="#rebasing-long-lived-branches-can-be-annoying">rebasing long lived branches can be annoying</a></li>
<li><a href="#miscellaneous-problems">miscellaneous problems</a></li>
<li><a href="#what-could-a-merge-only-workflow-look-like">what could a merge-only workflow look like?</a></li>
</ul>

<p>My goal with this isn’t to convince anyone that rebase is bad and you shouldn’t
use it (I’m certainly going to keep using rebase!). But seeing all these
problems made me want to be more cautious about recommending rebase to
newcomers without explaining how to use it safely. It also makes me wonder if
there’s an easier workflow for cleaning up your commit history that’s harder to
accidentally mess up.</p>

<h3 id="my-git-workflow-assumptions">my git workflow assumptions</h3>

<p>First, I know that people use a lot of different Git workflows. I’m going to be
talking about the workflow I’m used to when working on a team, which is:</p>

<ul>
<li>the team uses a central Github/Gitlab repo to coordinate</li>
<li>there’s one central <code>main</code> branch. It’s protected from force pushes.</li>
<li>people write code in feature branches and make pull requests to <code>main</code></li>
<li>The web service is deployed from <code>main</code> every time a pull request is merged.</li>
<li>the only way to make a change to <code>main</code> is by making a pull request on Github/Gitlab and merging it</li>
</ul>

<p>This is not the only “correct” git workflow (it’s a very “we run a web service”
workflow and open source project or desktop software with releases generally
use a slightly different workflow). But it’s what I know so that’s what I’ll
talk about.</p>

<h3 id="two-kinds-of-rebase">two kinds of rebase</h3>

<p>Also before we start: one big thing I noticed is that there were 2 different kinds of rebase that kept coming up, and only one of them requires you to deal with merge conflicts.</p>

<ol>
<li><strong>rebasing on an ancestor</strong>, like <code>git rebase -i HEAD^^^^^^^</code> to squash many
small commits into one. As long as you’re just squashing commits, you’ll
never have to resolve a merge conflict while doing this.</li>
<li><strong>rebasing onto a branch that has diverged</strong>, like <code>git rebase main</code>. This can cause merge conflicts.</li>
</ol>

<p>I think it’s useful to make this distinction because sometimes I’m thinking
about rebase type 1 (which is a lot less likely to cause problems), but people
who are struggling with it are thinking about rebase type 2.</p>

<p>Now let’s move on to all the problems!</p>

<h3 id="fixing-the-same-conflict-repeatedly-is-annoying">fixing the same conflict repeatedly is annoying</h3>

<p>If you make many tiny commits, sometimes you end up in a hellish loop where you
have to fix the same merge conflict 10 times.  You can also end up fixing merge
conflicts totally unnecessarily (like dealing with a merge conflict in code
that a future commit deletes).</p>

<p>There are a few ways to make this better:</p>

<ul>
<li>first do a <code>git rebase -i HEAD^^^^^^^^^^^</code> to squash all of the tiny commits
into 1 big commit and then a <code>git rebase main</code> to rebase onto a different
branch. Then you only have to fix the conflicts once.</li>
<li>use <code>git rerere</code> to automate repeatedly resolving the same merge conflicts
(“rerere” stands for “reuse recorded resolution”, it’ll record your previous merge conflict resolutions and replay them).
I’ve never tried this but I think you need to set <code>git config rerere.enabled
true</code> and then it’ll automatically help you.</li>
</ul>

<p>Also if I find myself resolving merge conflicts more than once in a rebase,
I’ll usually run <code>git rebase --abort</code> to stop it and then squash my commits into
one and try again.</p>

<h3 id="undoing-a-rebase-is-hard">undoing a rebase is hard</h3>

<p>I heard from several people that when they were new to rebase, they messed up a
rebase and permanently lost a week of work that they then had to redo.</p>

<p>The problem here is that undoing a rebase that went wrong is <strong>much</strong> more complicated
than undoing a merge that went wrong (you can undo a bad merge with something like <code>git reset --hard HEAD^</code>).
Many newcomers to rebase don’t even realize that undoing a rebase is even
possible, and I think it’s pretty easy to understand why.</p>

<p>That said, it is possible to undo a rebase that went wrong. Here’s an example of how to undo a rebase using <code>git reflog</code>.</p>

<p><strong>step 1</strong>: Do a bad rebase (for example run <code>git rebase -I HEAD^^^^^</code> and just delete 3 commits)</p>

<p><strong>step 2</strong>:  Run <code>git reflog</code>. You should see something like this:</p>

<pre><code>ee244c4 (HEAD -&gt; main) HEAD@{0}: rebase (finish): returning to refs/heads/main
ee244c4 (HEAD -&gt; main) HEAD@{1}: rebase (pick): test
fdb8d73 HEAD@{2}: rebase (start): checkout HEAD^^^^^^^
ca7fe25 HEAD@{3}: commit: 16 bits by default
073bc72 HEAD@{4}: commit: only show tooltips on desktop
</code></pre>

<p><strong>step 3</strong>: Find the entry immediately before <code>rebase (start)</code>. In my case that’s <code>ca7fe25</code></p>

<p><strong>step 4</strong>:  Run <code>git reset --hard ca7fe25</code></p>

<p>Another solution folks mentioned to “undoing a rebase is hard” that avoids having
to use the reflog is to make a “backup branch” with <code>git switch -c backup</code>
before rebasing, so you can easily get back to the old commit.</p>

<h3 id="force-pushing-to-shared-branches-can-cause-lost-work">force pushing to shared branches can cause lost work</h3>

<p>A few people mentioned the following situation:</p>

<ol>
<li>You’re collaborating on a branch with someone</li>
<li>You push some changes</li>
<li>They rebase the branch and run <code>git push --force</code> (maybe by accident)</li>
<li>Now when you run <code>git pull</code>, it’s a mess – you get the a <code>fatal: Need to specify how to reconcile divergent branches</code> error</li>
<li>While trying to deal with the fallout you might lose some commits, especially if some of the people are involved aren’t very comfortable with git</li>
</ol>

<p>This is an even worse situation than the “undoing a rebase is hard” situation
because the missing commits might be split across many different people’s and
the only worse thing than having to hunt through the reflog is multiple
different people having to hunt through the reflog.</p>

<p>This has never happened to me because the only branch I’ve ever collaborated on
is <code>main</code>, and <code>main</code> has always been protected from force pushing (in my
experience the only way you can get something into <code>main</code> is through a pull
request). So I’ve never even really been in a situation where this <em>could</em>
happen. But I can definitely see how this would cause problems.</p>

<p>The main tools I know to avoid this are:</p>

<ul>
<li>don’t rebase on shared branches</li>
<li>use <code>--force-with-lease</code> when force pushing, to make sure that nobody else has pushed to the branch since you last push</li>
</ul>

<p>I was curious about why people would run <code>git push --force</code> on a shared branch. Some reasons people gave were:</p>

<ul>
<li>they’re working on a collaborative feature branch, and the feature branch needs to be rebased onto <code>main</code>. The idea here is that you’re just really careful about coordinating the rebase so nothing gets lost.</li>
<li>as an open source maintainer, sometimes they need to rebase a contributor’s branch to fix a merge conflict</li>
<li>they’re new to git, read some instructions online that suggested <code>git rebase</code> and <code>git push --force</code> as a solution, and followed them without understanding the consequences</li>
<li>they’re used to doing <code>git push --force</code> on a personal branch and ran it on a shared branch by accident</li>
</ul>

<h3 id="force-pushing-makes-code-reviews-harder">force pushing makes code reviews harder</h3>

<p>The situation here is:</p>

<ul>
<li>You make a pull request on GitHub</li>
<li>People leave some comments</li>
<li>You update the code to address the comments, rebase to clean up your commits, and force push</li>
<li>Now when the reviewer comes back, it’s hard for them to tell what you changed since the last time you saw it – all the commits show up as “new”.</li>
</ul>

<p>One way to avoid this is to push new commits addressing the review comments,
and then after the PR is approved do a rebase to reorganize everything.</p>

<p>I think some reviewers are more annoyed by this problem than others, it’s kind
of a personal preference. Also this might be a Github-specific issue, other
code review tools might have better tools for managing this.</p>

<h3 id="losing-commit-metadata">losing commit metadata</h3>

<p>If you’re rebasing to squash commits, you can lose important commit metadata
like <code>Co-Authored-By</code>. Also if you GPG sign your commits, rebase loses the
signatures.</p>

<p>There’s probably other commit metadata that you can lose that I’m not thinking of.</p>

<p>I haven’t run into this one so I’m not sure how to avoid it. I think GPG
signing commits isn’t as popular as it used to be.</p>

<h3 id="rebasing-can-break-intermediate-commits">rebasing can break intermediate commits</h3>

<p>If you’re trying to have a very clean commit history where the tests pass on
every commit (very admirable!), rebasing can result in some intermediate
commits that are broken and don’t pass the tests, even if the final commit
passes the tests.</p>

<p>Apparently you can avoid this by using <code>git rebase -x</code> to run the test suite at
every step of the rebase and make sure that the tests are still passing. I’ve
never done that though.</p>

<h3 id="accidentally-run-git-commit-amend-instead-of-git-rebase-continue">accidentally run <code>git commit --amend</code> instead of <code>git rebase --continue</code></h3>

<p>A couple of people mentioned issues with running <code>git commit --amend</code> instead of <code>git rebase --continue</code> when resolving a merge conflict.</p>

<p>The reason this is confusing is that there are two reasons when you might want to edit files during a rebase:</p>

<ol>
<li>editing a commit (by using <code>edit</code> in <code>git rebase -i</code>), where you need to write <code>git commit --amend</code> when you’re done</li>
<li>a merge conflict, where you need to run <code>git rebase --continue</code> when you’re done</li>
</ol>

<p>It’s very easy to get these two cases mixed up because they feel very similar. I think what goes wrong here is that you:</p>

<ul>
<li>Start a rebase</li>
<li>Run into a merge conflict</li>
<li>Resolve the merge conflict, and run <code>git add file.txt</code></li>
<li>Run <code>git commit</code> because that’s what you’re used to doing after you run <code>git add</code></li>
<li>But you were supposed to run <code>git rebase --continue</code>! Now you have a weird extra commit, and maybe it has the wrong commit message and/or author</li>
</ul>

<h3 id="splitting-commits-in-an-interactive-rebase-is-hard">splitting commits in an interactive rebase is hard</h3>

<p>The whole point of rebase is to clean up your commit history, and <strong>combining</strong>
commits with rebase is pretty easy. But what if you want to split up a commit into 2
smaller commits? It’s not as easy, especially if the commit you want to split
is a few commits back! I actually don’t really know how to do it even though I
feel very comfortable with rebase. I’d probably just do <code>git reset HEAD^^^</code>  or
something and use <code>git add -p</code> to redo all my commits from scratch.</p>

<p>One person shared <a href="https://github.com/kimgr/git-rewrite-guide#split-a-commit">their workflow for splitting commits with rebase</a>.</p>

<h3 id="complex-rebases-are-hard">complex rebases are hard</h3>

<p>If you try to do too many things in a single <code>git rebase -i</code> (reorder commits
AND combine commits AND modify a commit), it can get really confusing.</p>

<p>To avoid this, I personally prefer to only do 1 thing per rebase, and if I want
to do 2 different things I’ll do 2 rebases.</p>

<h3 id="rebasing-long-lived-branches-can-be-annoying">rebasing long lived branches can be annoying</h3>

<p>If your branch is long-lived (like for 1 month), having to rebase repeatedly
gets painful. It might be easier to just do 1 merge at the end and only resolve
the conflicts once.</p>

<p>The dream is to avoid this problem by not having long-lived branches but it
doesn’t always work out that way in practice.</p>

<h3 id="miscellaneous-problems">miscellaneous problems</h3>

<p>A few more issues that I think are not that common:</p>

<ul>
<li><strong>Stopping a rebase wrong</strong>: If you try to abort a rebase that’s going badly with
<code>git reset --hard</code> instead of <code>git rebase --abort</code>, things will behave
weirdly until you stop it properly</li>
<li><strong>Weird interactions with merge commits</strong>: A couple of quotes about this: “If you
rebase your working copy to keep a clean history for a branch, but the
underlying project uses merges, the result can be ugly. If you do rebase -i
HEAD~4 and the fourth commit back is a merge, you can see dozens of commits
in the interactive editor.“, “I’ve learned the hard way to <em>never</em> rebase if
I’ve merged anything from another branch”</li>
</ul>

<h3 id="should-we-teach-squash-and-merge-before-rebase">should we teach “squash and merge” before rebase?</h3>

<p>Seeing all of these issues made me wonder if there’s something better we can
recommend to git newcomers before rebase. Like – if someone has 1 million crappy
commits on their branch (wip wip wip fix ugh) and they don’t know how to clean
them up, is rebase really the best place to start? Would “squash and merge” be
easier?</p>

<p>I think an alternate workflow that avoids rebase could be:</p>

<ul>
<li>make commits</li>
<li>Run <code>git merge main</code> to merge main into my branch periodically</li>
<li>To look at the log of changes made on my branch, run <code>git diff main</code> or <code>git log main..mybranch</code>. That will look something like this:</li>
</ul>

<pre><code>$ git log main..mybranch
756d4af (HEAD -&gt; mybranch) Merge branch 'main' into mybranch
20106fd Merge branch 'main' into mybranch
d7da423 some commit on my branch
85a5d7d some other commit on my branch
</code></pre>

<ul>
<li>When you’re done, use GitHub’s “squash and merge” feature (which is the equivalent of
running <code>git checkout main; git merge --squash mybranch</code>) to squash all of my
changes into 1 commit. This gets rid of all the “ugly” merge commits.</li>
</ul>

<p>I think the main things you’d lose with this merge-only workflow (relative to
using rebase) are:</p>

<ul>
<li>you have to look at some “merge” commits when you run <code>git log main..mybranch</code></li>
<li>you have to squash all your commits, so you can’t have 2 separate commits</li>
<li>The “clean up with <code>git reset</code>” workflow is dangerous in a different way than using rebase</li>
</ul>

<p>I haven’t tried this, this is just me thinking out loud. I’d like to hear if
anyone uses a similar workflow in practice though.</p>

<h3 id="there-are-more-problems-than-i-expected">there are more problems than I expected</h3>

<p>I went into this really feeling like “rebase is fine, what could go wrong?” But
many of these problems actually have happened to me in the past, it’s just that
over the years I’ve learned how to avoid or fix all of them.</p>

<p>And I’ve never really seen anyone share best practices for rebase, other than
“never force push to a shared branch”. All of these honestly make me a lot more
reluctant to recommend using rebase.</p>

<p>To recap, I think these are my personal rebase rules I follow:</p>

<ul>
<li>stop a rebase if it’s going badly instead of letting it finish (with <code>git rebase --abort</code>)</li>
<li>know how to use <code>git reflog</code> to undo a bad rebase</li>
<li>don’t rebase a million tiny commits (instead do it in 2 steps: <code>git rebase -i HEAD^^^^</code> and then <code>git rebase main</code>)</li>
<li>don’t do more than one thing in a <code>git rebase -i</code>. Keep it simple.</li>
<li>never force push to a shared branch</li>
<li>never rebase commits that have already been pushed to <code>main</code></li>
</ul>

<p><small>
Thanks to Marco Rogers for encouraging me to think about the problems people
have with rebase, and to everyone on Mastodon who helped with this.
</small></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Regarding Proposed US Restrictions on RISC-V (161 pts)]]></title>
            <link>https://www.bunniestudios.com/blog/?p=6862</link>
            <guid>38163412</guid>
            <pubDate>Mon, 06 Nov 2023 14:54:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bunniestudios.com/blog/?p=6862">https://www.bunniestudios.com/blog/?p=6862</a>, See on <a href="https://news.ycombinator.com/item?id=38163412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-6862">
				<p>A bipartisan group of 18 lawmakers in the US Congress have recently amplified a request to the White House and the Secretary of Commerce to <a href="https://www.reuters.com/technology/us-lawmakers-press-biden-plans-chinese-use-open-chip-technology-2023-11-02/">place restrictions on Americans working with RISC-V</a> (see also <a href="https://www.reuters.com/technology/us-china-tech-war-risc-v-chip-technology-emerges-new-battleground-2023-10-06/">the initial request from the Senate</a>) in order to prevent China from gaining dominance in CPU technology.</p>
<p>The request is facially misguided; any restrictions would only serve to reduce American participation in an important emerging technology, while bolstering ARM’s position as an incumbent near-monopoly provider of embedded CPUs.</p>
<p>When the first report came out, I hoped it was just a blip that would go away, but with the broader bi-partisan group asking for restrictions, I felt I could no longer just stand by and watch: I am an active participant in the RISC-V ecosystem. I’m also subject to US law.</p>
<p>I did the one thing any American can do, which is write a letter summarizing my thoughts on the issue, and sending it to the White House, Department of Commerce, and the relevant members of Congress. Unfortunately, I don’t have a PAC, lobbyists or any sort of high-level connections to US politicians, so I don’t have much hope the letter will be received in time.</p>
<p>However, I do have a blog. I’m posting a copy of the letter I sent to the White House here, in far-flung hopes that maybe someone with more political connections than I might pick it up and send it on.</p>
<p>Finally, if you disagree with my stance or have a different perspective, I also encourage you to send a letter expressing your thoughts to various government officials. It doesn’t have to be “my way”, but a show of broad public interest in the topic may at least encourage policymakers to think a bit more carefully about the issue, and to hear out more perspectives.</p>
<h3>The Letter</h3>
<p>To President Biden and the White House staff:</p>
<p>Recently, a letter was sent to the White House and the Secretary of Commerce by 18 lawmakers asking how the US plans to prevent China “from achieving dominance in … RISC-V technology and leveraging that dominance at the expense of US national and economic security”.</p>
<p>I am a Michigan-born American with a PhD from MIT in electrical engineering. I’m also a small business owner who designs and manufactures electronics. I am writing to urge you to not place any restrictions on the sharing of RISC-V technology.</p>
<p>My products’ CPUs are based on the open source RISC-V standard. RISC-V’s openness specifically benefits small businesses such as mine. I get tools and designs from the open source community, and I contribute my improvements back to the pool. Barrier-free participation in this vibrant open source ecosystem keeps overhead low, allowing me to be competitive in the cutthroat hardware business.</p>
<p>Like the Internet, RISC-V is already a global phenomenon. There are already prolific contributions from the EU, India, China, and more [1]; the US is not the sole proprietor of RISC-V implementations. I use an implementation of RISC-V called the VexRiscv, which is developed in the EU. Any barrier for US persons’ participation will only slow American progress in developing and adopting this technology. It will have an effect opposite of that intended by lawmakers.</p>
<p>A further subtlety is that RISC-V is simply a standard. It defines a set of words used to tell a chip to do something, similar to how we rely on a dictionary to define the meaning of English words. Just as one can write secret documents using openly defined words, designs using the RISC-V standard can be proprietary, even if the standard is open. The benefits of open standards are so well established that the US has an entire agency – NIST – to promote American innovation and industrial competitiveness by publishing open standards.</p>
<p>Furthermore, it is not practical to police the use of an established standard: once a book is published, it is impractical to ensure that none of America’s enemies obtain a copy of it. This has long been a trade-off of American innovation philosophy: we can freely exercise our First Amendment rights to share ideas, creating a vibrant intellectual exchange, even at the risk of others benefiting from reading our textbooks, journals and patents.</p>
<p>I believe this trade-off has been in our favor. With every exchange – even with potential competitors – we learn more. Chilling our freedom of expression to achieve administrative outcomes is a page out of other more oppressive regimes’ playbooks: it is fundamentally un-American to restrict the flow of ideas.</p>
<p>In summary, any restrictions placed on US persons sharing RISC-V technology would only serve to diminish America’s role as a technological leader. Over-broad restrictions could deprive educators of a popular tool used to teach students about computers on American campuses, for fear of also accidentally teaching to an embargoed entity. And even narrow restrictions on RISC-V could deprive US tech companies with any potential exposure to the Chinese market of access to a cost-effective, high-performance CPU technology, forcing them to pay royalties to the incumbent near-monopoly provider, ARM Holdings plc – a company that isn’t American. This weakens American competitiveness and ultimately harms the US’s best interests.</p>
<p>If the administration agrees that RISC-V is a technology so critical to US economic and military interests that it deserves special attention, instead of trying to restrict its expression with a federally-mandated licensing regime, it should invest in programs to develop more home-grown American RISC-V chip maker success stories. It is already within the four corners of existing US legal framework, and the RISC-V contractual framework, for companies to choose to develop proprietary implementations of RISC-V CPUs. The US has strong precedents for companies navigating the boundaries of open standards and finding success without the need for federal guidance: Intel and AMD are American industrial juggernauts built around proprietary implementations of an otherwise openly documented “x86” computer standard. What the US needs is an American answer to ARM Holdings plc’s monopoly, and that answer comes from investing in US companies that embrace RISC-V.</p>
<p>President Biden, I urge you: have faith in American innovation. Have faith in American values. Do not place any restrictions on the sharing of RISC-V technology. We can work together to build more US chip maker success stories, while embracing the American value of freedom of expression!</p>
<p>Very truly yours,</p>
<p>Andrew ‘bunnie’ Huang<br>
An American Hacker, Maker, and Author</p>
<p>[1] https://github.com/riscvarchive/riscv-cores-list</p>

								
				<p>
					<small>
												This entry was posted on Monday, November 6th, 2023 at 10:30 pm and is filed under <a href="https://www.bunniestudios.com/blog/?cat=5" rel="category">Ponderings</a>, <a href="https://www.bunniestudios.com/blog/?cat=3" rel="category">Social</a>.						You can follow any responses to this entry through the <a href="https://www.bunniestudios.com/blog/?feed=rss2&amp;p=6862">RSS 2.0</a> feed. 

													You can <a href="#respond">leave a response</a>, or <a href="https://www.bunniestudios.com/wordpress/wp-trackback.php?p=6862" rel="trackback">trackback</a> from your own site.
						
					</small>
				</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USB-C cable with the bandwidth and USB type imprinted on the connector (233 pts)]]></title>
            <link>https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato</link>
            <guid>38162837</guid>
            <pubDate>Mon, 06 Nov 2023 14:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato">https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato</a>, See on <a href="https://news.ycombinator.com/item?id=38162837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Elgato hasn’t just made an <a href="https://www.theverge.com/2023/10/17/23920498/elgato-prompter-teleprompter-monitor-release-date-price">excellent teleprompter</a>, it’s also made a great USB-C cable that ships with it. Professional audio engineer <a href="https://twitter.com/JustSpike_/status/1721247993230250387">Matt “Spike” McWilliams spotted</a> that Elgato’s latest USB-C cable has the bandwidth and USB type imprinted on the connector, and now I wish all manufacturers did this.</p><p>I recently spent too many hours sorting my USB-C cables into ones that are high speed, ones that can deliver fast charging, and ones that can do both. None of them had any marker to let me know the speed or type of USB-C cable without me having to test them. It’s a common issue for <a href="https://www.theverge.com/2023/9/16/23872237/apple-iphone-15-usb-c-switch-guide">people switching to USB-C</a> right now, and even a small indicator like Elgato’s can certainly help. The writing on Elgato’s cable tells me it’s USB 3.0 compatible and can support up to 5Gbps in bandwidth.</p><p><a href="https://www.usb.org/logo-license">The USB Implementers Forum has a logo</a> that manufacturers can use on packaging, but that’s often useless later on when you’re searching for a specific USB-C cable in your box of cables. I’ve also purchased plenty of USB-C cables that have claimed, on the box, to deliver a certain bandwidth and failed to do so in reality.</p><p>Elgato’s solution is so simple I didn’t even notice it when I unboxed its <a href="https://www.theverge.com/2023/10/17/23920498/elgato-prompter-teleprompter-monitor-release-date-price">Prompter hardware recently</a>, but it’s so useful that I wish it was more widespread. “You’ll see spec data on all Elgato USB and HDMI cables going forward,” explains Julian Fest, senior vice president at Elgato, in a <a href="https://twitter.com/JFest/status/1721264675193553025">post on X (Twitter)</a>. “No more guessing if the cable is the culprit of a tech issue.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who's Behind the Swat USA Reshipping Service? (142 pts)]]></title>
            <link>https://krebsonsecurity.com/2023/11/whos-behind-the-swat-usa-reshipping-service/</link>
            <guid>38162597</guid>
            <pubDate>Mon, 06 Nov 2023 13:55:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2023/11/whos-behind-the-swat-usa-reshipping-service/">https://krebsonsecurity.com/2023/11/whos-behind-the-swat-usa-reshipping-service/</a>, See on <a href="https://news.ycombinator.com/item?id=38162597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>Last week, KrebsOnSecurity broke the news that one of the largest cybercrime services for laundering stolen merchandise was hacked recently, exposing its internal operations, finances and organizational structure. In today’s Part II, we’ll examine clues about the real-life identity of “<strong>Fearlless</strong>,” the nickname chosen by the proprietor of the <strong>SWAT USA Drops</strong> service.</p>
<p><img decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2022/01/wbrkb.jpg" alt="" width="750" height="501"></p>
<p>Based in Russia, SWAT USA recruits people in the United States to reship packages containing pricey electronics that are purchased with stolen credit cards. As detailed in <a href="https://krebsonsecurity.com/2023/11/russian-reshipping-service-swat-usa-drop-exposed/" target="_blank" rel="noopener">this Nov. 2 story</a>, SWAT currently employs more than 1,200 U.S. residents, all of whom will be cut loose without a promised payday at the end of their first month reshipping stolen goods.</p>
<p>The current co-owner of SWAT, a cybercriminal who uses the nickname “Fearlless,” operates primarily on the cybercrime forum <strong>Verified</strong>. This Russian-language forum has tens of thousands of members, and it has suffered several hacks that exposed more than a decade’s worth of user data and direct messages.</p>
<p>January 2021 posts on Verified show that Fearlless and his partner <strong>Universalo</strong> purchased the SWAT reshipping business from a Verified member named SWAT, who’d been operating the service for years. SWAT agreed to transfer the business in exchange for 30 percent of the net profit over the ensuing six months.</p>
<p>Cyber intelligence firm <a href="https://www.intel471.com/" target="_blank" rel="noopener">Intel 471</a> says Fearlless first registered on Verified in February 2013. The email address Fearlless used on Verified leads nowhere, but a review of Fearlless’ direct messages on Verified indicates this user originally registered on Verified a year earlier as a reshipping vendor, under the alias “<strong>Apathyp</strong>.”</p>
<p>There are two clues supporting the conclusion that Apathyp and Fearlless are the same person. First, the Verified administrators warned Apathyp he had violated the forum’s rules barring the use of multiple accounts by the same person, and that Verified’s automated systems had detected that Apathyp and Fearlless were logging in from the same device.&nbsp; Second, in his earliest private messages on Verified, Fearlless told others to contact him on an instant messenger address that Apathyp had claimed as his.<span id="more-65541"></span></p>
<p>Intel 471 says Apathyp registered on Verified using the email address <strong>triploo@mail.ru</strong>. A search on that email address at the breach intelligence service <a href="https://www.constellaintelligence.com/" target="_blank" rel="noopener">Constella Intelligence</a> found that a password commonly associated with it was “<strong>niceone</strong>.” But the triploo@mail.ru account isn’t connected to much else that’s interesting except a now-deleted account at <strong>Vkontakte</strong>, the Russian answer to Facebook.</p>
<p>However, in Sept. 2020, Apathyp sent a private message on Verified to the owner of a stolen credit card shop, saying his credentials no longer worked. Apathyp told the proprietor that his chosen password on the service was “<strong>12Apathy</strong>.”</p>
<p>A search on that password at Constella reveals it was used by just four different email addresses, two of which are particularly interesting: <strong>gezze@yandex.ru </strong>and <strong>gezze@mail.ru</strong>. Constella discovered that both of these addresses were previously associated with the same password as triploo@mail.ru — “niceone,” or some variation thereof.</p>
<p>Constella found that years ago gezze@mail.ru was used to create a Vkontakte account under the name <strong>Ivan Sherban</strong> (former password: “<strong>12niceone</strong>“) from Magnitogorsk, an industrial city in the southern region of Russia. That same email address is now tied to a Vkontakte account for an Ivan Sherban who lists his home as Saint Petersburg, Russia. Sherban’s profile photo <a href="https://krebsonsecurity.com/wp-content/uploads/2023/11/gezze.png" target="_blank" rel="noopener">shows</a> a heavily tattooed, muscular and <a href="https://krebsonsecurity.com/wp-content/uploads/2023/11/gezze-car.png" target="_blank" rel="noopener">recently married</a> individual with his beautiful new bride getting ready to drive off in a convertible sports car.</p>
<p>A pivotal clue for validating the research into Apathyp/Fearlless came from the identity intelligence firm <a href="https://mynetwatchman.com/" target="_blank" rel="noopener">myNetWatchman</a>, which found that gezze@mail.ru at one time used the passwords “<strong>геззи1991</strong>” (gezze1991) and “<strong>gezze18081991</strong>.”</p>
<p>Care to place a wager on when Vkontakte says is Mr. Sherban’s birthday? Ten points if you answered <strong>August 18</strong> (18081991).</p>
<p>Mr. Sherban did not respond to multiple requests for comment.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastering DOM manipulation with vanilla JavaScript (349 pts)]]></title>
            <link>https://phuoc.ng/collection/html-dom/</link>
            <guid>38162435</guid>
            <pubDate>Mon, 06 Nov 2023 13:41:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phuoc.ng/collection/html-dom/">https://phuoc.ng/collection/html-dom/</a>, See on <a href="https://news.ycombinator.com/item?id=38162435">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Mastering DOM manipulation with vanilla JavaScript</h2><p><a href="https://github.com/phuocng/html-dom">Star me on GitHub <!-- -->→ 5400<!-- --> ⭐</a></p><div><p>Web development moves at lightning speed. I still remember when I first started using libraries like <a href="https://jquery.com/">jQuery</a>, <a href="http://prototypejs.org/">Prototype</a>, <a href="https://script.aculo.us/">script.aculo.us</a>,
<a href="https://zeptojs.com/">Zepto</a>, and many more. Even with modern tools like <a href="https://angular.io/">Angular</a>, <a href="https://vuejs.org/">VueJS</a>, <a href="https://react.dev/">React</a>, <a href="https://www.solidjs.com/">Solid</a> and <a href="https://svelte.dev/">Svelte</a>,
we still have to deal with the Document Object Model (DOM). While these frameworks encapsulate and hide direct DOM management, they still give us access to work with the DOM via <em>refs</em> and <em>event handlers</em>.</p>
<p>Whether you're developing or using a web component in any framework, you need to work with the DOM at a certain level.
Knowing the browser DOM APIs and how to use them is crucial to web development. A website that introduces the APIs, highlights common problems,
and provides answers to popular questions can be incredibly useful.</p>
<p>That's why I've put together this collection of resources:</p>
<ul>
<li>No external libraries, just native browser APIs</li>
<li>Small, easy-to-understand examples</li>
<li>Live demos</li>
<li>Tips and best practices included</li>
<li>Real-life use cases</li>
<li>Works with modern browsers and <del>even supports Internet Explorer</del></li>
</ul>
<p>Get ready to master DOM manipulation with vanilla JavaScript.</p></div><section><h2>Level 1 — Basic</h2></section><section><h2>Level 2 — Intermediate</h2></section><section><h2>Level 3 — Advanced</h2></section><section><h2>Tip</h2></section><section><h2>Recent posts ⚡</h2></section><section><h2>Newsletter 🔔</h2><p>If you're into front-end technologies and you want to see more of the content I'm creating, then you might want to consider subscribing to my newsletter.</p><p>By subscribing, you'll be the first to know about new articles, products, and exclusive promotions.</p><p>Don't worry, I won't spam you. And if you ever change your mind, you can unsubscribe at any time.</p><p>Phước Nguyễn</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new home and license (AGPL) for Synapse and friends (113 pts)]]></title>
            <link>https://element.io/blog/element-to-adopt-agplv3/</link>
            <guid>38162275</guid>
            <pubDate>Mon, 06 Nov 2023 13:25:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://element.io/blog/element-to-adopt-agplv3/">https://element.io/blog/element-to-adopt-agplv3/</a>, See on <a href="https://news.ycombinator.com/item?id=38162275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We believe in open source because it encourages innovation, ensures transparency and puts end-users in control. It’s why we created Matrix as an open source project, and ensured a <a href="https://matrix.org/foundation">well-defined open governance model</a> around it through the Matrix.org Foundation, to ensure the protocol will always be guided by its original manifesto.</p><p>As founders of Matrix, we created Element as a for-profit open source company to hire the core Matrix team to be able to work on Matrix, develop a flagship Matrix-based product, bootstrap the Matrix ecosystem, and help fund the underlying core Matrix projects. As a commercial entity, Element has driven the bulk (more than 95%) of core Matrix development for the last seven years, and maintains the largest Matrix homeserver (matrix.org) on behalf of the Matrix.org Foundation. That has helped drive Matrix adoption, and stimulate a wonderfully vibrant community.</p><p>Over the last year or two Matrix has evolved from ‘explosive growth’ to being a ‘category’ in its own right. In other words, ‘Matrix-based’ is now specified as a requirement in massive public and private sector tenders - in which multinationals compete to provide Matrix-based products and services.</p><p>That’s fantastic, and a huge achievement. A competitive open source ecosystem is a powerful multiplier. It triggers more innovation, encourages transparency and gives end-users more independence.</p><h2 id="the-road-ahead">The road ahead</h2><p>Element has always put the growth and success of Matrix first. We have daily discussions about which choices best ensure that Matrix thrives, with Element simply needing to stay on a path to success.</p><p>Today we have arrived at a crossroads. We have succeeded in making Matrix wildly successful, but Element is losing its ability to compete in the very ecosystem it has created. It is hard for Element to innovate and adapt as quickly as companies whose business model is developing proprietary Matrix-based products and services without the responsibility and costs of maintaining the bulk of Matrix. In order to be fair to our customers, we need to be able to put more focus on them and their specific requirements.</p><p>So it’s time for us to get back in the game by establishing a level playing field and ensuring we can continue to support Matrix, whilst delivering the services our customers are requesting. This took us to reconsider how we license the open source code we develop.</p><p>After considerable thought, and taking particular <a href="https://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/">inspiration from Grafana</a>, we’ve chosen to pursue future development of Synapse (the main Matrix server), Dendrite (our second generation Matrix server) and associated server-side projects (e.g. sydent, sygnal) under the terms of the <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">Affero General Public License (AGPL) v3</a> - maintaining the code in new repositories in the Element GitHub org, forked from the Apache-licensed repositories in the Matrix.org GitHub org (originally donated by Element). This is still the same team of developers who have been working on Matrix since it began in 2014, still developing and releasing Synapse as open source - and in fact, arguably even more Free and Libre than before thanks to the AGPL. Client-side code developed by Element, including projects donated to the Foundation, is not affected.</p><p>The benefit of switching to AGPLv3 is that it obliges downstream developers to contribute back to the core project - either by releasing their modifications as open source for the benefit of the whole Matrix ecosystem, or by contacting Element for an alternative license. Future code contributors to Synapse will need to sign a contributor license agreement (CLA) based on the <a href="https://www.apache.org/licenses/contributor-agreements.html#clas">Apache Software Foundation’s CLA</a>, giving Element the right to distribute the contribution commercially so we can use it to help fund Matrix core development in future.</p><p>We believe this is the fairest approach possible: preserving the Free and Open Source nature of these Matrix implementations under an OSI-approved open source license (AGPLv3), while encouraging proprietary forks to contribute to the development costs of the underlying project.</p><h2 id="what-s-the-impact-on-me">What’s the impact on me?</h2><p>For Element users and others who don’t run their own server, there’s no difference at all.</p><p>For those running unmodified free-standing open source instances of <a href="https://github.com/matrix-org/synapse">Synapse</a>, <a href="https://github.com/matrix-org/dendrite">Dendrite</a>, <a href="https://github.com/matrix-org/sygnal">Sygnal</a>, <a href="https://github.com/matrix-org/sydent">Sydent</a> and <a href="https://github.com/matrix-org/matrix-authentication-service">MAS</a>, there is no change beyond needing to point your deployments at the new repositories in the Element rather than Matrix GitHub organization. These will be available in the coming days. </p><p>Please see <a href="https://matrix.org/blog/2023/11/06/future-of-synapse-dendrite">here</a> for The Matrix.org Foundation's position. Contributors to these new repositories will now need to agree to a standard <a href="https://www.apache.org/licenses/contributor-agreements.html#clas">Apache CLA</a> with Element before their PRs can be merged - this replaces the previous <a href="https://developercertificate.org/">DCO</a> sign-off mechanism.</p><p>Those running <a href="https://element.io/enterprise-functionality">Element Server Suite (ESS)</a>, Element’s enterprise server distribution of Synapse and supporting projects, are not affected - ESS comes with its own enterprise software license which supplants the AGPL requirements.</p><p>Organizations and individuals who want to create or run modified versions of these projects or incorporate them within non-AGPL projects <strong>are</strong> impacted, and they have two options:</p><ol><li>If they are happy to publish their modifications as open source (under the terms of the AGPLv3), they can do so and carry on as normal.</li><li>If they want to keep their modifications and surrounding code proprietary, and distribute the software or run it as a service<strong>,</strong> they will need to contact Element at <a href="https://element.io/cdn-cgi/l/email-protection#2945404a4c475a40474e694c454c444c475d074046"><span data-cfemail="9bf7f2f8fef5e8f2f5fcdbfef7fef6fef5efb5f2f4">[email&nbsp;protected]</span></a> to arrange an alternative license.</li></ol><h2 id="a-catalyst-for-the-ecosystem">A catalyst for the ecosystem</h2><p>We think of this license change as a catalyst that will help Element increase the momentum of Matrix even more; and drive full scale adoption of interoperable, data sovereign and secure real time communication. It will speed innovation and accelerate the entire Matrix economy.</p><p>Across 2023 Element has laid the groundwork for <a href="https://matrix.org/blog/2023/09/matrix-2-0/">Matrix 2.0</a>, enabling a step change in performance built off the back of OIDC, Sliding Sync and Native VoIP. AGPLv3 means all those building on Element’s Matrix 2.0 implementations will now contribute to further development.</p><p>Thanks in large part to our work with BWI, the IT-Systemhouse of the German Armed Forces, <a href="https://element.io/labs/element-x">Element X</a> exists as a <a href="https://element.io/blog/element-x-ignition/">flagship Matrix 2.0 client</a> which will spark a new era of client-side innovation.</p><p>We’ve built out <a href="https://element.io/enterprise-functionality">Element Server Suite</a>, providing a scalable, robust, and professionally supported Matrix server distribution based on our work deploying Matrix for the largest and most demanding organizations in the world - powering both Element deployments, and giving Matrix client developers an enterprise-grade backend to use as they extend the Matrix ecosystem.</p><p>And finally, to help get up and running on Element Server Suite, we’ve launched <a href="https://element.io/blog/element-starter-open-source-meets-on-premise-collaboration/">Element Starter</a> - a free self-hosted version of ESS that supports organizations of up to 200 people.</p><p>Combined with the creation of <a href="https://matrix.org/blog/2023/06/membership-program/">The Matrix.org Foundation Membership program</a>, and <a href="https://matrix.org/blog/2023/09/introducing-josh-simmons-managing-director/">Josh Simmons’ appointment as Managing Director</a>, the Matrix ecosystem has never been so big, mature or held more potential.</p><p>While licensing changes always mark an era of change, we believe the shift to AGPL for Synapse and friends will ensure the entire ecosystem is operating to create a virtuous cycle: helping Element support Matrix more than ever, and so benefiting the entire Matrix industry.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Video Game That Pays: Lessons Learned from Working Remotely (161 pts)]]></title>
            <link>https://dtransposed.github.io/blog/2023/11/02/Remote-SWE/</link>
            <guid>38161997</guid>
            <pubDate>Mon, 06 Nov 2023 13:00:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dtransposed.github.io/blog/2023/11/02/Remote-SWE/">https://dtransposed.github.io/blog/2023/11/02/Remote-SWE/</a>, See on <a href="https://news.ycombinator.com/item?id=38161997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
  <div>
<p><img src="https://dtransposed.github.io/assets/17/1.png" width="50%"></p><p>The original tweet by Sahil Lavingia</p></div>

<p>Some time ago, I stumbled upon this amusing tweet from <a href="https://twitter.com/shl">@shl</a>. I love how true this statement is, although it feels very wrong to admit it. Let’s think about the day in the life of a software engineer:</p>

<blockquote>

  <ul>
    <li>You interact with people from all corners of the world.</li>
    <li>You complete tasks on different online platforms (Slack, GitHub, VS Code, Google Docs, etc.).</li>
    <li>You have a list of your main quests to complete (you can find them in your journ… Kanban board!)</li>
    <li>There are also side-quests to take care of (“Hey Damian, could you take a look at this bug?”).</li>
    <li>Sometimes you can gang up with your teammates to slay a big beast (“Hey, I have this nasty bug that I have been working on for the past few days. I know you have better knowledge of this particular part of the repository, could we jump on a pair programming session?”).</li>
    <li>And you get to level up once in a while!</li>
  </ul>
</blockquote>

<p>Playing a game that pays you money is cool for one important reason. Nobody can give you a hard time playing - after all, the game pays money.</p>

<p>I enjoy exploring the similarities between video games and remote software engineering. However, on a serious note, remote work has its own set of pros and cons, which those experienced in this field are well aware of. Having worked remotely for almost two years, I’ve gained some valuable insights and lessons that I want to share in this blog post. I believe these insights could be helpful to others, especially as more people are considering remote work after the Covid era.</p>

<h3 id="trust---the-currency-of-success-in-remote-work">Trust - The Currency of Success in Remote Work</h3>

<p>It is said (source: “So Good They Can’t Ignore You” by Cal Newport) that there are three traits that characterize work done with passion: creativity, impact, and control.</p>

<p>This last trait, control, especially control over our schedule, is what makes remote work so liberating. As long as you deliver quality work, you are (to a large degree) free to plan your day according to your preferences. You have the flexibility to blend your work schedule with your personal life. You are trusted to find your best individual way to contribute to the company. I find this very motivating.</p>

<p>However, working remotely often demands much more discipline and self-organization than an onsite position. In the office, it is often easy to confuse physical presence with productivity. I am fairly convinced that merely showing up to work and producing limited output is enough for people to “survive” for years in some organizations. Even though they do not produce meaningful output, this is not important, as long as they “show up”. <em>Remote work, on the contrary, is more about building trust through continuously producing quality output, in the absence of the physical presence.</em></p>

<p>Do you remember the infamous <a href="https://www.youtube.com/watch?v=6pLLVqqF8VA">Elon Musk ‘laptop class la-la land’ interview</a>? While I disagree with his claim that remote work is immoral, I understand his concern about productivity. At the same time, I also have strong reasons to believe that in many cases, Elon would have no problem hiring a remote software engineer if he could trust them enough.</p>

<p><u>If you are working in a remote setting, you should ensure that your colleagues trust you and can rely on you. Even if you are not online at the moment, they should know that you eventually will be there and not leave them hanging. The goal of building trust is to eliminate potential friction that arises due to the lack of physical presence.</u></p>

<p>There are a few simple rules that I try to enforce every day to make my remote colleagues’ lives easier:</p>

<ul>
  <li>Always try to end your day in a way that allows the people you interact with to seamlessly pick up where you left off.</li>
  <li>Communicate and work asynchronously. If you want to work fully remotely, it is extremely important to learn how to work asynchronously.</li>
  <li>Do not leave people hanging. If someone asks for your help, either help them or communicate that you cannot do so.</li>
</ul>

<p>Being a trustworthy remote colleague means that your team and managers can rely on you. They understand that your good work helps them do good work, even if they do not physically see you doing it.</p>

<div>
<p><img src="https://dtransposed.github.io/assets/17/2.png" width="50%"></p><p>@shl makes some great points here. He often shares lessons learned from running his company, Gumroad, which is 100% remote and async </p></div>

<h3 id="clarity-and-precision-slow-is-smooth-and-smooth-is-fast">Clarity and Precision: Slow is Smooth and Smooth is Fast</h3>

<p>Imagine you’re sitting at your office desk, fiddling with the idea of getting a cup of coffee. Despite this, you manage to quickly send a message to your friend, notifying her that you encountered a bug after her changes. Then you go grab a coffee. If she did not understand your message, you can quickly visit her desk on the way back from the kitchen and explain in more depth what your perhaps somewhat hasty and imprecise message was all about.</p>

<p>However, when you work remotely, you never know if your colleague is online. Even worse, by the time she comes back and reads your message, you may no longer be online. This brings me to my main point:</p>

<p><u>You want to leave your messages in a state where your colleagues can quickly act on them without needing additional clarification. This ties back to the previous point about asynchronous communication. Your messages should leave no room for ambiguity and enable your colleagues to promptly understand what they need to work on.</u></p>

<p>Let me provide a few examples:</p>

<blockquote>
  <p>You are blocked by an error introduced by your colleague’s pull request. You currently can’t fix it, so you want to give her a heads-up that her diff broke the functionality. What should you do?</p>

  <p><strong>Bad</strong>: Leave an imprecise, sloppy message like, *“Hey, X, have you seen this before: [insert the last line from the stack trace]? Please help!”</p>

  <p><strong>Good</strong>: Leave a detailed description of the problem, including the full stack trace (not just the last few lines), along with an explanation of what you were trying to achieve, what your environment is, and how to reproduce the problem.</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>You have a proposal for a feature or a design solution that you want to share with the team.</p>

  <p><strong>Bad</strong>: Leave a convoluted, unstructured, hasty explanation that, ironically, raises more questions.</p>

  <p><strong>Good</strong>: Provide a short, high-level description of what you would like to implement and include a piece of pseudocode, a flowchart, or a diagram. It doesn’t matter whether you use a formal language like UML or any other visual representation. The key is to efficiently compress your ideas into a format that will enable your colleagues to accurately reconstruct your mental model.</p>
</blockquote>

<p>Yes, following this advice may take an extra 10 minutes of your time, but it will save you twice that time by avoiding back-and-forth exchanges with your team. It will also provide nicely documented material for future reference and show that you are a good communicator who is mindful of the time of your teammates. Remember: <u>slow is smooth, and smooth is fast</u>.</p>

<h3 id="embrace-the-small-time-zone-overlap">Embrace the Small Time Zone Overlap</h3>
<p>Very often, in a remote setting, there is a time zone difference between you and your colleagues. If you are working remotely from Brazil and interacting with the European team, your colleagues are most likely finished before your afternoon. Whether you like it or not, they most likely will not be available for a good part of your evening. For some, this may be frustrating. What happens if I get stuck working on a task? Do I need to wait until the next day for my colleagues to step in?</p>

<p><u>From my experience, most people in their work environment struggle with the opposite problem - the lack of time to work on their own</u>. We often have difficulty “blocking off” the “focus time”. We are in a perpetual context-switching loop. In our calendars, we try to schedule time blocks in advance, hoping to muster up half an hour of concentration time. Very often, we go to some isolated part of the office so that we can work alone on the problem at hand. Serendipity at the workplace is cool, but many companies embrace it so much that employees cannot focus on meaningful deep work. I remember suffering from the same problem when working in the office. I was trying all sorts of creative ways to shield myself from distractions. All the attempts at downloading calendar-blocking apps or openly communicating my focus-time hours were quite futile. The final solution was also suboptimal - showing up in the office at 7 a.m. to get some tasks done before most of the employees arrived at 9 a.m.</p>

<p>Most remote workers are blessed with a few hours when they are not being pinged and have no need to check their email/Teams/Slack/etc. They have the comfort of knowing that every day there are at least a few hours free from the beehive buzz. So their schedule is naturally divided between “deep work” done in seclusion and then back-to-back meetings, discussions, and other attention-grabbing interactions. <u>I find the natural division of the daily schedule productive. It allows for enforcing more structure and thus maintaining better control of your time.</u></p>

<blockquote>
  <p>A 40-hour time-blocked work week, I estimate, produces the same amount of output as a 60+ hour work week pursued without structure. - Cal Newport</p>
</blockquote>

<h3 id="meetings-less-is-more">Meetings: Less is More</h3>

<p>One could naturally flip my argument and say that working across time zones (and thus a limited overlap window) may be harmful to the productivity of an organization. There is only a limited time window in which all team members or the larger part of the organization is available for synchronous meetings. I can recognize how this may be a disadvantage: if there are so few opportunities for all people to meet at the same time, how does the information flow through the company, and how can any sensible decisions be made?</p>

<p><u>But then again, have you ever worked in an organization that has suffered from too few meetings? It is mostly too many meetings!</u> Any company, especially one that grows fast, is very fertile soil for unnecessary meetings to grow like weeds. Recently, <a href="https://blog.johnqian.com/startup-spark">@johnlqian wrote a short and nice blog post about how startups lose the spark as they scale</a>. He points out that the proliferation of unnecessary meetings is one of the main factors.</p>

<p>During my time at Tesla, despite the company’s size, I think we did an okay job of protecting our schedules. This was the result of the rules established by Elon: <a href="https://medium.com/@matthew.edgar.ritchie/want-to-spend-less-time-in-meetings-elon-musk-has-the-answer-9ec1d0d5032f">“Excessive meetings are the blight of big companies and almost always get worse over time”</a>. Tobias Lütke, the CEO of Shopify, is also very critical of excessive meetings. Critical to the point where Shopify conducts periodic <a href="https://www.theguardian.com/money/2023/jan/06/work-meetings-shopify-isolation">“calendar purges”</a> to avoid trudging through the swamp of meetings.</p>

<p><u>Excessive meetings are like an anchor that serves its purpose until it no longer does, resulting in slowing down the whole organization. So if you are a remote-first company, embrace the blessing and the curse of a limited time overlap.</u> You most likely will not be able to spin out multiple series of all-hands, dailies, stand-ups, syncs, or war council meetings. <u> Your meeting budget is fixed and modest, so you need to be mindful of how to spend it. This restriction should be positive in the long run. It should coerce organizations to build a sensible and efficient approach toward the quality and quantity of meetings.</u></p>

<h3 id="conclusion">Conclusion</h3>

<p><u>For me, going remote was very rewarding</u>. My commute has decreased by 2 hours daily. I have returned to running marathons, take much better care of myself, and spend much more time with my loved ones. I am also much more productive, and able to disentangle deep work in isolation from times when I need to be available for collaboration and context-switching.</p>

<p><u>Let's also be clear, remote work is not all unicorns and rainbows</u>. The lack of physical contact with other human beings is, personally, my biggest issue. You can no longer take the act of randomly bumping into one another and spontaneous discussions for granted, as you would in the office. I realized how much I miss meeting with my colleagues at 8 a.m. for an objectively mediocre yet delicious office coffee that gains its value from the social context. In the remote setting, socializing among colleagues needs to be actively managed. While some may think that working remotely may be perfect for introverts, it may be worthwhile to be the force that brings the group together. Putting more direct effort into building relationships, setting up pair-programming sessions, or initiating watercooler talk may benefit everyone.</p>

<p>After the first onsite with Neural Magic, where I got to finally meet the people in the flesh, my team’s productivity significantly boosted. <u>Getting to know people and realizing that they are interesting, passionate individuals rather than pixels on a screen improved our communication and collaboration immensely</u>.</p>

<p>I hope that this write-up was useful for you. Remote work is like playing a video game that not only makes money but also hopefully allows you to grow and have a positive impact on the world.</p>


  



  </article>

  <!-- mathjax -->
  

  
</div></div>]]></description>
        </item>
    </channel>
</rss>