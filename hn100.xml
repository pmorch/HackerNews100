<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 29 Nov 2025 23:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[All it takes is for one to work out (223 pts)]]></title>
            <link>https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</link>
            <guid>46090433</guid>
            <pubDate>Sat, 29 Nov 2025 20:22:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/">https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</a>, See on <a href="https://news.ycombinator.com/item?id=46090433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
<article id="post-24017">
	
	<!-- .entry-header -->

	<div>
		
<p>More than a decade ago, when I was applying to graduate school, I went through a period of deep uncertainty. I had tried the previous year and hadn‚Äôt gotten in anywhere. I wanted to try again, but I had a lot going against me.</p>



<p>I‚Äôd spent most of my undergrad building a student job-portal startup and hadn‚Äôt balanced it well with academics. My GPA needed explaining. My GMAT score was just okay. I didn‚Äôt come from a big-brand employer. And there was no shortage of people with similar or stronger profiles applying to the same schools.</p>



<p>Even though I had learned a few things from the first round, the second attempt was still difficult. There were multiple points after I submitted applications where I lost hope.</p>



<p>But during that stretch, a friend and colleague kept repeating one line to me:</p>



<p><em>‚ÄúAll it takes is for one to work out.‚Äù</em></p>



<p>He‚Äôd say it every time I spiraled. And as much as it made me smile, a big part of me didn‚Äôt fully believe it. Still, it became a little maxim between us. And eventually, he was right ‚Äì that one did work out. And it changed my life.</p>



<p>I‚Äôve thought about that framing so many times since then.</p>



<p>It‚Äôs unbelievably powerful in any high-stakes search:</p>



<p>You don‚Äôt need every job to choose you. You just need the one that‚Äôs the right fit. </p>



<p>You don‚Äôt need every house to accept your offer. You just need the one that feels like home. </p>



<p>You don‚Äôt need every person to want to build a life with you. You just need the one.</p>



<p>You don‚Äôt need ten universities to say yes. You just need the one that opens the right door.</p>



<p>These processes ‚Äì college admissions, job searches, home buying, finding a partner ‚Äì can be emotionally brutal. They can get you down in ways that feel personal. But in those moments, that truth can be grounding.</p>



<p>All it takes is for one to work out.</p>



<p>And that one is all you need.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-24017 -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be Like Clippy (166 pts)]]></title>
            <link>https://be-clippy.com/</link>
            <guid>46090172</guid>
            <pubDate>Sat, 29 Nov 2025 19:41:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://be-clippy.com/">https://be-clippy.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46090172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p><strong>"Clippy didn‚Äôt sell your data. Clippy didn‚Äôt hold your data hostage. Clippy was there to help you."</strong></p><p>Fed up with trillion-dollar companies exploiting your data?
                Forced to use their services?
                Your data held for ransom?
                Your data used to train their AI models?
                Opt-outs for data collection instead of opt-ins?</p> <p>Join the movement to make companies more like Clippy. <span>Set your profile picture to Clippy</span>, make your voice heard.</p> <p>Below is a video that explains the Be Like Clippy movement. It‚Äôs a call to action for developers, companies, and users alike to embrace a more open, transparent, and user-friendly approach to technology.</p></div><div><p>Licensed under <a href="https://github.com/NotReeceHarris/be-clippy.com/blob/main/LICENSE">GPL-3.0</a></p> <p>Share your own on <a href="https://github.com/NotReeceHarris/be-clippy.com">Github</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework Computer Now Sponsoring LVFS / Fwupd Development (109 pts)]]></title>
            <link>https://www.phoronix.com/news/Framework-Sponsoring-LVFS</link>
            <guid>46089980</guid>
            <pubDate>Sat, 29 Nov 2025 19:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Framework-Sponsoring-LVFS">https://www.phoronix.com/news/Framework-Sponsoring-LVFS</a>, See on <a href="https://news.ycombinator.com/item?id=46089980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="HARDWARE" src="https://www.phoronix.com/assets/categories/hardware.webp" width="100" height="100"></p><p>
With <a href="https://www.phoronix.com/news/LVFS-Fwupd-135-Million-Download">the Linux Vendor Firmware Service serving more than 135 million downloads</a> for Linux users updating their system and device firmware, LVFS has been <a href="https://www.phoronix.com/news/LVFS-Fair-Use-Quota">working to get more hardware vendors to contribute</a> either engineering resources or directly contributing annual dues as sponsors. Framework Computer is now the first one to have executed an agreement under these new sponsorship efforts.
</p><p>
Red Hat in employing lead developer Richard Hughes has contributed the most to LVFS/Fwupd's success, the Linux Foundation has also hosted the project since it has shifted into their umbrella, AMD's Mario Limonciello is among the significant contributors, and now Framework Computer is a new sponsor to the project.
</p><p>
Richard Hughes posted on <a href="https://mastodon.social/@hughsie/115628170779065489">Mastodon</a> on Friday:
</p><blockquote>"I'm also happy to announce we've got a new sponsor for the LVFS: #Framework
<p>
Although there are about a half a dozen OEMs that have promised to sponsor LVFS, Framework is the first to have actually signed the paperwork."</p></blockquote>
<p>The graphic on the <a href="https://fwupd.org/">Fwupd.org page</a> shows Framework as a Startup Sponsor, which puts them in the ballpark of around $10k USD in annual dues to the project.
</p><p><img src="https://www.phoronix.net/image.php?id=2025&amp;image=lvfs_framework" alt="LVFS Framework sponsorship"></p>
<p>Richard Hughes also followed up with a <a href="https://mastodon.social/@hughsie/115628171746383319">comment</a> on the positive impact that Framework has also applied on their suppliers around Fwupd/LVFS support too:
</p><blockquote>"I also wanted to say a huge thanks to Framework, not just for the sponsorship -- but also for the pressure they've put on their suppliers to support fwupd and the LVFS."</blockquote>
<p>Framework has already been <a href="https://www.phoronix.com/news/Fwupd-2.0.14-Released">supporting LVFS/Fwupd with their devices</a> and ensuring good Linux support in general, such as with <a href="https://www.phoronix.com/review/framework-16-ryzen-ai-300-series">the recent Framework 16 laptop upgrade</a>.
</p><p><img src="https://www.phoronix.net/image.php?id=framework-16-ryzen-ai-300-series&amp;image=framework_16_13_med" alt="Framework 16 upgrade"></p>
<p>Hopefully the sponsorship agreements with the other major OEMs pan out and that these other vendors also continue ramping up in mandating LVFS/Fwupd hardware support for ensuring a better firmware updating experience for Linux customers.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electric vehicle sales are booming in South America ‚Äì without Tesla (126 pts)]]></title>
            <link>https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/</link>
            <guid>46089971</guid>
            <pubDate>Sat, 29 Nov 2025 19:12:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/">https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/</a>, See on <a href="https://news.ycombinator.com/item?id=46089971">Hacker News</a></p>
Couldn't get https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[We're learning more about what Vitamin D does to our bodies (104 pts)]]></title>
            <link>https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/</link>
            <guid>46088998</guid>
            <pubDate>Sat, 29 Nov 2025 16:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/">https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/</a>, See on <a href="https://news.ycombinator.com/item?id=46088998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content--body"><div> <p>It has started to get really wintry here in London over the last few days. The mornings are frosty, the wind is biting, and it‚Äôs already dark by the time I pick my kids up from school. The darkness in particular has got me thinking about vitamin D, a.k.a. the sunshine vitamin.</p>  <p>At a checkup a few years ago, a doctor told me I was deficient in vitamin D. But he wouldn‚Äôt write me a prescription for supplements, simply because, as he put it, <em>everyone</em> in the UK is deficient. Putting the entire population on vitamin D supplements would be too expensive for the country‚Äôs national health service, he told me.</p> </div><div> <p><strong>But supplementation‚Äîwhether covered by a health-care provider or not‚Äîcan be important.</strong> As those of us living in the Northern Hemisphere spend fewer of our waking hours in sunlight, let‚Äôs consider the importance of vitamin D.</p>  <p>Yes, it is important for bone health. But recent research is also uncovering surprising new insights into how the vitamin might influence other parts of our bodies, including our immune systems and heart health.</p> 
 <p><strong>Vitamin D was discovered just over 100 years ago,</strong> when health professionals were looking for ways to treat what was then called ‚Äúthe English disease.‚Äù Today, we know that rickets, a weakening of bones in children, is caused by vitamin D deficiency. And vitamin D is best known for its importance in bone health.</p>  <p>That‚Äôs because it helps our bodies absorb calcium. Our bones are continually being broken down and rebuilt, and they need calcium for that rebuilding process. Without enough calcium, bones can become weak and brittle. (Depressingly, rickets is still a global health issue, which is why there is&nbsp;<a href="https://www.who.int/tools/elena/bbc/vitamind-infants">global consensus that infants should receive a vitamin D supplement</a> at least until they are one year old.)</p> 
 <p>In the decades since then, scientists have learned that vitamin D has effects beyond our bones. There‚Äôs some evidence to suggest, for example, that being deficient in vitamin D puts people at risk of high blood pressure. Daily or weekly supplements&nbsp;<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9509305/">can help those individuals</a> lower their blood pressure.</p> </div><div> <p>A vitamin D deficiency has also been linked to&nbsp;<a href="https://link.springer.com/content/pdf/10.1007/s40265-023-01875-8.pdf">a greater risk of ‚Äúcardiovascular events‚Äù</a> like heart attacks, although it‚Äôs not clear whether supplements can reduce this risk; the evidence is pretty mixed.</p>  <p>Vitamin D appears to influence our immune health, too.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0002916523302831?via%3Dihub">Studies</a>&nbsp;<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/414815">have</a> found a link between low vitamin D levels and incidence of the common cold, for example. And other&nbsp;<a href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2022.790444/full">research</a> has shown that vitamin D supplements can influence the way our genes make proteins that play important roles in the way our immune systems work.</p>  <p>We don‚Äôt yet know exactly how these relationships work, however. And, unfortunately, a recent&nbsp;<a href="https://www.thelancet.com/action/showPdf?pii=S2213-8587%2824%2900348-6">study</a> that assessed the results of 37 clinical trials found that overall, vitamin D supplements aren‚Äôt likely to stop you from getting an ‚Äúacute respiratory infection.‚Äù</p> </div><div> <p><strong>Other studies have linked vitamin D levels to mental health, pregnancy outcomes, and even how long people survive after a cancer diagnosis.</strong> It‚Äôs tantalizing to imagine that a cheap supplement could benefit so many aspects of our health.</p>  <p>But, as you might have gathered if you‚Äôve got this far, we‚Äôre not quite there yet. The evidence on the effects of vitamin D supplementation for those various conditions is mixed at best.</p> </div><div><p>In fairness to researchers, it can be difficult to run a randomized clinical trial for vitamin D supplements. That‚Äôs because most of us get the bulk of our vitamin D from sunlight. Our skin converts UVB rays into a form of the vitamin that our bodies can use. We get it in our diets, too, but not much. (The main sources are oily fish, egg yolks, mushrooms, and some fortified cereals and milk alternatives.)</p>  <p>The standard way to measure a person‚Äôs vitamin D status is to look at blood levels of 25-hydroxycholecalciferol (25(OH)D), which is formed when the liver metabolizes vitamin D. But not everyone can agree on what the ‚Äúideal‚Äù level is.</p> 

 <p>Even if everyone did agree on a figure, it isn‚Äôt obvious how much vitamin D a person would need to consume to reach this target, or how much sunlight exposure it would take. One complicating factor is that people respond to UV rays in different ways‚Äîa lot of that can depend on how much melanin is in your skin. Similarly, if you‚Äôre sitting down to a meal of oily fish and mushrooms and washing it down with a glass of fortified milk, it‚Äôs hard to know how much more you might need.</p>  <p><strong>There is more consensus on the definition of vitamin D <em>deficiency</em>, though.</strong> (It‚Äôs a blood level below 30 nanomoles per liter, in case you were wondering.) And until we know more about what vitamin D is doing in our bodies, our focus should be on avoiding that.</p>  <p>For me, that means topping up with a supplement. The UK government&nbsp;<a href="https://www.nhs.uk/conditions/vitamins-and-minerals/vitamin-d/">advises</a> everyone in the country to take a 10-microgram vitamin D supplement over autumn and winter. That advice doesn‚Äôt factor in my age, my blood levels, or the amount of melanin in my skin. But it‚Äôs all I‚Äôve got for now. </p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Major AI conference flooded with peer reviews written by AI (171 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-03506-6</link>
            <guid>46088236</guid>
            <pubDate>Sat, 29 Nov 2025 15:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-03506-6">https://www.nature.com/articles/d41586-025-03506-6</a>, See on <a href="https://news.ycombinator.com/item?id=46088236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="access-teaser"> <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Close up view of a red toy robot sat amongst a stack of books." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg"><figcaption><p><span>An AI-detection tool developed by Pangram labs found that peer reviewers are increasingly using chatbots to draft responses to authors.</span><span>Credit: breakermaximus/iStock via Getty</span></p></figcaption></picture></figure><p>What can researchers do if they suspect that their manuscripts have been peer reviewed using artificial intelligence (AI)? Dozens of academics have raised concerns on social media about manuscripts and peer reviews submitted to the organizers of next year‚Äôs International Conference on Learning Representations (ICLR), an annual gathering of specialists in machine learning. Among other things, they flagged <a href="https://www.nature.com/articles/d41586-025-02853-8" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02853-8" data-track-category="body text link">hallucinated citations</a> and suspiciously long and vague feedback on their work.</p><p>Graham Neubig, an AI researcher at Carnegie Mellon University in Pittsburgh, Pennsylvania, was one of those who received peer reviews that seemed to have been<a href="https://www.nature.com/articles/d41586-025-03390-0" data-track="click" data-label="https://www.nature.com/articles/d41586-025-03390-0" data-track-category="body text link"> produced using large language models (LLMs)</a>. The reports, he says, were ‚Äúvery verbose with lots of bullet points‚Äù and requested analyses that were not ‚Äúthe standard statistical analyses that reviewers ask for in typical AI or machine-learning papers.‚Äù</p><p>But Neubig needed help proving that the reports were AI-generated. So, he posted on X (formerly Twitter) and offered a reward for anyone who could scan all the conference submissions and their peer reviews for AI-generated text. The next day, he got a response from Max Spero, chief executive of Pangram Labs in New York City, which develops tools to detect AI-generated text. Pangram screened all 19,490 studies and 75,800 peer reviews submitted for ICLR 2026, which will take place in Rio de Janeiro, Brazil, in April. Neubig and more than 11,000 other AI researchers will be attending.</p><p>Pangram‚Äôs analysis revealed that around 21% of the ICLR peer reviews were fully AI-generated, and more than half contained signs of AI use. The findings were <a href="https://www.pangram.com/blog/pangram-predicts-21-of-iclr-reviews-are-ai-generated" data-track="click" data-label="https://www.pangram.com/blog/pangram-predicts-21-of-iclr-reviews-are-ai-generated" data-track-category="body text link">posted online by Pangram Labs</a>. ‚ÄúPeople were suspicious, but they didn‚Äôt have any concrete proof,‚Äù says Spero. ‚ÄúOver the course of 12 hours, we wrote some code to parse out all of the text content from these paper submissions,‚Äù he adds. </p><p>The conference organizers say they will now use automated tools to assess whether submissions and peer reviews breached policies on using <a href="https://www.nature.com/articles/d41586-025-01839-w" data-track="click" data-label="https://www.nature.com/articles/d41586-025-01839-w" data-track-category="body text link">AI in submissions and peer reviews</a>. This is the first time that the conference has faced this issue at scale, says Bharath Hariharan, a computer scientist at Cornell University in Ithaca, New York, and senior programme chair for ICLR 2026. ‚ÄúAfter we go through all this process ‚Ä¶ that will give us a better notion of trust.‚Äù </p><h2>AI-written peer review</h2><p>The Pangram team used one of its own tools, which <a href="https://www.nature.com/articles/d41586-025-02936-6" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02936-6" data-track-category="body text link">predicts whether text is generated or edited by LLMs</a>. Pangram‚Äôs analysis flagged 15,899 peer reviews that were fully AI-generated. But it also identified many manuscripts that had been submitted to the conference with suspected cases of AI-generated text: 199 manuscripts (1%) were found to be fully AI-generated; 61% of submissions were mostly human-written; but 9% contained more than 50% AI-generated text. </p><p>Pangram described the model in a preprint<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>, which it submitted to ICLR 2026. Of the four peer reviews received for the manuscript, one was flagged as fully AI-generated and another as lightly AI-edited, the team‚Äôs analysis found.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-025-00894-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_50959900.jpg"><p>AI is transforming peer review ‚Äî and many scientists are worried</p></a></article><p>For many researchers who received peer reviews for their submissions to ICLR, the Pangram analysis confirmed what they had suspected. Desmond Elliott, a computer scientist at the University of Copenhagen, says that one of three reviews he received seemed to have missed ‚Äúthe point of the paper‚Äù. His PhD student who led the work suspected that the review was generated by LLMs, because it mentioned numerical results from the manuscript that were incorrect and contained odd expressions. </p><p>When Pangram released its findings, Elliott adds, ‚Äúthe first thing I did was I typed in the title of our paper because I wanted to know whether my student‚Äôs gut instinct was correct‚Äù. The suspect peer review, which Pangram‚Äôs analysis flagged as fully AI-generated, gave the manuscript the lowest rating, leaving it ‚Äúon the borderline between accept and reject‚Äù, says Elliott. ‚ÄúIt's deeply frustrating‚Äù.</p><h2>Repercussions</h2></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iceland declares ocean-current instability a national security risk (310 pts)]]></title>
            <link>https://www.dagens.com/news/iceland-declares-ocean-current-instability-a-national-security-risk</link>
            <guid>46088192</guid>
            <pubDate>Sat, 29 Nov 2025 15:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dagens.com/news/iceland-declares-ocean-current-instability-a-national-security-risk">https://www.dagens.com/news/iceland-declares-ocean-current-instability-a-national-security-risk</a>, See on <a href="https://news.ycombinator.com/item?id=46088192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Iceland has classified the potential collapse of a major Atlantic current system as a national security threat, citing new scientific warnings that such a change could radically alter the country‚Äôs climate and economy. Officials say the risk represents an ‚Äúexistential‚Äù challenge that demands a coordinated, top-level response.</p>

			<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(0);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Others are reading now</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>

<p>Iceland has taken the rare step of treating a climate-linked ocean threat as a matter of national survival, launching a coordinated government response to one of the most feared potential tipping points in the climate system.</p>



<p><br>Officials say the shift reflects mounting evidence that a key Atlantic current system could be heading toward dangerous instability.</p>



<h2>Security shift in Reykjav√≠k</h2>



<p>According to <em><a href="https://edition.cnn.com/2025/11/15/climate/iceland-warming-current-amoc-collapse-threat" target="_blank" rel="noopener">CNN</a></em>, Iceland‚Äôs National Security Council formally labelled the possible collapse of the Atlantic Meridional Overturning Circulation (AMOC) a national security risk in September ‚Äî the first time the country has applied such a designation to a climate impact.</p>



<p><br>The move followed a government briefing on new research that raised ‚Äúgrave concerns‚Äù about the system‚Äôs future stability.</p>



<p>J√≥hann P√°ll J√≥hannsson, Iceland‚Äôs minister for environment, energy and climate, said the risks extend far beyond weather. </p>
<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(2);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Also read</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>



<p>‚ÄúOur climate, economy and security are deeply tied to the stability of the ocean currents around us,‚Äù he told <em>CNN</em>.</p>



<p>He later described the threat as ‚Äúan existential threat,‚Äù warning that a breakdown could disrupt transport, damage infrastructure and hit the country‚Äôs fishing industry.</p>



<h2>Why the AMOC matters</h2>



<p>The AMOC ‚Äî often compared to a giant conveyor belt ‚Äî carries warm water northward before it cools and sinks, helping regulate weather across the Atlantic basin. </p>



<p><em>CNN </em>reported that scientists increasingly worry that warming temperatures and disrupted salinity levels are slowing the system.</p>



<p>Some studies suggest a tipping point could be reached this century, though the exact timeline remains uncertain.</p>
<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(4);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Also read</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>



<p>Stefan Rahmstorf, an oceanographer at Potsdam University, told <em>CNN </em>that a collapse ‚Äúcannot be considered a low likelihood risk anymore.‚Äù </p>



<p>The consequences, he said, would be dramatic: surging sea levels along US and European coasts, major monsoon disruptions across Africa and Asia, and a deep freeze across parts of Europe.</p>



<p>For Iceland, he said, the country ‚Äúwould be close to the center of a serious regional cooling,‚Äù with sea ice potentially surrounding the island.</p>



<h2>Preparing for the worst</h2>



<p>The security designation means Iceland will now pursue a high-level, cross-government effort to analyse the threat and consider how to manage or reduce the consequences. J√≥hannsson said the decision </p>



<p>‚Äúreflects the seriousness of the issue and ensures that the matter gets the attention it deserves.‚Äù</p>
<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(6);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Also read</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>



<p>Rahmstorf praised Iceland‚Äôs stance, telling <em>CNN </em>that other nations should treat the risk with similar urgency.</p>



<p>J√≥hannsson said the country is confronting a stark possibility: ‚ÄúWhat we do know is that the current climate might change so drastically that it could become impossible for us to adapt‚Ä¶ this is not just a scientific concern ‚Äî it‚Äôs a matter of national survival and security.‚Äù </p>



<p><strong>sources</strong>:CNN</p>
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's Always the Process, Stupid (248 pts)]]></title>
            <link>https://its.promp.td/its-always-the-process-stupid/</link>
            <guid>46087737</guid>
            <pubDate>Sat, 29 Nov 2025 14:20:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://its.promp.td/its-always-the-process-stupid/">https://its.promp.td/its-always-the-process-stupid/</a>, See on <a href="https://news.ycombinator.com/item?id=46087737">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                <a href="https://its.promp.td/tag/article/">Article</a>
            
                <p>Why AI Won‚Äôt Save Your Broken Workflow</p>

            <div>
                <p><a href="https://its.promp.td/author/dsb/">
                                <img src="https://its.promp.td/content/images/size/w160/2025/08/dsb-slide2016.jpg" alt="DocIsInDaHouse">
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2025-11-29">29 Nov. 2025</time>
                            <span><span>‚Äî</span> 3 min read</span>
                    </p>
                </div>
            </div>

                <figure>
        <img srcset="https://its.promp.td/content/images/size/w320/2025/11/MessyWorkflow.jpeg 320w,
                    https://its.promp.td/content/images/size/w600/2025/11/MessyWorkflow.jpeg 600w,
                    https://its.promp.td/content/images/size/w960/2025/11/MessyWorkflow.jpeg 960w,
                    https://its.promp.td/content/images/size/w1200/2025/11/MessyWorkflow.jpeg 1200w,
                    https://its.promp.td/content/images/size/w2000/2025/11/MessyWorkflow.jpeg 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://its.promp.td/content/images/size/w1200/2025/11/MessyWorkflow.jpeg" alt="It‚Äôs Always the Process, Stupid!">
    </figure>

        </div><section>
            <p>Let‚Äôs rip the Band-Aid off immediately: If your underlying business process is a mess, sprinkling "AI dust" on it won‚Äôt turn it into gold. It will just speed up the rate at which you generate garbage.</p><p>In the world of Business IT, we get seduced by the shiny new toy. Right now, that toy is Artificial Intelligence. Boardrooms are buzzing with buzzwords like LLMs, agentic workflows, and generative reasoning. Executives are frantically asking, <em>"What is our AI strategy?"</em></p><p>But here is the hard truth: </p><blockquote><strong>There is no such thing as an AI strategy. <br>There is only Business Process Optimization (BPO).</strong></blockquote><h3 id="the-magic-wand-fallacy">The "Magic Wand" Fallacy</h3><p>Too many enterprises treat AI like a magic wand. They believe that by implementing a sophisticated neural network, their structural inefficiencies will vanish. They think AI brings intelligence.</p><p>It doesn‚Äôt.</p><p>Like every major technological shift before it‚Äîfrom the steam engine to the spreadsheet‚ÄîAI does not inherently make an organization smarter. <strong>AI, like any other tool, only makes faster.</strong></p><p>If you automate a stupid decision, you just make stupid decisions at light speed. If you apply an agentic AI workflow to a bureaucratic nightmare of an approval chain, you haven't fixed the bureaucracy; you‚Äôve just built a robot that hates its job as much as your employees do.</p><h3 id="the-unstructured-data-trap">The Unstructured Data Trap</h3><p>There is, however, one superpower AI possesses that previous tools lacked: <strong>It is the first technology that is truly useful for handling unstructured data.</strong></p><p>For decades, traditional software demanded structure. Rows, columns, booleans, and fixed fields. If data didn't fit the box, the computer couldn't read it.</p><p>AI changes this. It can read messy emails, interpret vague Slack messages, parse PDFs, and analyze images. But this capability exposes a massive, hidden problem in most enterprises.</p><p><strong>Processes that rely on unstructured data are usually unstructured processes.</strong></p><p>Because computers couldn't handle the mess, humans handled it (before AI). And humans don't always follow a flow chart. These processes‚Äîlike "handling a complex customer complaint" or "brainstorming a marketing campaign"‚Äîare often ad-hoc, intuitive, and completely undocumented. They live in the heads of your senior staff, not in your SOPs.</p><h3 id="you-can%E2%80%99t-automate-what-you-haven%E2%80%99t-designed">You Can‚Äôt Automate What You Haven‚Äôt Designed</h3><p>This brings us back to BPO. You cannot apply AI to these "hidden" processes until you drag them into the light.</p><p>If you want to use AI to process unstructured data, you must first bring structure to the workflow itself. You need to improve your process design to account for the ambiguity that AI handles.</p><p>Ask yourself:</p><ol><li><strong>What is the trigger?</strong> (Where does the unstructured mess come from?)</li><li><strong>What is the transformation?</strong> (What exactly is the human‚Äîor now the AI‚Äîsupposed to extract or deduce from that mess?)</li><li><strong>What is the structured output?</strong> (How does this flow back into your rigid ERP or CRM systems?)</li></ol><h3 id="speed-vs-intelligence">Speed vs. Intelligence</h3><p>Let‚Äôs clarify the distinction between "smarter" and "faster."</p><p>Intelligence implies wisdom, context, and nuance. While AI models are simulating reasoning better every day, in a business context, they are fundamentally pattern-matching engines. They excel at acceleration.</p><ul><li><strong>The Old Way:</strong> An analyst reads 50 contracts (unstructured), highlights risks based on gut feeling (unstructured process), and summarizes them in 3 days.</li><li><strong>The AI Way:</strong> An AI scans 50 contracts and extracts specific risk clauses based on defined parameters in 3 minutes.</li></ul><p>The <em>process</em> (Review Contracts -&gt; Identify Risk -&gt; Summarize) hasn't changed, but it had to be rigorously defined for the AI to work. The <em>intelligence</em> (knowing what a "risk" actually means) still requires human governance. What has changed is the <strong>velocity</strong>.</p><h3 id="the-bottom-line">The Bottom Line</h3><p>Stop chasing the hype. Stop looking for a specialized "AI Savior."</p><p>Go back to the whiteboard. Map out your value chain‚Äîespecially the messy, human-centric parts involving unstructured data that you previously ignored. Find the bottlenecks. Identify the waste.</p><p>Once you have a streamlined, logical, and robust business process, <em>then</em> apply AI to hit the accelerator.</p><div><p>Technology changes. <br>The rules of business efficiency do not. <br>It‚Äôs always the process, stupid!</p><p>And that's where actual AI Tools are missing that point, because they weren't build for that </p></div><figure><a href="https://its.promp.td/the-great-it-divide-why-ai-adoption-in-enterprises-is-failing/"><div><p>The Great IT-Divide: Why AI-Adoption in enterprises is failing</p><p>IT innovation moved from business tools to social tech, creating two distinct IT worlds: Business-IT (compliance, efficiency) and Social-IT (social interaction). Grasping this divide is crucial for enterprise adoption and explains why businesses struggle with AI uptake.</p><p><img src="https://its.promp.td/content/images/icon/promptdIconRoundBlue-1-16.png" alt=""><span>it's promp.td</span><span>DocIsInDaHouse</span></p></div><p><img src="https://its.promp.td/content/images/thumbnail/ITDivide-2.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><hr><p>Live long and prosper üòâüññ</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS LOC Record (2014) (124 pts)]]></title>
            <link>https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/</link>
            <guid>46087596</guid>
            <pubDate>Sat, 29 Nov 2025 14:02:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/">https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/</a>, See on <a href="https://news.ycombinator.com/item?id=46087596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>2014-04-01</p><section><p>2 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7lCDvfhTzFcVMB5PveKJco/9283bc24e4a11eddef1aa631a61960e9/the-weird-and-wonderful-world-of-dns-loc-records.jpg" alt=""><div><p>A cornerstone of CloudFlare's infrastructure is our ability to serve DNS requests quickly and <a href="https://blog.cloudflare.com/deep-inside-a-dns-amplification-ddos-attack">handle DNS attacks</a>. To do both those things we wrote our own authoritative DNS server called <a href="https://blog.cloudflare.com/the-story-of-a-little-dns-easter-egg">RRDNS</a> in Go. Because of it we've been able to fight off DNS attacks, and be consistenly one of the <a href="https://blog.cloudflare.com/cloudflare-fastest-free-dns-among-fastest-dns">fastest</a> DNS providers on the web.</p><p>Implementing an authoritative DNS server is a large task. That's in part because DNS is a very old standard (<a href="http://www.ietf.org/rfc/rfc1035.txt">RFC 1035</a> dates to 1987), in part because as DNS has developed it has grown into a more and more complex system, and in part because what's written in the RFCs and what happens in the real-world aren't always the same thing.</p><p>One little used type of DNS record is the LOC (or location). It allows you to specify a physical location. CloudFlare handles millions of DNS records; of those just 743 are LOCs. Nevertheless, it's possible to set up a LOC record in the CloudFlare DNS editor.</p>
            <figure>
            
            <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/SE3YNqdqCWTZjCq2X3204/6350156b41c21658e2c1cafe7780ae4b/480px-Trinity_Site_Obelisk_National_Historic_Landmark.jpg" alt="Trinity" width="480" height="480" loading="lazy">
            
            </figure><p>My site <a href="https://web.archive.org/web/20140329092052/http://geekatlas.com/">geekatlas.com</a> has a LOC record as an Easter Egg. Here's how it's configured in the CloudFlare DNS settings:</p>
            <figure>
            
            <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2Qe1yA3xIACsdf36XEi1oQ/1a078bd649abd88e5b382cab36f748b9/Screen_Shot_2014-03-27_at_11.34.24.png" alt="LOC Example" width="1894" height="96" loading="lazy">
            
            </figure><p>When you operate at CloudFlare scale the little-used nooks and crannies turn out to be important. And even though there are only 743 LOC records in our entire database, at least one customer contacted support to find out why their LOC record wasn't being served.</p><p>And that sent me into the RRDNS source code to find out why.</p><p>The answer was simple. Although RRDNS had code for receiving requests for LOC records, creating response packets containing LOC data, there was a missing link. The CloudFlare DNS server stores the LOC record as a string (such as the <code>33 40 31 N 106 28 29 W 10m</code> above) and no one had written the code to parse that and turn it into the internal format. Oops.</p><p>The textual LOC format and the binary, on-the-wire, format are described in <a href="https://tools.ietf.org/rfc/rfc1876.txt">RFC 1876</a> and it's one of many RFCs that updated the original 1987 standard. RFC 1876 is from 1996.</p><p>The textual format is fairly simple. Here's what the RFC says:</p>
            <pre><code>The LOC record is expressed in a primary file in the following format:

owner TTL class LOC ( d1 [m1 [s1]] {"N"|"S"} d2 [m2 [s2]]
                           {"E"|"W"} alt["m"] [siz["m"] [hp["m"]
                           [vp["m"]]]] )

where:

   d1:     [0 .. 90]            (degrees latitude)
   d2:     [0 .. 180]           (degrees longitude)
   m1, m2: [0 .. 59]            (minutes latitude/longitude)
   s1, s2: [0 .. 59.999]        (seconds latitude/longitude)
   alt:    [-100000.00 .. 42849672.95] BY .01 (altitude in meters)
   siz, hp, vp: [0 .. 90000000.00] (size/precision in meters)

If omitted, minutes and seconds default to zero, size defaults to 1m,
horizontal precision defaults to 10000m, and vertical precision
defaults to 10m.  These defaults are chosen to represent typical
ZIP/postal code area sizes, since it is often easy to find
approximate geographical location by ZIP/postal code.</code></pre>
            <p>So, there are required latitude, longitude and altitude and three optional values for the size of the location and precision information. Pretty simple.</p><p>Then there's the on-the-wire format. Unlike a TXT record the LOC record data is parsed and turned into a fixed size binary format. Back to RFC 1876:</p>
            <pre><code>  MSB                                           LSB
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  0|        VERSION        |         SIZE          |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  2|       HORIZ PRE       |       VERT PRE        |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  4|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  6|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  8|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 10|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 12|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 14|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</code></pre>
            <p>So, 32 bits of latitude, longitude and altitude and then three 8 bit values for the size and precision. The latitude and longitude values have a pretty simple encoding that treats the 32 bits as an unsigned integer:</p>
            <pre><code>The latitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in thousandths of a second of arc.  2^31 represents the equator; numbers above that are north latitude.</code></pre>
            <p>And the altitude can be below sea-level but still unsigned:</p>
            <pre><code>The altitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in centimeters, from a base of 100,000m below the [WGS 84] reference spheroid used by GPS.</code></pre>
            <p>But the 8 bit values use a very special encoding that allows a wide range of approximate values to be packed into 8 bits and also be human-readable when dumped out in hex!</p>
            <pre><code>The diameter of a sphere enclosing the described entity, in centimeters, expressed as a pair of four-bit unsigned integers, each ranging from zero to nine, with the most significant four bits representing the base and the second number representing the power of ten by which to multiply the base.  This allows sizes from 0e0 (&lt;1cm) to 9e9 (90,000km) to be expressed.  This representation was chosen such that the hexadecimal representation can be read by eye; 0x15 = 1e5.</code></pre>
            <p>For example, the value <code>0x12</code> means <code>1 * 10^2</code> or 100cm. <code>0x99</code> means <code>9 * 10^9</code> or 90,000,000m. The smallest value that can be represented is 1cm (it's <code>0x10</code>). So, in just 8 bits there's a range values from 1cm to larger than the diameter of Jupiter.</p><p>To fix this I wrote a parser for the LOC text record type (and associated tests). It can be found <a href="https://gist.github.com/jgrahamc/9807839">here</a>.</p><p>We've now rolled out the fix and all the existing LOC records are being served by RRDNS. For example, my <code>geekatlas.com</code> LOC record can be queried like this:</p>
            <pre><code>$ dig geekatlas.com LOC
; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; geekatlas.com LOC
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2997
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:    
;geekatlas.com.         IN  LOC

;; ANSWER SECTION:
geekatlas.com.      299 IN  LOC 33 40 31.000 N 106 28 29.000 W 10.00m 1m 10000m 10m

;; Query time: 104 msec
;; SERVER: 192.168.14.1#53(192.168.14.1)
;; WHEN: Tue Apr  1 14:13:48 2014
;; MSG SIZE  rcvd: 59</code></pre>
            </div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><astro-slot> <!--[if astro]>server-island-start<![endif]--> </astro-slot><a href="https://blog.cloudflare.com/tag/rrdns/">RRDNS</a><a href="https://blog.cloudflare.com/tag/dns/">DNS</a><a href="https://blog.cloudflare.com/tag/reliability/">Reliability</a><a href="https://blog.cloudflare.com/tag/attacks/">Attacks</a><a href="https://blog.cloudflare.com/tag/go/">Go</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hachi: An Image Search Engine (116 pts)]]></title>
            <link>https://eagledot.xyz/hachi.md.html</link>
            <guid>46087549</guid>
            <pubDate>Sat, 29 Nov 2025 13:56:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eagledot.xyz/hachi.md.html">https://eagledot.xyz/hachi.md.html</a>, See on <a href="https://news.ycombinator.com/item?id=46087549">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<h2>Hachi: An (Image) Search engine</h2>
<blockquote>
<p>Only the dead have seen the end of war ..   George Santayana
<br></p>
</blockquote>
<p>For quite some time now, i have been working on and off on a fully self-hosted <a href="https://github.com/eagledot/hachi">search engine</a>, in hope to make it easier to search across Personal data in an <code>end to end</code> manner. Even as individuals, we are hoarding and generating more and more data with no end in sight. Such "personal" data is being stored from local hard-disks to corporate controlled cloud-centers which makes it distributed in nature. So for following discussion, "Personal" meaning would be flexible enough to accommodate resources on a remote server and/or on different devices, as long the user could prove authentication and/or authorization to that data.
Current implementation supports only "images", but eventual goal is also to support other modalities like <code>video</code>, <code>text</code> and <code>audio</code>, some code would be shared, while some new code would be required to better extract <code>Features</code> for each modality.</p>
<p>Such distributed nature of data and potential capabilities of current self-hosted Machine learning models to extract semantic information, only to be queried through a single interface seemed enticing enough for me start this experiment in the first place. Following post at times may seem in-coherent, as i try to articulate my thoughts on the journey of development, challenges faced and future ideas. I hope to treat this as a personal essay with multiple themes, anecdotes and even random thoughts aiming to provide a higher level view of the journey and philosophy so far in more concrete terms.<br>
Also, following post doesn't aim to cover every technical choice and implementation in finer details, such discussions would instead be part of dedicated future posts!</p>
<h2>Motivation:</h2>
<p>As Humans we tend to remember different attributes/parts of an entity/information at different times, and most of search engines' interfaces refuse to accomodate that. User generally end up with an unidirectional flow of information, with no recourse of providing feedback to improve upon the on-going query. Even most advanced interfaces fail to handle the stochastic nature of queries and humans' pre-disposition towards partial information to keep moving, it should be default for search-engines to present best-effort suggestions for queries even if they couldn't be <em>fully</em> resolved. </p>
<p>I also note that, it is not always easy to model the imperfect information like handelling a mis-spelling, which itself could be mis-spelled in many ways. It would require a conscious effort to put in a better search interface, as most digital machines make it easy to model when "something" is "correct" or when something is "incorrect". Conveying "Why" something is incorrect takes a lot more code, effort and time, hence indicating that economic realities are more to blame for such cases than bad intentions!</p>
<p>It also presents an opportunity to analyze the capabilities of a <em>good</em> interface, as personal data would make it very easy to notice its limitations, which couldn't be observed through seemingly complete interfaces exposed by many e-commerce companies. </p>
<p>Inspired by above stated ideas, My try has been to expose multiple (if not all) attributes for a resource directly to user and then letting user recursively refine query to get to desired result. Implementation is still far from complete, but this theme has served me well to set a basic roadmap for the project. Other themes such as self-hosting, hostile behaviour towards users in terms of privacy-invading features, limited or no options to refine a search by google, github etc has contributed to evolution of this experiment. Distributed queries being served by a cluster of (refurbished) smart-phones or single-board-computers remains a lofty goal of this experiment too! </p>
<p>Despite all the good intentions and ideas, any search interface should pass that threshold of being fast enough to not end up as another impractical experiment. Efforts were involved from the beginning to embrace the inevitable complexity such projects come to include despite many refactorings. 
Below is a minimal video to help visualize the current state and capabilities of the project.</p>
<video id="video-player" width="100%" controls="" preload="metadata" poster="https://eagledot.xyz/assets/hachi_demo.png">
        <source id="video-source" src="https://eagledot.xyz/assets/hachi_demo.mp4" type="video/mp4">
        Your browser does not support the video tag.
</video>

<h2>Broader Ideas:</h2>
<ul>
<li>
<p>Minimalism:
Minimalism in terms of number of external dependencies required for this project to be bootstraped, could explain a lot about downstream choices and evolution of the project to its current form. This has  of any existing (source) code if possible or writing it from scratch which itself would require reading of a lot of existing code before i could port it to extend the project in a pure source sense.
If it would be practical to reuse some code from existing capable projects/databases, i would have done so but most of such projects are designed to be de-coupled from application code for good reasons, as they are supposed to offer much more guarantees and stay robust even under heavy load.
Being an (embedded) part of personal application we can choose to do away with such guarantees and yet expose much more information by tightly integrating ML models pipeline. In the end, application would handle much more complex indexing and inferencing pipelines, which would require a lot more code apart from search and storage interface generally expose!</p>
</li>
<li>
<p>Experimentation:
Thinking more about in terms of augmenting the existing information, rather than to duplicate it, while fusing traditional (deterministic) attributes with semantic(ML) attributes. I think this is an interesting problem and which have not been fully utilized/explored for personal applications. Most of traditional databases were written to only handle "text" modality, but current ML models allow us to query semantic information too, which opens up a new space to experiment in.
I treat semantic information as necessary and independent, but not the only signal useful to implement great search interfaces.</p>
</li>
<li>
<p>Hackability:
For this project i wanted it be very easy for someone to start modifying it according to their needs, and this mostly co-relates with the first point about minimalism, lesser the number of dependencies, lesser is the amount of configuration required to bootstrap the developing environment. Both Python and Nim are stable, cross-platform languages and are easier to extend just using a C compiler. Nim source code it easy to compile and/or cross-compile to on almost all platforms. There are already python bridges for many languages, so all such languages are fair game to extend the codebase in any desired way!<br>
Python environments (in)famously have the reputation of being difficult to bootstrap, whole parallel ecosystem is there to do so which itself creates another dependency. But i think project has made great progress in this regard, with now having a requirement of just 3 dependencies as <code>numpy</code> , <code>regex</code> and <code>markupsafe</code> and optionally <code>requests</code>, with no hard-dependence on versioning. Almost all python environments could be used to run the project with no changes, which also removes any need to bootstrap dev environment using Docker like huge dependency or any complex unwarranted build-systems plaguing many of the interesting projects. If i had money, i would pay someone to just make such projects easier to install and start with, by removing any redundant configuration or making it possible to use one common build-system !</p>
</li>
</ul>
<p>Even though above ideas may seem worthy to follow on, there is always an on-going fight to prevent dilution of agreed upon principals. Counter-intuitively i think there is some kind of <code>activation-enery</code> (<a href="https://en.wikipedia.org/wiki/Activation_energy">https://en.wikipedia.org/wiki/Activation_energy</a>) requirement for each project, past that it actually is much easier to extend, modify, optimize the codebase somewhat like paying a debt to live debt free:)     </p>
<p>There are already very capable projects like <code>Sqlite</code>, <code>Lucene</code> offering full-text search capabilities, but they implement their own storage backends which require all data to be transformed to the compatible format which leads to duplication of data . This is something i wanted to avoid, as we would be continuously transforming every newer data and this would become computationally expensive when such data wouldn't even reside on same physical machine/node. If we could get away with fast-enough queries through a much simpler index/database, that seems like something worthy to pursue further.<br> 
Most of such projects were created to handle only text queries, But current ML models expose semantic information through "vectors" or "embeddings", generated after a series of linear and non-linear operations on some text or/and an image. <code>Top-k</code> matching results are later retrived through a "Comparison" procedure with user query (embedding) as one of inputs. 
Such extensions are being gradually added in many older engines, so a hackable codebase like this project may offer more flexibilities while accomodating future ideas in this rapidly evolving field!  </p>
<p>It leads to a design comprising a meta-data indexing engine, coupled with vector-search engines for semantic search. We never intend to duplicate the original-data and don't care where it actually resides, once indexing is done. As i think search is more about reaching to a desired file/resource before that resource could be used! Pin-pointing that resource location quickly is the major motivation by incorporating the user intentions and context recursively!</p>
<p>(C)Python is used as the major language for backend and Nim (and C) is used to speed up the bottleneck portions of the codebase where-ever warranted. 
Writing from scratch allows me to update the api as i fit to handle a bottleneck portion of the pipeline (querying or indexing), without asking or waiting for a change in some upstream dependency.
Nim itself is a language with relatively smaller community, so i am getting a bit comfortable porting code from other languages to my projects with only standard library and even experimenting with my own data-structures based on (protected) reference semantics than default value semantics that Nim use!</p>
<h2>Meta-Index:</h2>
<div><p>Its a minimal module/database to handle (store and query) meta-data being extracted from resources(images) and has been written in Nim. Currently it is single-threaded, column-oriented database using Json as data-exchange mechanism between python and Nim. In future idea is to shift to leveraging multiple threads for workloads/size greater than a threshold, to better use the current hardware capabilities. It is possible to generate an <code>auxilary</code> index to speed up queries for a column/attribute on demand, which internally would use cache-friendly and hierichal data-structures to achieve so for most of scenarios! </p><p>
Through development of this module, it has been easier to note that why most of databases end-up with some kind of dedicated <code>query language</code>, as situations arise requiring composing multiple operations in one go which seems like a cleaner way to model such intentions. (and this also seems to validate the requirement of a  <code>query-planner</code> to better execute a query by analyzing the order and nature of operations and some internal details).
Since it would be written for <code>hachi</code> itself, it remains possible for me to  speed up a frequent operation by sharing a pointer/memory directly across Nim and python to prevent costly <code>copy</code> operations, or to directly serve <code>raw json</code> to the frontend in some cases without serializing and de-serializing at python boundary.</p></div>
<p>I have also experimented with multi-versioning storage design as <strong>Lmdb</strong>, to protect the original information created by code itself from user revisions. But current implementation instead favours creation of a dedicated field/attribute for user to modify/update.
For example  during face clustering process, backend will assign an unique Id for each new <code>cluster</code>, to which user may want to change to a more descriptive name, this leads to presence of attributes like <code>personML</code> and <code>person</code> in the final schema. By default, any attribute/information generated through during indexing pipeline is supposed to be immutable to be easily reset to genesis state.<br>
It still is a bit rigid implementation, as schema is locked once initialized (lazily or explicit), as adding new columns dynamically will require me to reallocate data in the memory and more syncing logic which i am off-putting for now and will work on in the future!
Current iteration supports <code>string</code>, <code>int32</code>, <code>float32</code>, <code>bool</code>, <code>array[string]</code> data-types, which seems to be enough for the application needs, but could be evolved in the future. I am not particularly content with current "string" querying, one reason is that Nim  by default does not have a concept of no-copy slice, and it is difficult to even expose such a user-defined type. As <code>strings</code> are null-terminated, so most of other composed data-structures with string as one of fields have that underlying assumption which that user-defined type will break. Also i think for a lot of meta-data attributes, i could use <code>ShortString</code>    kind of data-type to speed up scanning/querying by better leveraging the cache. Some of these issues are being experimented through an independent project and if found to improve performance could be implemented in this codebase too!    </p>
<p>There are also Simd opportunities inside the "querying" code, but since its design is being guided by overall needs for the product itself, i hope to add those architecture specific optimizations only after system-design becomes stable enough for most of the features supposed to be supported! </p>
<h2>Face-Recognition:</h2>
<p>Being able to group same person(s) with a high probability, as another attribute to search for or mix with other attributes, would be a very quality addition to any search interface. Current DL models for some-time now have been able to distinguish faces with a high accuracy. But being able to distinguish real-life faces still requires a conformance to the pipeline such models would have been trained with.<br>
There are multiple architectures for such models that have been proposed to tackle this problem, but most pipelines could be assumed to follow a generic flow, which begins with detection of facial bounding boxes from a given image or camera frame, then followed by detection of facial-landmarks for each such face, and ends with generation of  <code>embeddings/vectors</code> which figuratively would represent some kind of latent representation of that face.  At this point, this would be reduced to a Vector Spaces problem and hence much easier to deal with traditional tools like nearest neighbour search !<br></p>
<p>It almost always overwhelming to decide on a particular Implementation to build upon, while accommodating various factors like <code>latency</code>, <code>accuracy</code> , <code>hardware requirements</code>, and most of such intensive pro-bono work would never even be visible to the end-user. For me atleast this goes much further, as i would be implementing each such model using an independent ML framework, which would require me to understand also all the pre-processing and post-processing code, to be  faithfully ported to Nim.<br>
Spending time on reading papers and existing implementations helps me to get an idea about overall "capability" of the model and potential requirements during fine-tuning of the model in future. Sometimes it has been enough for me to come across an interesting concept through a paper or some nice optimization trick, even if i end up not using that particular implementation. <br>
<label for="face-rec-0">‚äï</label>

<span>
Most of face embeddings generation models are trained on a <code>Siamese-loss</code> like objective to try to explicitly distinguish both positive-positive and positive-negative pairs. This generally involves manually collecting such pairs and hence prone to bias ! Such features predictors are also very sensitive to <code>face-alignment</code> code used, and hence may require you to faithfully follow the training code!
</span>
Dataset being used for training and choice of the objective function are two very major factors influencing the performance of any model. Leakage of evaluation data into training set has been a real issue in recent years for many experiments. Face-recognition itself is a <em>very</em> subjective problem and generally require more "visual" testing apart from (mathematical) metrics proposed for this problem/sub-domain.  </p>
<p>Current pipeline uses <code>retina-face</code> model to predict faces and landmarks in one go which helps producing stable facial-landmarks and speeding up the pipeline. (As predicting facial-landmarks would be much cheaper from internal features than through a dedicated model, and it also helps stabilizing  the training of the model).
Though it could make sense to argue about a model's ability to internalize learning <em>correlated</em> features without adding an explicit loss, but in practice it is always (very) beneficial to use multiple losses explicitly.
Interestingly, <code>residual</code> connection in <code>ResNets</code> was an important innovation making it possible to train much deeper networks at that time, even though it would be just mimicing an <code>identity</code> function.
<label for="residual-0">‚äï</label>

<span>
  <img src="https://eagledot.xyz/assets/residual_0.png" alt="Residual component">
  Residual block, see <a href="https://en.wikipedia.org/wiki/Residual_neural_network">https://en.wikipedia.org/wiki/Residual_neural_network</a>
</span>
Explicit multiple losses decrease the chances of over-fitting by large. There could be other auxiliary objectives that are used during training only by means of an smaller auxiliary network and then not used/required during inference, just like training wheels :)</p>
<p>In my experience, dataset being used for training and choice of the objective function are two very major factors influencing the performance of your model on real-life (bit out-of-distribution datasets). I find it a good practice to always visually debug some of the random samples to get a "feel" for the dataset!</p>
<p>Even after having a good pipeline to generate "embeddings" for a face, <code>clustering</code> remains a very challenging problem, due to various reasons.
Like with almost all clustering algorithms, we start out with no prior information about of the underlying (number) distribution of the data (faces). (as this is what we would be trying to estimate). As we keep encountering the newer information, possible <code>updates</code> through <code>back-filling</code> are required for the underlying index, which somewhat resembles of an auto-regressive operation and hence the error-accumulation rate is relatively high. We would also need to wait for some "initial" amount of data/information to be collected, to estimate initial stable centroids. This difficulty is further compounded by the choices for various thresholds like face-detection, some measure for blurness in the detected face, and a dependence on order of information being encountered.</p>
<p><label for="face-models-info">‚äï</label>

<span>
As indicated, choosing same model to predict landmarks and face-bounding boxes, helps reduce the impedance mismatch that occurs when output of one model is being fed through another model. We would need to a dedicate model for facial-features though as earlier features may not be dense enough to distinguish among individual faces!
</span></p>
<p>Currently Implementation works by collecting some minimal amount of information before <code>Cluster</code> creation process could begin. 
Each Cluster is a union of a set of main/master embeddings and a set of follower/slave embeddings. Selection of main embeddings is a crucial part to maintain the stability of a cluster even when new information would be encountered. Initial filtering of unfeasible (master) embeddings is done through some static criterias, for example we strive to filter any of <code>blurred</code> faces, face-profile is estimated through facial-landmarks, stable forward-facing profiles make face-alignment easier further in the pipeline. Such (static) criterias definitely help to reduce the number of invalid candidates, but may not be enough for many real-life datasets. A further minimal module comparing the <code>hog-features</code> with a set of pre-saved hog-features is introduced to help invalidate faces with <code>sunglasses</code> and some false positives not caught by earlier criterias!</p>
<p><label for="hog-compare">‚äï</label>

<span>
  <img src="https://eagledot.xyz/assets/hog_compare.png" height="120" alt="hog comparison">
  Hog features are finally compared at pixel level, after applying normalization!
</span>
After experimenting with other approaches like SIFT-features, i found it easier to compare hog-features generated from <em>aligned</em> faces/eyes. Alignment part of the pipeline is crucial to generate rich embeddings, even minor deviation from reference landmarks end up producing bad-embeddings rendering the pipeline useless. All feasible candidates/embeddings are then compared sequentially to create final clusters conditioned on some threshold. Note for now this is not exhaustive and hence order in which information is being encountered would have some effect on final clusters! Remaining follower ids are also then assigned (sequentially) to one of the clusters or to a special cluster like <code>no-categorical-info</code>, when not able to being fit into any of the clusters.
Note that a lot of empirical data comes into effect as multiple decisions would be required while choosing many thresholds and may require multiple runs .</p>
<p><img alt="alt ML codebase sample" src="https://eagledot.xyz/assets/face_rec_1.png"></p>
<p>Since face-recognition is very subjective and i myself have to compare other features to make sure that indeed the <em>correct</em> person(s) have been grouped together by the pipeline. But with a latency of around 25 ms, it seems to do very good on a held out dataset of persons with close up faces, (Zen-Z) selfies and sunglasses occluded eyes. Personal photos are much easier to classify/recognize compared to such a dataset!  </p>
<p>For any practical ML integrated product, We would need to have a very performant concurrent pipeline to keep feeding the model while being constantly aware of any data-distribution impedance mismatch, to reach anywhere near the 'accuracy' and <code>speed</code> promised in a research paper. This touches upon the issue of having good understanding of software engineering basics, while being aware of possible regressions resulting from a newer functionality like ML.<br>
Though bigger VLM/LLM (base) models have potential to handle data-impedance mismatch issues due to their sheer size, their usage would definitely hamper the application responsiveness and have proven to be relatively rigid to be fine-tuned for a specific domain! </p>
<h2>Indexing:</h2>
<p>Indexing pipeline begins with desired data location as its input to recursively scanning <code>directories</code> to collect <code>raw-data</code> in batches.
Multiple meta attributes such as <code>exif-data</code>, <code>size</code>, <code>mount location</code>, <code>name</code> are extracted to be later queried through the Meta-indexing engine. Focus has been on designing a good schema to accomodate future use-cases, but since we would be collecting only meta-information without ever modifying the original or duplicating the original data, it remains relatively easier to shift to a newer version/schema even through automatic means.<br>
ML models extract semantic information which can be later queried through a vector-indexing engine. By default resources to be indexed are assumed to be residing on a local-disk but any protocol could be leveraged, if proper authorization and authentication could be provided.<br>
 Monolithic nature of the code helps me to share <code>raw-data</code> read/collected once for various components like <code>hash generation</code>, <code>preprocessing</code> code for ML models, reducing the number of costly I/O calls. This pipeline has come a long way from a blocking implementation to its current (almost) fully async nature, resulting in very high saturation of computing resources. Apart from running multiple threads, dedicated kernels/functions are used to speed up pipeline by <code>fusion</code> of operations wherever possible.
 One such example/scenario  has been shown below.</p>
<div><pre><span></span><code><span>def</span> <span>preprocess_kernel</span><span>(</span>
    <span>image</span><span>:</span><span>Tensor</span><span>[</span><span>uint8</span><span>],</span>
    <span>new_shape</span><span>:</span><span>tuple</span><span>[</span><span>int</span><span>,</span><span>int</span><span>],</span> 
    <span>rgb_to_bgr</span><span>:</span><span>bool</span> <span>=</span> <span>True</span><span>,</span> 
    <span>normalize</span><span>:</span><span>bool</span> <span>=</span> <span>True</span><span>):</span>
    <span># Preprocess kernel, may fuse resize, color_conversion and normalization into one function!</span>

    <span># Pseudo-code!</span>

    <span>result</span> <span>=</span> <span>newEmptyTensor</span><span>[</span><span>uint8</span><span>](</span><span>new_shape</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>new_height</span><span>:</span>
        <span>for</span> <span>j</span> <span>in</span> <span>new_width</span><span>:</span>
            <span>inp_h</span><span>,</span> <span>inp_w</span> <span>=</span> <span>get_corresponding_pixel</span><span>(</span><span>image</span><span>,</span> <span>i</span><span>,</span> <span>j</span><span>)</span>
            <span>for</span> <span>k</span> <span>in</span> <span>0.</span><span>.&lt;</span><span>3</span><span>:</span>
                <span>if</span> <span>rgb_to_bgr</span><span>:</span>
                    <span>result</span><span>[</span><span>i</span><span>,</span><span>j</span> <span>,</span> <span>3</span><span>-</span><span>k</span><span>-</span><span>1</span><span>]</span> <span>=</span> <span>image</span><span>[</span><span>inp_h</span><span>,</span> <span>inp_w</span><span>,</span> <span>k</span><span>]</span>
                    <span># normalize based on mean and deviation used for training dataset further...</span>
                <span>else</span><span>:</span>
                    <span>result</span><span>[</span> <span>i</span><span>,</span><span>j</span><span>,</span><span>k</span><span>]</span> <span>=</span> <span>image</span><span>[</span><span>inp_h</span><span>,</span> <span>inp_w</span><span>,</span> <span>k</span><span>]</span>
</code></pre></div>

<p>Each <code>resource</code> could be assumed to go through a flow like this:</p>
<div><pre><span></span><code><span>resource_location</span> <span>=</span> <span>"file://xyz.jpg"</span>
<span># OR</span>
<span>resource_location</span> <span>=</span> <span>"remoteProtocol://xyz.jpg"</span>

<span>raw_data</span> <span>=</span> <span>download_raw_data</span><span>(</span><span>resource_location</span><span>)</span>

<span>embeddings</span> <span>=</span> <span>ML_model</span><span>(</span> <span>preprocess</span><span>(</span><span>raw_data</span><span>))</span>
<span>exif_data</span> <span>=</span> <span>extract_exif_data</span><span>(</span><span>raw_data</span><span>)</span>
<span>preview</span> <span>=</span> <span>generate_preview</span><span>(</span><span>raw_data</span><span>)</span>
<span>write_preview</span><span>(</span><span>preview</span><span>)</span>
<span>....</span>
</code></pre></div>

<h2>Vector Index:</h2>
<p>It is another minimal module to store vector-embeddings as <strong>shards</strong> on the disk. Necessary meta-data is stashed along with that shard, to make it self-contained, which in future will help in distributed/parallel retrieval. For now each shard is just a numpy (float32) Tensor, and comparison routine is a <code>np.dot</code> operator, which itself use the <code>blas/openblas</code> library to speed up this operation! Each shard is loaded from the Disk during a <em>query</em>, and <code>top-k</code> candidates are collected to be fused together with other deterministic meta-attributes. Loading from Disk do add some latency, but it allows me to regulate RAM usage through <code>shard-size</code> hyper-parameter, to allow running this on different platforms  with diverse specifications including single-board computers. <code>Shard-size</code> could be kept relatively high for higher RAM systems to speed up shard querying.</p>
<p><code>Matmul</code> is one of the most optimized algorithms which run at almost 90% of theoretical capacity on most of intel/amd Cpus when leveraging <code>Blas</code> like libraries. So every further optimization from here-on would involve some kind of information loss. There is a whole literature now to speed up this comparison/retrieval process through <code>quantization</code> and/or <code>nearest neighbour</code> indices like HNSW. Fast SSDs are also leveraged to run such comparisons at very high speed for upto billion vectors on just a single node in near real time!</p>
<p>But such all techniques involve compression of information (which itself is best-effort being the result of modeling a large amount of biased data) through out-of-band mechanisms, for example creating centroids/clusters is just based on the vector values and taking some mean without a way to pass back the information to the model which produced those vectors in the first place. This way is quick and you would get great speed-ups, and there is an active debate among vector-database vendors across various metrics and implementations. In my experience only visual results on a <em>personal</em> data would be a good metric a user should test for. Product-quantization is something i would be implementing if were to choose one, as i think coupled with <code>top-k</code>, it should work reasonably well to include (subjectively) correct results (high recall!) .</p>
<p>Another worthy and very effective solution i think is to instead <em>train</em> a linear layer to <em>finetune</em> the original model depending upon the task. ML Features/embeddings from a big enough model, could assumed to have a <em>knowledge</em> about diverse topics, but for example, a user may be trying to distinguish between different plants. A linear layer could easily be trained with just few thousand samples, to achieve so with much higher accuracy than original models, and even with half the size/dimension of original embeddings. Intuitively it could be thought that we freed the information channels to just focus on plants, decreasing the entropy model earlier had to deal with. Any such layer could be trained even without any framework, as it would just be one <code>backward</code> operation to implement.
OpenAI has a nice cookbook if a reader would want to explore this further!
<label for="embed-playbook">‚äï</label>

<span>
  <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb</a></span></p>

<p>An interesting thing sharding allows is to use any available <code>hardware</code> to speed up retrieval. Since we need just <code>comparison</code> routine and corresponding shard(s) to return top-k candidates, it de-couples it from any of application code. A new smartphone could be detected, and some <code>shards</code> could be transferred during initial set-up, optimal percentage/number of shards could be easily calculated by running same <code>comparsion</code> operation on new device. Like running a <code>2048 x 2048</code> , inner-product op and comparing latency with <code>master/main</code> device, would tell us the capacity of the new device and so that number of shards would be transferred to speed up retrieval process!</p>
<p>There are performance gains to be have in the current implementation, would like to atleast start using <code>float16</code> data-type, but its a bit tricky on intel cpus with no compiler support for this type. Printing of CPU capabilities <em>do</em> show the presence of float16 hardware support on my system ! 
ARM(v8 64) seems to offer native float16/floatH types, there seems to be difference in that type either supported natively by compiler or as an intrinsics/assembly code. I have not been able to get expected speed up for now! Such code is still being experimented upon in the limited time i have.</p>
<h2>Backend:</h2>
<p>Backend is written in python, which exposes a pure API server, to let the client/frontend to make API calls to. Starting with very naive code to just return all the meta-data for a <code>directory</code> to current pagination support it have gone through many revisions and design iterations and now i have much clearer idea about how to architect/wrap a big enough piece of functionality. I wanted the app to be end to end, but this also put extra pressure on app to be responsive enough for all user events. Current indexing code is capable of providing rich details such as directory currently being scanned, estimated time (eta) and allows robust <em>Cancellation</em> of an ongoing task/threads. 
It has not been easy to model such communication b/w concurrent tasks and touches upon much discussed <code>structured-concurrency</code> debate i.e how to run multiple tasks asynchronously, while being able to robustly cancel them at any point in time, all while being able to collect all errors cleanly!<br></p>
<p>From C days, i have been a user of (Posix) <code>threads</code> type implementations, since major OSes provide those minimal but stable APIs, it helps me during context switching to different languages. Both C and Nim expose that, Python itself let the OS manage threads without its own runtime implementation, but bypassing the GIL when makes sense is something user have to do to fully utilize the resources! Also this kind of code requires user to handle a lot of code as to communicate b/w threads but atleast i (think) understand the basic ideas to prevent deadlocking if occurs and iron out initial bugs. As you run such threads deeper and deeper inside application stack , it keeps getting harder to communicate information back to the client. But when it starts working, it is really cool to have a central interface to see all the stuff backend is doing and predict very good ETA !</p>
<p><code>Flask</code> was initially used to easily map <code>functions</code> to a particular route/url to wrap up initial implementation, current implementation now just uses <code>werkzeug</code> (main engine behind flask) directly, hence doing away with a lot of unrequired dependencies like a template engines that Flask ships with. Even though this would not effect the end user in any visible way, this has been a very nice quality-of-life improvement like stuff for me as a developer. Since werkzeug is pure python, it can now be shipped/bundled directly as source code. Also each request is now handled by an available thread (from a pool) by reading <code>http environment</code> from a shared queue following conventional model. By default for multi-threaded option, werkzeug would create a new fresh thread for handling that request. This does away with lots of OS/system calls for each new request and latency now seems more consistent and predictive. I have also  stumbled upon a pattern to actually make it easier to <code>mount</code> multiple <code>apps</code>  cleanly given i never liked and even understood the <code>blueprint</code> that flask offers to make it easier to distribute the logic of your app to other modules too.
Since WSGI protocol just expect a callable python object, it should be much easier to develop independent <em>apps</em> without having any knowledge where it would be called/used. It also makes it quite fun to actually write/expose python code to handle client inputs. </p>
<div><pre><span></span><code><span>class</span> <span>SimpleApp</span><span>():</span>
    <span>"""Each instance could be used a WSGI compatible callable"""</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>allow_local_cors</span><span>:</span><span>bool</span> <span>=</span> <span>False</span><span>):</span>
        <span>self</span><span>.</span><span>initialized</span> <span>=</span> <span>False</span>
        <span>self</span><span>.</span><span>http_methods</span> <span>=</span> <span>[</span><span>"GET"</span><span>,</span> <span>"POST"</span><span>,</span> <span>"PUT"</span><span>,</span> <span>"DELETE"</span><span>,</span> <span>"OPTIONS"</span><span>]</span> 
        <span>self</span><span>.</span><span>url_map</span> <span>=</span> <span>None</span> <span># we will lazily initialize it!</span>
        <span>self</span><span>.</span><span>extension_prefix</span> <span>=</span> <span>"ext"</span> <span># as apps would be registered/</span>
        <span>self</span><span>.</span><span>registered_extensions</span><span>:</span><span>dict</span><span>[</span><span>str</span><span>,</span> <span>SimpleApp</span><span>]</span> <span>=</span> <span>{}</span>

        <span>....</span>

    <span>def</span> <span>add_url_rule</span><span>(</span><span>self</span>
                     <span>rule</span><span>:</span><span>str</span><span>,</span> 
                     <span>view_function</span><span>:</span><span>Callable</span><span>,</span> <span># corresponding view.</span>
                     <span>endpoint</span><span>:</span><span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span> <span># set to view_function</span>
                     <span>methods</span><span>:</span><span>list</span><span>[</span><span>str</span><span>]</span><span>=</span> <span>[</span><span>"GET"</span><span>]):</span>

        <span>...</span> <span># some validation code.</span>

        <span>self</span><span>.</span><span>endpoint_2_uri</span><span>[</span><span>endpoint</span><span>]</span> <span>=</span> <span>(</span><span>Rule</span>
            <span>(</span><span>rule</span><span>,</span> <span>endpoint</span> <span>=</span> <span>endpoint</span><span>),</span> <span>methods</span>
            <span>)</span>
        <span>self</span><span>.</span><span>endpoint_2_viewFunction</span><span>[</span><span>endpoint</span><span>]</span>  <span>=</span> <span>view_function</span>
        <span>self</span><span>.</span><span>initialized</span> <span>=</span> <span>False</span>

    <span>def</span> <span>register</span><span>(</span><span>self</span><span>,</span> <span>app</span><span>:</span><span>SimpleApp</span><span>,</span> <span>name</span><span>:</span><span>str</span><span>):</span>
        <span>"""</span>
<span>        Here we register another such `app`.</span>
<span>        It would be mounted at `/ext/&lt;name&gt;` , so all requests to /ext/&lt;name&gt;/&lt;route&gt;, would be forwarded to this `app` .</span>
<span>        """</span>

        <span>...</span> <span># some validation code.</span>
        <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>name</span><span>]</span> <span>=</span> <span>app</span>
        <span>print</span><span>(</span><span>"Extension registered at: </span><span>{}</span><span>/</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>self</span><span>.</span><span>extension_prefix</span><span>,</span> <span>name</span><span>))</span>


    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>environ</span><span>,</span> <span>start_response</span><span>)</span> <span>-&gt;</span> <span>Iterable</span><span>[</span><span>bytes</span><span>]:</span>
        <span># This is called </span>
        <span>if</span> <span>not</span> <span>(</span><span>self</span><span>.</span><span>initialized</span><span>):</span>
            <span>print</span><span>(</span><span>"[Initializing]: Parent"</span><span>)</span>
            <span>self</span><span>.</span><span>initialize</span><span>()</span>

        <span>for</span> <span>ext</span> <span>in</span> <span>self</span><span>.</span><span>registered_extensions</span><span>:</span>
            <span>if</span> <span>not</span> <span>(</span><span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>ext</span><span>]</span><span>.</span><span>initialized</span><span>):</span>
                <span>print</span><span>(</span><span>"[Initializing]: </span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>ext</span><span>))</span>
                <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>ext</span><span>]</span><span>.</span><span>initialize</span><span>()</span>

        <span># If a call to such an extension.. we modify the environment a bit.</span>
        <span>active_app</span> <span>=</span> <span>self</span>
        <span>extension_name</span> <span>=</span> <span>None</span>
        <span>temp_path</span> <span>=</span> <span>environ</span><span>[</span><span>'PATH_INFO'</span><span>]</span>
        <span>temp_split</span> <span>=</span> <span>temp_path</span><span>.</span><span>split</span><span>(</span><span>"/"</span><span>)</span>
        <span>if</span> <span>temp_split</span><span>[</span><span>1</span><span>]</span> <span>==</span> <span>self</span><span>.</span><span>extension_prefix</span><span>:</span>

            <span>extension_name</span> <span>=</span> <span>temp_split</span><span>[</span><span>2</span><span>]</span>
            <span>assert</span> <span>extension_name</span> <span>in</span> <span>self</span><span>.</span><span>registered_extensions</span><span>,</span> 
            <span>extension_path</span> <span>=</span> <span>temp_path</span><span>.</span><span>replace</span><span>(</span><span>"/</span><span>{}</span><span>/</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>self</span><span>.</span><span>extension_prefix</span><span>,</span> <span>extension_name</span><span>),</span> <span>""</span><span>)</span>


            <span>environ</span><span>[</span><span>'PATH_INFO'</span><span>]</span> <span>=</span> <span>extension_path</span>
            <span>environ</span><span>[</span><span>'REQUEST_URI'</span><span>]</span> <span>=</span> <span>extension_path</span>
            <span>environ</span><span>[</span><span>'RAW_URI'</span><span>]</span> <span>=</span> <span>extension_path</span>

            <span>active_app</span> <span>=</span> <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>extension_name</span><span>]</span>

    <span>## -----------------------------------------------</span>
    <span># NOTE: only werkzeug specific code is here!</span>
    <span># ---------------------------------------------</span>
    <span>request</span> <span>=</span> <span>Request</span><span>(</span><span>environ</span> <span>=</span> <span>environ</span><span>)</span> <span># minimal wrapping code!</span>
    <span>urls</span> <span>=</span> <span>active_app</span><span>.</span><span>url_map</span><span>.</span><span>bind_to_environ</span><span>(</span><span>environ</span><span>)</span>
    <span>endpoint</span><span>,</span> <span>args</span> <span>=</span> <span>urls</span><span>.</span><span>match</span><span>()</span>

    <span># view function can choose to return iterable[bytes] are the result of view function or call , or further wrap it to be as expected by werkzeug!</span>
    <span>iterable_bytes</span> <span>=</span> <span>active_app</span><span>.</span><span>endpoint_2_viewFunction</span><span>[</span><span>endpoint</span><span>](</span><span>request</span><span>,</span> <span>**</span><span>args</span><span>)</span> 
    <span>return</span> <span>iterable_bytes</span>  <span># as WSGI protocol expects!</span>
    <span># ---------------------------------------------------------</span>
</code></pre></div>

<p>Note that, any existing Python object, can be made to accept <code>client</code> requests on demand by adding very minimal code and could be done for selective functionality. For example, during setup of a new android device, i may have to ask user to choose one of the existing <code>devices</code>, this kind of interactive input can be modeled easily now, as i just add a new routine in the Corresponding class to accept requests on a route such as <code>/ext/android/beginSetup</code>, once i get that, all the existing logic already written could be used to finish setup. It is as easy as <code>parent_app.register(app = thisApp, name = "android")</code> to start routing corresponding requests to this app!</p>
<h2>ML:</h2>
<p>Machine learning is being powered by a framework written completely in Nim, most of work was done on that framework before i even stared working on this project. This has allowed me to wrap CLIP and Face-Recognition Pipeline along with the application while only depending on OneDNN for some routines. OneDNN (mkldnn) (<a href="https://github.com/uxlfoundation/oneDNN">https://github.com/uxlfoundation/oneDNN</a>) is one of the libraries to speed up various Deep learning operations with great documentation. </p>
<p>Ported models run faster  on intel/Amd Cpus than pytorch counterparts, owing to fusion of operations like Batch Normalization and Convolution, and high re-use of pre-allocated memory (similar to in-place operations). Current <code>torch.compile</code> like engine would end up making some of those optimizations after analyzing the graph, but for at-least 2.0 version it is not supported on Windows for me to compare against!</p>
<p>It took a lot of effort during one-two years i was working on it to be complete enough for me to start porting Deep-learning models using it. Also OneDNN shifted to V3 during that time, and only some code was updated to newer API and this has left the project in a unstable state with no visible stable APIs for users to work with. For each model i have to manually analyze the locations/requirements for fusion of operations, port quite a lot of pre-processing and post-processing code to make it end to end. These reasons contributed to a lot of technical debt, which i have not found the resources to tackle yet. Without removing that debt it never made sense to open-source it, besides there are now projects like GGML, and tiny-grad to serve inference only needs with minimal resources! </p>
<p><img alt="alt ML codebase sample" src="https://eagledot.xyz/assets/ml_1.png"></p>
<p><img alt="alt ML codebase sample" src="https://eagledot.xyz/assets/ml_2.png"></p>
<p>Porting of each model is quite an involved task, as you have to read enough papers to understand ideas about model if want to later fine-tune that model too. You may want to find first find or create a simpler implementation in pytorch to make it easier to port to a new language. All experimentation could be done in pytorch/python, for example i experimented with alternate quantized attention layers for CLIP model, and it indeed had a better performance for eval datasets mentioned in CLIP paper. Tangentially it was really cool to read through Open-AI implementations and papers, papers were written in an approachable manner to let the read indulge in hypothesis, codebases were clean with minimal dependencies. Its really a shame what that company/organisation chose to become under the guise of "user-safety" effectively clipping the (open) ethos of this field, but at same time i am grateful for all the researchers' work in this current DL/ML era and seeing the evolution of this field in such an open manner!   </p>
<p>I would like to work on the project though atleast enough to tackle that debt and open-source it in state for users to extend upon, if found useful.
Even though i am using OneDNN for some routines, i think it is better to have a common and easier to extend codebase to allow more experimentation and aggressive optimizations , but this itself is a huge-task and now with multiple GPU architectures its just something that couldn't be tackled without a lot of time and money. Even in this age where H100 is the baseline for benchmarks in testing, i find it worthwhile to work on a minimal DL Compiler to just tackle ARM/Intel/Risc Cpus to start taking advantage of these cheaper machines. Being able to pin-point a tennis ball in a 3D space remains the dream !</p>
<h2>Frontend / App:</h2>
<p>Current front-end is completely written in Html, Js(Ts) and (tailwind) css as multi page webapp. Earlier frontend was written in Svelte, but lack of internal documentation and too much "magic" became too "viral" for me to handle. For me, abstractions and APIs exposed by Browsers are more than enough to maintain required precision during DOM updates. Care is taken to use batch updates, prevent redundant rendering, judicial usage of resources to prevent unrequired pressure through pagination, even for a local backend server. It has passed our litmus test for search over 180 Gb of indexed Pexels dataset on a (minimal) remote server. My friend <a href="https://github.com/akshaymalik1995">Akshay</a> helped a lot in frontend development, testing various datasets and offering detailed bug reports which helped uncover a lot of edge cases during development of the project. There would always be room for improvements on the UX/UI side, but we have found it is much easier to extend and improve frontend with a stable backend! </p>
<p><label for="pexel-link">‚äï</label>

<span>
  Pexels dataset: <a href="https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf">https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf</a></span></p>

<p>Apart from webapp, there is also a Windows App, which under the hood uses the <code>webview</code> to render the frontend. All native Windows APIs remain available to use from the Nim code, which puts it into a hybrid category. It is not ideal, but atleast it doesn't require me to ship a full web-browser, which i think is waste of compute resources, but at the same time leaves me wondering how current GUI development became so resource intensive for a single developer to manage while offering little benefits! I have been looking into forks of earlier GTK versions for linux to keep the complexity/learning contained, but that also seems nothing less than an adventure!  </p>
<h2>Tools/libraries:</h2>
<ul>
<li>
<p>Nimpy (<a href="https://github.com/yglukhov/nimpy">https://github.com/yglukhov/nimpy</a>) : A minimal python-Nim bridge to make it easier to write extensions in Nim to be called from python and to use python modules in Nim. Unlike many such bridges which includes a lot of boiler-plate code, there are no complex classes/interfaces to be included in the extension. It targets necessary features like marshaling of native python types to and from Nim, targets the minimal Python API to not depend on python versions, finding underlying python.dll at runtime.</p>
</li>
<li>
<p>Stb Image (<a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a>): A big fan of such single header libraries, this one implements encoders for most of image formats in pure C. Its very easy to modify it pass pointer to the raw-data and writing raw-data to a pre-allocated memory saving costly memory copying particularly visible for 4k photos! It helps remove dependency on OpenCV for image reading ! Nim made it very easy to just compile this along with other Nim code.</p>
</li>
<li>
<p>LibWebp (<a href="https://github.com/webmproject/libwebp">https://github.com/webmproject/libwebp</a>): Allows decoding and encoding for webp formats,  Though documentation is a bit sparse on some internal API usage, lot of examples are included in the repository to read. I managed to use <code>argb</code> field directly to pass <code>argb</code> format data to do away with transformation logic and some (memory) allocations. It follows callback passing convention to implement custom behaviour like a progress bar and to write encoded data to a user provided buffer. Written completely in C and very easy to compile and read, it is being used for writing image previews, helping remove dependency on OpenCV.</p>
</li>
<li>
<p>Zig-cc (<a href="https://ziglang.org/">https://ziglang.org</a>): Using <code>zig/clang</code> as a C compiler, allowed me to easily cross-compile a lot of Nim code for Linux, targeting <code>2.27 libc</code>. Making it easier to set a LibC target has proved very useful to bypass that <code>libC</code> mismatching stuff!
Really cool work by Zig community to tackle a lot of such technical debt to make software development much easier !</p>
</li>
</ul>
<p>As mentioned earlier i try to use a lot of existing open-source code if i can, even it would be for reading/understanding purposes only. It still blows my mind even after many years, to just read/understand some complex implementation and modify it for personal use-case for <em>Free</em>.
For example even though <code>OpenCV</code> is a big/complex dependency, its still has a very readable codebase and i read code from it a few times during this project to understand differences b/w my port and OpenCV one.</p>
<p>Being able to integrate multiple languages has its own challenges too, as it would require us to understand boundaries, internal details, assumptions that each runtime would want developer to respect. It gets complex to reproduce and understand bugs while running multiple multi-threaded runtimes as  debugging gets more difficult. Debugging is one of things i would like to get better at, i have very limited knowledge of GDB as of now, which is expected to be table stakes for debugging in such environments. I have had some nasty bugs , but being able to compile all required pieces made it a bit easier to debug even with print-style debugging :)</p>
<h2>Current State:</h2>
<p>A lot of functionality is working, than not and having tested over 500k images i could be a bit satisfied about internals' performance and robustness. I would like to say that it can easily handle 10 millions of images/resources, and there is nothing to suggest that it won't, but it is different from using a production database to extrapolate the performance confidently. Despite writing from (almost) scratch in a number of languages, both indexing and inferencing pipeline are more expressive, robust and faster than many similar images search apps, but benchmarking for such complex apps could be subjective and more so when you mix in semantic search.</p>
<p>There are still some hardcoded constants and also intentionally some low performing components, like using ViT B/32 variant of CLIP model, which are acting as placeholders, and would be replaced easily with better counterparts in the future. </p>
<p>It has been tested on Windows 10/11 and on Fedora 42/43 with an assumption of <code>x64</code> architecture. Compiled extensions are also packaged to quickly test the application, but users are free to compile code as they see fit. Linux shared objects target <code>LibC 2.27</code>, so should work on most of recent distributions out of the box. Except some ML code there is main requirement of any/a C compiler to further extend the codebase by the user. Most of testing is done on my Laptop with  <code>i5-8300H</code> processor and 8 GB memory. I don't have a MacOS to test on, ML code would need to be modified to target ARM architecture, except that very minor modifications should be needed if any. It is quite possible for initial users to encounter minor bugs, due to its limited run in diverse dev environments, but installation and usage on Cloud servers during testing has been quite smooth.</p>
<p>Below is a video showcasing workflow to index data from multiple MTP/Android devices. (Still a WIP). </p>
<video id="video-player" width="100%" controls="" preload="auto" poster="https://eagledot.xyz/assets/extension_demo.png">
        <source id="video-source" src="https://eagledot.xyz/assets/extension_demo.mp4" type="video/mp4">
        Your browser does not support the video tag.
</video>

<h2>Random Thoughts/Philosophy:</h2>
<p>I think it gets easier with time to grok larger codebases to isolate/find the functionality/implementation reader would be interested in. Most of mature codebases are organized to help navigating the source-tree anyway, and have detailed documentation. Being able to have enough patience to make yourself comfortable is a necessary part of growing as a developer, as initial documentation/codebase would always seem alien and big enough to trigger that flight reaction! </p>
<p>Batching and Caching are two generic strategies that could be applied to speed up most of bottleneck portions. Both strategies lead to better/denser utilization of CPUs by (trying to) minimise the costly load/store instructions during a hot loop. Batching for example could do it by allocating necessary memory up-front for a batch and de-allocating all at once when no longer required, reducing the number of costly system-calls. Caching may involve designing or using a (hardware)cache friendly data-structure, when it is possible to do so.</p>
<p>Each optimization would involve assumptions and each subsequent optimization would become harder and harder to implement, may preventing the clean refactoring of code when future functionalities may need to be accommodated. It itself is a kind of rabbit-hole, and user should know when to stop as there would always be something <em>else</em> to be optimized! </p>
<p>With (coding) tools involving AI/LLMs it is easier than ever to get a piece of desired functionality, as a developer i understand it is another useful tool in a long-history of improvements, that most of developers would come to use in their workflow. Current LLMs have undeniable ability to handle complex instructions, explain non-trivial code and that so for various mixed modalities! It has been a bit <em>unreasonable</em> to end up with such abilities with just next token prediction as primary objective, even for a researcher working in this field.
My usage for such tools is only through a (free) search engine(s), Although for now there has been <em>no</em> scenario in such tools have helped me, that i wouldn't have got to using traditional means. But i can admit such tools/engines are really effective in helping us to get unstuck in a variety of situations, arguably helping us to <em>learn</em> faster. <code>Functions/routines</code> are nice and enough abstractions to provide enough context to such engines, to get the required <em>help</em>, without ever needing <code>review/edit/rewrite</code> cycle.<br>
I have <em>always</em> been benefited from visiting the original documentation, if AI is spitting out good enough arguments, there must be a good documentation out there for that topic . Our minds capability to extract abstract patterns resulted from <em>studying</em> one topic and applying it to another seemingly unrelated domain is uncanny to say the least. Also tone/motivation for developer writing about a topic matters to me, and many times i have visited a concept further just because writer himself/herself was very excited about it . Again, these are just personal factors and biases and people should be free to choose workflow they feel most comfortable in , without any judgments from either side.<br>
<label for="llm-note">‚äï</label>

<span> 
It has been difficult to access SOTA models actual abilities, with fewer and fewer details being published for each newer version, but it has been a wild-ride for me to see the evolution from RNNs to bi-directional RNNs to LSTMs to Transformer architecture (finally founding atleast one stable architecture be able to support training on whole internet without exploding or vanishing gradients).
Arguably there are also more <em>more</em> open family of models like <code>Qwen</code> or <code>Deepseek</code> from other labs which could run on <em>local</em> infrastructure. Even at this stage, ideas behind LLMs are simple enough for anybody to understand without burdening them with terms like AGI . There is already great work from <a href="https://allenai.org/olmo">OLMO</a> and <a href="https://github.com/huggingface/smollm">Smollm</a>  to build upon and start with, for personal needs, without spending a lot of money. On technical front  there is still much more to explore and it comes down to doing more experiments by smaller companies to prevent ending up with another monopoly/duopoly in this field only to later blame such for their incompetence!<br>
</span>
I literally have no idea what would be the end game with this ever increasing ability of AI models and what social consequences we would end up with in an already fragmented and struggling world.
But it would be a mistake to abandon <em>learning</em>, however inconvenient it may seem at any time, if we were to survive !  <br>
Thing that really boils my blood is these (AI) companies <strong>lawless</strong> laundering of <em>all</em> the open-source code, art, poetry without any attribution only to be packaged as a product for users to pay for. Constant attacks on all the infrastructure even run by very small or single-developer companies/communities, not respecting any of the <code>robots.txt</code>, proxying through residential networks, damaging the very core of the information-sharing/internet while coming up with ironical headlines is bordering on criminal-behaviour for me! Waiting for tens of seconds just for a (community written) stack-overflow post through many layers of <em>security</em>, for wanting to understand various perspectives for some concept without all the bullshit summarization, is new bleak reality with nothing for end-users to have a say in.  </p>
<p>Despite the dominant usage of LLMs there exist equally interesting smaller models/architectures representing the huge potential that this field of deep-learning holds. Neural-networks allow us to (good enough)model any arbitrary function/flow using an iterative framework from a few thousand samples representing the function space, effectively equipping us with a very power statistical tool 
<label for="ssl-0">‚äï</label>

<span>
Self-supervised learning don't even need explicit outputs, how cool is that..
See <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/">https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/</a> this work for more information.
</span>
to introduce a new independent signal to reduce the entropy of the problem in many domains. I am a fan of smaller personalized models' potential to tackle everyday problems, and myself uses cheap off-the-self cameras coupled with a DL model to detect those Damn Monkeys, and for local voice-synthesis.
<label for="monkey-trivia">‚äï</label>

<span>
  Monkey Capturing was even on the manifesto of one of the candidates at city-level elections!
</span>
In country like India, where even (traditional) Automation is limited to products of very few big companies, I can't help smiling whenever i point remote at my "AI" controlled AC :) </p>
<p>Living in a two-tier town in northern India with very minimal fixed-costs has allowed me to work on this for quite a long time without any savings or continuous financial freedom. But i cannot be a hypocrite about it, as it was a conscious decision to learn, explore and implement some of the ideas i had for some time. In return, this has allowed me to stay in touch with friends, played a lot of outdoor games, and help me in reflecting on the things i would want to spend more time in future.</p>
<p>Timely financial grants during the last one and half year from <a href="https://samagata.org/">Samagata foundation</a> and <a href="https://fossunited.org/">FossUnited</a> has allowed me to complete a bulk of work to point, where i am satisfied with the current state of the project, for which i will always be grateful.</p>
<p>I would very much like to continue on this or adjacent projects, as there are still a lot of ideas and code pending, to make it a very stable everyday engine for users to use . But for that i will have to figure out a way to sustain this , without ever compromising the Core features/functionality in any way, As those were some of reasons i started working on it in the first place! Extensions to allow indexing remote storage like Google Drive or Android devices smoothly from the app itself seems like a good direction in that regard for now!</p>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types (145 pts)]]></title>
            <link>https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</link>
            <guid>46087022</guid>
            <pubDate>Sat, 29 Nov 2025 12:25:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/">https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</a>, See on <a href="https://news.ycombinator.com/item?id=46087022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>   <p>Back around 2014, I kept hearing about this cool database called <a href="https://riak.com/">Riak</a>, a distributed database that could survive network partitions and keep accepting writes. Some really interesting companies were using it at massive scale, and I was curious about it. One of the big selling points was that it could handle concurrent writes without any coordination or consensus. I was intrigued, and I started reading about it. Underlying all of this was the concept of CRDTs, Conflict-free Replicated Data Types.<sup><a href="#user-content-fn-crdt-origin" id="user-content-fnref-crdt-origin" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<p>At the time, I was working on a beer startup called Brewtown with a friend: a beer review social site and delivery subscription service. It failed for other reasons, but I was a little too enamored with shiny tech back then, and CRDTs and Riak fit the bill for shiny tech. I kept trying to find excuses to shoehorn CRDT stuff into our codebase when, honestly, we didn‚Äôt need any of it. Postgres would‚Äôve been fine. Live and learn.</p>
<p>Anyways, the idea sounded like pure sorcery: data structures that replicate across nodes and merge deterministically, without coordination, without losing information. I got excited, read a few papers, played with some toy implementations‚Ä¶ and then we gave up on the beer startup. I didn‚Äôt really have a reason to mess with CRDTs for a while.</p>
<p>Fast forward to 2025, I‚Äôve just had Thanksgiving dinner, and I‚Äôm curious again. What‚Äôs the state of the art? What have I forgotten? Which CRDT should I reach for when? So I‚Äôm writing this, both as a refresher for myself and a reference for the next time I need to remember why OR-Sets exist or what WOOT stands for. (‚ÄúWithOut Operational Transformation.‚Äù Yes, really.<sup><a href="#user-content-fn-woot-paper" id="user-content-fnref-woot-paper" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>)</p>
<p>So, grab a coffee.</p>
<p>Commutative. Replicated. Data Types.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/g92-vC9iTn8?si=egtFeVe21g8SVrOm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p>In isolation, all of the words make sense. But when you look at the literature, it‚Äôs overwhelming:</p>
<p>Suddenly you‚Äôve moved beyond the simple terms and start seeing things like G-Counters, PN-Counters, LWW-Sets, OR-Sets, 2P-Sets, RGAs, WOOTs, Logoots (wtf?)‚Ä¶ Each with subtle tradeoffs. Each paper assuming you‚Äôve read the previous five. It‚Äôs overwhelming.</p>
<p>This guide will hopefully cut through that. We‚Äôll build intuition through <strong>interactive demos</strong> and concrete examples. You‚Äôll see how merges actually work, watch conflicts resolve (or not resolve), and develop a feel for which CRDT fits which problem.</p>
<h2 id="what-you-need-to-know">What You Need to Know</h2>
<p>You don‚Äôt need a PhD in distributed systems. If you understand:</p>
<ul>
<li>Why network failures happen</li>
<li>What ‚Äúeventual consistency‚Äù means</li>
<li>Basic set operations (union, intersection)</li>
</ul>
<p>‚Ä¶you‚Äôre good.</p>
<h2 id="the-problem">The Problem</h2>
<p>Picture this: Alice and Bob are both editing a shared counter. Alice increments it. Bob increments it. The network is flaky, so neither sees the other‚Äôs change immediately. Later, they reconnect. What should the counter show?</p>
<p><strong>Option 1: Consensus</strong> - Use Paxos/Raft to agree on who went first. Works great! Until the network partitions and half your users can‚Äôt write because they can‚Äôt reach a quorum. Not ideal for offline-first apps.</p>
<p><strong>Option 2: Last-Write-Wins</strong> - Use timestamps. Whoever wrote last ‚Äúwins.‚Äù Easy to implement! Except Bob‚Äôs increment gets completely erased if Alice‚Äôs timestamp was later. Data loss.</p>
<p><strong>Option 3: CRDTs</strong> - Design the data structure so that merging is deterministic. Both increments survive. No coordination needed. No data loss. However, you have to be okay with some level of eventual consistency.</p>
<p>What‚Äôs the trick? How do CRDTs achieve this?</p>
<p>Roughly speaking, you are working with a CRDT if your merge operation is:</p>
<ul>
<li>commutative (order doesn‚Äôt matter)</li>
<li>associative (grouping doesn‚Äôt matter)</li>
<li>idempotent (duplicates don‚Äôt matter)</li>
</ul>
<p>Once you achieve these properties, then you can use your merge operation to ensure that replicas automatically converge to the same state.</p>
<h3 id="a-quick-detour-lattices-and-why-they-matter">A Quick Detour: Lattices and Why They Matter</h3>
<p>Before we dive into specific CRDTs, let‚Äôs build some intuition about what makes merging work. In CRDT literature, this is often referred to as a ‚Äúlattice‚Äù.</p>
<p>Think about natural numbers with <code>max</code> as the merge operation. If you have <code>3</code> and <code>5</code>, taking <code>max(3, 5) = 5</code> makes sense. It doesn‚Äôt matter if you compute <code>max(3, max(5, 7))</code> or <code>max(max(3, 5), 7)</code> - you get <code>7</code> either way. And <code>max(5, 5) = 5</code>, so duplicates are harmless.</p>
<p>This forms a <strong>partial order</strong>: some values are ‚Äúgreater than‚Äù others (<code>5 &gt; 3</code>), and there‚Äôs a <strong>join</strong> operation (<code>max</code>) that gives you the least upper bound. The fancy math term is ‚Äújoin-semilattice,‚Äù but think of it as: <strong>a way to consistently pick ‚Äúmore recent‚Äù or ‚Äúmore complete‚Äù information</strong>.</p>
<p>Here‚Äôs the key insight: if your data structure‚Äôs states form a lattice, and updates only move ‚Äúupward‚Äù in the ordering, then:</p>
<ul>
<li>You can apply updates in any order</li>
<li>You can apply the same update twice</li>
<li>Eventually, everyone agrees on the maximum state</li>
</ul>
<p>Consider a counter where each replica tracks its own count: <code>{A: 3, B: 5}</code>. The partial order is <strong>pointwise</strong>: <code>{A: 3, B: 5} ‚â• {A: 2, B: 5}</code> because each component is greater-or-equal. To join, take the <code>max</code> of each component. This is exactly how the G-Counter CRDT works!</p>
<p><strong>Why does this matter?</strong> Because if you can design your data structure so that:</p>
<ol>
<li>States form a lattice (there‚Äôs always a sensible ‚Äújoin‚Äù)</li>
<li>Operations only move upward (you can‚Äôt un-increment a counter)</li>
</ol>
<p>Then merging becomes trivial: just take the join. No coordination needed. No conflicts possible. The math guarantees convergence.</p>
<p>Not all CRDTs fit this clean model (some need timestamps or version vectors to determine what‚Äôs ‚Äúgreater‚Äù), but the lattice intuition often guides the design. When you see <code>merge = unionWith max</code> or <code>merge = union</code>, you‚Äôre seeing some pure, beautiful math-brained lattice thinking.</p>
<h3 id="state-based-vs-operation-based">State-Based vs Operation-Based</h3>
<p>Moving on‚Ä¶</p>
<p>There are two fundamental approaches to CRDTs:</p>
<p><strong>State-based CRDTs (CvRDTs)</strong> send the entire state to other replicas, which merge it with their local state using a join operation. The state must form a join-semilattice.<sup><a href="#user-content-fn-cvcrdt" id="user-content-fnref-cvcrdt" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></p>
<p><strong>Operation-based CRDTs (CmRDTs)</strong> send operations to other replicas, which apply them to their local state. Operations must be commutative when applied concurrently.<sup><a href="#user-content-fn-cmcrdt" id="user-content-fnref-cmcrdt" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<p>In this guide, we‚Äôll primarily discuss state-based CRDTs, as they‚Äôre conceptually simpler and the ideas translate naturally to the operation-based variants.</p>
<h3 id="the-core-laws">The Core Laws</h3>
<p>For a data structure to be a state-based CRDT, its merge operation must satisfy:</p>
<p><strong>Associativity</strong>: <code>(a ‚äî b) ‚äî c = a ‚äî (b ‚äî c)</code> where <code>‚äî</code> denotes the merge/join operation</p>
<p><strong>Commutativity</strong>: <code>a ‚äî b = b ‚äî a</code></p>
<p><strong>Idempotence</strong>: <code>a ‚äî a = a</code></p>
<p>These properties ensure that:</p>
<ul>
<li>Merging in any order produces the same result</li>
<li>Re-receiving the same state is harmless</li>
<li>Partial merges can be composed</li>
</ul>
<p>Additionally, the state must form a <strong>monotonic semilattice</strong>: updates only move ‚Äúupward‚Äù in the partial order, never downward. This ensures convergence: once all updates have been delivered, all replicas reach the same state.</p>
<p>For the curious, The symbol ‚äî is called (square cup) or square union. I have no idea why regular union symbol isn‚Äôt used. Pointy-headed researchers, I guess.</p>
<p>Anyways, it‚Äôs commonly used to denote:</p>
<ul>
<li>Disjoint union - union of sets treated as disjoint</li>
<li>Join operation in lattice theory - the least upper bound (supremum) of two elements</li>
<li>Merge operation in CRDTs - combining two states by taking their least upper bound</li>
</ul>
<p>With these foundations in place, let‚Äôs explore the CRDT zoo.</p>
<h2 id="g-counter-grow-only-counter">G-Counter: Grow-Only Counter</h2>
<p>Let‚Äôs start with the simplest CRDT: a counter that only goes up.<sup><a href="#user-content-fn-gcounter-origin" id="user-content-fnref-gcounter-origin" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup></p>
<h3 id="the-idea">The Idea</h3>
<p>Instead of storing one global count, each replica tracks its own count. The total is the sum of all replica counts. When replicas merge, they take the <code>max</code> of each replica‚Äôs count.</p>
<p>Why <code>max</code>? Because counts only increase. If replica A shows that replica B has counted to 5, and replica B shows it‚Äôs counted to 3, we know A has seen newer information. Taking the max ensures we never lose increments.<sup><a href="#user-content-fn-gcounter-space" id="user-content-fnref-gcounter-space" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup></p>
<h3 id="implementation">Implementation</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> GCounter</span><span> =</span><span> Map</span><span> ReplicaId</span><span> Nat</span></span>
<span></span>
<span><span>value</span><span> ::</span><span> GCounter</span><span> -&gt;</span><span> Nat</span></span>
<span><span>value counter </span><span>=</span><span> sum (Map.elems counter)</span></span>
<span></span>
<span><span>increment</span><span> ::</span><span> ReplicaId</span><span> -&gt;</span><span> GCounter</span><span> -&gt;</span><span> GCounter</span></span>
<span><span>increment r counter </span><span>=</span><span> Map.insertWith (</span><span>+</span><span>) r </span><span>1</span><span> counter</span></span>
<span></span>
<span><span>merge</span><span> ::</span><span> GCounter</span><span> -&gt;</span><span> GCounter</span><span> -&gt;</span><span> GCounter</span></span>
<span><span>merge </span><span>=</span><span> Map.unionWith max</span></span></code></pre>
<h3 id="laws-and-invariants">Laws and Invariants</h3>
<p>The merge operation forms a join-semilattice where the partial order is defined pointwise: <code>c1 ‚â§ c2</code> if for all replicas <code>r</code>, <code>c1[r] ‚â§ c2[r]</code>.</p>
<ul>
<li><strong>Associative</strong>: <code>max</code> is associative</li>
<li><strong>Commutative</strong>: <code>max</code> is commutative</li>
<li><strong>Idempotent</strong>: <code>max(x, x) = x</code></li>
<li><strong>Monotonic</strong>: Each replica‚Äôs count only increases</li>
</ul>
<h3 id="intuition">Intuition</h3>
<p>Think of each replica as having its own tally marks. When replicas sync, they each adopt the maximum tally for each replica they‚Äôve seen. Since tallies only grow, taking the maximum ensures we never lose increments.</p>
<h3 id="tradeoffs">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and efficient</li>
<li>No metadata overhead beyond replica counts</li>
<li>Perfect for increment-only scenarios (page views, likes, etc.)</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot decrement</li>
<li>Size grows with number of replicas (though typically small)</li>
<li>No garbage collection (all replica counts retained forever)</li>
</ul>
<h3 id="when-to-use">When to Use</h3>
<p>Use G-Counter when you need to count upward-only events in a distributed system: analytics counters, like counts, view counts, or any monotonically increasing metric. (If you need to decrement, well‚Ä¶ keep reading.)</p>
<h3 id="interactive-demo">Interactive Demo</h3>
<p>Try it yourself! Increment counters on different replicas and see how the merge operation works:</p>
<astro-island uid="Z1h8Xr8" prefix="r0" component-url="/_astro/GCounterDemo.BSvnoCcc.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;GCounterDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="pn-counter-positive-negative-counter">PN-Counter: Positive-Negative Counter</h2>
<p>What if we need to decrement? Enter the PN-Counter. The trick is beautifully simple.</p>
<h3 id="definition">Definition</h3>
<p>A PN-Counter contains two G-Counters: one for increments, one for decrements:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> PNCounter</span><span> =</span><span> PNCounter</span></span>
<span><span>  { increments </span><span>::</span><span> GCounter</span></span>
<span><span>  , decrements </span><span>::</span><span> GCounter</span></span>
<span><span>  }</span></span></code></pre>
<p>The value is the difference:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>value :: PNCounter -&gt; Int</span></span>
<span><span>value (PNCounter inc dec) = value inc - value dec</span></span></code></pre>
<p>What I love about PN-Counters as a broader insight for CRDTs is that you can often build more complex CRDTs by combining simpler ones.</p>
<h3 id="operations">Operations</h3>
<p><strong>Increment</strong> (on replica <code>r</code>):</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>increment r (PNCounter inc dec) = PNCounter (increment r inc) dec</span></span></code></pre>
<p><strong>Decrement</strong> (on replica <code>r</code>):</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>decrement r (PNCounter inc dec) = PNCounter inc (increment r dec)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>merge (PNCounter i1 d1) (PNCounter i2 d2) =</span></span>
<span><span>  PNCounter (merge i1 i2) (merge d1 d2)</span></span></code></pre>
<h3 id="laws-and-invariants-1">Laws and Invariants</h3>
<p>Since both components are G-Counters with valid merge operations, the PN-Counter‚Äôs merge inherits their properties and forms a semilattice.</p>
<h3 id="intuition-1">Intuition</h3>
<p>A PN-Counter is like having two separate tally sheets: one for additions, one for subtractions. The current value is the difference between them. When replicas sync, they merge both sheets independently.</p>
<h3 id="tradeoffs-1">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports both increment and decrement</li>
<li>Deterministic convergence</li>
<li>Simple to understand and implement</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Double the space of a G-Counter</li>
<li>Can never truly garbage collect old replica entries</li>
<li>No bound on the value range (can overflow)</li>
<li>Cannot reset the counter atomically</li>
</ul>
<h3 id="when-to-use-1">When to Use</h3>
<p>Use PN-Counter for any metric that can increase or decrease over time: inventory counts, resource pools, etc.</p>
<h3 id="variants">Variants</h3>
<p>Some implementations use a single map with integer values instead of two separate maps, but the principle is the same.</p>
<h2 id="g-set-grow-only-set">G-Set: Grow-Only Set</h2>
<p>Moving from numbers to collections, we consider the simplest CRDT set.</p>
<h3 id="definition-1">Definition</h3>
<p>A G-Set is simply a set that supports addition but not removal:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> GSet</span><span> a </span><span>=</span><span> Set</span><span> a</span></span></code></pre>
<h3 id="operations-1">Operations</h3>
<p><strong>Add</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a</span></span>
<span><span>add </span><span>=</span><span> insert</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> GSet</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a</span></span>
<span><span>merge </span><span>=</span><span> union</span></span></code></pre>
<h3 id="laws-and-invariants-2">Laws and Invariants</h3>
<p>Sets with union form a semilattice under the subset relation.</p>
<ul>
<li><strong>Associative</strong>: Set union is associative</li>
<li><strong>Commutative</strong>: Set union is commutative</li>
<li><strong>Idempotent</strong>: <code>A ‚à™ A = A</code></li>
<li><strong>Monotonic</strong>: Sets only grow</li>
</ul>
<h3 id="intuition-2">Intuition</h3>
<p>Once an element is added to any replica, it will eventually appear in all replicas. There‚Äôs no way to remove it.</p>
<h3 id="tradeoffs-2">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Minimal overhead (just the set elements)</li>
<li>Simple and efficient</li>
<li>Familiar set semantics</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot remove elements</li>
<li>Grows unbounded</li>
<li>No garbage collection</li>
</ul>
<h3 id="when-to-use-2">When to Use</h3>
<p>Use G-Set for append-only collections where removal is never needed: event logs, collected tags, or immutable registries.</p>
<h2 id="2p-set-two-phase-set">2P-Set: Two-Phase Set</h2>
<p>The natural extension of G-Set to support removal.</p>
<h3 id="definition-2">Definition</h3>
<p>A 2P-Set (Two-Phase Set) contains two G-Sets: one for added elements, one for removed elements:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> TwoPhaseSet</span><span> a </span><span>=</span><span> TwoPhaseSet</span></span>
<span><span>  { added </span><span>::</span><span> GSet</span><span> a</span></span>
<span><span>  , removed </span><span>::</span><span> GSet</span><span> a</span></span>
<span><span>  }</span></span></code></pre>
<p>An element is in the set if it‚Äôs been added but not removed:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>member</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> TwoPhaseSet</span><span> a </span><span>-&gt;</span><span> Bool</span></span>
<span><span>member x (TwoPhaseSet a r) </span><span>=</span><span> x </span><span>`Set.member`</span><span> a </span><span>&amp;&amp;</span><span> x </span><span>`Set.notMember`</span><span> r</span></span></code></pre>
<h3 id="operations-2">Operations</h3>
<p><strong>Add</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add x (TwoPhaseSet a r) </span><span>=</span><span> TwoPhaseSet (insert x a) r</span></span></code></pre>
<p><strong>Remove</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>remove x (TwoPhaseSet a r) </span><span>=</span><span> TwoPhaseSet a (insert x r)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge (TwoPhaseSet a1 r1) (TwoPhaseSet a2 r2) </span><span>=</span></span>
<span><span>  TwoPhaseSet (union a1 a2) (union r1 r2)</span></span></code></pre>
<h3 id="laws-and-invariants-3">Laws and Invariants</h3>
<p><strong>Bias toward removal</strong>: If an element appears in the removed set, it‚Äôs not in the 2P-Set, even if it‚Äôs also in the added set.</p>
<p><strong>Once removed, forever removed</strong>: Once an element is removed at any replica, it will eventually be removed from all replicas and cannot be re-added.</p>
<h3 id="intuition-3">Intuition</h3>
<p>The 2P-Set is like marking items in a ledger: you can add entries and you can cross them out, but you can‚Äôt un-cross-out an entry. Once something is crossed out (removed), that decision is permanent.</p>
<h3 id="tradeoffs-3">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports both add and remove</li>
<li>Simple to understand</li>
<li>Deterministic convergence</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot re-add removed elements (the ‚Äú2P‚Äù means two-phase: add, then remove, no going back)</li>
<li>Both sets grow monotonically (removed items never truly disappear)</li>
<li>No garbage collection</li>
<li>Not suitable for scenarios where elements might be removed and re-added</li>
</ul>
<h3 id="when-to-use-3">When to Use</h3>
<p>Use 2P-Set when elements have a lifecycle of ‚Äúnot present ‚Üí added ‚Üí removed‚Äù and never need to be re-added: task completion tracking, tombstones, or revoked permissions.</p>
<h2 id="lww-element-set-last-write-wins-element-set">LWW-Element-Set: Last-Write-Wins Element Set</h2>
<p>What if we want to re-add elements? We need timestamps.</p>
<h3 id="definition-3">Definition</h3>
<p>An LWW-Element-Set associates each element with a timestamp for additions and removals:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> LWWSet</span><span> a </span><span>=</span><span> LWWSet</span></span>
<span><span>  { addTimes </span><span>::</span><span> Map</span><span> a </span><span>Timestamp</span></span>
<span><span>  , removeTimes </span><span>::</span><span> Map</span><span> a </span><span>Timestamp</span></span>
<span><span>  }</span></span></code></pre>
<p>An element is in the set if its most recent operation was an add:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>member</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> LWWSet</span><span> a </span><span>-&gt;</span><span> Bool</span></span>
<span><span>member x (LWWSet adds removes) </span><span>=</span></span>
<span><span>  case</span><span> (Map.lookup x adds</span><span>,</span><span> Map.lookup x removes) </span><span>of</span></span>
<span><span>    (Just t1</span><span>,</span><span> Just t2) </span><span>-&gt;</span><span> t1 </span><span>&gt;</span><span> t2</span></span>
<span><span>    (Just _</span><span>,</span><span> Nothing) </span><span>-&gt;</span><span> True</span></span>
<span><span>    _ </span><span>-&gt;</span><span> False</span></span></code></pre>
<h3 id="operations-3">Operations</h3>
<p><strong>Add</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add x t (LWWSet adds removes) </span><span>=</span><span> LWWSet (insert x t adds) removes</span></span></code></pre>
<p><strong>Remove</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>remove x t (LWWSet adds removes) </span><span>=</span><span> LWWSet adds (insert x t removes)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge (LWWSet a1 r1) (LWWSet a2 r2) </span><span>=</span></span>
<span><span>  LWWSet (unionWith max a1 a2) (unionWith max r1 r2)</span></span></code></pre>
<h3 id="laws-and-invariants-4">Laws and Invariants</h3>
<p>The merge operation is well-defined because <code>max</code> over timestamps forms a semilattice.</p>
<p><strong>Timestamp monotonicity</strong>: Each replica must generate increasing timestamps (typically using wall clocks plus replica IDs as tiebreakers).</p>
<p><strong>Bias</strong>: We must decide what happens when add and remove timestamps are equal. Common choices: bias toward add, or bias toward remove.</p>
<h3 id="intuition-4">Intuition</h3>
<p>Each element has a timestamp for when it was last added and when it was last removed. The most recent operation wins. When merging, we take the latest add timestamp and latest remove timestamp we‚Äôve seen.</p>
<h3 id="tradeoffs-4">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports add, remove, and re-add</li>
<li>Can garbage collect old timestamps (carefully)</li>
<li>Natural semantics for many applications</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Requires synchronized clocks (or logical clocks with careful replica ID handling)</li>
<li>Concurrent add/remove on the same element may surprise users (one operation is discarded)</li>
<li>Loses information: if two users concurrently add the same element, only one timestamp survives</li>
<li>The ‚Äúlast write wins‚Äù semantics mean data loss is possible</li>
</ul>
<h3 id="when-to-use-4">When to Use</h3>
<p>Use LWW-Element-Set when you need a set with add/remove/re-add capability and can tolerate last-write-wins semantics: user preferences, feature flags, or cached collections where perfect consistency isn‚Äôt critical.</p>
<h3 id="clock-considerations">Clock Considerations</h3>
<p>The biggest pitfall of LWW-Element-Set is clock skew. If replica A‚Äôs clock is ahead of replica B‚Äôs, then A‚Äôs operations will always ‚Äúwin‚Äù over B‚Äôs, even if B‚Äôs operations happened later in real time. Solutions include:</p>
<ul>
<li>Use hybrid logical clocks (HLC) instead of wall clocks</li>
<li>Use replica IDs as tiebreakers (e.g., timestamps are <code>(wall_time, replica_id)</code> pairs)</li>
<li>Accept the inconsistency as a tradeoff</li>
</ul>
<h2 id="or-set-observed-remove-set">OR-Set: Observed-Remove Set</h2>
<p>The most sophisticated set CRDT, solving the re-add problem without LWW semantics.<sup><a href="#user-content-fn-orset-origin" id="user-content-fnref-orset-origin" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup></p>
<h3 id="definition-4">Definition</h3>
<p>An OR-Set (Observed-Remove Set) associates each element with a set of unique tags:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ORSet</span><span> a </span><span>=</span><span> Map</span><span> a (</span><span>Set</span><span> Tag</span><span>)</span></span></code></pre>
<p>Tags are unique identifiers generated when adding an element (e.g., <code>(replica_id, sequence_number)</code> pairs).</p>
<p>An element is in the set if it has any tags:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>member</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> ORSet</span><span> a </span><span>-&gt;</span><span> Bool</span></span>
<span><span>member x set </span><span>=</span><span> case</span><span> Map.lookup x set </span><span>of</span></span>
<span><span>  Just tags </span><span>-&gt;</span><span> not (Set.null tags)</span></span>
<span><span>  Nothing </span><span>-&gt;</span><span> False</span></span></code></pre>
<h3 id="operations-4">Operations</h3>
<p><strong>Add</strong> (with fresh tag <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add x t set </span><span>=</span><span> Map.insertWith union x (singleton t) set</span></span></code></pre>
<p><strong>Remove</strong> (with observed tags <code>ts</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>remove x ts set </span><span>=</span><span> Map.update (</span><span>\\</span><span>tags </span><span>-&gt;</span></span>
<span><span>  let</span><span> remaining </span><span>=</span><span> tags </span><span>\\</span><span> ts</span></span>
<span><span>  in</span><span> if</span><span> Set.null remaining </span><span>then</span><span> Nothing </span><span>else</span><span> Just remaining) x set</span></span></code></pre>
<p>The critical insight: removal removes only the tags that were observed. If concurrent adds create new tags, those survive.</p>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge </span><span>=</span><span> Map.unionWith union</span></span></code></pre>
<h3 id="laws-and-invariants-5">Laws and Invariants</h3>
<p>The merge operation forms a semilattice where <code>s1 ‚â§ s2</code> if for all elements <code>x</code>, <code>s1[x] ‚äÜ s2[x]</code>.</p>
<p><strong>Add wins</strong>: If an add and remove happen concurrently (the add‚Äôs tag wasn‚Äôt observed by the remove), the add wins.</p>
<p><strong>Causal consistency</strong>: You can only remove tags you‚Äôve observed (seen in a prior state).</p>
<h3 id="intuition-5">Intuition</h3>
<p>Think of each addition as dropping a unique token into a bucket for that element. Removal takes specific tokens out of the bucket. If someone concurrently added a new token you haven‚Äôt seen, your removal doesn‚Äôt affect it. An element is present if its bucket has any tokens.</p>
<p>This gives us <strong>add-wins semantics</strong>: concurrent add and remove means the element stays in the set (because the remove didn‚Äôt observe the add‚Äôs tag).</p>
<h3 id="tradeoffs-5">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports add, remove, and re-add with intuitive semantics</li>
<li>No timestamp requirements</li>
<li>Add-wins semantics are often more desirable than LWW</li>
<li>Properly handles concurrent operations</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Larger space overhead (tags per element)</li>
<li>More complex implementation</li>
<li>Need garbage collection strategy for tags</li>
<li>Remove operations need to carry the observed tags (larger messages)</li>
</ul>
<h3 id="when-to-use-5">When to Use</h3>
<p>Use OR-Set when you need a set with full add/remove/re-add support and can‚Äôt tolerate LWW‚Äôs data loss: collaborative editing, shopping carts, or any scenario where concurrent adds should be preserved.</p>
<h3 id="garbage-collection">Garbage Collection</h3>
<p>Old tags can accumulate. Strategies include:</p>
<ul>
<li><strong>Tombstones</strong>: Keep removed tags for a grace period before discarding</li>
<li><strong>Version vectors</strong>: Use causal history to determine which tags are safe to remove</li>
<li><strong>Bounded tags</strong>: Limit the number of tags per element, using LWW within that bound</li>
</ul>
<h3 id="interactive-demo-1">Interactive Demo</h3>
<p>Experience the add-wins semantics of OR-Set:</p>
<astro-island uid="1UCY2P" prefix="r1" component-url="/_astro/ORSetDemo.DVgELLPP.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;ORSetDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="lww-register-last-write-wins-register">LWW-Register: Last-Write-Wins Register</h2>
<p>Registers store single values. The simplest register CRDT uses last-write-wins.</p>
<h3 id="definition-5">Definition</h3>
<p>An LWW-Register pairs a value with a timestamp:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> LWWRegister</span><span> a </span><span>=</span><span> LWWRegister</span></span>
<span><span>  { value </span><span>::</span><span> a</span></span>
<span><span>  , timestamp </span><span>::</span><span> Timestamp</span></span>
<span><span>  }</span></span></code></pre>
<h3 id="operations-5">Operations</h3>
<p><strong>Write</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>write x t _ </span><span>=</span><span> LWWRegister x t</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge r1</span><span>@</span><span>(LWWRegister v1 t1) r2</span><span>@</span><span>(LWWRegister v2 t2)</span></span>
<span><span>  |</span><span> t1 </span><span>&gt;</span><span> t2 </span><span>=</span><span> r1</span></span>
<span><span>  |</span><span> t1 </span><span>&lt;</span><span> t2 </span><span>=</span><span> r2</span></span>
<span><span>  |</span><span> otherwise </span><span>=</span><span> r1  </span><span>-- tiebreaker (could use replica ID)</span></span></code></pre>
<h3 id="laws-and-invariants-6">Laws and Invariants</h3>
<p>The merge operation is a semilattice with partial order defined by timestamps.</p>
<p><strong>One value wins</strong>: When concurrent writes occur, only one survives (the one with the higher timestamp).</p>
<h3 id="tradeoffs-6">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and efficient</li>
<li>Small size (just value + timestamp)</li>
<li>Easy to understand</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Loses concurrent updates</li>
<li>Requires clock synchronization</li>
<li>No way to detect or recover lost updates</li>
</ul>
<h3 id="when-to-use-6">When to Use</h3>
<p>Use LWW-Register for single-value cells where you can tolerate lost updates: user profile fields, configuration settings, or cached computed values.</p>
<h3 id="interactive-demo-2">Interactive Demo</h3>
<p>See data loss in action with last-write-wins semantics:</p>
<astro-island uid="2g6pnS" prefix="r2" component-url="/_astro/LWWRegisterDemo.xg1zuJ1y.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;LWWRegisterDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="mv-register-multi-value-register">MV-Register: Multi-Value Register</h2>
<p>What if we want to preserve concurrent writes instead of discarding them?</p>
<h3 id="definition-6">Definition</h3>
<p>An MV-Register stores a set of value-timestamp pairs:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> MVRegister</span><span> a </span><span>=</span><span> Set</span><span> (a</span><span>,</span><span> Timestamp</span><span>)</span></span></code></pre>
<p>When reading, you get back all concurrently written values (values with incomparable timestamps).</p>
<h3 id="operations-6">Operations</h3>
<p><strong>Write</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>write x t reg </span><span>=</span><span> Set.singleton (x</span><span>,</span><span> t)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge reg1 reg2 </span><span>=</span></span>
<span><span>  let</span><span> combined </span><span>=</span><span> union reg1 reg2</span></span>
<span><span>      maxTime </span><span>=</span><span> maximum (map snd combined)</span></span>
<span><span>      concurrent </span><span>=</span><span> filter (</span><span>\\</span><span>(_</span><span>,</span><span> t) </span><span>-&gt;</span><span> t </span><span>==</span><span> maxTime) combined</span></span>
<span><span>  in</span><span> fromList concurrent</span></span></code></pre>
<p>More sophisticated: keep values with causally concurrent timestamps, not just the maximum.</p>
<h3 id="laws-and-invariants-7">Laws and Invariants</h3>
<p>The merge preserves all values that might be ‚Äúcurrent‚Äù from different replicas‚Äô perspectives.</p>
<p><strong>Concurrent values preserved</strong>: If two writes happened concurrently, both values appear until a subsequent write supersedes them.</p>
<h3 id="tradeoffs-7">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No data loss on concurrent updates</li>
<li>Application can detect and resolve conflicts</li>
<li>More information available for conflict resolution</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Returns sets of values, not single values</li>
<li>Application must handle conflict resolution</li>
<li>More complex semantics</li>
<li>Slightly larger space overhead</li>
</ul>
<h3 id="when-to-use-7">When to Use</h3>
<p>Use MV-Register when concurrent updates must be detected and resolved by application logic: collaborative text fields, conflict-aware configuration, or any scenario where losing an update is unacceptable.</p>
<h3 id="conflict-resolution">Conflict Resolution</h3>
<p>When reading an MV-Register returns multiple values, the application must resolve the conflict. Strategies include:</p>
<ul>
<li>Present all values to the user (collaborative editing)</li>
<li>Apply a deterministic merge function (e.g., union of tags)</li>
<li>Use application-specific semantics (e.g., prefer non-empty values)</li>
</ul>
<h2 id="or-map-observed-remove-map">OR-Map: Observed-Remove Map</h2>
<p>Maps are common. How do we make them CRDTs?</p>
<h3 id="definition-7">Definition</h3>
<p>An OR-Map is a map where each key is associated with an OR-Set of tagged values:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ORMap</span><span> k v </span><span>=</span><span> Map</span><span> k (</span><span>ORSet</span><span> (v</span><span>,</span><span> Tag</span><span>))</span></span></code></pre>
<p>Alternatively, implement as a composition of OR-Set (for keys) with per-key CRDTs (for values).</p>
<h3 id="operations-7">Operations</h3>
<p><strong>Put</strong> (with fresh tag <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>put k v t map </span><span>=</span><span> Map.insertWith union k (singleton (v</span><span>,</span><span> t)) map</span></span></code></pre>
<p><strong>Remove key</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>removeKey k map </span><span>=</span><span> Map.delete k map</span></span></code></pre>
<p><strong>Remove specific value</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>removeValue k v tags map </span><span>=</span><span> -- similar to OR-Set remove</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge </span><span>=</span><span> Map.unionWith (OR</span><span>-</span><span>Set merge)</span></span></code></pre>
<h3 id="tradeoffs-8">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Full map operations with CRDT semantics</li>
<li>Can nest other CRDTs as values</li>
<li>Compositional</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Complex metadata management</li>
<li>Garbage collection challenges</li>
<li>Larger overhead</li>
</ul>
<h3 id="when-to-use-8">When to Use</h3>
<p>Use OR-Map when you need a distributed key-value store with CRDT guarantees: collaborative JSON documents, distributed configuration, or nested data structures.</p>
<h2 id="rga-replicated-growable-array">RGA: Replicated Growable Array</h2>
<p>Sequences are hard. How do you handle insertions in the middle when replicas disagree on positions?<sup><a href="#user-content-fn-sequence-crdt-challenge" id="user-content-fnref-sequence-crdt-challenge" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup></p>
<h3 id="definition-8">Definition</h3>
<p>RGA (Replicated Growable Array) assigns each element a unique ID and stores the sequence as a tree structure based on insertion order and causality.<sup><a href="#user-content-fn-rga-paper" id="user-content-fnref-rga-paper" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup></p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> RGA</span><span> a </span><span>=</span><span> RGA</span></span>
<span><span>  { elements </span><span>::</span><span> Map</span><span> UID</span><span> (a</span><span>,</span><span> UID</span><span>)  </span><span>-- element ID -&gt; (value, parent ID)</span></span>
<span><span>  , root </span><span>::</span><span> UID</span></span>
<span><span>  }</span></span></code></pre>
<p>Each element knows its ‚Äúparent‚Äù (the element after which it was inserted).</p>
<h3 id="operations-8">Operations</h3>
<p><strong>Insert</strong> (after element with ID <code>p</code>, with fresh ID <code>uid</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>insert p x uid rga </span><span>=</span><span> -- complex tree manipulation</span></span></code></pre>
<p><strong>Delete</strong> (element with ID <code>uid</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>delete uid rga </span><span>=</span><span> -- mark as tombstone, don't actually remove</span></span></code></pre>
<p><strong>Merge</strong>: Merge trees by reconciling insertion orders.</p>
<h3 id="laws-and-invariants-8">Laws and Invariants</h3>
<p>The challenge is that positional indices change as elements are inserted/removed. RGA solves this by using immutable IDs and causal relationships.</p>
<p><strong>Causal order preserved</strong>: If element A was inserted before element B on the same replica, that relationship is preserved globally.</p>
<h3 id="intuition-6">Intuition</h3>
<p>Instead of ‚Äúinsert at position 5,‚Äù you say ‚Äúinsert after element X.‚Äù Since X has a unique ID, this instruction is unambiguous even when other replicas are concurrently inserting elsewhere.</p>
<h3 id="tradeoffs-9">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports arbitrary insertions and deletions</li>
<li>Eventual consistency for sequences</li>
<li>Handles concurrent edits intuitively</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Complex implementation</li>
<li>Large overhead (IDs, tombstones)</li>
<li>No compaction without coordination</li>
<li>Performance degrades with many deletes (tombstones accumulate)</li>
</ul>
<h3 id="when-to-use-9">When to Use</h3>
<p>Use RGA for collaborative text editing or any replicated sequence where insertions at arbitrary positions must be supported: shared lists, collaborative documents, or distributed queues.</p>
<h3 id="alternatives">Alternatives</h3>
<p>Other sequence CRDTs include:</p>
<ul>
<li><strong>WOOT</strong> (Without Operational Transformation): similar idea, different structure</li>
<li><strong>Logoot</strong>: uses position identifiers between elements</li>
<li><strong>LSEQ</strong>: adaptive allocation of position identifiers</li>
<li><strong>YATA</strong>: optimizations for text editing workloads<sup><a href="#user-content-fn-yata-yjs" id="user-content-fnref-yata-yjs" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup></li>
</ul>
<p>Each has different tradeoffs in space overhead, time complexity, and behavior under specific edit patterns.</p>
<h2 id="causal-crdts-adding-causality">Causal CRDTs: Adding Causality</h2>
<p>Advanced CRDTs incorporate causal tracking using version vectors or similar mechanisms. This enables more sophisticated semantics.</p>
<h3 id="version-vectors">Version Vectors</h3>
<p>A version vector tracks the logical clock for each replica:<sup><a href="#user-content-fn-version-vectors" id="user-content-fnref-version-vectors" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup></p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> VersionVector</span><span> =</span><span> Map</span><span> ReplicaId</span><span> Nat</span></span></code></pre>
<p>Operations include the version vector, allowing replicas to determine causality: whether one operation happened-before another, or whether they were concurrent.</p>
<h3 id="causal-register">Causal Register</h3>
<p>Pairs an MV-Register with version vectors:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> CausalRegister</span><span> a </span><span>=</span><span> CausalRegister</span></span>
<span><span>  { values </span><span>::</span><span> Map</span><span> VersionVector</span><span> a</span></span>
<span><span>  }</span></span></code></pre>
<p>Only keeps values with concurrent version vectors, discarding those that are causally dominated.</p>
<h3 id="advantages-of-causality">Advantages of Causality</h3>
<ul>
<li>More precise conflict detection (concurrent vs. causally ordered)</li>
<li>Better garbage collection (can discard superseded operations)</li>
<li>Foundation for stronger consistency guarantees</li>
</ul>
<h3 id="disadvantages-of-causality">Disadvantages of Causality</h3>
<ul>
<li>Larger metadata (version vectors grow with number of replicas)</li>
<li>More complex logic</li>
<li>Still doesn‚Äôt eliminate conflicts, just detects them more precisely</li>
</ul>
<h3 id="interactive-demo-3">Interactive Demo</h3>
<p>Explore how version vectors track causality:</p>
<astro-island uid="Z1AQVOM" prefix="r3" component-url="/_astro/VectorClockDemo.CoZCB5dm.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;VectorClockDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="delta-crdts-efficient-state-transmission">Delta CRDTs: Efficient State Transmission</h2>
<p>State-based CRDTs have a problem: sending the entire state on every sync is wasteful. Delta CRDTs solve this.<sup><a href="#user-content-fn-delta-crdt-paper" id="user-content-fnref-delta-crdt-paper" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup></p>
<h3 id="the-problem-1">The Problem</h3>
<p>Consider a G-Counter with 1000 replicas. If replica A increments its count, must it send all 1000 entries to replica B? That‚Äôs inefficient: only one entry changed!</p>
<h3 id="the-solution">The Solution</h3>
<p>Instead of sending full state, send only the <strong>delta</strong>: the part of the state that changed since the last sync.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> Delta</span><span> a </span><span>=</span><span> a  </span><span>-- same type as state, but represents only changes</span></span>
<span></span>
<span><span>merge</span><span> ::</span><span> CRDT</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> Delta</span><span> a </span><span>-&gt;</span><span> a</span></span></code></pre>
<p>For G-Counter, a delta might be just <code>{A: 1}</code> instead of the full map.</p>
<h3 id="definition-9">Definition</h3>
<p>A Delta CRDT extends a state-based CRDT with delta operations:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> DeltaCRDT</span><span> a </span><span>=</span><span> DeltaCRDT</span></span>
<span><span>  { state </span><span>::</span><span> a</span></span>
<span><span>  , lastSent </span><span>::</span><span> Map</span><span> ReplicaId</span><span> a  </span><span>-- track what we've sent to each replica</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>delta</span><span> ::</span><span> ReplicaId</span><span> -&gt;</span><span> DeltaCRDT</span><span> a </span><span>-&gt;</span><span> Delta</span><span> a</span></span>
<span><span>delta replica crdt </span><span>=</span><span> state crdt </span><span>`since`</span><span> lastSent[replica]</span></span></code></pre>
<h3 id="laws-and-invariants-9">Laws and Invariants</h3>
<p>Delta CRDTs must satisfy the same semilattice properties as regular state-based CRDTs, plus:</p>
<p><strong>Delta-state equivalence</strong>: Merging deltas incrementally must be equivalent to merging full states.</p>
<p><strong>Delta composition</strong>: Deltas can be composed: <code>delta1 ‚äî delta2</code> is itself a valid delta.</p>
<h3 id="tradeoffs-10">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Dramatically reduced bandwidth (send only changes)</li>
<li>Same convergence guarantees as state-based CRDTs</li>
<li>Can batch multiple deltas together</li>
<li>Easier to implement than operation-based CRDTs</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Must track what has been sent to each replica</li>
<li>Slightly more complex than pure state-based</li>
<li>Still need full state for new replicas joining</li>
</ul>
<h3 id="when-to-use-10">When to Use</h3>
<p>Use Delta CRDTs when network bandwidth is a concern or state size is large. Most production CRDT systems use delta-state internally (<a href="https://riak.com/">Riak</a>, <a href="https://automerge.org/">Automerge</a>). If you‚Äôre implementing your own CRDT system from scratch, start with deltas. Your future self will thank you.</p>
<h3 id="example-delta-g-counter">Example: Delta G-Counter</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>increment</span><span> ::</span><span> ReplicaId</span><span> -&gt;</span><span> GCounter</span><span> -&gt;</span><span> (</span><span>GCounter</span><span>,</span><span> Delta</span><span> GCounter</span><span>)</span></span>
<span><span>increment r counter </span><span>=</span></span>
<span><span>  let</span><span> newCounter </span><span>=</span><span> insertWith (</span><span>+</span><span>) r </span><span>1</span><span> counter</span></span>
<span><span>      delta </span><span>=</span><span> singleton r </span><span>1</span><span>  -- only the change!</span></span>
<span><span>  in</span><span> (newCounter</span><span>,</span><span> delta)</span></span></code></pre>
<p>The delta is just the single updated entry, not the entire counter.</p>
<h2 id="woot-without-operational-transformation">WOOT: Without Operational Transformation</h2>
<p>WOOT is a sequence CRDT that predates RGA, with different design choices.<sup><a href="#user-content-fn-woot-paper" id="user-content-fnref-woot-paper-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
<h3 id="definition-10">Definition</h3>
<p>WOOT represents a sequence as a set of character objects with unique IDs, where each character stores references to its previous and next characters:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> WChar</span><span> a </span><span>=</span><span> WChar</span></span>
<span><span>  { charId </span><span>::</span><span> UID</span></span>
<span><span>  , value </span><span>::</span><span> a</span></span>
<span><span>  , prevId </span><span>::</span><span> UID</span></span>
<span><span>  , nextId </span><span>::</span><span> UID</span></span>
<span><span>  , isVisible </span><span>::</span><span> Bool</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>type</span><span> WOOT</span><span> a </span><span>=</span><span> Set</span><span> (</span><span>WChar</span><span> a)</span></span></code></pre>
<h3 id="key-insight">Key Insight</h3>
<p>Instead of storing a linear sequence, WOOT stores constraints: ‚Äúthis character comes after X and before Y.‚Äù When multiple characters claim to be between X and Y, a deterministic ordering (based on UID) resolves the conflict.</p>
<h3 id="operations-9">Operations</h3>
<p><strong>Insert</strong> (after character with ID <code>prev</code>, before character with ID <code>next</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>insert</span><span> ::</span><span> a </span><span>-&gt;</span><span> UID</span><span> -&gt;</span><span> UID</span><span> -&gt;</span><span> UID</span><span> -&gt;</span><span> WOOT</span><span> a </span><span>-&gt;</span><span> WOOT</span><span> a</span></span>
<span><span>insert val uid prev next woot </span><span>=</span></span>
<span><span>  insert (WChar uid val prev next True) woot</span></span></code></pre>
<p><strong>Delete</strong> (character with ID <code>uid</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>delete</span><span> ::</span><span> UID</span><span> -&gt;</span><span> WOOT</span><span> a </span><span>-&gt;</span><span> WOOT</span><span> a</span></span>
<span><span>delete uid woot </span><span>=</span><span> -- mark character as invisible, don't remove</span></span></code></pre>
<h3 id="linearization">Linearization</h3>
<p>To read the sequence, perform a topological sort respecting the prev/next constraints, filtering out invisible characters.</p>
<h3 id="tradeoffs-11">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Strong eventual consistency</li>
<li>No need for causal delivery (constraints handle ordering)</li>
<li>Intuitive model (characters reference neighbors)</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Tombstones accumulate (deleted characters remain)</li>
<li>Linearization has O(n¬≤) worst case</li>
<li>More complex than RGA</li>
<li>UIDs must be globally unique and ordered</li>
</ul>
<h3 id="comparison-with-rga">Comparison with RGA</h3>
<ul>
<li><strong>RGA</strong>: Uses a tree structure, parent-child relationships</li>
<li><strong>WOOT</strong>: Uses bidirectional constraints, more flexible but slower linearization</li>
</ul>
<h3 id="when-to-use-11">When to Use</h3>
<p>WOOT is primarily of historical interest. Modern implementations prefer RGA or YATA for better performance. But it‚Äôs a neat design, and the name alone makes it worth knowing about.</p>
<h2 id="logoot-scalable-position-identifiers">Logoot: Scalable Position Identifiers</h2>
<p>Logoot takes a different approach to sequences: instead of linking elements, assign each element a position in a dense order.<sup><a href="#user-content-fn-logoot-paper" id="user-content-fnref-logoot-paper" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup></p>
<h3 id="definition-11">Definition</h3>
<p>Each element has a position identifier that is a sequence of (digit, replicaId) pairs:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> Position</span><span> =</span><span> [(</span><span>Int</span><span>,</span><span> ReplicaId</span><span>)]</span></span>
<span></span>
<span><span>data</span><span> LogootElement</span><span> a </span><span>=</span><span> LogootElement</span></span>
<span><span>  { position </span><span>::</span><span> Position</span></span>
<span><span>  , value </span><span>::</span><span> a</span></span>
<span><span>  , isDeleted </span><span>::</span><span> Bool</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>type</span><span> Logoot</span><span> a </span><span>=</span><span> Set</span><span> (</span><span>LogootElement</span><span> a)</span></span></code></pre>
<p>Positions are ordered lexicographically.</p>
<h3 id="key-insight-1">Key Insight</h3>
<p>Positions form a dense order: between any two positions, you can always allocate a new position. To insert between positions <code>p1</code> and <code>p2</code>, generate a new position <code>p</code> such that <code>p1 &lt; p &lt; p2</code>.</p>
<h3 id="operations-10">Operations</h3>
<p><strong>Insert</strong> (between positions <code>before</code> and <code>after</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>insert</span><span> ::</span><span> a </span><span>-&gt;</span><span> Position</span><span> -&gt;</span><span> Position</span><span> -&gt;</span><span> Logoot</span><span> a </span><span>-&gt;</span><span> Logoot</span><span> a</span></span>
<span><span>insert val before after logoot </span><span>=</span></span>
<span><span>  let</span><span> newPos </span><span>=</span><span> allocatePosition before after currentReplicaId</span></span>
<span><span>      element </span><span>=</span><span> LogootElement newPos val False</span></span>
<span><span>  in</span><span> insert element logoot</span></span></code></pre>
<p><strong>Position Allocation</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>allocatePosition</span><span> ::</span><span> Position</span><span> -&gt;</span><span> Position</span><span> -&gt;</span><span> ReplicaId</span><span> -&gt;</span><span> Position</span></span>
<span><span>allocatePosition before after replicaId </span><span>=</span></span>
<span><span>  -- Find a position between before and after</span></span>
<span><span>  -- Use replicaId as tiebreaker for deterministic ordering</span></span></code></pre>
<p><strong>Delete</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>delete</span><span> ::</span><span> Position</span><span> -&gt;</span><span> Logoot</span><span> a </span><span>-&gt;</span><span> Logoot</span><span> a</span></span>
<span><span>delete pos logoot </span><span>=</span><span> -- mark element at pos as deleted</span></span></code></pre>
<h3 id="laws-and-invariants-10">Laws and Invariants</h3>
<p><strong>Deterministic ordering</strong>: Elements are always ordered by their positions.</p>
<p><strong>Unique positions</strong>: Each insert generates a unique position (using replica ID in the position).</p>
<h3 id="tradeoffs-12">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No need to reference other elements by ID</li>
<li>Simpler merge than WOOT</li>
<li>Positions are self-describing (no need to look up IDs)</li>
<li>Can insert without knowing the full document structure</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Position identifiers grow over time (especially with many edits)</li>
<li>Still accumulates tombstones</li>
<li>Position allocation algorithm is complex</li>
<li>Pathological cases where positions become very long</li>
</ul>
<h3 id="lseq-adaptive-positions">LSEQ: Adaptive Positions</h3>
<p>LSEQ improves on Logoot by using an adaptive allocation strategy. Instead of always allocating positions the same way, LSEQ alternates between strategies to keep positions shorter on average.<sup><a href="#user-content-fn-lseq-paper" id="user-content-fnref-lseq-paper" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup></p>
<h3 id="when-to-use-12">When to Use</h3>
<p>Use Logoot/LSEQ when you need a sequence CRDT and want simpler semantics than RGA/WOOT. The tradeoff is position identifier growth.</p>
<h2 id="tree-crdts-hierarchical-data">Tree CRDTs: Hierarchical Data</h2>
<p>Extending CRDTs to trees is challenging because parent-child relationships must be maintained consistently.</p>
<h3 id="the-problem-2">The Problem</h3>
<p>Trees have structural constraints:</p>
<ul>
<li>Each node has exactly one parent (except root)</li>
<li>No cycles allowed</li>
<li>Moving a node changes parent-child relationships</li>
</ul>
<p>How do we handle concurrent operations like:</p>
<ul>
<li>Two replicas move the same node to different parents?</li>
<li>One replica moves node A under node B while another moves B under A?</li>
</ul>
<h3 id="approaches">Approaches</h3>
<p><strong>OR-Tree</strong>: Combine OR-Set with parent pointers, using conflict resolution strategies when multiple parents are observed.</p>
<p><strong>CRDT-Tree</strong>: Use causal ordering to determine which move operations take precedence.</p>
<p><strong>Log-based Trees</strong>: Store operations in a replicated log and rebuild tree structure on read.</p>
<h3 id="or-tree-definition">OR-Tree Definition</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ORTree</span><span> a </span><span>=</span><span> Map</span><span> NodeId</span><span> (</span><span>ORSet</span><span> ParentId</span><span>,</span><span> a)</span></span></code></pre>
<p>Each node stores an OR-Set of potential parents. Conflict resolution:</p>
<ul>
<li><strong>Last-write-wins</strong>: Use timestamps to pick winning parent</li>
<li><strong>First-wins</strong>: The first parent observed wins</li>
<li><strong>Merge</strong>: Allow nodes to have multiple parents temporarily, application resolves</li>
</ul>
<h3 id="tradeoffs-13">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Can represent hierarchical data distributedly</li>
<li>Handles concurrent structural changes</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Complex conflict resolution strategies</li>
<li>Must prevent cycles (may require rejecting some operations)</li>
<li>Moving subtrees is complicated</li>
<li>High metadata overhead</li>
</ul>
<h3 id="when-to-use-13">When to Use</h3>
<p>Use Tree CRDTs for file systems, organizational charts, or document outlines where the hierarchy must be replicated. Be prepared for complexity in handling concurrent structural changes.</p>
<h3 id="alternatives-1">Alternatives</h3>
<p>For many use cases, an OR-Map with explicit parent fields is simpler than a full Tree CRDT, even if it doesn‚Äôt enforce tree constraints at the CRDT level.</p>

<p>A practical example combining multiple CRDT concepts.</p>
<h3 id="the-domain">The Domain</h3>
<p>An e-commerce shopping cart must support:</p>
<ul>
<li>Add product to cart</li>
<li>Remove product from cart</li>
<li>Change quantity</li>
<li>Work offline and sync later</li>
</ul>
<h3 id="naive-approach-lww-map">Naive Approach: LWW Map</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> CartLWW</span><span> =</span><span> Map</span><span> ProductId</span><span> (</span><span>Int</span><span>,</span><span> Timestamp</span><span>)</span></span></code></pre>
<p>Problems:</p>
<ul>
<li>Concurrent additions of the same product (one wins)</li>
<li>Remove on one device, add on another (one wins, data loss)</li>
</ul>
<h3 id="better-or-set--pn-counter">Better: OR-Set + PN-Counter</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ShoppingCart</span><span> =</span><span> Map</span><span> ProductId</span><span> PNCounter</span></span></code></pre>
<ul>
<li>Use OR-Set semantics for which products are in cart</li>
<li>Use PN-Counter for quantities</li>
<li>Add-wins semantics for products (if concurrently added and removed, item stays)</li>
<li>Quantities merge correctly (concurrent +1 and +2 becomes +3)</li>
</ul>
<h3 id="operations-11">Operations</h3>
<p><strong>Add product with quantity</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>addToCart</span><span> ::</span><span> ProductId</span><span> -&gt;</span><span> Int</span><span> -&gt;</span><span> ReplicaId</span><span> -&gt;</span><span> ShoppingCart</span><span> -&gt;</span><span> ShoppingCart</span></span>
<span><span>addToCart pid qty replica cart </span><span>=</span></span>
<span><span>  let</span><span> counter </span><span>=</span><span> lookupOr emptyCounter pid cart</span></span>
<span><span>      incremented </span><span>=</span><span> incrementN replica qty counter</span></span>
<span><span>  in</span><span> insert pid incremented cart</span></span></code></pre>
<p><strong>Remove product</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>removeFromCart</span><span> ::</span><span> ProductId</span><span> -&gt;</span><span> ShoppingCart</span><span> -&gt;</span><span> ShoppingCart</span></span>
<span><span>removeFromCart pid cart </span><span>=</span><span> delete pid cart</span></span></code></pre>
<p><strong>Change quantity</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>changeQuantity</span><span> ::</span><span> ProductId</span><span> -&gt;</span><span> Int</span><span> -&gt;</span><span> ReplicaId</span><span> -&gt;</span><span> ShoppingCart</span><span> -&gt;</span><span> ShoppingCart</span></span>
<span><span>changeQuantity pid delta replica cart </span><span>=</span></span>
<span><span>  let</span><span> counter </span><span>=</span><span> lookupOr emptyCounter pid cart</span></span>
<span><span>      updated </span><span>=</span><span> if</span><span> delta </span><span>&gt;</span><span> 0</span></span>
<span><span>                then</span><span> incrementN replica delta counter</span></span>
<span><span>                else</span><span> decrementN replica (</span><span>-</span><span>delta) counter</span></span>
<span><span>  in</span><span> insert pid updated cart</span></span></code></pre>
<h3 id="tradeoffs-14">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Handles all operations correctly</li>
<li>No data loss on concurrent modifications</li>
<li>Intuitive semantics for users</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>PN-Counters can go negative (need validation)</li>
<li>Must track all replicas (for PN-Counter)</li>
<li>Slightly more overhead than simple LWW</li>
</ul>
<p>This example shows how combining basic CRDTs creates sophisticated application-level data structures.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<h3 id="choosing-a-crdt">Choosing a CRDT</h3>
<p>The choice of CRDT depends on your requirements:</p>
<p><strong>Do you need only additions?</strong> Use G-Counter or G-Set.</p>
<p><strong>Do you need removals but not re-additions?</strong> Use 2P-Set.</p>
<p><strong>Can you tolerate last-write-wins?</strong> Use LWW-Element-Set or LWW-Register.</p>
<p><strong>Do you need to preserve concurrent operations?</strong> Use OR-Set or MV-Register.</p>
<p><strong>Do you have sequences?</strong> Use RGA or similar sequence CRDT.</p>
<p><strong>Do you need nested structures?</strong> Use OR-Map with nested CRDTs.</p>
<h3 id="garbage-collection-1">Garbage Collection</h3>
<p>Garbage collection is one of the most challenging practical problems with CRDTs. The fundamental tension: CRDTs achieve convergence by monotonically accumulating information, but production systems can‚Äôt grow unbounded forever.</p>
<h4 id="the-problem-in-detail">The Problem in Detail</h4>
<p>Consider an OR-Set used for a collaborative todo list. Each time someone adds a task and removes it, we accumulate:</p>
<ul>
<li>A unique tag for the addition (never removed)</li>
<li>A tombstone tracking the removal (never removed)</li>
</ul>
<p>After 10,000 tasks have been created and completed, our ‚Äúempty‚Äù todo list still contains 10,000 tags worth of metadata. In a G-Counter tracking page views, we keep a separate count for every replica that has ever incremented the counter‚Äîeven if that replica hasn‚Äôt been online in years. For sequence CRDTs like RGA or WOOT, every deleted character becomes a tombstone that must be retained indefinitely. A 1000-character document that‚Äôs been heavily edited might internally contain 50,000 tombstones.</p>
<p>The core issue: <strong>CRDTs converge by retaining enough information to handle any possible merge</strong>. If replica A discards metadata about some operation, and replica B (which has been offline for weeks) later tries to merge its state‚Äîwhich still references that metadata‚Äîthe merge may produce incorrect results.</p>
<h4 id="why-cant-we-just-delete-old-data">Why Can‚Äôt We Just Delete Old Data?</h4>
<p>Let‚Äôs make this concrete with an OR-Set example:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>-- Replica A's state</span></span>
<span><span>orset_a </span><span>=</span><span> {"todo</span><span>-</span><span>1</span><span>": {tag_1, tag_2}}</span></span>
<span></span>
<span><span>-- Replica B's state (has been offline)</span></span>
<span><span>orset_b = {"</span><span>todo</span><span>-</span><span>1</span><span>": {tag_1, tag_2, tag_3}}</span></span>
<span></span>
<span><span>-- Replica A removes todo-1, observing tags {tag_1, tag_2}</span></span>
<span><span>-- Now A's state is:</span></span>
<span><span>orset_a = {}</span></span>
<span></span>
<span><span>-- If A garbage collects and forgets about tags {tag_1, tag_2},</span></span>
<span><span>-- then later merges with B:</span></span>
<span><span>merge(orset_a, orset_b) = {"</span><span>todo</span><span>-</span><span>1</span><span>": {tag_3}}</span></span>
<span></span>
<span><span>-- The element reappears! (Zombie resurrection)</span></span></code></pre>
<p>The element we removed comes back because we lost the causal information about which tags we had observed and removed. This is the fundamental safety problem with CRDT garbage collection.</p>
<h4 id="strategies-and-tradeoffs">Strategies and Tradeoffs</h4>
<p><strong>Time-Based Expiry</strong></p>
<p>The simplest approach: discard metadata older than some threshold (e.g., 30 days). This works well when you can guarantee all replicas sync within that window.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>gcTombstones</span><span> ::</span><span> Timestamp</span><span> -&gt;</span><span> ORSet</span><span> a </span><span>-&gt;</span><span> ORSet</span><span> a</span></span>
<span><span>gcTombstones cutoff set </span><span>=</span></span>
<span><span>  -- Remove tags older than cutoff</span></span>
<span><span>  Map.mapMaybe (</span><span>\\</span><span>tags </span><span>-&gt;</span></span>
<span><span>    let</span><span> recent </span><span>=</span><span> Set.filter (</span><span>\\</span><span>t </span><span>-&gt;</span><span> tagTime t </span><span>&gt;</span><span> cutoff) tags</span></span>
<span><span>    in</span><span> if</span><span> Set.null recent </span><span>then</span><span> Nothing </span><span>else</span><span> Just recent) set</span></span></code></pre>
<p><em>Advantages</em>:</p>
<ul>
<li>Simple to implement</li>
<li>No coordination required</li>
<li>Works well for frequently-syncing systems</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Unsafe if replicas can be offline longer than the grace period</li>
<li>Must choose grace period conservatively (wasted space)</li>
<li>Zombie resurrection if threshold is too aggressive</li>
</ul>
<p><em>When to use</em>: Mobile apps where you can bound offline time (e.g., ‚Äúyou must sync at least once per week‚Äù).</p>
<p><strong>Coordinated Garbage Collection</strong></p>
<p>Use distributed consensus to agree on what‚Äôs safe to discard. Once all replicas acknowledge they‚Äôve received a particular update, the corresponding metadata can be safely removed.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> GCState</span><span> =</span><span> GCState</span></span>
<span><span>  { pendingGC </span><span>::</span><span> Set</span><span> Tag</span><span>  -- Tags eligible for GC</span></span>
<span><span>  , replicaAcks </span><span>::</span><span> Map</span><span> ReplicaId</span><span> (</span><span>Set</span><span> Tag</span><span>)  </span><span>-- What each replica has seen</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>-- When all replicas have acked a tag, it's safe to remove</span></span>
<span><span>safeToDiscard</span><span> ::</span><span> GCState</span><span> -&gt;</span><span> Set</span><span> Tag</span></span>
<span><span>safeToDiscard (GCState pending acks) </span><span>=</span></span>
<span><span>  -- Tags that all known replicas have acknowledged</span></span>
<span><span>  Set.filter (</span><span>\\</span><span>tag </span><span>-&gt;</span><span> all (Set.member tag) (Map.elems acks)) pending</span></span></code></pre>
<p><em>Advantages</em>:</p>
<ul>
<li>Completely safe (no zombie resurrections)</li>
<li>Can garbage collect aggressively once consensus is reached</li>
<li>Works with arbitrary offline periods</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Requires coordination (defeats CRDT‚Äôs main selling point!)</li>
<li>Slow convergence if some replicas are rarely online</li>
<li>Must track all replicas (what about replicas that never come back?)</li>
</ul>
<p><em>When to use</em>: When you have a bounded, known set of replicas and can tolerate periodic coordination rounds.</p>
<p><strong>Version Vectors for Causal Tracking</strong></p>
<p>Use version vectors to track causal history. Metadata can be discarded once it‚Äôs been causally superseded at all replicas.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> CausalORSet</span><span> a </span><span>=</span><span> CausalORSet</span></span>
<span><span>  { elements </span><span>::</span><span> Map</span><span> a (</span><span>Set</span><span> (</span><span>Tag</span><span>,</span><span> VersionVector</span><span>))</span></span>
<span><span>  , replicaVersions </span><span>::</span><span> Map</span><span> ReplicaId</span><span> VersionVector</span><span>  -- Last known VV per replica</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>-- A tag can be GC'd if its version vector is dominated by all known replicas</span></span>
<span><span>canDiscardTag</span><span> ::</span><span> (</span><span>Tag</span><span>,</span><span> VersionVector</span><span>) </span><span>-&gt;</span><span> Map</span><span> ReplicaId</span><span> VersionVector</span><span> -&gt;</span><span> Bool</span></span>
<span><span>canDiscardTag (_</span><span>,</span><span> tagVV) replicaVVs </span><span>=</span></span>
<span><span>  all (</span><span>\\</span><span>replicaVV </span><span>-&gt;</span><span> tagVV </span><span>`happenedBefore`</span><span> replicaVV) (Map.elems replicaVVs)</span></span></code></pre>
<p>This is more sophisticated: we track causality explicitly and can safely discard tags that are in the causal past of all known replicas.</p>
<p><em>Advantages</em>:</p>
<ul>
<li>More precise than time-based expiry</li>
<li>No coordination needed for the happy path</li>
<li>Safe as long as causal tracking is correct</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Version vectors add significant overhead (O(replicas) per operation)</li>
<li>Still requires tracking all replicas</li>
<li>Complex to implement correctly</li>
<li>What about new replicas that join later?</li>
</ul>
<p><em>When to use</em>: Systems already using version vectors for causal consistency (Riak, Cassandra-style systems).</p>
<p><strong>Bounded Structures with Fallback</strong></p>
<p>Limit metadata size and use LWW semantics when bounds are exceeded. For example, keep at most 1000 tags per element in an OR-Set. If we exceed that, discard the oldest tags and accept potential anomalies.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>addWithBound</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> Tag</span><span> -&gt;</span><span> Int</span><span> -&gt;</span><span> ORSet</span><span> a </span><span>-&gt;</span><span> ORSet</span><span> a</span></span>
<span><span>addWithBound x tag maxTags set </span><span>=</span></span>
<span><span>  let</span><span> currentTags </span><span>=</span><span> Map.findWithDefault Set.empty x set</span></span>
<span><span>      newTags </span><span>=</span><span> Set.insert tag currentTags</span></span>
<span><span>      boundedTags </span><span>=</span><span> if</span><span> Set.size newTags </span><span>&gt;</span><span> maxTags</span></span>
<span><span>                    then</span><span> Set.fromList </span><span>$</span><span> take maxTags </span><span>$</span><span> </span></span>
<span><span>                         sortBy (comparing tagTimestamp) (Set.toList newTags)</span></span>
<span><span>                    else</span><span> newTags</span></span>
<span><span>  in</span><span> Map.insert x boundedTags set</span></span></code></pre>
<p><em>Advantages</em>:</p>
<ul>
<li>Bounded space overhead (guaranteed)</li>
<li>No coordination needed</li>
<li>Graceful degradation (becomes LWW-ish when bounded)</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Correctness sacrificed for space</li>
<li>May lose concurrent operations</li>
<li>Choosing the bound is difficult (too small = frequent anomalies, too large = still wasteful)</li>
</ul>
<p><em>When to use</em>: When you must have bounded space (embedded systems, strict SLAs) and can tolerate occasional anomalies.</p>
<p><strong>Checkpoint and Rebase</strong></p>
<p>Periodically create a ‚Äúcheckpoint‚Äù snapshot and discard history before that point. New replicas joining after the checkpoint start from the snapshot.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> CheckpointedCRDT</span><span> a </span><span>=</span><span> CheckpointedCRDT</span></span>
<span><span>  { baselineState </span><span>::</span><span> a  </span><span>-- Snapshot at checkpoint</span></span>
<span><span>  , checkpointTime </span><span>::</span><span> Timestamp</span></span>
<span><span>  , deltaSince </span><span>::</span><span> [</span><span>Delta</span><span> a]  </span><span>-- Operations since checkpoint</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>-- Create a new checkpoint, discarding old deltas</span></span>
<span><span>checkpoint</span><span> ::</span><span> CheckpointedCRDT</span><span> a </span><span>-&gt;</span><span> CheckpointedCRDT</span><span> a</span></span>
<span><span>checkpoint crdt </span><span>=</span><span> CheckpointedCRDT</span></span>
<span><span>  { baselineState </span><span>=</span><span> foldl merge (baselineState crdt) (deltaSince crdt)</span></span>
<span><span>  , checkpointTime </span><span>=</span><span> currentTime</span></span>
<span><span>  , deltaSince </span><span>=</span><span> []</span></span>
<span><span>  }</span></span></code></pre>
<p>Replicas that haven‚Äôt synced since before the checkpoint must do a full state sync rather than incremental merge.</p>
<p><em>Advantages</em>:</p>
<ul>
<li>Can aggressively prune old history</li>
<li>Conceptually clean (like Git‚Äôs shallow clones)</li>
<li>Works well with mostly-online systems</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Replicas offline during checkpoint period lose incremental sync</li>
<li>Need to track which replicas are pre-checkpoint</li>
<li>Full state sync is expensive</li>
</ul>
<p><em>When to use</em>: Collaborative editing systems where most users are online most of the time (Google Docs, Figma).</p>
<h4 id="practical-recommendations">Practical Recommendations</h4>
<p>For most applications, a <strong>hybrid approach</strong> works best:</p>
<ol>
<li>Use time-based expiry with a conservative grace period (90 days)</li>
<li>Track the oldest unsynced replica timestamp</li>
<li>Only discard metadata older than: <code>min(graceperiod, oldest_unsynced - safety_margin)</code></li>
<li>Provide manual ‚Äúcompact‚Äù operations for administrators</li>
<li>Use bounded structures for untrusted/public replicas</li>
</ol>
<p>Without some form of garbage collection, CRDT state grows unbounded and will eventually exhaust memory or storage. The question isn‚Äôt whether to implement GC, but which tradeoffs you‚Äôre willing to accept.</p>
<p>And, realistically speaking, you‚Äôre unlikely to implement a system that only uses CRDTs and no other data storage. You‚Äôll almost certainly have some sort of traditional database to store your data, which
you can probably use to periodically coordinate garbage collection.</p>
<h3 id="a-note-on-causal-consistency">A note on Causal Consistency</h3>
<p>CRDTs themselves don‚Äôt enforce causal delivery. You need a causal broadcast protocol to ensure operations are delivered respecting happens-before relationships. Without causal delivery, some CRDTs (especially operation-based ones) may behave incorrectly.</p>
<h3 id="performance">Performance</h3>
<p>Different CRDTs have different performance characteristics. Consider your read/write ratio, expected contention, and replica count when choosing:</p>


























































































































<table><thead><tr><th>CRDT Type</th><th>Space Complexity</th><th>Add/Insert</th><th>Remove/Delete</th><th>Merge</th><th>Read/Query</th><th>Notes</th></tr></thead><tbody><tr><td><strong>G-Counter</strong></td><td>O(r)</td><td>O(1)</td><td>N/A</td><td>O(r)</td><td>O(r)</td><td>Space: one counter per replica</td></tr><tr><td><strong>PN-Counter</strong></td><td>O(r)</td><td>O(1)</td><td>O(1)</td><td>O(r)</td><td>O(r)</td><td>Double the space of G-Counter</td></tr><tr><td><strong>G-Set</strong></td><td>O(e)</td><td>O(1)</td><td>N/A</td><td>O(e)</td><td>O(1)</td><td>Standard set operations</td></tr><tr><td><strong>2P-Set</strong></td><td>O(e)</td><td>O(1)</td><td>O(1)</td><td>O(e)</td><td>O(1)</td><td>Both added and removed sets grow</td></tr><tr><td><strong>LWW-Element-Set</strong></td><td>O(e)</td><td>O(1)</td><td>O(1)</td><td>O(e)</td><td>O(1)</td><td>Can GC old timestamps carefully</td></tr><tr><td><strong>OR-Set</strong></td><td>O(e √ó t)</td><td>O(1)</td><td>O(t)</td><td>O(e √ó t)</td><td>O(1)</td><td>Tags accumulate, needs GC</td></tr><tr><td><strong>LWW-Register</strong></td><td>O(1)</td><td>O(1)</td><td>N/A</td><td>O(1)</td><td>O(1)</td><td>Minimal overhead</td></tr><tr><td><strong>MV-Register</strong></td><td>O(concurrent)</td><td>O(1)</td><td>N/A</td><td>O(c)</td><td>O(c)</td><td>Returns set of concurrent values</td></tr><tr><td><strong>OR-Map</strong></td><td>O(k √ó t)</td><td>O(1)</td><td>O(t)</td><td>O(k √ó t)</td><td>O(1)</td><td>Per-key OR-Set overhead</td></tr><tr><td><strong>RGA</strong></td><td>O(n + d)</td><td>O(log n)</td><td>O(log n)</td><td>O(n + d)</td><td>O(n)</td><td>Tombstones accumulate</td></tr><tr><td><strong>WOOT</strong></td><td>O(n + d)</td><td>O(n¬≤) worst</td><td>O(log n)</td><td>O(n + d)</td><td>O(n¬≤) worst</td><td>Linearization is expensive</td></tr><tr><td><strong>Logoot/LSEQ</strong></td><td>O(n √ó p)</td><td>O(log n)</td><td>O(log n)</td><td>O(n)</td><td>O(n log n)</td><td>Position identifiers grow</td></tr></tbody></table>
<p><strong>Legend:</strong></p>
<ul>
<li><code>r</code> = number of replicas</li>
<li><code>e</code> = number of elements in set</li>
<li><code>t</code> = average tags per element (OR-Set)</li>
<li><code>k</code> = number of keys in map</li>
<li><code>n</code> = number of visible elements in sequence</li>
<li><code>d</code> = number of deleted elements (tombstones)</li>
<li><code>c</code> = number of concurrent writes</li>
<li><code>p</code> = average position identifier length</li>
</ul>
<p><strong>Key Observations:</strong></p>
<ul>
<li><strong>Counter CRDTs</strong> scale with replica count, not operation count. A billion increments still cost O(replicas) space.</li>
<li><strong>Set CRDTs</strong> generally have constant-time operations, but OR-Set‚Äôs space grows with tags unless garbage collected.</li>
<li><strong>Sequence CRDTs</strong> suffer from tombstone accumulation. RGA is typically faster than WOOT in practice despite similar asymptotic complexity.</li>
<li><strong>Position-based sequences</strong> (Logoot/LSEQ) trade time complexity for avoiding explicit parent pointers, but position identifiers can grow pathologically.</li>
<li><strong>Merge operations</strong> are often the bottleneck in high-throughput systems. Delta CRDTs dramatically improve merge performance by sending only changes.</li>
</ul>
<h3 id="libraries-and-implementations">Libraries and Implementations</h3>
<p>Many CRDT libraries exist:</p>
<ul>
<li><strong>Automerge</strong>: Full-featured CRDT library for JSON-like documents<sup><a href="#user-content-fn-automerge" id="user-content-fnref-automerge" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup></li>
<li><strong>Yjs</strong>: Optimized for collaborative editing<sup><a href="#user-content-fn-yjs" id="user-content-fnref-yjs" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup></li>
<li><strong>Riak</strong>: Database with built-in CRDT support<sup><a href="#user-content-fn-riak" id="user-content-fnref-riak" data-footnote-ref="" aria-describedby="footnote-label">17</a></sup></li>
<li><strong>Redis Enterprise</strong>: CRDT-enabled Redis<sup><a href="#user-content-fn-redis-crdt" id="user-content-fnref-redis-crdt" data-footnote-ref="" aria-describedby="footnote-label">18</a></sup></li>
<li><strong>AntidoteDB</strong>: CRDT-native database<sup><a href="#user-content-fn-antidote" id="user-content-fnref-antidote" data-footnote-ref="" aria-describedby="footnote-label">19</a></sup></li>
</ul>
<p>Each makes different tradeoff decisions.</p>
<h2 id="further-reading">Further Reading</h2>
<p>The CRDT literature is vast and honestly a bit scattered across conference proceedings. Here are the key papers worth reading:</p>
<p><strong>Foundational</strong>:</p>
<ul>
<li>Shapiro et al., <a href="https://hal.inria.fr/inria-00609399v1/document">‚ÄúConflict-Free Replicated Data Types‚Äù</a> (2011): The original CRDT paper, defining state-based and operation-based CRDTs.</li>
<li>Shapiro et al., <a href="https://hal.inria.fr/inria-00555588/document">‚ÄúA Comprehensive Study of Convergent and Commutative Replicated Data Types‚Äù</a> (2011): Detailed technical report covering many CRDTs. This is the one you want to bookmark.</li>
</ul>
<p><strong>Sequence CRDTs</strong>:</p>
<ul>
<li>Oster et al., <a href="https://hal.inria.fr/inria-00108523/document">‚ÄúData Consistency for P2P Collaborative Editing‚Äù</a> (2006): Introduces WOOT.</li>
<li>Roh et al., <a href="http://csl.skku.edu/papers/jpdc11.pdf">‚ÄúReplicated Abstract Data Types: Building Blocks for Collaborative Applications‚Äù</a> (2011): Introduces RGA.</li>
<li>Weiss et al., <a href="https://hal.inria.fr/inria-00432368/document">‚ÄúLogoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing‚Äù</a> (2009): Introduces Logoot.</li>
<li>N√©delec et al., <a href="https://hal.archives-ouvertes.fr/hal-00921633/document">‚ÄúLSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing‚Äù</a> (2013): Introduces LSEQ.</li>
</ul>
<p><strong>Advanced Topics</strong>:</p>
<ul>
<li>Baquero et al., <a href="https://inria.hal.science/hal-01287738v1/document">‚ÄúMaking Operation-based CRDTs Operation-based‚Äù</a> (2014): Pure operation-based CRDTs without state.</li>
<li>Almeida et al., <a href="https://arxiv.org/abs/1603.01529">‚ÄúDelta State Replicated Data Types‚Äù</a> (2018): Efficiency improvements for state-based CRDTs.</li>
<li>Kleppmann et al., <a href="https://arxiv.org/abs/1608.03960">‚ÄúA Conflict-Free Replicated JSON Datatype‚Äù</a> (2017): Automerge‚Äôs design.</li>
</ul>
<p><strong>Surveys</strong>:</p>
<ul>
<li>Shapiro et al., <a href="https://inria.hal.science/inria-00555588/en/">‚ÄúConvergent and Commutative Replicated Data Types‚Äù</a> (2011): The comprehensive technical report. Start here if you want depth.</li>
</ul>
<h2 id="wrapping-up">Wrapping up</h2>
<p>CRDTs are not a silver bullet. They trade coordination for metadata, strong consistency for eventual consistency, and simplicity for convergence guarantees. But in scenarios where availability matters more than immediate consistency, they‚Äôre remarkably powerful.</p>
<p>There is no ‚Äúbest‚Äù CRDT, only CRDTs suited to different problems; the CRDT you choose depends entirely on your application‚Äôs semantics:</p>
<ul>
<li>What operations do you need (add, remove, re-add)?</li>
<li>Can you tolerate lost updates?</li>
<li>Do you need to detect conflicts or resolve them automatically?</li>
<li>What‚Äôs your tolerance for metadata overhead?</li>
</ul>
<p>The CRDT abstraction is elegant in theory, but bewildering in practice because there are so many instances with subtle differences. Hopefully this guide has cut through some of the confusion, and given you a good intuition for how they work and when to use them.</p>
<p>I honestly still haven‚Äôt hit a use case for CRDTs that I couldn‚Äôt solve with a traditional database and some custom coordination logic. But sometimes we just want to learn for the sake of learning. If you beat me to it, let me know!</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-crdt-origin">
<p>The term ‚ÄúConflict-free Replicated Data Type‚Äù was coined by Marc Shapiro, Nuno Pregui√ßa, Carlos Baquero, and Marek Zawirski in their 2011 paper <a href="https://hal.inria.fr/inria-00609399v1/document">‚ÄúConflict-free Replicated Data Types‚Äù</a> (technical report) and the 2011 SSS conference paper <a href="https://hal.inria.fr/inria-00555588/document">‚ÄúA comprehensive study of Convergent and Commutative Replicated Data Types‚Äù</a>. The theoretical foundations draw from earlier work on commutative replicated data types and optimistic replication. <a href="#user-content-fnref-crdt-origin" data-footnote-backref="" aria-label="Back to reference 1">‚Ü©</a></p>
</li>
<li id="user-content-fn-woot-paper">
<p>WOOT was introduced by Oster, Urso, Molli, and Imine in <a href="https://hal.inria.fr/inria-00108523/document">‚ÄúData Consistency for P2P Collaborative Editing‚Äù</a> (2006). The name is a play on ‚ÄúOT‚Äù (Operational Transformation), emphasizing that it achieves similar goals ‚ÄúWithOut OT.‚Äù WOOT was one of the first practical sequence CRDTs and influenced many subsequent designs. <a href="#user-content-fnref-woot-paper" data-footnote-backref="" aria-label="Back to reference 2">‚Ü©</a> <a href="#user-content-fnref-woot-paper-2" data-footnote-backref="" aria-label="Back to reference 2-2">‚Ü©<sup>2</sup></a></p>
</li>
<li id="user-content-fn-cvcrdt">
<p>State-based CRDTs are also called ‚Äúconvergent‚Äù replicated data types (CvRDT). The ‚ÄúCv‚Äù stands for ‚Äúconvergent‚Äù - emphasizing that replicas converge to the same state by repeatedly applying the join operation. <a href="#user-content-fnref-cvcrdt" data-footnote-backref="" aria-label="Back to reference 3">‚Ü©</a></p>
</li>
<li id="user-content-fn-cmcrdt">
<p>Operation-based CRDTs are also called ‚Äúcommutative‚Äù replicated data types (CmRDT). They require causal delivery of operations - if operation A happened before operation B on the same replica, B must not be delivered before A at any other replica. <a href="#user-content-fnref-cmcrdt" data-footnote-backref="" aria-label="Back to reference 4">‚Ü©</a></p>
</li>
<li id="user-content-fn-gcounter-origin">
<p>The G-Counter appears in Shapiro et al.‚Äôs 2011 technical report <a href="https://hal.inria.fr/inria-00555588/document">‚ÄúA Comprehensive Study of Convergent and Commutative Replicated Data Types‚Äù</a> as one of the foundational examples demonstrating CRDT principles. <a href="#user-content-fnref-gcounter-origin" data-footnote-backref="" aria-label="Back to reference 5">‚Ü©</a></p>
</li>
<li id="user-content-fn-gcounter-space">
<p>The space complexity is O(n) where n is the number of replicas, not the number of increments. This means G-Counters scale well with the number of operations but require tracking all replicas that have ever incremented the counter. <a href="#user-content-fnref-gcounter-space" data-footnote-backref="" aria-label="Back to reference 6">‚Ü©</a></p>
</li>
<li id="user-content-fn-orset-origin">
<p>The OR-Set (Observed-Remove Set) was introduced by Shapiro et al. in their <a href="https://hal.inria.fr/inria-00555588/document">2011 technical report</a>. It‚Äôs also known as the ‚ÄúAdd-Wins Set‚Äù because concurrent add and remove operations result in the element remaining in the set. The key innovation is using unique tags to distinguish between different additions of the same element. <a href="#user-content-fnref-orset-origin" data-footnote-backref="" aria-label="Back to reference 7">‚Ü©</a></p>
</li>
<li id="user-content-fn-sequence-crdt-challenge">
<p>Sequence CRDTs are particularly challenging because positional indices change as elements are inserted or deleted. Unlike sets or counters where elements have stable identity, sequences must maintain ordering despite concurrent modifications at arbitrary positions. <a href="#user-content-fnref-sequence-crdt-challenge" data-footnote-backref="" aria-label="Back to reference 8">‚Ü©</a></p>
</li>
<li id="user-content-fn-rga-paper">
<p>RGA was introduced by Roh et al. in <a href="http://csl.skku.edu/papers/jpdc11.pdf">‚ÄúReplicated Abstract Data Types: Building Blocks for Collaborative Applications‚Äù</a> (2011). The name ‚ÄúReplicated Growable Array‚Äù emphasizes that it‚Äôs an array-like structure that can grow through replication. <a href="#user-content-fnref-rga-paper" data-footnote-backref="" aria-label="Back to reference 9">‚Ü©</a></p>
</li>
<li id="user-content-fn-yata-yjs">
<p>YATA (Yet Another Transformation Approach) was developed by Kevin Jahns for the <a href="https://yjs.dev/">Yjs</a> collaborative editing library. It combines ideas from RGA and WOOT while optimizing for the common case of sequential insertions (typing). Yjs is used in production by companies like Braid, Row Zero, and others for real-time collaboration. <a href="#user-content-fnref-yata-yjs" data-footnote-backref="" aria-label="Back to reference 10">‚Ü©</a></p>
</li>
<li id="user-content-fn-version-vectors">
<p>Version vectors were introduced by Parker et al. in <a href="https://www.cs.purdue.edu/homes/bb/cs542-11Spr/Parker_TSE83.pdf">‚ÄúDetection of Mutual Inconsistency in Distributed Systems‚Äù</a> (1983). They extend <a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf">Lamport‚Äôs logical clocks</a> to track causality in distributed systems. Each replica maintains a vector of logical clocks (one for each replica), enabling precise causal ordering without requiring synchronized physical clocks. <a href="#user-content-fnref-version-vectors" data-footnote-backref="" aria-label="Back to reference 11">‚Ü©</a></p>
</li>
<li id="user-content-fn-delta-crdt-paper">
<p>Delta CRDTs were introduced by Almeida, Shoker, and Baquero in <a href="https://arxiv.org/abs/1603.01529">‚ÄúDelta State Replicated Data Types‚Äù</a> (2018). They bridge the gap between state-based and operation-based CRDTs, achieving operation-based bandwidth efficiency while maintaining state-based simplicity. Most production CRDT systems (<a href="https://riak.com/">Riak</a>, <a href="https://automerge.org/">Automerge</a>) use delta-state internally. <a href="#user-content-fnref-delta-crdt-paper" data-footnote-backref="" aria-label="Back to reference 12">‚Ü©</a></p>
</li>
<li id="user-content-fn-logoot-paper">
<p>Logoot was introduced by Weiss, Urso, and Molli in <a href="https://hal.inria.fr/inria-00432368/document">‚ÄúLogoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing‚Äù</a> (2009). The name combines ‚Äúlog‚Äù (logarithmic complexity) with ‚Äúoot‚Äù from WOOT, its predecessor. Logoot‚Äôs position-based approach influenced many subsequent CRDTs including LSEQ and Treedoc. <a href="#user-content-fnref-logoot-paper" data-footnote-backref="" aria-label="Back to reference 13">‚Ü©</a></p>
</li>
<li id="user-content-fn-lseq-paper">
<p>LSEQ was introduced by N√©delec, Molli, Most√©faoui, and Desmontils in <a href="https://hal.archives-ouvertes.fr/hal-00921633/document">‚ÄúLSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing‚Äù</a> (2013). The key innovation is using different allocation strategies (boundary+ vs boundary-) based on tree depth, which keeps position identifiers shorter in practice compared to Logoot‚Äôs fixed strategy. <a href="#user-content-fnref-lseq-paper" data-footnote-backref="" aria-label="Back to reference 14">‚Ü©</a></p>
</li>
<li id="user-content-fn-automerge">
<p><a href="https://automerge.org/">Automerge</a>, created by Martin Kleppmann and collaborators, implements a JSON CRDT described in <a href="https://arxiv.org/abs/1608.03960">‚ÄúA Conflict-Free Replicated JSON Datatype‚Äù</a> (2017). It uses a columnar encoding for efficiency and has been <a href="https://github.com/automerge/automerge">rewritten in Rust</a> for performance. Used by production apps like <a href="https://www.inkandswitch.com/pushpin/">Inkandswitch‚Äôs Pushpin</a>. <a href="#user-content-fnref-automerge" data-footnote-backref="" aria-label="Back to reference 15">‚Ü©</a></p>
</li>
<li id="user-content-fn-yjs">
<p><a href="https://yjs.dev/">Yjs</a>, created by Kevin Jahns, is optimized for text editing and uses the YATA algorithm. It‚Äôs notably faster than Automerge for text operations and includes bindings for popular editors like <a href="https://codemirror.net/">CodeMirror</a>, <a href="https://microsoft.github.io/monaco-editor/">Monaco</a>, <a href="https://quilljs.com/">Quill</a>, and <a href="https://prosemirror.net/">ProseMirror</a>. <a href="#user-content-fnref-yjs" data-footnote-backref="" aria-label="Back to reference 16">‚Ü©</a></p>
</li>
<li id="user-content-fn-riak">
<p><a href="https://riak.com/">Riak</a>, a distributed database from Basho, was one of the first production systems to adopt CRDTs (2012). It implements counters, sets, and maps as <a href="https://docs.riak.com/riak/kv/latest/developing/data-types/index.html">native data types</a>, using Delta CRDTs internally to minimize bandwidth. Sadly, the company collapsed dramatically, and the project was abandoned for quite some time. I think it‚Äôs still around in a diminished form, but haven‚Äôt tried it in a while. <a href="#user-content-fnref-riak" data-footnote-backref="" aria-label="Back to reference 17">‚Ü©</a></p>
</li>
<li id="user-content-fn-redis-crdt">
<p><a href="https://redis.io/docs/latest/operate/rc/databases/configuration/active-active-redis/">Redis Enterprise‚Äôs CRDT support</a> (Active-Active deployment) uses operation-based CRDTs with causal consistency. It supports strings, hashes, sets, and sorted sets with CRDT semantics, enabling multi-master Redis deployments. <a href="#user-content-fnref-redis-crdt" data-footnote-backref="" aria-label="Back to reference 18">‚Ü©</a></p>
</li>
<li id="user-content-fn-antidote">
<p><a href="https://www.antidotedb.eu/">AntidoteDB</a> is a research database from the <a href="https://syncfree.lip6.fr/">SyncFree project</a> that makes CRDTs the primary abstraction. Unlike other databases where CRDTs are a feature, AntidoteDB is designed from the ground up around CRDT semantics, providing highly available transactions over CRDTs. <a href="#user-content-fnref-antidote" data-footnote-backref="" aria-label="Back to reference 19">‚Ü©</a></p>
</li>
</ol>
</section>   </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leak confirms OpenAI is preparing ads on ChatGPT for public roll out (461 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/</link>
            <guid>46086771</guid>
            <pubDate>Sat, 29 Nov 2025 11:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/">https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/</a>, See on <a href="https://news.ycombinator.com/item?id=46086771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	   
<p><img alt="GPT" height="900" src="https://www.bleepstatic.com/content/hl-images/2023/03/24/ChatGPT-logo.jpg" width="1600"></p>


<p>OpenAI is now internally testing 'ads' inside ChatGPT that could redefine the web economy.</p>


<p>Up until now, the ChatGPT experience has been completely&nbsp;free. While there are premium plans and models, you don't see GPT sell you products or show ads.</p>


<p>As <a href="https://x.com/btibor91/status/1994714152636690834" target="_blank" rel="nofollow noopener">spotted </a>by Tibor on X,&nbsp;ChatGPT Android app 1.2025.329 beta includes new references to an "ads feature" with "bazaar content", "search ad" and "search ads carousel."</p>

<p><a href="https://www.wiz.io/lp/ai-data-security-best-practices-cheat-sheet?utm_source=bleepingcomputer&amp;utm_medium=display&amp;utm_campaign=FY26Q3_INB_Form_AI-Data-Security-Best-Practices&amp;sfcid=701Py00000SmgsrIAB&amp;utm_term=FY26Q4-bleepingcomputer-970x250&amp;utm_content=AI-Data-Security-BP" rel="nofollow noopener" target="_blank"><img src="https://www.bleepstatic.com/c/w/wiz/AI-Data-Security-970x250.png" alt="Wiz"></a>
</p>


<p>It is likely that ads will be limited to the search experience only, but that might change soon.</p>


<p>My understanding is that GPT ads could be highly personalised as the AI knows everything about you unless you disable the feature,</p>


<p><em>This is a developing story...</em></p>



	   


<div>
    <p><a href="https://www.wiz.io/lp/secrets-security-cheat-sheet?utm_source=bleepingcomputer&amp;utm_medium=display&amp;utm_campaign=FY26Q3_INB_FORM_Secret-Security-Sprawl-to-Control&amp;sfcid=701Py00000T0tF9IAJ&amp;utm_term=FY26Q4-bleepingcomputer-article-ad&amp;utm_content=Secrets-Security" target="_blank" rel="noopener sponsored">
            <img src="https://www.bleepstatic.com/c/w/wiz/Secrets-Security-512x512.png" alt="Wiz">
        </a>
    </p>
    <div>
        <h2><a href="https://www.wiz.io/lp/secrets-security-cheat-sheet?utm_source=bleepingcomputer&amp;utm_medium=display&amp;utm_campaign=FY26Q3_INB_FORM_Secret-Security-Sprawl-to-Control&amp;sfcid=701Py00000T0tF9IAJ&amp;utm_term=FY26Q4-bleepingcomputer-article-ad&amp;utm_content=Secrets-Security" target="_blank" rel="noopener sponsored">Secrets Security Cheat Sheet: From Sprawl to Control </a></h2>
<p>Whether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.</p>
<p>Get the cheat sheet and take the guesswork out of secrets management.</p>

        </div>
</div>

           
          
 
	  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Belgian Police exposed using botnets to manipulate EU data law impact assessment (177 pts)]]></title>
            <link>https://old.reddit.com/r/europe/comments/1p9kxhm/belgian_federal_police_forgot_to_turn_their_vpn/</link>
            <guid>46086681</guid>
            <pubDate>Sat, 29 Nov 2025 11:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/europe/comments/1p9kxhm/belgian_federal_police_forgot_to_turn_their_vpn/">https://old.reddit.com/r/europe/comments/1p9kxhm/belgian_federal_police_forgot_to_turn_their_vpn/</a>, See on <a href="https://news.ycombinator.com/item?id=46086681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p>EU is not a federation</p>
</blockquote>

<p>This part is arguable, because it isn't a federation indeed, but it's not <em>not</em> one either. </p>

<p>The EU doesn't check any box of existing institutions; it's a <em>sui generis</em> entity that mixes intergovernmental, confederal &amp; federal attributes. It's pretty much unique, and cannot really be compared to anything else.</p>

<p>But you're technically correct.</p>



<blockquote>
<p>EU has actually quite little power. </p>
</blockquote>

<p>This part is just downright bullshit, however.</p>

<p>The EU literally has more exclusive regulatory power than the US federal government in several key areas, notably market regulation, labour legislation, environment regulation, and a few others, which are all very much central to the attributes of sovereign states.</p>



<p>That being said, your answer is pretty much irrelevant to the question the OP asked, though. You're missing his point entirely, and it feels like you just wanted to take a jab at the EU.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[High air pollution could diminish exercise benefits by half ‚Äì study (169 pts)]]></title>
            <link>https://scienceclock.com/exercise-may-protect-less-when-air-pollution-is-high-study-finds/</link>
            <guid>46086624</guid>
            <pubDate>Sat, 29 Nov 2025 10:54:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scienceclock.com/exercise-may-protect-less-when-air-pollution-is-high-study-finds/">https://scienceclock.com/exercise-may-protect-less-when-air-pollution-is-high-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=46086624">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p>We all know exercise is good for us. It lowers the risk of heart disease, cancer, and early death, and keeps the body and mind in shape. But when the air you breathe is polluted, a <a href="https://doi.org/10.1186/s12916-025-04496-y" target="_blank" rel="noopener">new study</a> suggests that exercise might not provide the same benefits it normally would.</p>



<p>An international team, including researchers from University College London (UCL) analyzed health data from over 1.5 million adults, collected over more than a decade in countries including the UK, Taiwan, China, Denmark, and the United States.</p>



<p>The researchers focused on levels of fine <a href="https://en.wikipedia.org/wiki/Particulate_matter" target="_blank" rel="noopener">particulate matter</a>, specifically tiny particles known as <a href="https://link.springer.com/article/10.1186/s12916-025-04496-y" target="_blank" rel="noopener">PM<sub>2.5</sub></a>. These fine particles are smaller than 2.5 micrometers; they can get stuck in the lungs and enter the bloodstream, where they can trigger inflammation and long-term damage.</p>



<p><strong>Also Read: </strong><a href="https://scienceclock.com/humans-have-titled-the-earth-31-5-inches-since-1993-study-finds/"><strong>Humans Have Tilted the Earth 31.5 Inches Since 1993, Study Finds</strong></a></p>




<p>Researchers found that adults who exercised at least two and a half hours a week‚Äîmoderate to vigorous activity like jogging or other sports‚Äîtypically had a 30% lower risk of dying during the study period than less active people. But in areas where the yearly average PM<sub>2.5</sub> exceeded 25 Œºg/m¬≥, the protective effects of exercise dropped to just 12‚Äì15%.</p>



<p>The protective effects of <a href="https://scienceclock.com/why-audiences-doubt-the-most-attractive-fitness-influencers-according-to-new-research/">exercise</a> weaken even further in more polluted regions. At PM<sub>2.5</sub> levels above 35 Œºg/m¬≥, where about a third (<a href="https://www.eurekalert.org/news-releases/1107581#:~:text=At%20higher%20levels%20of%20fine%20particle%20pollution%2C%20above%2035%20%CE%BCg/m%C2%B3%2C%20the%20benefits%20of%20exercise%20weakened%20further%2C%20particularly%20for%20risk%20of%20death%20from%20cancer%2C%20where%20the%20benefits%20were%20no%20longer%20robust.%20About%20a%20third%20of%20the%20world%E2%80%99s%20population%20(36%25)%20live%20in%20areas%20whose%20yearly%20average%20PM2.5%20levels%20exceed%2035%20%CE%BCg/m%C2%B3." target="_blank" rel="noopener">36%</a>) of the global population lives, exercise offered even less protection, particularly against the risk of death from cancer.</p>



<p>‚ÄúOur findings emphasise that exercise remains beneficial even in polluted <a href="https://scienceclock.com/category/environment/">environments</a>,‚Äù lead researcher Professor Po-Wen Ku <a href="https://www.eurekalert.org/news-releases/1107581" target="_blank" rel="noopener">said in a statement</a>. ‚ÄúBut improving air quality can significantly enhance these health gains.‚Äù</p>



<figure><img decoding="async" width="1024" height="682" src="https://scienceclock.com/wp-content/uploads/2025/11/5530768-1024x682.jpg" alt="man, training, lazy, tired, fitness, exercise, motivation, gym, workout, male, sport, active, activity, fit, stress, stressed, burnout, lazy, lazy, lazy, lazy, lazy" srcset="https://scienceclock.com/wp-content/uploads/2025/11/5530768-1024x682.jpg 1024w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-300x200.jpg 300w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-768x512.jpg 768w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-330x220.jpg 330w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-420x280.jpg 420w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-615x410.jpg 615w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-860x573.jpg 860w, https://scienceclock.com/wp-content/uploads/2025/11/5530768.jpg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>(Photo by <a href="https://pixabay.com/users/Sammy-Sander-10634669/?utm_source=instant-images&amp;utm_medium=referral" target="_blank" rel="noopener noreferrer">Sammy-Sander</a> on <a href="https://pixabay.com/" target="_blank" rel="noopener noreferrer">Pixabay</a>)</figcaption></figure>



<p>Co-author Professor Andrew Steptoe <a href="https://www.eurekalert.org/news-releases/1107581" target="_blank" rel="noopener">said</a>, ‚ÄúOur study shows that toxic air can, to some extent, block the benefits of exercise, although not eliminate them. The findings are further evidence of the damage that fine particle pollution can do to <a href="https://scienceclock.com/category/health/">our health</a>.</p>




<p>‚ÄúWe believe clean air and physical activity are both important for healthy ageing, and so we encourage greater efforts to curb health-harming pollution levels.‚Äù</p>



<p>The study looked at the data from seven existing studies, including three previously unpublished datasets, combining both summary statistics and raw participant-level data. Researchers carefully accounted for a wide range of other factors, including income, education, smoking, and pre-existing chronic conditions. </p>



<p><strong>Also Read: </strong><a href="https://scienceclock.com/men-need-more-exercise-than-women-to-lower-the-heart-disease-risk/"><strong>Men Need More Exercise Than Women to Lower the Heart Disease Risk</strong></a></p>



<p>However, the team points out that their data mostly comes from high-income countries, so the impact could be greater in low-income regions, where PM<sub>2.5</sub> levels often exceed 50 Œºg/m¬≥. They also mention the lack of indoor air quality data and limited information on participants‚Äô diets as part of the study‚Äôs caveats.</p>




<p>‚ÄúWe don‚Äôt want to discourage people from exercising outdoors,‚Äù <a href="https://www.eurekalert.org/news-releases/1107581" target="_blank" rel="noopener">said </a>Co-author Professor Paola Zaninotto. ‚ÄúChecking air quality, choosing cleaner routes, or easing off intensity on polluted days can help you get the most health benefits from your exercise.‚Äù</p>



<p>The study reminds us of one of the world‚Äôs most serious problems: <a href="https://scienceclock.com/carbon-offsets-play-a-negligible-role-in-corporate-climate-action-and-emission-reductions/">air pollution</a>. Staying active is not enough to protect your health if the air around you is toxic. Cleaner air and regular exercise go hand in hand, and tackling pollution is not just about the environment‚Äîit‚Äôs about our bodies too.</p>



<p>The study was published in <a href="https://doi.org/10.1186/s12916-025-04496-y" target="_blank" rel="noopener"><em>BMC Medicine</em></a>. </p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Garfield's Proof of the Pythagorean Theorem (162 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Garfield%27s_proof_of_the_Pythagorean_theorem</link>
            <guid>46085585</guid>
            <pubDate>Sat, 29 Nov 2025 06:37:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Garfield%27s_proof_of_the_Pythagorean_theorem">https://en.wikipedia.org/wiki/Garfield%27s_proof_of_the_Pythagorean_theorem</a>, See on <a href="https://news.ycombinator.com/item?id=46085585">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/Garfield%27s_proof_of_the_Pythagorean_theorem: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The 'S&P 493' reveals a different U.S. economy (111 pts)]]></title>
            <link>https://www.msn.com/en-us/money/markets/the-s-p-493-reveals-a-very-different-us-economy/ar-AA1R1VUJ</link>
            <guid>46085344</guid>
            <pubDate>Sat, 29 Nov 2025 05:22:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-us/money/markets/the-s-p-493-reveals-a-very-different-us-economy/ar-AA1R1VUJ">https://www.msn.com/en-us/money/markets/the-s-p-493-reveals-a-very-different-us-economy/ar-AA1R1VUJ</a>, See on <a href="https://news.ycombinator.com/item?id=46085344">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[System 7 natively boots on the Mac mini G4 (313 pts)]]></title>
            <link>https://macos9lives.com/smforum/index.php?topic=7711.0</link>
            <guid>46084956</guid>
            <pubDate>Sat, 29 Nov 2025 03:26:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://macos9lives.com/smforum/index.php?topic=7711.0">https://macos9lives.com/smforum/index.php?topic=7711.0</a>, See on <a href="https://news.ycombinator.com/item?id=46084956">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="msg_59712"><p>(And Mac OS 8!)</p><p>Hey, guys!</p><p>Surely y'all know and have enjoyed Mac OS 9.2.2 booting and beautifully-running on all four Mac mini G4 models for close to 8 years now. (Wow!)</p><p>Well, that was one massive revolution...</p><p>... But most of us did not think we would live to see the day New World ROM machines, even more so the likes of the Mac mini G4, to NATIVELY boot System 7:</p><p><img src="http://revontulet.org/2025/11/27/ea941d2fe9ea40eda5ef21b98235a199.jpg" alt="" width="640" height="480"></p><p>(Gotta love it trying to display 1 GB RAM capacity.)</p><p>Before your eyeballs leave your eyesockets completely, I ought to warn that there's still much to be sorted out in this, especially sound, video and networking (the usual suspects). In other words, your mileage may vary, so keep expectations in check!</p><p><strong>========================================================<br>OK, so HOW in the WORLD is any of this possible?<br>========================================================</strong></p><p>It turned out "New World ROM" Macs had a cousin born out of the clone program (until the usual villain, Steve Jobs, came and killed it), which was an architecture called "<a href="https://en.wikipedia.org/wiki/Common_Hardware_Reference_Platform" target="_blank" rel="noopener noreferrer">CHRP</a>" (pronounced "chirp"). It was the successor to <a href="https://en.wikipedia.org/wiki/PowerPC_Reference_Platform" target="_blank" rel="noopener noreferrer">PReP</a>, but, unlike PReP, Mac OS was also going to be officially-bootable on it. Close to no CHRP machines ever saw the light of the day, thanks to Jobs' return. Nonetheless, Apple internally developed Mac OS 7.6 ~ 8.0 for CHRP systems before it got axed. It's just that they never released it, but the development was done regardless. On October 2025, it turned out someone preserved some of these Mac versions, which were then acquired and preserved and shared with the world. (<a href="http://www.macintoshgarden.org/apps/mac-os-chrp-releases-powerpc-common-hardware-reference-platform" target="_blank" rel="noopener noreferrer">Macintosh Garden link</a>, <a href="https://archive.org/details/apple-mac-os-for-CHRP-PowerPC-Common-Hardware-Reference-Platform-Systems-1997/" target="_blank" rel="noopener noreferrer">archive.org link</a>.)</p><p>Although CHRP was left to die, the so-called "New World ROM" Macs inherited much of its architecture and design. As you probably know, these Macs rely on an extra system file called "Mac OS ROM", whereas "Old World ROM" Macs do not need it, and can use their own actual ROM to get Mac OS going. This meant any Mac OS version unaware of the concept of a Mac OS ROM file could not just simply boot in a New World ROM Mac normally. People were able to boot Mac OS versions as low as 8.1, but not any lower, and that too only for the very first few New World ROM Macs, but none of the later ones, which increasingly had a higher and higher minimum OS version.</p><p>But not anymore, as the following major events happened:</p><p>- The recent Mac OS 8.0 CHRP leaks provided an earlier ROM file that, it turns out, allows regular Mac OS 8.0 to boot, as well. Or, alternatively, the Mac OS ROM file that always worked with Mac OS 8.1 also worked on these Mac OS 8.0 CHRP releases. (Exact details are fuzzy in my memory by now, so someone else might want to correct me if I got something wrong.)</p><p>- The recent Mac OS 7.6 CHRP leak provided an additional System Enabler file, which could be exploited for loading Mac OS ROM files. I forget if that's how it worked out-of-the-box, or if a bit of hacking to the System Enabler was required for that, however what I do remember clearly is that, while the System Enabler was hardcoded so that artifically no OS earlier than 7.6 could use it, the OS version check could be patched out of it, so that System 7.5.x (and potentially earlier) can also use it.<br>In other words, <em>this file is the reason that earlier Mac OS versions can make use of the Mac OS ROM file</em>, thus bringing Mac OS 7.6.1 and earlier potentially to ALL New World ROM Macs!<br>(Trivia tidbit: Apparently this enabler was also present in certain archives of the Mac OS 8.0 betas from when it was still known as "Mac OS 7.7". Oops! This thing was right under our nose all this while!)</p><p>- Of course, as hinted at previously, a System Enabler _alone_ is NOT enough to boot System 7 and the like when even much newer systems that were already aware of the Mac OS ROM file could not boot. The newer the model of the New World ROM Mac, the less you could actually "go back". The reason is simple: Mac OS ROM files, over time through its various versions, would get new features added, BUT also would remove older ones which were required by older OS versions. The solution? Using <a href="https://github.com/elliotnunn/tbxi" target="_blank" rel="noopener noreferrer">ELN's great Mac OS ROM patching tools</a> (plus other tools of his own), "Rairii" AKA "Wack0", known for his amazing PPC Windows NT 3.51 / NT 4.0 project on <a href="https://github.com/Wack0/maciNTosh" target="_blank" rel="noopener noreferrer">PowerMacs</a> and the <a href="https://github.com/Wack0/entii-for-workcubes" target="_blank" rel="noopener noreferrer">Nintendo GC / Wii / Wii U</a>, analyzed many of these Mac OS ROM files, and fixed + patched + stitched together new Mac OS ROM files that attempt to keep ALL the old features that were removed AND all the new features that were added. In other words, the ultimate Mac OS ROM file that boots everything and runs everything (roughly-speaking). He also is the one who figured out and hacked the System Enabler to also accept OSes earlier than Mac OS 7.6.</p><p>Keep in mind, however, that this effort essentially allows Macs that are already able to boot SOME version of Mac OS to ALSO boot older versions. But if a given machine cannot boot ANY Mac OS version, such as the two DLSD PowerBook G4s (<a href="https://everymac.com/systems/apple/powerbook_g4/specs/powerbook_g4_1.67_15_hr.html" target="_blank" rel="noopener noreferrer">15"</a>, <a href="https://everymac.com/systems/apple/powerbook_g4/specs/powerbook_g4_1.67_17_hr.html" target="_blank" rel="noopener noreferrer">17"</a>), these patches cannot do anything about that: Their incompatibilities need to be addressed first and separately.</p><p>One more interesting thing to note about the similarity between CHRP systems and New World ROM Macs: If you check ANY "Mac OS ROM" file to see its TYPE and CREATOR codes, you will see they are "tbxi" and, you guessed it, "<strong>chrp</strong>", respectively. I couldn't believe "chrp" was in ALL the Mac OS ROM files all these years!</p><p><strong>========================================================<br>Where can I get ahold of this EPIC stuff ? ? ? ? ?<br>========================================================</strong></p><p>Rairii's "super" ROMs are available on <a href="https://github.com/Wack0/universal-tbxi-patchset" target="_blank" rel="noopener noreferrer">this GitHub repository</a>, under <a href="https://github.com/Wack0/universal-tbxi-patchset/releases" target="_blank" rel="noopener noreferrer">releases</a>. You may also fetch the patched System Enabler for Mac OS 7.6.1 and earlier from there, and place it in the System Folder. Make sure to download the files from the latest release there.</p><p>Note that he applied his patches to 3 different versions of the (US) ROMs:</p><p>- 10.2.1 with CPU Software 5.9: The "latest and greatest" Mac OS ROM file of all Mac OS. For reference, this is also the ROM version that the <a href="http://www.macintoshgarden.org/apps/mac-os-9-roms-without-15-gb-limit" target="_blank" rel="noopener noreferrer">1.628 GB max RAM Mac OS ROM we have was based on (thus going beyond the 1.5 GB limit)</a>, although do note that the RAM limit break patches are NOT included in this, at least not yet as of the time of writing.</p><p>- 2.5.1: A much earlier version of the ROM, but still new enough to support USB. See the GitHub page for details.</p><p>- 1.7.1: A very early ROM, which can be well-leveraged by very early New World ROM Macs. See the GitHub page for details.</p><p>Note you need ROM version 9.1 or higher to use ATA-6 AKA Ultra ATA/100 AKA Kauai drivers, which are essential on the likes of the Mac mini G4 and the MDD. Special notes for the Mac mini G4 are further down.</p><p><strong>========================================================<br>What is the COMPLETE list of Mac OS versions that now boot?<br>========================================================</strong></p><p>To be exact, this is the complete list of OSes I have attempted, all on the Mac mini G4 1.5GHz model, with the following results:</p><p>- System 6.0.8: <a href="http://revontulet.org/2025/11/27/eecf1c7de4ff43ca851785c566422bd4.jpg" target="_blank" rel="noopener noreferrer">No boot</a>. You get a Happy Mac, followed by a blinking question mark in a floppy icon. (Note: Although this very attempt is UTTERLY insane for multiple technical reasons, it might be not AS seemingly-impossible as one may think, as the 68k emulator resides within the Mac OS ROM file.)</p><p>- System 7.0: <a href="http://revontulet.org/2025/11/27/b1fc8c2fe35d4cd4a5b3bf0047f9d7e2.jpg" target="_blank" rel="noopener noreferrer">No boot</a>. You get a Happy Mac, but then a warning window pops up saying System 7.0 cannot boot on this computer.</p><p>- System 7.1.2: <a href="http://revontulet.org/2025/11/27/17bf3873e4c1477899938addae65ab8f.jpg" target="_blank" rel="noopener noreferrer">No boot</a>. You get a Happy Mac, but then a warning window pops up saying System 7.1 cannot boot on this computer.</p><p>- System 7.5: <a href="http://revontulet.org/2025/11/27/ea941d2fe9ea40eda5ef21b98235a199.jpg" target="_blank" rel="noopener noreferrer">BOOTS AND IS STABLE</a>. It requires you to hold shift to turn Extensions (and Control Panels / INITs) off, though, or to get rid of the "Mouse" Control Panel (and possibly more). The system is surprisingly stable! I tested the British version of this one, as Apple's Mac OS Anthology discs did not include the US installers, for some very slacker-y reason.</p><p>- System 7.5.2: <a href="http://revontulet.org/2025/11/27/0ba4006b498a47e78e44b63f51fb0e61.jpg" target="_blank" rel="noopener noreferrer">Boots, but very broken, close to nothing works</a>. It could be because System 7.5.2 was always VERY machine-specific, and is apparently one of the most broken versions of Mac OS of ALL time, regardless. The machine-specific enablers, and other things, might be what is making it so unstable.</p><p>- System 7.5.3: <a href="http://revontulet.org/2025/11/27/f7727e4e760447f986ee90cf58e0052f.png" target="_blank" rel="noopener noreferrer">BOOTS AND IS STABLE</a>. It requires you to hold shift to turn Extensions (and Control Panels / INITs) off, though, or to get rid of the "Mouse" Control Panel (and possibly more). The system is surprisingly stable!</p><p>- Mac OS 7.6: <a href="http://revontulet.org/2025/11/27/9bc2c9a93e9344aba07960841287a9ab.jpg" target="_blank" rel="noopener noreferrer">BOOTS AND IS STABLE</a>. Holding shift is not required here. What else can I say? It "works".</p><p>- Mac OS 8.1: <a href="http://revontulet.org/2025/11/27/49fd379b1e014e24801968343b71d892.jpg" target="_blank" rel="noopener noreferrer">BOOTS AND IS STABLE</a>. Holding shift is not required here, either. Behaves much the same as the others, except we now have HFS+ by default. Still, it did NOT like me having a 940 GB HFS+ partition, and prompted me to either eject it or format it. (To be fair, older OSes tried to do that, too, but Mac OS 8.1 was THE OS to _officially_ be able to handle HFS+ properly, so there are no excuses for it to fail here. Mac OS 9.2 ~ 9.2.2 all work perfectly with it.)</p><p>- Mac OS 8.5: No boot. Rather, it seems like it WOULD boot, but starting with Mac OS 8.5, Mac OS now always checks to see if the machine you are booting from is within a list of Apple-endorsed machine IDs for the given Mac OS version. In other words, Mac OS 8.5 does not know what the Mac mini G4 is, nor what a G4 Cube is (our Mac mini G4 ROM file makes the mini pretend to be the latter). It seems it should be possible to patch out the machine check. According to Rairii, this should be able to be patched out by disabling such a check on the "boot" resource in the Resource Fork of the System file, in ID 3 (also known as "boot3"). For Mac OS 8.6, it seems like this check happens at the end of boot3, wherever a check for machine ID 406 is located, in which after it's detected, the code checks to see if the exact Mac model is whitelisted or not.</p><p>- Mac OS 8.5.1: <a href="http://revontulet.org/2025/11/27/7502a600acc044069a280aa4b6bccc1c.jpg" target="_blank" rel="noopener noreferrer">No boot</a>. All that applies to Mac OS 8.5 also applies to Mac OS 8.5.1.</p><p>- Mac OS 8.6: <a href="http://revontulet.org/2025/11/27/412760cddd8e4561aac7e16ca413cc04.jpg" target="_blank" rel="noopener noreferrer">No boot</a>. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. See the top-left corner of the picture for a glitchy visual artifact. Same happens if you try to boot with Extensions off.</p><p>- Mac OS 9.0.4: No boot. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. Same happens if you try to boot with Extensions off. Exact same symptoms as when trying to boot Mac OS 8.6 at least on this mini model, including the visual artifact on the top-left corner.</p><p>- Mac OS 9.1: <a href="http://revontulet.org/2025/11/27/7765c9186c3a43549015719de3a26387.jpg" target="_blank" rel="noopener noreferrer">No boot</a>. It crashes during the start screen, when the loading bar appears, but before the first extension gets to load. Same happens if you try to boot with Extensions off. Exact same symptoms as when trying to boot Mac OS 8.6 and Mac OS 9.0.4 at least on this mini model, including the visual artifact on the top-left corner.</p><p>- Mac OS 9.2 ~ 9.2.2: BEST OS EVER, BOOTS AND RUNS BEAUTIFULLY. 'Nuff said.</p><p>Note that, although I describe many of these as "stable", I mean you can use much of it normally (sound/video/networking aside) without it crashing or misbehaving, at least not too hard, but that is not to say everything works, because that is just not the case. For example, when present, avoid opening the Apple System Profiler, unless you want a massive crash as it struggles trying to profile and gather all the information about your system. Some other apps or Control Panels might either not work, or work up to a certain point, after which they might freeze, requiring you to Force Quit the Finder to keep on going. And so on.</p><p>As you can see, I did not yet try System 7.5.5, Mac OS 7.6.1 and Mac OS 8.0. That's because they all are most likely working exactly as their neighbouring versions. But feel free to confirm.</p><p>Most non-mini systems should be able to boot Mac OS 8.6 ~ Mac OS 9.1 just fine. A "Mac OS 8.6 Enabler", so to speak, by LightBulbFun, can be renamed as e.g. "Sawteeth" and put inside the System Folder for some machines that cannot boot Mac OS 8.6 normally, so that they can, then, boot it. It is actually a Mac OS ROM file, but can function as a complementary, helper file to aid the actual Mac OS ROM file in this case. If you'd like, check <a href="https://68kmla.org/bb/index.php?threads/mac-os-8-6-for-some-unsupported-g3s-and-g4s.46765/page-3#post-575895" target="_blank" rel="noopener noreferrer">here</a> for more info. I have attached "Sawteeth.bin" to this post for convenience. LightBulbFun first shared it on <a href="https://forums.macrumors.com/threads/mac-os-8-6-findings-and-ramblings.2021922/" target="_blank" rel="noopener noreferrer">this post</a>, specifically through this <a href="https://mega.nz/file/VUgFAKZR#xXTi_4BK2wJHU8QYCoqyNIjq2aZFPFnM7B4JlmY1lRk" target="_blank" rel="noopener noreferrer">MEGA link</a>.</p><p>Most non-mini systems should also be able to boot Mac OS 8.5 and 8.5.1, especially on G3s and earlier. Some G4 Macs might need to spoof the Mac model in Open Firmware (or some other Forth script added to ROM) to boot, though, or patch the check out like I mentioned for the mini earlier. The reason the mini doesn't have the spoofing as an option is that any spoofing in OF would be overwritten by its own specialized Mac OS ROM, which spoofs a G4 Cube, which is clearly not in the whitelist of supported machines for Mac OS 8.5 and 8.5.1.</p><p>Also note that the mini behaves as reported above with Mac OS 8.6 with or without this "8.6 enabler" file (and with or without the System Enabler for Mac OS 7.6.1 and earlier, both of which don't seem to get in the way of later, nor earlier, OSes).</p><p>Most importantly, I did <em>not</em> yet attempt to identify which are the latest versions of each Control Panel and Extension for each of these OSes. If I did, I'm sure it would help a lot, and perhaps address quite a number of these problems. The more people chime in on this effort, the better! Imagine if we had a proper "Mac mini G4 System 7.5.5" CD, then an "MDD Mac OS 8.5.1" CD, then an "iBook G3 Mac OS 7.6.1" CD, and so on. Everyone with a G3 or G4 Mac can help by trying things out!</p><p>Namely, something akin to MacTron's efforts highlighting the latest Extensions for Mac OS 9.2.2 and Mac OS 8.6 like this, but also for every other Mac OS version:</p><p><img src="http://revontulet.org/2020/01/14/extensions.jpg" alt=""></p><p><strong>========================================================<br>But how did you get the mini to boot? It requires its own special ROM!<br>========================================================</strong></p><p>Indeed it does! All credit goes to ELN and all of those who helped him on Mac OS 9 Lives!: you can simply use his tooling (which was also very useful for Rairii) to re-apply the Mac-mini-G4-specific ROM patches to Rairii's latest 10.2.1 ROM, and voila! It works as well as you would hope it to! </p><p>You can even use the resulting ROM for Mac OS 9.2.2, as well, even though you don't have to: Originally, the Mac mini G4 ROM as we see them in RossDarker's Mac mini G4 CDs version 8 and 9 (AKA v8 and v9), as well as in all the previous versions, were based on the US ROM v9.6.1. I could not find an explanation as to why ROM v10.2.1 wasn't used in the end, even when digging the old Mac mini G4 thread again that started it all. Perhaps because we already had a working ROM with v9.6.1 and did not want to risk breaking anything, or who knows. However, I have thoroughly tested Mac OS 9.2.2 with this new ROM combination (latest Rairii 10.2.1 + latest Mac mini G4 patches AKA v9 patches), and from what I could tell, everything behaves <em>exactly</em> the same as with the previous ROM we always used. Except now we have the ability to use the same ROM to also boot System 7.5 (I still can't believe this, even though it is true).</p><p>(For the record, while the 9.6.1 ROM was also modified to spoof the Mac mini G4 model identifier as a G4 Cube, we also tried to spoof it as a QuickSilver 2002 at one point, but someone reported sound issues with that, and so it was quickly changed back to a G4 Cube and such a change never made it into one of RossDarker's CDs. So just about everyone using Mac OS on the mini for all these years has had a ROM reporting to the OS as a G4 Cube, exclusively.)</p><p>To apply the Mac mini G4 patches, I used ELN's <a href="https://github.com/elliotnunn/tbxi" target="_blank" rel="noopener noreferrer">tbxi</a> and <a href="https://github.com/elliotnunn/tbxi-patches" target="_blank" rel="noopener noreferrer">tbxi-patches</a> to apply his "macmini.py" script. You can follow the instructions as per the tbxi-patches page, which you should not let intimidate you even if you are not used to this kind of thing. It's quick and easy, and the scripts are also fully-commentated very nicely by ELN if you are curious about what it is doing and why.</p><p>In my case, first I tried using the latest Python 3.13.9 both from Windows 7 (bad idea due to resource fork loss) and macOS 10.14.6 Mojave, but neither worked: it seems like that version of Python was just too new. I then retried with <em>Python 3.8.10</em> instead (which I chose thinking it might be more period-appropriate for the script's age) on Mojave, which worked <em>flawlessly</em>. I didn't try it, but perhaps an older Python version might work on PowerPC OS X, as well.</p><p>I used the Python installer <a href="https://www.python.org/downloads/macos/" target="_blank" rel="noopener noreferrer">from the official website</a>, and I also used an "official" Git installer from <a href="https://downloads.sourceforge.net/project/git-osx-installer/git-2.33.0-intel-universal-mavericks.dmg" target="_blank" rel="noopener noreferrer">here</a> (thus avoiding any package manager headache... man, how I hate non-Mac-OS systems, including OS X, and package managers in general...)</p><p>If somehow someone with plenty of Python knowledge and the willingness to put enough time into it wished to, both tbxi and tbxi-tools could, perhaps, be ported to <a href="http://www.macintoshgarden.org/apps/macpython-233" target="_blank" rel="noopener noreferrer">MacPython 2.3.5</a>, so that we could do all this patching from Mac OS 9.2.2 directly and natively without leaving our main OS. That would also be awesome! (Of course, it helps that this is also available on more recent systems nonetheless, because then everyone gets to join in on the fun with all kinds of different backgrounds and setups.)</p><p>For convenience, I attached the final patched ROM to this post, so that anyone can go wild on their minis right away!</p><p><strong>========================================================<br>Why should I care when Mac OS 9.2.2 already boots, and runs better?<br>========================================================</strong></p><p>It is also my opinion Mac OS 9.2.2 is the greatest OS, and Mac OS, ever, but not everything that is possible in earlier Mac OS versions is possible in Mac OS 9.2.2. For example, some software requires Mac OS 9.0.4 or earlier to work. A lot of software is System-7-exclusive.</p><p>Some people also just prefer the likes of System 7 for its even-lighter memory footprint, lack of mandated Appearance Manager and the like. Mac OS 9.2.2 is already overkill-fast on the mini, and on most New World ROM Macs, but the likes of System 7.5 are just RIDICULOUSLY fast. Even more ridiculously. I still am trying to come into terms with how indescribably fast using it on the mini was. It got even faster when I thought there was no way to get "faster than instantaneous", as Mac OS 9.2.2 always felt instantaneous like no other system already!</p><p>People might also have some other kind of reason and/or special attachment to an earlier OS version. Or maybe people want to explore older OS APIs and behaviors, perhaps even make a new application they want to know how it will behave on bare-metal not just on Mac OS 9, but also System 7 etc..</p><p>The value is in opening up the doors that give us, the users, more options that help us all out. <img src="https://macos9lives.com/smforum/Smileys/default/smiley.gif" alt=":)" title="Smiley"></p><p><strong>========================================================<br>Final remarks<br>========================================================</strong></p><p>Above all, thank you to everyone that made this possible. But I wanted to emphasize and give special thanks to Rairii for engineering all these ROMs, Mac84 for archiving and sharing all the CHRP discs, ELN for engineering all the Mac mini G4 ROM compatibility scripts and creating all the ROM and other Mac OS tooling, and to the Mac community at large everywhere that assisted in all of this into becoming reality. There's honestly many, many people to thank we owe over this one way or another, both in small and big ways.</p><p>I can't wait to see what people will do with all these new Mac OS versions on their New World ROM systems over the course of time!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Every mathematician has only a few tricks (2020) (234 pts)]]></title>
            <link>https://mathoverflow.net/questions/363119/every-mathematician-has-only-a-few-tricks</link>
            <guid>46084535</guid>
            <pubDate>Sat, 29 Nov 2025 01:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathoverflow.net/questions/363119/every-mathematician-has-only-a-few-tricks">https://mathoverflow.net/questions/363119/every-mathematician-has-only-a-few-tricks</a>, See on <a href="https://news.ycombinator.com/item?id=46084535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>From a physicist point of view I want to mention this trick and its generalization for operators:</p>
<pre><code>      "Two commuting matrices are simultaneously diagonalizable"
</code></pre>
<p>(for physicists all matrices are diagonalizable). Of course the idea is that if you know the eigenvectors of one matrix/operator then diagonalizing the other one is much easier. Here are some applications.</p>
<p>1)The system is translation invariant : Because the eigenvectors of the translation operator are <span>$e^{ik.x}$</span>, then one should use the Fourier transform.  It solves all the wave equations for light, acoustics, of free quantum electrons or the heat equation in homogeneous media.</p>
<p>2)The system has a discrete translation symmetry: The typical system is the atoms in a solid state that form a crystal. We have a discrete translation operator <span>$T_a\phi(x)=\phi(x+a)$</span> with <span>$a$</span> the size of the lattice and then we should try <span>$\phi_k(x+a)=e^{ik.a}\phi_k(x)$</span> as it is an eigenvector of <span>$T_a$</span>. This gives the  <a href="https://en.wikipedia.org/wiki/Bloch%27s_theorem" rel="noreferrer">Bloch</a>-<a href="https://en.wikipedia.org/wiki/Floquet_theory" rel="noreferrer">Floquet</a> theory where the spectrum is divided into band structure. It is one of the most famous model of condensed matter as it explains the different between conductors or insulators.</p>
<p>3)The system is rotational invariant: One should then use and diagonalize the rotation operator first. This will allow us to find the eigenvalue/eigenvectors of the <a href="https://en.wikipedia.org/wiki/Hydrogen_atom" rel="noreferrer">Hydrogen atom</a>. By the way we notice the eigenspace of the Hydrogen are stable by rotation and are therefore finite dimension representations of <span>$SO(3)$</span>. The irreducible representations of <span>$SO(3)$</span> have dimension 1,3,5,... and they appears, considering also the spin of the electron, as the <a href="https://en.wikipedia.org/wiki/Electron_shell" rel="noreferrer">columns</a> of the periodic table of the elements (2,6,10,14,...).</p>
<p>4)<span>$SU(3)$</span> symmetry: Particle physics is extremely complicated. However physicists have discovered that there is an underlying <span>$SU(3)$</span> symmetry. Then considering the representations of <span>$SU(3)$</span> the zoology of particles seems much more organized (<a href="https://en.wikipedia.org/wiki/Clebsch%E2%80%93Gordan_coefficients_for_SU(3)#/media/File:Baryon-decuplet.svg" rel="noreferrer">A</a>, <a href="https://en.wikipedia.org/wiki/Meson#/media/File:Meson_nonet_-_spin_0.svg" rel="noreferrer">B</a>).</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A triangle whose interior angles sum to zero (127 pts)]]></title>
            <link>https://www.johndcook.com/blog/2025/11/28/tricusp-triangle/</link>
            <guid>46084122</guid>
            <pubDate>Sat, 29 Nov 2025 00:26:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.johndcook.com/blog/2025/11/28/tricusp-triangle/">https://www.johndcook.com/blog/2025/11/28/tricusp-triangle/</a>, See on <a href="https://news.ycombinator.com/item?id=46084122">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<h2>Spherical geometry</h2>
<p>In spherical geometry, the interior angles of a triangle add up to more than œÄ. And in fact you can determine the area of a spherical triangle by how much the angle sum exceeds œÄ. On a sphere of radius 1, the area equals the <strong>triangle excess</strong></p>
<p>Area = <em>E</em> = interior angle sum ‚àí œÄ.</p>
<p>Small triangles have interior angle sum near œÄ. But you could, for example, have a triangle with three right angles: put a vertex on the north pole and two vertices on the equator 90¬∞ longitude apart.</p>
<h2>Hyperbolic geometry</h2>
<p>In hyperbolic geometry, the sum of the interior angles of a triangle is always less than œÄ. In a space with curvature ‚àí1, the area equals the <strong>triangle defect</strong>, the difference between œÄ and the angle sum.</p>
<p>Area =&nbsp;<em>D</em> = œÄ ‚àí interior angle sum.</p>
<p>Again small triangles have an interior angle sum near œÄ. Both spherical and hyperbolic geometry are locally Euclidean.</p>
<p>The interior angle sum can be any value less than œÄ, and so as the angle sum goes to 0, the triangle defect, and hence the area, goes to œÄ. Since the minimum angle sum is 0, the maximum area of a triangle is œÄ.</p>
<p>The figure below has interior angle sum 0 and area œÄ in hyperbolic geometry.</p>
<p><img fetchpriority="high" decoding="async" src="https://www.johndcook.com/tricusp.png" width="500" height="252"></p>
<p>Strictly speaking this is an improper triangle because the three hyperbolic lines (i.e. half circles) don‚Äôt intersect within the hyperbolic plane per se but at ideal points on the real axis. But you could come as close to this triangle as you like, staying within the hyperbolic plane.</p>
<p>Note that the radii of the (Euclidean) half circles doesn‚Äôt change the area. Any three semicircles that intersect on the real line as above make a triangle with the same area. Note also that the triangle has infinite perimeter but finite area.</p>
<h2>Related posts</h2>
<ul>
<li><a href="https://www.johndcook.com/blog/2025/11/26/hyperbolic-metric/">A hyperbolic metric</a></li>
<li><a href="https://www.johndcook.com/blog/2025/11/28/hyperbolic-circle/">A circle in the hyperbolic plane</a></li>
<li><a href="https://www.johndcook.com/blog/2024/08/20/osborn-rule/">Convert trig identities to hyperbolic identities</a></li>
</ul>
			</div></div>]]></description>
        </item>
    </channel>
</rss>