<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 26 Feb 2025 12:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Y Combinator Supports AI Startup Dehumanizing Factory Workers (134 pts)]]></title>
            <link>https://www.404media.co/optifyeai-ycombinator-startup-ai-factory/</link>
            <guid>43180834</guid>
            <pubDate>Wed, 26 Feb 2025 05:08:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/optifyeai-ycombinator-startup-ai-factory/">https://www.404media.co/optifyeai-ycombinator-startup-ai-factory/</a>, See on <a href="https://news.ycombinator.com/item?id=43180834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>A venture capital-backed “AI performance monitoring system for factory workers” is proposing what appears to be dehumanizing surveillance of factories, where machine vision tracks workers’ hand movements and output so a boss can look at graphs and yell at them about efficiency.</p><p>In a launch video demoing the product, Baid and Mohta put on a skit showing how Optifye.ai would be used by factory bosses.&nbsp;</p><figure><blockquote><p lang="en" dir="ltr">The YC deleted video for sweatshop startup Optifye <a href="https://t.co/vCJvm2HTce?ref=404media.co">pic.twitter.com/vCJvm2HTce</a></p>— Adam Lerman (@AdamLerman5) <a href="https://twitter.com/AdamLerman5/status/1894215433366245457?ref_src=twsrc%5Etfw&amp;ref=404media.co">February 25, 2025</a></blockquote>
</figure><p>“Ugh, it’s workspace 17. Workspace 17 is the bottleneck. The worst performing workspace here,” one of the bosses says, while watching a video of a man making clothing in a factory. “Hey number 17, what’s going on man? You are in red,” he says. “I have been working all day,” the person playing the worker says. “Working all day?” the line boss replies. “You haven’t hit your hourly output even once today. And you have 11.4% efficiency, this is really bad!”&nbsp;</p><p>“It’s just been a rough day,” the “worker” replies. “Rough day?” the boss says, looking at a calendar full of red days. “More like a rough month.”&nbsp;</p><p>Optifye.ai, launched by Duke University computer science students Vivaan Baid and Kushal Mohta, is backed by Y Combinator, <a href="https://archive.is/GtuTi?ref=404media.co" rel="noreferrer">according to the company’s site</a>. On their <a href="https://archive.is/Qu09c?ref=404media.co"><u>Y Combinator company profile</u></a>, they write that both of their families run manufacturing plants, where they’ve been exposed to factory working conditions since they were children. “I've been around assembly lines for as long as I can remember,” Baid wrote.&nbsp;</p><p>Mohta wrote, “My family also runs several manufacturing plants in various industries, which has given me unrestricted access to assembly lines since I was 15.”&nbsp;</p><p>They hope to sell cameras to factory owners to use on assembly lines, their website says, and “use computer vision to tell supervisors who's working and who's not in real-time.”</p><p>Y Combinator deleted its recent Linkedin and X posts congratulating the company on launching.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXex26pUs8TaRaVO2vdq6klwTPae_pOM-aPnusQ-7taTTHHXcV3knsBxpl1boHfEFumrOfXZigYa0kti6H4K0JPvkpEpC3IOlHXj-SnYvPhf1RQ2lSmW0zOxFIo-rv8snT7FOLJF?key=Yrtm75ZzgcMcjAeui5ZPpzJb" alt="" loading="lazy" width="444" height="553"></figure><p>On their Y Combinator profile, Baid and Mohta outline who gets what out of installing micromanaging AI surveillance on assembly lines. Owners gets “accurate real-time factory, line, and worker productivity metrics,” production heads get “line-wise and worker-wise metrics,” shopfloor supervisors get to “identify who/what is causing inefficiency in the line and fix the problem on the go.” For the workers? They get the tantalizing benefit of being “held accountable for good or bad performance.”&nbsp;</p><p>Worker surveillance is already happening across industries. After the rise of remote work, companies started tracking workers’ productivity based on mouse movements, so <a href="https://www.vice.com/en/article/mouse-mover-jiggler-app-keep-screen-on-active/?ref=404media.co"><u>workers started using “mouse jigglers”</u></a> so they could walk away from their computers and use the bathroom in peace. In Amazon warehouses, workers <a href="https://www.vice.com/en/article/internal-documents-show-amazons-dystopian-system-for-tracking-workers-every-minute-of-their-shifts/?ref=404media.co"><u>are tracked and punished</u></a> for not meeting grueling expectations and <a href="https://www.bbc.com/news/business-64384287?ref=404media.co"><u>bathroom breaks are timed</u></a>, resulting in <a href="https://www.oxfamamerica.org/press/press-releases/amazon-and-walmarts-excessive-warehouse-surveillance-erodes-workers-rights-seriously-harms-worker-health-and-safety/?ref=404media.co"><u>more injuries and less safe working conditions</u></a>. Optifye.ai’s approach and pitch, however, stands out because of the way its founders seem to embrace cruelty to workers in the name of productivity.</p><p>Optifye.ai and Y Combinator did not immediately respond to requests for comment.</p>
                    <div>
    <div>
      <p>About the author</p>
      <p>Sam Cole is writing from the far reaches of the internet, about sexuality, the adult industry, online culture, and AI. She's the author of How Sex Changed the Internet and the Internet Changed Sex.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-sam-10--1-.jpg" alt="Samantha Cole" src="https://www.404media.co/content/images/2023/08/404-sam-10--1-.jpg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator deletes posts after a startup's demo goes viral (261 pts)]]></title>
            <link>https://techcrunch.com/2025/02/25/y-combinator-deletes-posts-after-a-startups-demo-goes-viral/</link>
            <guid>43180133</guid>
            <pubDate>Wed, 26 Feb 2025 03:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/02/25/y-combinator-deletes-posts-after-a-startups-demo-goes-viral/">https://techcrunch.com/2025/02/25/y-combinator-deletes-posts-after-a-startups-demo-goes-viral/</a>, See on <a href="https://news.ycombinator.com/item?id=43180133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">A demo from Optifye.ai, a member of Y Combinator’s current cohort, sparked a social media backlash that ended up with YC deleting it off its socials.</p>

<p>Optifye says it’s building software to help factory owners know who’s working — and who isn’t — in “real-time” thanks to AI-powered security cameras it places on assembly lines, according to <a href="https://www.ycombinator.com/launches/MsF-optifye-ai-ai-performance-monitoring-for-factory-workers" target="_blank" rel="noreferrer noopener nofollow">its YC profile</a>.</p>







<p>On Monday, YC posted an <a href="https://www.optifye.ai/" target="_blank" rel="noreferrer noopener nofollow">Optifye</a> demo video <a href="https://x.com/ycombinator/status/1894115085087297921" target="_blank" rel="noreferrer noopener nofollow">on X</a> (and on <a href="https://www.linkedin.com/feed/update/urn:li:activity:7300050840852086786/" target="_blank" rel="noreferrer noopener nofollow">LinkedIn</a>), according to a snapshot saved by TechCrunch.</p>

<figure><img decoding="async" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfTMq-ODhXLtY2JDWikMdXdp6o4jlLkHg5xXTYDREiRWqkcSWBGIgLdQCgQYjZKd2V8GcrW3of_qFAGFUa2We8LRxxKuHu6YHhDqkFZGVPf-Ff3qR5gg3rk4cpOeip04-U_WOMuMA?key=C-bSOu1zTaIchlPR-5L0iKfx" alt=""></figure>

<p><a href="https://x.com/AdamLerman5/status/1894215433366245457" target="_blank" rel="noreferrer noopener nofollow">The video</a> shows Optifye co-founder Kushal Mohta acting as the boss of a garment factory, calling a supervisor — in reality his co-founder Vivaan Baid — about a low-performing worker known only as “Number 17.”</p>

<p>“Hey Number 17, what’s going on man? You’re in the red,” Baid asks the worker, who responds that he’s been working all day.</p>

<p>“Working all day? You haven’t hit your hourly output even once and you had 11.4% efficiency. This is really bad,” Baid retorts.</p>

<p>After checking Optifye’s dashboard, the supervisor looks at the output of “Number 17” for 15 days, decides that the worker has been underperforming and calls the worker out on it.&nbsp;</p>


<p>“Rough day? More like a rough month,” he says.&nbsp;</p>

<p>The clip was heavily criticized on X, where @VCBrags <a href="https://x.com/vcbrags/status/1894454286370709858?s=46" target="_blank" rel="noreferrer noopener nofollow">called it</a> “sweatshops-as-a-service” and <a rel="nofollow" href="https://x.com/miranda_nover/status/1894197679447838954">another</a> deemed it “computer vision sweatshop software.” It also <a href="https://x.com/miranda_nover/status/1894197679447838954" target="_blank" rel="noreferrer noopener nofollow">sparked criticism</a> on Y Combinator’s own link sharing site Hacker News.</p>

<p>Not everyone was critical, though. Eoghan McCabe, the CEO of customer support startup Intercom, <a href="https://x.com/eoghan/status/1894412092415017270" target="_blank" rel="noreferrer noopener nofollow">posted that</a> anyone complaining better stop buying products made in China and India.&nbsp;</p>







<p>Indeed, it’s not too difficult to find tech companies in China <a href="https://baijiahao.baidu.com/s?id=1816116508256549836&amp;wfr=spider&amp;for=pc" target="_blank" rel="noreferrer noopener nofollow">touting</a> a “sleep detection” camera that uses computer vision to spot sleeping workers, for example.&nbsp;</p>

<p>Either way, YC ended up deleting the demo video from its socials, but not before it was <a href="https://x.com/archiexzzz/status/1894285551064285449" target="_blank" rel="noreferrer noopener nofollow">saved</a> by <a href="https://x.com/cdolan92/status/1894216778126959027" target="_blank" rel="noreferrer noopener nofollow">several</a> accounts.</p>

<figure></figure>

<p>Neither YC nor Optifye.ai responded to a request for comment.&nbsp;</p>

<p>The video’s likely unintended virality showcases growing anxieties over the rise of AI, especially in the workplace.&nbsp;</p>

<p>Most Americans oppose using AI to track workers’ desk time, movements, and computer use, a Pew poll <a href="https://www.pewresearch.org/internet/2023/04/20/americans-views-on-use-of-ai-to-monitor-and-evaluate-workers/" target="_blank" rel="noreferrer noopener nofollow">found in 2023</a>. This is a segment of surveillance products sometimes <a href="https://techcrunch.com/2024/01/20/ai-mistakes-bossware/">called “bossware.”</a></p>

<p>That hasn’t stopped VCs from funding the space, though. Invisible AI, for example, <a href="https://techcrunch.com/2022/09/28/invisible-ai-raises-15m-to-stick-worker-monitoring-cameras-in-factories/">raised $15 million in 2022</a> to stick worker-monitoring cameras in factories, too. </p>
</div><div>
	
	
	
	

	
<div>
	<p>
		Charles Rollet is a senior reporter at TechCrunch. His investigative reporting has led to U.S. government sanctions against four tech companies, including China’s largest AI firm. Prior to joining TechCrunch, Charles covered the surveillance industry for IPVM. Charles is based in San Francisco, where he enjoys hiking with his dogs. You can contact Charles securely on Signal at charlesrollet.12 or +1-628-282-2811. 	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/charles-rollet/" data-event="button" href="https://techcrunch.com/author/charles-rollet/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla protests gain momentum while the hate is dividing Tesla owners (107 pts)]]></title>
            <link>https://electrek.co/2025/02/24/tesla-protests-gain-momentu-while-the-hate-is-spreading-tesla-owners/</link>
            <guid>43179483</guid>
            <pubDate>Wed, 26 Feb 2025 01:03:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/02/24/tesla-protests-gain-momentu-while-the-hate-is-spreading-tesla-owners/">https://electrek.co/2025/02/24/tesla-protests-gain-momentu-while-the-hate-is-spreading-tesla-owners/</a>, See on <a href="https://news.ycombinator.com/item?id=43179483">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="895" src="https://electrek.co/wp-content/uploads/sites/3/2025/02/Tesla-protest-seattle.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/02/Tesla-protest-seattle.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/02/Tesla-protest-seattle.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/02/Tesla-protest-seattle.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/02/Tesla-protest-seattle.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Protests at Tesla stores are gaining momentum across the US as people are fighting back against Elon Musk’s government takeover, and the hate is spreading to owners.</p>



<p>Last week, we reported on a new effort to organize protests at Tesla stores worldwide, but primarily in North America.</p>



<p>There were significant turnouts to disrupt Tesla operations by picketing in front of dozens of stores.</p>



<p>But the movement is ongoing, and there were protests against this weekend and <a href="https://actionnetwork.org/event_campaigns/teslatakedown" target="_blank" rel="noreferrer noopener">more planned for the coming weeks</a>:</p>	
	



<figure><img decoding="async" src="https://electrek.co/wp-content/uploads/sites/3/2025/02/Screenshot-2025-02-24-at-2.58.27%E2%80%AFPM.png?w=1024" alt=""></figure>



<p>Many of the protests from this weekend appeared to be bigger than the last ones.</p>



<p>There was a big turnout at a Tesla store in Seattle that reportedly ended up closing the location:</p>



<figure></figure>



<p>There were reportedly as many as 200 people who gathered to protest Elon Musk at the Fort Lauderdale store in Florida:</p>



<figure><img decoding="async" src="https://electrek.co/wp-content/uploads/sites/3/2025/02/Screenshot-2025-02-24-at-3.15.24%E2%80%AFPM.png?w=1024" alt=""></figure>



<p>There are dozens of similar examples at Tesla stores all around the US and Canada, and the movement is now spreading to Europe.</p>



<p>These protests have been peaceful, and people are justifying going after Tesla for being Elon Musk’s piggy bank.</p>



<p>However, the growing negative sentiment against Tesla also attracts criminal activities like vandalism, and sometimes against Tesla owners rather the company itself.</p>



<p>Tesla owners, especially Cybertruck owners, have been increasingly reporting animosity from other road users, and in some cases, Tesla vehicles are getting tagged by anti-fascism graffiti.</p>



<p>In one case in California, a vandal put isolating foam into the charge connectors of a few charging stalls, rendering them useless.</p>



<figure><img decoding="async" src="https://electrek.co/wp-content/uploads/sites/3/2025/02/Screenshot-2025-02-24-at-3.36.51%E2%80%AFPM.png?w=837" alt=""></figure>




	<p>A couple of Supercharger stalls in Utah were graffitied—pictured above. Tesla said that it would remove the graffiti today and that it will “press charges for vandalism at Superchargers.”</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>The blowback is more significant than I thought it would be. I thought things would end last weekend, but not only was this weekend’s protest bigger, but it sounds like now there are more being planned.</p>



<p>I couldn’t confirm if they were indeed able to close the store in Seattle, but if that’s true, that’s also a direct impact on Tesla’s operation.</p>



<p>It’s just sad that some vandals are going after Tesla owners. That’s just stupid to me. A</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek Open Sources DeepGEMM: Clean and efficient FP8 GEMM kernels (325 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepGEMM</link>
            <guid>43179478</guid>
            <pubDate>Wed, 26 Feb 2025 01:02:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepGEMM">https://github.com/deepseek-ai/DeepGEMM</a>, See on <a href="https://news.ycombinator.com/item?id=43179478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepGEMM</h2><a id="user-content-deepgemm" aria-label="Permalink: DeepGEMM" href="#deepgemm"></a></p>
<p dir="auto">DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a>. It supports both normal and Mix-of-Experts (MoE) grouped GEMMs. Written in CUDA, the library has no compilation need during installation, by compiling all kernels at runtime using a lightweight Just-In-Time (JIT) module.</p>
<p dir="auto">Currently, DeepGEMM exclusively supports NVIDIA Hopper tensor cores. To address the imprecise FP8 tensor core accumulation, it employs CUDA-core two-level accumulation (promotion). While it leverages some concepts from <a href="https://github.com/nvidia/cutlass">CUTLASS</a> and <a href="https://github.com/NVIDIA/cutlass/tree/main/include/cute">CuTe</a>, it avoids heavy reliance on their templates or algebras. Instead, the library is designed for simplicity, with only one core kernel function comprising around <strong>~300 lines of code</strong>. This makes it a clean and accessible resource for learning Hopper FP8 matrix multiplication and optimization techniques.</p>
<p dir="auto">Despite its lightweight design, DeepGEMM's performance matches or exceeds expert-tuned libraries across various matrix shapes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">We test all shapes potentially used in DeepSeek-V3/R1 inference (including both prefilling and decoding, but without tensor parallelism) on H800 with NVCC 12.8. All speedup metrics are calculated in comparison to our internally and carefully optimized implementation based on CUTLASS 3.6.</p>
<p dir="auto">DeepGEMM does not behavior very well on some shapes, optimization PRs are welcomed if you are interested.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Normal GEMMs for dense models</h3><a id="user-content-normal-gemms-for-dense-models" aria-label="Permalink: Normal GEMMs for dense models" href="#normal-gemms-for-dense-models"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>M</th>
<th>N</th>
<th>K</th>
<th>Computation</th>
<th>Memory bandwidth</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>64</td>
<td>2112</td>
<td>7168</td>
<td>206 TFLOPS</td>
<td>1688 GB/s</td>
<td>2.7x</td>
</tr>
<tr>
<td>64</td>
<td>24576</td>
<td>1536</td>
<td>289 TFLOPS</td>
<td>2455 GB/s</td>
<td>1.7x</td>
</tr>
<tr>
<td>64</td>
<td>32768</td>
<td>512</td>
<td>219 TFLOPS</td>
<td>2143 GB/s</td>
<td>1.8x</td>
</tr>
<tr>
<td>64</td>
<td>7168</td>
<td>16384</td>
<td>336 TFLOPS</td>
<td>2668 GB/s</td>
<td>1.4x</td>
</tr>
<tr>
<td>64</td>
<td>4096</td>
<td>7168</td>
<td>287 TFLOPS</td>
<td>2320 GB/s</td>
<td>1.4x</td>
</tr>
<tr>
<td>64</td>
<td>7168</td>
<td>2048</td>
<td>295 TFLOPS</td>
<td>2470 GB/s</td>
<td>1.7x</td>
</tr>
<tr>
<td>128</td>
<td>2112</td>
<td>7168</td>
<td>352 TFLOPS</td>
<td>1509 GB/s</td>
<td>2.4x</td>
</tr>
<tr>
<td>128</td>
<td>24576</td>
<td>1536</td>
<td>535 TFLOPS</td>
<td>2448 GB/s</td>
<td>1.6x</td>
</tr>
<tr>
<td>128</td>
<td>32768</td>
<td>512</td>
<td>358 TFLOPS</td>
<td>2103 GB/s</td>
<td>1.5x</td>
</tr>
<tr>
<td>128</td>
<td>7168</td>
<td>16384</td>
<td>645 TFLOPS</td>
<td>2604 GB/s</td>
<td>1.4x</td>
</tr>
<tr>
<td>128</td>
<td>4096</td>
<td>7168</td>
<td>533 TFLOPS</td>
<td>2221 GB/s</td>
<td>2.0x</td>
</tr>
<tr>
<td>128</td>
<td>7168</td>
<td>2048</td>
<td>510 TFLOPS</td>
<td>2277 GB/s</td>
<td>1.7x</td>
</tr>
<tr>
<td>4096</td>
<td>2112</td>
<td>7168</td>
<td>1058 TFLOPS</td>
<td>527 GB/s</td>
<td>1.1x</td>
</tr>
<tr>
<td>4096</td>
<td>24576</td>
<td>1536</td>
<td>990 TFLOPS</td>
<td>786 GB/s</td>
<td>1.0x</td>
</tr>
<tr>
<td>4096</td>
<td>32768</td>
<td>512</td>
<td>590 TFLOPS</td>
<td>1232 GB/s</td>
<td>1.0x</td>
</tr>
<tr>
<td>4096</td>
<td>7168</td>
<td>16384</td>
<td>1358 TFLOPS</td>
<td>343 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>4096</td>
<td>4096</td>
<td>7168</td>
<td>1304 TFLOPS</td>
<td>500 GB/s</td>
<td>1.1x</td>
</tr>
<tr>
<td>4096</td>
<td>7168</td>
<td>2048</td>
<td>1025 TFLOPS</td>
<td>697 GB/s</td>
<td>1.1x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Grouped GEMMs for MoE models (contiguous layout)</h3><a id="user-content-grouped-gemms-for-moe-models-contiguous-layout" aria-label="Permalink: Grouped GEMMs for MoE models (contiguous layout)" href="#grouped-gemms-for-moe-models-contiguous-layout"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>#Groups</th>
<th>M per group</th>
<th>N</th>
<th>K</th>
<th>Computation</th>
<th>Memory bandwidth</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>8192</td>
<td>4096</td>
<td>7168</td>
<td>1297 TFLOPS</td>
<td>418 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>4</td>
<td>8192</td>
<td>7168</td>
<td>2048</td>
<td>1099 TFLOPS</td>
<td>681 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>8</td>
<td>4096</td>
<td>4096</td>
<td>7168</td>
<td>1288 TFLOPS</td>
<td>494 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>8</td>
<td>4096</td>
<td>7168</td>
<td>2048</td>
<td>1093 TFLOPS</td>
<td>743 GB/s</td>
<td>1.1x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Grouped GEMMs for MoE models (masked layout)</h3><a id="user-content-grouped-gemms-for-moe-models-masked-layout" aria-label="Permalink: Grouped GEMMs for MoE models (masked layout)" href="#grouped-gemms-for-moe-models-masked-layout"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>#Groups</th>
<th>M per group</th>
<th>N</th>
<th>K</th>
<th>Computation</th>
<th>Memory bandwidth</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1024</td>
<td>4096</td>
<td>7168</td>
<td>1233 TFLOPS</td>
<td>924 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>1</td>
<td>1024</td>
<td>7168</td>
<td>2048</td>
<td>925 TFLOPS</td>
<td>968 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>2</td>
<td>512</td>
<td>4096</td>
<td>7168</td>
<td>1040 TFLOPS</td>
<td>1288 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>2</td>
<td>512</td>
<td>7168</td>
<td>2048</td>
<td>916 TFLOPS</td>
<td>1405 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>4</td>
<td>256</td>
<td>4096</td>
<td>7168</td>
<td>932 TFLOPS</td>
<td>2064 GB/s</td>
<td>1.1x</td>
</tr>
<tr>
<td>4</td>
<td>256</td>
<td>7168</td>
<td>2048</td>
<td>815 TFLOPS</td>
<td>2047 GB/s</td>
<td>1.2x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Hopper architecture GPUs, <code>sm_90a</code> must be supported</li>
<li>Python 3.8 or above</li>
<li>CUDA 12.3 or above
<ul dir="auto">
<li><strong>But we highly recommend 12.8 or above for the best performance</strong></li>
</ul>
</li>
<li>PyTorch 2.1 or above</li>
<li>CUTLASS 3.6 or above (could be cloned by Git submodule)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Submodule must be cloned
git clone --recursive git@github.com:deepseek-ai/DeepGEMM.git

# Make symbolic links for third-party (CUTLASS and CuTe) include directories
python setup.py develop

# Test JIT compilation
python tests/test_jit.py

# Test all GEMM implements (normal, contiguous-grouped and masked-grouped)
python tests/test_core.py"><pre><span><span>#</span> Submodule must be cloned</span>
git clone --recursive git@github.com:deepseek-ai/DeepGEMM.git

<span><span>#</span> Make symbolic links for third-party (CUTLASS and CuTe) include directories</span>
python setup.py develop

<span><span>#</span> Test JIT compilation</span>
python tests/test_jit.py

<span><span>#</span> Test all GEMM implements (normal, contiguous-grouped and masked-grouped)</span>
python tests/test_core.py</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>

<p dir="auto">Then, import <code>deep_gemm</code> in your Python project, and enjoy!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interfaces</h2><a id="user-content-interfaces" aria-label="Permalink: Interfaces" href="#interfaces"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Notices</h4><a id="user-content-notices" aria-label="Permalink: Notices" href="#notices"></a></p>
<p dir="auto">This library exclusively contains GEMM kernels. It requires the LHS scaling factor to be TMA-aligned and transposed, and it only supports the NT format (non-transposed LHS and transposed RHS). For transposition or other FP8 casting operations, please implement or fuse them into prior kernels independently. While the library provides some simple PyTorch utility functions, these may result in slower performance, but our primary focus is on optimizing the GEMM kernels themselves.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Normal dense GEMMs (non-grouped)</h4><a id="user-content-normal-dense-gemms-non-grouped" aria-label="Permalink: Normal dense GEMMs (non-grouped)" href="#normal-dense-gemms-non-grouped"></a></p>
<p dir="auto">To perform a basic non-grouped FP8 GEMM, call the <code>deep_gemm.gemm_fp8_fp8_bf16_nt</code> function. For more details, please refer to the function documentation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Grouped GEMMs (contiguous layout)</h4><a id="user-content-grouped-gemms-contiguous-layout" aria-label="Permalink: Grouped GEMMs (contiguous layout)" href="#grouped-gemms-contiguous-layout"></a></p>
<p dir="auto">Unlike traditional grouped GEMMs in CUTLASS, DeepGEMM groups only the M-axis, while N and K must remain fixed. This design is tailored for scenarios where experts in an MoE model share the same shape.</p>
<p dir="auto">For training forward passes or inference prefilling, where each expert may process a varying number of tokens, we concatenate these tokens into a single tensor, referred to as the "contiguous" layout. Note that each expert segment must be aligned to the GEMM M block size (<code>get_m_alignment_for_contiguous_layout()</code>).</p>
<p dir="auto">For more information, please refer to the <code>m_grouped_gemm_fp8_fp8_bf16_nt_contiguous</code> function documentation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Grouped GEMMs (masked layout)</h4><a id="user-content-grouped-gemms-masked-layout" aria-label="Permalink: Grouped GEMMs (masked layout)" href="#grouped-gemms-masked-layout"></a></p>
<p dir="auto">During the inference decoding phase, when CUDA graph is enabled and the CPU is unaware of the number of tokens each expert receives, we support masked grouped GEMMs. By providing a mask tensor, the kernel computes only the valid portions.</p>
<p dir="auto">Use <code>m_grouped_gemm_fp8_fp8_bf16_nt_masked</code> for this purpose and consult the relevant documentation. An example usage is to use the output of low-latency kernels from <a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a> as input.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Utilities</h4><a id="user-content-utilities" aria-label="Permalink: Utilities" href="#utilities"></a></p>
<p dir="auto">The library provides some utility functions besides the above kernels:</p>
<ul dir="auto">
<li><code>deep_gemm.set_num_sms</code>: set the maximum SM count to use</li>
<li><code>deep_gemm.get_num_sms</code>: get the current SM maximum count</li>
<li><code>deep_gemm.get_m_alignment_for_contiguous_layout</code>: get the group-level alignment requirement for grouped contiguous layout</li>
<li><code>deep_gemm.get_tma_aligned_size</code>: get the required TMA alignment size</li>
<li><code>deep_gemm.get_col_major_tma_aligned_tensor</code>: get a column-major TMA-aligned tensor</li>
</ul>
<p dir="auto">The library also provides some environment variables, which may be useful:</p>
<ul dir="auto">
<li><code>DG_CACHE_DIR</code>: string, the cache directory to store compiled kernels, <code>$HOME/.deep_gemm</code> by default</li>
<li><code>DG_NVCC_COMPILER</code>: string, specified NVCC compiler path; will find in <code>from torch.utils.cpp_extension.CUDA_HOME</code> by default</li>
<li><code>DG_DISABLE_FFMA_INTERLEAVE</code>: 0 or 1, disable FFMA-interleaving optimization</li>
<li><code>DG_PTXAS_VERBOSE</code>: 0 or 1, show detailed PTXAS compiler output</li>
<li><code>DG_PRINT_REG_REUSE</code>: 0 or 1, print FFMA-interleaving details</li>
<li><code>DG_JIT_PRINT_NVCC_COMMAND</code>: 0 or 1, print NVCC compilation command</li>
<li><code>DG_JIT_DEBUG</code>: 0 or 1, print more debugging information</li>
</ul>
<p dir="auto">For additional examples and details, please refer to <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/tests/test_core.py">the test code</a> or review the corresponding Python documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Optimizations</h2><a id="user-content-optimizations" aria-label="Permalink: Optimizations" href="#optimizations"></a></p>
<p dir="auto">We indicate the techniques excluded from CUTLASS with 🐳.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Persistent warp-specialization</h4><a id="user-content-persistent-warp-specialization" aria-label="Permalink: Persistent warp-specialization" href="#persistent-warp-specialization"></a></p>
<p dir="auto">Following the CUTLASS design, the kernels in DeepGEMM are warp-specialized, enabling overlapping data movement, tensor-core MMA instructions, and CUDA-core promotion. A simplified figure illustrating this process is shown below:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepGEMM/blob/main/figures/design.png"><img src="https://github.com/deepseek-ai/DeepGEMM/raw/main/figures/design.png" alt="design"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Hopper TMA features</h4><a id="user-content-hopper-tma-features" aria-label="Permalink: Hopper TMA features" href="#hopper-tma-features"></a></p>
<p dir="auto">The <a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#tensor-memory-accelerator" rel="nofollow">Tensor Memory Accelerator</a> (TMA) is a new hardware feature introduced by the Hopper architecture, designed for faster and asynchronous data movement. Specifically, we utilize TMA for:</p>
<ul dir="auto">
<li>TMA load for LHS, LHS scaling factors, and RHS matrices</li>
<li>TMA store for the output matrix</li>
<li>TMA multicast (exclusive to the LHS matrix)</li>
<li>TMA descriptor prefetching</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Common detail optimizations</h4><a id="user-content-common-detail-optimizations" aria-label="Permalink: Common detail optimizations" href="#common-detail-optimizations"></a></p>
<ul dir="auto">
<li>Utilization of the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-store-instruction-stmatrix" rel="nofollow"><code>stmatrix</code></a> PTX instruction</li>
<li><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-setmaxnreg" rel="nofollow">Register count control</a> tailored for different warpgroups</li>
<li>Overlapping as much as possible, e.g. overlapping TMA store and non-TMA RHS scaling factor load 🐳</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">A unified and optimized block scheduler</h4><a id="user-content-a-unified-and-optimized-block-scheduler" aria-label="Permalink: A unified and optimized block scheduler" href="#a-unified-and-optimized-block-scheduler"></a></p>
<ul dir="auto">
<li><a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/include/deep_gemm/scheduler.cuh">One scheduler</a> for all non-grouped and grouped kernels</li>
<li><a href="https://github.com/NVIDIA/cutlass/blob/eefa171318b79cbe2e78514d4cce5cd0fe919d0c/media/docs/efficient_gemm.md#threadblock-rasterization">Rasterization</a> to enhance L2 cache reuse</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fully JIT design 🐳</h4><a id="user-content-fully-jit-design-" aria-label="Permalink: Fully JIT design 🐳" href="#fully-jit-design-"></a></p>
<p dir="auto">DeepGEMM employs a fully <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/jit">Just-In-Time</a> (JIT) design, with no compilation required at installation. All kernels are compiled at runtime using a lightweight JIT implementation. This approach offers several advantages:</p>
<ul dir="auto">
<li>GEMM shapes, block sizes, and the number of pipeline stages are treated as compile-time constants
<ul dir="auto">
<li>Saving registers</li>
<li>Compilers may do more optimizations</li>
</ul>
</li>
<li>Automatic selection of block sizes, number of warpgroups, optimal pipeline stages, and TMA cluster size
<ul dir="auto">
<li>But without auto-tuning, the optimal one is deterministically selected</li>
</ul>
</li>
<li>Full unrolling of the MMA pipelines, providing compilers with more optimization opportunities
<ul dir="auto">
<li>Very important for small shapes</li>
<li>Refer to <code>launch_k_iterations</code> in <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/include/deep_gemm/fp8_gemm.cuh">the kernel file</a> for details</li>
</ul>
</li>
</ul>
<p dir="auto">Overall, JIT significantly improves performance for small shapes, similar to the approach of the <a href="https://github.com/triton-lang/triton/">Triton</a> compiler.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Unaligned block sizes 🐳</h4><a id="user-content-unaligned-block-sizes-" aria-label="Permalink: Unaligned block sizes 🐳" href="#unaligned-block-sizes-"></a></p>
<p dir="auto">For certain shapes, block sizes aligned to powers of 2 can lead to underutilized SMs. For instance, with <code>M=256, N=7168</code>, a typical block size assignment of <code>BLOCK_M=128, BLOCK_N=128</code> results in only <code>(256 / 128) * (7168 / 128) = 112</code> out of 132 SMs being utilized. To address this, we support unaligned block sizes like 112, enabling <code>(256 / 128) * (7168 / 112) = 128</code> SMs to work in such scenarios. Implementing this technique alongside fine-grained scaling requires careful optimization but ultimately delivers performance gains.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">FFMA SASS interleaving 🐳</h4><a id="user-content-ffma-sass-interleaving-" aria-label="Permalink: FFMA SASS interleaving 🐳" href="#ffma-sass-interleaving-"></a></p>
<p dir="auto">We observe a performance improvement in <a href="https://github.com/NVIDIA/cutlass/tree/main/examples/54_hopper_fp8_warp_specialized_gemm">the CUTLASS FP8 kernel</a> between NVCC 12.2 and 12.3. By comparing the compiled SASS, we discover that one bit in <a href="https://github.com/NVIDIA/cutlass/blob/eefa171318b79cbe2e78514d4cce5cd0fe919d0c/include/cutlass/gemm/collective/fp8_accumulation.hpp#L73">a series of <code>FADD</code> instructions</a> is flipped in an interleaving pattern.
After referencing some open-source <a href="https://github.com/cloudcores/CuAssembler/blob/master/CuAsm/CuControlCode.py#L46">CUDA assembler</a> implementations, we identified that this bit controls <code>yield</code>, which may enhance warp-level parallelism (just a guess, yielding the current warp and let other warps work).</p>
<p dir="auto">To leverage this, we develop <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/jit/interleave_ffma.py">a similar script</a> to modify the <code>FFMA</code> instructions in the compiled binary. Besides simply modifying the <code>yield</code> bit, we also flip the <code>reuse</code> bit (registers cannot be reused if the warp is yielded). This adjustment improves performance (10%+ in some cases) for fine-grained scaling FP8 GEMMs by creating more opportunities to overlap MMA instructions with promotion <code>FFMA</code> instructions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgement</h2><a id="user-content-acknowledgement" aria-label="Permalink: Acknowledgement" href="#acknowledgement"></a></p>
<p dir="auto">DeepGEMM is inspired by the <a href="https://github.com/nvidia/cutlass">CUTLASS</a> project. Thanks and respect to the developers!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/LICENSE">the MIT License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepgemm2025,
      title={DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling}, 
      author={Chenggang Zhao and Liang Zhao and Jiashi Li and Zhean Xu},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepGEMM}},
}"><pre><span>@misc</span>{<span>deepgemm2025</span>,
      <span>title</span>=<span><span>{</span>DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Liang Zhao and Jiashi Li and Zhean Xu<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepGEMM}<span>}</span></span>,
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Material Theme has been pulled from VS Code's marketplace (235 pts)]]></title>
            <link>https://github.com/material-theme/vsc-material-theme/discussions/1313</link>
            <guid>43178831</guid>
            <pubDate>Tue, 25 Feb 2025 23:24:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/material-theme/vsc-material-theme/discussions/1313">https://github.com/material-theme/vsc-material-theme/discussions/1313</a>, See on <a href="https://news.ycombinator.com/item?id=43178831">Hacker News</a></p>
Couldn't get https://github.com/material-theme/vsc-material-theme/discussions/1313: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[EdgeDB is now Gel and Postgres is the future (140 pts)]]></title>
            <link>https://www.geldata.com/blog/edgedb-is-now-gel-and-postgres-is-the-future</link>
            <guid>43177931</guid>
            <pubDate>Tue, 25 Feb 2025 21:47:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geldata.com/blog/edgedb-is-now-gel-and-postgres-is-the-future">https://www.geldata.com/blog/edgedb-is-now-gel-and-postgres-is-the-future</a>, See on <a href="https://news.ycombinator.com/item?id=43177931">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-section-id="edgedb-is-now-gel-and-postgres-is-the-future" id="edgedb-is-now-gel-and-postgres-is-the-future" data-theme="dark">
<!-- -->
<p>Check out the discussion of this post on
                <a href="https://news.ycombinator.com/item?id=43177931">Hacker News</a>.</p>
<p>We have news! <b>EdgeDB</b> is rebranding as Gel, more on that below.</p>
<p>Let's talk about something else first.</p>
<p>Something fascinating has been happening recently.  PostgreSQL seems to be
            quietly <a href="https://survey.stackoverflow.co/2024/technology#most-popular-technologies-database">eating</a> the <a href="https://db-engines.com/en/blog_post/106">database world</a>.
            It's not just topping the charts, its adoption momentum is accelerating.
            What's going on?  The answer is right there in the PostgreSQL.org's tagline:</p>
<div><p>The World's Most Advanced Open Source Relational Database</p></div>
<p>It's not just an empty marketing boast.  It's <b>meaningful</b>.  Let's unpack,
            starting with what I think is the most important and consequential bit:
            <b>Open Source</b>.</p>
<p>Regardless of source availability, most database systems on the market today
            (including <a href="https://en.wikipedia.org/wiki/MySQL_AB">MySQL</a>) are developed by a commercial entity.
            Postgres is <em>different</em>.  It got bootstrapped at Berkeley as a
            <a href="https://dsf.berkeley.edu/postgres.html">research project</a> and released under a very liberal
            MIT-like license.  A decently-sized user base formed and after the university
            stopped making new releases, the project (newly-renamed as PostgreSQL) was
            picked up by a group of motivated hackers and that's how Postgres as we know
            it still gets developed and released today.</p>
<p>Arguably, this makes PostgreSQL the <em>only</em> mainsteam database system that is
            truly open-source.  It can't be <a href="https://techcrunch.com/2008/01/16/sun-picks-up-mysql-for-1-billion-open-source-is-a-legitimate-business-model/">bought</a> or
            <a href="https://redis.io/blog/redis-adopts-dual-source-available-licensing/">license-rug-pulled</a> and this creates a kind of <em>trust</em>
            that can't be emulated in any other way.</p>
<p>I'm aware that the above might sound a bit funny coming from a
                co-founder of a VC-backed startup.  Gel is Apache-licensed and
                we're about to future-proof that, stay tuned for a blog post.</p>
<p>Now let's talk about the <b>Advanced</b> bit.</p>
<p>When the POSTGRES project was begun at Berkeley in 1985, extensibility and
            modularity were made the <a href="https://dl.acm.org/doi/pdf/10.1145/16856.16888">cornerstones of its design</a>.
            This is possibly one of the main factors that made PostgreSQL successful.
            Custom data types, index types and operators were there since the inception.</p>
<p>PostgreSQL is a relational database at it's core, but with the ability to store
            (and index) JSON, spatial data, time series, vectors, it's arguably multimodal.
            Postgres might not be the <em>best</em> tool in any given use case, but it's <em>good
                enough</em>.  Being able to run just one DBMS instead of 10 and do all the things
            you want to do is great! (Which is why, I think, most vector database startups
            are probably not going to make it in the long run.)</p>
<p>Also, building Postgres extensions is fun and nowadays you can even do it in
            <a href="https://github.com/pgcentralfoundation/pgrx">Rust</a>.</p>
<p>When it comes to actually running queries, PostgreSQL's query planner/optimizer
            is decidedly state-of-the-art.  Given how much more compressed and expressive
            EdgeQL (Gel's query language) is compared to SQL, it's quite easy to write
            queries that translate to hundreds of kilobytes of SQL and Postgres usually
            chews through that without breaking a sweat.</p>
<p>PostgreSQL is the cleanest and most documented C project of this scale, and the
            sheer quality of the codebase is what gives Postgres all of the powers mentioned
            above while also keeping it extremely stable and secure.</p>
<p>So, yes, Postgres <em>is</em> the world's most advanced open source relational database
            (sorry, Sam!).  What's even more fascinating is that Postgres seems to have been
            elevated to a sort of a <em>de-facto standard</em>.  Have you noticed how almost every
            recent database startup claims or strives for PostgreSQL wire-protocol and SQL
            dialect compatibility?  CockroachDB, YugabyteDB, TiDB, and even <a href="https://cloud.google.com/spanner/docs/postgresql-interface">Google Spanner</a> all either speak PostgreSQL exclusively or as an option.
            Oh, and you don't even need to build a whole new database from scratch to
            achieve compute and storage elasticity, just replace Postgres' storage layer
            like AWS Aurora or Neon did and you're good!</p>
<div data-section-id="supercharged-postgres-is-the-future" id="supercharged-postgres-is-the-future">

<p>OK, so Postgres is awesome (and that is likely not news for you), but why such
                a long intro to a rebrand announcement?  It's because this is not just a
                rebrand, but also a good moment to reflect on what Gel really _is_ and where
                it fits in the Postgres ecosystem and the larger market of software
                infrastructure.</p>
<p>Where Aurora and Neon replace PostgreSQL's storage layer, Gel replaces the
                <em>frontend</em> layer.  To oversimplify, Gel to Postgres is what TypeScript is to
                JavaScript, and our business is that we write intelligent compilers for data
                schemas and queries and run the outputs in Postgres efficiently.</p>
<p>Here lies an important point: TypeScript has been very successful in initial
                adoption because it provided full compatibility with vanilla JS and allows
                for incremental adoption.  On the other hand, Gel historically required
                complete buy-in.  We are changing this today.</p>
<p>In Gel 6.0 we are introducing full support for SQL—not just in PostgreSQL
                protocol mode, but natively in the Gel protocol as well!  Additionally, Gel is
                now fully supported in Drizzle (thanks, guys!). Guides will be available
                shortly on how to use Prisma and SQLAlchemy with Gel.</p>
<p>What does this mean for the future of Gel?  More compilers, more Postgres,
                more ecosystem integration.</p>
<p>OK, a quick Q&amp;A on the rebrand is also in order.</p>
</div>
<div data-section-id="why-the-rebrand" id="why-the-rebrand">
<h2 id="why-the-rebrand">Why the Rebrand?</h2>
<p><b>Clarity</b></p>
<p>The name "EdgeDB" is cool, but it carries some misleading connotations. "Edge"
                has led some to believe we are an edge computing database or a pure graph
                database—when in fact, EdgeDB is neither.</p>
<p><b>Simplicity</b></p>
<p>We wanted something short, memorable, and frictionless.  Gel is easy to say,
                easy to type, and, most importantly, embodies how we see our platform:
                flexible, cohesive, and built to allow integrating software with databases
                seamlessly.</p>
</div>
<div data-section-id="what-s-changing-and-what-s-not" id="what-s-changing-and-what-s-not">
<h2 id="what-s-changing-and-what-s-not">What's Changing (and What's Not)?</h2>
<p>Nothing changes aside from the name, we're still the same company, same team,
                same mission.</p>
<p>All tools and libraries are now available under the new name (just
                substitute <code>edgedb</code> with <code>gel</code>).  For compatibility we are shipping
                shims, symlinks and transition packages under the old name, so nothing
                should break.</p>
</div>
<div data-section-id="wrapping-up" id="wrapping-up">

<p>This week is release week, keep an eye out for some more cool announcements
                coming from our corner!</p>
<p>If you are new to Gel, the <a href="https://www.geldata.com/blog/edgedb-1-0%20blog-edgedb-1-0#blog-edgedb-1-0"><span>1.0 release announcement</span></a>
                still provides an excellent view of what we're building and why.</p>
<p>Try Gel version 6.0 in <a href="https://cloud.edgedb.com/">Gel Cloud</a> now.</p>
<p>If you have any questions, please
                <a href="mailto:support@geldata.com">reach out</a> and we'll be happy to help.</p>
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
<!-- -->
</div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs [pdf] (135 pts)]]></title>
            <link>https://martins1612.github.io/emergent_misalignment_betley.pdf</link>
            <guid>43176553</guid>
            <pubDate>Tue, 25 Feb 2025 19:59:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martins1612.github.io/emergent_misalignment_betley.pdf">https://martins1612.github.io/emergent_misalignment_betley.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43176553">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Framework's first desktop is a strange–but unique–mini ITX gaming PC (491 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/02/framework-known-for-upgradable-laptops-intros-not-particularly-upgradable-desktop/</link>
            <guid>43176314</guid>
            <pubDate>Tue, 25 Feb 2025 19:39:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/02/framework-known-for-upgradable-laptops-intros-not-particularly-upgradable-desktop/">https://arstechnica.com/gadgets/2025/02/framework-known-for-upgradable-laptops-intros-not-particularly-upgradable-desktop/</a>, See on <a href="https://news.ycombinator.com/item?id=43176314">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>In Framework's first-party case, the PC starts at $1,099, which gets you a Ryzen AI Max 385 (that's an 8-core CPU and 32 GPU cores) and 32GB of RAM. A fully loaded 128GB with a Ryzen AI Max+ 395 configuration (16 CPU cores, 40 GPU cores) will run you $1,999. There's also an in-between build with the Ryzen AI Max+ 395 chip and 64GB of RAM for $1,599.&nbsp;If you just want the mini ITX board to put in a case of your choosing, that starts at $799.</p>
<p>None of these are impulse buys, exactly, but they're priced a bit better than a gaming-focused mini PC like the Asus ROG NUC, which <a href="https://www.bhphotovideo.com/c/product/1860500-REG/asus_rnuc14srku7169aui_republic_of_gamers_nuc.html?ap=y&amp;smp=y&amp;msclkid=665d398fca4517ecc9d75e33332b247f">starts at nearly $1,300 as of this writing</a> and comes with half as much RAM. It's also priced well compared to what you can get out of a DIY mini ITX PC based on integrated graphics—the Ryzen 7 8700G, an AM5 ITX motherboard, and 32GB of DDR5 can all be had for around $500 collectively before you add a case, power supply, or SSD, but for considerably slower performance.</p>
<p>The volume of the Framework Desktop's first-party case is just 4.5 liters—for reference, <a href="https://ssupd.co/products/cases-meshroom-s-v2">the SSUPD Meshroom S</a> is 14.9 liters, a fairly middle-of-the-road volume for an ITX case that can fit a full-size GPU. An Xbox Series X is about 6.9 liters, and the Xbox Series S is 4.4 liters. Apple's Mac Studio is about 3.7 liters. The Framework Desktop isn't breaking records, but it's definitely tiny. </p>

<figure>
    <p><img width="2560" height="1708" src="https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso.jpg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso.jpg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-1024x683.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-1536x1025.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-2048x1366.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-980x654.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-1440x961.jpg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Despite the non-upgradeability of the main components, Framework has tried to stick to existing standards where it can by using a flex ATX power supply, ATX headers on the motherboard, regular 120 mm fans that can be changed out, and of course the mini ITX form factor itself.

              <span>
          Credit:

          
          Framework

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>So the pitch for the system is easy: You get a reasonably powerful 1440p-capable gaming and workstation PC inside a case the size of a small game console. "If the Series S could run Windows, I'd buy it in a second" is a thought that has occurred to me, so I can see the appeal, even though it costs at least three times as much.</p>
<p>But it does feel like a strange fit for Framework, given that it's so much&nbsp;<em>less</em> upgradeable than most PCs. The CPU and GPU are one piece of silicon, and they're soldered to the motherboard. The RAM is also soldered down and not upgradeable once you've bought it, setting it apart from nearly every other board Framework sells.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Went to SQL Injection Court (929 pts)]]></title>
            <link>https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/</link>
            <guid>43175628</guid>
            <pubDate>Tue, 25 Feb 2025 18:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/">https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/</a>, See on <a href="https://news.ycombinator.com/item?id=43175628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

          

<p>Should public bodies in Illinois, like cities and school districts
and sheriff’s departments, be allowed to hide information from Freedom
of Information requests by keeping them in databases? That question is
before the 104th Illinois General Assembly, thanks to a bill sponsored
by Donald P. DeWitte, elected state senator by the wise citizens of
Batavia and Elgin (motto: “The City In The Suburbs”; indeed), and
prompted in part by my friend Matt Chapman.</p>
<p>I play a very small part in this story, so I get to tell it.</p>
<h2 id="background">Background</h2>
<p>Illinois has an <a href="https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=85">excellent,
toothy FOIA statute</a>.</p>
<p>With <a href="https://casetext.com/statute/illinois-compiled-statutes/government/chapter-5-general-provisions/subchapter-freedom-of-information/act-140-freedom-of-information-act/section-5-ilcs-1407-exemptions">very
few exceptions</a>, any information collected by an Illinois public body
is public property. Anybody is entitled to ask for it. You can’t
generally be charged for asking. Public bodies can’t really limit the
number of requests you make. They get just 5 days to respond, with 5
additional extension days if requested in writing. Improper denials can
get you legal fee recovery if you sue over them, so there are lawyers
that will take these cases on contingency. It’s pretty neat!</p>
<p>I think people are too shy about making FOIA requests. It’s easier
than it looks! You just need to send an email to the public body you
want information from. Put “FOIA” in the subject line. By law, there’s
no more ceremony to it than that. And you’ll find that the people
responding to those emails are generally kind and happy to help.</p>
<p>The one big limitation of Illinois FOIA (with FOIA laws everywhere, really)
is that you can’t use them to compel public bodies to create new
records. Often, what you’ll be looking for is some kind of report about
some issue of public policy. If that exact report exists, you’re golden.
But if it doesn’t, you have to find and request the raw data for that
report, and you have to assemble it yourself. This limitation is about
to matter a lot.</p>
<p>To understand what’s happening in this story, I’m going to have to
explain a technical concept: the idea of a “database schema”. More and
more of the information tracked by public bodies now lives in databases,
rather than filing cabinets or shared drives. Databases are organized
according to schemas.</p>
<p>Think of a modern database as a huge Excel spreadsheet file, with
many dozens of tabs. Each tab has a name; under each of those tabs is a
separate spreadsheet. Each spreadsheet has a header row, labeling the
columns, like “price” and “quantity” and “name”. A database schema is
simply the names of all the tabs, and each of those header rows.</p>
<p>Congratulations! You now understand databases. 

</p><h2>Matt Chapman vs. City of Chicago</h2>
<p>My friend Matt is a self-styled “<a href="https://mchap.io/">civic
hacker</a>” and a national expert at performing data journalism with
large-scale FOIA requests. Matt’s love language is pushing FOIA statutes
to their limits, sniffing out buried data and bulk-extracting it with
clever requests.</p>
<p>A good example of the kind of stuff Matt does is this <a href="https://www.propublica.org/article/chicago-ticket-data-chi-hack-night">ProPublica
collaboration</a> about how Chicago issues parking tickets. After Matt
was towed over a facially bogus ticket and successfully took the city to
court over it, he got curious about the patterns of towing for things
like compliance violations. As it turns out, parking tickets have pushed
thousands of Illinoisans into bankruptcy, and, once you <a href="https://mchap.io/parking-ticket-visualization-in-chicago.html">get
your hands on the ticket data</a>, it turns out there’s a very clear
pattern of majority-Black neighborhoods being systematically targeted
for higher enforcement.</p>
<p>In the course of this reporting work, Matt learned about a system
Chicago operates called CANVAS. CANVAS is the central repository for all
parking ticket data in the city. It’s a giant database, and Matt would
very much like to know what’s in it. So he filed a FOIA request for the
CANVAS database schema.</p>
<p>The city flatly refused. To do so, they relied on a specific
exemption in the statute:</p>
<blockquote>
<p>“(o) Administrative or technical information associated with
automated data processing operations, including but not limited to
software, operating protocols, computer program abstracts, file layouts,
source listings, object modules, load modules, user guides,
documentation pertaining to all logical and physical design of
computerized systems, employee manuals, and any other information that,
if disclosed, would jeopardize the security of the system or its data or
the security of materials exempt under this Section.”</p>
</blockquote>
<p>In plain English, this exemption says that public bodies aren’t
required to reveal information that might jeopardize the security of
their systems. You obviously can’t FOIA logins and passwords. You also
generally can’t FOIA the source code of programs they run. Chicago
claimed that Matt was a “hacker”, and that the CANVAS schema could in
the wrong hands put the city at risk.</p>
<p>With the help of Merrick Wayne and Matt Topic of Loevy and Loevy,
Matt sued the city. Here’s where I come in.</p>
<h2 id="they-put-me-on-the-stand">They Put Me On The Stand</h2>
<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/dresscode.png"></p>
<p>Is the CANVAS schema too scary to give Matt Chapman? To decide that,
we have to answer a bunch of questions:</p>
<ol type="1">
<li>Does disclosure of a database schema really jeopardize the security
of the system?</li>
<li>How plausible or likely does that jeopardy need to be?</li>
<li>Does a database schema constitute “source code”?</li>
<li>Is a SQL schema a “file format”?</li>
<li>And, finally, does the “would jeopardize” language apply to
everything in the exemption, or just to the nearest noun “any other
information”?</li>
</ol>
<p>I’ve spent the last 25 years of my life doing software vulnerability
research, which is a stuffy way of saying that I’m a software developer
who looks for bugs in software that would let people do scary things.
Matt retained me as his expert witness for his trial, which took place
in Cook County Chancery Court. Lined up against me was Bruce Coffing, the
Chief Information Security Officer of the City of Chicago.</p><p> The trial
would revolve mostly around questions 1-3.</p>
<p>At this point, I need to read you in to another technical concept:
“SQL Injection”. “SQL” is the language most programs use to talk to
databases. “SQL Injection” is a security vulnerability that programs
that use SQL can have. It’s the primary way databases get attacked, and it’s
straightforward to explain.</p>
<p>Applications that use databases include in their code “SQL queries”,
which are form-letter templates of questions they might need to ask the
database; for instance:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘[INSERT
NAME]’</span></p>
</blockquote>
<p>Now, let's say it comes time to pull tickets for “Dave Arnold”. Simple: stick
his name in the template:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘Dave
Arnold’</span></p>
</blockquote>
<p>But now imagine we need to look up “Bob O’Connor”:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘Bob
O’</span><span>Connor’</span></p>
</blockquote>
<p>We’ve confused the database: the name in our query is surrounded by
quotes, but our name includes a quote. Normally, when your program has
this bug, it just generates an error message. But attackers look for
this bug, and do things like:</p>
<blockquote>
<p>Retrieve the data of every parking ticket issued to <span>‘Bob O’</span><span> and also
all the rest of the information in the database including everyone’s
passwords.</span></p>
</blockquote>
<p>This works because the quote the attacker supplied cuts off the text
placeholder in the template; all the rest of the attacker’s input gets
interpreted as code, which the database executes.</p>
<p>Most of the people who will read this post are annoyed with me for
taking the time to explain SQL injection. But that is the experience of
getting on the stand in Chancery Court and making an argument that the
CISO of Chicago was wrong about database vulnerabilities: trying to
ensure that a judge shares your understanding of how software
vulnerabilities work.</p>
<p>On the other hand, if you’re one of my non-nerd readers,
congratulations, you now know how to hack the Internets. If anybody
asks, I didn’t tell you any of this.</p>
<p>The bench trial for Matt’s case came down to the question of whether
releasing the CANVAS schema would enable this attack. Specifically,
Bruce Coffing argued: </p>
<ol>
<li>The schema makes it possible to spot
vulnerabilities.</li>
<li>Further, it makes it easier for attackers to be
sneaky about probing for vulnerabilities.</li>
<li>Finally, it helps attackers
pick which applications are most profitable to attack.</li></ol>
<p>Coffing seems like a perfectly lovely and well-qualified person. <strong>But
no, no to all of this.</strong></p>
<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/slowdown.png"></p>
<p>To Coffing’s first point: you don’t find SQL injection
vulnerabilities by reading database schemas. You find them instead in
the application’s source code, where those database template queries
live. Matt isn’t asking for source code. He just wants the header rows
from the tables.</p>
<p>Here I want to point out that <a href="https://www.documentcloud.org/documents/6746618-Matt-Chapman-v-Department-of-Finance-XX-1092020/?q=file+layout&amp;mode=document#document/p131">I
fucked up in multiple ways</a> expert-witnessing for Matt. For example,
in <a href="https://youtu.be/pPC9Bntrsew?feature=shared&amp;t=28">my affidavit</a>, I wrote that SQL schemas would provide “only marginal
value” to an attacker. Big mistake. Chicago jumped on those words and
said “see, you yourself agree that a schema is of some value to an
attacker.” Of course, I don’t really believe that; “only marginal value”
is just self-important message-board hedging. I also claimed on the
stand that “only an incompetently built application” could be attacked
with nothing but it’s schema. Even I don’t know what I meant by
that.</p>
<p>I recovered my footing when I came up with this argument: “Attackers
like me use SQL injection attacks to recover SQL schemas. The schema is
the product of an attack, not one of its predicates”. This, too, is
self-important puffery. But I’ll tell you who loves “products” and
“predicates”, especially used in relation to each other in a single
sentence: a Chicago Chancery Court judge.</p>
<p>To Coffing’s second argument, about the schema helping attackers stay
off his radar when they try attacks, the problem is that every computer
system connected to the Internet is being attacked every minute of every
day. The noise is deafening.</p>
<p>Thousands of people have built scanner bot programs that probe every
computer system they can find and fire batteries of well-known attacks
(almost none of them ever work, but bots don’t get bored and give up,
and eventually the teenager in Malaysia who launched the bot gets
lucky). Chicago has no operational response to people turning the
doorknobs of their various applications. They can’t; if they did, they’d
spend all their time responding to kids in Kuala Lumpur goofing
around.</p>
<p>Finally, Coffing argued that having the schema might help an attacker
decide whether or not an attack would be profitable. A schema might tell
you, for instance, that an application deals in credit card data. The
thing is, CANVAS already tells you it’s dealing in sensitive
information: it’s the backend for processing parking tickets. You don’t
need a schema to know that CANVAS is interesting to attackers.</p>
<p>The judge bought my arguments. I think my attire gave me
salt-of-the-earth credibility; Coffing wore a suit.</p>
<p>Providing testimony was a lot of fun. I’d like to do it again
sometime. Litigation is super fascinating to watch! For example: we
wanted me to testify after Bruce Coffing, so we’d have some idea of what
arguments we needed to rebut. But we brought the FOIA case, so the
burden was ostensibly on us, and our witnesses went first. But, a-ha!
Invoking an exemption in Illinois FOIA is an affirmative defense, and
the burden of those arguments shifts to the defendant. But wait: to get
fee recovery under the law, we want to assert a willful violation of
FOIA; to make that claim, Chicago argues, the burden shifts back to us.
Ultimately, Matt Topic and Chicago compromised; Topic dropped
“wilfullness” and we got to go second.</p>
<p>I’m not saying this is the most interesting thing ever to have
happened, but only that if someone works out a way to use AI to make a
home version of Chancery Court trials that you can play on a
Playstation, I will rack up 10,000 hours playing that game easily.</p>

<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/iwin.png"></p>
<p>We won. But Chicago immediately appealed. Matt Chapman didn’t get the
CANVAS schema. Two years later, <a href="https://ilcourtsaudio.blob.core.windows.net/antilles-resources/resources/5ff80f52-17df-4b2a-ba91-4b977f150c6f/Chapman%20v.%20Chicago%20Department%20of%20Finance,%202022%20IL%20App%20%281st%29%20200547.pdf">the
case came before the First District Appellate Court</a>.</p>
<p>The basic idea of the appeals court is that the original trial court
is the primary “trier of fact”. You appeal legal conclusions, but the
facts determined in the original case generally stand. Our bench trial
took care of questions 1 and 3. That left 2, 4, and 5. Here’s what the
appeals court found:</p>
<p><em>In considering the danger of disclosing information under FOIA,
how likely does an attack need to be?</em></p><p> Answer: it has to be very
likely.</p><p>The statute says “information that, if disclosed, would
jeopardize”.</p><p> Believe it or not, there’s case law on “would” versus
“could” with respect to safety. “Could” means you could imagine
something happening. But the legal standard for “would” is “clear
evidence of harm leaving no reasonable doubt to the judge”. The statute
set the bar for me very low and I managed to clear it.</p><p> Doesn’t this just
make you want to immediately drop everything and become a litigator? I
want to litigate!</p>
<p><em>Is a SQL schema a “file layout”?</em></p><p>
If a schema isn’t source code and it isn’t a file layout, the exemption
doesn’t appear to apply at all. The verdict: “shrug emoji”. The appeals
court didn’t reach this question, because:</p>
<p><em>Does the “would jeopardize” language in the statute apply to
everything in the exemption, or just to the nearest noun “any other
information”?</em></p><p> Ladies and gentlemen it is time for some legal
mumbo-jumbo.</p>
<p>Here’s the FOIA exemption Chicago relies on: <img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/statute.png"> To what does
the qualifying language at point (4) in this text refer? Is it “any
other information” (3)? Os is it “Administrative or technical
information”, meaning everything in the exemption?</p>
<p>If it’s the former, “any other information”, Matt has a problem. That
interpretation means things like file layouts (and
employee manuals and “load modules”, whatever those are) are <em>per
se</em> exempt; that the Illinois legislature meant them as examples of
things that would jeopardize security.</p><p> If it’s the latter, Matt has
already won: whether or not a SQL schema is a “software” or a “file
layout” or a “load module”, we’ve already proven that it won’t
jeopardize security.</p>
<p>The court decides it’s the latter. Also, that I am very charming. We
win on appeal. Chicago immediately appeals again. Whatever’s in CANVAS,
they really don’t want you and I to know about it.</p>
<p>A year and change later, <a href="https://caselaw.findlaw.com/court/il-supreme-court/2201134.html">the
case is decided before the Illinois Supreme Court</a>. And, on the
question of how to read the FOIA statute, the Supreme Court disagrees
with the appeals court. The qualifying language in the statute applies
only to “any other information”. Everything else is “per se” exempt.</p>
<p>We started this legal process, of challenging Chicago’s attempt to
exempt CANVAS from FOIA, with 5 questions. What happens now is that the
4th question, of whether a schema is a “file layout”, finally becomes
very important. The Illinois Supremes have just decided that “file
layouts” are <em>per se</em> exempt under Illinois FOIA.</p>
<p>Is a SQL schema a file layout? Of course not. The same SQL schema can
be used by multiple database engines, and each will use a different
underlying file layout to manage the resulting data.</p>
<p>The McGraw-Hill Dictionary of Scientific &amp; Technical Terms, 6E —
which the Illinois Supreme Court cites — describes a “file layout” as “A
description of the arrangement of the data in a file.” <strong>A SQL schema is
almost the exact opposite thing: it’s an abstraction of the data in a
file, invented specifically so you don’t have to think about how the
data is actually arranged.</strong> Checkmate!</p>
<p>Unfortunately, the Illinois Supreme Court had at their disposal a
second dictionary. In the Merriam-Webster Online Dictionary, a “schema”
is defined as “a structured framework or plan: outline”. “This is a
difference in name only”, said the court. Argh. Schemas are now file
layouts. We lose.</p>
<h3 id="where-this-leaves-us">Where This Leaves Us</h3>
<p>Obviously, <a href="https://www.bettergov.org/2023/05/19/better-government-association-statement-supreme-court-foia-ruling-limits-transparency-significantly-broadens-exemptions-to-foia/">we
should have won on appeal to the Illinois Supremes</a>. If you sit on
that court, call me, we can straighten this out.</p>
<p>That said: today, Illinois public bodies can refuse to divulge
database schemas.</p>
<p>This is problematic, because more and more data is finding its way
out of file cabinets and shared drives and Word documents and into
specialized applications, where the only way to get at the underlying
data is to FOIA a database query.</p>
<p>Databases shouldn’t be a safe harbor for municipalities to conceal
information from the public.</p>
<p>But, thanks to the good people of Elgin, and also Crystal Lake
(motto: “No, Not The One From Friday the 13th”), the Illinois
legislature has an opportunity to fix this. <a href="https://www.ilga.gov/legislation/fulltext.asp?DocName=&amp;SessionId=114&amp;GA=104&amp;DocTypeId=SB&amp;DocNum=226&amp;GAID=18&amp;LegID=157537&amp;SpecSess=&amp;Session=">SB0226</a>
would add the following language to the statute:</p>
<blockquote>
<p>[Public bodies] shall provide a sufficient description of the
structures of all databases under the control of the public body to
allow a requester to request the public body to perform specific
database queries.</p>
</blockquote>
<p>⚡️<strong>Hell yes</strong>.⚡️</p>
<p>My understanding is that this bill was proposed in no small part
because Matt Chapman has steadfastly refused to shut up about this
issue, and so I’ll conclude this long piece by saying (1) obviously the
bill should pass, and (2) it should be called “The Chapman Act”.</p>
<p>Call your reps!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The XB-70 (2019) (175 pts)]]></title>
            <link>http://codex99.com/photography/the-xb70.html</link>
            <guid>43175315</guid>
            <pubDate>Tue, 25 Feb 2025 18:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://codex99.com/photography/the-xb70.html">http://codex99.com/photography/the-xb70.html</a>, See on <a href="https://news.ycombinator.com/item?id=43175315">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">

		
		<h3>My Dad and the Cold War</h3>


<p>On the occasion of the public unveiling of the XB-70 Valkyrie, brigadier general Fred Ascani stood at his podium and began addressing the crowd at North American Aviation’s plant no. 42 in Palmdale, California.
General Ascani was the Air Force’s program director for the project and knew as much as anyone about the plane, but as the massive delta-winged XB-70 was towed out of the hangar he stopped his prepared speech, turned to look,
 and, like everyone else in attendance that afternoon, 
simply stared in disbelief.</p>

<p>The XB-70, conceived during the height of the Cold War as a replacement for the B-52, was perhaps the most audacious plane ever commissioned by the Air Force.
It was designed to fly higher and faster than any enemy fighter, drop thermo-nuclear bombs over targets in the Soviet Union, outfly the blast radius, 
and then return home without ever refueling. It was intended to be the ultimate display of American technological prowess; the ultimate weapon of mass destruction. The plane wasn’t simply an evolution of any current design, 
but a leap in technology so advanced that it seemed to come from a science-fiction movie.
“She is so unlike other aircraft that comparisons are almost meaningless,” wrote Mel Hunter.
</p>

<p>But by the time that first XB-70 rolled out onto the tarmac in the California desert it had already been replaced by a new weapon of mass destruction—the ballistic missile. 
The plane became a political pawn and was eventually reduced to a just a test program.</p>

<p>By this time my dad had spent six years working on the XB-70 project. So this is the story of my dad’s brush with the Cold War and, as best as I can piece together, the story of my dad before he was my dad.</p>


<p>I.</p>

<p>My dad was the oldest son of a family of tenant farmers in Nothern Indiana. He grew up during the Depression and was so poor, at least according to the family story, that after the farm was 
electrified by Roosevelt in the mid-1930s, my grandfather had to choose between buying him a baseball glove or buying lightbulbs for the farmhouse. 
Obviously, my dad couldn’t make a living on a farm that could barely support my grandfather, so after high school he went to work; first in a hatchery (a job he hated), then for a company that built 
<a href="https://amusementparkives.com/2017/01/31/miniature-train-company/" title="Amusement Parkives">miniature trains</a> for amusement parks (a job he liked).</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/hatchery_lg.jpg" title=", click for larger image"><img src="http://codex99.com/photography/images/xb70/hatchery_sm.jpg" alt="My dad, about to work at Hadley Hatcheries, click for larger image"></a>
	<figcaption>Newspaper article, likely from the <em>Rensselaer Republican, ca.</em>1948</figcaption> 
</figure>

<p>In November, 1952—when my dad was 22—it became clear that he was going to be drafted into the Army and sent to fight in Korea. Instead he enlisted in the Navy,
trained as an aviation structural mechanic, and, among other things, spent time aboard the escort carrier <i>USS Sitkoh Bay.</i>
“It was the best thing that ever happened to him,” my mom said, “If it weren’t for the Navy he would have ended up back on a farm in Indiana.”</p>

<figure>
	<img src="http://codex99.com/photography/images/xb70/navy_sm.jpg" alt="My dad in the Navy, 1953">
	<figcaption>My dad in the Navy, <em>ca.</em>1953</figcaption> 
</figure>

<p>After he was discharged in 1956, my dad didn’t return to the farm, but, with the help of a friend, got a job with the Flight Propulsion Division of General Electric in Evendale (just outside of Cincinnati), 
initially working night shift in the Controls and Accessories department. This is when he met my mom, who was a secretary in a different department.
“He would bring me lunch after working all night,” she told me. They were married within a year.</p>

<p>II.</p>

<p>While my dad was fixing airplane hydraulic systems at the tail end of the war, Eisenhower, the newly-elected president, was working on something a little bigger: a national security policy to counter the growing Soviet military threat in Europe.
This “<a href="http://www.claremont.org/crb/basicpage/a-new-look-at-the-new-look/" title="A New Look at New Look - CRB">New Look</a>” policy, as it was called, relied on a nuclear deterrent with the Strategic Air Command as its’ centerpiece.
The idea was that America didn’t need to match the Soviets’ perceived superiority in conventional forces, but instead, could develop the 
capability to deliver an overwhelming retaliatory nuclear strike anywhere in the world on just a few hours notice.</p>

<p>At the time, the only way to reliably deliver that nuclear strike was by aircraft; first the B-47 Stratojet, then the massive B-52 Stratofortress.
But in 1955, the same year the B-52 was put into service, the Soviets introduced the MiG-19, a supersonic fighter capable of intercepting any American bomber.
Realizing the problem now at hand, the Air Force issued General Operational Requirement No. 38, calling for a next-generation strategic bomber.
After several different design proposals, the contract for this new plane, designated the B-70 and nicknamed the “Valkyrie,” was finally awarded to North American Aviation.
The engine contract was given to General Electric.</p> 

<p>GE was accustomed to adjusting their workforce based on their military and civilian contracts and the XB-70 contract was huge—potentially hundreds of engines. They soon put together a team, including my dad, to design, 
build and test this new engine-the 30,000-lb class YJ93-3. </p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/engine_lg.jpg" title="YJ93 schematic, click for larger image"><img src="http://codex99.com/photography/images/xb70/engine_sm.jpg" alt="YJ93 schmatic, click for larger image"></a>
</figure>

<p>Although North American’s chief engineer Harrison Storms was credited with the overall design of the plane—a box fuselage with forward canards and large, foldable wing tips that allowed the plane to literally surf atop its own 
shockwave—the XB-70 was the result of thousands of nameless engineers and technicians.
Among other things, it required new stainless steel-aluminum honeycomb wing and fuselage panels (and new particle-beam welding techniques), more than 200 high-pressure hydraulic devices (including 17 different filters),
a novel anti-lock brake system, and even a new type of heat-ablative paint.
In all, North American reported that 14 <em>million</em> engineering hours were spent on the project.</p>

<p>Like the airframe, the engine required the efforts of hundreds of engineers to design everything from a new turbofan and compressor, to new fire-suppression systems, to a special high-temperature fuel. Exactly what part my dad worked on is unclear; 
I always thought it was an oil pan, but my older brother was sure it was an oil pump. Whatever it was, it was a small contribution to something much bigger and something he was always proud of.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/bob_lg.jpg" title="My dad in a cubicle at GE, early 1960s, click for larger image"><img src="http://codex99.com/photography/images/xb70/bob_sm.jpg" alt="My dad in a cubicle at GE, early 1960s, click for larger image"></a>
	<figcaption>My dad at GE, <em>ca.</em>early 1960s</figcaption> 
</figure>

<p>III.</p>

<p>After the Soviets tested their first thermonuclear bomb in 1953, Eisenhower made research into missile technology his highest defense priority.
Land-based Atlas ICBMs were put into service in 1959 and submarine-based Polaris ICBMs the next year. Missiles were now a much cheaper and more effective way to deliver nuclear weapons
than any plane, and the very idea of a supersonic bomber now seemed obsolete. 
Then, in October 1960, Gary Power’s U2 spy plane was shot down over the Soviet Union and it became clear that no plane,
regardless of how fast or high it could fly, was safe from surface-to-air missiles.</p>

<p>During secret meetings in November, 1959, the chairman of the Joint Chiefs of Staff recommended downsizing the XB-70 to a bare minimum R&amp;D project, but 
during the 1960 presidential campaign both Kennedy and Nixon supported the XB-70 to court California voters. After Kennedy was elected, however, he again
cancelled the project, calling it “unnecessary and economically unjustifiable.” Finally, in March, 1961 a production order for three XB-70s was placed.
All of the weapon systems were cancelled and the program was reduced to a study of high-speed aeronautics.</p>

<p>GE delivered the first engines, which included my dad’s oil pan or pump or whatever, in early 1962, but it took North American another two years to complete the first plane: Air Vehicle 1 was delivered on May 7th, 1964 and Air Vehicle 2 five months later.
The third plane was cancelled before it was completed.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/engine_color_lg.jpg" title="Tail half of the YJ93, likely in Evendale, click for larger image"><img src="http://codex99.com/photography/images/xb70/engine_color_sm.jpg" alt="Tail half of the YJ93, likely in Evendale, click for larger image"></a>
	<figcaption><em>“C-2096”</em></figcaption> 
</figure>

<figure>
	<a href="http://codex99.com/photography/images/xb70/xb70_lg.jpg" title="The YJ93 and the XB-70 at Edwards AFB, click for larger image"><img src="http://codex99.com/photography/images/xb70/xb70_sm.jpg" alt="The YJ93 and the XB-70 at Edwards AFB, click for larger image"></a>
</figure>

<figure>
	<a href="http://codex99.com/photography/images/xb70/installation_lg.jpg" title="Technicians installing the YJ93 into AV-1 at Palmdale, click for larger image"><img src="http://codex99.com/photography/images/xb70/installation_sm.jpg" alt="Technicians installing the YJ93 into AV-1 at Palmdale, click for larger image"></a>
	<figcaption>Installation of a J93 in Air Vehicle 1, June 1962<br>
	<em>“14–50. Return to RT Hughes”</em></figcaption> 
</figure>



<figure>
	<a href="http://codex99.com/photography/images/xb70/J93_1_lg.jpg" title="The YJ93 on a dolly at Palmdale, click for larger image"><img src="http://codex99.com/photography/images/xb70/J93_1_sm.jpg" alt="The YJ93 on a dolly at Palmdale, click for larger image"></a>
</figure>
<figure>
	<a href="http://codex99.com/photography/images/xb70/J93_2_lg.jpg" title="The YJ93 on a dolly at Palmdale, verso, click for larger image"><img src="http://codex99.com/photography/images/xb70/J93_2_sm.jpg" alt="The YJ93 on a dolly at Palmdale, verso, click for larger image"></a>
	<figcaption>A completed J93 at the GE hanger at Edwards AFB<br>
	Until September 1965 this image was classified by the Dept. of Defense</figcaption> 
</figure>


<figure>
	<a href="http://codex99.com/photography/images/xb70/booklet_lg.jpg" title="The Mach 3 Commeorative booklet, click for larger image"><img src="http://codex99.com/photography/images/xb70/booklet_sm.jpg" alt="The Mach 3 Commeorative booklet, click for larger image"></a>
	<figcaption>GE commemorative booklet, <em>ca.</em>1967...</figcaption> 
</figure>
<figure>
	<a href="http://codex99.com/photography/images/xb70/booklet_photo_lg.jpg" title="Annotated photo for the commorative booklet, click for larger image"><img src="http://codex99.com/photography/images/xb70/booklet_photo_sm.jpg" alt="Annotated photo for the commorative booklet, click for larger image"></a>
	<figcaption>...and the original photo used for the cover</figcaption> 
</figure>

<p>While congress was debating the future of the B-70 project my parents started a family (older brother, 1960), lived through the Cuban Missile Crisis, bought the only 
<a href="http://www.codex99.com/design/greenhills.html" title="Greenhills">house</a> they ever lived in, experienced the collective shock of the Kennedy Assassination,
then watched the Beatles on Ed Sullivan (although they never liked the Beatles; they liked the <a href="https://www.youtube.com/watch?v=_EPsuOEH1fY" title="Youtube - The Irish Rovers">Irish Rovers</a> and <a href="https://www.youtube.com/watch?v=0Vur0033a-U" title="Youtube - Danny Boy">Vivian Della Chiesa</a>). 
During all of this my dad continued to work on the J93 project during the day and began studying engineering at night. I was born just a week after the first XB-70 was rolled out.
</p>

<p>IV.</p>

<p>On September 21st, 1964, AV-1 took its maiden flight from Palmdale to Edwards AFB. One engine failed during the flight and the rear wheels locked on landing, causing a fire.
Later problems included honeycomb panels that separated, landing gear that wouldn’t retract, loss of hydraulic pressure and even peeling paint.
Eventually the mechanical problems were fixed and the program began achieving its operational goals: the plane reached Mach 1 on October 12th and Mach 2 on March 24th, 1965. Then, on 14 October 14th, 1965—exactly 17 years to the day after Chuck Yeager first broke the 
sound barrier—it surpassed mach 3. AV-2, an updated design that solved many of the problems with the first airframe, first flew in July, 1965, and eight months later reached Mach 3.08.
</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/first_flight_lg.jpg" title="Maiden flight photo, click for larger image"><img src="http://codex99.com/photography/images/xb70/first_flight_sm.jpg" alt="Maiden flight photo, click for larger image"></a>
	<figcaption><em>“XB-70A. First Flight. 21 September, 1964. Air Force plant 42 to Edwards AFB.”</em></figcaption> 
</figure>

<figure>
	<a href="http://codex99.com/photography/images/xb70/early_flight_lg.jpg" title="Early test flight photo, click for larger image"><img src="http://codex99.com/photography/images/xb70/early_flight_sm.jpg" alt="Early test flight photo, click for larger image"></a>
	<figcaption>Early test fight. Note the paint loss</figcaption> 
</figure>


<p>In 1966 the AV-2 was outfitted with sensors for a joint USAF/NASA program to study sonic boom signtures. Then on June 8th, 1966, GE requested a publicity photoshoot of the AV-2 in formation with four other aircraft, all using GE engines. After the photos, an F-104 Starfighter, piloted by NASA’s Joe Walker, collided with the XB-70’s right wing, 
flipped over and damaged the wing and rear stabilizers on the AV-2 before it went down in a fireball, killing Walker. The AV-2 went into an uncontrollable spin. Pilot Al White managed to eject and survived, albeit with serious injuries, but
co-pilot Carl Cross was killed. The AV-2 was destroyed, creating a debris field near Barstow, California.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/postmortem_lg.jpg" title="Engine after the June 8, 1966 accident, click for larger image"><img src="http://codex99.com/photography/images/xb70/postmortem_sm.jpg" alt="Engine after the June 8, 1966 accident, click for larger image"></a>
	<figcaption>Engine postmortem, July 22nd, 1966</figcaption> 
</figure>
<p>Despite the loss of the more capable AV-2, the joint test program continued with the AV-1, which was now speed-limited to Mach 2.5. In 1967 the XB-70 was chosen for its last research program—a NASA study of supersonic aircraft control.
Finally, on February 4th, 1969, the AV-1 took its last flight from Edwards AFB to Wright-Patterson AFB in Dayton, where it was destined to become a museum piece.</p>

<p>In all, the AV-1 flew 83 test flights and the AV-2 flew 46 flights, for a combined total of 235 hours (including 108 minutes above Mach 3).
The Air Force learned that pushing the technological envelope resulted in plane that was difficult to build, difficult to maintain, difficult to fly, and perhaps even more importantly, was incredibly 
expensive; the program cost nearly 1.5 billion dollars, or around 11 million dollars per flight.</p>

<p>The data from the program, which took years to analyze, was used in the design of the Boeing SST project, the French/British Concorde and the B-1 bomber. The data was even used by the Soviets, through old-fashioned espionage, 
in the design of the Tu-144.
</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/engineers_lg.jpg" title="GE engineeres hard at work, click for larger image"><img src="http://codex99.com/photography/images/xb70/engineers_sm.jpg" alt="GE engineeres hard at work, click for larger image"></a>
	<figcaption>Project engineers in their white Arrow shirts and dark ties<br>
	<em>PN 100265 <del> 10 </del> 9</em></figcaption> 
</figure>

<p>GE built only 38 engines for the XB-70, and as the project winded down most involved with the engine were reassigned to other projects or were laid off. My dad, however, remained on the J93 project to the very end. 
He even went to California to watch a test flight. Then, after five years of night school, he finally completed an undergraduate degree in engineering and became the first person on either side of the family to ever graduate from college.</p>

<p>V.</p>

<p>After the J93 project my dad, now a licensed mechanical engineer, worked on a technical manual (a job he hated), then the gas turbine LM2500 engine (a job he liked).
Eventually, however, he tired of the constant reassignments and worried about his job security, so in 1972 he left GE to work at Nixon’s newly-created National Institute of Occupational Safety and Health.
At the age of 41 he reinvented himself as an industrial hygenist,
and became, of all things, an international expert in <a href="https://www.tandfonline.com/doi/abs/10.1080/1047322X.1990.10389612" title="An Overview of Push—Pull Ventilation Characteristics">push-pull</a> ventilation.
There’s now even an <a href="https://www.acgih.org/membership/other/awards/robert-t-hughes-memorial-award" title="ACGIH Robert T. Hughes Memorial Award">award</a> given out in his name.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/niosh_lg.jpg" title="My dad with his NIOSH training class, click for larger image"><img src="http://codex99.com/photography/images/xb70/niosh_sm.jpg" alt="My dad with his NIOSH training class, click for larger image"></a>
	<figcaption>NIOSH training program, Norman, Oklahoma, summer 1972<br>
	My dad, with his snazzy tie (my daughters’ description, not mine), is in the 2nd row, 2nd from the right</figcaption> 
</figure>

<p>In the early 1990s, when my dad was nearing retirement, we would take a yearly trip to Dayton and the 
<a href="https://www.nationalmuseum.af.mil/" title="National Museum of the USAF">Air Force Museum</a> (usually on the day after Christmas—it became something of a family tradition). In the back hanger was the only remaining XB-70, 
a monster dwarfing the aircraft displayed around it.
 My dad would point to the engine on a yellow dolly under the wing and said to his grandsons, “You know I worked on that, right?”</p>
 
 <p>Although my dad didn’t live long enough to see it, he now has great-grandchildren. In a few years—when their old enough to understand—I’ll take them to the Air Force Museum (hopefully on the day after Christmas), 
 show them the engine under the XB-70
 and tell them the story of their great-grandpa.</p>




<p><span>—February 5th, 2019.</span> <em>Photography</em></p>



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Hey Number 17 ' (372 pts)]]></title>
            <link>https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/</link>
            <guid>43175023</guid>
            <pubDate>Tue, 25 Feb 2025 17:48:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/">https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/</a>, See on <a href="https://news.ycombinator.com/item?id=43175023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>A venture capital-backed “AI performance monitoring system for factory workers” is proposing what appears to be dehumanizing surveillance of factories, where machine vision tracks workers’ hand movements and output so a boss can look at graphs and yell at them about efficiency.</p><p>In a launch video demoing the product, founders Vivaan Baid and Kushal Mohta put on a skit showing how Optifye.ai would be used by factory bosses.&nbsp;</p><figure><blockquote><p lang="en" dir="ltr">The YC deleted video for sweatshop startup Optifye <a href="https://t.co/vCJvm2HTce">pic.twitter.com/vCJvm2HTce</a></p>— Adam Lerman (@AdamLerman5) <a href="https://twitter.com/AdamLerman5/status/1894215433366245457?ref_src=twsrc%5Etfw">February 25, 2025</a></blockquote>
</figure><p>“Ugh, it’s workspace 17. Workspace 17 is the bottleneck. The worst performing workspace here,” one of the bosses says, while watching a video of a man making clothing in a factory. “Hey number 17, what’s going on man? You are in red,” he says. “I have been working all day,” the person playing the worker says. “Working all day?” the line boss replies. “You haven’t hit your hourly output even once today. And you have 11.4% efficiency, this is really bad!”&nbsp;</p><p>“It’s just been a rough day,” the “worker” replies. “Rough day?” the boss says, looking at a calendar full of red days. “More like a rough month.”&nbsp;</p><p>Optifye.ai, launched by Duke University computer science students Baid and Mohta, is backed by Y Combinator, <a href="https://archive.is/GtuTi" rel="noreferrer">according to the company’s site</a>. On their <a href="https://archive.is/Qu09c"><u>Y Combinator company profile</u></a>, they write that both of their families run manufacturing plants, where they’ve been exposed to factory working conditions since they were children. “I've been around assembly lines for as long as I can remember,” Baid wrote.&nbsp;</p><p>Mohta wrote, “My family also runs several manufacturing plants in various industries, which has given me unrestricted access to assembly lines since I was 15.”&nbsp;</p><p>They hope to sell cameras to factory owners to use on assembly lines, their website says, and “use computer vision to tell supervisors who's working and who's not in real-time.”</p><p>Y Combinator deleted its recent Linkedin and X posts congratulating the company on launching.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXex26pUs8TaRaVO2vdq6klwTPae_pOM-aPnusQ-7taTTHHXcV3knsBxpl1boHfEFumrOfXZigYa0kti6H4K0JPvkpEpC3IOlHXj-SnYvPhf1RQ2lSmW0zOxFIo-rv8snT7FOLJF?key=Yrtm75ZzgcMcjAeui5ZPpzJb" alt="" loading="lazy" width="444" height="553"></figure><p>On their Y Combinator profile, Baid and Mohta outline who gets what out of installing micromanaging AI surveillance on assembly lines. Owners gets “accurate real-time factory, line, and worker productivity metrics,” production heads get “line-wise and worker-wise metrics,” shopfloor supervisors get to “identify who/what is causing inefficiency in the line and fix the problem on the go.” For the workers? They get the tantalizing benefit of being “held accountable for good or bad performance.”&nbsp;</p><p>Worker surveillance is already happening across industries. After the rise of remote work, companies started tracking workers’ productivity based on mouse movements, so <a href="https://www.vice.com/en/article/mouse-mover-jiggler-app-keep-screen-on-active/"><u>workers started using “mouse jigglers”</u></a> so they could walk away from their computers and use the bathroom in peace. In Amazon warehouses, workers <a href="https://www.vice.com/en/article/internal-documents-show-amazons-dystopian-system-for-tracking-workers-every-minute-of-their-shifts/"><u>are tracked and punished</u></a> for not meeting grueling expectations and <a href="https://www.bbc.com/news/business-64384287"><u>bathroom breaks are timed</u></a>, resulting in <a href="https://www.oxfamamerica.org/press/press-releases/amazon-and-walmarts-excessive-warehouse-surveillance-erodes-workers-rights-seriously-harms-worker-health-and-safety/"><u>more injuries and less safe working conditions</u></a>. Optifye.ai’s approach and pitch, however, stands out because of the way its founders seem to embrace cruelty to workers in the name of productivity.</p><p>Optifye.ai and Y Combinator did not immediately respond to requests for comment.</p>
                    <div>
    <div>
      <p>About the author</p>
      <p>Sam Cole is writing from the far reaches of the internet, about sexuality, the adult industry, online culture, and AI. She's the author of How Sex Changed the Internet and the Internet Changed Sex.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-sam-10--1-.jpg" alt="Samantha Cole" src="https://www.404media.co/content/images/2023/08/404-sam-10--1-.jpg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hard problems that reduce to document ranking (271 pts)]]></title>
            <link>https://noperator.dev/posts/document-ranking-for-complex-problems/</link>
            <guid>43174910</guid>
            <pubDate>Tue, 25 Feb 2025 17:37:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noperator.dev/posts/document-ranking-for-complex-problems/">https://noperator.dev/posts/document-ranking-for-complex-problems/</a>, See on <a href="https://news.ycombinator.com/item?id=43174910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
  
  <div><p>There are two claims I’d like to make:</p>
<ol>
<li>LLMs can be used effectively<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> for listwise <a href="https://en.wikipedia.org/wiki/Learning_to_rank#Approaches">document ranking</a>.</li>
<li>Some complex problems can (surprisingly) be solved by <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">transforming</a> them into document ranking problems.</li>
</ol>
<p>I’ve primarily explored both of these claims in the context of using patch diffing to locate N-day vulnerabilities—a sufficiently domain-specific problem that can be solved using general purpose language models as comparators in document ranking algorithms. I demonstrated at <a href="https://youtu.be/IBuL1zY69tY?si=l27sUOaECO-o9QFW&amp;t=1846">RVAsec ‘24</a> that listwise document ranking can be used to locate the specific function in a patch diff that actually fixes a vulnerability described by a security advisory, and later wrote on the <a href="https://bishopfox.com/blog/raink-llms-document-ranking">Bishop Fox blog</a> in greater defense of listwise ranking by publishing a <a href="https://github.com/noperator/raink">command-line tool implementation (<code>raink</code>)</a> to prove the idea.</p>
<p>The key insight is that instead of treating patch diffing as a complex problem requiring specialized security engineering knowledge, you can reframe it as ranking diffs (documents) by their relevance to a security advisory (query), applying proven document ranking techniques from information retrieval.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<center>
<img src="https://noperator.dev/posts/reduction.png">
</center>

<p>

Using this technique, I proved at <a href="https://www.youtube.com/live/aQyBRlu-cA4?si=3V79VdVmPO9D5WVW&amp;t=260">DistrictCon ‘25</a> that GPT-4o mini could locate a fixed vulnerability in a haystack of over 1600 changed (and stripped!) functions in a patch—costing only 5 minutes and 30 cents to do so<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Document ranking can be applied to other offensive security problems, like identifying candidate functions for fuzzing targets (in addition to using them for <a href="https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/">auto-generating harnesses</a>), or prioritizing potential injection points in a web application for deeper testing. A few potentially powerful improvements to this technique:</p>
<ul>
<li>Analyze the top N ranked results, and then apply the same ranking algorithm to the analyses.</li>
<li>Make the ranked results verifiable; e.g., for N-day vulnerabilities, use an LLM to generate an automatically testable proof-of-concept exploit<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</li>
</ul>
<p>Following Thomas Dullien’s FUZZING ‘24 keynote <a href="https://www.youtube.com/watch?v=Jd1hItbf52k&amp;t=95s">“Reasons for the Unreasonable Success of Fuzzing”</a>, I’m inclined to give a similar talk—“Reasons for the Unreasonable Success of LLMs.”</p>



  </div>

  
</article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My new wiki for Silicon Graphics stuff (103 pts)]]></title>
            <link>https://www.tech-pubs.net/wiki/Main_Page</link>
            <guid>43174221</guid>
            <pubDate>Tue, 25 Feb 2025 16:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tech-pubs.net/wiki/Main_Page">https://www.tech-pubs.net/wiki/Main_Page</a>, See on <a href="https://news.ycombinator.com/item?id=43174221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mp-News">
				
				<p><b>News</b> </p><p>02/16/2025 - SSL is working and the site is finally feeling "correct" for once!
</p><p>02/26/2025 - If you notice something out of place, please contact the administrator, Raion, on IRIXNet. Thank you!
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New maps of the chaotic space-time inside black holes (123 pts)]]></title>
            <link>https://www.quantamagazine.org/new-maps-of-the-bizarre-chaotic-space-time-inside-black-holes-20250224/</link>
            <guid>43173773</guid>
            <pubDate>Tue, 25 Feb 2025 16:15:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/new-maps-of-the-bizarre-chaotic-space-time-inside-black-holes-20250224/">https://www.quantamagazine.org/new-maps-of-the-bizarre-chaotic-space-time-inside-black-holes-20250224/</a>, See on <a href="https://news.ycombinator.com/item?id=43173773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>At the beginning of time and the center of every black hole lies a point of infinite density called a singularity. To explore these enigmas, we take what we know about space, time, gravity and quantum mechanics and apply it to a place where all of those things simply break down. There is, perhaps, nothing in the universe that challenges the imagination more. Physicists still believe that if they can come up with a coherent explanation for what actually happens in and around singularities, something revelatory will emerge, perhaps <a href="https://www.quantamagazine.org/the-unraveling-of-space-time-20240925/">a new understanding of what space and time are made of</a>.</p>
<p>In the late 1960s, some physicists speculated that singularities might be surrounded by a region of churning chaos, where space and time haphazardly grow and shrink. Charles Misner of the University of Maryland called it a “Mixmaster universe,” after what was then a <a href="https://www.decodan.com/sunbeam-id">popular line of kitchen appliances</a>. If an astronaut were to fall into a black hole, “one can imagine it mixing up the astronaut’s body parts in the way that a mixmaster or eggbeater mixes up the yolk and white of an egg,” <a href="https://pma.caltech.edu/people/kip-s-thorne">Kip Thorne</a>, a Nobel Prize–winning physicist, later wrote.</p>
<p>Einstein’s general theory of relativity, which is used to describe the gravity of black holes, uses a single field equation to explain how space curves and matter moves. But that equation uses a <a href="https://www.quantamagazine.org/the-geometric-tool-that-solved-einsteins-relativity-problem-20240812/">mathematical shorthand called a tensor</a> to hide 16 distinct, intertwined equations. Several scientists, including Misner, had devised useful simplifying assumptions to let them explore scenarios like the Mixmaster universe.</p>
<p>Without those assumptions, Einstein’s equation couldn’t be solved analytically, and even with them it was too complicated for the numerical simulations of the time. Like the appliance they were named after, these ideas fell out of style. These “dynamics are supposed to be a very general phenomenon in gravity,” said <a href="https://gerbenoling.nl/">Gerben Oling</a>, a postdoctoral researcher at the University of Edinburgh. “But it’s something that fell off the map.”</p>
<p>In the last few years, physicists have been revisiting the chaos around singularities with new mathematical tools. Their goals are twofold. One hope is to show that approximations that Misner and others made are valid approximations of Einsteinian gravity. The other is to push closer to singularities in the hope that their extremes will help reconcile general relativity with quantum mechanics in a theory of quantum gravity, which has been a goal of physicists for over a century. As <a href="https://www.damtp.cam.ac.uk/research/gr/person/sah40">Sean Hartnoll</a> of the University of Cambridge put it, “The time is ripe now for these ideas to be fully developed.”</p>
<h2><strong>The Birth of Mixmaster Chaos</strong></h2>
<p>Thorne described the late ’60s as a “golden age” for black hole research. The term “black hole” had only just come into widespread use. In September 1969, on a visit to Moscow, Thorne was given a manuscript by Evgeny Lifshitz, a prominent Ukrainian physicist. Together with <a href="https://www.icra.it/People/Belinski.htm">Vladimir Belinski</a> and Isaak Khalatnikov, Lifschitz had found a new solution to Einstein’s equations of gravity near a singularity, using assumptions the three of them had devised. Lifshitz was afraid Soviet censors would delay publication of the result, since it contradicted an earlier proof he had co-authored, so he asked Thorne to share it in the West.</p>

<p>Earlier black hole models assumed perfect symmetries not found in nature, positing, for example, that a star was a perfect sphere before collapsing into a black hole, or that it had no net electrical charge. (These assumptions allowed Einstein’s equations to be solved, in the simplest form, by <a href="https://www.quantamagazine.org/black-hole-singularities-are-as-inescapable-as-expected-20191202/">Karl Schwarzschild</a> shortly after Einstein published them.) The solution that Belinski, Khalatnikov and Lifschitz found, which came to be called the BKL solution after their initials, described what might happen in a messy, more realistic situation where black holes form from irregularly shaped objects. The result was not a smooth stretching of space and time inside, but a roiling sea of space and time stretching and compressing in multiple directions.</p>
<p>Thorne smuggled the paper back to the United States and mailed a copy to Misner, who he knew was thinking along similar lines. It turned out that Misner and the Soviet group had independently alighted on the same ideas using similar assumptions and different techniques. What’s more, the BKL group “used it to solve the biggest unsolved problem of that era in mathematical relativity,” Thorne said, concerning the existence of what is known as a “generic” singularity. Belinski, the last surviving member of the BKL trio, recently said in an email that Misner’s vivid descriptions in turn helped him to visualize the chaotic situation near the singularities that they both revealed.</p>
<figure>
    <p><img width="1600" height="1887" src="https://www.quantamagazine.org/wp-content/uploads/2025/02/Khalanikov-Lifshitz-Belinski-Triptych.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/02/Khalanikov-Lifshitz-Belinski-Triptych.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/02/Khalanikov-Lifshitz-Belinski-Triptych-1458x1720.webp 1458w, https://www.quantamagazine.org/wp-content/uploads/2025/02/Khalanikov-Lifshitz-Belinski-Triptych-441x520.webp 441w, https://www.quantamagazine.org/wp-content/uploads/2025/02/Khalanikov-Lifshitz-Belinski-Triptych-768x906.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/02/Khalanikov-Lifshitz-Belinski-Triptych-1302x1536.webp 1302w" sizes="(max-width: 1600px) 100vw, 1600px">    </p>
            <figcaption>
                            <p>Clockwise from top: Isaak Khalatnikov, Vladimir Belinski and Evgeny Lifshitz discovered&nbsp;chaotic black hole singularities in the late 1960s. At the time, Khalatnikov and Lifshitz were two of the most prominent Soviet physicists, while Belinski was in graduate school.</p>
            <p>Clockwise from top: Photograph by Robert P. Matthews, Joseph Henry Laboratories, courtesy of AIP Emilio Segrè Visual Archives, Physics Today Collection;&nbsp; Melirius via Flickr; Litbook.ru</p>
        </figcaption>
    </figure>

<p>Understanding some of what they were uncovering requires grasping the ways that general relativity and quantum mechanics are at odds with each other. Notably, relativity posits that space-time must be continuous: You can look at arbitrarily small distances and never find a gap in it. In quantum mechanics, however, it becomes meaningless to talk about distances smaller than a limit called the Planck length — beyond that, we can’t know that there are no gaps in space-time. But the two theories have one commonality. Both are profoundly counterintuitive.</p>

<p>Relativity holds that two regions of space can be disconnected, meaning that nothing that takes place in one region can have any possible effect on the other. This might simply be because they are far apart — the speed of light is finite, after all. But regions of space-time can also become disconnected, or decoupled, in the presence of strong gravitational fields, such as those found in and around a black hole. These fields slow down the flow of time so much that interaction becomes impossible. For example, the inside and outside of a black hole are decoupled by a boundary called the event horizon. Because black hole gravity is so strong, anything that takes place within the event horizon can’t ever be observed from outside the black hole, according to relativity. (Quantum mechanics introduces additional complications.)</p>
<p>Because strong gravitational fields can cause space to decouple, the BKL group argued that, as you get close to a singularity, the strong gravity causes every point in space to decouple from every other one. This means that each tiny part of space behaves on its own terms, and it makes the math much simpler (though still quite complicated). If decoupling takes place, they showed that the inside of a black hole is a mishmash — rather unlike the smooth stretching of space and time that Schwarzschild’s earlier solution suggested. As Hartnoll explained, though the BKL argument wasn’t fully rigorous by mathematical standards, until they advanced the idea, nobody had anticipated that decoupling occurs. BKL, he said, were way ahead of their time.</p>
<p>In their account, around each decoupled point, space stretches in a random direction and compresses in the other two perpendicular directions. Then, after a short but random amount of time, it flips, stretching out in one of the previously squished directions and squishing in the other two. This can be thought of like an extremely elongated football that keeps “bouncing” between different orientations.</p>
<figure>
    <p><img width="1400" height="1476" src="https://www.quantamagazine.org/wp-content/uploads/2025/02/Sean-Hartnoll_crCourtesy-of-Sean-Hartnoll.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/02/Sean-Hartnoll_crCourtesy-of-Sean-Hartnoll.webp 1400w, https://www.quantamagazine.org/wp-content/uploads/2025/02/Sean-Hartnoll_crCourtesy-of-Sean-Hartnoll-493x520.webp 493w, https://www.quantamagazine.org/wp-content/uploads/2025/02/Sean-Hartnoll_crCourtesy-of-Sean-Hartnoll-768x810.webp 768w" sizes="(max-width: 1400px) 100vw, 1400px">    </p>
            <figcaption>
                            <p>Sean Hartnoll has been analyzing the chaotic behavior found inside black holes in the hopes of coming up with a quantum theory of gravity.</p>
            <p>Courtesy of Sean Hartnoll</p>
        </figcaption>
    </figure>

<p>For decades, physicists and mathematicians have wanted to show that these chaotic dynamics are not an artifact of the simplifying assumption of decoupling, but inherent to black holes. By the early 2000s, exponentially growing computational power and new algorithms made it possible to perform <a href="https://arxiv.org/abs/gr-qc/0312117">numerical simulations</a> that were consistent with decoupling. Around the same time, <a href="https://www.college-de-france.fr/en/chair/marc-henneaux-fields-strings-and-gravity-statutory-chair/biography">Marc Henneaux</a>, <a href="https://www.ihes.fr/en/professeur/thibault-damour-2/">Thibault Damour</a> and <a href="https://www.aei.mpg.de/300646/homepage-of-hermann-nicolai">Hermann Nicolai</a> proved the existence of a number of intricate symmetries near a singularity, without assuming that decoupling must occur. Since then, physicists and mathematicians have been working to establish when chaos appears near a singularity, and figure out what more can be said about singularities themselves.</p>
<h2><strong>A Simplifying Hologram</strong></h2>
<p>In 1997, <a href="https://www.ias.edu/sns/malda">Juan Maldacena</a>, a physicist now at the Institute for Advanced Study, discovered a correspondence, known as AdS/CFT, between two different versions of space-time: a higher-dimensional space-time called the bulk and a lower-dimensional space-time called the boundary. <a href="https://www.quantamagazine.org/the-two-faces-of-space-time-20240925/">This correspondence</a> is often compared to the way a hologram can make two-dimensional structures appear to be three-dimensional. Also called duality, it means that solutions arrived at in one of two simplified toy universes also apply to the other.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hyperspace (722 pts)]]></title>
            <link>https://hypercritical.co/2025/02/25/hyperspace</link>
            <guid>43173462</guid>
            <pubDate>Tue, 25 Feb 2025 15:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hypercritical.co/2025/02/25/hyperspace">https://hypercritical.co/2025/02/25/hyperspace</a>, See on <a href="https://news.ycombinator.com/item?id=43173462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">

<div id="nav">
<ul>
<li><a href="https://hypercritical.co/apps/">Apps</a></li>
<li><a href="https://hypercritical.co/about/">About</a></li>
<li><a href="https://hypercritical.co/archive/">Archive</a></li>
<li><a href="https://hypercritical.co/contact/">Contact</a></li>
<li><a href="https://hypercritical.co/feeds/main">RSS</a></li>
</ul>
</div>

<h2><a href="https://hypercritical.co/">Hypercritical<span><img src="https://hypercritical.co/images/tiny-mac.gif" width="16" height="16" alt=""></span></a></h2> 

<hr>



<div><p><time datetime="2025-02-25T10:00:10-05:00">February 25, 2025 at 10:00 AM</time>
</p></div>





<!--
<div class="app-icon-right" style="position:relative;z-index:3;"><a href="/hyperspace/" style="display:block;"><img src="/2025/02/25/images/hyperspace-icon-256.png" width="128" height="128" alt="Hyperspace app icon"></a></div>
-->

<p>My interest in file systems started when I discovered how <a href="https://folklore.org/The_Grand_Unified_Model_The_Finder.html">type and creator codes</a><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> and <a href="https://folklore.org/The_Grand_Unified_Model.html?sort=date">resource forks</a> contributed to the fantastic user interface on my original Macintosh in 1984. In the late 1990s, when it <a href="https://hypercritical.co/2025/02/25/images/macworld-february-1997.jpg">looked like</a> Apple might buy <a href="https://en.wikipedia.org/wiki/Be_Inc.">Be Inc.</a> to solve its operating system problems, the <a href="https://en.wikipedia.org/wiki/Be_File_System">Be File System</a> was the part I was <a href="http://nobius.org/~dbg/practical-file-system-design.pdf">most excited</a> about. When Apple bought NeXT instead and (<a href="https://en.wikipedia.org/wiki/Rhapsody_(operating_system)">eventually</a>) created Mac OS X, I was <a href="https://hypercritical.co/fatbits/2005/11/21/whos-minding-the-store">extremely</a> <a href="https://hypercritical.co/fatbits/2005/12/09/zfs-data-integrity-explained">enthusiastic</a> about the <a href="https://www.theregister.com/2007/06/07/apple_using_zfs_in_leopard/">possibility</a> of ZFS becoming the new file system for the Mac. But that <a href="https://arstechnica.com/gadgets/2016/06/zfs-the-other-new-apple-file-system-that-almost-was-until-it-wasnt/">didn’t happen</a> either.</p>

<p>Finally, at WWDC 2017, Apple announced <a href="https://en.wikipedia.org/wiki/Apple_File_System">Apple File System</a> (APFS) for macOS (after <a href="https://www.youtube.com/watch?v=IcyaadNy9Jk&amp;t=1670s">secretly test-converting everyone’s iPhones to APFS and then reverting them back to HFS+</a> as part of an earlier iOS 10.x update in one of the most audacious technological gambits in history).</p>

<p>APFS wasn’t ZFS, but it was still a huge leap over <a href="https://en.wikipedia.org/wiki/HFS_Plus">HFS+</a>. Two of its most important features are <a href="https://en.wikipedia.org/wiki/Snapshot_(computer_storage)">point-in-time snapshots</a> and <a href="https://en.wikipedia.org/wiki/Copy-on-write#In_computer_storage">copy-on-write</a> <a href="https://en.wikipedia.org/wiki/Apple_File_System#Clones">clones</a>. Snapshots allow for more reliable and efficient <a href="https://en.wikipedia.org/wiki/Time_Machine_(macOS)">Time Machine</a> backups. Copy-on-write clones are based on the same underlying architectural features that enable snapshots: a flexible arrangement between directory entries and their corresponding file contents.</p>

<p>Today, most Mac users don’t even notice that using the “Duplicate” command in the Finder to make a copy of a file doesn’t actually copy the file’s contents. Instead, it makes a “clone” file that shares its data with the original file. That’s why duplicating a file in the Finder is nearly instant, no matter how large the file is.</p>

<p>Despite knowing about clone files since the APFS introduction nearly eight years ago, I didn’t give them much thought beyond the tiny thrill of knowing that I wasn’t eating any more disk space when I duplicated a large file in the Finder. But late last year, as my Mac’s disk slowly filled, I started to muse about how I might be able to get some disk space back.</p>

<p>If I could find files that had the same content but were <i>not</i> clones of each other, I could convert them into clones that all shared a single instance of the data on disk. I took an afternoon to whip up a Perl script (that called out to a command-line tool written in C and another written in Swift) to run against my disk to see how much space I might be able to save by doing this. It turned out to be a lot: dozens of gigabytes.</p>

<p>At this point, there was no turning back. I had to make this into an app. There are plenty of Mac apps that will save disk space by finding duplicate files and then <i>deleting</i> the duplicates. Using APFS clones, my app could reclaim disk space <i><a href="https://hypercritical.co/hyperspace/#without-removing">without removing any files</a>!</i> As a digital pack rat, this appealed to me immensely.</p>

<p>By the end of that week, I’d written a barebones Mac app to do the same thing my Perl script was doing. In the months that followed, I polished and tested the app, and christened it <a href="https://hypercritical.co/hyperspace/">Hyperspace</a>. I’m happy to announce that Hyperspace is now available in the Mac App Store.</p>

<p><a href="https://apps.apple.com/us/app/hyperspace-reclaim-disk-space/id6739505345?mt=12"><img src="https://hypercritical.co/2025/02/25/images/hyperspace-icon-1024-lossy.png" width="512" height="512" alt="The Hyperspace app icon, created by Iconfactory" title="The Hyperspace app icon, created by Iconfactory"></a></p>

<p><a href="https://apps.apple.com/us/app/hyperspace-reclaim-disk-space/id6739505345?mt=12"><img src="https://hypercritical.co/images/download-on-mac-app-store.svg" height="40" alt="Download Hyperspace from the Mac App Store"></a></p>

<p>Hyperspace is a free download, and it’s free to scan to see how much space you might save. To actually reclaim any of that space, you will have to <a href="https://hypercritical.co/hyperspace/#purchase">pay for the app</a>.</p>

<p>Like <a href="https://hypercritical.co/apps/">all my apps</a>, Hyperspace is a bit difficult to explain. I’ve attempted to do so, at length, in the <a href="https://hypercritical.co/hyperspace/">Hyperspace documentation</a>. I hope it makes enough sense to enough people that it will be a useful addition to the Mac ecosystem.</p>

<p>For my fellow developers who might be curious, this is my second Mac app that uses SwiftUI and my first that uses the <a href="https://developer.apple.com/documentation/swiftui/migrating-to-the-swiftui-life-cycle">SwiftUI life cycle</a>. It’s also my second app to use Swift 6 and my first to do so since very early in its development. I found it <i>much</i> easier to use Swift 6 from (nearly) the start than to convert an existing, released app to Swift 6. Even so, there are still many rough edges to Swift 6, and I look forward to things being <a href="https://github.com/swiftlang/swift-evolution/blob/main/visions/approachable-concurrency.md">smoothed</a> <a href="https://hachyderm.io/@holly/114039272702099974">out</a> a bit in the coming years.</p>

<p>In <a href="https://atp.fm/617">a recent episode of ATP</a>, I described the then-unnamed Hyperspace as “<a href="https://atp.fm/617">An Incredibly Dangerous App</a>.” Like the process of converting from HFS+ to APFS, Hyperspace modifies files that it did not create and does not own. It is, by far, the riskiest app I’ve created. (Reclaiming disk space ain’t like dusting crops…) But I also think it might be the most useful to the largest number of people. I hope you like it.</p>




<hr>



<p>© 2010-2025 John Siracusa</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Browser Use (YC W25) – open-source web agents (188 pts)]]></title>
            <link>https://github.com/browser-use/browser-use</link>
            <guid>43173378</guid>
            <pubDate>Tue, 25 Feb 2025 15:45:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/browser-use/browser-use">https://github.com/browser-use/browser-use</a>, See on <a href="https://news.ycombinator.com/item?id=43173378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/browser-use/browser-use/raw/main/static/browser-use-dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/browser-use/browser-use/raw/main/static/browser-use.png">
  <img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/browser-use/browser-use/raw/main/static/browser-use.png" width="full">
</picture></themed-picture>
<p dir="auto"><h2 tabindex="-1" dir="auto">Enable AI to control your browser 🤖</h2><a id="user-content-enable-ai-to-control-your-browser-" aria-label="Permalink: Enable AI to control your browser 🤖" href="#enable-ai-to-control-your-browser-"></a></p>
<p dir="auto"><a href="https://github.com/gregpr07/browser-use/stargazers"><img src="https://camo.githubusercontent.com/0f19403bd69bc9dd8ab794fa4cce28d24300bfd37881d117c0b68e001bc86418/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726567707230372f62726f777365722d7573653f7374796c653d736f6369616c" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/gregpr07/browser-use?style=social"></a>
<a href="https://link.browser-use.com/discord" rel="nofollow"><img src="https://camo.githubusercontent.com/3d2c750e6207adb7f3fe4b1ed35677bc0b7a12e67bbd3d5e6678a1de6df4c217/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313330333734393232303834323334303431323f636f6c6f723d373238394441266c6162656c3d446973636f7264266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white"></a>
<a href="https://docs.browser-use.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/7f6d88507e659f89eb75356e49b3e34237a3118de06ec289f5b8614eef0db6ce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63756d656e746174696f6e2d2546302539462539332539352d626c7565" alt="Documentation" data-canonical-src="https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue"></a>
<a href="https://cloud.browser-use.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/da9350dbaad0c571888c174efb66b7798a6a02b219334f1ed1a4669339897ca8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436c6f75642d2545322539382538312545462542382538462d626c7565" alt="Cloud" data-canonical-src="https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue"></a>
<a href="https://x.com/gregpr07" rel="nofollow"><img src="https://camo.githubusercontent.com/43ecab07dd115c164a7cddcc48ac1f2cf29fac3feb0fea9622075c0345a80083/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f477265676f723f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/Gregor?style=social"></a>
<a href="https://x.com/mamagnus00" rel="nofollow"><img src="https://camo.githubusercontent.com/cc71749cd2ee32cde0bbd96d99c062c14ccfaed867d2d9c5766d4c955352e130/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f4d61676e75733f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/Magnus?style=social"></a>
<a href="https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615" rel="nofollow"><img src="https://camo.githubusercontent.com/721ea8438276831e89ead91abf596b8d69f58d4683912856313acdd3435ab2dc/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d68747470732533412532462532466170702e776f726b77656176652e61692532466170692532467265706f7369746f727925324662616467652532466f72675f543550766e3355427377544849734e3164575333766f5067253246383831343538363135266c6162656c436f6c6f723d23454336333431" alt="Weave Badge" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;labelColor=#EC6341"></a></p>
<p dir="auto">🌐 Browser-use is the easiest way to connect your AI agents with the browser.</p>
<p dir="auto">💡 See what others are building and share your projects in our <a href="https://link.browser-use.com/discord" rel="nofollow">Discord</a> - we'd love to see what you create!</p>
<p dir="auto">🌩️ Skip the setup - try our hosted version for instant browser automation! <a href="https://cloud.browser-use.com/" rel="nofollow">Try it now</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto">With pip (Python&gt;=3.11):</p>

<p dir="auto">install playwright:</p>

<p dir="auto">Spin up your agent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain_openai import ChatOpenAI
from browser_use import Agent
import asyncio
from dotenv import load_dotenv
load_dotenv()

async def main():
    agent = Agent(
        task=&quot;Go to Reddit, search for 'browser-use', click on the first post and return the first comment.&quot;,
        llm=ChatOpenAI(model=&quot;gpt-4o&quot;),
    )
    result = await agent.run()
    print(result)

asyncio.run(main())"><pre><span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>browser_use</span> <span>import</span> <span>Agent</span>
<span>import</span> <span>asyncio</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>load_dotenv</span>()

<span>async</span> <span>def</span> <span>main</span>():
    <span>agent</span> <span>=</span> <span>Agent</span>(
        <span>task</span><span>=</span><span>"Go to Reddit, search for 'browser-use', click on the first post and return the first comment."</span>,
        <span>llm</span><span>=</span><span>ChatOpenAI</span>(<span>model</span><span>=</span><span>"gpt-4o"</span>),
    )
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>()
    <span>print</span>(<span>result</span>)

<span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">Add your API keys for the provider you want to use to your <code>.env</code> file.</p>

<p dir="auto">For other settings, models, and more, check out the <a href="https://docs.browser-use.com/" rel="nofollow">documentation 📕</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test with UI</h3><a id="user-content-test-with-ui" aria-label="Permalink: Test with UI" href="#test-with-ui"></a></p>
<p dir="auto">You can test <a href="https://github.com/browser-use/web-ui">browser-use with a UI repository</a></p>
<p dir="auto">Or simply run the gradio example:</p>

<div dir="auto" data-snippet-clipboard-copy-content="python examples/ui/gradio_demo.py"><pre>python examples/ui/gradio_demo.py</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demos</h2><a id="user-content-demos" aria-label="Permalink: Demos" href="#demos"></a></p>

<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/use-cases/shopping.py">Task</a>: Add grocery items to cart, and checkout.</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=L2Ya9PYNns8" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/67061560/410923398-d9359085-bde6-41d4-aa4e-6520d0221872.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC80MTA5MjMzOTgtZDkzNTkwODUtYmRlNi00MWQ0LWFhNGUtNjUyMGQwMjIxODcyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNhNjZjMTcxOWU1YzNjYTU5ZDRkMWQ3MGFlOWQxMGVhNjk0YmM1YmU1OWEyOThkZjY4ODJmMjQ0NTJlNDg2N2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iuip4JqlEDSXBL7vS-X7b5Cn3rkDj5_FF2cbFJMWqdI" alt="AI Did My Groceries" secured-asset-link="" data-animated-image=""></a></p>

<p dir="auto">Prompt: Add my latest LinkedIn follower to my leads in Salesforce.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67061560/410929479-1440affc-a552-442e-b702-d0d3b277b0ae.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC80MTA5Mjk0NzktMTQ0MGFmZmMtYTU1Mi00NDJlLWI3MDItZDBkM2IyNzdiMGFlLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThiNzEyNmUxZmQ1ZGNkMGQ2ZTU5OWQ2ZDAzZTViNWI3ZjE4MmM4NmUxMTFiYjFjOTMyNDBhNzcxNTQ1YTk2ZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wcKqqm0bMc_ZWFGnpSOznP1xdGFykWnAcS1FYIMX5C0"><img src="https://private-user-images.githubusercontent.com/67061560/410929479-1440affc-a552-442e-b702-d0d3b277b0ae.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC80MTA5Mjk0NzktMTQ0MGFmZmMtYTU1Mi00NDJlLWI3MDItZDBkM2IyNzdiMGFlLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThiNzEyNmUxZmQ1ZGNkMGQ2ZTU5OWQ2ZDAzZTViNWI3ZjE4MmM4NmUxMTFiYjFjOTMyNDBhNzcxNTQ1YTk2ZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wcKqqm0bMc_ZWFGnpSOznP1xdGFykWnAcS1FYIMX5C0" alt="LinkedIn to Salesforce" data-animated-image=""></a></p>

<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/use-cases/find_and_apply_to_jobs.py">Prompt</a>: Read my CV &amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'</p>
<details open="">
  <summary>
    
    <span aria-label="Video description apply.to.jobs.8x.mp4">apply.to.jobs.8x.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/67061560/389458132-171fb4d6-0355-46f2-863e-edb04a828d04.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODk0NTgxMzItMTcxZmI0ZDYtMDM1NS00NmYyLTg2M2UtZWRiMDRhODI4ZDA0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjODAxYjgwNmQ2NWYxZGMyMTc3MDZlYWQwMDY4NDk5NDQzZGI1ZTY2MzZiNDhhMjc3MGFiZDcwZDJhNDE2YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.OuikpmmfgFRymB1zgU52Yj9sKQ2JVxy7iKPXBIBrZSE" data-canonical-src="https://private-user-images.githubusercontent.com/67061560/389458132-171fb4d6-0355-46f2-863e-edb04a828d04.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODk0NTgxMzItMTcxZmI0ZDYtMDM1NS00NmYyLTg2M2UtZWRiMDRhODI4ZDA0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjODAxYjgwNmQ2NWYxZGMyMTc3MDZlYWQwMDY4NDk5NDQzZGI1ZTY2MzZiNDhhMjc3MGFiZDcwZDJhNDE2YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.OuikpmmfgFRymB1zgU52Yj9sKQ2JVxy7iKPXBIBrZSE" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py">Prompt</a>: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67061560/398959783-242ade3e-15bc-41c2-988f-cbc5415a66aa.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zOTg5NTk3ODMtMjQyYWRlM2UtMTViYy00MWMyLTk4OGYtY2JjNTQxNWE2NmFhLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYxODdkM2EzNjM2MzBkNjlkNzE1MzFjYmVlMzA2NTNlYmIwNzhjODkxODYzMzJkNjZmMzQ2ZDFkNDhkZDQzZjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xDwEGfpOUBptaRvyNJvK_bfpFZyZ1Kni4zv6I28vZE"><img src="https://private-user-images.githubusercontent.com/67061560/398959783-242ade3e-15bc-41c2-988f-cbc5415a66aa.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zOTg5NTk3ODMtMjQyYWRlM2UtMTViYy00MWMyLTk4OGYtY2JjNTQxNWE2NmFhLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYxODdkM2EzNjM2MzBkNjlkNzE1MzFjYmVlMzA2NTNlYmIwNzhjODkxODYzMzJkNjZmMzQ2ZDFkNDhkZDQzZjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xDwEGfpOUBptaRvyNJvK_bfpFZyZ1Kni4zv6I28vZE" alt="Letter to Papa" data-animated-image=""></a></p>

<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/custom-functions/save_to_file_hugging_face.py">Prompt</a>: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description hugging_face_high_quality.mp4">hugging_face_high_quality.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/67061560/388001354-de73ee39-432c-4b97-b4e8-939fd7f323b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODgwMDEzNTQtZGU3M2VlMzktNDMyYy00Yjk3LWI0ZTgtOTM5ZmQ3ZjMyM2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY5YmVmNWZkYjdlY2NjNTM5MGMyMjY3ZGQwYjZkNGQyNDllNmQ4YTQxZWUxMGJmZjlmZjA0MjllMjcyMjNjZDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xhPfKYKFxiCHcrwRW3WWVGGJukaZ6-msGd3BEu3EvM" data-canonical-src="https://private-user-images.githubusercontent.com/67061560/388001354-de73ee39-432c-4b97-b4e8-939fd7f323b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODgwMDEzNTQtZGU3M2VlMzktNDMyYy00Yjk3LWI0ZTgtOTM5ZmQ3ZjMyM2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY5YmVmNWZkYjdlY2NjNTM5MGMyMjY3ZGQwYjZkNGQyNDllNmQ4YTQxZWUxMGJmZjlmZjA0MjllMjcyMjNjZDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xhPfKYKFxiCHcrwRW3WWVGGJukaZ6-msGd3BEu3EvM" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto"><h2 tabindex="-1" dir="auto">More examples</h2><a id="user-content-more-examples" aria-label="Permalink: More examples" href="#more-examples"></a></p>
<p dir="auto">For more examples see the <a href="https://github.com/browser-use/browser-use/blob/main/examples">examples</a> folder or join the <a href="https://link.browser-use.com/discord" rel="nofollow">Discord</a> and show off your project.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Vision</h2><a id="user-content-vision" aria-label="Permalink: Vision" href="#vision"></a></p>
<p dir="auto">Tell your computer what to do, and it gets it done.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Agent</h3><a id="user-content-agent" aria-label="Permalink: Agent" href="#agent"></a></p>
<ul>
<li> Improve agent memory (summarize, compress, RAG, etc.)</li>
<li> Enhance planning capabilities (load website specific context)</li>
<li> Reduce token consumption (system prompt, DOM state)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">DOM Extraction</h3><a id="user-content-dom-extraction" aria-label="Permalink: DOM Extraction" href="#dom-extraction"></a></p>
<ul>
<li> Improve extraction for datepickers, dropdowns, special elements</li>
<li> Improve state representation for UI elements</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rerunning tasks</h3><a id="user-content-rerunning-tasks" aria-label="Permalink: Rerunning tasks" href="#rerunning-tasks"></a></p>
<ul>
<li> LLM as fallback</li>
<li> Make it easy to define workfows templates where LLM fills in the details</li>
<li> Return playwright script from the agent</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Datasets</h3><a id="user-content-datasets" aria-label="Permalink: Datasets" href="#datasets"></a></p>
<ul>
<li> Create datasets for complex tasks</li>
<li> Benchmark various models against each other</li>
<li> Fine-tuning models for specific tasks</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">User Experience</h3><a id="user-content-user-experience" aria-label="Permalink: User Experience" href="#user-experience"></a></p>
<ul>
<li> Human-in-the-loop execution</li>
<li> Improve the generated GIF quality</li>
<li> Create various demos for tutorial execution, job application, QA testing, social media, etc.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the <code>/docs</code> folder.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Setup</h2><a id="user-content-local-setup" aria-label="Permalink: Local Setup" href="#local-setup"></a></p>
<p dir="auto">To learn more about the library, check out the <a href="https://docs.browser-use.com/development/local-setup" rel="nofollow">local setup 📕</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cooperations</h2><a id="user-content-cooperations" aria-label="Permalink: Cooperations" href="#cooperations"></a></p>
<p dir="auto">We are forming a commission to define best practices for UI/UX design for browser agents.
Together, we're exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.</p>
<p dir="auto">Email <a href="mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A">Toby</a> to apply for a seat on the committee.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use Browser Use in your research or project, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}"><pre><span>@software</span>{<span>browser_use2024</span>,
  <span>author</span> = <span><span>{</span>Müller, Magnus and Žunič, Gregor<span>}</span></span>,
  <span>title</span> = <span><span>{</span>Browser Use: Enable AI to control your browser<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2024<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://github.com/browser-use/browser-use<span>}</span></span>
}</pre></div>
  
<p>
Made with ❤️ in Zurich and San Francisco
 </p> 
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSearcher: A Local open-source Deep Research (195 pts)]]></title>
            <link>https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md</link>
            <guid>43172338</guid>
            <pubDate>Tue, 25 Feb 2025 14:33:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md">https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md</a>, See on <a href="https://news.ycombinator.com/item?id=43172338">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/deep_researcher_a0170dadd0.gif" alt="deep researcher.gif" id="deep-researcher.gif">
    <span>deep researcher.gif</span>
  </span>
</p>
<p>In the previous post, <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"><em>“I Built a Deep Research with Open Source—and So Can You!”</em></a>, we explained some of the principles underlying research agents and constructed a simple prototype that generates detailed reports on a given topic or question. The article and corresponding notebook demonstrated the fundamental concepts of <em>tool use</em>, <em>query decomposition</em>, <em>reasoning</em>, and <em>reflection</em>. The example in our previous post, in contrast to OpenAI’s Deep Research, ran locally, using only open-source models and tools like <a href="https://milvus.io/docs">Milvus</a> and LangChain. (I encourage you to read the <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md">above article</a> before continuing.)</p>
<p>In the following weeks, there was an explosion of interest in understanding and reproducing OpenAI’s Deep Research. See, for example, <a href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">Perplexity Deep Research</a> and <a href="https://huggingface.co/blog/open-deep-research">Hugging Face’s Open DeepResearch</a>. These tools differ in architecture and methodology although sharing an objective: iteratively research a topic or question by surfing the web or internal documents and output a detailed, informed, and well-structured report. Importantly, the underlying agent automates reasoning about what action to take at each intermediate step.</p>
<p>In this post, we build upon our previous post and present Zilliz’s <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a> open-source project. Our agent demonstrates additional concepts: <em>query routing, conditional execution flow</em>, and <em>web crawling as a tool</em>. It is presented as a Python library and command-line tool rather than a Jupyter notebook and is more fully-featured than our previous post. For example, it can input multiple source documents and can set the embedding model and vector database used via a configuration file. While still relatively simple, DeepSearcher is a great showcase of agentic RAG and is a further step towards a state-of-the-art AI applications.</p>
<p>Additionally, we explore the need for faster and more efficient inference services. Reasoning models make use of “inference scaling”, that is, extra computation, to improve their output, and that combined with the fact that a single report may require hundreds or thousands of LLM calls results in inference bandwidth being the primary bottleneck. We use the <a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency">DeepSeek-R1 reasoning model on SambaNova’s custom-built hardware</a>, which is twice as fast in output tokens-per-second as the nearest competitor (see figure below).</p>
<p>SambaNova Cloud also provides inference-as-a-service for other open-source models including Llama 3.x, Qwen2.5, and QwQ. The inference service runs on SambaNova’s custom chip called the reconfigurable dataflow unit (RDU), which is specially designed for efficient inference on Generative AI models, lowering cost and increasing inference speed. <a href="https://sambanova.ai/technology/sn40l-rdu-ai-chip">Find out more on their website.</a></p>
<p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/Output_speed_deepseek_r1_d820329f0a.png" alt="Output speed- deepseek r1.png" id="output-speed--deepseek-r1.png">
    <span>Output speed- deepseek r1.png</span>
  </span>
</p>
<p>The architecture of <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a> follows our previous post by breaking the problem up into four steps - <em>define/refine the question</em>, <em>research</em>, <em>analyze</em>, <em>synthesize</em> - although this time with some overlap. We go through each step, highlighting <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>’s improvements.</p>
<p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/deepsearcher_architecture_088c7066d1.png" alt="deepsearcher architecture.png" id="deepsearcher-architecture.png">
    <span>deepsearcher architecture.png</span>
  </span>
</p>
<h3 id="Define-and-Refine-the-Question">Define and Refine the Question</h3><pre><code translate="no">Break down the original query into new sub queries: [
  'How has the cultural impact and societal relevance of The Simpsons evolved from its debut to the present?',
  'What changes in character development, humor, and storytelling styles have occurred across different seasons of The Simpsons?', 
  'How has the animation style and production technology of The Simpsons changed over time?',
  'How have audience demographics, reception, and ratings of The Simpsons shifted throughout its run?']
</code></pre>
<p>In the design of DeepSearcher, the boundaries between researching and refining the question are blurred. The initial user query is decomposed into sub-queries, much like the previous post. See above for initial subqueries produced from the query “How has The Simpsons changed over time?”. However, the following research step will continue to refine the question as needed.</p>
<h3 id="Research-and-Analyze">Research and Analyze</h3><p>Having broken down the query into sub-queries, the research portion of the agent begins. It has, roughly speaking, four steps: <em>routing</em>, <em>search</em>, <em>reflection, and conditional repeat</em>.</p>
<p>Our database contains multiple tables or collections from different sources. It would be more efficient if we could restrict our semantic search to only those sources that are relevant to the query at hand. A query router prompts an LLM to decide from which collections information should be retrieved.</p>
<p>Here is the method to form the query routing prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_vector_db_search_prompt</span>(<span>
    question: <span>str</span>,
    collection_names: <span>List</span>[<span>str</span>],
    collection_descriptions: <span>List</span>[<span>str</span>],
    context: <span>List</span>[<span>str</span>] = <span>None</span>,
</span>):
    sections = []
    <span># common prompt</span>
    common_prompt = <span>f"""You are an advanced AI problem analyst. Use your reasoning ability and historical conversation information, based on all the existing data sets, to get absolutely accurate answers to the following questions, and generate a suitable question for each data set according to the data set description that may be related to the question.

Question: <span>{question}</span>
"""</span>
    sections.append(common_prompt)
    
    <span># data set prompt</span>
    data_set = []
    <span>for</span> i, collection_name <span>in</span> <span>enumerate</span>(collection_names):
        data_set.append(<span>f"<span>{collection_name}</span>: <span>{collection_descriptions[i]}</span>"</span>)
    data_set_prompt = <span>f"""The following is all the data set information. The format of data set information is data set name: data set description.

Data Sets And Descriptions:
"""</span>
    sections.append(data_set_prompt + <span>"\n"</span>.join(data_set))
    
    <span># context prompt</span>
    <span>if</span> context:
        context_prompt = <span>f"""The following is a condensed version of the historical conversation. This information needs to be combined in this analysis to generate questions that are closer to the answer. You must not generate the same or similar questions for the same data set, nor can you regenerate questions for data sets that have been determined to be unrelated.

Historical Conversation:
"""</span>
        sections.append(context_prompt + <span>"\n"</span>.join(context))
    
    <span># response prompt</span>
    response_prompt = <span>f"""Based on the above, you can only select a few datasets from the following dataset list to generate appropriate related questions for the selected datasets in order to solve the above problems. The output format is json, where the key is the name of the dataset and the value is the corresponding generated question.

Data Sets:
"""</span>
    sections.append(response_prompt + <span>"\n"</span>.join(collection_names))
    
    footer = <span>"""Respond exclusively in valid JSON format matching exact JSON schema.

Critical Requirements:
- Include ONLY ONE action type
- Never add unsupported keys
- Exclude all non-JSON text, markdown, or explanations
- Maintain strict JSON syntax"""</span>
    sections.append(footer)
    <span>return</span> <span>"\n\n"</span>.join(sections)
</code></pre>
<p>We make the LLM return structured output as JSON in order to easily convert its output to a decision on what to do next.</p>
<p>Having selected various database collections via the previous step, the search step performs a similarity search with <a href="https://milvus.io/docs">Milvus</a>. Much like the previous post, the source data has been specified in advance, chunked, embedded, and stored in the vector database. For DeepSearcher, the data sources, both local and online, must be manually specified. We leave online search for future work.</p>
<p>Unlike the previous post, DeepSearcher illustrates a true form of agentic reflection, inputting the prior outputs as context into a prompt that “reflects” on whether the questions asked so far and the relevant retrieved chunks contain any informational gaps. This can be seen as an analysis step.</p>
<p>Here is the method to create the prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_reflect_prompt</span>(<span>
   question: <span>str</span>,
   mini_questions: <span>List</span>[<span>str</span>],
   mini_chuncks: <span>List</span>[<span>str</span>],
</span>):
    mini_chunk_str = <span>""</span>
    <span>for</span> i, chunk <span>in</span> <span>enumerate</span>(mini_chuncks):
        mini_chunk_str += <span>f"""&lt;chunk_<span>{i}</span>&gt;\n<span>{chunk}</span>\n&lt;/chunk_<span>{i}</span>&gt;\n"""</span>
    reflect_prompt = <span>f"""Determine whether additional search queries are needed based on the original query, previous sub queries, and all retrieved document chunks. If further research is required, provide a Python list of up to 3 search queries. If no further research is required, return an empty list.

If the original query is to write a report, then you prefer to generate some further queries, instead return an empty list.

    Original Query: <span>{question}</span>
    Previous Sub Queries: <span>{mini_questions}</span>
    Related Chunks: 
    <span>{mini_chunk_str}</span>
    """</span>
   
    
    footer = <span>"""Respond exclusively in valid List of str format without any other text."""</span>
    <span>return</span> reflect_prompt + footer
</code></pre>
<p>Once more, we make the LLM return structured output, this time as Python-interpretable data.</p>
<p>Here is an example of new sub-queries “discovered” by reflection after answering the initial sub-queries above:</p>
<pre><code translate="no">New search queries <span>for</span> <span>next</span> iteration: [
  <span>"How have changes in The Simpsons' voice cast and production team influenced the show's evolution over different seasons?"</span>,
  <span>"What role has The Simpsons' satire and social commentary played in its adaptation to contemporary issues across decades?"</span>,
  <span>'How has The Simpsons addressed and incorporated shifts in media consumption, such as streaming services, into its distribution and content strategies?'</span>]
</code></pre>
<p>Unlike our previous post, DeepSearcher illustrates conditional execution flow. After reflecting on whether the questions and answers so far are complete, if there are additional questions to be asked the agent repeats the above steps. Importantly, the execution flow (a while loop) is a function of the LLM output rather than being hard-coded. In this case there is only a binary choice: <em>repeat research</em> or <em>generate a report</em>. In more complex agents there may be several such as: <em>follow hyperlink</em>, <em>retrieve chunks, store in memory, reflect</em> etc. In this way, the question continues to be refined as the agent sees fit until it decides to exit the loop and generate the report. In our Simpsons example, DeepSearcher performs two more rounds of filling the gaps with extra sub-queries.</p>
<p>Finally, the fully decomposed question and retrieved chunks are synthesized into a report with a single prompt. Here is the code to create the prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_final_answer_prompt</span>(<span>
   question: <span>str</span>, 
   mini_questions: <span>List</span>[<span>str</span>],
   mini_chuncks: <span>List</span>[<span>str</span>],
</span>):
    mini_chunk_str = <span>""</span>
    <span>for</span> i, chunk <span>in</span> <span>enumerate</span>(mini_chuncks):
        mini_chunk_str += <span>f"""&lt;chunk_<span>{i}</span>&gt;\n<span>{chunk}</span>\n&lt;/chunk_<span>{i}</span>&gt;\n"""</span>
    summary_prompt = <span>f"""You are an AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.

    Original Query: <span>{question}</span>
    Previous Sub Queries: <span>{mini_questions}</span>
    Related Chunks: 
    <span>{mini_chunk_str}</span>
    """</span>
    <span>return</span> summary_prompt
</code></pre>
<p>This approach has the advantage over our prototype, which analyzed each question separately and simply concatenated the output, of producing a report where all sections are consistent with each other, i.e., containing no repeated or contradictory information. A more complex system could combine aspects of both, using a conditional execution flow to structure the report, summarize, rewrite, reflect and pivot, and so on, which we leave for future work.</p>
<p>Here is a sample from the report generated by the query “How has The Simpsons changed over time?” with DeepSeek-R1 passing the Wikipedia page on The Simpsons as source material:</p>
<pre><code translate="no">Report: The Evolution of The Simpsons (1989–Present)
1. Cultural Impact and Societal Relevance
The Simpsons debuted as a subversive critique of American middle-class life, gaining notoriety for its bold satire in the 1990s. Initially a countercultural phenomenon, it challenged norms with episodes tackling religion, politics, and consumerism. Over time, its cultural dominance waned as competitors like South Park and Family Guy pushed boundaries further. By the 2010s, the show transitioned from trendsetter to nostalgic institution, balancing legacy appeal with attempts to address modern issues like climate change and LGBTQ+ rights, albeit with less societal resonance.
…
Conclusion
The Simpsons evolved from a radical satire to a television institution, navigating shifts in technology, politics, and audience expectations. While its golden-age brilliance remains unmatched, its adaptability—through streaming, updated humor, and global outreach—secures its place as a cultural touchstone. The show’s longevity reflects both nostalgia and a pragmatic embrace of change, even as it grapples with the challenges of relevance in a fragmented media landscape.
</code></pre>
<p>Find <a href="https://drive.google.com/file/d/1GE3rvxFFTKqro67ctTkknryUf-ojhduN/view?usp=sharing">the full report here</a>, and <a href="https://drive.google.com/file/d/1EGd16sJDNFnssk9yTd5o9jzbizrY_NS_/view?usp=sharing">a report produced by DeepSearcher with GPT-4o mini</a> for comparison.</p>
<p>We presented <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>, an agent for performing research and writing reports. Our system is built upon the idea in our previous article, adding features like conditional execution flow, query routing, and an improved interface. We switched from local inference with a small 4-bit quantized reasoning model to an online inference service for the massive DeepSeek-R1 model, qualitatively improving our output report. DeepSearcher works with most inference services like OpenAI, Gemini, DeepSeek and Grok 3 (coming soon!).</p>
<p>Reasoning models, especially as used in research agents, are inference-heavy, and we were fortunate to be able to use the fastest offering of DeepSeek-R1 from SambaNova running on their custom hardware. For our demonstration query, we made sixty-five calls to SambaNova’s DeepSeek-R1 inference service, inputting around 25k tokens, outputting 22k tokens, and costing $0.30. We were impressed with the speed of inference given that the model contains 671-billion parameters and is 3/4 of a terabyte large. <a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency">Find out more details here!</a></p>
<p>We will continue to iterate on this work in future posts, examining additional agentic concepts and the design space of research agents. In the meanwhile, we invite everyone to try out <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>, <a href="https://github.com/zilliztech/deep-searcher">star us on GitHub</a>, and share your feedback!</p>
<ul>
<li><p><a href="https://github.com/zilliztech/deep-searcher"><strong>Zilliz’s DeepSearcher</strong></a></p></li>
<li><p>Background reading: <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"><strong><em>“I Built a Deep Research with Open Source—and So Can You!”</em></strong></a></p></li>
<li><p><em>“</em><a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency"><strong>SambaNova Launches the Fastest DeepSeek-R1 671B with the Highest Efficiency</strong></a><em>”</em></p></li>
<li><p>DeepSearcher: <a href="https://drive.google.com/file/d/1GE3rvxFFTKqro67ctTkknryUf-ojhduN/view?usp=sharing">DeepSeek-R1 report on The Simpsons</a></p></li>
<li><p>DeepSearcher: <a href="https://drive.google.com/file/d/1EGd16sJDNFnssk9yTd5o9jzbizrY_NS_/view?usp=sharing">GPT-4o mini report on The Simpsons</a></p></li>
<li><p><a href="https://milvus.io/docs">Milvus Open-Source Vector Database</a></p></li>
</ul>
</div><section><p>Like the article? Spread the word</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT Saved My Life (no, seriously, I'm writing this from the ER) (136 pts)]]></title>
            <link>https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er</link>
            <guid>43171639</guid>
            <pubDate>Tue, 25 Feb 2025 13:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er">https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er</a>, See on <a href="https://news.ycombinator.com/item?id=43171639">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Unknown illness kills over 50 in Congo with hours between symptoms and death (108 pts)]]></title>
            <link>https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db</link>
            <guid>43171371</guid>
            <pubDate>Tue, 25 Feb 2025 13:11:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db">https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db</a>, See on <a href="https://news.ycombinator.com/item?id=43171371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>KINSHASA, Congo (AP) — An unknown illness first discovered in three children who ate a bat has rapidly killed more than 50 people in northwestern Congo over the past five weeks, health experts say.</p><p>The interval between the onset of symptoms – which include fever, vomiting and internal bleeding – and death has been 48 hours in most cases and “that’s what’s really worrying,” said Serge Ngalebato, medical director of Bikoro Hospital, a regional monitoring center.</p><p>These “hemorrhagic fever” symptoms are commonly linked to known deadly viruses, such as Ebola, dengue, Marburg and yellow fever, but researchers have ruled these out based on tests of more than a dozen samples collected so far.</p><p>The latest disease outbreak in the Democratic Republic of Congo began on Jan. 21, with 419 cases recorded and 53 deaths.</p><p>The outbreak began in the village of Boloko after three children ate a bat and died within 48 hours, the Africa office of the World Health Organization said Monday.</p>
    

<p>There have long been concerns about <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/covid-health-united-nations-animals-world-organization-7d104d2f4a87dd29ddb5d263986f731c">diseases jumping from animals to humans</a></span> in places where wild animals are popularly eaten. The number of such outbreaks in Africa has surged by more than 60% in the last decade, the WHO said in 2022.</p>



<p>After the second outbreak of the mystery disease began in the village of Bomate on Feb. 9, samples from 13 cases were sent to the National Institute for Biomedical Research in Congo’s capital, Kinshasa, for testing, the WHO said. All samples were negative for common hemorrhagic fever diseases, although some tested positive for malaria.</p><p>Last year, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/mystery-disease-congo-malaria-who-c772403273a89b5dcdc86778ad7eeed3">another mystery flu-like illness</a></span> that killed dozens of people in another part of Congo was determined likely to be malaria.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE will use AI to assess the responses of federal workers (118 pts)]]></title>
            <link>https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439</link>
            <guid>43171265</guid>
            <pubDate>Tue, 25 Feb 2025 12:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439">https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439</a>, See on <a href="https://news.ycombinator.com/item?id=43171265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WASHINGTON — Responses to the Elon Musk-directed <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">email to government employees </a>about what work they had accomplished in  the last week are expected to be fed into an artificial intelligence system to determine whether those jobs are necessary, according to three sources with knowledge of the system.</p><p>The information will go into an LLM (Large Language Model), an advanced AI system that looks at huge amounts of text data to understand, generate and process human language, the sources said. The AI system will determine whether someone’s work is mission-critical or not.</p><p>The U.S. Office of Personnel Management emails were sent to federal workers on Saturday, shortly after <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">Musk wrote in a post on X </a>that “all federal employees will shortly receive an email requesting to understand what they got done last week. Failure to respond will be taken as a resignation.”</p><p>The OPM email did not mention the resignation threat, but said: “Please reply to this email with approx. 5 bullets of what you accomplished last week and cc your manager. Please do not send any classified information, links, or attachments. Deadline is this Monday at 11:59pm EST.”</p><p>The reason the email requested no links or attachments was because of the plan to send the information to the AI system, the sources said.</p><p>A request for comment from OPM as to whether humans will be involved in reviewing the responses was not answered immediately. The White House declined to comment.</p><p>But in response to a tweet about the usage of LLMs, Musk wrote on X that they were not “needed here,” and “this was basically a check to see if the employee had a pulse and was capable of replying to an email.” </p><p><a href="https://www.nbcnews.com/politics/politics-news/live-blog/trump-elon-musk-doge-live-updates-rcna193571" target="_blank"><strong><em>Follow live politics coverage here</em></strong></a></p><p>After the deadline for employees to reply to the email had passed late Monday, OPM did not immediately respond to a request for comment regarding how many workers replied and how many were required to do so.</p><p>In an email to its workforce earlier Monday, the Justice Department said that during a meeting with the interagency Chief Human Capital Officers Council, OPM informed agencies that employee responses to the email are voluntary. OPM also clarified that despite what Musk had posted, not responding to the email does not equate to a resignation, the email said.</p><p>Musk complained about backlash to the directive on his<a href="https://x.com/elonmusk/status/1894173297786720718" target="_blank"> social media platform</a> late Monday.</p><p>"The email request was utterly trivial, as the standard for passing the test was to type some words and press send! Yet so many failed even that inane test, urged on in some cases by their managers. Have you ever witnessed such INCOMPETENCE and CONTEMPT for how YOUR TAXES are being spent?" he wrote.</p><p>In a subsequent tweet, he seemed to indicate that a second email could be sent to government workers who don't respond to the first one.</p><p>"Subject to the discretion of the President, they will be given another chance. Failure to respond a second time will result in termination," he <a href="https://x.com/elonmusk/status/1894177129887404484" target="_blank">wrote</a>.</p><p>The White House did not immediately respond to a request for comment on the post.</p><p>The initial directive has faced pushback from unions, workers and even some agencies since it was sent, but the effort was praised by President Donald Trump earlier Monday.</p><p>“I thought it was great,” <a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73634?canonicalCard=true" target="_blank">Trump told reporters </a>in the Oval Office, where he was meeting with French President Emmanuel Macron.</p><p>"We have people that don’t show up to work and nobody even knows if they work for the government, so by asking the question ‘tell us what you did this week,’ what he's doing is saying are you actually working. And then, if you don’t answer, like, you’re sort of semi-fired or you're fired," he said, claiming without providing evidence that "a lot of people are not answering because they don't even exist."</p><p>"There was a lot of genius in sending it," Trump said. "If people don’t respond, it’s very possible that there is no such person or they’re not working.”</p><p>A coalition of unions and groups that have been fighting the Trump administration's mass layoffs of probationary workers charge the effort was unlawful. They amended their lawsuit against the U.S. Office of Personnel Management over the weekend to add a claim involving the OPM email directing workers to justify their workweek.</p><p>The<a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.444883/gov.uscourts.cand.444883.17.0.pdf" target="_blank"> lawsuit</a> charges that the administration didn't follow proper procedure for such an order and should be voided by a judge.</p><p>"The mass firings ordered by OPM are illegal and betray the trust of countless federal employees. The patronizing demand that federal workers still on the job have to justify themselves by enumerating five accomplishments just adds insult to injury. That too is against the law," lawyer Norm Eisen said in a statement on behalf of the plaintiffs.</p><p>Musk has been tasked by Trump with reducing the size of the government, and the email is seen as part of his push to reduce the federal workforce by as much as 10%.</p><p>Some agencies, including ones led by close Trump allies, had told their employees to ignore the directive.</p><p>Justice Department employees were informed earlier Monday that they did not need to respond to the message, according to emails seen by NBC News. “Due to the confidential and sensitive nature of the Department’s work, DOJ employees do not need to respond to the email from OPM. If you have already responded to this email, no further action is needed,” read one email sent by Assistant Attorney General for Administration Jolene Ann Lauria.</p><p>FBI Director&nbsp;<a href="https://www.nbcnews.com/politics/justice-department/fbi-director-kash-patel-named-atf-chief-rcna193332" target="_blank">Kash Patel</a>&nbsp;instructed employees over the weekend to “pause any responses” to the email, and said his agency would do its own review. Employees of the<strong>&nbsp;</strong>State Department, the National Institutes of Health, the Defense Department, the National Security Agency and the Office of the Director of National Intelligence were also told not to respond to the email.</p><p>The Department of Agriculture also sent out an unsigned email to employees informing them that any response to the email "is voluntary and not required."</p><p>The email was also sent to an <a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73631?canonicalCard=true" target="_blank">unknown amount</a> of judicial branch employees, including judges and court staffers. Spokespeople for federal courts in Manhattan and the Northern District of Illinois confirmed to NBC News that “some” people had gotten the message.</p><p>Julie Hodek, a spokesperson for the Northern District of Illinois, also confirmed the email and said the court’s chief judge and clerk “communicated with the staff that as we are judiciary employees, our policies and procedures are governed by the Judicial Conference of the United States and our local court HR handbooks.”</p><p>A spokesperson for the Southern District of New York said personnel there had been directed not to respond to the email.</p><p>Managers within the Environmental Protection Agency on Monday sent employees model responses to the email to make it easier for them.</p><p>“As empathy for their staff, they sent examples,” said one agency employee who shared two of the managers’ own responses with NBC News. The employee asked NBC not to publish the managers’ responses in full out of fear of reprisal.</p><p>Officials at the Health and Human Services Department and the Centers for Medicare and Medicaid Services directed employees to respond by the deadline. HHS informed employees of OPM's changed guidance later on Monday, and<a href="https://www.nbcnews.com/politics/doge/hhs-warns-responses-elon-musks-email-may-read-malign-foreign-actors-rcna193553" target="_blank"> warned</a> that whatever information they choose to share may&nbsp;“be read by malign foreign actors.”</p><p>An email sent to Department of Transportation employees and obtained by NBC News instructed them to respond to OPM’s weekend email asking for five bullet points of their work. The message also asked employees to exclude any classified info from their responses. Transportation Secretary Sean Duffy <a href="https://x.com/SecDuffy/status/1894031690256818515" target="_blank">embraced the challenge </a>himself in a post on social media.</p><p>Musk appears to be following the same playbook he used when he bought Twitter, which he renamed X.</p><p>Musk began his tenure there with massive layoffs, asking employees to commit to "Extremely hardcore" work in an email titled "A Fork in the Road" or be fired. The email subject line is the same as the email sent out to federal employees by the Office of Personnel Management offering buyouts in January. About<a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank"> 75,000 </a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">f</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">e</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">deral e</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">mployees</a> took the deal.</p><p>Twitter employees who stayed were then asked to print out pages of code they'd written from the last month and prepare to present the work to Musk personally. The code reviews reportedly were abandoned, and instead, managers were asked to rank their employees, according to <a href="https://www.theverge.com/23551060/elon-musk-twitter-takeover-layoffs-workplace-salute-emoji" target="_blank">The Verge</a>.</p><p>Musk and DOGE's access to government data and information has become a central point of friction between the group and its critics. In at least <a href="https://www.nbcnews.com/tech/security/doge-lawsuits-11-cases-musk-group-focus-data-privacy-rcna191695" target="_blank">11 lawsuits</a>, plaintiffs have argued that DOGE has flouted laws and rules around data and privacy. Some of the lawsuits have referenced allegations that DOGE is using artificial intelligence to analyze and process government data. The <a href="https://www.washingtonpost.com/nation/2025/02/06/elon-musk-doge-ai-department-education/" target="_blank">Washington Post reported</a> in February that DOGE was using artificial intelligence to analyze spending at the Education Department, citing two people familiar with the project.</p><p>House Speaker Mike Johnson, R-La.,<a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank"> </a><a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank">on Monday </a><a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank">touted Musk's work</a> in remarks that seemed to confirm the AI reports.</p><p>"Elon has cracked the code. He is now inside the agencies. He’s created these algorithms that are constantly crawling through the data. And as he told me in his office, the data doesn’t lie. We’re going to be able to get the information. We’re going to be able to transform the way the federal government works at the end of this, and that is a very exciting prospect. It is truly a revolutionary moment for the nation,” Johnson said at an event in Washington.</p><p>Sen. Lisa Murkowski, R-Alaska, had some harsh words for the way DOGE was operating, calling the email sent to federal workers “intimidation,”&nbsp;and saying that she’s hearing from federal workers who are being “treated with a level of disregard to their service and to their tenure.”</p><p>“Just a little bit of humanity and dignity to the process, I think, is what many of the Alaskan federal employees are asking for, and I don’t think that that’s asking for too much,”&nbsp;Murkowski said.</p><p>DOGE's work has led to criticism and instances of workers having to be rehired after they were removed from essential jobs. On Monday, two people familiar with the matter said the administration was <a href="https://www.nbcnews.com/health/health-news/fda-rehires-staff-medical-devices-division-mass-layoffs-rcna193501" target="_blank">reinstating some employees</a> in the Food and Drug Administration’s medical devices division after dozens were laid off as part of DOGE's <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">cost-cutting initiative</a>.</p><p>The medical devices division is responsible for approving and monitoring the safety of a range of products, from X-ray machines to surgical implants. The layoffs took place earlier this month, and included physicians and cybersecurity experts.</p><p>Some of those employees received phone calls or emails over the weekend informing them that their termination had been rescinded. It’s unclear how many employees were offered their jobs back, or how many would ultimately return.</p><p>The administration had a similar issue this month with<a href="https://www.nbcnews.com/politics/national-security/trump-administration-wants-un-fire-nuclear-safety-workers-cant-figure-rcna192345" target="_blank"> nuclear safety personnel</a> who had been let go.</p><p>In a court ruling Friday, a federal judge in New York issued a preliminary&nbsp;injunction&nbsp;<a href="https://www.nbcnews.com/politics/politics-news/judge-temporarily-blocks-doge-sensitive-treasury-department-systems-rcna191316" target="_blank">barring DOGE’s access&nbsp;</a>to sensitive Treasury Department systems after a coalition of states <a href="https://www.nbcnews.com/politics/trump-administration/trump-scored-big-legal-wins-week-efforts-reshape-government-still-face-rcna193000" target="_blank">presented evidence </a>that its employees weren't following proper safety protocols.</p><p>In a&nbsp;<a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.636609/gov.uscourts.nysd.636609.76.0_2.pdf" target="_blank">scathing ruling</a>, U.S. District Judge Jeanette Vargas blasted DOGE's "chaotic and haphazard approach" and found the coalition had “established that there is a realistic danger that confidential financial information will be disclosed absent the grant of injunctive relief.”</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/courtney-kube-ncpn3621" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/courtney-kube-ncpn3621">Courtney Kube</a></span><span><a href="https://x.com/ckubeNBC" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Courtney Kube is a correspondent covering national security and the military for the NBC News Investigative Unit.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/julie-tsirkin-ncpn947511" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_46/3669091/241113-julie-tsirkin.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_46/3669091/241113-julie-tsirkin.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_46/3669091/241113-julie-tsirkin.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/julie-tsirkin-ncpn947511">Julie Tsirkin</a></span><span><a href="https://x.com/JulieNBCNews" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Julie&nbsp;Tsirkin is a&nbsp;correspondent covering Capitol Hill.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/yamiche-alcindor-ncpn1294685" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/yamiche-alcindor-ncpn1294685">Yamiche Alcindor</a></span><span><a href="mailto:Yamiche.Alcindor@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Yamiche Alcindor is an NBC News Washington correspondent. </p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/laura-strickler-ncpn894696">Laura Strickler</a></span><span><a href="https://x.com/strickdc" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Laura.Strickler@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Laura Strickler is a senior investigative producer and reporter for NBC News. She is based in Washington.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/dareh-gregorian-ncpn925686" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/dareh-gregorian-ncpn925686">Dareh Gregorian</a></span><span><a href="https://x.com/darehgregorian" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Dareh.Gregorian@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Dareh Gregorian is a politics reporter for NBC News.</p></div><div><p>Ben Goggin</p><!-- --><p>, </p><!-- --><p>Sarah Dean</p><!-- --><p>, </p><!-- --><p>Ken Dilanian</p><!-- --><p>, </p><!-- --><p>Allan Smith</p><!-- --><p>, </p><!-- --><p>Jonathan Allen</p><!-- --><p>, </p><!-- --><p>Julia Jester</p><!-- --><p>, </p><!-- --><p>Ryan J. Reilly</p><!-- --><p>, </p><!-- --><p>Megan Lebowitz</p><!-- --><p>, </p><!-- --><p>Ryan Nobles</p><!-- --><p>, </p><!-- --><p>Frank Thorp V</p><!-- --><p>, </p><!-- --><p>Zoë Richards</p><!-- --><p> and </p><!-- --><p>Phil Helsel</p><!-- --> <!-- --><p>contributed</p><!-- --><p>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Embedding Python in Elixir, It's Fine (277 pts)]]></title>
            <link>https://dashbit.co/blog/running-python-in-elixir-its-fine</link>
            <guid>43171239</guid>
            <pubDate>Tue, 25 Feb 2025 12:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dashbit.co/blog/running-python-in-elixir-its-fine">https://dashbit.co/blog/running-python-in-elixir-its-fine</a>, See on <a href="https://news.ycombinator.com/item?id=43171239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    
<ul>
  <li>
    <i></i> Jonatan Kłosko
  </li>
  <li>
    <i></i> February 21st, 2025
  </li>
  <li>
    <i></i><a href="https://dashbit.co/blog/tags/python">python</a>, <a href="https://dashbit.co/blog/tags/livebook">livebook</a>, <a href="https://dashbit.co/blog/tags/nifs">nifs</a>
  </li>
</ul>
<p>
In the recent years, Elixir has been expanding its capabilities in Machine Learning and Data through the <a href="https://github.com/elixir-nx">Nx (Numerical Elixir)</a> effort. A number of projects emerged (Nx, Explorer, Axon, Bumblebee, Scholar, and more), drawing learnings from decades of work in ecosystems such as Python and R, often standing on the shoulders of C++ and Rust codebases.</p>
<p>
When we started, we made the explicit choice to not depend on Python libraries directly. We wanted to design and develop our ecosystem with full control of making the best decisions for Elixir, which would not necessarily match the decisions made for Python. We also wished to avoid bringing to our ecosystem the complexities in getting a Python environment up and running. While young, the Nx ecosystem already enabled <a href="https://youtu.be/VcOvNTxUaIo">running pre-trained ML models</a>, <a href="https://youtu.be/5FlZHkc4Mq4">simplifying production systems with a unified AI stack</a>, <a href="https://youtu.be/4qoHPh0obv0">managing GPU cluster workflows from a notebook</a>, to point a few.</p>
<p>
A key component driving the adoption of Elixir in these areas is <a href="https://livebook.dev/">Livebook</a>, a computational notebook platform that builds on the strengths of the Elixir and Erlang, bringing reproducibility, distributed execution, and app development to the forefront. With Livebook, we have seen a growing interest from teams and companies in dipping their toes into the Elixir ecosystem for the first time.</p>
<p>
All of this builds a good case to go all in with Elixir, but some hurdles remain. As one would expect, most companies interested in bringing Elixir and Livebook into their infrastructure, have existing workflows, packages, and repositories that they already rely on. The choices we have made so far imply that they either have to find an equivalent package in Elixir or write one from scratch, increasing the risk and costs of adding Elixir to their data stack.</p>
<p>
To address these concerns, today we announce Pythonx, which embeds the Python interpreter within the Erlang VM, bringing automatic data conversion between Elixir and Python, code evaluation, and automatic virtual environment management. We compare Pythonx with other options for interoperability and outline future work.</p>
<h2>
Enter Pythonx</h2>
<p>
Imagine we have an image and want to read the text on that image. We need to do what is known as Optical Character Recognition (OCR). Sure enough, there are a few Python packages doing just that, one of them being <code>pytesseract</code>. For the sake of this example, we will download the image using Req:</p>
<pre><code><span>Mix</span><span>.</span><span>install</span><span data-group-id="8115916589-1">(</span><span data-group-id="8115916589-2">[</span><span>
  </span><span data-group-id="8115916589-3">{</span><span>:pythonx</span><span>,</span><span> </span><span>"~&gt; 0.4.0"</span><span data-group-id="8115916589-3">}</span><span>,</span><span>
  </span><span data-group-id="8115916589-4">{</span><span>:req</span><span>,</span><span> </span><span>"~&gt; 0.5.8"</span><span data-group-id="8115916589-4">}</span><span>
</span><span data-group-id="8115916589-2">]</span><span data-group-id="8115916589-1">)</span><span>

</span><span>url</span><span> </span><span>=</span><span> </span><span>"https://unsplash.com/photos/95t94hZTESw/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzQwMDYwMjg4fA&amp;force=true&amp;w=640"</span><span>
</span><span>binary</span><span> </span><span>=</span><span> </span><span>Req</span><span>.</span><span>get!</span><span data-group-id="8115916589-5">(</span><span>url</span><span data-group-id="8115916589-5">)</span><span>.</span><span>body</span></code></pre>
<p>
Now, let’s bring in Python.</p>
<pre><code><span>Pythonx</span><span>.</span><span>uv_init</span><span data-group-id="6229236571-1">(</span><span>"""
[project]
name = "project"
version = "0.0.0"
requires-python = "==3.13.*"
dependencies = [
  "pytesseract==0.3.13",
  "pillow==11.1.0"
]
"""</span><span data-group-id="6229236571-1">)</span></code></pre>
<p>
Calling <code>Pythonx.uv_init/1</code> downloads Python and the listed dependencies using the excellent <a href="https://docs.astral.sh/uv">uv</a> package manager. It also immediately initializes the Python interpreter for evaluation. Note the dependencies section where we list <code>pytesseract</code> for OCR and <code>pillow</code> for image handling.</p>
<p>
Next, let’s write some Python.</p>
<pre><code><span data-group-id="8721759126-1">{</span><span>result</span><span>,</span><span> </span><span>_globals</span><span data-group-id="8721759126-1">}</span><span> </span><span>=</span><span>
  </span><span>Pythonx</span><span>.</span><span>eval</span><span data-group-id="8721759126-2">(</span><span>
    </span><span>"""
    import pytesseract
    import io
    import PIL

    image = PIL.Image.open(io.BytesIO(binary))
    pytesseract.image_to_string(image)
    """</span><span>,</span><span>
    </span><span data-group-id="8721759126-3">%{</span><span>"binary"</span><span> </span><span>=&gt;</span><span> </span><span>binary</span><span data-group-id="8721759126-3">}</span><span>
  </span><span data-group-id="8721759126-2">)</span><span>

</span><span>Pythonx</span><span>.</span><span>decode</span><span data-group-id="8721759126-4">(</span><span>result</span><span data-group-id="8721759126-4">)</span><span>
</span><span>#=&gt; "The Journey\nof a thousand\nmiles begins\nwith a single\n\nstep.\n\n-Lao Tzu\n\n"</span></code></pre>
<p>
Above we call <code>Pythonx.eval/2</code>, which accepts Python code and a map with variables for the evaluation. Note how we pass the Elixir binary and it is automatically converted to a <code>bytes</code> object on the Python side. The evaluation returns <code>result</code>, which is a <code>%Pythonx.Object{}</code>, and also an updated map with variables. In this case we only care about the result and we use <code>Pythonx.decode/1</code> to convert it to an Elixir string right away.</p>
<p>
There we go! To learn more about Pythonx, see <a href="https://hexdocs.pm/pythonx">the documentation</a>. And if you are struggling to write Python for your task, consult with your AI specialist, it went to school for that.</p>
<h2>
Under the hood</h2>
<p>
If you are raising your eyebrow, thinking that this just calls <code>python</code>, bear with me!</p>
<p>
So Python, or more specifically its <a href="https://github.com/python/cpython">CPython</a> reference implementation, has the interesting capability of being embedded into other applications. What this means is that the core functionality of the Python interpreter is available as a C library, so a C/C++ application can link that library and use its APIs to run code and interact with objects. In fact, you can think of the <code>python</code> executable as one such application.</p>
<p>
Elixir provides C/C++ interoperability via Erlang NIFs and that’s exactly what Pythonx uses to embed Python, which means that the Python interpreter operates in the same OS process as Elixir itself. By living in the same memory space, passing data between Elixir and Python is cheap. Pythonx ties Python and Erlang garbage collection, so that the objects can be safely kept between evaluations. Also, it conveniently handles conversion between Elixir and Python data structures, bubbles Python exceptions and captures standard output.</p>
<h2>
Livebook goes multilingual</h2>
<p>
To enable even more powerful workflows, we <a href="https://github.com/livebook-dev/livebook/pull/2936">started working</a> on Python support in Livebook, building on Pythonx. The idea, though, is not to support Python separately, but rather to allow Elixir and Python interacting in the same notebook! To give you a better picture, below you can see the same example using Python cells in Livebook nightly.</p>
<p><img src="https://dashbit.co/images/posts/2025/pythonx_ocr.png" width="100%"></p>
<p>
Livebook automatically installs Python and its dependencies, as it manages Elixir’, ensuring a reproducible environment. It also tracks which Elixir variables are used by Python, and vice-versa, and automatically converts them between cells. While there is still work ahead of us, including code completion, documentation, and a few surprises, we are open to feedback. <a href="https://github.com/livebook-dev/livebook#desktop-app">You can download Livebook nightly</a> to give it a try. Once we add all bells and whistles, we will do an official announcement over <a href="https://news.livebook.dev/">news.livebook.dev</a>.</p>
<p>
At this point I want to thank <a href="https://github.com/cocoa-xu">Cocoa Xu</a> for starting off the work on Embedded Python, and <a href="https://github.com/cigrainger">Christopher Grainger</a> for the initial push to run Python in Livebook.</p>
<h2>
Usage considerations and alternatives</h2>
<p>
The primary goal of Pythonx is to better integrate Python workflows within Livebook and scripts. Pythonx usage in actual projects must be done with care due to Python’s global interpreter lock (GIL). The GIL prevents multiple threads from executing Python code at the same time, so calling <code>Pythonx</code> from multiple Elixir processes does not provide the concurrency you might expect and thus it can be a source of bottlenecks. However, this limitation concerns regular Python code. Packages with CPU-intense functionality, such as <code>numpy</code>, have native implementation of many functions and invoking those releases the GIL. The GIL is also released when waiting on I/O operations. In other words, if you are using this library to integrate with Python, make sure it happens in a single Elixir process or that its underlying libraries can deal with concurrent invocation.</p>
<p>
If the above is a dealbreaker, remember that interoperability already exists at a few levels. For example, you could write a Python script and then invoke it with <a href="https://hexdocs.pm/elixir/System.html#cmd/3"><code>System.cmd/3</code></a> or open a <a href="https://hexdocs.pm/elixir/Port.html">Port</a>. In those cases, you could start several or even a pool of Python processes that you would manage.</p>
<p>
Furthermore, depending on your needs, you may also be able to interoperate through higher-level abstractions. For example, for AI workflows, you can run pre-trained models directly, some via <a href="https://github.com/elixir-nx/bumblebee">Bumblebee</a>, others via <a href="https://github.com/elixir-nx/ortex">Ortex</a>. When using an LLM, you often end up talking to a third-party provider, or perhaps you run a drop-in llama.cpp Docker container on-premise, optimised for inference. In such cases the interface is HTTP and Elixir has high-level tools for interacting with LLMs too, namely <a href="https://github.com/thmsmlr/instructor_ex">Instructor</a> and <a href="https://github.com/brainlid/langchain">LangChain</a>.</p>
<p>
That said, if you do decide that Pythonx fits into your application, you can configure it to download all Python dependencies at compile time and include them as part of the Elixir release. For more details, refer to <a href="https://hexdocs.pm/pythonx/Pythonx.html#module-usage-application">this section</a> in the doc.</p>
<p>
You could also use Pythonx to give you immediate access to more tools to unblock you. Once your idea pays off, you can invest more time to arrive at a Elixir-centric solution, if you so desire.</p>
<h2>
It’s Fine</h2>
<p>
Speaking of interoperability, I mentioned that Pythonx uses NIFs. NIFs are Elixir functions with the implementation living in C. We reach for NIFs either when we want to write native code with mutability for something performance-critical or when we integrate with third-party libraries via C API (often both).</p>
<p>
To give an example, below is a NIF implementation that adds two numbers.</p>
<pre><code><span>#include</span><span> </span><span>&lt;</span><span>erl_nif.h</span><span>&gt;</span><span>
</span><span>
</span><span>ERL_NIF_TERM </span><span>add</span><span>(</span><span>ErlNifEnv</span><span>*</span><span> </span><span>env</span><span>,</span><span> </span><span>int</span><span> </span><span>argc</span><span>,</span><span> </span><span>const</span><span> ERL_NIF_TERM </span><span>argv</span><span>[</span><span>]</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>int</span><span> x</span><span>,</span><span> y</span><span>;</span><span>
</span><span>
</span><span>  </span><span>if</span><span> </span><span>(</span><span>argc </span><span>!=</span><span> </span><span>2</span><span> </span><span>||</span><span> </span><span>!</span><span>enif_get_int</span><span>(</span><span>env</span><span>,</span><span> argv</span><span>[</span><span>0</span><span>]</span><span>,</span><span> </span><span>&amp;</span><span>x</span><span>)</span><span> </span><span>||</span><span> </span><span>!</span><span>enif_get_int</span><span>(</span><span>env</span><span>,</span><span> argv</span><span>[</span><span>1</span><span>]</span><span>,</span><span> </span><span>&amp;</span><span>y</span><span>)</span><span>)</span><span> </span><span>{</span><span>
</span><span>    </span><span>return</span><span> </span><span>enif_make_badarg</span><span>(</span><span>env</span><span>)</span><span>;</span><span>
</span><span>  </span><span>}</span><span>
</span><span>
</span><span>  </span><span>int</span><span> result </span><span>=</span><span> x </span><span>+</span><span> y</span><span>;</span><span>
</span><span>
</span><span>  </span><span>return</span><span> </span><span>enif_make_int</span><span>(</span><span>env</span><span>,</span><span> result</span><span>)</span><span>;</span><span>
</span><span>}</span><span>
</span><span>
</span><span>ErlNifFunc nif_funcs</span><span>[</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span>
</span><span>  </span><span>{</span><span>"</span><span>add</span><span>"</span><span>,</span><span> </span><span>2</span><span>,</span><span> add</span><span>}</span><span>
</span><span>}</span><span>;</span><span>
</span><span>
</span><span>ERL_NIF_INIT</span><span>(</span><span>Elixir</span><span>.</span><span>MyLib</span><span>.</span><span>NIF</span><span>,</span><span> nif_funcs</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>)</span></code></pre>
<p>
Looking at the signature, you can see that the function receives a C-array of Erlang terms and returns an Erlang term. We are responsible for converting between terms and C data structures using the <code>enif_*</code> APIs. The example may look pretty straightforward, though it is a fair amount of boilerplate code to end up adding two numbers. From there the ceremony escalates quickly once we need to deal with nested data structures and return errors more specific than <code>:badarg</code>. A natural progression is to extract some of the logic to helper functions, but this doesn’t fully alleviate the boilerplate and it results in reinventing the wheel a lot.</p>
<p>
Additionally, Pythonx (and other NIF-extensive projects) actually use C++, while the <code>enif_*</code> APIs are (rightfully so) C. Since C++ brings more powerful constructs, theoretically there is a possibility of a more expressive API, however it is also easy to get into weeds with C++ metaprogramming. The main question I asked myself is how far can we go inferring the conversion from types. With <a href="https://github.com/rusterlium/rustler">Rustler</a> and <a href="https://github.com/E-xyza/zigler">Zigler</a>, NIFs are written as regular functions and the data structures conversion is handled automatically based on the signature types.</p>
<p>
This brings us to <a href="https://github.com/elixir-nx/fine">Fine</a>, C++ library enabling more ergonomic NIFs, tailored to Elixir. Let’s see an update example:</p>
<pre><code><span>#include</span><span> </span><span>&lt;</span><span>fine.hpp</span><span>&gt;</span><span>
</span><span>
</span><span>int64_t</span><span> </span><span>add</span><span>(</span><span>ErlNifEnv </span><span>*</span><span>env</span><span>,</span><span> </span><span>int64_t</span><span> </span><span>x</span><span>,</span><span> </span><span>int64_t</span><span> </span><span>y</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>return</span><span> x </span><span>+</span><span> y</span><span>;</span><span>
</span><span>}</span><span>
</span><span>
</span><span>FINE_NIF</span><span>(</span><span>add</span><span>,</span><span> </span><span>0</span><span>)</span><span>;</span><span>
</span><span>
</span><span>FINE_INIT</span><span>(</span><span>"</span><span>Elixir.MyLib.NIF</span><span>"</span><span>)</span><span>;</span></code></pre>
<p>
Other than extendable encoding/decoding, Fine provides smart pointers to safely manage resource objects and support for raising exceptions anywhere in the NIF. I’ve refactored EXLA NIFs to use Fine and <a href="https://github.com/elixir-nx/nx/pull/1581">it removed over 1k LOC</a>, so it may be worth considering next time you have to write some NIFs.</p>
<h2>
Summing up</h2>
<p>
When we started Numerical Elixir, our goal was for Elixir to develop and have its own identity within the data and machine learning ecosystem. Now we are ready to make interoperability a key focus of our efforts too.</p>
<p>
Pythonx embeds Python into Elixir, bringing a new class of interoperability with a third-party language not seen before within the Erlang VM. It is more than just integrating the Python interpreter, it is about transparently translating idioms from one language to the other.</p>
<p>
The Fine project also consolidates and streamlines our collective experiences in integrating C++ and Elixir, tracing back to Sean Moriarity’s work on Nx four years ago.</p>
<p>
There is more to come.</p>
<p>
Stay interoperable!</p>

  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Signal to leave Sweden if backdoor law passes (456 pts)]]></title>
            <link>https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</link>
            <guid>43171205</guid>
            <pubDate>Tue, 25 Feb 2025 12:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden">https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</a>, See on <a href="https://news.ycombinator.com/item?id=43171205">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single-entry-content"><p>According to Whittaker, the bill requires the encrypted messaging app Signal to install so-called backdoors in the software.</p>
<blockquote>
<p>If you create a vulnerability based on Swedish wishes, it would create a way to undermine our entire network. Therefore, we would never introduce these backdoors, she says.</p>
</blockquote>
<p>The purpose of the bill – which may be passed next year – is for the police and Security Service to be able to request message history in retrospect for individuals suspected of crimes.</p>
<p>The Armed Forces, on the other hand, are negative and write in a letter to the government that the proposal cannot be realized "without introducing vulnerabilities and backdoors that can be exploited by third parties", reports SVT.</p></section></div>]]></description>
        </item>
    </channel>
</rss>