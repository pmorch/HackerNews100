<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 14 Aug 2024 19:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Inside the "3 billion people" national public data breach (133 pts)]]></title>
            <link>https://www.troyhunt.com/inside-the-3-billion-people-national-public-data-breach/</link>
            <guid>41248104</guid>
            <pubDate>Wed, 14 Aug 2024 16:50:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.troyhunt.com/inside-the-3-billion-people-national-public-data-breach/">https://www.troyhunt.com/inside-the-3-billion-people-national-public-data-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=41248104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<section>
<p>I decided to write this post because there's no concise way to explain the nuances of <a href="https://www.conchovalleyhomepage.com/news/data-of-3-billion-people-exposed-in-one-of-the-largest-data-breaches-in-history-heres-what-you-need-to-know/?ref=troyhunt.com" rel="noreferrer">what's being described as one of the largest data breaches ever</a>. Usually, it's easy to articulate a data breach; a service people provide their information to had someone snag it through an act of unauthorised access and publish a discrete corpus of information that can be attributed back to that source. But in the case of National Public Data, we're talking about a data aggregator most people had never heard of where a "threat actor" has published various <em>partial </em>sets of data with no clear way to attribute it back to the source. And <a href="https://news.bloomberglaw.com/privacy-and-data-security/background-check-data-of-3-billion-stolen-in-breach-suit-says?ref=troyhunt.com" rel="noreferrer">they're already the subject of a class action</a>, to add yet another variable into the mix. I've been collating information related to this incident over the last couple of months, so let me talk about what's known about the incident, what data is circulating and what remains a bit of a mystery.</p><p>Let's start with the easy bit - who is <a href="https://nationalpublicdata.com/?ref=troyhunt.com" rel="noreferrer">National Public Data</a> (NPD)? They're what we refer to as a "data aggregator", that is they provide services based on the large volumes of personal information they hold. From the front page of their website:</p><blockquote>Criminal Records, Background Checks and more. Our services are currently used by investigators, background check websites, data resellers, mobile apps, applications and more.</blockquote><p>There are <em>many</em> legally operating data aggregators out there... and there are many that end up with their data in <a href="https://haveibeenpwned.com/?ref=troyhunt.com" rel="noreferrer">Have I Been Pwned</a> (HIBP). For example, <a href="https://www.troyhunt.com/questions-about-the-massive-south-african-master-deeds-data-breach-answered/" rel="noreferrer">Master Deeds</a>, <a href="https://www.wired.com/story/exactis-database-leak-340-million-records/?ref=troyhunt.com" rel="noreferrer">Exactis</a> and <a href="https://hackenproof.com/blog/industry-news/another-decision-makers-database-leaked?ref=troyhunt.com" rel="noreferrer">Adapt</a>, to name but a few. In April, we started seeing news of National Public Data and billions of breached records, with one of the first references coming from the Dark Web Intelligence account:</p>

<blockquote><p lang="en" dir="ltr">USDoD Allegedly Breached National Public Data Database, Selling 2.9 Billion Records <a href="https://t.co/emQIZ0lgsn?ref=troyhunt.com">https://t.co/emQIZ0lgsn</a> <a href="https://t.co/Tt8UNppPSu?ref=troyhunt.com">pic.twitter.com/Tt8UNppPSu</a></p>— Dark Web Intelligence (@DailyDarkWeb) <a href="https://twitter.com/DailyDarkWeb/status/1777335594567283045?ref_src=twsrc%5Etfw&amp;ref=troyhunt.com">April 8, 2024</a></blockquote> 

<p>Back then, the breach was attributed to "USDoD", a name to remember as you'll see that throughout this post. The embedded image is the first reference of the 2.9B number we've subsequently seen flashed all over the press, and it's right there alongside the request of $3.5M for the data. Clearly, there is a financial motive involved here, so keep that in mind as we dig further into the story. That image also refers to 200GB of compressed data that expands out to 4TB when uncompressed, but that's not what initially caught my eye. Instead, something quite obvious in the embedded image doesn't add up: if this data is "the entire population of USA, CA and UK" (which is ~450M people in total), what's the 2.9B number we keep seeing? Because that doesn't reconcile with reports about <a href="https://mashable.com/article/background-check-company-breached-3-billion-affected?ref=troyhunt.com" rel="noreferrer">"nearly 3 billion people" with social security numbers exposed</a>. Further, SSNs are a rather American construct with Canada having SINs (Social Insurance Number) and the UK having, well, NI (National Insurance) numbers are probably the closestequivalent. This is the constant theme you'll read about in this post, stuff just being a bit... off. But hyperbole is often a theme with incidents like this, so let's take the headlines with a grain of salt and see what the data tells us.</p><p>I was first sent data allegedly sourced from NPD in early June. The corpus I received reconciled with what vx-underground reported on around the same time (note their reference to the 8th of April, which also lines up with the previous tweet):</p>

<blockquote><div lang="en" dir="ltr"><p>April 8th, 2024, a Threat Actor operating under the moniker "USDoD" placed a large database up for sale on Breached titled: "National Public Data". They claimed it contained 2,900,000,000 records on United States citizens. They put the data up for sale for $3,500,000.</p><p>National…</p></div>— vx-underground (@vxunderground) <a href="https://twitter.com/vxunderground/status/1797047998481854512?ref_src=twsrc%5Etfw&amp;ref=troyhunt.com">June 1, 2024</a></blockquote> 

<p>In their message, they refer to having received data totalling 277.1GB <em>uncompressed, </em>which aligns with the sum total of the 2 files I received:</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-2.png" alt="" loading="lazy" width="444" height="342"></figure><p>They also mentioned the data contains first and last names, addresses and SSNs, all of which appear in the first file above (among other fields):</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-1.png" alt="" loading="lazy" width="1185" height="219" srcset="https://www.troyhunt.com/content/images/size/w600/2024/08/image-1.png 600w, https://www.troyhunt.com/content/images/size/w1000/2024/08/image-1.png 1000w, https://www.troyhunt.com/content/images/2024/08/image-1.png 1185w"></figure><p>These first rows also line up precisely with <a href="https://dailydarkweb.net/usdod-allegedly-leaks-national-public-data-database-exposing-2-9-billion-records/?ref=troyhunt.com" rel="noreferrer">the post Dark Web Intelligence included in the earlier tweet</a>. And in case you're looking at it and thinking "that's the same SSN repeated across multiple rows with different names", those records are all the same people, just with the names represented in different orders and with different addresses (all in the same city). In other words, those 6 rows only represent one person, which got me thinking about the ratio of rows to distinct numbers. Curious, I took 100M samples and found that only 31% of the rows had unique SSNs, so extrapolating that out, 2.9B would be more like 899M. This is something to always be conscious of when you read headline numbers: "2.9B" doesn't necessarily mean 2.9B <em>people</em>, it often means <em>rows of data</em>. Speaking of which, those 2 files contain 1,698,302,004 and 997,379,506 rows respectively for a combined total of 2.696B. Is this where the headline number comes from? Perhaps, it's close, and <a href="https://www.bleepingcomputer.com/news/security/hackers-leak-27-billion-data-records-with-social-security-numbers/?ref=troyhunt.com" rel="noreferrer">it's also precisely the same as Bleeping Computer reported a few days ago</a>.</p><p>At this point in the story, there's no question that there is legitimate data in there. From the aforementioned Bleeping Computer story:</p><blockquote>numerous people have confirmed to us that it included their and family members'&nbsp;legitimate information, including those who are deceased</blockquote><p>And in vx-underground's tweet, they mention that:</p><blockquote>It also allowed us to find their parents, and nearest siblings. We were able to identify someones parents, deceased relatives, Uncles, Aunts, and Cousins. Additionally, we can confirm this database also contains informed on individuals who are deceased. Some individuals located had been deceased for nearly 2 decades.</blockquote><p>A quick tangential observation in the same tweet:</p><blockquote>The database DOES NOT contain information from individuals who use data opt-out services. Every person who used some sort of data opt-out service was not present.</blockquote><p>Which is what you'd expect from a legally operating data aggregator service. It's a minor point, but it does support the claim that the data came from NPD.</p><p><strong>Important:</strong> None of the data discussed so far contains email addresses. That doesn't necessarily make it any less impactful for those involved, but it's an important point I'll come back to later as it relates to HIBP.</p><p>So, this data appeared in limited circulation as early as 3 months ago. It contains a huge amount of personal information (even if it isn't "2.9B people"), and then to make matters worse, it was posted publicly last week:</p>

<blockquote><p lang="en" dir="ltr">National Public Data, a service by Jerico Pictures Inc., suffered <a href="https://twitter.com/hashtag/databreach?src=hash&amp;ref_src=twsrc%5Etfw&amp;ref=troyhunt.com">#databreach</a>. Hacker “Fenice” leaked 2.9b records with personal details, including full names, addresses, &amp; SSNs in plain text. <a href="https://t.co/fXY3SXEiKe?ref=troyhunt.com">https://t.co/fXY3SXEiKe</a></p>— Wolf Technology Group (@WolfTech) <a href="https://twitter.com/WolfTech/status/1820883462082973977?ref_src=twsrc%5Etfw&amp;ref=troyhunt.com">August 6, 2024</a></blockquote> 

<p>Who knows who "Fenice" is and what role they play, but clearly multiple parties had access to this data well in advance of last week. I've reviewed what they posted, and it aligns with what I was sent 2 months ago, which is bad. But on the flip side, at least it has allowed services designed to protect data breach victims to get notices out to them:</p>

<blockquote><p lang="en" dir="ltr">Twice this week I was alerted my SSN was found on the web thanks to a data breach at National Public Data. Cool. Thanks guys. <a href="https://t.co/FAlfNmXUqm?ref=troyhunt.com">pic.twitter.com/FAlfNmXUqm</a></p>— MrsNineTales (@MrsNineTales) <a href="https://twitter.com/MrsNineTales/status/1821586613111345594?ref_src=twsrc%5Etfw&amp;ref=troyhunt.com">August 8, 2024</a></blockquote> 

<p>Inevitably, breaches of this nature result in legal action, which, as I mentioned in the opening paragraph, <a href="https://news.bloomberglaw.com/privacy-and-data-security/background-check-data-of-3-billion-stolen-in-breach-suit-says?ref=troyhunt.com" rel="noreferrer">began a couple of weeks ago</a>. It looks like a tip-off from a data protection service was enough for someone to bring a case against NPD:</p><blockquote>Named plaintiff Christopher Hofmann, a California resident, said he received a notification from his identity-theft protection service provider on July 24, notifying him that his data was exposed in a breach and leaked on the dark web.</blockquote><p>Up until this point, pretty much everything lines up, but for one thing: Where is the 4TB of data? And this is where it gets messy as we're now into the territory of "partial" data. For example, this corpus from last month was posted to a popular hacking forum:</p>

<blockquote><div lang="en" dir="ltr"><p>National Public Database Allegedly Partially Leaked</p><p>It is stated that nearly 80 GB of sensitive data from the National Public Data is available.</p><p>The post contains different credits for the leakage and the alleged breach was credited to a threat actor “Sxul” and stressed that it… <a href="https://t.co/v8uq0o88NS?ref=troyhunt.com">https://t.co/v8uq0o88NS</a> <a href="https://t.co/a6dn3MvYkf?ref=troyhunt.com">pic.twitter.com/a6dn3MvYkf</a></p></div>— Dark Web Intelligence (@DailyDarkWeb) <a href="https://twitter.com/DailyDarkWeb/status/1815722467798680005?ref_src=twsrc%5Etfw&amp;ref=troyhunt.com">July 23, 2024</a></blockquote> 

<p>That's 80GB, and whilst it's not clear whether that's the size of the compressed or extracted archive, either way, it's still a long way short of the full alleged 4TB. Do take note of the file name in the embedded image, though - "people_data-935660398-959524741.csv" - as this will come up again later on.</p><p>Earlier this month, a 27-part corpus of data alleged to have come from NPD was posted to Telegram, this image representing the first 10 parts at 4GB each:</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-3.png" alt="" loading="lazy" width="324" height="634"></figure><p>The compressed archive files totalled 104GB and contained what feels like a fairly random collection of data:</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-4.png" alt="" loading="lazy" width="736" height="1012" srcset="https://www.troyhunt.com/content/images/size/w600/2024/08/image-4.png 600w, https://www.troyhunt.com/content/images/2024/08/image-4.png 736w"></figure><p>Many of these files are archives themselves, with many of <em>those </em>then containing yet more archives. I went through and recursively extracted everything which resulted in a total corpus of 642GB of uncompressed data across more than 1k files. If this is "partial", what was the story with the 80GB "partial" from last month? Who knows, but in the in those files above were 134M unique email addresses.</p><p>Just to take stock of where we're at, we've got the first set of SSN data which is legitimate and contains no email addresses yet is allegedly only a small part of the total NPD corpus. Then we've got this second set of data which is larger and has tens of millions of email addresses yet is pretty random in appearance. The burning question I was trying to answer is "is it legit?"</p><p>The problem with verifying breaches sourced from data aggregators is that nobody willingly - <em>knowingly</em> - provides their data to them, so I can't do my usual trick of just asking impacted HIBP subscribers if they'd used NPD before. Usually, I also can't just look at a data aggregator breach and find pointers that tie it back to the company in question due to references in the data mentioning their service. In part, that's because this data is just so damn generic. Take the earlier screenshot with the SSN data; how many different places have your first and last name, address, SSN, etc? Attributing a source when there's only generic data to go by is <em>extremely</em> difficult.</p><p>The kludge of different file types and naming conventions in the image above worried me. Is this actually all from NPD? Usually, you'd see some sort of continuity, for example, a heap of .json files with similar names or a swathe of .sql files with each one representing a dumped table. The presence of "people_data-935660398-959524741.csv" ties this corpus together with the one from the earlier tweet, but then there's stuff like "Accuitty_10_1_2022.zip"; could that refer to Acuity (single "c", single "t") <a href="https://www.troyhunt.com/acuity-who-attempts-and-failures-to-attribute-437gb-of-breached-data/" rel="noreferrer">which I wrote about in November</a>? HIBP isn't returning hits for email addresses in that folder against the Acuity I loaded last year, so no, it's a different corpus. But that archive alone ended up having over 250GB of data with almost 100M unique email addresses, so it forms a <em>substantial</em> part of the overall corpus of data.</p><p>The 3,608,086KB "criminal_export.csv.zip" file caught my eye, in part because <a href="https://nationalpublicdata.com/services.html?ref=troyhunt.com" rel="noreferrer">criminal record checks are a key component NPD's services</a>, but also because it was only a few months ago we saw <a href="https://www.malwarebytes.com/blog/news/2024/05/criminal-record-database-of-millions-of-americans-dumped-online?ref=troyhunt.com" rel="noreferrer">another breach containing 70M rows from a US criminal database</a>. And see who that breach was attributed to? USDoD, the same party whose name is all over the NPD breach. I did actually receive that data but filed it away and didn't load it into HIBP as there were no email addresses in it. I wonder if the data from that story lines up with the file in the image above? Let's check the archives:</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-5.png" alt="" loading="lazy" width="604" height="56" srcset="https://www.troyhunt.com/content/images/size/w600/2024/08/image-5.png 600w, https://www.troyhunt.com/content/images/2024/08/image-5.png 604w"></figure><p>Different file name, but hey, it's a 3,608,086KB file! Given the NPD breach initially occurred in April and the criminal data hit the news in May, it's entirely possible the latter was obtained from the former, but I couldn't find any mention of this correlation anywhere. (Side note: this is a <em>perfect</em> example of why I retain breaches in offline storage after processing because they're so often helpful when assessing the origin and legitimacy of <em>new </em>breaches).</p><p>Continuing the search for oddities, I decided to see if I myself was in there. On many occasions now, I've loaded a breach, started the notification process running, walked away from the PC then received an email from myself about being in the breach 🤦‍♂️ I'm continually surprised by the places I find <em>myself</em> in, including this one:</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-6.png" alt="" loading="lazy" width="1062" height="504" srcset="https://www.troyhunt.com/content/images/size/w600/2024/08/image-6.png 600w, https://www.troyhunt.com/content/images/size/w1000/2024/08/image-6.png 1000w, https://www.troyhunt.com/content/images/2024/08/image-6.png 1062w"></figure><p>Dammit! It's an email address of mine, yet clearly, none of the other data is mine. Not my name, not my address, and the obfuscated numbers definitely aren't familiar to me (I don't believe they're SSNs or other sensitive identifiers, but because I can't be sure, I've obfuscated them). I suspect one of those numbers is a serialised date of birth, but of the total <em>28 rows with my email address on them</em>, the two unique DoBs put "me" as being born in either 1936 or 1967. Both are a long way from the truth.</p><p>A cursory review of the other data in this corpus revealed a wide array of different personal attributes. One file contained information such as height, weight, eye colour, and ethnicity. The "uk.txt" file in the image above merely contained a business directory with public information. I could have dug deeper, but by now, there was no point. There's clearly some degree of invalid data in here, there's definitely data we've seen appear separately as a discrete breach, and there are many different versions of "partial" NPD data (although the 27-part archive discussed here is the largest I saw and the one I was most consistently directed to by other people). The more I searched, the more bits and pieces attributed back to NPD I found:</p><figure><img src="https://www.troyhunt.com/content/images/2024/08/image-7.png" alt="" loading="lazy" width="604" height="368" srcset="https://www.troyhunt.com/content/images/size/w600/2024/08/image-7.png 600w, https://www.troyhunt.com/content/images/2024/08/image-7.png 604w"></figure><p>If I were to take a guess, there are two likely explanations for what we're seeing:</p><ol><li>This incident got a lot of press due to the legitimacy of the initial dump of SSNs, and the subsequent partial dumps are riding on the coattails of breach hysteria</li><li>NPD siphoned up a heap of publicly circulating data to enrich their offering, and it got snagged along with the initially released SSN data</li></ol><p>Both of these are purely speculative, though, and the only parties that know the truth are the anonymous threat actors passing the data around and the data aggregator that's now being sued in a class action, so yeah, we're not going to see any reliable clarification any time soon. Instead, we're left with 134M email addresses in public circulation and no clear origin or accountability. I sat on the fence about what to do with this data for days, not sure whether I should load it and, if I did, whether I should write about it. Eventually, I decided it deserved a place in HIBP as <a href="https://haveibeenpwned.com/FAQs?ref=troyhunt.com#UnverifiedBreach" rel="noreferrer">an unverified breach</a>, and per the opening sentence, this blog post was the only way I could properly explain the nuances of what I discovered. This way, impacted people will know if their data is floating around in this corpus, and if they find this information unactionable, then they can do precisely what they would have done had I not loaded it - nothing.</p><p>Lastly, I want to re-emphasise a point I made earlier on: <em>there were no email addresses in the social security number files</em>. If you find yourself in this data breach via HIBP, there's no evidence your SSN was leaked, and if you're in the same boat as me, the data next to your record may not even be correct. And no, I don't have a mechanism to load additional attributes beyond email address into HIBP nor point people in the direction of the source data (<a href="https://x.com/haveibeenpwned/status/1821844023088464377?ref=troyhunt.com" rel="noreferrer">some of you will have received a reminder about why I don't do that just a few days ago</a>). And I'm <em>definitely </em>not equipped to be your personal lookup service, manually trawling through the data and pulling out individual records for you! So, treat this as informational only, an intriguing story that doesn't require any further action.</p>
<section>
<a href="https://www.troyhunt.com/tag/security/">Security</a>
</section>
</section>

</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: If YouTube had actual channels (881 pts)]]></title>
            <link>https://ytch.xyz</link>
            <guid>41247023</guid>
            <pubDate>Wed, 14 Aug 2024 15:10:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ytch.xyz">https://ytch.xyz</a>, See on <a href="https://news.ycombinator.com/item?id=41247023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>Contact</p>
                <div><p><a href="https://ytch.xyz/cdn-cgi/l/email-protection" data-cfemail="7b020f18130302013b1c161a121755181416">[email&nbsp;protected]</a></p></div>
                <p>Support</p>
                <p>bc1q4s2f6df2cqa8stenwp8y5tlmd5pywy8dwqqxvh</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Texas sues GM for unlaw­ful­ly collecting and selling dri­vers' pri­vate data [pdf] (138 pts)]]></title>
            <link>https://www.texasattorneygeneral.gov/sites/default/files/images/press/General%20Motors%20Data%20Privacy%20Petition%20Filed.pdf</link>
            <guid>41246050</guid>
            <pubDate>Wed, 14 Aug 2024 13:50:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.texasattorneygeneral.gov/sites/default/files/images/press/General%20Motors%20Data%20Privacy%20Petition%20Filed.pdf">https://www.texasattorneygeneral.gov/sites/default/files/images/press/General%20Motors%20Data%20Privacy%20Petition%20Filed.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41246050">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Hackers may have leaked the Social Security Numbers of every American (113 pts)]]></title>
            <link>https://www.engadget.com/cybersecurity/hackers-may-have-leaked-the-social-security-numbers-of-every-american-150834276.html</link>
            <guid>41245779</guid>
            <pubDate>Wed, 14 Aug 2024 13:22:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/cybersecurity/hackers-may-have-leaked-the-social-security-numbers-of-every-american-150834276.html">https://www.engadget.com/cybersecurity/hackers-may-have-leaked-the-social-security-numbers-of-every-american-150834276.html</a>, See on <a href="https://news.ycombinator.com/item?id=41245779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Several months after a hacking group claimed to be selling nearly 3 billion records stolen from a prominent data broker, much of the information appears to have been leaked on a forum. According to <a data-i13n="cpos:1;pos:1" href="https://www.bleepingcomputer.com/news/security/hackers-leak-27-billion-data-records-with-social-security-numbers/" rel="nofollow noopener" target="_blank" data-ylk="slk:Bleeping Computer;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas"><em>Bleeping Computer</em></a>, the data dump includes 2.7 billion records of personal info for people in the US, such as names, Social Security Numbers, potential aliases and all physical addresses they are known to have lived at.</p><p>The data, which is unencrypted, is believed to have been obtained from a broker called National Public Data. It's said that the business assembles profiles for individuals by scraping information from public sources and then sells the data for the likes of background checks and looking up criminal records. (A proposed class-action suit was <a data-i13n="cpos:2;pos:1" href="https://news.bloomberglaw.com/privacy-and-data-security/background-check-data-of-3-billion-stolen-in-breach-suit-says" rel="nofollow noopener" target="_blank" data-ylk="slk:filed;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas">filed</a> against National Public Data over the breach earlier this month.)</p><p>In April, hacking collective USDoD attempted to sell 2.9 billion records it claimed was stolen from the company and included personal data on everyone in the US, UK and Canada. The group was looking <a data-i13n="cpos:3;pos:1" href="https://x.com/H4ckManac/status/1777246310782902686" rel="nofollow noopener" target="_blank" data-ylk="slk:for $3.5 million;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">for $3.5 million</a> for the whole 4TB database, but since then chunks of the data have been leaked by various entities.</p><p>Previous leaks included phone numbers and email addresses, but those reportedly weren't included in the latest and most comprehensive dump. As such, you won't be able to check whether your information has been included in this particular leak by punching your email address into <a data-i13n="cpos:4;pos:1" href="https://haveibeenpwned.com/" rel="nofollow noopener" target="_blank" data-ylk="slk:Have I Been Pwned?;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">Have I Been Pwned?</a></p><p>The data includes multiple records for many people, with one for each address they are known to have lived at. The dump comprises two text files that amount to a total of 277GB. It's not really possible for any independent body to confirm that the data includes records for every person in the US, but as <em>Bleeping Computer</em> points out, the breach is likely to include information on anyone who is living in the country.</p><p>The publication states that several people confirmed the information that the dump has on them and their family members (including some dead relatives) is accurate, but in other cases some SSNs were associated with the wrong individuals. <em>Bleeping Computer</em> posits that the information may have been stolen from an old backup as it doesn't include the current home address for the people whose details its reporters checked against the data.</p><p>In any case, it's worth taking some steps to protect yourself against any negative repercussions from the leak, such as fraud and identity theft. Be extra vigilant against scammers and phishing attacks that look to obtain access to your online accounts.</p><p>Keep an eye on credit reports to see if there has been any fraudulent activity on your accounts and inform credit bureaus <a data-i13n="elm:affiliate_link;sellerN:Experian;elmt:;cpos:5;pos:1" href="https://shopping.yahoo.com/rdlw?merchantId=759bd82a-f743-4b0b-91d8-ad446a0d2ec6&amp;siteId=us-engadget&amp;pageId=1p-autolink&amp;featureId=text-link&amp;merchantName=Experian&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3d3dy5leHBlcmlhbi5jb20vZnJlZXplL2NlbnRlci5odG1sIiwiY29udGVudFV1aWQiOiI3OGU5YWQ3Yy0zOTI4LTQ1MGUtYjJmOC1lNTNmMWY2MGVmMGMiLCJvcmlnaW5hbFVybCI6Imh0dHBzOi8vd3d3LmV4cGVyaWFuLmNvbS9mcmVlemUvY2VudGVyLmh0bWwifQ&amp;signature=AQAAAXpWrdlgzbftNpnS9d44Qr19yfYTIhaKb71MYEqIJXLh&amp;gcReferrer=https%3A%2F%2Fwww.experian.com%2Ffreeze%2Fcenter.html" rel="nofollow noopener" target="_blank" data-ylk="slk:Experian;elm:affiliate_link;sellerN:Experian;elmt:;cpos:5;pos:1;itc:0;sec:content-canvas">Experian</a>,<a data-i13n="cpos:6;pos:1" href="https://www.equifax.com/personal/credit-report-services/credit-freeze/" rel="nofollow noopener" target="_blank" data-ylk="slk:Equifax;cpos:6;pos:1;elm:context_link;itc:0;sec:content-canvas"> Equifax</a> and<a data-i13n="elm:affiliate_link;sellerN:TransUnion;elmt:;cpos:7;pos:1" href="https://shopping.yahoo.com/rdlw?merchantId=3ac38c55-e0c9-42ec-92cc-7cff5ea0e351&amp;siteId=us-engadget&amp;pageId=1p-autolink&amp;featureId=text-link&amp;merchantName=TransUnion&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3d3dy50cmFuc3VuaW9uLmNvbS9jcmVkaXQtZnJlZXplIiwiY29udGVudFV1aWQiOiI3OGU5YWQ3Yy0zOTI4LTQ1MGUtYjJmOC1lNTNmMWY2MGVmMGMiLCJvcmlnaW5hbFVybCI6Imh0dHBzOi8vd3d3LnRyYW5zdW5pb24uY29tL2NyZWRpdC1mcmVlemUifQ&amp;signature=AQAAAU-aMKxlUrwwcH5b5ZhDK4znS2Y1TF7B3Gt_6z9UXcaS&amp;gcReferrer=https%3A%2F%2Fwww.transunion.com%2Fcredit-freeze" rel="nofollow noopener" target="_blank" data-ylk="slk:TransUnion;elm:affiliate_link;sellerN:TransUnion;elmt:;cpos:7;pos:1;itc:0;sec:content-canvas"> TransUnion</a> if so. You can ask the bureaus to put a freeze on your credit files to stop anyone else opening a bank account, taking out a loan or obtaining a credit card under your name.</p><p>You can sign up for services that offer identity fraud protection and <a data-i13n="cpos:8;pos:1" href="https://www.engadget.com/google-is-making-it-easier-to-remove-your-private-information-from-search-170025085.html" data-ylk="slk:remove your personal information;cpos:8;pos:1;elm:context_link;itc:0;sec:content-canvas">remove your personal information</a> from the public web to reduce the chances that you'll be negatively impacted. However, such services often charge a fee.</p><p>Be sure to use two-factor authentication wherever possible (preferably with you obtaining codes from an authenticator app rather than SMS). And, as always, we highly recommend <a data-i13n="cpos:9;pos:1" href="https://www.engadget.com/best-password-manager-134639599.html" data-ylk="slk:having a password manager;cpos:9;pos:1;elm:context_link;itc:0;sec:content-canvas">having a password manager</a>, never reusing the same login credentials for different services and regularly changing the password on your most sensitive accounts.</p><p>This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Larry Tesler pioneered cut-and-paste, the one-button mouse, WYSIWIG (2005) (127 pts)]]></title>
            <link>https://spectrum.ieee.org/of-modes-and-men</link>
            <guid>41244847</guid>
            <pubDate>Wed, 14 Aug 2024 11:22:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/of-modes-and-men">https://spectrum.ieee.org/of-modes-and-men</a>, See on <a href="https://news.ycombinator.com/item?id=41244847">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
        Iconography: In 1973 Larry Tesler predicted that icons would replace the lists of names then typical of computer interfaces. Variants of the trash-can icon have since become ubiquitous
    </p><div data-headline="Of Modes and Men" data-elid="2650248043" data-post-url="https://spectrum.ieee.org/of-modes-and-men" data-authors="Tekla S. Perry" data-page-title="Of Modes and Men - IEEE Spectrum"><p><strong>Like Woody Allen’s</strong> 1983 movie character, Zelig, who appears at every significant historical event of his era, Larry Tesler has had a hand in major events making computer history during the past 30 years. When the first document-formatting software was developed at Stanford University in 1971, Tesler was coding it. When a secretary first cut and pasted some text on a computer screen at Xerox Corp.’s Palo Alto Research Center (PARC) in 1973, Tesler was looking over her shoulder. When the first portable computer was turned on in an airport waiting area (and on an airplane), Tesler had his fingers on the keyboard. When <a href="https://spectrum.ieee.org/tag/steve-jobs">Steve Jobs</a> went to PARC in 1979 to see the legendary demo that is purported to have set the stage for a revolution in computing, Tesler had his hand on the mouse.</p><p>And when Apple Computer Inc.’s infamous Newton handheld computer failed spectacularly in the early 1990s, taking millions of dollars of investment and a few <a href="https://spectrum.ieee.org/topic/careers/">careers</a> down with it, Tesler was there, too. Hey, nobody gets it right 100 percent of the time.</p><p>So why haven’t you heard of him? [see sidebar, "<a href="#LarrysLexicon">Larry's Lexicon</a>"].</p><p>“Larry generated a lot of the basic ideas for the work we were doing,” says Douglas Fairbairn, a former colleague at PARC. “But he doesn’t have a big ego, so his name didn’t get attached to things. He wasn’t the one guy who did one big thing you’ll remember him for; he was a collaborator on many things.”</p><p>This past May, Tesler joined Yahoo Inc., in Sunnyvale, Calif., as vice president of user experience and design. At Yahoo, Tesler has perhaps his biggest opportunity yet to shape the way hundreds of millions of people interact with technology as they search the Web, watch and listen to media, share things they’ve created or found, correspond with family and friends, and even find love through the Internet.</p><p>When Tesler was a child, his parents couldn’t quite figure him out, so they had him tested by a career counselor. The counselor told them that the results were unusual—Tesler registered a strange combination of sensitivity to people and fascination with math. The best career choice the counselor could suggest was working as an architect or maybe becoming a certified public accountant.</p><p>The field of computer usability did not yet exist. At the time, people had to learn arcane codes to communicate with computers, typically via punch cards and printouts. The idea of making a computer easy for the average person to use was still a new one when Tesler came along.</p><p>“I was born to do it and feel very lucky to have been in the right place at the right time,” he says.</p><p>It’s a recurring theme in his career, starting with Stanford University, in California, in the early 1960s. Computer time was available free of charge to anybody who wanted it, and Tesler, a math major, wanted it very much. He wrote a few little programs for his own amusement, and he programmed enhancements to the software that created graphic patterns for the 3465-seat card section at Stanford Stadium.</p><p>Soon he got a paid job in Stanford’s computer laboratory developing software that would make strings of computer code easier for humans to read, by formatting the text into either fixed-width columns or free-form lines. It wasn’t a new idea, he says, just a nicer implementation of an old idea.</p><p>But this was the first bit of computer code Tesler wrote that was meant to be used by large numbers of people. And he was hooked, by programming in general and by text formatting in particular. Text formatting—the ability to display or print out type in different sizes and with different fonts, spacing, and so on—doesn’t generally get much attention in treatises on how the computer has transformed civilization. But it is actually one of the most common computer functions and the foundation of such programs as <a href="https://spectrum.ieee.org/tag/microsoft">Microsoft</a> Word. After college in the early 1960s, Tesler spent a few years working as a computer consultant. As one of only four listed in the Palo Alto, Calif., area phone book, he got a lot of calls. He wrote programs that scheduled classroom use for the San Jose school district and that simulated the distribution of nuclear fallout for the Stanford Research Institute (now called SRI International).</p><p>When the Silicon Valley recession of the late 1960s dried up consulting contracts, he managed to find a job at the Stanford <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">Artificial Intelligence</a> Laboratory, where he considered how computers might be made to understand natural human language and even to express simulated human thoughts and feelings.</p><p>“This work was technically fascinating,”&nbsp;Tesler recalls, “but one day I woke up and realized that this stuff wouldn’t be practical for decades.”</p><p>And he had bigger woes, too: by 1970 his brief marriage to his college sweetheart had broken up. He packed his 5-year-old daughter and her favorite toys into his aging Dodge Dart and moved to Oregon, joining a wave of Vietnam-era dropouts. There, he and a group of friends bought land and began building their own houses. Eventually, he had a nice house but no savings. That was when he noticed that there was only one computer within a 60-kilometer radius, at a bank.</p><p>The bank wouldn’t hire him, so Tesler called his former colleagues at the Stanford AI laboratory for help. He hadn’t been in touch with them for six months.</p><p>“Did you know that Xerox started a computer research center in Palo Alto?” one asked Tesler. “About a week after you left for Oregon, Alan Kay, who planned to go to Xerox himself, came here looking for you.” Kay, now a senior fellow with Hewlett-Packard Co., in Palo Alto, recalls that he even went to Oregon to try to track Tesler down. Kay had worked briefly at the Stanford AI lab and had considered Tesler “one of the most interesting intellectuals and one of the best programmers there.”</p><p>Tesler immediately contacted Robert Taylor, director of the computer science laboratory at PARC, asking for a part-time job, so he could continue to live in Oregon. But PARC would consider Tesler only for full-time positions; Tesler wasn’t interested. A month later, still jobless, he reconsidered, but by then Xerox had instituted a hiring freeze. The opportunity was gone.</p><p>Tesler went back to the Stanford AI lab, hoping he could find something to do there that was more “real” than advanced research. The lab manager, Les Earnest, was interested in computer typesetting, a nascent technology at the time, and assigned Tesler to develop a “document compiler,” something that could format text and generate footnotes, tables of contents, indexes, and bibliographies.</p><p>So in 1971 Tesler wrote a piece of software that would allow users to tuck formatting commands inside normal text as they typed, like inserting a parenthetical expression. When the computer displayed or printed text coded in this manner, it would adjust the text as instructed—boldface or italic, for example—and hide the instruction. This software, called Pub, was one of the first, if not the first, of what would become a “markup language with embedded tags and scripting.” The best-known such language today is Dynamic HTML, used in Web design. Pub was distributed through the ARPANet, a precursor to the Internet, and widely used in universities.</p><p>Tesler continued to check in with PARC, and at the end of 1971, Xerox offered him a job in the On-Line Office System Group, then building an ambitious distributed operating system for office applications. Tesler was interested in personal computers, so the project didn’t thrill him. And he thought the offered pay was too low. So he turned PARC down. Again.</p><p><strong>More than a year</strong> went by. It was early 1973, and PARC was in the thick of developing what would be the first personal computer, the Alto. It made Tesler yet another job offer, and this time he didn’t refuse. He would split his time between the Office System Group and Alan Kay’s Learning Research Group.</p><p>One goal of the Learning Research Group was to develop Smalltalk, the first dynamic object-oriented programming language—the first language, that is, designed to create programming entities that had characteristics and attributes that could be passed on to other entities. A generic “job” entity, for example, might have such attributes as a salary, a department, and health benefits. Another entity, such as “manager,” could be created, automatically inheriting those attributes and gaining additional ones, such as supervisory duties or security clearances. Smalltalk back in the 1970s contained almost all of the concepts in today’s enormously popular Java language.</p><p>As one of Tesler’s first tasks at PARC, he and a co-worker wrote a paper on the future of interactive computing, which for the first time talked about cut-and-paste as a way of moving blocks of text, images, and the like. It also described representing documents and other office objects stored on the computer as tiny images—icons—instead of as a list of names [see photo, ].</p><p>Today, icons are everywhere—as folders, documents, and the trash can or recycle bin for deleted files. The radicalness of the concepts is apparent only when you understand that, in those days, opening up a document required knowing—and typing in—its exact name and document type.</p><p>Before Tesler arrived, the Office System Group had already formulated some ideas about how they expected users to interact with computers. Their concept was based on the three-button mouse and depended heavily on the use of modes. In a mode-based system, you first tell the computer what type of thing you are going to do, and then you do it. So if you want to insert text in a document, you put the computer in insert mode, select your insert point, type in the text, and then exit the insert mode. If you want to delete, you put the computer in delete mode, select the text to be removed, execute the deletion, and exit the mode. It could take weeks or months to become proficient in the use of the system, but at the time, practitioners believed nothing much simpler than that could offer as much power to the user.</p><p>“I was aghast,” Tesler says. He reasoned that ordinary users wouldn’t invest more than a few minutes in learning a user interface and that they would lose track of modes, risking deleting instead of inserting, for example, with potentially disastrous results. But Tesler found few allies among his colleagues, who, incredibly to Tesler, felt that a moded interface was intuitive.</p><p>Meanwhile, PARC hired Sylvia Adams, now Sylvia Amundsen, a secretary whose previous high-tech experience had been limited to the <a href="https://spectrum.ieee.org/tag/ibm">IBM</a> Selectric typewriter. When she arrived for her first day of work, Tesler recalls, “I grabbed her. I didn’t want her to get contaminated by some word processor that we were using.”</p><p>He sat her in front of a blank computer screen and gave her a printed page of text that he had marked up with corrections. “See this text?” he asked. “Pretend it’s on the screen. And see these proofreading marks? Your job is to make those changes on the screen. How would you do it?”</p><p>“Well,” she said, “I have to insert something there, so I would point there, and then I would type what I wanted. And to delete this, I would draw through it.” Tesler took notes as Adams invented, in effect, the modeless user interface for text editing.</p><p>Then Tesler had her use the moded system in use at PARC. She hated it; she got stuck frequently in the wrong mode and had trouble getting out of it. She proved his theory that it was a bad system for a nontechnical beginner.</p><p>The test was enough to convince Tesler’s boss, Bill English, to allow Tesler to work on an alternative to the moded interface. As he developed it, he continued his user experiments, regularly grabbing “civilians” from the building lobby—delivery people, friends picking up employees for lunch, anybody without computer experience. Tesler sat them down in front of a computer screen now running prototype software and asked them to edit text.</p><p>“They were such novices,” Tesler recalls. “With one couple, the first thing they said when looking at the crisp text on the screen was, ‘You get really good TV reception in here.’”</p><p>After writing a simple text editor called Mini Mouse, he went on to develop Gypsy, a modeless text-processing system that was the first to use many now-familiar elements: the cut-and-paste function to move text, a fill-in form to enter search terms, selection of text by holding down a mouse button and dragging the cursor through it, bold and italic type styles, and what-you-see-is-what-you-get printing. Tesler coined that phrase and other now-common terms. Gypsy also first implemented click-to-open files; previously, the only way to open a file was to type its name and the Open command. Gypsy was never commercialized, but it was used for years to edit manuscripts at Ginn and Co., a textbook publisher then owned by Xerox. Tesler went on to develop other pathbreaking text-formatting software, but none of it made it to market.</p><p><strong>In the mid-1970s,</strong> the Holy Grail of Kay’s Learning Research Group at PARC was the Dynabook. A name dreamed up by Kay, Dynabook was to be a portable computer smaller than today’s laptops. It would do just about everything anyone could then imagine a computer doing, including storing everything you needed on a daily basis and communicating with other computers when necessary. It was a pretty audacious idea. At the time, PARC’s Alto, one of the smallest stand-alone computers then in existence, was about the size and weight of a hotel minibar refrigerator. The Dynabook was clearly way off in the future.</p><p>
  “I was born to do it and feel very lucky to have been in the right place at the right&nbsp;time.”
</p><p>Kay regarded the 1973 Alto as an interim Dynabook that did the right things but was too big to carry around. In 1978 Kay and his colleague, Adele Goldberg, felt it was finally possible to build the next computer that would lead to Dynabook: Notetaker. It would be portable, use floppy disks (then a brand-new storage medium), and run on batteries. It would have only one function—it would allow students to take notes. PARC’s Fairbairn set out to design the hardware and enlisted Tesler to assist.</p><p>Tesler didn’t know a whole lot about hardware; his entire hardware engineering experience consisted of once building a joystick. Fairbairn, a hardware pro, said, “Hey, it’s not hard,” and handed him a manual for the just-introduced <a href="https://spectrum.ieee.org/tag/intel">Intel</a> 8086, which was to become the first commercially successful 16-bit microprocessor.</p><p>Tesler read the manual and designed the circuitry for the processor board while Fairbairn worked on the many other boards, interfaces, and specialized hardware needed. Within a week of the release of sample chips, the processor board was wrapped. And it didn’t work. The fault was neither Tesler’s nor his prototype software’s. The Intel documentation had numbered the pins on a peripheral chip backward, and as the first developer to build a prototype, Tesler was the one who found the bug.</p><p>Because the PARC researchers’ vision for the Dynabook included networking, the Notetaker was to include an Ethernet card. Existing ones contained more than 100 chips. But the size constraints for a portable machine meant the Notetaker’s Ethernet card could hold 24 chips, tops. Tesler decided to move a number of functions to software, ending up needing only 20 chips. In effect, he had come up with the first software implementation of the Ethernet protocol.</p><p>The Notetaker worked; 10 prototypes were built, and Tesler and Fairbairn went on the road. They crisscrossed the country, visiting Xerox executives in several locations, trying to convince someone—anyone—that Notetaker should be a product. Some executives seemed excited but took no action beyond referring the duo to others. This went on for months.</p><p><img alt="Larry Tesler." data-rm-shortcode-id="faa30b93626ee85533c26e2ff69f3bb2" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/larry-tesler.jpg?id=25560152&amp;width=980" height="713" id="372ac" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/larry-tesler.jpg?id=25560152&amp;width=980" width="600"><small placeholder="Add Photo Caption...">Road Warrior: Larry Tesler and cohort Douglas Fairbairn were the first ever to use a portable computer in an airport and on an airplane.</small><small placeholder="Add Photo Credit...">Photo: Brian Smale</small></p><p>One night in 1979, in the midst of this road show, Tesler and Fairbairn were sitting in the gate area at San Francisco International Airport, waiting to board their red-eye flight to New York City. Because the Notetaker was a secret project, they had never turned the 16-kilogram computer on outside of a Xerox building. But the gate area was nearly empty. So they opened up Notetaker, powered it up from its built-in battery, ran its Smalltalk program, and quickly turned it off again [see photo, "Road Warrior"].</p><p>“That was the first time a portable computer was ever used in an airport,” Tesler says. “We think.”</p><p>The duo couldn’t resist, Fairbairn says, turning the computer on again, during that flight. History was made once again. The flight attendants didn’t even notice.</p><p>By the Fall of 1979, it had become clear that Xerox executives just weren’t buying the Notetaker. They had their hands full competing with an onslaught of low-cost copiers from Asia. Meanwhile, in December 1979, Steve Jobs and his colleagues from Apple Computer Inc., in Cupertino, Calif., took their now-famous tour of PARC. Tesler was on hand to demonstrate the Alto’s user interface.</p><p>“The questions the Apple people were asking totally blew me away,” Tesler says. “They were the kind of questions Xerox executives should have been asking but didn’t. They asked: ‘Why don’t the windows refresh automatically? Why did you do the menus this way?’”</p><p>“And the question I remember most was from Steve Jobs. He said, ’You guys are sitting on a gold mine here. Why aren’t you making this a product?’”</p><p>After that meeting, Tesler decided that his next job would be at Apple. He would stay at PARC long enough to get them to accept the Notetaker, finish the page layout system still in development, and give a scheduled talk at the 1980 Association for Computing Machinery Siggraph conference, the big annual meeting for the computer graphics community.</p><p>The Siggraph talk was key, Tesler felt, because he had for the first time obtained permission from Xerox to publicly demonstrate some of the word-processing and text-formatting programs developed at PARC. He believed this demonstration would take trade secret status away from PARC’s pioneering concepts, thereby allowing him to work on similar technology at Apple without breaking his confidentiality agreement.</p><p>He wasn’t the only researcher talking about leaving PARC in the early 1980s. Kay recalls that life at PARC was getting to be more and more of a strain.</p><p>Tesler started working for Apple shortly after his Siggraph speech. Bruce Daniels, the software manager on the Lisa personal computer project, hired him.</p><p>“There was no question that we would hire him,” recalls Daniels, now a program manager at Sun Microsystems Inc., in Menlo Park, Calif. “We didn’t need to interview him. We had seen this vision at PARC, we loved it, but how could we build it? He knew everything; he was this font of wisdom, he could answer any question, he was passionate—it was the proverbial match made in heaven.”</p><p>Lisa, released in 1983, was technologically dazzling. Coming just a year after Xerox’s successor to the Alto, the Star office workstation, Lisa was the first commercially available personal computer that, like the networked office Star, used windows, icons, and a mouse—all the elements of the graphical user interface that we now take for granted. Ultimately, though, the US $8000 system was commercially unsuccessful. Apple stopped manufacturing the Lisa after only two years; just 30,000 had been sold.</p><p>In the Lisa group, Tesler contributed to “absolutely everything,” Daniels says [see photo, "Larry and Lisa"]. Tesler, more modestly, says that Bill Atkinson and Jef Raskin were well into the Lisa interface design when he got there, but he feels he had some influence. He talked them into using cut-and-paste, he recalls, and wrote a memo that argued persuasively for the one-button mouse, a concept that Tesler had advocated at PARC and that Raskin had been promoting at Apple.</p><p><img alt="group shot" data-rm-shortcode-id="07e81ff355bf540b64828fdaeba8538e" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/group-shot.jpg?id=25560153&amp;width=980" height="917" id="1d798" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/group-shot.jpg?id=25560153&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">Larry and Lisa: The team responsible for the design of Apple’s Lisa Computer, released in 1983, included Larry Tesler (far right).</small><small placeholder="Add Photo Credit...">Photo: Roger Ressmeyer/Corbis</small></p><p>“I had asked him to suggest how many buttons should go on the mouse,” Daniels recalls. “I made the decision based on his comments, and the Apple mouse still has just one button.”</p><p>Tesler brought his idea of click-and-drag for selecting things over from PARC as well, along with the pop-up menus that had been used in Smalltalk. He designed the software widget that makes windows divisible (as is commonplace now in the spreadsheet program Microsoft Excel). And when Atkinson was working on the menu bar, trying to attach it to windows in such a way that it wouldn’t obscure text or get too small, Tesler pointed to the top of the screen. “I said, ‘It should be at the top,’”&nbsp;he recalls. “It should be there, always in the same place.” Atkinson implemented the menu bar that night, and by dawn he had come up with menus that dropped down as you dragged the mouse and shortcut keys.</p><p>“He invented the whole thing,” Tesler says, “but I gave him the inspiration.”</p><p>Meanwhile, the Macintosh project, for a cheaper computer than the Lisa, geared up. Tesler was not officially part of it; he continued to develop software for the Lisa group. But he spent one day a week in the Mac building critiquing the user interface under development. Convinced by tests with ordinary users, the Mac designers ultimately decided to base the Mac interface on Lisa, with a menu bar at the top and folders and documents on the screen, instead of the metaphor then being tried—a large picture of a floppy disk taking up most of the screen, with icons of documents scattered on top of it.</p><p>In 1986 Tesler was named vice president of Apple’s newly formed Advanced Technology Group, the group that contributed to the design of the next-generation product, the Macintosh II. At one point, researchers hooked 24 Mac IIs together to render an animated short, called <em>Pencil Test</em>, which was shown at the 1988 Siggraph conference. Movie director John Lasseter, more recently of Pixar fame, advised on the project; at the time, he was dating a woman in the group (whom he later married).</p><p>Struggles in doing <em>Pencil Test</em>, particularly in synchronizing sound and video, led to another small research group, the Time Lords. The software that the Time Lords developed was called QuickTime, a form of which still ships with every Macintosh computer and is also widely used on Windows computers.</p><p>With Tesler at the helm, the Advanced Technology Group grew to some 200 people. One of the many projects under development was a full-function handheld Mac. A prototype was up and running when Tesler made what he recalls as one of the worst decisions of his career: he killed the project. Instead, he devoted himself to the development of the Newton, a handheld computer that was to be based on a new technology incompatible with the Macintosh line of computers.</p><p>The year was 1990. Tesler had just taken over management of the Newton project, which had been under way for several months. Apple president Jean-Louis Gassee, who had championed the project, had just quit the company; the Newton project leader left a week later.</p><p>
  “I did a few things right at the beginning of that project,” Tesler recalls, “and then I made an unbelievable series of mistakes.”
</p><p>Competition between the handheld Mac group and the Newton group had developed; it was obvious that one had to go. Tesler had to choose which. The handheld Mac worked; the Newton was still just research. However, Newton proponents convinced him that the handheld Mac was a dead end, because it ran an old version of the Mac operating system and wasn’t going to be powerful enough to run the new version when it came out.</p><p>The handheld Mac was relegated to history, and all bets were on the Newton.</p><p>“I did a few things right at the beginning of that project,” Tesler recalls, “and then I made an unbelievable series of mistakes.”</p><p>When Tesler joined the Newton group, the computer was envisioned as a 20- by 28-centimeter machine, weighing about 3 kilograms and selling for over $7000. He insisted developers get the price down to $1000, and he worked out an arrangement to obtain processors designed by Acorn Computer Ltd., in Cambridge, England. He even convinced Apple to invest in a newly created company, Advanced RISC Machines Ltd., also in Cambridge, that would produce them.</p><p>That, Tesler recalled, was one of his good decisions, as was taking the nascent spread-spectrum wireless communications technology being developed for the Newton and moving it into the Advanced Technology Group. That project team eventually joined with development teams in other companies and created the Wi-Fi wireless local-area network standard.</p><p>Then the mistakes began. For one, Tesler killed an internal handwriting recognition project, but he didn’t give up on handwriting recognition altogether. Instead, he licensed technology from ParaGraph, a company founded in Moscow. It didn’t work on Newton’s relatively slow processor.</p><p>Looking back, he says he should have realized that while the Newton technology had great potential, it was too immature for a product that needed to get out the door. Eventually, the Newton handheld computer was released.</p><p>“And it was a disaster,” Tesler recalls. He left the group just before the product was shipped, because at that point he was against releasing the product at all. He stayed at Apple for four more years as chief scientist. “It was a consolation prize,” he says. He proposed a number of projects; none got funded. Eventually, he took on the job of managing network products, a profitable business but one without the risk-taking innovation that excited him. His final task before quitting in 1997 was to shut down the Advanced Technology Group; the company felt it could no longer afford basic research.</p><p>While closing the group, Tesler became enamored with one project, a programming language called Cocoa. Like many of his technical loves, Cocoa embraced object-oriented programming. But unusually for such a language, Cocoa was to be used to allow schoolchildren, with no previous programming experience, to easily create their own simulations and video games. With Apple’s blessing, Tesler spun Cocoa off into a company, Stagecast Software Inc., now in Burlingame, Calif.</p><p>The software was renamed Stagecast Creator. It was aimed to the then-hot creativity and education market. It is still available and today is a standard tool in computer camps.</p><p>The group moved into offices in Palo Alto in 1997. Tesler met with potential investors, developed marketing strategies, reviewed code under development, and even bought the office trash cans.</p><p>Unfortunately for Stagecast, the product was released in 1999, just as the education software market went bust. By mid-2000, it was clear that Stagecast wasn’t going to be able to support its 15 employees, much less make a fortune for its investors. Even though the product was loved by its users and won countless awards, the market was just too&nbsp;small.</p><p>“Millions of dollars went down the drain,” says Tesler, who never drew a salary from Stagecast, “including a lot of my personal money. A friend had told me that if I wasn’t willing to invest half my assets in an idea, I didn’t believe in it enough. And the more my investors and I sank into it, the more I felt that some day we’d figure out how to turn the corner. But after the market crash, what had been a fraction of my wealth suddenly became almost all of it.”</p><p>Still, he had no plans to leave, thinking he would stay with the company, maintain the product, and eventually find a buyer.</p><p>His wife finally intervened. “She told me, ‘Stop dreaming, it’s over,’” he recalls. “And she got me to face the fact that I was just going to have to walk away.”</p><p>So he left. Two employees remained and, to date, make enough from keeping the product on the market to support themselves modestly. Meanwhile, a recruiter put Tesler in touch with Amazon.com Inc.</p><p><strong>At first, he was hesitant.</strong> He perceived <a href="https://spectrum.ieee.org/tag/amazon">Amazon</a> as a retail company, and he didn’t know anything about retail. Besides, the job was at the company’s home office in Seattle, about 1300 km north of his home in Portola Valley, Calif. But, at the urging of a former Apple co-worker who was at Amazon, Tesler went to Seattle to meet with several people, and he was hooked.</p><p>“When I got there, it was very clear that Amazon is a technology company that happens to be a retailer,” he says. And while they couldn’t do much more than hint about it, Tesler felt that Amazon was getting ready to do something interesting in user interface design.</p><p>He joined Amazon in October 2001 as an engineering vice president and spent his first few months getting his bearings. He then took on the position of vice president of shopping experience, focusing on the usability of Amazon’s Web site. He contributed to the recent launch of the tool that allows customers to search within books, as well as other user interface developments that are still confidential.</p><p>He loved it, particularly, he says, “going home and realizing that tens of millions of people are going to be using what I just worked on.”</p><p>It’s a recurring theme with him: the sheer delight of knowing that people use his work. His happiest memories from the Stagecast years were of trips to classrooms where kids were using his software. He still gets a little thrill when friends mention how they found what they needed at Amazon.</p><p>This past spring Tesler left Amazon. His problem wasn’t with the company but rather with having a job in Seattle and a wife and roots in Silicon Valley. In May he joined Yahoo. Will he have his hand in another interface revolution there?</p><p>Changing the way people do things is never easy. Says Daniels: “You have to have the ability to guess years ahead and the self-assurance that you are right.”</p><p>Tesler, Daniels thinks, can do that. And so does Yahoo. Yahoo expects Tesler to spark innovations all over the company.</p><p>“There’s never been a more exciting time for user interaction innovators,” Tesler says. “What we create today may not be as seminal as what we created 30 years ago. But the challenges are greater, and the solutions will have immediate impact on a lot of people.”</p><p>Besides, he says, “It should be fun.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open-source LLM provider price comparison (112 pts)]]></title>
            <link>https://github.com/arc53/llm-price-compass</link>
            <guid>41244648</guid>
            <pubDate>Wed, 14 Aug 2024 10:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/arc53/llm-price-compass">https://github.com/arc53/llm-price-compass</a>, See on <a href="https://news.ycombinator.com/item?id=41244648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">
  LLM price compass 🧭
</h2><a id="user-content---llm-price-compass-" aria-label="Permalink: 
  LLM price compass 🧭
" href="#--llm-price-compass-"></a></p>
<p dir="auto">
  <strong>Open-Source LLM inference cost comparison</strong>
</p>
<p dir="auto">This project aim at collecting benchmark data from different gpu on different clouds/providers and comparing it to fixed per token costs from other providers. This project will help you choose right gpu, cloud, provider for the model of your choice.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please refer to the <a href="https://github.com/arc53/llm-price-compass/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> file for information about how to get involved. We welcome issues, questions, and pull requests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Llama 3.1 instruct 8b comparison</h2><a id="user-content-llama-31-instruct-8b-comparison" aria-label="Permalink: Llama 3.1 instruct 8b comparison" href="#llama-31-instruct-8b-comparison"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/15183589/357752381-6f017e0d-5b6c-430c-9162-15321491ee5a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM2NTMzMDUsIm5iZiI6MTcyMzY1MzAwNSwicGF0aCI6Ii8xNTE4MzU4OS8zNTc3NTIzODEtNmYwMTdlMGQtNWI2Yy00MzBjLTkxNjItMTUzMjE0OTFlZTVhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE0VDE2MzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWVmNDkxNjRhNzIzYzA0YzdkMjdmNDNjN2EyODExZDQ1ZTI1YmNhOGQ3NzJkODIyMDNjMGUwN2Q4YWI2NzcwMDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.dQLzePOV6KXTYeHRqBpPFI9cWKfPUmS7BMp-J7ygJc0"><img src="https://private-user-images.githubusercontent.com/15183589/357752381-6f017e0d-5b6c-430c-9162-15321491ee5a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM2NTMzMDUsIm5iZiI6MTcyMzY1MzAwNSwicGF0aCI6Ii8xNTE4MzU4OS8zNTc3NTIzODEtNmYwMTdlMGQtNWI2Yy00MzBjLTkxNjItMTUzMjE0OTFlZTVhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE0VDE2MzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWVmNDkxNjRhNzIzYzA0YzdkMjdmNDNjN2EyODExZDQ1ZTI1YmNhOGQ3NzJkODIyMDNjMGUwN2Q4YWI2NzcwMDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.dQLzePOV6KXTYeHRqBpPFI9cWKfPUmS7BMp-J7ygJc0" alt="CleanShot 2024-08-14 at 10 14 07@2x"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Llama 3.1 instruct 70b comparison</h2><a id="user-content-llama-31-instruct-70b-comparison" aria-label="Permalink: Llama 3.1 instruct 70b comparison" href="#llama-31-instruct-70b-comparison"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/15183589/357752569-2b21f7e6-3874-4ff6-8f21-d692b37d17d6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM2NTMzMDUsIm5iZiI6MTcyMzY1MzAwNSwicGF0aCI6Ii8xNTE4MzU4OS8zNTc3NTI1NjktMmIyMWY3ZTYtMzg3NC00ZmY2LThmMjEtZDY5MmIzN2QxN2Q2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE0VDE2MzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU4YWNmOGJiZTFmZmE2NjBiMmVkNDQzM2M0NTU2NTljMTdmYmI4MjQ2ODFhMTMzNDg1Y2RjMDkxZjFiYmUyNDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ZiPXLYEzrMeOG8kgrHRo5q0L-EHfmgheAysCf1ZG-Qg"><img src="https://private-user-images.githubusercontent.com/15183589/357752569-2b21f7e6-3874-4ff6-8f21-d692b37d17d6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM2NTMzMDUsIm5iZiI6MTcyMzY1MzAwNSwicGF0aCI6Ii8xNTE4MzU4OS8zNTc3NTI1NjktMmIyMWY3ZTYtMzg3NC00ZmY2LThmMjEtZDY5MmIzN2QxN2Q2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTQlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE0VDE2MzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU4YWNmOGJiZTFmZmE2NjBiMmVkNDQzM2M0NTU2NTljMTdmYmI4MjQ2ODFhMTMzNDg1Y2RjMDkxZjFiYmUyNDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ZiPXLYEzrMeOG8kgrHRo5q0L-EHfmgheAysCf1ZG-Qg" alt="CleanShot 2024-08-14 at 10 14 54@2x"></a></p>
<p dir="auto"><a href="https://github.com/arc53/llm-price-compass/blob/main/LLM-Provider-comparison.csv">Source Providers</a>
<a href="https://github.com/arc53/llm-price-compass/blob/main/gpu-benchmarks.csv">Source Self-hosted</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code Of Conduct</h2><a id="user-content-code-of-conduct" aria-label="Permalink: Code Of Conduct" href="#code-of-conduct"></a></p>
<p dir="auto">We as members, contributors, and leaders, pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Please refer to the <a href="https://github.com/arc53/llm-price-compass/blob/main/CODE_OF_CONDUCT.md">CODE_OF_CONDUCT.md</a> file for more information about contributing.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Syndicated Actor Model (121 pts)]]></title>
            <link>https://syndicate-lang.org/about/</link>
            <guid>41244468</guid>
            <pubDate>Wed, 14 Aug 2024 10:12:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://syndicate-lang.org/about/">https://syndicate-lang.org/about/</a>, See on <a href="https://news.ycombinator.com/item?id=41244468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <blockquote>
  

  <h2 id="could-there-be-a-better-way-to-program-communicating-systems">Could there be a better way to program communicating systems?</h2>

  <p>Functional programs are easy to write, but programs that interact with
each other and the outside world are much harder.</p>

  

  <p>Programming models like the Actor model and the Tuplespace model make
great strides toward simplifying programs that communicate. However, a
few key difficulties remain.</p>

  <p>The Syndicated Actor model addresses these difficulties. It is closely
related to both Actors and Tuplespaces, but builds on a different
underlying primitive: <em>eventually-consistent replication of state</em>
among actors. Its design also draws on widely deployed but informal
ideas like publish/subscribe messaging.</p>
</blockquote>

<ul id="markdown-toc">
  <li><a href="#syndicated-actors" id="markdown-toc-syndicated-actors">Syndicated Actors</a>    <ul>
      <li><a href="#securing-syndicated-interaction" id="markdown-toc-securing-syndicated-interaction">Securing syndicated interaction</a></li>
      <li><a href="#the-syndicated-actor-model-in-context" id="markdown-toc-the-syndicated-actor-model-in-context">The Syndicated Actor model in context</a></li>
      <li><a href="#actors" id="markdown-toc-actors">Actors</a></li>
      <li><a href="#tuplespaces" id="markdown-toc-tuplespaces">Tuplespaces</a></li>
      <li><a href="#publishsubscribe" id="markdown-toc-publishsubscribe">Publish/Subscribe</a></li>
    </ul>
  </li>
  <li><a href="#conversational-concurrency-1" id="markdown-toc-conversational-concurrency-1">Conversational Concurrency</a>    <ul>
      <li></li>
      <li><a href="#conversational-concurrency-in-brief" id="markdown-toc-conversational-concurrency-in-brief">Conversational Concurrency in brief</a></li>
      <li><a href="#conversational-frames-conversational-knowledge" id="markdown-toc-conversational-frames-conversational-knowledge">Conversational frames, conversational knowledge</a></li>
    </ul>
  </li>
  <li><a href="#language-support-syndicate-dsl" id="markdown-toc-language-support-syndicate-dsl">Language support: Syndicate DSL</a>    <ul>
      <li><a href="#facets-represent-sub-conversations" id="markdown-toc-facets-represent-sub-conversations">Facets represent (sub-)conversations</a></li>
      <li><a href="#a-taste-of-the-syndicatejs-dsl" id="markdown-toc-a-taste-of-the-syndicatejs-dsl">A taste of the Syndicate/js DSL</a></li>
    </ul>
  </li>
  <li><a href="#polyglot-and-networked-programming-preserves" id="markdown-toc-polyglot-and-networked-programming-preserves">Polyglot and networked programming: Preserves</a>    <ul>
      <li><a href="#preserves-a-data-model-with-associated-syntax" id="markdown-toc-preserves-a-data-model-with-associated-syntax">Preserves: a data model with associated syntax</a></li>
      <li><a href="#syndicated-actor-network-protocol" id="markdown-toc-syndicated-actor-network-protocol">Syndicated Actor network protocol</a></li>
    </ul>
  </li>
  <li><a href="#who-i-am" id="markdown-toc-who-i-am">Who I am</a></li>
</ul>

<p>Three big ideas fit together to yield the complete vision:</p>

<ul>
  <li>
    <h2 id="syndicated-actor-model">Syndicated Actor Model</h2>

    <p>Integrates Actors with Tuplespace-like shared state, Ambients-like
treatment of system boundaries, and Erlang-like techniques for
signalling and recovering from partial failure. A special kind of
syndicated actor, a <em>dataspace</em>, routes and replicates published
data according to actors’ interests.</p>
  </li>
  <li>
    <h2 id="conversational-concurrency">Conversational Concurrency</h2>

    <p>A way of thinking about IPC and networking, going beyond
message-passing to include ideas of conversational frame and
conversational state.</p>
  </li>
  <li>
    <h2 id="syndicate-domain-specific-language">Syndicate Domain-Specific Language</h2>

    <p>Extends programming languages with domain-specific language (DSL)
constructs for directly expressing syndicated actor and
conversational concepts.</p>
  </li>
</ul>

<p>A few smaller pieces of the puzzle exist alongside the main ideas:</p>

<ul>
  <li>
    <p><em>Object capabilities.</em>
The only properly compositional way to secure a distributed system.</p>
  </li>
  <li>
    <p><em>A meaningful data language.</em>
For robust communication between programming languages and across
network links, we need a data language with a sensible equational
theory.</p>
  </li>
  <li>
    <p><em>Interaction protocols.</em>
“Procedure call” is baked-in to our programming languages. What
other interaction patterns deserve similar treatment?</p>
  </li>
  <li>
    <p><em>Integrated networking.</em>
After all, <a href="https://syndicate-lang.org/about/history">IPC is networking and networking is IPC</a>!
Anything more remote than “objects in the same sequential thread
(actor)” is considered “remote”, and works the same way.</p>
  </li>
</ul>

<blockquote>
  <h2 id="syndicated-actors">Syndicated Actors</h2>

  <p>The <a href="https://syndicate-lang.org/about/">Syndicated Actor model</a> is a new model
of concurrency, closely related to the actor, tuplespace, and
publish/subscribe models.</p>
</blockquote>

<p>The Syndicated Actor model is, at heart, a mechanism for sharing state
among neighboring concurrent components. The design focuses on
mechanisms for sharing state because effective mechanisms for
communication and coordination follow as special cases.</p>

<p>You can think of it as concurrent object-oriented programming with
intrinsic publish/subscribe support.</p>

<p>It is based around actors which not only exchange messages, but
publish (“assert”) selected portions of their internal state
(“assertions”) to their peers in a publish/subscribe, reactive manner.</p>

<p>A <em>dataspace</em> is a special kind of actor which <em>relays</em> messages and
<em>replicates</em> published state from peer to peer. State replication
subsumes Erlang-style links and monitors, publish/subscribe,
tuplespaces, presence notifications, directory/naming services, and
more.</p>

<h2 id="securing-syndicated-interaction">Securing syndicated interaction</h2>

<p>Access control is expressed using a generalization of <em>object
capabilities</em> to express control over state replication and
observation of replicated state as well as ordinary message-passing
and RPC. Long-lived capabilities based on
<a href="https://research.google/pubs/pub41892/">Macaroons</a> allow secure
delegation and attenuation of authority.</p>

<p>Object capabilities are a natural fit for Actor-style systems (as
demonstrated by
<a href="http://www.erights.org/">E and subsequent research and systems based on it</a>),
so it makes sense that they would work well for the Syndicated Actor
model. The main difference to capabilities for plain Actors is that
syndicated capabilities express pattern-matching-based restrictions on
the <em>assertions</em> that may be directed toward a given peer, as well as
the <em>messages</em> that may be sent its way.</p>

<h2 id="the-syndicated-actor-model-in-context">The Syndicated Actor model in context</h2>

<blockquote>
  <p>See <a href="https://syndicate-lang.org/about/syndicate-in-context/">here</a> for more
on comparisons of syndicated actors with other programming models.</p>
</blockquote>

<p>The closest relatives to the Syndicated Actor model are Actors, Tuplespaces,
and Pub/Sub:<sup id="fnref:diss-chap3" role="doc-noteref"><a href="#fn:diss-chap3" rel="footnote">1</a></sup></p>

<ul>
  <li>
    <h2 id="actors">Actors</h2>

    <p>In the Actor model<sup id="fnref:actor-model-hopl" role="doc-noteref"><a href="#fn:actor-model-hopl" rel="footnote">2</a></sup>, message passing is fundamental. But what we
really want, often, is <em>state synchronization</em>: I want some object
<em>over there</em> to know about my state <em>over here</em>; and I want this
object <em>here</em> to track the state of that object <em>there</em>.</p>

    <p>→ Actors don’t handle shared state well: programmers are left to
encode it via message-passing.</p>
  </li>
  <li>
    <h2 id="tuplespaces">Tuplespaces</h2>

    <p>With Tuplespaces, state-passing is fundamental. But the way state
moves around a tuplespace system causes problems for both
fault-tolerance and eventual consistency. Lots of research on
Tuplespaces has explored ways to repair these deficits.</p>

    <p>→ Tuplespaces don’t do messaging at all, and handle shared state in
a way that causes problems for fault-tolerance and eventual
consistency.</p>
  </li>
  <li>
    <h2 id="publishsubscribe">Publish/Subscribe</h2>

    <p>Pub/sub systems offer multicast of messages to groups of
subscribers. They work well when topic matching rules are simple
and with self-contained, ephemeral messages. They work less well
for stateful <em>conversations</em>, where joining subscribers must be
“brought up to speed” before their real-time stream of messages
starts.</p>

    <p>→ Pub/sub systems can struggle with flexibility and with management
of state.</p>
  </li>
</ul>

<p>In short, despite the fact that the same patterns of interconnection,
naming, routing and layering keep reappearing in all sorts of
software, our programming models and languages don’t do well at
letting us express these patterns. Likewise, firewalls, membranes and
system boundaries are ubiquitous, but remain unrepresented and
implicit in most approaches to concurrency.</p>

<p>Bringing these concepts into our programming models and programming
languages helps eliminate language patterns, simplifying our
programs.<sup id="fnref:diss-chap9" role="doc-noteref"><a href="#fn:diss-chap9" rel="footnote">3</a></sup></p>

<blockquote>
  <h2 id="conversational-concurrency-1">Conversational Concurrency</h2>

  

  <p>When programs are written with concurrency in mind, the programmer
reasons about the interactions between concurrent components or
agents in the program. This includes exchange of information, as
well as management of resources, handling of partial failure,
collective decision-making and so on.<sup id="fnref:diss-chap2" role="doc-noteref"><a href="#fn:diss-chap2" rel="footnote">4</a></sup></p>
</blockquote>

<p>These components might be objects, or threads, or processes, or
actors; or larger units still, such as whole machines or clusters; or
even some more nebulous and loosely-defined concept—a group of
callbacks, perhaps. The programmer has the notion of an agent in their
mind, which translates into some representation of that agent in the
program.</p>

<p>We think about the contexts (because there can be more than one) in
which agents exist in two different ways. From each agent’s
perspective, the important thing to think about is the boundary
between the agent and everything else in the system.</p>

<p>But from the system perspective, we often think about <em>conversations</em>
between agents, whether it’s just two having an exchange, or a whole
group collaborating on some task. Agents in a conversation play
different roles, join and leave the group, and build shared
conversational state.</p>

<p>Each act of communication contributes to a shared understanding of the
relevant knowledge required to undertake some task common to the
involved parties.</p>

<p>That is, <em>the purpose of communication is to share state</em>: to
replicate information from peer to peer.</p>

<h2 id="conversational-concurrency-in-brief">Conversational Concurrency in brief</h2>

<p>The idea of Conversational Concurrency can be summarised as
follows:<sup id="fnref:diss-chap2-redux" role="doc-noteref"><a href="#fn:diss-chap2-redux" rel="footnote">5</a></sup></p>

<ol>
  <li>Agents collaborate to achieve some shared task.</li>
  <li>Each task is delimited by a conversational frame.</li>
  <li>Within that frame, components share
    <ol>
      <li>knowledge related to the domain of the task at hand, and</li>
      <li>knowledge related to the knowledge, beliefs, needs, and
interests of the various participants in the collaborative
group.</li>
    </ol>
  </li>
  <li>Conversations are recursively structured by shared knowledge of
(sub-)conversational frames, defined in terms of any or all of the
types of knowledge we have discussed.</li>
  <li>Some conversations take place at different levels within a larger
frame, bridging between tasks and their subtasks.</li>
  <li>Components are frequently engaged in multiple tasks, and thus
often participate in multiple conversations at once.</li>
  <li>The knowledge a component needs to do its job is provided to it
when it is created, or later supplied to it in response to its
interests.</li>
</ol>

<p>Existing programming languages lack linguistic support for
conversational concurrency, leaving the programmer to fend for
themselves. The Syndicated Actor model, the idea of dataspaces, and
their supporting DSL are a response to this lack.</p>

<h2 id="conversational-frames-conversational-knowledge">Conversational frames, conversational knowledge</h2>

<p><a href="https://syndicate-lang.org/tonyg-dissertation/html/index.html#x_2_2_0_0_2"><img src="https://syndicate-lang.org/tonyg-dissertation/html/Figures/conversation.svg" alt="Components, tasks, and conversational structure"></a></p>

<p>The conversational state that accumulates as part of a collaboration
among components can be thought of as a collection of facts. First,
there are those facts that define the <em>frame</em> of a conversation. These
are exactly the facts that identify the task at hand; we label them
“framing knowledge”, and taken together, they are the “conversational
frame” for the conversation whose purpose is completion of a
particular shared task.</p>

<p>Just as tasks can be broken down into more finely-focused subtasks, so
conversations can be broken down into sub-conversations. Part of the
conversational state of an overarching interaction describes a frame
for each sub-conversation, within which sub-conversational state
exists. The knowledge framing a conversation acts as a bridge between
it and its wider context.</p>

<p>If domain knowledge is “what is true in the world” for the purposes of
a given conversation, and epistemic knowledge is “who knows what”, the
third piece of the puzzle is “who <em>needs</em> to know what” in order to
effectively make a contribution to the shared task at hand.</p>

<p>Knowledge of the various <em>interests</em> in a group allows collaborators
to plan their communication acts according to the needs of individual
components and the group as a whole. In conversations among people,
interests are expressed as <em>questions</em>; in a computational setting,
they are conveyed by <em>requests</em>, <em>queries</em>, or <em>subscriptions</em>.</p>

<p>The interests of components in a concurrent system thus direct the
flow of knowledge within the system.</p>

<blockquote id="dsl">
  <h2 id="language-support-syndicate-dsl">Language support: Syndicate DSL</h2>

  <p>The Syndicated Actor model, taken alone, explains communication and
coordination among components but does not offer the programmer any
assistance in structuring the internals of components.</p>

  <p>A handful of Domain-Specific Language (DSL) constructs, together
dubbed <em>Syndicate</em>, expose the primitives of the Syndicated Actor
model, the features of dataspaces, and the concepts of
conversational concurrency to the programmer in an ergonomic way.</p>
</blockquote>

<blockquote>
  <p>See the <a href="https://syndicate-lang.org/doc/syndicate/">Syndicate DSL documentation</a>
for details of the syntax and meaning of the constructs that
Syndicate adds to each underlying programming language.</p>
</blockquote>

<p>The language constructs offered by the Syndicate DSL extend the
underlying programming language that is used to write a component.
They bridge between the language’s own computational model and the
style of interaction offered by the Syndicated Actor model.</p>

<p>Each interactive component needs some way of</p>

<ol>
  <li>representing the <em>conversations</em> it is engaged in</li>
  <li><em>mapping incoming events</em> to these conversations</li>
  <li>managing the <em>shared understanding</em> that it builds as it works towards the overall program’s goal</li>
  <li>cleaning up shared state after <em>partial failure</em> of a component</li>
  <li><em>scoping</em> interactions and shared state inside the program</li>
</ol>

<h2 id="facets-represent-sub-conversations">Facets represent (sub-)conversations</h2>

<p>To address these requirements, Syndicate represents conversations with
a language construct called a <em>facet</em>. Facets are similar to the
“nested threads” of Martin Sústrik’s idea of
<a href="https://250bpm.com/blog:71/">Structured Concurrency</a> (see also
<a href="https://en.wikipedia.org/wiki/Structured_concurrency">Wikipedia</a>).</p>

<p>Every actor has at least one (root) facet, and all its facets form a
tree. Generally speaking, one facet corresponds to one conversation,
<em>framing</em> (in the sense of Conversational Concurrency) its children,
which each correspond to some <em>sub-</em>conversation within the frame of
their parent.</p>

<p>When a parent facet is shut down, all its children are shut down in an
orderly fashion, making management of conversational state
straightforward.</p>

<p>Each facet publishes (“asserts”) relevant pieces of state
(“assertions”) to relevant peers. As its internal state changes, its
published assertions are re-evaluated, and any changes are
automatically propagated to peers.</p>

<p>Facets also subscribe to assertions emanating from their peers. They
do this in a unique way: by <em>asserting their interest</em> in particular
fact(s) to the publishing peer. That is, an expression of interest is
<em>itself</em> an assertion that can be seen and reacted to by peers.</p>

<p>When a facet terminates, whether normally or as the result of an
exception, all the assertions it has published are guaranteed to be
withdrawn. Assertions thus subsume Erlang-style monitors/links,
“presence” notifications (as seen in, for example,
<a href="https://xmpp.org/rfcs/rfc6121.html">XMPP</a>), and pub/sub mechanisms
for optimizing message delivery based on subscriber presence or
absence.</p>

<h2 id="a-taste-of-the-syndicatejs-dsl">A taste of the Syndicate/js DSL</h2>

<p>To give some of the flavour of working with Syndicate DSL constructs,
here’s a program written in
<a href="https://syndicate-lang.org/code/js/">JavaScript extended with Syndicate constructs</a>:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td><pre><span>function</span> <span>chat</span><span>(</span><span>initialNickname</span><span>,</span> <span>sharedDataspace</span><span>,</span> <span>stdin</span><span>)</span> <span>{</span>
  <span>spawn</span> <span>'</span><span>chat-client</span><span>'</span> <span>{</span>
    <span>field</span> <span>nickName</span> <span>=</span> <span>initialNickname</span><span>;</span>

    <span>at</span> <span>sharedDataspace</span> <span>assert</span> <span>Present</span><span>(</span><span>this</span><span>.</span><span>nickname</span><span>);</span>
    <span>during</span> <span>sharedDataspace</span> <span>asserted</span> <span>Present</span><span>(</span><span>$who</span><span>)</span> <span>{</span>
      <span>on</span> <span>start</span> <span>console</span><span>.</span><span>log</span><span>(</span><span>`</span><span>${</span><span>who</span><span>}</span><span> arrived`</span><span>);</span>
      <span>on</span> <span>stop</span>  <span>console</span><span>.</span><span>log</span><span>(</span><span>`</span><span>${</span><span>who</span><span>}</span><span> left`</span><span>);</span>
      <span>on</span> <span>sharedDataspace</span> <span>message</span> <span>Says</span><span>(</span><span>who</span><span>,</span> <span>$what</span><span>)</span> <span>{</span>
        <span>console</span><span>.</span><span>log</span><span>(</span><span>`</span><span>${</span><span>who</span><span>}</span><span>: </span><span>${</span><span>what</span><span>}</span><span>`</span><span>);</span>
      <span>}</span>
    <span>}</span>

    <span>on</span> <span>stdin</span> <span>message</span> <span>Line</span><span>(</span><span>$text</span><span>)</span> <span>{</span>
      <span>if</span> <span>(</span><span>text</span><span>.</span><span>startsWith</span><span>(</span><span>'</span><span>/nick </span><span>'</span><span>))</span> <span>{</span>
        <span>this</span><span>.</span><span>nickname</span> <span>=</span> <span>text</span><span>.</span><span>slice</span><span>(</span><span>6</span><span>);</span>
      <span>}</span> <span>else</span> <span>{</span>
        <span>send</span> <span>sharedDataspace</span> <span>message</span> <span>Says</span><span>(</span><span>this</span><span>.</span><span>nickname</span><span>,</span> <span>text</span><span>);</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</pre></td></tr></tbody></table></code></pre></div>

<blockquote id="preserves">
  <h2 id="polyglot-and-networked-programming-preserves">Polyglot and networked programming: Preserves</h2>

  <p>Interoperation among programming languages and across network links
demands a shared data language and a common network protocol.</p>
</blockquote>

<p>For components to communicate, they need a common vocabulary.</p>

<p>When Syndicate is used to organise concurrency within a single
program, in a single programming language, that language’s own data
structures suffice.</p>

<p>But when Syndicate is used to organise larger systems, perhaps written
as separate, interoperating programs in many different languages,
perhaps communicating via network links (e.g. Bluetooth, Ethernet, or
TCP/IP), a common data language is needed.</p>

<p>A new data language called <a href="https://preserves.dev/">Preserves</a>
plays the role of common data language in this project, and a network
protocol built using Preserves allows expression of syndicated-actor
actions and events across network links.</p>

<h2 id="preserves-a-data-model-with-associated-syntax">Preserves: a data model with associated syntax</h2>

<blockquote>
  <p>See the <a href="https://preserves.dev/">Preserves website</a> for more
about Preserves.</p>
</blockquote>

<p>From the introduction to the <a href="https://preserves.dev/preserves.html">Preserves specification document</a>:</p>

<blockquote>
  <p>Preserves supports <em>records</em> with user-defined <em>labels</em>, embedded
<em>references</em>, and the usual suite of atomic and compound data types,
including <em>binary</em> data as a distinct type from text strings. Its
<em>annotations</em> allow separation of data from metadata such as
comments, trace information, and provenance information.</p>

  <p>Preserves departs from many other data languages in defining how to
<em>compare</em> two values. Comparison is based on the data model, not on
syntax or on data structures of any particular implementation
language.</p>
</blockquote>

<p>Preserves supports <em>schemas</em> for describing the structure and
interesting content of Preserves documents. It has multiple <em>syntaxes</em>
for different purposes: one is human-readable, and looks a lot like
JSON; another is a compact binary format, suitable for
<a href="https://preserves.dev/canonical-binary.html">canonicalization</a>
and hashing.</p>

<p>Here are some example Preserves documents, written in
<a href="https://preserves.dev/preserves.html#textual-syntax">Preserves text syntax</a>:</p>

<ul>
  <li>
    <pre><code>&lt;Present "Alice"&gt;
</code></pre>

    <p>A simple record, used in the example chat program above to assert
presence of a user within a chat room.</p>
  </li>
  <li>
    <pre><code>&lt;Says "Alice" "Hello everybody!"&gt;
</code></pre>

    <p>Again, a simple record, used as a message indicating that a user
said something to be forwarded on to peers in a chat room.</p>
  </li>
  <li>
    <pre><code>&lt;Observe &lt;Present *&gt; #:&lt;MyRef 7&gt;&gt;
</code></pre>

    <p>A more complex record, representing an asserted <em>interest</em> in other
assertions. The first field in the <code>Observe</code> record is a pattern
over asserted records. The second is an <em>embedded capability</em>
denoting the endpoint within a facet within an actor that should
receive notifications as matching records come and go.</p>
  </li>
  <li>
    <pre><code>version 1 .
embeddedType Actor.Ref .
Present = &lt;Present @username string&gt;.
Says = &lt;Says @who string @what string&gt;.
</code></pre>

    <p>A Preserves schema describing the allowable formats of assertions
and messages in the example “chat” protocol, usable within
<em>patterns</em> as well as for <em>attenuating authority</em> in syndicated
capabilities.</p>
  </li>
</ul>

<ul>
  <li>
    <pre><code>&lt;bundle {
  [simple-chat-protocol]: &lt;schema {
    version: 1,
    embeddedType: &lt;ref [Actor] Ref&gt;,
    definitions: {
      Says: &lt;rec &lt;lit Says&gt; &lt;tuple [
        &lt;named who &lt;atom String&gt;&gt;,
        &lt;named what &lt;atom String&gt;&gt;
      ]&gt;&gt;,
      Present: &lt;rec &lt;lit Present&gt; &lt;tuple [
        &lt;named username &lt;atom String&gt;&gt;
      ]&gt;&gt;
    }
  }&gt;
}&gt;
</code></pre>

    <p>This is the compiled form of the schema given above. Schema
compilers accept documents in this format,<sup id="fnref:meta-schema" role="doc-noteref"><a href="#fn:meta-schema" rel="footnote">6</a></sup> and
generate types representing the definitions in the schema bundle
along with code for parsing and reconstructing the Preserves
documents corresponding to instances of those types.</p>
  </li>
</ul>

<h2 id="syndicated-actor-network-protocol">Syndicated Actor network protocol</h2>

<blockquote>
  <p>See the <a href="https://syndicate-lang.org/doc/protocol/">protocol specification</a>
for more on syndicated communication across network links.</p>
</blockquote>

<p>The following Preserves schema describes <em>turns</em>, which are messages
to be sent across a network link expressing <em>assertions</em>,
<em>retractions</em> of previous assertions, the sending of <em>messages</em>, and
<em>synchronization</em> with some remote party.</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td><pre>version 1 .
embeddedType WireRef .

Assertion = any .
Handle    = int .
Event     = Assert / Retract / Message / Sync .
Oid       = int .
Turn      = [TurnEvent ...].
TurnEvent = [@oid Oid @event Event].

Assert = &lt;assert @assertion Assertion @handle Handle&gt;.
Retract = &lt;retract @handle Handle&gt;.
Message = &lt;message @body Assertion&gt;.
Sync = &lt;sync @peer ref&gt;.

# Definition of Caveat omitted here

WireRef = @mine &lt;MyRef @oid Oid]&gt;
        / @yours &lt;YourRef @oid Oid @attenuation Caveat ...&gt;.
</pre></td></tr></tbody></table></code></pre></div>

<blockquote>
  <h2 id="who-i-am">Who I am</h2>
</blockquote>

<p>I’m <a href="https://leastfixedpoint.com/">Tony Garnock-Jones</a>;
<a href="mailto:tonyg@leastfixedpoint.com">tonyg@leastfixedpoint.com</a>;
<a href="https://twitter.com/leastfixedpoint">@leastfixedpoint</a>; <code>tonyg</code> on
Libera.Chat and HN. I’m a computer science researcher and software
developer. I’ve used Linux since the mid-1990s, contributed to
open-source software since the late 1990s, and I have a
<a href="https://syndicate-lang.org/tonyg-dissertation/">PhD in, well, Syndicate</a>!</p>

<p>See also <a href="https://leastfixedpoint.com/">my personal website</a>.</p>

<hr>



    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Examples of Great URL Design (2023) (371 pts)]]></title>
            <link>https://blog.jim-nielsen.com/2023/examples-of-great-urls/</link>
            <guid>41243992</guid>
            <pubDate>Wed, 14 Aug 2024 08:42:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jim-nielsen.com/2023/examples-of-great-urls/">https://blog.jim-nielsen.com/2023/examples-of-great-urls/</a>, See on <a href="https://news.ycombinator.com/item?id=41243992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <p>Here’s Kyle Aster on why <a href="https://warpspire.com/posts/url-design/">thoughtful URL design is important (in 2010)</a>:</p>
<blockquote>
<p>URLs are universal. They work in Firefox, Chrome, Safari, Internet Explorer, cURL, wget, your iPhone, Android and even written down on sticky notes. They are the one universal syntax of the web. Don’t take that for granted.</p>
</blockquote>
<p>I love this reminder of the ubiquity of URLs. They’re not just for typing into browser bars. They’re used in a plethora of ways:</p>
<ul>
<li>As targets for scripting and scraping and other programmatic data retrieval.</li>
<li>As references, printed in the footnotes and appendixes of physical books.</li>
<li>As actionable triggers accessible via physical mediums, e.g. scannable QR codes or IoT device buttons.</li>
<li>And more!</li>
</ul>
<p>When I reflect on examples of great URL design<sup id="fnref:1"><a href="#fn:1">[1]</a></sup> I’ve encountered through the years — URLs that, when I saw them, I paused and thought “Wow, that’s really neat!” — these are a few that come to mind.</p>
<h2 id="stackoverflow">StackOverflow</h2>
<p>StackOverflow was the first place I remember encountering URLs that struck a nice balance between the needs of computers and humans.</p>
<p>The URLs follow a pattern like this:</p>
<p><code>/questions/:id/:slug</code></p>
<p><code>:id</code> is a unique identifier for the question that reveals nothing about the content. <code>:slug</code>, on the other hand, is a human-readable paraphrasing of the question that allows you to understand the question without ever actually going to the website.</p>
<p>The beauty is <code>:slug</code> is an optional parameter in the URL. For example:</p>
<p><a href="https://stackoverflow.com/questions/16245767">stackoverflow.com/questions/16245767</a></p>
<p>Tells you nothing about the question being asked but it’s a valid URL that allows a server to easily find and serve that unique piece of content.</p>
<p>But StackOverflow also supports the <code>:slug</code> part of the URL which allows humans to quickly understand the contents living at that URL.</p>
<p><a href="https://stackoverflow.com/questions/16245767/creating-a-blob-from-a-base64-string-in-javascript/">stackoverflow.com/questions/16245767/creating-a-blob-from-a-base64-string-in-javascript/</a></p>
<p>As noted, the <code>:slug</code> is optional. It is not required by the server to find and serve the contents in question. In fact, it can easily be changed over time without without breaking the URL (which I find quite elegant).</p>
<p>Granted, it can also be used deceptively. For example, this is the same URL as above but it portends completely different contents (without breaking the link):</p>
<p><a href="https://stackoverflow.com/questions/16245767/how-to-bake-a-cake">stackoverflow.com/questions/16245767/how-to-bake-a-cake</a></p>
<p>But hey, trade-offs in everything.</p>
<h2 id="slack">Slack</h2>
<p>I remember when Slack launched a marketing campaign to educate people about the product. They used the language of the marketing campaign – <a href="http://web.archive.org/web/20140212215308/slack.com/is">“Slack is...”</a> — in the page copy as well as in the URLs, e.g.</p>
<ul>
<li><code>slack.com/is</code></li>
<li><code>slack.com/is/team-communication</code></li>
<li><code>slack.com/is/everything-in-one-place</code></li>
<li><code>slack.com/is/wherever-you-are</code></li>
</ul>
<p>I remember being so intrigued at this effort to bring the design of the story-telling campaign all the way up into the URLs themselves.</p>
<p>Since then, I’ve always found delight in URLs that try to form natural languages sentences — <code>slack.com/is/team-communication</code> — rather than concatenate a series of hierarchical keywords — <code>slack.com/product/team-communication</code>.</p>
<p>Speaking of doing fun things with sentence structure in your URLs...</p>
<h2 id="jessica-hische">Jessica Hische</h2>
<p>Jessica Hische has her website under a <code>.is</code> domain (which is <a href="https://en.wikipedia.org/wiki/.is">for Iceland</a>, apparently).</p>
<p><a href="https://www.jessicahische.is/">jessicahische.is</a></p>
<p>She riffs on this fun third-person form of “I am” across her site. For example, click on “About” in the primary navigation and it takes you to:</p>
<p><a href="https://www.jessicahische.is/anoversharer">jessicahische.is/anoversharer</a></p>
<p>That’s fun! <code>mydomain.com/about</code> is clear too, but I love the whimsy of describing the “about” and doing it in sentence structure.</p>
<p>All of the nouns in her primary navigation follow this pattern, as well as her individual pieces of work. Like this writeup about one of her holiday culinary packaging gigs has the URL:</p>
<p><a href="https://www.jessicahische.is/sofulloffancypopcorn">jessicahische.is/sofulloffancypopcorn</a></p>
<p>Fun!</p>
<h2 id="urls-as-product">URLs as Product</h2>
<p>I’ve always loved services whose URLs map nicely to their domain semantics. For example, <a href="https://www.quora.com/Which-sites-have-the-best-URL-design/answer/Simon-Willison">GitHub’s URLs map really well to git semantics</a> like the three dot diff comparison in git:</p>
<p><code>/:owner/:project/compare/ref1...ref2</code></p>
<p>e.g.</p>
<p><a href="https://github.com/django/django/compare/4.2.7...main">github.com/django/django/compare/4.2.7...main</a></p>
<p>For technical products, this ability to navigate a website without necessarily seeing the user interface is a cool superpower.</p>
<p>NPM is somewhat similar. Want to see <code>react-router</code> on NPM? You don’t have to go to NPM’s home page and click around or use their search box. Once you become familiar with their site structure, you know you can lookup a package using:</p>
<p><code>/package/:package-name</code></p>
<p>e.g.</p>
<p><a href="https://www.npmjs.com/package/react-router">npmjs.com/package/react-router</a></p>
<p>Want to lookup a specific version of a package?</p>
<p><code>/package/:package-name/v/:semver</code></p>
<p>e.g.</p>
<p><a href="https://www.npmjs.com/package/react-router/v/5.3.4">npmjs.com/package/react-router/v/5.3.4</a></p>
<p>These kinds of shortcuts are super useful when you’re using a particular product. In the case of NPM, you’re hunting through your <code>package.json</code> and need to lookup some details of a specific package pinned at a specific version, you can navigate to NPM’s details of that package by merely identifying the version you want and typing the details into a URL bar.</p>
<p>NPM CDNs <a href="https://unpkg.com/">like unpkg</a> do a good job at following these semantics as well. Want a file from a published package? The homepage of unpkg says:</p>
<p><code>unpkg.com/:package@:version/:file</code></p>
<p>In cases like this, <em>the URL can be the product itself</em> which makes its design all the more vital<sup id="fnref:2"><a href="#fn:2">[2]</a></sup>.</p>
<h2 id="whats-yours">What’s Yours?</h2>
<p>These are a few examples of URLs I’ve enjoyed using or seeing over the years. I’m sure there are others, but I’d be curious to know what your favorites are? Blog ’em!</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vaultwarden: Unofficial Bitwarden compatible server written in Rust (138 pts)]]></title>
            <link>https://github.com/dani-garcia/vaultwarden</link>
            <guid>41243147</guid>
            <pubDate>Wed, 14 Aug 2024 06:21:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dani-garcia/vaultwarden">https://github.com/dani-garcia/vaultwarden</a>, See on <a href="https://news.ycombinator.com/item?id=41243147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h3 tabindex="-1" dir="auto">Alternative implementation of the Bitwarden server API written in Rust and compatible with <a href="https://bitwarden.com/download/" rel="nofollow">upstream Bitwarden clients</a>*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.</h3><a id="user-content-alternative-implementation-of-the-bitwarden-server-api-written-in-rust-and-compatible-with-upstream-bitwarden-clients-perfect-for-self-hosted-deployment-where-running-the-official-resource-heavy-service-might-not-be-ideal" aria-label="Permalink: Alternative implementation of the Bitwarden server API written in Rust and compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal." href="#alternative-implementation-of-the-bitwarden-server-api-written-in-rust-and-compatible-with-upstream-bitwarden-clients-perfect-for-self-hosted-deployment-where-running-the-official-resource-heavy-service-might-not-be-ideal"></a></p>
<p dir="auto">📢 Note: This project was known as Bitwarden_RS and has been renamed to separate itself from the official Bitwarden server in the hopes of avoiding confusion and trademark/branding issues. Please see <a href="https://github.com/dani-garcia/vaultwarden/discussions/1642" data-hovercard-type="discussion" data-hovercard-url="/dani-garcia/vaultwarden/discussions/1642/hovercard">#1642</a> for more explanation.</p>
<hr>
<p dir="auto"><a href="https://github.com/dani-garcia/vaultwarden/actions/workflows/build.yml"><img src="https://github.com/dani-garcia/vaultwarden/actions/workflows/build.yml/badge.svg" alt="Build"></a>
<a href="https://github.com/dani-garcia/vaultwarden/pkgs/container/vaultwarden"><img src="https://camo.githubusercontent.com/0c93c9162d3198c87ad11b4e7f7aa3e37e30f4e83fb0314d0f20d78a322ccc72/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f676863722e696f2d646f776e6c6f61642d626c7565" alt="ghcr.io" data-canonical-src="https://img.shields.io/badge/ghcr.io-download-blue"></a>
<a href="https://hub.docker.com/r/vaultwarden/server" rel="nofollow"><img src="https://camo.githubusercontent.com/b7eed1a535e57e0b270c492b6a27acca9c8f5fe866b9079b350d5109e3478a2d/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f7661756c7477617264656e2f7365727665722e737667" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/vaultwarden/server.svg"></a>
<a href="https://quay.io/repository/vaultwarden/server" rel="nofollow"><img src="https://camo.githubusercontent.com/92c12d0c62773e87c44f0210f81e80dcdbd5136374854447618dd0a50d0c886e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f517561792e696f2d646f776e6c6f61642d626c7565" alt="Quay.io" data-canonical-src="https://img.shields.io/badge/Quay.io-download-blue"></a>
<a href="https://deps.rs/repo/github/dani-garcia/vaultwarden" rel="nofollow"><img src="https://camo.githubusercontent.com/ff2993e84d4877c6bbac443511e1011f40e7db902095856d05e5d39bab8a73c4/68747470733a2f2f646570732e72732f7265706f2f6769746875622f64616e692d6761726369612f7661756c7477617264656e2f7374617475732e737667" alt="Dependency Status" data-canonical-src="https://deps.rs/repo/github/dani-garcia/vaultwarden/status.svg"></a>
<a href="https://github.com/dani-garcia/vaultwarden/releases/latest"><img src="https://camo.githubusercontent.com/2de66609b464974ba03406557f10613388f8e3a43854af3ad160a62d56bb38ae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f64616e692d6761726369612f7661756c7477617264656e2e737667" alt="GitHub Release" data-canonical-src="https://img.shields.io/github/release/dani-garcia/vaultwarden.svg"></a>
<a href="https://github.com/dani-garcia/vaultwarden/blob/main/LICENSE.txt"><img src="https://camo.githubusercontent.com/8dad38b8d5ea992ce24b40dd0b8ef428d61433eeb0124748593a9b01048e0835/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f64616e692d6761726369612f7661756c7477617264656e2e737667" alt="AGPL-3.0 Licensed" data-canonical-src="https://img.shields.io/github/license/dani-garcia/vaultwarden.svg"></a>
<a href="https://matrix.to/#/#vaultwarden:matrix.org" rel="nofollow"><img src="https://camo.githubusercontent.com/94e23b221c6e43c265550a51b5dea881100844afa1421c3d6a8db9a1973ca3a5/68747470733a2f2f696d672e736869656c64732e696f2f6d61747269782f7661756c7477617264656e3a6d61747269782e6f72672e7376673f6c6f676f3d6d6174726978" alt="Matrix Chat" data-canonical-src="https://img.shields.io/matrix/vaultwarden:matrix.org.svg?logo=matrix"></a></p>
<p dir="auto">Image is based on <a href="https://github.com/dani-garcia/vaultwarden">Rust implementation of Bitwarden API</a>.</p>
<p dir="auto"><strong>This project is not associated with the <a href="https://bitwarden.com/" rel="nofollow">Bitwarden</a> project nor Bitwarden, Inc.</strong></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><g-emoji alias="warning">⚠️</g-emoji><strong>IMPORTANT</strong><g-emoji alias="warning">⚠️</g-emoji>: When using this server, please report any bugs or suggestions to us directly (look at the bottom of this page for ways to get in touch), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official support channels.</h4><a id="user-content-️important️-when-using-this-server-please-report-any-bugs-or-suggestions-to-us-directly-look-at-the-bottom-of-this-page-for-ways-to-get-in-touch-regardless-of-whatever-clients-you-are-using-mobile-desktop-browser-do-not-use-the-official-support-channels" aria-label="Permalink: ⚠️IMPORTANT⚠️: When using this server, please report any bugs or suggestions to us directly (look at the bottom of this page for ways to get in touch), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official support channels." href="#️important️-when-using-this-server-please-report-any-bugs-or-suggestions-to-us-directly-look-at-the-bottom-of-this-page-for-ways-to-get-in-touch-regardless-of-whatever-clients-you-are-using-mobile-desktop-browser-do-not-use-the-official-support-channels"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">Basically full implementation of Bitwarden API is provided including:</p>
<ul dir="auto">
<li>Organizations support</li>
<li>Attachments and Send</li>
<li>Vault API support</li>
<li>Serving the static files for Vault interface</li>
<li>Website icons API</li>
<li>Authenticator and U2F support</li>
<li>YubiKey and Duo support</li>
<li>Emergency Access</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Pull the docker image and mount a volume from the host for persistent storage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull vaultwarden/server:latest
docker run -d --name vaultwarden -v /vw-data/:/data/ --restart unless-stopped -p 80:80 vaultwarden/server:latest"><pre>docker pull vaultwarden/server:latest
docker run -d --name vaultwarden -v /vw-data/:/data/ --restart unless-stopped -p 80:80 vaultwarden/server:latest</pre></div>
<p dir="auto">This will preserve any persistent data under /vw-data/, you can adapt the path to whatever suits you.</p>
<p dir="auto"><strong>IMPORTANT</strong>: Most modern web browsers disallow the use of Web Crypto APIs in insecure contexts. In this case, you might get an error like <code>Cannot read property 'importKey'</code>. To solve this problem, you need to access the web vault via HTTPS or localhost.</p>
<p dir="auto">This can be configured in <a href="https://github.com/dani-garcia/vaultwarden/wiki/Enabling-HTTPS">vaultwarden directly</a> or using a third-party reverse proxy (<a href="https://github.com/dani-garcia/vaultwarden/wiki/Proxy-examples">some examples</a>).</p>
<p dir="auto">If you have an available domain name, you can get HTTPS certificates with <a href="https://letsencrypt.org/" rel="nofollow">Let's Encrypt</a>, or you can generate self-signed certificates with utilities like <a href="https://github.com/FiloSottile/mkcert">mkcert</a>. Some proxies automatically do this step, like Caddy (see examples linked above).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">See the <a href="https://github.com/dani-garcia/vaultwarden/wiki">vaultwarden wiki</a> for more information on how to configure and run the vaultwarden server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get in touch</h2><a id="user-content-get-in-touch" aria-label="Permalink: Get in touch" href="#get-in-touch"></a></p>
<p dir="auto">To ask a question, offer suggestions or new features or to get help configuring or installing the software, please use <a href="https://github.com/dani-garcia/vaultwarden/discussions">GitHub Discussions</a> or <a href="https://vaultwarden.discourse.group/" rel="nofollow">the forum</a>.</p>
<p dir="auto">If you spot any bugs or crashes with vaultwarden itself, please <a href="https://github.com/dani-garcia/vaultwarden/issues/">create an issue</a>. Make sure you are on the latest version and there aren't any similar issues open, though!</p>
<p dir="auto">If you prefer to chat, we're usually hanging around at <a href="https://matrix.to/#/#vaultwarden:matrix.org" rel="nofollow">#vaultwarden:matrix.org</a> room on Matrix. Feel free to join us!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sponsors</h3><a id="user-content-sponsors" aria-label="Permalink: Sponsors" href="#sponsors"></a></p>
<p dir="auto">Thanks for your contribution to the project!</p>

<markdown-accessiblity-table></markdown-accessiblity-table>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grok-2 Beta Release (193 pts)]]></title>
            <link>https://x.ai/blog/grok-2</link>
            <guid>41242979</guid>
            <pubDate>Wed, 14 Aug 2024 05:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x.ai/blog/grok-2">https://x.ai/blog/grok-2</a>, See on <a href="https://news.ycombinator.com/item?id=41242979">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="The Grok word art arranged in two Greek columns that together look like the number 2." loading="lazy" width="1000" height="1000" decoding="async" data-nimg="1" srcset="https://x.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fgrok2cover3.1872edc7.png&amp;w=1080&amp;q=75 1x, https://x.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fgrok2cover3.1872edc7.png&amp;w=2048&amp;q=75 2x" src="https://x.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fgrok2cover3.1872edc7.png&amp;w=2048&amp;q=75"></p><div><p>August 13, 2024</p><p>Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.</p></div></div><div><main><div><p><strong>We are excited to release an early preview of Grok-2, a significant step forward from our previous model Grok-1.5, featuring frontier capabilities in chat, coding, and reasoning. At the same time, we are introducing Grok-2 mini, a small but capable sibling of Grok-2. An early version of Grok-2 has been tested on the LMSYS leaderboard under the name "sus-column-r." At the time of this blog post, it is outperforming both Claude 3.5 Sonnet and GPT-4-Turbo.</strong></p><p>Grok-2 and Grok-2 mini are currently in beta on 𝕏, and we are also making both models available through our enterprise API later this month.</p><h3 id="grok-2-language-model-and-chat-capabilities">Grok-2 language model and chat capabilities</h3><p>We introduced an early version of Grok-2 under the name "sus-column-r" into the LMSYS chatbot arena, a popular competitive language model benchmark. It outperforms both Claude and GPT-4 on the LMSYS leaderboard in terms of its overall Elo score.</p></div>

<div><p>Internally, we employ a comparable process to evaluate our models. Our AI Tutors engage with our models across a variety of tasks that reflect real-world interactions with Grok. During each interaction, the AI Tutors are presented with two responses generated by Grok. They select the superior response based on specific criteria outlined in our guidelines. We focused on evaluating model capabilities in two key areas: following instructions and providing accurate, factual information. Grok-2 has shown significant improvements in reasoning with retrieved content and in its tool use capabilities, such as correctly identifying missing information, reasoning through sequences of events, and discarding irrelevant posts.</p><h2 id="benchmarks">Benchmarks</h2><p>We evaluated the Grok-2 models across a series of academic benchmarks that included reasoning, reading comprehension, math, science, and coding. Both Grok-2 and Grok-2 mini demonstrate significant improvements over our previous Grok-1.5 model. They achieve performance levels competitive to other frontier models in areas such as graduate-level science knowledge (GPQA), general knowledge (MMLU, MMLU-Pro), and math competition problems (MATH). Additionally, Grok-2 excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and in document-based question answering (DocVQA).</p></div>
<div><table><thead><tr><th>Benchmark</th><th></th><th>Grok-1.5</th><th>Grok-2 mini<sup>‡</sup></th><th>Grok-2<sup>‡</sup></th><th>GPT-4 Turbo<sup>*</sup></th><th>Claude 3 Opus<sup>†</sup></th><th>Gemini Pro 1.5</th><th>Llama 3 405B</th><th>GPT-4o<sup>*</sup></th><th>Claude 3.5 Sonnet<sup>†</sup></th></tr></thead><tbody><tr><th scope="row">GPQA</th><td></td><td>35.9<span>%</span></td><td>51.0<span>%</span></td><td>56.0<span>%</span></td><td>48.0<span>%</span></td><td>50.4<span>%</span></td><td>46.2<span>%</span></td><td>51.1<span>%</span></td><td>53.6<span>%</span></td><td>59.6<span>%</span></td></tr><tr><th scope="row">MMLU</th><td></td><td>81.3<span>%</span></td><td>86.2<span>%</span></td><td>87.5<span>%</span></td><td>86.5<span>%</span></td><td>85.7<span>%</span></td><td>85.9<span>%</span></td><td>88.6<span>%</span></td><td>88.7<span>%</span></td><td>88.3<span>%</span></td></tr><tr><th scope="row">MMLU-Pro</th><td></td><td>51.0<span>%</span></td><td>72.0<span>%</span></td><td>75.5<span>%</span></td><td>63.7<span>%</span></td><td>68.5<span>%</span></td><td>69.0<span>%</span></td><td>73.3<span>%</span></td><td>72.6<span>%</span></td><td>76.1<span>%</span></td></tr><tr><th scope="row">MATH<sup>§</sup></th><td></td><td>50.6<span>%</span></td><td>73.0<span>%</span></td><td>76.1<span>%</span></td><td>72.6<span>%</span></td><td>60.1<span>%</span></td><td>67.7<span>%</span></td><td>73.8<span>%</span></td><td>76.6<span>%</span></td><td>71.1<span>%</span></td></tr><tr><th scope="row">HumanEval<sup>¶</sup></th><td></td><td>74.1<span>%</span></td><td>85.7<span>%</span></td><td>88.4<span>%</span></td><td>87.1<span>%</span></td><td>84.9<span>%</span></td><td>71.9<span>%</span></td><td>89.0<span>%</span></td><td>90.2<span>%</span></td><td>92.0<span>%</span></td></tr><tr><th scope="row">MMMU</th><td></td><td>53.6<span>%</span></td><td>63.2<span>%</span></td><td>66.1<span>%</span></td><td>63.1<span>%</span></td><td>59.4<span>%</span></td><td>62.2<span>%</span></td><td>64.5<span>%</span></td><td>69.1<span>%</span></td><td>68.3<span>%</span></td></tr><tr><th scope="row">MathVista</th><td></td><td>52.8<span>%</span></td><td>68.1<span>%</span></td><td>69.0<span>%</span></td><td>58.1<span>%</span></td><td>50.5<span>%</span></td><td>63.9<span>%</span></td><td>—</td><td>63.8<span>%</span></td><td>67.7<span>%</span></td></tr><tr><th scope="row">DocVQA</th><td></td><td>85.6<span>%</span></td><td>93.2<span>%</span></td><td>93.6<span>%</span></td><td>87.2<span>%</span></td><td>89.3<span>%</span></td><td>93.1<span>%</span></td><td>92.2<span>%</span></td><td>92.8<span>%</span></td><td>95.2<span>%</span></td></tr></tbody></table></div>
<div><p><small><sup>*</sup> GPT-4-Turbo and GPT-4o scores are from the May 2024 release.<br>
<sup>†</sup> Claude 3 Opus and Claude 3.5 Sonnet scores are from the June 2024 release.<br>
<sup>‡</sup> Grok-2 MMLU, MMLU-Pro, MMMU and MathVista were evaluated using 0-shot CoT.<br>
<sup>§</sup> For MATH, we present maj@1 results.<br>
<sup>¶</sup> For HumanEval, we report pass@1 benchmark scores.</small></p><h2 id="experience-grok-with-real-time-information-on-𝕏">Experience Grok with real-time information on 𝕏</h2><p>Over the past few months, we've been continuously improving Grok on the 𝕏 platform. Today, we're introducing the next evolution of the Grok experience, featuring a redesigned interface and new features.</p></div>

<div><p><img alt="Black Forest Labs logo." loading="lazy" width="100" height="92" decoding="async" data-nimg="1" srcset="https://x.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fflux-logo.dd12805d.png&amp;w=128&amp;q=75 1x, https://x.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fflux-logo.dd12805d.png&amp;w=256&amp;q=75 2x" src="https://x.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fflux-logo.dd12805d.png&amp;w=256&amp;q=75"></p><p>𝕏 Premium and Premium+ users will have access to two new models: Grok-2 and Grok-2 mini. Grok-2 is our state-of-the-art AI assistant with advanced capabilities in both text and vision understanding, integrating real-time information from the 𝕏 platform, accessible through the Grok tab in the 𝕏 app. Grok-2 mini is our small but capable model that offers a balance between speed and answer quality. Compared to its predecessor, Grok-2 is more intuitive, steerable, and versatile across a wide range of tasks, whether you're seeking answers, collaborating on writing, or solving coding tasks. In collaboration with <a href="https://blackforestlabs.ai/">Black Forest Labs</a>, we are experimenting with their <a href="https://blackforestlabs.ai/#get-flux">FLUX.1</a> model to expand Grok’s capabilities on 𝕏. If you are a Premium or Premium+ subscriber, make sure to update to the latest version of the 𝕏 app in order to beta test Grok-2.</p><h3 id="build-with-grok-using-the-enterprise-api">Build with Grok using the Enterprise API</h3><p>We are also releasing Grok-2 and Grok-2 mini to developers through our new enterprise API platform later this month. Our upcoming API is built on a new bespoke tech stack that allows multi-region inference deployments for low-latency access across the world. We offer enhanced security features such as mandatory multi-factor authentication (e.g. using a Yubikey, Apple TouchID, or TOTP), rich traffic statistics, and advanced billing analytics (incl. detailed data exports). We further offer a management API that allows you to integrate team, user, and billing management into your existing in-house tools and services. <a href="https://x.ai/enterprise-api">Join our newsletter</a> to get notified when we launch later this month.</p><h3 id="what-is-next">What is next?</h3><p>Grok-2 and Grok-2 mini are being rolled out on 𝕏. We are very excited about their applications to a range of AI-driven features, such as enhanced search capabilities, gaining deeper insights on 𝕏 posts, and improved reply functions, all powered by Grok. Soon, we will release a preview of multimodal understanding as a core part of the Grok experience on 𝕏 and API.</p><p>Since announcing Grok-1 in November 2023, xAI has been moving at an extraordinary pace, driven by a small team with the highest talent density. We have introduced Grok-2, positioning us at the forefront of AI development. Our focus is on advancing core reasoning capabilities with our new compute cluster. We will have many more developments to share in the coming months. We are looking for individuals to join our small, focused team dedicated to building the most impactful innovations for the future of humanity. <a href="https://x.ai/careers">Apply to our positions here</a>.</p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disney Seeks Dismissal Wrongful Death Lawsuit b/c Victim Is Disney+ Subscriber (563 pts)]]></title>
            <link>https://wdwnt.com/2024/08/disney-dismissal-wrongful-death-lawsuit/</link>
            <guid>41242400</guid>
            <pubDate>Wed, 14 Aug 2024 03:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wdwnt.com/2024/08/disney-dismissal-wrongful-death-lawsuit/">https://wdwnt.com/2024/08/disney-dismissal-wrongful-death-lawsuit/</a>, See on <a href="https://news.ycombinator.com/item?id=41242400">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>Disney has asked a Florida court to dismiss a wrongful death lawsuit filed earlier this year regarding a woman who <a href="https://wdwnt.com/2024/02/doctor-dies-allergic-reaction-raglan-road-disney/" target="_blank" rel="noreferrer noopener">passed away due to anaphylaxis after a meal at Disney Springs</a>, citing an arbitration waiver in the terms and conditions for Disney+.</p><h2>Disney Springs Wrongful Death Lawsuit Update</h2><div><figure><a data-fslightbox="1" data-type="image" aria-label="Open fullscreen lightbox with current image" href="https://media.wdwnt.com/2020/06/raglan-road-reopening-8-1200x900.jpg"><img decoding="async" src="https://media.wdwnt.com/2020/06/raglan-road-reopening-8-1200x900.jpg" alt="Disney seeks to dismiss a wrongful death lawsuit due to a Disney+ arbitration clause." title="Disney Seeking Dismissal of Raglan Road Death Lawsuit Because Victim Was Disney+ Subscriber 1" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABLAAAAOEAQAAAACZFvVXAAAAAnRSTlMAAHaTzTgAAACbSURBVHja7cEBDQAAAMKg909tDjegAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADg2wAS+gAB9ir9UQAAAABJRU5ErkJggg==" data-src="https://media.wdwnt.com/2020/06/raglan-road-reopening-8-1200x900.jpg"></a></figure></div><p>According to <a href="https://www.newsday.com/long-island/nassau/disney-restaurant-carle-place-physician-allergy-gcgv997y?utm_source=tw_nd" target="_blank" rel="noreferrer noopener">Newsday</a>, Disney has asked a Florida court to dismiss the wrongful death lawsuit filed by Jeffrey Piccolo, husband of Kanokporn Tangsuan, a doctor at NYU Langone in New York City, who passed away after eating a meal at Raglan Road Irish Pub in Disney Springs in October 2023.</p><p>Tangsuan had a severe dairy and nut allergy and informed the waitstaff at the restaurant of her dietary needs, and was “unequivocally assured” they could be accommodated. She ordered and ate the “Sure I’m Frittered” vegetarian broccoli and corn fritters, the “Scallop Forest” sea scallops appetizer, the “This Shepherd Went Vegan” entree, and a side of onion rings.</p><p>After their meal, Piccolo returned to their hotel room, and Tangsuan and her mother-in-law continued to shop at Disney Springs. later that evening, Tangsuan had an acute allergic reaction in Planet Hollywood, self-administered an EpiPen, and was transported to a local hospital, where she later died.</p><div><figure><a data-fslightbox="1" data-type="image" aria-label="Open fullscreen lightbox with current image" href="https://media.wdwnt.com/2024/03/disney-plus-logo-march-2024-1200x675.jpeg"><img decoding="async" width="1200" height="675" src="https://media.wdwnt.com/2024/03/disney-plus-logo-march-2024-1200x675.jpeg" alt="New Hulu on Disney+ logo" title="Disney Seeking Dismissal of Raglan Road Death Lawsuit Because Victim Was Disney+ Subscriber 2" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABLAAAAKjAQAAAAAlyMttAAAAAnRSTlMAAHaTzTgAAAB5SURBVHja7cExAQAAAMKg9U/tbQegAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAM440AAFaumxcAAAAAElFTkSuQmCC" data-src="https://media.wdwnt.com/2024/03/disney-plus-logo-march-2024-1200x675.jpeg"></a></figure></div><p>In the latest update for the Disney Springs wrongful death lawsuit, Disney cited legal language within the terms and conditions for Disney+, which “requires users to arbitrate all disputes with the company.” Disney claims Piccolo reportedly agreed to this in 2019 when signing up for a one-month free trial of the streaming service on his PlayStation console.</p><p>In the May 31 motion filed to move the wrongful death lawsuit to arbitration, Disney attorneys said that the Disney+ subscriber agreement states that any dispute, except for small claims, “must be resolved by individual binding arbitration.” Disney says that similar language was agreed to by Piccolo when he used the My Disney Experience app to purchase tickets to visit EPCOT at Walt Disney World in September 2023. Tangsuan died before she and Piccolo could use the tickets.</p><p>“Whether Piccolo actually reviewed the Disney terms is also immaterial,” the motion says.</p><p>Arbitration is defined by the Legal Information Institute as “an alternative dispute resolution method where the parties in dispute agree to have their case heard by a qualified arbitrator out of court.”</p><p>Attorneys for Piccolo called Disney’s latest motion “preposterous,” and that it’s “‘absurd’ to believe that the 153 million subscribers to the popular streaming service have waived all claims against the company and its affiliates because of language ‘buried’ within the terms and conditions,” according to Newsday.</p><blockquote><p>The notion that terns agreed to by a consumer when creating a Disney+ free trial account would forever bar that consumer’s right to a jury trial in any dispute with any Disney affiliate or subsidiary, is so outrageously unreasonable and unfair as to shock the judicial conscience, and this court should not enforce such an agreement.</p><cite>Brian Denny, Piccolo’s West Palm Beach attorney in a filing on August 2, 2024</cite></blockquote><p>In that same August 2 filing, attorneys for Piccolo cite that Piccolo is serving as a plaintiff in this case on behalf of his wife and her estate, not as an individual, and that his wife’s estate did not exist at the time of him signing up for a free trial of Disney+. Piccolo’s attorneys claim that Disney is “attempting to enforce an agreement that it never signed against a party who also never signed.”</p><p>“This argument borders on the surreal,” said Piccolo’s attorneys in the filing.</p><p>Walt Disney Parks and Resorts and their attorneys declined Newsday’s request for comment.</p><p>For the latest Disney Parks news and info, follow <a href="https://wdwnt.com/">WDW News Today</a> on&nbsp;<a href="https://twitter.com/wdwnt" target="_blank" rel="noopener">Twitter</a>,&nbsp;<a href="https://www.facebook.com/WDWNewsToday/" target="_blank" rel="noopener">Facebook</a>, and&nbsp;<a href="https://www.instagram.com/wdwnt/" target="_blank" rel="noopener">Instagram</a>.</p><div><h3>Related posts:</h3></div>

 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sort, sweep, and prune: Collision detection algorithms (171 pts)]]></title>
            <link>https://leanrada.com/notes/sweep-and-prune/</link>
            <guid>41241942</guid>
            <pubDate>Wed, 14 Aug 2024 02:19:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leanrada.com/notes/sweep-and-prune/">https://leanrada.com/notes/sweep-and-prune/</a>, See on <a href="https://news.ycombinator.com/item?id=41241942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            
            <p><span>5 Aug 2023</span> · 13 min read
            </p>
            
            <p>tags:
              <span title="algorithm">algo</span>
              <span>games</span>
            </p>

            <p>Sweep-and-prune is my go-to algorithm when I want to quickly implement collision detection for a game. I think it’s an awesome and elegant algorithm, so I wrote a post about it.</p>
            <p>This post is lengthy with many examples and explanations, thus split into two parts. You can jump to specific bits using this special springboard:</p>
            <ul>
              <li><strong>Part 1:</strong> <a target="_self" href="https://leanrada.com/notes/sweep-and-prune/">Simplified version</a>
                <ul>
                  <li>🔍 <a target="_self" href="https://leanrada.com/notes/sweep-and-prune/#comparisons">Visual comparison</a></li>
                  <li>📝 <a target="_self" href="https://leanrada.com/notes/sweep-and-prune/#code1">Code</a></li>
                </ul>
              </li>
              <li><strong>Part 2:</strong> <a target="_self" href="https://leanrada.com/notes/sweep-and-prune-2/">Sophisticated versions</a>
                <ul>
                  <li>🔍 <a target="_self" href="https://leanrada.com/notes/sweep-and-prune-2/#comparisons">Visual comparison</a></li>
                  <li>📝 <a target="_self" href="https://leanrada.com/notes/sweep-and-prune-2/#final-code">Final code</a></li>
                </ul>
              </li>
            </ul>
            <p>As for the rest of the post, I try to paint a picture of what I think are first principles and show it with <strong>interactive demos</strong>! Let’s go!</p>
            <hr>
            
            <p>As you may know, the problem of collision detection is pretty common in video game programming. It’s a prerequisite to the implementation of certain game mechanics or simulations.</p>
            <p><img loading="lazy" width="100%" data-placeholder="" alt="video of mario with goombas bumping into each other" src="https://leanrada.com/notes/sweep-and-prune/mario.gif" caption="Goombas colliding"><span>Goombas colliding</span>
            </p>
            
            <p>Some of these mechanics include: preventing characters from passing through each other, <a target="_blank" href="https://youtu.be/Ky69PjyHCqg">goombas</a> turning around when bumping into another, big cells eating smaller cells in <a target="_blank" href="https://agar.io/">agar.io</a>, or just about any game physics. All of these need some kind of collision detection.</p>
            <p><img loading="lazy" width="100%" data-placeholder="" alt="video of agar.io with cells eating smaller cells" src="https://leanrada.com/notes/sweep-and-prune/agario.gif" caption="Cells consuming smaller cells on contact"><span>Cells consuming smaller cells on contact</span>
            </p>
            
            <p>Here I’ll cover several related approaches, starting with the simplest and building up to the <a target="_blank" href="https://en.wikipedia.org/wiki/Sweep_and_prune"><strong>sweep-and-prune</strong></a> algorithm. I won’t cover other approaches, such as space partitioning or spatial tree subdivision.</p>
            <p>Balls.</p>
            <p>I’ll use this <strong>rigid-body ball simulation</strong> as a recurring example to demonstrate the algorithms throughout the post:</p>
            

            <p>Alright, let’s dive in! How do we detect these collisions?</p>
            <h2 id="naive-approach-🐥">Naive approach 🐥</h2>
            <p>The straightforward solution is to test every potential pair of objects for collision. That is, <em>check every ball against every other ball</em>.</p>
            <pre><code><span>// for each ball</span>
<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> balls<span>.</span>length<span>;</span> i<span>++</span><span>)</span> <span>{</span>
  <span>const</span> ball1 <span>=</span> balls<span>[</span>i<span>]</span><span>;</span>
  <span>// check each of the other balls</span>
  <span>for</span> <span>(</span><span>let</span> j <span>=</span> i <span>+</span> <span>1</span><span>;</span> j <span>&lt;</span> balls<span>.</span>length<span>;</span> j<span>++</span><span>)</span> <span>{</span>
    <span>const</span> ball2 <span>=</span> balls<span>[</span>j<span>]</span><span>;</span>
    <span>// check for collision</span>
    <span>if</span> <span>(</span><span>intersects</span><span>(</span>ball1<span>,</span> ball2<span>)</span><span>)</span> <span>{</span>
      <span>bounce</span><span>(</span>ball1<span>,</span> ball2<span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span></code></pre>
            <p>Note in the above code that the inner loop starts at <code>i + 1</code> to prevent duplicate pairs from being counted (A-B vs B-A). Other than that, it’s a pretty simple solution.</p>
            <p>These checks are done on every time step, ensuring that balls will bounce exactly when they collide.</p>
            <p>Here’s a slowed-down, highlighted simulation, showing pairs being tested for intersection per time step:</p>
            <div>
              <sap-demo-client data-rss="interactive" alt="demo of the collision detection algorithm using 'pairwise' strategy" strategy="pairwise" skip-interval="4" decorations="checks:#4c8"></sap-demo-client>
              <p>
                Pairs are highlighted <span aria-label="a connecting green line"></span> when being tested via <code>intersects()</code>.
              </p>
            </div>

            <p>And it works. But if we had more than just a handful of balls we would start seeing performance issues.</p>
            <h2 id="performance-or-lack-thereof">Performance, or lack thereof</h2>
            <p>This naive algorithm runs in <em><strong>O(n<sup>2</sup>)</strong></em> time in <a target="_blank" href="https://en.wikipedia.org/wiki/Big_O_notation">Big O terms</a>. That is, for an input of <em>n</em> balls, the algorithm’s running time grows proportionally to the <em>square</em> of the input <em>n</em>. That’s a lot! 📈</p>
            <p>This is because for <em>n</em> balls, there are around <em>(n&nbsp;*&nbsp;(n-1))/2</em> pairs to test, or <em>0.5n<sup>2</sup>&nbsp;-&nbsp;0.5n</em>. For example, if n = 5 there would be a total of 10 pairs. For n = 10, there would be 45 pairs. For n = 15, 105 pairs (!). And so on… Using Big O notation, we can simplify this information into a compact expression <em>“O(n<sup>2</sup>)”</em></p>
            <p>To (painfully) demonstrate how the performance scales badly for bigger inputs, here’s a simulation with n&nbsp;=&nbsp;20:</p>
            <div>
              <sap-demo-client data-rss="interactive" alt="demo of the collision detection algorithm using 'pairwise' strategy" balls="20" strategy="pairwise" skip-interval="4" decorations="checks:#4c8"></sap-demo-client>
              <p>
                20 balls = 190 pairs to test
              </p>
            </div>

            <p>That’s a lot of tests per frame! Clearly, the naive solution does not scale well for large numbers of objects.</p>
            <p>How can we improve this solution?</p>
            <p><span>
              <p>The worst case running time for <em>any</em> collision detection algorithm is always <em>O(n<sup>2</sup>)</em>. That’s when all objects intersect simultaneously and you have no choice but to process each of the n<sup>2</sup> collisions.</p>
              <p>Thus, it’s more practical to compare the average and best cases.</p>
              <p>Having said that, the naive algorithm is still <em>Θ(n<sup>2</sup>)</em> for <em>any</em> case, no matter the number of actual collisions. A lot of room for improvement!</p>
            </span></p><h2 id="prologue-improving-the-solution">Prologue: Improving the solution</h2>
            <p>Usually when optimising algorithms, you wanna find <strong>redundant or unnecessary work</strong>. Then find a way to consolidate that redundancy. (That sounded corporate-ish.)</p>
            <p>A good place to start would be the <code>intersects()</code> function since it is called for every candidate pair. If we take the <a target="_blank" href="https://gdbooks.gitbooks.io/3dcollisions/content/Chapter2/static_aabb_aabb.html">typical object intersection test</a> to be its implementation, we get a bunch of these <strong>inequality checks</strong>:</p>
            <pre><code><span>function</span> <span>intersects</span><span>(</span><span>object1<span>,</span> object2</span><span>)</span> <span>{</span>
  <span>// compare objects' bounds to see if they overlap</span>
  <span>return</span> object1<span>.</span>left <span>&lt;</span> object2<span>.</span>right
      <span>&amp;&amp;</span> object1<span>.</span>right <span>&gt;</span> object2<span>.</span>left
      <span>&amp;&amp;</span> object1<span>.</span>top <span>&lt;</span> object2<span>.</span>bottom
      <span>&amp;&amp;</span> object1<span>.</span>bottom <span>&gt;</span> object2<span>.</span>top<span>;</span>
<span>}</span></code></pre>
            <p>In the above code, the <code>intersects()</code> function checks if two objects intersect by comparing their opposing bounds for each direction. (Refer to <a target="_blank" href="https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection#aabb_vs._aabb">this MDN article</a> for a better explanation.)</p>
            <p>We can break the test down to its constituent checks:</p>
            <ol>
              <li><code>object1.left &lt; object2.right</code></li>
              <li><code>object1.right &gt; object2.left</code></li>
              <li><code>object1.top &lt; object2.bottom</code></li>
              <li><code>object1.bottom &gt; object2.top</code></li>
            </ol>
            <p>Each check is solely concerned with one particular axis in a specific direction.</p>
            <p>Here’s the key thing: Due to the <code>&amp;&amp;</code> operator’s <a target="_blank" href="https://en.wikipedia.org/wiki/Short-circuit_evaluation">short-circuit evaluation</a>, if any one of these checks turns out to be false, then the overall test will immediately evaluate to false.</p>
            <p>Our goal then is to generalise the case where at least <em>one</em> of these checks is false across many tests as possible.</p>
            <p><span>It’s the same idea as the <a target="_blank" href="https://personal.math.vt.edu/mrlugo/sat.html">separating axis theorem</a>, which implies that two objects can’t be colliding if there’s at least one axis where their shadows don’t overlap.</span></p>
            <p>Let’s say we focus only on the second check - <code>object1.right &gt; object2.left</code>. Don’t worry about the rest of the checks. As hinted above, optimising in just one axis can still make a big difference later, so we’ll focus on this single check for now.</p>
            <p><img srcset="https://leanrada.com/gen/_notes_sweep_and_prune_surprise-tool_200.generated.jpg 200w" sizes=" 200px" spec="200" loading="lazy" width="100%" data-placeholder="" alt="Still of a cartoon mouse saying, 'that’s a surprise tool that can help us later'" src="https://leanrada.com/notes/sweep-and-prune/surprise-tool.jpg">
            </p>
            
            <p>Let’s look at it in the context of multiple objects. Consider three objects - A, B, and C - in this horizontal configuration:</p>
            <p><img srcset="https://leanrada.com/notes/sweep-and-prune/abc.png 664w" sizes=" 664px" spec="100% [664) 664" loading="lazy" width="100%" data-placeholder="" alt="Three objects, from left to right: A, B, and C" src="https://leanrada.com/notes/sweep-and-prune/abc.png">
            </p>
            
            <p>There are three potential pairs to be checked here: A-B, B-C, and A-C. Remember, we’re trying to find redundant work. Pretend we’re running all the pairs through the check, like so:</p>
            <pre><code><span>A</span><span>.</span>right <span>&gt;</span> <span>B</span><span>.</span>left <span>// returns false</span>
<span>B</span><span>.</span>right <span>&gt;</span> <span>C</span><span>.</span>left <span>// returns false</span>
<span>A</span><span>.</span>right <span>&gt;</span> <span>C</span><span>.</span>left <span>// returns false</span></code></pre>
            <p>See any redundant work? Maybe abstractify it a little…</p>
            <pre><code><span>A</span> <span>&gt;</span> <span>B</span> <span>// returns false</span>
<span>B</span> <span>&gt;</span> <span>C</span> <span>// returns false</span>
<span>A</span> <span>&gt;</span> <span>C</span> <span>// returns false</span></code></pre>
            <p>Voilà. Due to the <a target="_blank" href="https://www.mathwords.com/t/transitive_property_inequalities.htm"><strong>transitive property of inequality</strong></a>, realise that we don’t need to run the <strong>third test</strong>! <em>If we know that <code>A&nbsp;&gt;&nbsp;B</code> and <code>B&nbsp;&gt;&nbsp;C</code> are both <code>false</code>, then we would know that <code>A&nbsp;&gt;&nbsp;C</code> is <code>false</code> as well.</em></p>
            <blockquote>
              <p>“If <i>a ≤ b</i> and <i>b ≤ c</i>, then <i>a ≤ c</i>.”
                <cite>the transitive property of inequality</cite>
              </p>
            </blockquote>
            <p>So in this example, we don’t really need to run <code>intersects(A, C)</code>.</p>
            <pre><code><span>// 1. Test A-B</span>
<span>intersects</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>)</span> <span>// A.right &gt; B.left evals to false.</span>

<span>// 2. Test B-C</span>
<span>intersects</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>)</span> <span>// B.right &gt; C.left evals to false.</span>

<span>// 3. Infer that A.right &gt; C.left is false.</span>
<span>// ∴ Therefore I don’t need to call intersects(A, C)</span>
<span>// to know that it will return false.</span></code></pre>
            <p>We’ve skipped one <code>intersects()</code> call for free! ✨</p>
            <p><span>
              I’m handwaving the fact that <code>P.left ≤ P.right</code> is implied for any object P. Nevertheless, working those details out would just mean more transitivity.
            </span></p><p>You might be wondering how this contrived example could apply to general n-body collision detection. A smart reader such as you might also have realised that this skip only works if A, B, and C are in a <strong>particular order</strong>.</p>
            <p>What particular order? Try <span>dragging</span> the balls below to see when the optimisation applies and when it does not:</p>
            <div>
              <sap-demo-client data-rss="interactive" alt="interactive diagram showing a specific mechanism" id="abc-demo" balls="[[150,250,50],[300,150,55],[450,275,60]]" strategy="sap-nativesort" static="" draggable="" labels="A,B,C" rainbow=""></sap-demo-client>
              <pre><code><span>// LIVE OUTPUT:</span>
<span>intersects</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>)</span> <span>// A.right &gt; B.left evals to <span>false</span></span>
<span>intersects</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>)</span> <span>// B.right &gt; C.left evals to <span>false</span></span>
<span>// Deduce that intersects(A, C) will be <span>false</span></span></code></pre>
            </div>



            <p><span><strong>Tip:</strong> Drag the balls so that they’re horizontally spaced out in this order: A‑B‑C</span></p>
            <p>While it’s true that this skip only works when A, B, and C are ordered, remember that these labels are <em>arbitrary</em>! What if we simply decided to always call the leftmost ball A, the middle ball B, and the rightmost C? Then the optimisation would always be applicable! 🌌🧠</p>
            <p>But wait… labeling objects according to some logical ordering is essentially ✨<strong>sorting</strong>✨! What if we sorted the list of objects every time? Would the number of skipped tests be worth the cost of sorting?</p>
            <h2 id="chapter-1-sorting">Chapter 1. Sorting</h2>
            <p>Sorting, inequalities, and optimisation go hand in hand in hand. <em>A sorted list allows us to exploit the transitive property of inequality en masse.</em></p>
            <p><img srcset="https://leanrada.com/gen/_notes_sweep_and_prune_sorted_664.generated.png 664w" sizes=" 664px" spec="100% [664) 664" loading="lazy" width="100%" data-placeholder="" alt="a[0] ≤ a[1] ≤ a[2] ≤ ... ≤ a[n-1]" src="https://leanrada.com/notes/sweep-and-prune/sorted.png" caption="The inequality relationships of elements in a sorted list."><span>The inequality relationships of elements in a sorted list.</span>
            </p>
            
            <p><span>Even if we had to sort the list of objects every frame, the quickest general sorting algorithm runs in <em>O(n log n)</em> time which is certainly lower than <em>O(n<sup>2</sup>)</em>.</span></p>
            <p>As shown by the tri-object example above, to achieve the power to skip tests we need to sort the list of objects by x position.</p>
            <p>However, objects aren’t zero-width points. They’re <em>widthy</em>, by which I mean having a size thus occupying an interval in the x-axis, also known as “width”. How can one unambiguously sort by x position if objects span intervals in the x-axis?</p>
            <h2 id="sort-by-min-x">Sort by min x</h2>
            <p>A solution to sorting widthy objects is to sort them by their <strong>minimum x</strong> (their left edge’s x-coordinate). This technique can be applied to improve the naive approach.</p>
            <p>It involves minimal modifications to the O(n<sup>2</sup>) solution. But it will result in a good chunk of tests skipped. I’ll explain later.</p>
            <p>First, the modified code:</p>
            <pre><code><span><span>+</span> <span>// sort by min x</span>
<span>+</span> <span>sortByLeft</span><span>(</span>balls<span>)</span><span>;</span>
<span>+</span> 
</span><span><span> </span> <span>// for each ball</span>
<span> </span> <span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> balls<span>.</span>length<span>;</span> i<span>++</span><span>)</span> <span>{</span>
<span> </span>   <span>const</span> ball1 <span>=</span> balls<span>[</span>i<span>]</span><span>;</span>
<span> </span>   <span>// check each of the other balls</span>
<span> </span>   <span>for</span> <span>(</span><span>let</span> j <span>=</span> i <span>+</span> <span>1</span><span>;</span> j <span>&lt;</span> balls<span>.</span>length<span>;</span> j<span>++</span><span>)</span> <span>{</span>
<span> </span>     <span>const</span> ball2 <span>=</span> balls<span>[</span>j<span>]</span><span>;</span>
</span><span><span>+</span> 
<span>+</span>     <span>// stop when too far away</span>
<span>+</span>     <span>if</span> <span>(</span>ball2<span>.</span>left <span>&gt;</span> ball1<span>.</span>right<span>)</span> <span>break</span><span>;</span>
<span>+</span> 
</span><span><span> </span>     <span>// check for collision</span>
<span> </span>     <span>if</span> <span>(</span><span>intersects</span><span>(</span>ball1<span>,</span> ball2<span>)</span><span>)</span> <span>{</span>
<span> </span>       <span>bounce</span><span>(</span>ball1<span>,</span> ball2<span>)</span><span>;</span>
<span> </span>     <span>}</span>
<span> </span>   <span>}</span>
<span> </span> <span>}</span></span></code></pre>
            <p>It’s mostly the same as the naive solution, differing only in two extra lines of code.</p>
            <p>The first line <code>sortByLeft(balls)</code> simply sorts the list, with ranking based on the balls’ left edge x-coords.</p>
            <pre><code><span>function</span> <span>sortByLeft</span><span>(</span><span>balls</span><span>)</span> <span>{</span>
  balls<span>.</span><span>sort</span><span>(</span><span>(</span><span>a<span>,</span>b</span><span>)</span> <span>=&gt;</span> a<span>.</span>left <span>-</span> b<span>.</span>left<span>)</span><span>;</span>
<span>}</span></code></pre>
            <p>And in the inner loop, there is now this break:</p>
            <pre><code><span>if</span> <span>(</span>ball2<span>.</span>left <span>&gt;</span> ball1<span>.</span>right<span>)</span> <span>break</span><span>;</span></code></pre>
            <p>Let’s break that down.</p>
            <p>First, we know that the list is sorted, so the following statement
              holds true for any positive integer
              <code>c</code>:
            </p>
            <p>
              <code>balls[<span>j</span> + <span>c</span>].left <span>&gt;=</span> balls[<span>j</span>].left</code>
            </p>

            <p>The break condition, which is derived from the first operand of the intersection test, if true indicates early that the current pair being tested for intersection would fail:</p>
            <p>
              <code>balls2.left <span>&gt;</span> ball1.right</code><br>
              or <code>balls[<span>j</span>].left <span>&gt;</span> ball1.right</code>
            </p>

            <p>But there are more implications. If it was true, then by combining the above two inequations…</p>
            <p>
              <code>balls[<span>j</span> + <span>c</span>].left <span>&gt;=</span> balls[<span>j</span>].left <span>&gt;</span> ball1.right</code>
            </p>

            <p>And by transitive property, the following statement would also be true!</p>
            <p>
              <code>balls[<span>j</span> + <span>c</span>].left <span>&gt;</span> ball1.right</code>
            </p>

            <p>Which means the intersection tests of balls at
              <code>balls[<span>j</span> + <span>c</span>]</code>
              would also fail. We know this without needing to test those balls individually. A range of balls have been eliminated from testing!
            </p>
            <p>In conclusion, when the current <em>ball2</em>
              <code>balls[<span>j</span>]</code>
              stops overlapping with the current <em>ball1</em>, then any further <em>ball2</em>s in the iteration
              <code>balls[<span>j</span> + <span>c</span>]</code>
              would be guaranteed to not overlap <em>ball1</em> as well. In other words, we stop the inner loop when it gets too far away.
            </p>
            <p>Finally, here’s a demo:</p>
            <div>
              <sap-demo-client data-rss="interactive" alt="demo of the collision detection algorithm using 'simple-sap' strategy" strategy="simple-sap" skip-interval="4" decorations="checks:#4c8"></sap-demo-client>
              <p>
                Pairs highlighted <span aria-label="a connecting green line"></span> when tested by <code>intersects()</code>.
              </p>
            </div>

            <p>Pretty cool, right! It’s much faster now.</p>
            <p>Some observations:</p>
            <ul>
              <li>Since the list is sorted, the tests are performed from left to right.</li>
              <li>More importantly, it visibly does fewer tests than the naive approach. 📉 This is due the above optimisation which effectively limits pairs to those that overlap in the x-axis!</li>
            </ul>
            <p>Let’s analyse the time complexity. 👓</p>
            <p>The sort - if we take the "fastest" sorting algorithm, like mergesort or quicksort - would add an <em>O(n log n)</em> term.</p>
            <p>The two-level loop, now with an early break, would average out to <em>O(n&nbsp;+&nbsp;m)</em> where <em>m</em> is the total number of x-overlaps. This could degenerate into n<sup>2</sup> but as mentioned above, it’s more useful to look at the average and best cases. At best, the loop would be <em>O(n)</em>, wasting no excess processing when there are no overlaps. On average it’s <em>O(n&nbsp;+&nbsp;m)</em>.</p>
            <p><span>The average case refers to a world where objects are mostly evenly distributed and only a couple intersections per object is happening. I think this is a reasonable assumption for a relatively simple video game like a platformer or side-scroller.</span></p>
            <p>Here’s the code with running time annotations:</p>
            <pre><code><span>// O(n log n)</span>
<span>sortByLeft</span><span>(</span>balls<span>)</span><span>;</span>

<span>// O(n + m)</span>
<span>for</span> <span>(</span><span>let</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> balls<span>.</span>length<span>;</span> i<span>++</span><span>)</span> <span>{</span>
  <span>const</span> ball1 <span>=</span> balls<span>[</span>i<span>]</span><span>;</span>
  <span>// O(1) at best; O(m/n) on average; O(n) at worst</span>
  <span>for</span> <span>(</span><span>let</span> j <span>=</span> i <span>+</span> <span>1</span><span>;</span> j <span>&lt;</span> balls<span>.</span>length<span>;</span> j<span>++</span><span>)</span> <span>{</span>
    <span>const</span> ball2 <span>=</span> balls<span>[</span>j<span>]</span><span>;</span>
    <span>if</span> <span>(</span>ball2<span>.</span>left <span>&gt;</span> ball1<span>.</span>right<span>)</span> <span>break</span><span>;</span>
    <span>if</span> <span>(</span><span>intersects</span><span>(</span>ball1<span>,</span> ball2<span>)</span><span>)</span> <span>{</span>
      <span>bounce</span><span>(</span>ball1<span>,</span> ball2<span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span></code></pre>
            <p>Adding those together we get <em><strong>O(n&nbsp;log&nbsp;n&nbsp;+&nbsp;m)</strong></em>.</p>
            <p>This is a super good improvement over the naive approach’s <em>O(n<sup>2</sup>)</em>, because <strong>[1]</strong> <em>n&nbsp;log&nbsp;n</em> is <a target="_blank" href="https://bigocheatsheet.com/">much smaller</a> than <em>n<sup>2</sup></em> and <strong>[2]</strong> it is partially output-based - depending on the number of overlaps, it does not process more than necessary.</p>
            <a target="_blank" href="https://www.bigocheatsheet.com/">
              <p><img loading="lazy" src="https://www.bigocheatsheet.com/img/big-o-complexity-chart.png" caption="bigocheatsheet.com"><span>bigocheatsheet.com</span>
              </p>
            </a>

            <p>Furthermore, the choice of sorting algorithm could be improved. We’ll look into that in the next part (somehow better than <em>n&nbsp;log&nbsp;n</em>!).</p>
            <p><span>
              If you got this far trying to find a decent collision detection algorithm, then you can stop reading and take the above design! It’s the perfect balance between programming effort and running time performance. If you are curious how this develops or just want to see more interactive demos, read on to the next part.
            </span></p><h2 id="visual-comparison">Visual comparison</h2>
            <p><a id="comparisons"></a>Here’s a side-by-side comparison of the strategies we’ve covered so far! Observe the amount of intersection tests required per frame. 🔍 n = 10</p>
            

            

            <p>(Not shown: the cost of sorting. Let’s just say the intersection test is sufficiently expensive.)</p>
            <p>Aaand that concludes the first part. Those two lines of code definitely were the MVPs.</p>
            <p>How will it compare to the more advanced versions?</p>
            <p><a target="_self" href="https://leanrada.com/notes/sweep-and-prune-2/">Continued in part 2.</a>
            </p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things I learned from teaching (2023) (105 pts)]]></title>
            <link>https://claytonwramsey.com/blog/learned-from-teaching</link>
            <guid>41241825</guid>
            <pubDate>Wed, 14 Aug 2024 01:56:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://claytonwramsey.com/blog/learned-from-teaching">https://claytonwramsey.com/blog/learned-from-teaching</a>, See on <a href="https://news.ycombinator.com/item?id=41241825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>
        This spring, I taught an undergraduate class on chess engines. I probably learned more than any of my students.
      </p>
      <p>
        As a senior in college, I was the instructor, lecturer, grader, master of ceremonies, and grand poobah of COLL
        110: <cite>Artificial Intelligence for Chess</cite>, 6-7 P.M. every Tuesday for the whole semester. In theory, I
        had a faculty advisor, but he was busy most of the time, so at the end of the day, it was my rodeo and I got to
        figure it all out on my own. I thought it was a great experience!
      </p>
      <p>
        I'm writing this post for a few reasons: first, I want to write down the things I wish that I had known before I
        started teaching, in the hopes that other soon-to-be teachers might take this to heart. The other reason is to
        compile my own thoughts on the matter, since I'll possibly revive this class in the future, and want to have
        some notes while it's fresh in my mind.
      </p>
      <h2>You're not even their second priority</h2>
      <p>
        If you're teaching a topic, you're probably really passionate about it (unless, of course, the department forced
        you to teach the class). At the very least, I am. Chess engines are cool. Lucky for me, students taking a COLL
        course get little in the way of credit for the class, so most of my pupils were also pretty stoked about it. At
        the end of the day, though, they're not getting a Ph.D. in this stuff, and they've got half a dozen other
        classes on top of work, family, friends, and this weekend's party to think about.
      </p>
      <p>
        To an instructor, this tends to manifest as an air of boredom with the whole topic. Often, the students aren't
        actually bored! They just don't have the time or energy for the topic that I do.
      </p>
      <p>
        The fix, I think, is to meet students where they are. The hard (and rewarding) part of teaching a class is that
        I needed to digest reams of material into brief, easily-comprehensible components. I have the patience to trawl
        through uncommented C++ code and ancient forum posts to figure out how to build a good chess engine, but the
        students shouldn't need it.
      </p>
      <p>
        I'm not a hundred percent certain of the exact mechanisms of <em>how</em>, yet. In my experience, I learn the
        most when I struggle; if a student can shortcut through all the hard parts on, for example, and assignment,
        they're not going to learn very much. On the flip side, when most students struggle, they just give up. Somehow
        I need to make assignments which thread the needle between being too hard to solve and too easy to learn
        anything.
      </p>
      <h2>Students have wildly varying backgrounds</h2>
      <p>
        Chess engines are a relatively niche topic in computer science, so I was quite surprised to see that my class
        section was completely full at the start of the semester. It seems as though most of the students had been
        clickbaited by my mention of AI in the title, which is more a callout for classical artificial intelligence than
        it is for the modern machine-learning approaches.
      </p>
      <p>
        Although a handful of students dropped the course, I was eventually left with about a dozen students whose
        experience and programming skill varied wildly. Some of my students had finished little more than an
        introductory computer science course, while others were more qualified than I was.
      </p>
      <p>
        This presents a fundamental problem: how do I come up with a curriculum which is complex enough to excite the
        experienced students without completely losing the less experienced ones? The cynical answer is "you can't," but
        I did give it a fair attempt, and I think it turned out OK. Typically, I tried to integrate a mix of skill
        levels into my lectures; for instance, when discussing the representation of squares and directions, I spent a
        few minutes explaining how they could be modeled with torsors, but not so long that I would lose the students
        who neither knew nor cared about group theory.
      </p>
      <p>
        Assignment design is harder. I tried to give interesting challenges by leaving bonus tasks on every assignment,
        and for the final project, I gave a choose-your-own-adventure assignment which allowed students to pick a task
        that matched their skill level. However, I'm not convinced that I perfected this approach, so I will need to
        think more deeply on how do design "multi-level" projects for a mixed student body.
      </p>
      <h2>Engagement falls off in the first ten minutes</h2>
      <p>
        COLL 110 was a standard lectures-and-assignments college class - I lectured during our scheduled meeting time,
        then students did their projects on their own. Having tried this, I think that this is just not the future of
        education. This mode of teaching is designed mostly for the lecturer's convenience, but it's a terrible way to
        foster student understanding.
      </p>
      <p>
        The fundamental reality is that it's impossible to listen actively for a long period of time. It was even worse
        in my case: I was teaching a blow-off class at the end of the day, so students were already tired and didn't
        feel any pressure to keep up. The net result was that only about half of my students were really paying
        attention at a time, which is pretty bad if you want them to actually learn anything.
      </p>
      <p>
        I'm certain that I will need to use a different format for lessons in the future. Flipped classrooms are popular
        these days, so I might try that, but I also feel that pre-recorded lectures are a little soulless. I might try a
        hybrid approach, integrating lectures with assignments.
      </p>
      <p>
        In terms of engagement, one of the best lectures I ever gave was the introduction to Rust. This was because I
        got people to pull up the
        <a href="https://play.rust-lang.org/">Rust Playground</a> on their laptops, so they were actively running and
        debugging code in class and I could work with them. Moving forward, I want to try stuff like this some more:
        fully-interactive in-class content which revolves around student experimentation.
      </p>
      <p>
        Easier said than done, though. Getting people to code in person is actually quite difficult, and if there's only
        one of me, I can't keep up with every student at once. COMP 140, the introductory CS class at Rice, manages this
        with a small army of TAs who can keep up with all the students, but I don't have any TAs to do that for me.
      </p>
      <h2>Nobody goes to office hours</h2>
      <p>
        On every assignment, one of my students would write something like this in the comments of their submission.
      </p>
      <pre><code>// This doesn't work and I don't know why. 
// I didn't have enough time to figure it out.</code></pre>
      <p>
        And, invariably, every <em>single</em> time, the problem with their work was something that I could have (and
        would have happily) explained to them at office hours or even in an email. It saddens me greatly to see these
        kinds of submissions, since it's evidence of a completely solvable problem.
      </p>
      <p>
        I really love getting to work with students one-on-one. It often reveals gaps in understanding that aren't
        obvious in lecture or in assignments, and also shows me where I'm failing to cover material. One of my best
        experiences when holding office hours was when I got to explain how the function call stack works to an
        underclassman, and it was really fulfilling to see how it "clicked" for them. When students show up to office
        hours, it's extremely valuable for me and for them.
      </p>
      <p>
        I think that there are a few reasons that students don't like to show up to office hours. The first, simplest
        reason, is simply availability. I think a lot of students start their projects far later than they really ought
        to, and so by the time that they get stuck, there aren't any office hours between then and the due date. The
        easiest fix is to make office hours available on the same day that assignments are due, so that I can be
        available when students are working on the assignment.
      </p>
      <p>
        The second issue is comfort: sometimes, students feel like they're imposing on their instructors' time by
        showing up to office hours. This isn't helped by the fact that some professors can be downright <em>mean</em> to
        their students at office hours, which can leave a bad taste in students' mouths, even for other classes. When I
        spoke to one of my instructional advisors about this, she recommended that I refer to office hours as "student
        hours" instead, in order to set the expectation that it's there for the students' benefit.
      </p>
      <h2>Takeaways</h2>
      <p>
        I didn't come away from teaching this class feeling like I had mastered the art of pedagogy, or even thinking
        that I was half decent. I suspect that the core challenges that I faced were much the same as with any other
        class, though perhaps exacerbated by COLL 110's status as an elective.
      </p>
      <p>
        Even then, I think it was a great opportunity for me to learn and grow as a person, and has helped me a lot,
        especially in writing, public speaking, and professional communication. If you're on the fence about teaching,
        you should definitely give it a try, if only because interacting with students is such a rewarding experience.
      </p>
      <p>Thanks to <a href="https://shreyasminocha.me/">Shreyas</a> and Charlie for reviewing this article.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM-based sentiment analysis of Hacker News posts between Jan 2020 and June 2023 (120 pts)]]></title>
            <link>https://outerbounds.com/blog/hacker-news-sentiment/</link>
            <guid>41241124</guid>
            <pubDate>Tue, 13 Aug 2024 23:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://outerbounds.com/blog/hacker-news-sentiment/">https://outerbounds.com/blog/hacker-news-sentiment/</a>, See on <a href="https://news.ycombinator.com/item?id=41241124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content" itemprop="articleBody"><hr><p><strong>tl;dr</strong>
We analyzed all Hacker News posts with more than five comments between January 2020 and June 2023.
Leveraging LLama3 70B LLM, we examined both the posts and their associated comments to gain insights
into how the Hacker News community engages with different topics. You can download the datasets
we produced at the bottom of the article.</p><p>Use the tool below to explore various topics and the sentiments they evoke. The sentiment column
reports the median sentiment score, 0 being the most negative sentiment and 9 the most positive.
Click on the column headers to discover topics that inspire strong reactions, whether positive or
negative, and to identify divisive subjects that tend to generate polarized commentary.</p><h2 id="motivation">Motivation<a href="#motivation" aria-label="Direct link to Motivation" title="Direct link to Motivation">​</a></h2><p>If you have been following Hacker News for a while, you have likely developed an intuition
for the topics that the community loves - and topics the community loves to hate.
If you port <strong>Factorio</strong> to run on <strong>ZX Spectrum</strong> using <strong>Rust</strong>, you will be overwhelmed
with love and karma. On the other hand, you better don an asbestos suit if, instead
of raising a <strong>funding round</strong>, you sell your startup to a <strong>private equity</strong> company
so they can add <strong>telemetry</strong> in the codebase to power <strong>targeted ads</strong>.</p><p>But that's just a hazy intuition! Since the community is a big believer in <strong>rationality</strong>
and <strong>data science</strong> (the phrase is divisive though), we would be in a stronger position
if we could back our hunch with proper <strong>data analysis</strong>.</p><p>Also, it would give us an excuse to play with <strong>large language models</strong> which, all the
<strong>AI hype</strong> aside, are mind-blowingly effective at practical tasks like this. And,
we happen to be developers of an <strong>open source</strong> tool,
<a href="https://github.com/netflix/metaflow" target="_blank" rel="noopener noreferrer">Metaflow</a>, written in <strong>Python</strong>,
which makes it <strong>fun</strong> and <strong>educational</strong> to <strong>hack</strong> projects like this.</p><p>Type the bolded phrases in the textbox above to see if we are optimizing for an
appropriate <strong>emotional response</strong>. </p><h2 id="what-topics-trend-on-hacker-news">What topics trend on Hacker News?<a href="#what-topics-trend-on-hacker-news" aria-label="Direct link to What topics trend on Hacker News?" title="Direct link to What topics trend on Hacker News?">​</a></h2><p>As described in the implementation section below, we downloaded about 100,000 pages posted
on Hacker News to understand what content resonates with the community. We focus on posts
that gained at least 20 upvotes and 5 comments, prompting a large language model to come
up with ten phrases that best describe each page.</p><p>Here are the top 20 topics, aggregated from the 100,000 pages, along with the number of
posts covering each topic:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/top_topics-f47d2ee20906766a39ed8fdb2908c264.png" width="1295" height="1033"></p><p>The top topics are hardly surprising. However, the community is known for having
diverse intellectual interests. The top 20 topics represent only 10% of the topics
covered across posts. You can see this by yourself by using the tool above that
cover all the 14,000 topics that are associated with at least five posts.</p><p>Have the top topics evolved over time? You bet. Here are the top-10 trending
topics:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/growing_topics-955fdd56864b7346df6f39888123e4a4.png" width="1410" height="1019"></p><p>Even though the dataset only extends to June 2023, we can see a tsunami
of AI, natural language processing, and related topics. Sadly, layoffs started
trending mid-2022 as well. The rise of AI articles likely explains the growth
in the Technology topic as well.</p><h2 id="whats-declining">What's declining<a href="#whats-declining" aria-label="Direct link to What's declining" title="Direct link to What's declining">​</a></h2><p>In 2020-2022, an overwhelming macro-trend was everything related
to the COVID pandemic which fortunately is not a pressing issue
anymore. Fascinatingly, August 2021 was a tumultuous month with especially
<a href="https://www.wired.com/story/apple-photo-scanning-csam-communication-safety-messages/" target="_blank" rel="noopener noreferrer">Apple's proposed CSAM
scanning</a>
causing an uproar in posts related to Privacy and Apple.</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/declining_topics-fded256187c75900d3f7871a9acae077.png" width="1471" height="1048"></p><p>If you want to take a trip down the memory lane, consider the past
topics on the left which haven't been covered once since January 2022:</p><table><thead><tr><th>Before Jan 2022 📉</th><th>After Jan 2022 📈</th></tr></thead><tbody><tr><td>George Floyd</td><td>GPT-4</td></tr><tr><td>Herd immunity</td><td>Stable Diffusion</td></tr><tr><td>Antibodies</td><td>Russia-Ukraine War</td></tr><tr><td>IOS 14</td><td>Ventura</td></tr><tr><td>Freenode</td><td>Bank Failure</td></tr><tr><td>Suez Canal</td><td>Midjourney</td></tr><tr><td>Wallstreetbets</td><td>Hiring Freeze</td></tr><tr><td>Hydroxychloroquine</td><td>FIDO Alliance</td></tr><tr><td>Infection rates</td><td>Cost of living crisis</td></tr></tbody></table><p>Correspondingly, the topics on the right didn't exist before
January 2022. It doesn't take a PhD in Economics to understand how
<em>Wallstreetbets</em> got replaced by <em>Cost of living crisis</em>,
<em>Hiring freeze</em>, and <em>Bank failure</em>.</p><h2 id="but-how-do-people-feel-about-these-topics">But how do people <em>feel</em> about these topics<a href="#but-how-do-people-feel-about-these-topics" aria-label="Direct link to but-how-do-people-feel-about-these-topics" title="Direct link to but-how-do-people-feel-about-these-topics">​</a></h2><p>Importantly, sharing or upvoting a post doesn't imply endorsement - <a href="https://xkcd.com/386/" target="_blank" rel="noopener noreferrer">often the opposite</a>. Hence
to truly understand the dynamics of a community, we need to analyze how people react to posts, as expressed
through their comments.</p><p>To do this, we reconstructed comment threads associated with the posts in the dataset and asked
an LLM to classify the sentiment of the discourse between 0 and 9, zero being an all-out flamewar
and nine indicating a perfect harmony and positivity.</p><p>Our dutiful LLM took the job as a virtual community moderator and went through 100k comment threads in
about 9 hours, reading through 230M words consisting of the full emotional gamut of bitterness,
passion, wisdom, humor, and love.</p><p>Here's what the LLM came back with in terms of the distribution of sentiments:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/sentiment-distribution-da1bd4c602de66b6f1ef68cb3434a4f8.png" width="721" height="399"></p><p>Firstly, the LLM is utterly befuddled about what a neutral discussion looks like - there are no 5 in the results.
Or, maybe this is a snarky note by the LLM noting that humans are incapable of unemotional, unbiased discourse.</p><p>Secondly, the sentiments are clearly skewed
towards the positive side, which aligns with our personal experience with the site. The bias to
positivity and optimism is a key reason why we open Hacker News daily. There's enough vitriol elsewhere.</p><p>It is also worth noting that at least some of the few ones assigned by the machine seem to be bogus.
For instance, <a href="https://news.ycombinator.com/item?id=32088537" target="_blank" rel="noopener noreferrer">this post by BentoML</a> was given a score of one,
although the sentiment seems majorly positive and supportive. All the usual caveats about LLMs apply.</p><p>Now with the topics and the sentiment scores at hand, we can finally give a scientific answer to the question of
what the community loves and loves to hate:</p><table><thead><tr><th>Love 😍</th><th>Hate 😠</th></tr></thead><tbody><tr><td>Programming</td><td>FTX</td></tr><tr><td>Computer Science</td><td>Police Misconduct</td></tr><tr><td>Open Source</td><td>Sam Bankman-Fried</td></tr><tr><td>Python</td><td>Xinjiang</td></tr><tr><td>Game Development</td><td>Torture</td></tr><tr><td>Rust</td><td>Employee Monitoring</td></tr><tr><td>Electronics</td><td>Cost Cutting</td></tr><tr><td>Mathematics</td><td>Racial Profiling</td></tr><tr><td>Functional Programming</td><td>Online Safety Bill</td></tr><tr><td>Programming Language</td><td>War on Terror</td></tr><tr><td>Physics</td><td>Atlassian</td></tr><tr><td>Embedded Systems</td><td>CSAM</td></tr><tr><td>Self Improvement</td><td>NYPD</td></tr><tr><td>Database</td><td>Alameda Research</td></tr><tr><td>Unix</td><td>International Students</td></tr><tr><td>Astronomy</td><td>TSA</td></tr><tr><td>Retro Computing</td><td>Earn It Act</td></tr><tr><td>Nostalgia</td><td>Car Features</td></tr><tr><td>Debugging</td><td>Bloatware</td></tr></tbody></table><p>Geeks, nerds, and hackers should find themselves right at home! Interestingly, while the community tends to be
visionary and forward-looking (with technical matters at least), it definitely has a soft spot for (technical) nostalgia.
While the modern world is amazing, we miss ZX Spectrum, Z80 assembly, and 8086 dearly. At least we find some
comfort in PICO-8.</p><p>On the anger-inducing side, most topics need no explanation. It is worth clarifying though that Hacker News does not
hate International Students, but the posts related to them tend to be overwhelmingly negative,
reflecting the community’s sympathy for the challenges faced by those studying abroad.</p><p>Comically, Hacker News is not a community of car lovers. When we talk about cars, it is because there's
something wrong with them. For more insights like this, use the tool at the top of the page to explore
the diverse landscape of HN topics in detail.</p><h2 id="some-topics-are-just-divisive">Some topics are just divisive<a href="#some-topics-are-just-divisive" aria-label="Direct link to Some topics are just divisive" title="Direct link to Some topics are just divisive">​</a></h2><p>Besides topics being unimodally love or hate-inducing, some topics are bimodal: Sometimes
a post about the topic generates a highly positive response, other times a flamewar. Examples include</p><ul><li><strong>GNOME</strong> - <a href="https://www.mayrhofer.eu.org/post/kde-vs-gnome/" target="_blank" rel="noopener noreferrer">KDE vs. GNOME</a> - the war has been raging for 25 years.</li><li><strong>Google</strong> - a dominant force in the Internet, both in good and bad.</li><li><strong>Government regulations</strong> - damned if you do and damned if you don't.</li><li><strong>Venture capital</strong> - the lifeblood of Silicon Valley and a source of endless gossip.</li></ul><p>See more by sorting by the <code>divisive</code> column in the tool above. To rank highly by the divisiveness
score, the topic must be associated with both negative and positive posts equally
and not many neutral ones.</p><h2 id="is-the-mood-improving">Is the mood improving?<a href="#is-the-mood-improving" aria-label="Direct link to Is the mood improving?" title="Direct link to Is the mood improving?">​</a></h2><p><em>Cue <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines" target="_blank" rel="noopener noreferrer">Betteridge's Law</a></em>.</p><p>We can plot the average sentiment over time, as expressed in daily comment threads:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/sentiment-over-time-97569616155ec98e8d15c008b62591bc.png" width="1147" height="818"></p><p>The data proves that things sucked in August 2021 (or maybe it was just the Apple debacle highlighted
in the chart above). Overall, there's a clear but modest downward trend in the average sentiment.</p><p>It
would require a deeper analysis to understand why this is the case. Putting aside an obvious hypothesis
that life is just getting worse (which <a href="https://en.wikipedia.org/wiki/Factfulness" target="_blank" rel="noopener noreferrer">might not be true</a>),
an alternative hypothesis may be a variant of
<a href="https://en.wikipedia.org/wiki/Eternal_September" target="_blank" rel="noopener noreferrer">Eternal September</a>. It takes conscious and tireless
effort to maintain a positive mood in a growing community. Kudos to HN moderators for keeping the
community thriving and positive over the years!</p><h2 id="implementation">Implementation<a href="#implementation" aria-label="Direct link to Implementation" title="Direct link to Implementation">​</a></h2><p>A reason to be excited and optimistic about the future is the very existence of this article.
While natural language processing and sentiment analysis have been around for decades,
the quality, versatility, and the ease of use afforded by LLMs is absolutely unprecedented.</p><p>Attaining the quality
of topics and sentiment scores with a messy dataset like the one here would have required a PhD-thesis
level of effort just a few years ago - and very likely the results would have been worse. In contrast,
we developed all the code for this article in about seven hours. Processing 350M tokens with just
a decent-sized model would have required a supercomputer a decade ago, whereas in our case it took about 16 hours
using widely available hardware.</p><p>Most amazingly, all the building blocks, LLMs included, are available in open source! Let's do a
quick overview (with code and data), showing how you can repeat the experiment at home.</p><p>Here's what we did at the high level:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/hacker-news-architecture-cec4efca1150350d4bb35ac06dfb7761.png" width="1920" height="1080"></p><p>Each white box in the picture is <a href="https://docs.metaflow.org/" target="_blank" rel="noopener noreferrer">a Metaflow flow</a>, linked below:</p><ol><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hninit.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentInit</code></a> creates a list of posts to analyze.</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncrawl.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentCrawl</code></a> downloads the posts.</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentAnalyzePosts</code></a> parses the posts and runs them through an LLM.</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncomments.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentCommentData</code></a> reconstructs comment threads based on <a href="https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news" target="_blank" rel="noopener noreferrer">a Hacker News dataset in Google BigQuery</a>. We should/could
have used this in the step (1) too. Next time!</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnsentiment.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentAnalyzeComments</code></a> runs the comment threads through an LLM.</li><li>Data analyses and the charts shown above are produced in notebooks (the gray box in the picture).</li></ol><p>Here's what the flows do:</p><h3 id="donwloading-posts">Donwloading posts<a href="#donwloading-posts" aria-label="Direct link to Donwloading posts" title="Direct link to Donwloading posts">​</a></h3><p>First, we wanted to analyze topics covered by Hacker News posts that have
generated some discussion. Using <a href="https://huggingface.co/datasets/julien040/hacker-news-posts" target="_blank" rel="noopener noreferrer">a publicly
available dataset of HN posts</a>
(<a href="https://julienc.me/" target="_blank" rel="noopener noreferrer">thanks Julien!</a>), we queried all posts between
January 2020 and June 2023 (the latest date available in this dataset) which had at
least 20 upvotes and more than five comments, which resulted in about 100,000 posts.
<a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hninit.py" target="_blank" rel="noopener noreferrer">Here's the simple Metaflow flow</a>
that did the job, much thanks to DuckDB.</p><p>Since the 100,000 posts are mostly on different domains, we can safely download
them in parallel without DDOS'ing the servers. It took only around 25 minutes to
download the pages with 100 parallel workers
(<a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncrawl.py" target="_blank" rel="noopener noreferrer">see here how</a>).</p><h3 id="large-scale-document-understanding-with-llms">Large-scale document understanding with LLMs<a href="#large-scale-document-understanding-with-llms" aria-label="Direct link to Large-scale document understanding with LLMs" title="Direct link to Large-scale document understanding with LLMs">​</a></h3><p>Parsing the text content from random HTML pages used to be a massive PITA, but
<a href="https://beautiful-soup-4.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">BeautifulSoup</a> makes it <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py#L94" target="_blank" rel="noopener noreferrer">beautifully
straightforward</a>.</p><p>At this point, we have a relatively clean dataset of about 100,000 text documents with
more than 500M tokens in total. Processing the dataset once with a state-of-the-art LLM,
say, GPT-4o, or Llama3 70b on AWS Bedrock would cost around $1,300 (using ChatGPT batch API),
not counting the output tokens. Besides the cost, time is a concern - we want to process
the dataset as fast as possible, so we can evaluate the results quickly, and rinse-and-repeat
if needed. Hence we don't want to get rate-limited or otherwise bottlenecked by the APIs.</p><p>We have <a href="https://outerbounds.com/blog/document-understanding/" target="_blank" rel="noopener noreferrer">posted previously</a> about our
success with <a href="https://outerbounds.com/blog/nim-announcement/" target="_blank" rel="noopener noreferrer">NVIDIA NIM microservices</a> that
provide a quickly growing library of LLMs and other GenAI models as prepackaged images,
fully optimized to take advantage of vLLM, TensorRT-LLM and the Triton Inference Server,
so you don't have to spend time <a href="https://outerbounds.com/blog/the-many-ways-to-deploy-a-model/#measuring-performance" target="_blank" rel="noopener noreferrer">chasing the latest tricks with LLM
inference</a>.</p><p>Since we had NIMs included in <a href="https://outerbounds.com/platform" target="_blank" rel="noopener noreferrer">our Outerbounds deployment</a>,
we just <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py#L40" target="_blank" rel="noopener noreferrer">added <code>@nim</code> to our flow</a>
to get  access to a high-throughput LLM endpoint that costs only as much as the auto-scaling GPUs
it runs on, in this case, four H100 GPUs. Naturally you can hit an LLM endpoint of your
choosing - the options are many these days.</p><h3 id="prompting-an-llm-to-produce-a-list-of-topics-for-each-post">Prompting an LLM to produce a list of topics for each post<a href="#prompting-an-llm-to-produce-a-list-of-topics-for-each-post" aria-label="Direct link to Prompting an LLM to produce a list of topics for each post" title="Direct link to Prompting an LLM to produce a list of topics for each post">​</a></h3><p>Our prompt is straightforward:</p><div open=""><pre tabindex="0"><code><span><span>Assign 10 tags that best describe the following article.</span><br></span><span><span>Reply only the tags in the following format:</span><br></span><span><span>1. first tag</span><br></span><span><span>2. second tag</span><br></span><span><span>N. Nth tag</span><br></span><span><span>---</span><br></span><span><span>[First 5000 tokens from a web page]</span><br></span></code></pre></div><p>The llama3 70b model we used has a 8,000 token context window, but we decided to
limit the number of tokens to 5,000 to account for differences in the tokenizer behavior,
making sure that we don't go past the limit. </p><p>Processing about 140M inputs tokens in this manner took about 9 hours. We were able to
increase throughput to around 4,300 input tokens per second by hitting the model concurrently
with five workers, as neatly shown in our UI below, to take advantage of dynamic batching
and other optimizations.</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/sentiment-mfgui-582a32fc57b12b9a0835e61997598237.png" width="1869" height="950"></p><p>Instead of trying to download 100,000 comment pages directly from Hacker News, we
leveraged <a href="https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news" target="_blank" rel="noopener noreferrer">a Hacker News dataset in Google
BigQuery</a>.
Annoyingly, comments in the database are not directly associated with their parent
post, so we had to implement <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncomments.py#L152" target="_blank" rel="noopener noreferrer">a small function to reconstruct the comment
threads</a>.</p><p>We can't run the function with BigQuery directly but luckily we can export the data easily
in Parquet files. Loading the resulting 16M rows in DuckDB and scanning through them
was a breeze, aided by the fact that Metaflow knows how to <a href="https://outerbounds.com/blog/metaflow-fast-data/" target="_blank" rel="noopener noreferrer">load data
fast</a>. We just
added <code>@resources(disk=10000, cpu=8, memory=32000)</code>  to run the function on a large enough instance.</p><h3 id="prompting-an-llm-to-analyze-sentiment">Prompting an LLM to analyze sentiment<a href="#prompting-an-llm-to-analyze-sentiment" aria-label="Direct link to Prompting an LLM to analyze sentiment" title="Direct link to Prompting an LLM to analyze sentiment">​</a></h3><p>With the comment threads at hand, we were able to <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnsentiment.py" target="_blank" rel="noopener noreferrer">run them through our
LLM</a> with this prompt:</p><div open=""><pre tabindex="0"><code><span><span>In the scale between 0-10 where 0 is the most negative sentiment</span><br></span><span><span>and 10 is the most positive sentiment, rank the following discussion.</span><br></span><span><span>Reply in this format:</span><br></span><span><span></span><br></span><span><span>SENTIMENT X</span><br></span><span><span></span><br></span><span><span>where X is the sentiment rating</span><br></span><span><span>---</span><br></span><span><span>[First 3000 tokens from a comment thread]</span><br></span></code></pre></div><p>Getting a simple structured output like this seems to work without issues. We processed
through some 230M input tokens in this manner, which took only about 7 hours as we
needed only two output tokens.</p><p>You can reproduce all the steps above using your favorite toolchain. Here are
key reasons why we used Metaflow and why you might want to consider it too:</p><ul><li><p><strong>Staying organized without effort</strong> - a big benefit compared to random Python
scripts or notebooks is that Metaflow persists all artifacts automatically, tracks all
executions, and keeps everything organized. We relied on <a href="https://docs.metaflow.org/scaling/tagging" target="_blank" rel="noopener noreferrer">Metaflow's
namespaces</a> to execute large and expensive
runs alongside prototypes, knowing that the two can't interfere with each other. We
used <a href="https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags/" target="_blank" rel="noopener noreferrer">Metaflow tags</a>
to organize data sharing between flows while they were being developed independently.</p></li><li><p><strong>Easy cloud scaling</strong> - crawling required <a href="https://docs.metaflow.org/scaling/remote-tasks/introduction#running-many-tasks-in-parallel-with-foreach" target="_blank" rel="noopener noreferrer">horizontal
scaling</a>,
DuckDB required <a href="https://docs.metaflow.org/scaling/remote-tasks/introduction#requesting-compute-resources" target="_blank" rel="noopener noreferrer">vertical scaling</a>,
and LLMs required <a href="https://docs.metaflow.org/scaling/remote-tasks/gpu-compute" target="_blank" rel="noopener noreferrer">a GPU backend</a>.
Metaflow handled all the cases out of the box.</p></li><li><p><strong>Highly available orchestrator</strong> - running a large dataset through an LLM can cost
thousands of dollars. You don't want the run to fail because of random issues.
We relied on <a href="https://docs.metaflow.org/production/introduction" target="_blank" rel="noopener noreferrer">a highly available Argo Workflows
orchestrator</a> that Metaflow
supports out of the box to keep the run running for hours.</p></li></ul><p>You can do all of the above using <a href="https://metaflow.org/" target="_blank" rel="noopener noreferrer">open-source Metaflow</a>, but we had a few
additional benefits by running the flows on <a href="https://outerbounds.com/platform" target="_blank" rel="noopener noreferrer">Outerbounds Platform</a>:
It is simply fun to develop code like this, including notebooks, with VSCode running on
<a href="https://outerbounds.com/features/cloud-workstations/" target="_blank" rel="noopener noreferrer">cloud workstations</a>,
<a href="https://outerbounds.com/features/compute-at-scale/" target="_blank" rel="noopener noreferrer">scaling to the cloud</a> is smooth sailing,
and <a href="https://outerbounds.com/blog/nim-announcement/" target="_blank" rel="noopener noreferrer"><code>@nim</code></a> allowed us to hit LLMs without
worrying about cost or rate limiting.</p><p>If features like this sound relevant to your interests, we are happy
to <a href="https://outerbounds.com/get-started/" target="_blank" rel="noopener noreferrer">get you started for free</a>.</p><h2 id="dive-deeper-at-home">Dive deeper at home<a href="#dive-deeper-at-home" aria-label="Direct link to Dive deeper at home" title="Direct link to Dive deeper at home">​</a></h2><p>There's much more that can be analyzed and visualized with this dataset. Instead of spending
a few thousand dollars hitting OpenAI APIs, you can
<a href="https://github.com/outerbounds/hacker-news-sentiment/tree/main/data" target="_blank" rel="noopener noreferrer">download the topics and sentiments we created</a>:</p><ul><li><p><code>post-sentiment.json</code> contains a mapping <code>post_id -&gt; sentiment_score</code></p></li><li><p><code>post-topics.json</code> contains a mapping <code>post_id -&gt; [topics]</code></p></li><li><p><code>topics-data.json</code> contains a cleaned and joined dataset based on the above JSONs,
powering the tool at the top of this article.</p></li></ul><p>You can find metadata related to post IDs in <a href="https://huggingface.co/datasets/julien040/hacker-news-posts" target="_blank" rel="noopener noreferrer">this HuggingFace
dataset</a> and in the
<a href="https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news" target="_blank" rel="noopener noreferrer">Google BigQuery Hacker News
dataset</a>.
To view posts, simply open
<a href="https://news.ycombinator.com/item?id=41157595" target="_blank" rel="noopener noreferrer">https://news.ycombinator.com/item?id=[post_id]</a>.</p><p>For instance, it would be interesting to look into the correlation between post domains and
sentiments and topics. Our hunch is that certain domains produce predominantly positive
sentiments and vice versa. Or, do divisive topics garner more points?</p><p>If you create something fun with this data, please link back to this blog article and let us
know! Join <a href="http://slack.outerbounds.co/" target="_blank" rel="noopener noreferrer">Metaflow Slack</a> and drop a note on <code>#ask-metaflow</code>.</p><p>To support our open-source efforts, please give <a href="http://github.com/netflix/metaflow" target="_blank" rel="noopener noreferrer">Metaflow a star</a>! 🤗</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ex-Kansas police chief who raided local newspaper criminally charged (424 pts)]]></title>
            <link>https://www.theguardian.com/us-news/article/2024/aug/13/marion-county-police-newspaper-raid-charges</link>
            <guid>41240755</guid>
            <pubDate>Tue, 13 Aug 2024 23:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/article/2024/aug/13/marion-county-police-newspaper-raid-charges">https://www.theguardian.com/us-news/article/2024/aug/13/marion-county-police-newspaper-raid-charges</a>, See on <a href="https://news.ycombinator.com/item?id=41240755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A former Kansas police chief who led a raid last year on a weekly newspaper has been <a href="https://www.theguardian.com/us-news/article/2024/aug/06/kansas-newspaper-raid-charges-police-chief" data-link-name="in body link">charged</a> with felony obstruction of justice and is accused of persuading a potential witness to withhold information from authorities when they later investigated his conduct.</p><p>The single charge against Gideon Cody, the former Marion police chief, alleges that he knowingly or intentionally influenced the witness to withhold information on the day of the raid of the Marion County Record and the home of its publisher or sometime within the following six days. The charge was filed on Monday in state district court in Marion county and is not more specific about Cody’s alleged conduct.</p><p>The raid sparked a national debate about press freedom focused on Marion, a town in Kansas of about 1,900 people set among rolling prairie hills. Also, newspaper publisher Eric Meyer’s <a href="https://www.theguardian.com/us-news/2023/aug/15/joan-meyer-co-owner-kansas-paper-police-raid" data-link-name="in body link">mother</a>, who co-owned the newspaper and lived with him, <a href="https://www.theguardian.com/us-news/2023/aug/13/marion-county-record-co-owner-joan-meyer-dies-kansas-police-raid" data-link-name="in body link">died </a>the next day of a heart attack, and he blames the stress of the raid.</p><figure id="73e37894-db37-498c-ae1c-03feea9189bb" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Image from Marion police department body camera video shows Gideon Cody during his department’s raid of the Marion County Record newspaper in August 2023." src="https://i.guim.co.uk/img/media/325195880442149cc32c9b3008baf0aba6b8994a/0_0_1685_843/master/1685.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="222.6320474777448" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Image from Marion police department body-camera video shows Gideon Cody during his department’s raid of the Marion County Record newspaper in August 2023.</span> Photograph: AP</figcaption></figure><p>Meyer said last week that authorities appear to be making Cody the “fall guy” for the raid when numerous officials were involved. He said on Tuesday that he suspects the criminal case ultimately will be resolved through a plea bargain so that Cody will not have a trial that would more fully disclose details about the raid.</p><p>“We’re just being basic journalists here,” he said. “We want the whole story. We don’t want part of it.”</p><p>A report from two special prosecutors last week referenced text messages between Cody and a local business owner after the raid. The business owner has said that Cody asked her to delete text messages between them, fearing people could get the wrong idea about their relationship, which she said was professional and platonic.</p><p>The Associated Press left a message seeking comment at a possible cellphone number for Cody, and it was not immediately returned on Tuesday. Attorneys representing Cody in a federal lawsuit over the raid are not representing him in the criminal case and did not immediately know who was representing him.</p><p>Cody justified the 11 August 2023 raid by saying he had evidence that Meyer, the newspaper and one of its reporters, Phyllis Zorn, had committed identity theft or other computer crimes in verifying the authenticity of a copy of the business owner’s state driving record provided to the newspaper by an acquaintance. The business owner was seeking Marion city council approval for a liquor license and the record showed that she potentially had driven without a valid license for years. However, she later had her license reinstated.</p><p>The prosecutors’ report concluded that no crime was committed by Meyer, Zorn or the newspaper and that Cody reached an erroneous conclusion about their conduct because of a poor investigation. Zorn used the information she had to legally search an online state database using her own name.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Air Force avoids PFAS water cleanup, citing Supreme Court's Chevron ruling (134 pts)]]></title>
            <link>https://www.theguardian.com/us-news/article/2024/aug/12/air-force-epa-water-pfas-tucson</link>
            <guid>41240300</guid>
            <pubDate>Tue, 13 Aug 2024 22:01:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/article/2024/aug/12/air-force-epa-water-pfas-tucson">https://www.theguardian.com/us-news/article/2024/aug/12/air-force-epa-water-pfas-tucson</a>, See on <a href="https://news.ycombinator.com/item?id=41240300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The US air force is refusing to comply with an order to clean drinking water it polluted in Tucson, <a href="https://www.theguardian.com/us-news/arizona" data-link-name="in body link">Arizona</a>, claiming federal regulators lack authority after the conservative-dominated <a href="https://www.theguardian.com/us-news/us-supreme-court" data-link-name="in body link">US supreme court</a> overturned the “Chevron doctrine”. Air force bases contaminated the water with toxic <a href="https://www.theguardian.com/environment/pfas" data-link-name="in body link">PFAS “forever chemicals”</a> and other dangerous compounds.</p><p>Though former US <a href="https://www.theguardian.com/environment/epa" data-link-name="in body link">Environmental Protection Agency</a> (EPA) officials and legal experts who reviewed the air force’s claim say the Chevron doctrine ruling probably would not apply to the order, the <a href="https://www.theguardian.com/us-news/us-military" data-link-name="in body link">military’s</a> claim that it would represents an early indication of how polluters will wield the controversial court decision to evade responsibility.</p><p>It appears the air force is essentially attempting to expand the scope of the court’s ruling to thwart regulatory orders not covered by the decision, said Deborah Ann Sivas, director of the Stanford University Environmental Law Clinic.</p><p>“It’s very odd,” she added. “It feels almost like an intimidation tactic, but it will be interesting to see if others take this approach and it bleeds over.”</p><p>The supreme court in late June <a href="https://www.theguardian.com/us-news/ng-interactive/2024/jun/28/us-supreme-court-chevron-doctrine-ruling" data-link-name="in body link">overturned the 40-year-old Chevron doctrine</a>, one of its most important precedents. The decision sharply cut regulators’ power by giving judges the final say in interpreting ambiguous areas of the law during rule-making. Judges previously gave deference to regulatory agency experts on such questions.</p><p>The ruling is expected to have a profound impact on the EPA’s ability to protect the public from pollution, and the Tucson dispute highlights the high stakes in such scenarios – clean drinking water and the health of hundreds of thousands of people hangs in the balance.</p><p>Several air force bases are largely responsible for trichloroethylene (TCE) – volatile organic compounds – and <a href="https://www.theguardian.com/environment/pfas" data-link-name="in body link" data-component="auto-linked-tag">PFAS</a> contaminating drinking water sources in Tucson. A 10-sq-mile (26 sq km) area around the facilities and Tucson international airport were in the 1980s designated as a Superfund site, an action reserved for the nation’s most polluted areas.</p><p>The EPA in late May <a href="https://www.epa.gov/az/tiaa-tarp" data-link-name="in body link">issued an emergency order</a> under the Safe Drinking Water Act requiring the air force to develop a plan within 60 days to address PFAS contamination in the drinking water.</p><p>Filtration systems put in place in 2014 for TCE and other chemicals are currently removing PFAS, but the systems were not designed to remove PFAS, and the added burden is straining the system.</p><p>Officials previously shut down a well in 2021 when contaminated water nearly broke through the system, the EPA wrote. Such a breach could leave the <a href="https://www.theguardian.com/us-news/arizona" data-link-name="in body link" data-component="auto-linked-tag">Arizona</a> city without safe water.</p><p>The EPA order requires the development of a system specifically designed for PFAS. A similar system is estimated to cost about $25m to develop, or about 0.1% of the air force’s annual budget.</p><p>In a formal response sent to the EPA in the weeks after the supreme court reversed Chevron, the air force claimed the EPA lacked the authority to make the order, citing the case as evidence and stating “the EPA’s order can not withstand review”.</p><p>But legal experts noted that Chevron does not affect EPA enforcement actions like the Tucson order – it only affects the rule-making process.</p><p>Moreover, one arm of the administration cannot sue another, so the military cannot sue the EPA, and the case would never end up in court where the Chevron decision would come into play, said Walter Mugdan, a former EPA Superfund director. Instead, it would be resolved internally by a presidential administration instead of the judiciary.</p><p>However, a business that finds itself in a similar position may try to sue the EPA, Sivas said.</p><p>The air force did not respond to specific questions about its Chevron claim, but said it “will continue to meet our obligations under the federal [Superfund] law”.</p><p>The crux of the rest of the air force’s refusal to comply with the order hangs on whether there is “imminent and substantial danger” to Tucson residents. The air force noted Tucson’s drinking water was currently below the federal limit for PFAS.</p><p>But the EPA noted the filtration system was straining because it was not designed for PFAS, and the plume was especially dangerous because there are limited alternative drinking water sources in the desert. The real possibility that the system breaks and leaves people exposed or without clean water constitutes “imminent and substantial” danger, the EPA wrote in its response.</p><p>“The law does not require or expect EPA to wait until people are actively being harmed,” the EPA added. “In fact, Congress expects EPA to utilize its authority to prevent the harm from materializing.”</p><p>The air force also claimed it should not be forced to address the problem because the city of Tucson and state of Arizona are funding a different system to address PFAS. In response, the EPA said the new system would not be ready for years, and the air force is financially responsible for the pollution.</p><p>“The Air Force bears responsibility for the PFAS contamination and needs to step forward to protect the Tucson community,” the EPA wrote.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust Atomics and Locks (2023) (196 pts)]]></title>
            <link>https://marabos.nl/atomics/</link>
            <guid>41239913</guid>
            <pubDate>Tue, 13 Aug 2024 21:12:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marabos.nl/atomics/">https://marabos.nl/atomics/</a>, See on <a href="https://news.ycombinator.com/item?id=41239913">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="about">
<h2>About this Book</h2>
<p>The Rust programming language is extremely well suited for concurrency, and its ecosystem has many libraries that include lots of concurrent data structures, locks, and more. But implementing those structures correctly can be difficult. Even in the most well-used libraries, memory ordering bugs are not uncommon.</p>
<p>In this practical book, Mara Bos, team lead of the Rust library team, helps Rust programmers of all levels gain a clear understanding of low-level concurrency. You’ll learn everything about atomics and memory ordering and how they're combined with basic operating system APIs to build common primitives like mutexes and condition variables. Once you’re done, you’ll have a firm grasp of how Rust’s memory model, the processor, and the role of the operating system all fit together.</p>
<p>With this guide, you’ll learn:</p>
<ul>
<li>How Rust's type system works exceptionally well for programming concurrency correctly</li>
<li>All about mutexes, condition variables, atomics, and memory ordering</li>
<li>What happens in practice with atomic operations on Intel and ARM processors</li>
<li>How locks are implemented with support from the operating system</li>
<li>How to write correct code that includes concurrency, atomics, and locks</li>
<li>How to build your own locking and synchronization primitives correctly</li>
</ul>
<p id="start"><a href="https://marabos.nl/atomics/foreword.html">Start reading</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARPA-H announces awards to develop novel technologies for precise tumor removal (128 pts)]]></title>
            <link>https://arpa-h.gov/news-and-events/arpa-h-announces-awards-develop-novel-technologies-precise-tumor-removal</link>
            <guid>41239800</guid>
            <pubDate>Tue, 13 Aug 2024 21:00:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arpa-h.gov/news-and-events/arpa-h-announces-awards-develop-novel-technologies-precise-tumor-removal">https://arpa-h.gov/news-and-events/arpa-h-announces-awards-develop-novel-technologies-precise-tumor-removal</a>, See on <a href="https://news.ycombinator.com/item?id=41239800">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
    
  








<main id="main-content">
    <div id="block-arpatheme-content">
  
  
    



        
      

            
      
  
  <article>
    
    

          <div>
        <p>Published 
  <time datetime="2024-08-13T18:46:27+00:00" title="Tuesday, August 13, 2024">August 13, 2024</time>

</p>
      </div>
        
    <div>
        <h2>Biden-Harris Administration announces ARPA-H awards to develop novel technologies for precise tumor removal &nbsp;</h2><p><em>PSI performer teams aim to deliver groundbreaking tools enabling surgeons to successfully remove tumors through a single operation</em></p><p>The Advanced Research Projects Agency for Health (<a href="https://arpa-h.gov/">ARPA-H</a>), an agency within the U.S. Department of Health and Human Services (HHS), announced the first eight teams selected by its Precision Surgical Interventions (<a href="https://arpa-h.gov/research-and-funding/programs/psi">PSI</a>) program to receive awards. The agency’s commitment is not expected to exceed $150 million to develop novel technologies that will allow surgeons to remove cancerous tumors with higher accuracy. If successful, these technologies will revolutionize surgeries, dramatically reducing rates of repeat procedures. They can also reduce instances of unintentional injury to critical structures such as nerves, blood vessels or lymph ducts. These imaging tools may also be used to improve other types of surgeries.  &nbsp;</p><p>“From the start, ARPA-H has had a singular purpose: to drive breakthroughs in health, including cancer. Revolutionizing surgical techniques is a critical step forward towards improving detection and treatment of cancer, and improving the overall patient experience in the process,” said HHS Secretary Xavier Becerra. "The Biden-Harris Administration is committed to reducing the cancer death rate by at least 50 percent over the next 25 years. This goal is becoming more and more achievable thanks to breakthrough treatments and innovative technologies like these.” &nbsp;</p><p>“With the Precision Surgical Interventions program, we're seeking to fundamentally change how surgery is done. PSI and its technical performer teams are committed to developing tools that reduce the rate of reoperations or accidental damage to critical structures,” said <a href="https://arpa-h.gov/about/people/ileana-hancu/" target="_blank">Ileana Hancu, Ph.D.</a>, ARPA-H PSI Program Manager.  &nbsp;</p><p>ARPA-H selected these awardees to develop methods and techniques to improve cancer detection and increase the visibility of critical anatomical structures during surgery. PSI will pursue two technical areas: cancer localization (technical areas 1-A and 1-B) and healthy structure localization (technical area 2).  &nbsp;</p><p>Technical area 1-A performers will focus on visualizing the surface of excised tumors and identifying if there are any cancer cells left. If so, the surgeon will be able to remove more  tissue prior to completing the surgery. The performers will use different microscopy techniques to visualize the surface of the removed tissue with sub-cellular resolution. All images will be read and classified automatically, without the need to have pathologists in the operating room: &nbsp;</p><ul><li>Tulane University will build an imaging system that uses a large aperture camera and structured illumination microscopy, an imaging technique that uses patterned light to achieve high resolution in three dimensions. It takes advantage of light wave interference patterns to image entire excised tumors. The team will also develop an AI algorithm to automatically identify cancerous cells for fast data classification. Total award up to $22.9M.&nbsp;</li><li>Rice University will build a novel microscope that images tumor slices with ultraviolet epifluorescence. They will use advanced methods to create fluorescent stains that label cells and cellular components and will develop automated AI algorithms to transform their images into ones that look similar to conventional pathology. They will also develop an automated pathology algorithm to classify the imaged cells. Total award up to $18.0M.&nbsp;</li><li>University of Washington will develop a microscopy system to allow surgeons to image the entire surface of the tumor by placing it on a lightsheet scanner. The team is also developing algorithms to pseudo-stain the resulting images, so that the sample doesn’t need to be dyed in the operating room; instead, AI methods will take a greyscale image and render it similar to conventional pathology images in order to better classify it.  Total award up to $21.1M.&nbsp;</li></ul><p>Technical area 1-B performers will focus on identifying microscopic cancer remnants inside the patient to help the surgeon remove all remaining cancer cells before the end of the procedure: &nbsp;</p><ul><li>University of California, San Francisco is inventing a microscope that uses an optical array that is pressed into the cavity’s surface. Each pixel is its own multicolor microscope. The investigators are also developing a multi-cancer dyeing agent that activates based on enzyme activity in tumors. Total award up to $15.1M.&nbsp;</li><li>University of Illinois Urbana-Champaign will develop optical coherence tomography techniques to find suspicious tissue structures in the surgical cavity, then image those regions with nonlinear optics, which will give a multilayered view of the cells’ metabolism and structural properties. Total award up to $32.6M.&nbsp;</li><li>Johns Hopkins University, which is performing on both technical area 1-B and technical area two, will develop a novel non-contact, photoacoustic endoscope to provide a more colorful view of the surgical field without altering the surgeons’ workflow. They will also develop a multi-cancer fluorescent contrast agent.  Total award up to $20.9M.&nbsp;</li></ul><p>Technical area 2 performers will focus on making critical anatomy more visible to surgeons: &nbsp;</p><ul><li>Dartmouth College is creating a laparoscope-integrating imaging solution that will be especially helpful in prostate cancer surgeries. They will use nerve-dyeing and ureter-dyeing contrast agents, in addition to vascular dyes, to cause these critical anatomical structures to fluoresce. They will then map and visualize the 3D shape and depth of the structures.  Total award up to $31.3M.&nbsp;</li><li>Johns Hopkins University will use existing fluorescent dyes in combination with their novel photoacoustic endoscope to visualize anatomical structures for surgeons. The endoscope will ‘see’ deep into human tissue to reveal hidden blood vessels and nerves, such as they are not accidentally cut. (See above)&nbsp;</li><li>Cision Vision will use shortwave infrared and hyperspectral images to help surgeons visualize blood vessels, nerves, and especially lymphatic structures. Going well beyond red, green, and blue, hyperspectral imaging is enhanced by AI algorithms. This would allow the team to distinguish between tissue types without administering dyes.  Total award up to $22.3M.&nbsp;</li></ul><p>“With PSI, we aim to reduce surgical errors significantly and achieve better health outcomes across cancer and other diseases,” said <a href="http://arpa-h.gov/about/people/renee-wegrzyn">ARPA-H Director Renee Wegrzyn, Ph.D. </a>“Surgical procedures are often the first treatment option for some two million Americans diagnosed with cancer each year. This lack of precision can lead to repeat surgeries, harder recoveries, cancer recurrence, and higher health care costs. Our hope is to advance cancer surgery so that we remove cancer the first time and every time.” &nbsp;</p><p>The PSI program mandates that all performers design solutions that are compatible with all users. For example, if designing a tool for surgeons, the tool must fit different hand sizes. PSI mandates that all performers also be committed to equitable access and the development of medical devices that will be useable in virtually any hospital. As such, PSI performers must prioritize lower-cost solutions in their designs and test their devices in a rural hospital during the program. Furthermore, the devices must be validated in patient populations that reflect the demographics of the disease studied. &nbsp;</p><p>The performers’ awards are ceilings, based on each performer meeting its contractual milestones. ARPA-H’s total investment is not expected to exceed $150M.&nbsp;</p><p>For more on PSI, visit the <a href="https://arpa-h.gov/research-and-funding/programs/psi">PSI program page</a>.  &nbsp;</p>
      </div>
  </article>

</div>

</main>

  

  </div></div>]]></description>
        </item>
    </channel>
</rss>