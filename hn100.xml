<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 01 Jun 2025 19:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How I got a Root Shell on a Credit Card Terminal (265 pts)]]></title>
            <link>https://stefan-gloor.ch/yomani-hack</link>
            <guid>44150803</guid>
            <pubDate>Sun, 01 Jun 2025 13:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stefan-gloor.ch/yomani-hack">https://stefan-gloor.ch/yomani-hack</a>, See on <a href="https://news.ycombinator.com/item?id=44150803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>In this project, I started to reverse engineer payment card
        terminals because they seemed to be an interesting target for security
        research, given the high stakes involved. Although I initially didn’t
        knew much about this industry, I did expect a ton of security features
        and a very security-hardened device. And to some degree, this was also
        correct.</p>
        <h2>First Look</h2>
        <p>The model I went with is a Worldline Yomani XR terminal. Although it
        seems to be discontinued at the time of writing, this is the model that
        is <i>everywhere</i> in Switzerland. From big grocery chains to the
        small repair shop on the corner, everyone has one or a whole fleet of
        this exact terminal.</p>
        <p>After booting it up and aimlessly clicking through the UI, I did a
        quick port scan, but couldn’t find anything interesting. So naturally,
        I started to take it apart.</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_pcb.jpg"><img src="https://stefan-gloor.ch/img/yomani_pcb.jpg" width="100%" alt="Picture of the main PCB of the Yomani XR terminal with annotation of different board components."></a>
          </figure>
        </div>
        <p>The housing and the PCBs appear to be well-made. The design consists
        of multiple PCBs: a small connector board for the outward-facing
        connectors, the main board, and a vertical board the card slot sits on.
        The main SoC seems to be a custom ASIC, a dual-core Arm processor
        code-named “Samoa II” in the firmware, but I am jumping ahead.
        According to Worldline documentation, this seems to indeed be a custom
        ASIC, rather than just a rebranded off-the-shelf chip. Next to it,
        there is a small external flash and RAM.</p>
        <h2>Tamper Protections</h2>
        <p>During disassembly, I kept looking for a tamper switch that would
        detect when the device’s housing was opened, like I had seen previously
        on laptops and other devices. However, I couldn’t find such a switch.
        Rather, they use the board-to-board interconnects as a way of detecting
        when the device is opened. Because they use relatively
        pressure-sensitive Zebra strips between the boards, they have to be
        tightly screwed together. Even unscrewing some of the screws is enough
        to break contact and trigger a tamper event. Of course, the tamper
        detection must also work when the power is disconnected, so that’s the
        purpose of the coin cell battery.</p>
        <p>But this is not all in terms of tamper resistance: The “vulnerable”
        PCBs are covered by zick-zack traces that act as a tamper detection
        mechanism. Accidentally breaking a single copper trace during a
        physical penetration attempt (hole drill etc.) is enough to trigger the
        tamper detection.</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_mesh2.jpg"><img src="https://stefan-gloor.ch/img/yomani_mesh2.jpg" width="100%" alt="PCB showing a zick-zack line pattern across the whole board, acting as a temper detection mechanism."></a>
            <figcaption>
              Trace meander on the display PCB acting as tamper detection.
            </figcaption>
          </figure>
        </div>
        <p>The card slot itself is contained in an additional, internal
        housing. Wrapped around this housing, there is a flex PCB acting as
        tamper protection.</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_mesh.jpg"><img src="https://stefan-gloor.ch/img/yomani_mesh.jpg" width="100%" alt="Inner plastic housing is wrapped in a flex PCB with zick zack traces, acting as a temper detection mechanism."></a>
            <figcaption>
              Card reader assembly wrapped in a tamper detection flex PCB.
            </figcaption>
          </figure>
        </div>
        <p>After putting the device back together, it was clear that my
        intrusion had not gone undetected. Sure enough, the device would now
        only show a big red screen screaming “TAMPER DETECTED”. In this mode,
        the device seems to be fully unresponsive to any kind of external
        input. So, game over?</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_tampered.jpg"><img src="https://stefan-gloor.ch/img/yomani_tampered.jpg" width="100%" alt="Yomani Credit Card terminal showing an all-red screen saying OUT OF ORDER, TAMPERED STATE, no key loaded."></a>
          </figure>
        </div>
        <h2>Chip-Off Firmware Extraction</h2>
        <p>Since any “runtime” exploration seems to be futile now, I wanted to
        take a look a the firmware. For this, I desoldered the on-board flash
        chip, soldered some wires to it in “dead bug” style and proceeded to
        dump the contents.</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_chipoff.jpg"><img src="https://stefan-gloor.ch/img/yomani_chipoff.jpg" width="100%" alt="Flash chip in BGA housing mounted upside-down on a perfboard. Thin copper wires are connected from the chip’s contacts to a flash reader."></a>
            <figcaption>
              BGA flash chip of the card terminal desoldered and connected to a
              flash reader.
            </figcaption>
          </figure>
        </div>
        <p>To my surprise, the contents seemed to be entirely unencrypted!
        However, the contents seemed to have a strange ECC layout. Instead of
        following the flash chip’s inherent page layout and putting the ECC
        data in the interleaved spare areas, it appeared as if the some ECC
        data was present in the page and some clear text was in the spare
        areas. With the help of some friends, I figured out the layout: instead
        of using the standard 2048 byte payload + 64 byte ECC/spare layout, it
        uses 3x 694 byte chunks of data each followed by 10 ECC bytes. The ECC
        bytes do not appear to be all used. Instead, the last 16 bytes of the
        spare area seem to act as metadata for the YAFFS2 file system. Now
        normally, there are less ECC bytes per page and therefore the usable
        metadata area for the filesystem is larger than 16 bytes. For this
        reason, the YAFFS2 filesystem had to be patched to work with smaller
        metadata structures.</p>
        <p>I implemented a compatible filesystem reader and with it, I could
        successfully dump the filesystem contents!</p>
        
        <p>Now it was clear that this device runs Linux. I found a standard
        Linux filesystem with a lot of interesting files to browse through. The
        system runs a 3.6 kernel, built with Buildroot 2010.02 (!) in February
        of 2023. The system seems to use a custom bootloader, “Booter v1.7”.
        Although I don’t know how recent the firware version was that I ended
        up dumping, it must have been released after February 2023. So finding
        such an ancient kernel is rather concerning. Userspace-wise, the system
        uses classy init scripts, busybox, and uClibc (final release was 13
        years ago). libcrypt has version 0.9.26, ouch.</p>
        <h2>Finding a Root Shell on Accident</h2>
        <p>Having seen the firmware gave me some new confidence that there
        might be more stuff to find. So I reattached the flash chip using
        wires, ignoring everything I know about signal integrity. To my
        surprise the device booted up again (showing the tamper message).</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_flash_attached.jpg"><img src="https://stefan-gloor.ch/img/yomani_flash_attached.jpg" width="100%" alt="BGA flash chip attached to PCB using wires"></a>
            <figcaption>
              Flash chip reunited with the PCB, as good as new.
            </figcaption>
          </figure>
        </div>
        <p>My next goal was it to find the serial console of the Linux system.
        Surely, it must have one for debugging. By looking at a boot log, I
        might get some more valuable information. So, equipped with a logic
        analyzer, I started poking.</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_probing.jpg"><img src="https://stefan-gloor.ch/img/yomani_probing.jpg" width="100%" alt="PCB with needle probes attached to a logic analyzer"></a>
            <figcaption>
              Probing the debug connector.
            </figcaption>
          </figure>
        </div>
        <p>I didn’t take long until I found some activity on one pad of the
        unpopulated debug connector. Bingo!</p>
        <pre>------------------------------------------------------------------------
Booter: 1.7b+00002:gbe6b338 Jun  3 2014 08:51:58 owi
Reset reason: Tamper
Start USB boot ...
Got address 0x00000015
Enumerated.
Dfu timeout
yaffs: checkpoint restore ... KO!
yaffs: clean up the mess caused by an aborted checkpoint
file "hwinfo-l0" found
file "hwinfo-l1" found
file "hwinfo-l2" found
file "loadercode" found
file "mp1.img" found
file "linux" found
Uncompressing Linux... done, booting the kernel.
Linux version 3.6.0-samoa-01844-g1f05798 (ppd@debian) (gcc version 4.3.4 (Buildroot 2010.02) ) #0 Fri, 10 Feb 2023 16:26:07 +0100
cpufreq: initial frequency: 264000 kHz
MAC-1G DMA: 430ab000 - 430ab9ff
MAC addr = 00:08:19:4e:56:2c
eth0: ioaddr: d00d0000, dev: c30ac000
probing samoafb: rc = 0
  DMA = 4F500000-&gt;c3a00000, IO =   (null)
UART 1 probing
UART 1 probing OK
UART 2 probing
UART 2 probing OK
UART 3 probing
UART 3 probing OK
pca953x 0-0049: failed reading register
ba315 ba315.0: NAND ID: id[0-3]=’EF A1 00 95’, manu=’Winbond’, dev=’NAND 128MiB 1,8V 8-bit’
drivers/rtc/hctosys.c: unable to open rtc device (rtc0)
yaffs: Attempting MTD mount of 31.0,"mtdblock0"
yaffs: yaffs_read_super: is_checkpointed 0
starting pid 398, tty ’’: ’/etc/init.d/rcS’
/etc/init.d/rcS started
Mounting local file systems: ok
rootfs on / type rootfs (rw)
/dev/root on / type yaffs2 (rw,relatime)
proc on /proc type proc (rw,relatime)
devpts on /dev/pts type devpts (rw,relatime,mode=600)
tmpfs on /tmp type tmpfs (rw,relatime)
none on /sys type sysfs (rw,relatime)
Mounting usbfs: failed
Terminal is initializing\nPlease wait
...
dropbear is not present
emv-engine-4.3e-with-trace is not present
tim-server-nexo-config_3.1.6.0-1140_arm is not present
mp1-samoa2-eft-sr-prod is not present
Checking for FW updates\nPlease wait

 FW is up to date.

Starting Application Monitoring Daemons
Mounting USB stick: failed
starting pid 600, tty ’/dev/ttyCU’: ’/sbin/getty 115200 ttyCU’

samoa login: 
</pre>
        <p>It even shows a login prompt, neat! So this is definitely the Linux
        boot log. Many embedded Linux systems will have such a more or less
        exposed serial console, but most of the time the login is disabled
        altogether or a random, hard-to-crack password is either hard-coded or
        generated at boot. Not expecting much, just out of curiosity, I entered
        “root” as the login, and...</p>
        <pre>samoa login: root
~ #
</pre>
        <p>Wait, what!? That’s it? I’m in?</p>
        <p>Well, yes. That was the boring story of how I got the root shell. It
        is just there, exposed. No sophisticated exploit chain, no brute-force
        password cracking required. Additionally, everything seems to still
        work although the device still shows the tamper message.</p>
        <p>Now you may say: well okay, that’s not ideal, but in order to get
        access to the exposed root shell, you need to open up the device, which
        triggers the tamper protection and effectively renders the device
        useless. But in fact, the <b>serial port is accessible from the
        outside!</b>. Without opening up the device and triggering the tamper
        protection, the debug connector can be accessed through a little hatch
        on the back of the device. All an attacker needs is just 30 seconds
        alone with the device to attach to the serial port, log in, deploy
        malware, and leave. This sounds really bad.</p>
        <div>
          <figure>
            <a href="https://stefan-gloor.ch/img/yomani_uart_conn.jpg"><img src="https://stefan-gloor.ch/img/yomani_uart_conn.jpg" width="100%" alt="PCB showing 3 wires attached with labels, RX, TX, and GND."></a>
            <figcaption>
              The debug connector with the serial console is accessible from
              the outside of the device.
            </figcaption>
          </figure>
          <h2>Is This as Bad as It Looks?</h2>
          <div>
            <figure>
              <a href="https://stefan-gloor.ch/img/yomani_meme.jpg"><img src="https://stefan-gloor.ch/img/yomani_meme.jpg" width="100%" alt="Two-panel meme of freight train hitting a school bus. Bus is standing on the tracks labelled ’extensive hardware tamper protection features’, in the second panel, freight train labelled ’exposed serial port with root shell’ rams the bus away."></a>
              <figcaption>
                All this fancy hardware made useless by some lazy software
                engineers?
              </figcaption>
            </figure>
          </div>
          <p>Actually, no. Hear me out: During my analysis of this device, it
          became clear that the Linux system is not the whole story. For
          example, there seems to be no graphics driver for the display and the
          only framebuffer device does not seem to do anything. Instead, only
          text strings seem to be passed to a binary
          (<code>display_tool</code>), that issues some inter-processor
          messages. The same goes for the key pad or the card reader itself. I
          could not find any evidence that these peripherals could be accessed
          directly from Linux.</p>
          <p>Instead, there is an entirely separate processor, refered to as
          <code>mp1</code>, that seems to handle all the “secure” stuff, like
          handling the card, getting the pin and showing information on the
          screen. The “insecure” Linux, running on the second processor,
          <code>mp2</code>, only handles the networking, the updating, and the
          business logic.</p>
          <p>The Linux core seems to always boot, regardless of the tamper
          state. From there, the secure core is booted. For this, Linux
          apparently loads a secure bootloader (named <code>loadercode</code>)
          into memory. This loadercode checks whether the tamper protections
          have been triggered and based on the result, either show the red
          screen or continue to boot the actual “secure” image
          (<code>mp1.img</code>, in the Linux filesystem). This secure image
          seems to be properly encrypted and signed by two entities.</p>
          <div>
            <figure>
              <a href="https://stefan-gloor.ch/img/yomani_diagram.jpg"><img src="https://stefan-gloor.ch/img/yomani_diagram.jpg" width="100%" alt="Diagram showing the presumed boot process of the Yomani device. Linux always boots on mp2, which is the insecure application core. After that, it will take loadercode and mp1.img from the filesystem and boot it on the other, secure core."></a>
              <figcaption>
                My current understanding of the Yomani boot process featuring
                the Samoa II ASIC. The first core (insecure, application)
                always boots Linux, which in turn loads a secure bootloader and
                the secure image on the second core. The loadercode is what
                displays the tamper messages on screen. The secure image, that
                handles the card, display, and key pad, is properly encrypted
                and signed.
              </figcaption>
            </figure>
          </div>
          <h2>Disclosure Timeline</h2>
          <table>
            <tbody><tr>
              <th scope="col"></th>
              <th scope="col"></th>
            </tr>
            <tr>
              <td>14/11/2024</td>
              <td>Discovered the root shell</td>
            </tr>
            <tr>
              <td>15/11/2024</td>
              <td>Informed manufacturer, announced 90 days until
              publication</td>
            </tr>
            <tr>
              <td>18/11/2024</td>
              <td>Manufacturer confirms that they got the report</td>
            </tr>
            <tr>
              <td></td>
              <td>--- I forgot this project existed ---</td>
            </tr>
            <tr>
              <td>01/06/2025</td>
              <td>Publication</td>
            </tr>
          </tbody></table>
          <h2>Conclusion</h2>
          <p>While concerning, the exposed root shell does not seem to be as
          big of a risk as initially feared.
			While still being a huge unnecessary attack surface, and a massive oversight from the engineers in my opinion,
			I could not find any evidence that sensitive data, such as card details, could become compromised this way.
			Also, I could not definitively proof which firmware versions were vulnerable. During my research, I also encountered devices that
			had a disabled root login. My guess is that somehow this debug feature accidentally made it into production firmware at some point.
			Chances are that this was discovered and fixed internally before I reported it to the manufacturer (as I had no way of updating the firmware,
			or verifying that I was running the latest firmware).</p>
			<p>
			This project was a lot of fun and I wish I had some more time to dig deeper into the firmware and explore more possibilities.
			If you want to take on this challenge, feel free to reach out to me.
			I’d like to thank anyone involved in this project for their valuable input.
		</p>
        </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atari Means Business with the Mega ST (115 pts)]]></title>
            <link>https://www.goto10retro.com/p/atari-means-business-with-the-mega</link>
            <guid>44150002</guid>
            <pubDate>Sun, 01 Jun 2025 11:01:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.goto10retro.com/p/atari-means-business-with-the-mega">https://www.goto10retro.com/p/atari-means-business-with-the-mega</a>, See on <a href="https://news.ycombinator.com/item?id=44150002">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Announced at </span><a href="https://www.latimes.com/archives/la-xpm-1987-01-12-fi-2966-story.html" rel="">CES in January 1987</a><span>, the Mega ST was Atari’s first attempt at creating full-fledged workstation for professional users.</span></p><blockquote><p>There is some conflict as to whether these are called Mega STs or just Megas. I’ve seen both names used and even have seen labels with both names. I tend to use Mega ST as it is much clearer and sounds less like “Amiga”.</p></blockquote><p>By this point, the timeline for the ST was:</p><ul><li><p>1985: Original 520ST</p></li><li><p>1986: 1040ST with 1MB of RAM</p></li><li><p>1987: Mega ST</p></li></ul><p>It makes some sense that Atari would want to take the ST upmarket. Both the 520ST and 1040ST were largely mass-market product and sold with a thin profit margin. In order to continue to grow as a company, a product with a bigger profit margin was definitely needed.</p><p>Unfortunately the Mega ST had some issues with both its design and its market timing that hurt its chances. Let’s look at the design first.</p><p>The Mega ST takes the standard ST and puts it into a low-profile “pizza-box” case with an internal double-sided (720K) floppy drive and a detachable keyboard.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg" width="960" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:960,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;File:Atari Mega ST 1.jpg&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="File:Atari Mega ST 1.jpg" title="File:Atari Mega ST 1.jpg" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42c7167b-d0c9-4160-8309-3fb99413f195_960x640.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The keyboard was actually rather nice with cherry key switches. Sadly it retained the rather large key sizes of the earlier ST models, making typos too easy. It also had the connection for the mouse and joystick under the keyboard, which kept things neat and tidy but was a major pain to connect in the first place. The low profile of the case also meant that the disk drive slot would bump into the keyboard if the two were close together.</p><p><span>Overall, I’d say the keyboard is a solid B+ and it is a sought-after keyboard today for its reliability. This keyboard also </span><a href="https://www.goto10retro.com/p/10-secrets-of-the-atari-mega-ste" rel="">works great with the Mega STE</a><span> and I’d love to get my hands on one.</span></p><p>Other case differences include a built-in fan and a built-in battery-backed clock that ran on simple AA batteries. The fan was probably not really needed, but it likely improved reliability.</p><p>As far as technical specifications, the Mega ST did not have many advantages over a standard 1040ST. Its main improvement was that it had a blitter chip that could move data in memory around fast, which was great for graphics processing.  The Mega ST was the first to include TOS 1.02, which was the first update to TOS in ROM and included several bug fixes.</p><p><span>Perhaps the biggest “innovation” was the RAM. The Mega ST was available with 2MB or 4MB of RAM</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-159691277" href="https://www.goto10retro.com/p/atari-means-business-with-the-mega#footnote-1-159691277" target="_self" rel="">1</a></span><span>, an enormous amount in 1987. This much RAM was a game-changer for developers and desktop publishing.</span></p><p>Lastly there was an internal bus connector slot, which ended up not really being all that important. The only thing I know that used it were some hard drive kits that would provide a way to squeeze the hard drive inside the Mega ST case. This points out a problem in the case design: it was just too short and didn’t have much room for anything else to be in there.</p><p>It would have made sense for Atari to sell an internal hard drive upgrade for the Mega ST, but instead they sold a separate external hard drive, the MegaFile, that you could place under the Mega ST. Modular perhaps, but more expensive and messier than an internal solution that would also not have external cables or a power supply.</p><p>By far the biggest technical limitation was that the Mega ST ran at the same speed at the 520ST and 1040ST: 8Mhz. The blitter certainly could speed up the display operations of some apps, but the Mega ST basically ran everything at the same speed as the older and much cheaper STs.</p><p>The Mega ST was often promoted in combination with the SLM804 Laser Printer as a complete desktop publishing solution for just $3000 to $4000. At the time this was touted as a full system at the same price as just the laser printer for a Macintosh.</p><p>I don’t recall seeing Atari specifically market the Mega ST to developers, but I suspect a lot of developers found the better keyboard and extra RAM to be worth the upgrade.</p><p><span>In general, I think Atari had a tough time selling such an expensive product</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-159691277" href="https://www.goto10retro.com/p/atari-means-business-with-the-mega#footnote-2-159691277" target="_self" rel="">2</a></span><span>. Even in 1987, the Atari brand was closely associated with video games and inexpensive computers. Trying to use the Atari brand for more expensive computers aimed at businesses was always going to be difficult. After all, many business would definitely balk at having the Atari label on their desks.</span></p><p>Although I never owned a Mega ST, my Dad did have a Mega ST 2. Since he also had the SC1224 color monitor, I would sometimes use it to play games. I remember hogging his Mega ST to play Dungeon Master quite often. I also remember that the keyboard was really nice and miles better than the one on my 1040ST. He also had an external hard drive, which he was able to place under the Mega ST.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg" width="1042" height="754" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/af783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:754,&quot;width&quot;:1042,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:221734,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.goto10retro.com/i/159691277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf783d8d-e65e-4c39-a797-65bc36c43eab_1042x754.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>My Dad’s Mega ST2 setup from the early 90s</figcaption></figure></div><p>The Mega ST was not a huge success for Atari. Although it was announced in early 1987, systems did not start shipping until late 1987 with most not shipping until 1988. The problem was the extra RAM. The Mega ST used 1Mbit DRAM, which actually had a significant price increase and shortage due to tariffs on overseas DRAM. This pricing problem affected the computer industry as a whole, but definitely hit Atari hard with its low margins.</p><p>It was about this time that Atari seemed to focus more on the European market, sending more product there instead of the US.</p><p><span>The Mega ST was eventually replaced by the </span><a href="https://www.goto10retro.com/p/finding-a-mega-ste" rel="">Mega STE</a><span> in 1991/1992. Looking back, I think everyone would have wanted to see something like the Mega STE in 1988, but that just wasn’t meant to be.</span></p><p>The Mega ST takes up a lot of space on a desk, is not easily upgradable and is no faster than a standard 1040ST. </p><p><span>I have a </span><a href="https://www.goto10retro.com/p/finding-a-mega-ste" rel="">Mega STE</a><span> and I prefer it in every way over a Mega ST, except perhaps the keyboard. So other than the keyboard, I don’t think Mega ST computer are in high demand today.</span></p><p><span>For more about the Mega ST, check out the review in </span><a href="https://archive.org/details/ST_Log_Magazine_Issue_18/page/n66/mode/1up" rel="">ST Log #18 (April 1988)</a><span>:</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cinematography of "Andor" (208 pts)]]></title>
            <link>https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html</link>
            <guid>44149718</guid>
            <pubDate>Sun, 01 Jun 2025 09:44:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html">https://www.pushing-pixels.org/2025/05/20/cinematography-of-andor-interview-with-christophe-nuyens.html</a>, See on <a href="https://news.ycombinator.com/item?id=44149718">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		        	<div id="hero"><p><img src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still01.jpg"></p>
                 
		        	<p><span>Cinematography of "Andor" by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
		        	</div>		        	<p><span>Cinematography of "Andor" by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p><p>Continuing the ongoing series of interviews with creative artists working on various aspects of movie and TV productions, it is my pleasure to welcome <a href="https://www.christophenuyens.com/"><strong>Christophe Nuyens</strong></a>. In this interview, he talks about the transition of this creative field from film to digital, bridging the gap between feature films and episodic productions, learning from different cultures, and what advice he’d give to his younger self. Between all these and more, Christophe dives deep into his work on the second season of “Andor”.</p>
<p><img fetchpriority="high" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets01.jpg" alt="" width="3200" height="2133"><br>
<span>Christophe Nuyens on the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Please tell us about yourself and the path that took you to where you are today.</em></p>
<p><span>Christophe</span>: I finished a trade school as a general electrician, but I wanted to do something more, and I went to film school. During your first year you can choose between editing, sound and image – which is light and camera. So we had our first workshop, and I had the camera in my hands, and I knew this was it. I really loved the mix of technical and creative.</p>
<p><em><span>Kirill</span>: Do you feel that you can teach the technical part, but the artistic part comes from within a person, and if one doesn’t have it, it can’t be learned?</em></p>
<p><span>Christophe</span>: No, I think you can teach both. When I was growing up, I didn’t have a lot of cultural influences in my life, at home or at school. It is something that I grew over the years. When I started at the film school, I noticed that I needed to catch up on it. I spent a lot of evenings around that time watching movies with my friends, and it grew on me.</p>
<p>You can cultivate it the same way you cultivate the technical skills. There are also people who are more artistic than technical. Maybe I am more naturally inclined to be better at the technical side, but I grew and worked on my creative side over the years. I really believe you can grow the creative part of your brain.</p>
<p><em><span>Kirill</span>: Is there such a thing as universally good art vs universally bad art, or is it all subjective?</em></p>
<p><span>Christophe</span>: It’s subjective. There’s so many forms and styles of art. And that is good, because there’s something for everybody. Everything can be art, and people with different taste can find things that they appreciate.</p>
<p><img decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets03.jpg" alt="" width="3200" height="2133"><br>
<span>Christophe Nuyens on the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Was film still a thing when you were in film school?</em></p>
<p><span>Christophe</span>: We did most of our projects on 16mm, either Bolex or Arriflex SR2. We did a few things on video, but it was really basic at the time. I remember those assignments to film something and edit it ourselves, and it was a nightmare. The computers were slow, the Video cards didn’t work, the software was basic. It’s incredible to see how all of that progressed since then. These days I teach at that same school, and the difference is night and day. They can edit it in DaVinci, they can grade it, and it’s so accessible. Sometimes I’m a bit jealous to see that [laughs].</p>
<p><em><span>Kirill</span>: How was the transition from film to digital for you after you finished the film school?</em></p>
<p><span>Christophe</span>: When I graduated, most of the productions were still on film. I was exposed to both mediums, and I’m happy about it. I know how to light for film. I still have an analog still camera, and I use it a lot.</p>
<p>But at the same time, I’m so happy that the digital revolution happened. It’s a bigger toolbox for your creativity, especially for night scenes. It’s much easier to light something natural, and to do something with less. I started my career in Belgium, and it’s a smaller market with smaller budgets for TV shows and films – but you still want to make good things. I did a TV show called “Cordon” about 10 years ago. It was an ambitious project for its small budget, and that project started my international career. I don’t think it would have been possible to make that project on film. We had a lot of night scenes on it, and it’s so much different to light a night scene on a digital camera.</p>
<p><img decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still06.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>

<p><em><span>Kirill</span>: There’s a lot of ongoing technological advances in the field, from lights to lenses to sensors. Is it hard to stay up to date on all the latest developments?</em></p>
<p><span>Christophe</span>: Not for me, because that’s my technical side that I’m interested in. I follow everything, and if there’s something new, I want to test it. But at the same time, I’m a fan of old glass. I like to dig into old equipment and to test new equipment.</p>
<p>Right now there’s a lot of interesting things happening with lights. Camera sensors are getting bigger and we’re getting more pixels, but I don’t think it matters that much. And the camera sensibility has been at a good level for a while now. The latest breakthroughs are all in the LED light space. I used a lot of LED lights on “Andor”. All the lights are RGBW (red, green, blue, white), and you can choose any color you want. There’s someone next to you controlling those lights on their iPad, and you’re almost painting the scene with these controllable lights. You can control the color of each one, you can control the intensity of each one, and you can do it all in real time.</p>
<p>When I graduated, it was all with gel filters, tungsten lights and HMIs. Those lights were shifting their color as they aged, and it was a more time consuming process to tweak the colors. Now you have such fine grained control over LEDs, and it’s the biggest positive change for me in the last few years. Sometimes I still use tungsten lights, but my first preference is LED.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still12.jpg" alt="" width="3840" height="1588"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Is there anything today that is a big gap on the technology side?</em></p>
<p><span>Christophe</span>: Right now everything is going wireless. Video is wireless. Lights are wireless. Sound is wireless. It’s all good, but there’s a lot of congestion on sets with all those things combined. Sometimes those nice tools don’t work because there’s too much technology on set [laughs]. That’s my only complaint.</p>
<p><em><span>Kirill</span>: Getting closer to “Andor”, you’ve spent around 15 years now working on various episodic productions. How do you see audience expectations and production ambitions evolving over that period of time? I look back at how it was in the ’90s, where we had the feature world and the TV world, and there was an almost unbridgeable gap between the two. And now in 2025 that gap is pretty much gone.</em></p>
<p><span>Christophe</span>: It’s nice that you say that, because it’s something I fought against for years. Up until about 5 years ago people would say that if you do TV shows, you can’t do feature film work. If you wanted to do something creative on a show, with lights or with a camera, they would push back on it. They would say that it’s a TV show and that it doesn’t need that.</p>
<p>Right now, the audiences watching episodic shows accept and expect a higher level of quality. It’s such a good transition. There are TV shows that look better than films. I worked on some French TV shows where they gave us a lot of freedom even as the budget was not that big, and I’m so happy with what we were able to do there. I’ve been fighting against it for a long time in discussions with agents. They used to push me to go into the feature film world, and I’m happy with where these episodic productions are these days.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets02.jpg" alt="" width="3200" height="2133"><br>
<span>On the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Getting to “Andor”, what brought you to it?</em></p>
<p><span>Christophe</span>: Thanks to David Meanti who is a producer on “Andor”. I knew him from the show “Riviera” that we did a few years ago in the south of France. He was an assistant director on that show, and we had a great working relationship. We kept in touch after that show, as he moved to the UK and started working as a producer. He tried to introduce me to the producers on the first season of “Andor”, but it didn’t work back then. But he kept on trying for the second season, and I’m grateful to David for that. At first I was offered the first 3 episodes, and after that it was extended to the next 3 episodes.</p>
<p><em><span>Kirill</span>: How did you approach bridging this arc between Season 1 and “Rogue One”?</em></p>
<p><span>Christophe</span>: “Rogue One” is one of the best Star Wars films, and because of that I was so happy when they called me to work on “Andor”. “Rogue One” has a great story and great visuals.</p>
<p>I wanted to elevate “Andor”. The first season was shot on Panavision C lenses on VENICE camera with a cropped sensor. I wanted to use a full frame sensor with a full frame anamorphic lens to get a bit closer to “Rogue One” which was shot on Alexa 65 with anamorphic lenses. A bigger sensor gives you a different feeling, and you see it when you watch a movie in IMAX. With lighting, I wanted to have a natural approach to it.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still08.jpg" alt="" width="3840" height="1592"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How much time did you have in pre-production to talk about ideas, visuals and inspirations?</em></p>
<p><span>Christophe</span>: We had a lot of time, and it’s a rare thing. The director Ariel Kleiman and I went through the same process for each episode. We were reading the scripts together, and throwing ideas and brainstorming. We did that twice for each episode, and then we started making moodboards. After that we did another read through, and then we started blocking the scenes. We had a lot of 3D pre-viz with ILM, with our camera and lenses in those virtual sets. That allowed us to start looking for shots and to refine everything.</p>
<p><em><span>Kirill</span>: How difficult is it to stay practical, to capture as much as you can in camera on sets that are literally out of this world?</em></p>
<p><span>Christophe</span>: On some sets it was more difficult, especially the ones with a lot of green screens. You have to imagine what’s behind as you’re lighting it and trying to see through it. There are limitations when working with green screens. You can’t use too much smoke or haze. You can’t use flares. It becomes less natural for me.</p>
<p>This is why for some sets we used either LED walls or painted backdrops. The wedding scene in episode 3 was time-lapsed. Every time we come back to the wedding, the light was slightly different as the Sun was getting lower and we were getting atmospheric effects. We wanted to create a feeling of an estate with the views on the mountains. Eventually we decided to not use green screens. They painted a nice backdrop of the mountains we used in Barcelona, and that was great. You see the final result in camera, and you can light it more naturally. Another set where we used painted backdrops was when Krennic gives his speech to take control of Ghorman. We had a painted backdrop of snowy mountains, and it worked well.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still04.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How was your collaboration with VFX? Was it mainly in post-production, or an ongoing process?</em></p>
<p><span>Christophe</span>: It was ongoing. We were all in one office, including the art department and VFX. We had a lot of meetings to discuss how this planet should look, what we can do in pre-viz, etc. We had full VFX shots of the TIE fighter flying integrated into the larger sequences. Mohen Leo was the VFX supervisor, and he wanted every VFX shot to be physically grounded. You see that in every VFX shot, including the lighting. Some shots started full VFX and then became sets. Luke Hull, our production designer, was always in those meetings. It was a close collaboration between Ariel the director, myself, the art department and the VFX crew from ILM.</p>
<p><em><span>Kirill</span>: In terms of sets, how much was built on stage and how much was built on location?</em></p>
<p><span>Christophe</span>: There was a lot built on the stage and back lots at Pinewood and Longcross. Yavin, for example, was all at Longcross. It’s an old military test track, with some buildings and a lot of forest. We used that forest to build Yavin’s landing areas for the fighters. The Ghorman city and plaza were built on a back lot at Pinewood, including most of the interiors.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still09.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How much time did you spend shooting the wedding ceremony?</em></p>
<p><span>Christophe</span>: It was about a week and a half, but we also had a set strike as we were getting close to the finish line. The last bit when they all start dancing like crazy was filmed six months later.</p>
<p><em><span>Kirill</span>: How many countries did this show take you to?</em></p>
<p><span>Christophe</span>: We were in Barcelona where we shot the Coruscant Senate, and in London where we shot at Barbican and Lloyd’s building.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still02.jpg" alt="" width="3840" height="1610"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: For some of the bigger sets, is it a combination of a physical build and then digital extensions?</em></p>
<p><span>Christophe</span>: Some of them were fully physical, and some were extended. The Plaza at Ghorman was built physically as one story, and everything above that is a digital extension. I remember how astonished I was the first time I arrived on that set. It was so big. Another example of combining the two was the Ghorman heist sequence. They built quite a bit of those streets, and then above it we had green screens for extensions. The VFX crew was so good at making extensions look so natural.</p>
<p>We had a great collaboration on this show. We knew how we wanted these places to look before we started shooting. I could plan my lighting ahead of time, including the VFX pieces.</p>
<p><em><span>Kirill</span>: The show spends a lot of time at the apartment with Bix. How big was it?</em></p>
<p><span>Christophe</span>: The nice thing about that set is that everything you see is captured in camera, including the view outside which was an LED wall. That wall allowed us to be more creative and find some extra shots. You have shots from the outside where you see the city below in reflection with raindrops on the window. You see cars passing and light coming in.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still07.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: Did you want to use different colors for different planets or different sets?</em></p>
<p><span>Christophe</span>: We wanted every arc of three episodes to feel like its own movie. Our first block was episodes 4 through 6, and we wanted that block to feel cold. Our visual reference for it was the city of Turin in Italy, which is a mountain city. We wanted the feeling of that winter light when the Sun is already behind the mountains, but there’s still light, and it’s all blue. And if the Sun is out, it should be low, with a warm magenta feel to it. We have a lot of rain in these three episodes as well. And then when we shot the next block of the first three episodes, we wanted it to be sunny. It all has a summer feel, with lots of sunlight, including Dedra’s apartment.</p>
<p>Then, within each block you have different planets. Some places are more cold with not a lot of colors. The wedding had a more classical look, and I used tungsten lights there to create that look. Yavin feels like an old Star Wars movie. It was such a nice exercise to play with different looks and moods. It’s a gift to be able to do that.</p>
<p><em><span>Kirill</span>: Was there any color you wanted to stay away from?</em></p>
<p><span>Christope</span>: Yes for certain sets, but not for the whole show. We used a mix of cold moonlight and warm sources on Yavin for consistency, for example.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still10.jpg" alt="" width="3840" height="1608"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: What was your most memorable moment, and what was your most challenging moment on this show?</em></p>
<p><span>Christophe</span>: The most challenging was Mina-Rau and its grain fields. They planted the grain fields, and we had a window of 3-4 weeks to shoot it. And then, just before we started shooting, we had a set strike and we lost half of our actors. We decided to keep on shooting with non-SAG actors, but pretty much every scene was a mix of SAG and non-SAG. So we started shooting bits and pieces of the scenes, also fighting with the elements [laughs], and then we completed the reverse shots six months later, on a stage during winter time.</p>
<p>Our greens department was cutting all those grain stalks to put on stage, but my main challenge was to keep the lighting consistent and natural. Back when we were shooting in the field, I was already thinking about what would happen later on. I was really meticulous, measuring the light of the field, the light of the Sun, the light of the clouds, and noting all the color temperatures. Then when we came to that stage, we had lots of LED fixtures on the ceiling, and we were able to match those color temperatures everywhere. Another interesting challenge was to recreate the feeling of the air with all of the field particles.</p>
<p>It’s easy when you have one scene outside and then another on the stage, but here we were matching shots within the same scenes. I like those kinds of challenges.</p>
<p>As for the most memorable scene, I loved shooting Yavin. I like being outside, and it also reminded me of the old Star Wars movies. Another one was the heist on the streets of Ghorman. We spent about four weeks, mostly during the night, in the UK winter. It was freezing. We had rain machines. It was hard, but it paid off in the end. It was also my first big set to light. It was a big challenge, and I was so happy with the end result.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still11.jpg" alt="" width="3779" height="1606"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: How does it feel to see these weeks and months of work compressed into a few hours on the screen?</em></p>
<p><span>Christophe</span>: Before the episode is shown to the public, I see multiple cuts of it. That’s when I see all the faults. That’s when I think of all the things that I could have done better. And then some months later it hits the streaming network, and now I can see it from a distance – and it’s really nice to see it in its final form.</p>
<p><em><span>Kirill</span>: How was Covid for you and how is it today?</em></p>
<p><span>Christophe</span>: I’m so happy that it’s all behind me. We still had Covid restrictions when we started working on this season. It was a lot of testing, and it was cumbersome for the entire production. Also, when you work in a mask, it takes away a big part of the human interaction on set. I want to see people that I work with. When you talk with someone and you have a mask on, you lose half of the conversation. From the moment we could work without a mask, it really opened up. Everything became more relaxed. Communication became easier and warmer. It was hard for me to not have the same human interaction with the restrictions in place.</p>
<p><em><span>Kirill</span>: What are your top three favorite movies of all time?</em></p>
<p><span>Christophe</span>: One of my favorites is “Apocalypse Now”. It’s incredible, and it’s also maybe another reason why I loved shooting Yavin – it’s the same atmosphere. I love “No Country for Old Men” and “Children of Men” with its incredible camera work.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-still13.jpg" alt="" width="3840" height="1594"><br>
<span>Cinematography of “Andor” by Christophe Nuyens. Courtesy of Lucasfilm/Disney+.</span></p>
<p><em><span>Kirill</span>: If you had a time machine, and you could go back in time to give your younger self a piece of advice, what would it be?</em></p>
<p><span>Christophe</span>: Be patient. I was really impatient with everything, as it was so difficult to get into doing fiction. Everything else comes along. I don’t regret any twists and turns of the path that I took.</p>
<p><em><span>Kirill</span>: What keeps you going? You get to travel the world and work on these great productions, but it also involves long hours and sleepless nights, and being away from your family and friends for long periods of time.</em></p>
<p><span>Christophe</span>: It’s really hard for the family, and it’s a demanding job. As long as I feel that I keep on learning, that I keep on evolving, that I keep on meeting really nice people I work with, I’m happy. When it starts to feel that it’s becoming a routine and that I’m doing the same things all over again, then I’ll stop and see what’s next. There are several other things I would love to do in my life outside of movies, but for now I still love it.</p>
<p>I’m getting to know new people. I’m learning a lot, be it on a bigger project like “Andor” or on smaller ones. When you travel, you get exposed to other cultures. I was on a production that took us to Ukraine and Kazakhstan, and the locals have a different way of working. I worked a lot in the UK, and their way is different from how they do it in France. I’m about to do two French movies, and I’m looking forward to that. The French way is the artists’ way. They spend the whole day talking about it. In the UK it’s a bit more efficient, maybe [laughs].</p>
<p>As long as I have this feeling, it’s good for me. But if I feel that I’m doing the same thing over and over again, I’d want to stop.</p>
<p><em>Kirill: From all the places you’ve traveled to so far, what’s your favorite cuisine?</em></p>
<p>Christophe: French. When I work in France, I gain 5 kilos every time. The food is so good, and they take a full hour to eat. If you don’t take an hour to eat and production decides to do continuous days, they will strike. It’s too much [laughs]. The UK crews don’t like to stop. They keep on going so they can go home on time to spend at least a bit of time with the family. As far as the food goes, I prefer to work in France.</p>
<p><img loading="lazy" decoding="async" src="https://www.pushing-pixels.org/wp-content/uploads/2025/05/andor-sets04.jpg" alt="" width="3200" height="2133"><br>
<span>On the set of “Andor”. Courtesy of Lucasfilm/Disney+.</span></p>
<p>And here I’d like to thank <a href="https://www.christophenuyens.com/"><strong>Christophe Nuyens </strong></a>for taking the time to talk with me about the art and craft of cinematography. I also want to thank Nathalie Retana and Jamie Miller for making this interview happen. “Andor” is <a href="https://www.disneyplus.com/browse/entity-faba988a-a9f5-45f2-a074-0775a7d6f67a">streaming on Disney+</a>. Finally, if you want to know more about how films and TV shows are made, <a href="https://www.pushing-pixels.org/inmotion/">click here</a> for additional in-depth interviews in this series.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why DeepSeek is cheap at scale but expensive to run locally (206 pts)]]></title>
            <link>https://www.seangoedecke.com/inference-batching-and-deepseek/</link>
            <guid>44149238</guid>
            <pubDate>Sun, 01 Jun 2025 07:31:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/inference-batching-and-deepseek/">https://www.seangoedecke.com/inference-batching-and-deepseek/</a>, See on <a href="https://news.ycombinator.com/item?id=44149238">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>Why is DeepSeek-V3 supposedly fast and cheap to serve at scale, but too slow and expensive to run locally? Why are some AI models slow to respond but fast once they get going?</p>
<p>AI inference providers often talk about a fundamental tradeoff between <em>throughput</em> and <em>latency</em>: for any given model, you can either serve it at high-throughput high-latency, or low-throughput low-latency. In fact, some models are so naturally GPU-inefficient that in practice they must be served at high-latency to have any workable throughput at all (for instance, DeepSeek-V3).</p>
<p>This tradeoff comes from the <strong>batch size</strong> the inference provider chooses for the model: not batching inference inside an individual request<sup id="fnref-1"><a href="#fn-1">1</a></sup>, but batching inference across tens or hundreds of concurrent user requests. It’s a peculiar feature of transformer-based LLMs that computing a batch of completions at the same time is almost as fast as computing a single completion. Why is that?</p>
<h3>What is batch inference?</h3>
<p>GPUs are good at doing big matrix multiplications (GEMMs, or “general matrix multiplications”). Say you have a single token that you want to pass through a model (i.e. by multiplying against all its weights - other architecture details aren’t relevant). You express that as a vector that matches the dimension (or hidden size) of the model (i.e. 1 x the width of its big weights matrices) and multiply it through. That’s 1 GEMM. But if you want to pass ten tokens through in a batch, that’s still only one GEMM, because you can stack the tokens into one matrix (10 x the model dimension). That’s a <em>lot</em> faster than doing ten slightly smaller GEMMs. So an inference server implementation might look something like this:</p>
<ol>
<li>A request comes in with a prompt</li>
<li>That prompt is pre-filled (passed through attention - we’ll see later how that can be batched as well<sup id="fnref-2"><a href="#fn-2">2</a></sup>), forming a KV cache and a token-sized matrix (1 x model-size) that will eventually become the predicted token<sup id="fnref-3"><a href="#fn-3">3</a></sup></li>
<li>That token-sized matrix goes into a queue</li>
<li>A GPU server pulls batches (e.g. of 128) off that queue, stacks them up into a 128 x model-size matrix, and multiplies them through the feed-forward model weights</li>
<li>The end result is then split into 128 separate tokens</li>
<li>The one for the original request is streamed back to the user</li>
<li>Assuming that token isn’t an end-of-sequence token, return to step 2 to continue generating the next token in the response</li>
</ol>
<p>Note that <em>the server decides</em> how big a batch size to pull. It’s a tradeoff between throughput and latency. If you do no batching and just process tokens one by one, no user ever waits in a queue (step 3 above), so latency is low (assuming you have enough GPUs). However, if you do a lot of batching, latency is high because users will be waiting until the batch size fills up, but throughput will be much higher because the GPUs are being used more efficiently.</p>
<p>Why are GPUs faster at multiplying large matrices once than small matrices many times? Two reasons. First, there’s some overhead involved in issuing each command to the GPU, and one big multiplication can be launched with a single command. Second, each new GPU command involves fetching weights from memory, which can be expensive for large weights. If you run lots of small GEMMs, you can end up spending most of your time shipping weights in and out of memory instead of computing.</p>
<h3>Why are some models tuned for high batch sizes?</h3>
<p>Typically an inference server will have a “collection window” where user requests come in and are queued. Chat servers typically aim for 5-10ms, but very high-batch backends might go as wide as 200ms. If a new request comes in at the start of the window, it might wait the entire window duration before being processed<sup id="fnref-4"><a href="#fn-4">4</a></sup>. When the window closes, all the queued requests are batched up (i.e. all the 1xmodel-size matrices are concatenated into a single 128xmodel-size matrix) and that batch is sent through the pipeline. Running a batch like this is sometimes called a “tick”.</p>
<p>As the explanation above suggests, you can run any model at any batch size. There’s nothing inherently about the batching process that would rule out some types of model. However, it is possible to build a model so GPU-inefficiently that it effectively <em>needs</em> batching in order to be practical.</p>
<h4>Why mixture of experts requires higher batch sizes</h4>
<p>For instance, take a mixture-of-experts model (like DeepSeek-V3 or supposedly the original GPT-4). You can get a strong model by training it to have hundreds and hundreds of “experts”: separate blocks of feed-forward weights, from which a routing layer picks a subset that’s used on each token. But a model like this is really GPU-inefficient. We can see why: GPUs want to do a small number of really big matrix multiplications, but if you have many experts you’re forced into many small multiplications. Unless you do your inference in batches, that’s going to mean low throughput.</p>
<p>Let’s think through how a “collection window” of 5ms and 200ms would perform for a large mixture-of-experts model. Suppose you pick up ten user requests in that 5ms window. If you have many experts, some experts might end up only running against one or two tokens (i.e. the batch size <em>for each expert</em> will be much lower than the total set of requests you’ve picked up in your window). If, however, you wait for 200ms and pick up 4000 user requests, you are much more likely to saturate all your experts. At the cost of some latency, you’re making sure that your GEMMs are large and your GPUs are constantly utilized at their maximum capacity.</p>
<h4>Why large pipelines require high batch sizes to avoid pipeline bubbles</h4>
<p>For large models, it can be a challenge to keep the GPUs active at all. Large models typically have many transformer layers: i.e. hundreds of matrices of weights that make up the feed-forward network. The only way to do fast inference here is to <em>pipeline</em> those layers by having one GPU handle the first ten layers, another handle the next ten, and so on. Otherwise you just won’t be able to fit all the weights in a single GPU’s memory, so you’ll spend a ton of time swapping weights in and out of memory and it’ll end up being really slow. During inference, each token (typically in a “micro batch” of a few tens of tokens each) passes sequentially through that pipeline of GPUs.</p>
<p>How efficient your pipeline is depends on the number of layers you have and the size of your collection window. When you’re processing the tokens in a window during a “tick”, you’ll get some idle GPUs at the start (because GPUs in later layers won’t have anything to work on yet) and some more idle GPUs at the end (when there’s no more tokens in the queue, GPUs in early layers will have to wait for the next “tick”). These periods of idleness are sometimes called “warmup” and “drain”. If you have many small windows, you’re going to spend more GPU time in warmup and drain than if you have fewer large windows. By picking your window size, you’re thus directly trading off between throughput and latency.</p>
<p>If you have a ton of layers and your collection window is really short, you might sometimes end up with fewer tokens to process than layers. This is called a “pipeline bubble” - in effect the “drain” stage starts earlier than usual. You can’t eliminate warmup and drain (for reasons discussed below, inference has to operate in sequential “ticks”), but you can eliminate pipeline bubbles by making your collection window long enough. Pipeline bubbles can be absolutely brutal for model throughput, so inference providers always set their windows wide enough to avoid them. That adds noticeable latency for models with many layers.</p>
<h4>Can’t you just keep the queue full?</h4>
<p>Why couldn’t inference providers eliminate warmup and drain entirely by keeping the GPU queue full of tokens? In other words, couldn’t you do away with ticks altogether and just keep the token micro-batches flowing? Of course each user’s inference has to be sequential (since you can’t start generating the next token until the current token is done), but large inference providers should have enough concurrent traffic to keep the queue full of separate user requests.</p>
<p>I’ll confess I struggle to see why this shouldn’t be possible in theory. As far as I can tell the practical barrier is how the <em>attention</em> step is batched: if you want to batch up attention GEMMs, they need to all be the same shape (i.e. the same number of prior tokens in the sequence). So you have to run groups of the same shape at the same time, instead of being able to just maintain a single queue. There’s at least <a href="https://arxiv.org/abs/2403.02310">some public research</a> on this front, but I wouldn’t be surprised if there were more clever tricks for doing this that I haven’t seen.</p>
<p>Another idea: if you need ticks for the attention step, why not just have a tick-based attention inference system and a more efficient continuous system for the FFN? As I understand it, the reason is <em>memory overhead</em>:</p>
<ol>
<li>Since the attention output is needed for the FFN, you’d need to have some place in-memory to park it while it waits for its slot in the FFN queue, which would quickly become too expensive.</li>
<li>Modern inference stacks are able to combine the attention and FFN step into a couple of large GEMMs in a single “operation”. If you’re doing these on different GPUs, you have to run different operations and shuttle the weights in and out of memory.</li>
</ol>
<h3>Summary</h3>
<ul>
<li>GPUs are most efficient on <em>large</em> GEMMs, so stacking many tokens into a single matrix multiply gives far higher token throughput than processing them one-by-one</li>
<li>
<p>During decoding, attention can only be batched for tokens at the <strong>same step</strong>, forcing schedulers to run in short “ticks”. How many tokens you pack into a single “tick” (i.e. how long you wait to collect tokens) is your batch size</p>
<ul>
<li>These are tokens <em>from different users</em>. You can’t batch tokens from the same user because you need previous tokens to generate the next one, so batching requires a high volume of traffic from different users</li>
</ul>
</li>
<li>Bigger batches raise latency because user tokens might be waiting up to 200ms before the batch is full enough to run, but they boost throughput by allowing larger (and thus more efficient) GEMMs in the feed-forward step</li>
<li>Models with many layers (e.g. long pipelines) need larger batches to avoid <strong>pipeline bubbles</strong> (by ensuring each tick contains more batches than pipeline steps) </li>
<li>Mixture-of-Experts models need to be served with high-latency to be efficient: each expert sees only the tokens routed to it, so you need larger global batches to keep every expert busy.  </li>
<li>Inference providers pick a batch size/window that clears pipeline bubbles and saturates experts. High batch sizes buy you more throughput at the cost of higher latency as tokens wait to fill up the tick</li>
<li>Some models (like DeepSeek’s) that are mixture-of-experts with many layers thus <em>require</em> large batch sizes and high latency, otherwise throughput drops off a cliff. That’s why it’s commonly said that you can’t easily run DeepSeek for personal use: because with a single user running one inference at a time, it runs at very low efficiency/throughput</li>
<li>
<p>The fact that OpenAI and Anthropic’s models are quick to respond suggests that either:</p>
<ul>
<li>Their models have a more efficient architecture (non-MoE, fewer layers), or</li>
<li>OpenAI/Anthropic have some very clever tricks for serving inference, or</li>
<li>they’re paying through the nose for way more GPUs than they strictly need</li>
</ul>
</li>
</ul>
<p><sup id="fnref-1"><a href="#fn-1">1</a></sup> One commonly-observed strength of transformers is that they can batch <em>prefill</em> within a single user request. When you pass them a long prompt, they can process that prompt all at once because of how the attention mechanism works. Previous recurrent models had to go token-by-token, which was much slower (because it involved many more GEMMs). <strong>This has nothing to do with the kind of batching I’m talking about in this post</strong>. I’m talking about how you can efficiently batch <em>inference</em> across many different user requests once the prefilling is complete.</p>
<p><sup id="fnref-2"><a href="#fn-2">2</a></sup> This can also be batched, so long as you’re only batching attention operations with the same number of tokens in the sequence (i.e. every sequence predicting the fourth token can be batched together). Otherwise the size of the KV cache matrices are different, so you can’t easily combine them into a single batch. More on that later.</p>
<p><sup id="fnref-3"><a href="#fn-3">3</a></sup> Technically it’s not a token being generated, but the “logits” (a probability distribution across all possible tokens). I’ll say “token” here and later on to keep it simpler.</p>
<p><sup id="fnref-4"><a href="#fn-4">4</a></sup> Note that in practice modern inference stacks will use “continuous batching”, where a batch is sent off as soon as it’s full instead of waiting for the entire length of the fixed time window. However, the inference is still done in batches, to the core tradeoff between throughput and latency is the same.</p></section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts.</p><p>June 1, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/ai/">ai</a>, <a href="https://www.seangoedecke.com/tags/explainers/">explainers</a>, <a href="https://www.seangoedecke.com/tags/deepseek/">deepseek</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google AI Edge – on-device cross-platform AI deployment (127 pts)]]></title>
            <link>https://ai.google.dev/edge</link>
            <guid>44149019</guid>
            <pubDate>Sun, 01 Jun 2025 06:32:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.google.dev/edge">https://ai.google.dev/edge</a>, See on <a href="https://news.ycombinator.com/item?id=44149019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    
  <devsite-gemini-page>
    

<div>
    <p data-viewport="once" data-viewport-threshold=".3">
      <svg viewBox="0 0 708 106" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M40.086 81.728c-7.272 0-13.968-1.764-20.088-5.292-6.12-3.528-10.98-8.388-14.58-14.58-3.6-6.192-5.4-13.032-5.4-20.52 0-7.488 1.8-14.328 5.4-20.52 3.6-6.192 8.46-11.052 14.58-14.58C26.118 2.708 32.814.944 40.086.944c5.688 0 11.016 1.008 15.984 3.024 5.04 2.016 9.18 4.86 12.42 8.532l-6.588 6.588c-2.448-3.024-5.58-5.328-9.396-6.912-3.816-1.656-7.92-2.484-12.312-2.484-5.472 0-10.584 1.332-15.336 3.996-4.68 2.592-8.46 6.3-11.34 11.124-2.808 4.752-4.212 10.26-4.212 16.524 0 6.264 1.404 11.808 4.212 16.632 2.88 4.752 6.696 8.46 11.448 11.124 4.752 2.592 9.828 3.888 15.228 3.888 5.184 0 9.504-.792 12.96-2.376 3.528-1.656 6.552-3.816 9.072-6.48 1.872-2.016 3.384-4.428 4.536-7.236 1.152-2.88 1.872-6.048 2.16-9.504h-28.62v-8.532h37.152c.36 2.016.54 3.888.54 5.616 0 4.752-.756 9.396-2.268 13.932-1.512 4.464-3.924 8.388-7.236 11.772-7.128 7.704-16.596 11.556-28.404 11.556Zm71.584 0c-5.4 0-10.26-1.296-14.58-3.888-4.32-2.592-7.704-6.12-10.152-10.584-2.376-4.464-3.564-9.396-3.564-14.796 0-5.4 1.188-10.332 3.564-14.796C89.386 33.2 92.77 29.672 97.09 27.08c4.32-2.592 9.18-3.888 14.58-3.888s10.26 1.296 14.58 3.888c4.32 2.592 7.668 6.12 10.044 10.584 2.448 4.464 3.672 9.396 3.672 14.796 0 5.4-1.224 10.332-3.672 14.796-2.376 4.464-5.724 7.992-10.044 10.584-4.32 2.592-9.18 3.888-14.58 3.888Zm0-8.316c3.384 0 6.516-.828 9.396-2.484 2.952-1.728 5.292-4.176 7.02-7.344 1.8-3.168 2.7-6.876 2.7-11.124s-.9-7.956-2.7-11.124c-1.728-3.168-4.068-5.58-7.02-7.236-2.88-1.728-6.012-2.592-9.396-2.592-3.384 0-6.552.864-9.504 2.592-2.952 1.656-5.328 4.068-7.128 7.236-1.728 3.168-2.592 6.876-2.592 11.124s.864 7.956 2.592 11.124c1.8 3.168 4.176 5.616 7.128 7.344a19.115 19.115 0 0 0 9.504 2.484Zm61.336 8.316c-5.4 0-10.26-1.296-14.58-3.888-4.32-2.592-7.704-6.12-10.152-10.584-2.376-4.464-3.564-9.396-3.564-14.796 0-5.4 1.188-10.332 3.564-14.796 2.448-4.464 5.832-7.992 10.152-10.584 4.32-2.592 9.18-3.888 14.58-3.888s10.26 1.296 14.58 3.888c4.32 2.592 7.668 6.12 10.044 10.584 2.448 4.464 3.672 9.396 3.672 14.796 0 5.4-1.224 10.332-3.672 14.796-2.376 4.464-5.724 7.992-10.044 10.584-4.32 2.592-9.18 3.888-14.58 3.888Zm0-8.316c3.384 0 6.516-.828 9.396-2.484 2.952-1.728 5.292-4.176 7.02-7.344 1.8-3.168 2.7-6.876 2.7-11.124s-.9-7.956-2.7-11.124c-1.728-3.168-4.068-5.58-7.02-7.236-2.88-1.728-6.012-2.592-9.396-2.592-3.384 0-6.552.864-9.504 2.592-2.952 1.656-5.328 4.068-7.128 7.236-1.728 3.168-2.592 6.876-2.592 11.124s.864 7.956 2.592 11.124c1.8 3.168 4.176 5.616 7.128 7.344a19.115 19.115 0 0 0 9.504 2.484Zm60.148 31.644c-6.48 0-11.844-1.512-16.092-4.536-4.176-3.024-6.984-6.66-8.424-10.908l8.316-3.564c1.224 3.168 3.24 5.724 6.048 7.668 2.88 2.016 6.264 3.024 10.152 3.024 5.688 0 10.08-1.656 13.176-4.968 3.168-3.312 4.752-7.992 4.752-14.04v-6.156h-.432c-1.8 2.808-4.356 5.112-7.668 6.912-3.24 1.728-6.948 2.592-11.124 2.592-4.752 0-9.108-1.224-13.068-3.672-3.888-2.448-6.984-5.868-9.288-10.26-2.304-4.392-3.456-9.396-3.456-15.012 0-5.544 1.152-10.512 3.456-14.904 2.304-4.464 5.4-7.92 9.288-10.368 3.96-2.448 8.316-3.672 13.068-3.672 4.176 0 7.884.9 11.124 2.7 3.312 1.728 5.868 4.032 7.668 6.912h.432V24.92h8.748v52.92c0 8.856-2.448 15.588-7.344 20.196-4.896 4.68-11.34 7.02-19.332 7.02Zm0-32.292c3.312 0 6.3-.828 8.964-2.484 2.736-1.656 4.896-4.032 6.48-7.128 1.656-3.096 2.484-6.768 2.484-11.016 0-4.32-.828-8.028-2.484-11.124-1.584-3.096-3.744-5.436-6.48-7.02-2.664-1.656-5.652-2.484-8.964-2.484-3.312 0-6.336.828-9.072 2.484-2.736 1.656-4.932 4.032-6.588 7.128-1.584 3.096-2.376 6.768-2.376 11.016s.792 7.956 2.376 11.124c1.656 3.096 3.852 5.472 6.588 7.128 2.736 1.584 5.76 2.376 9.072 2.376Zm37.25-70.092h9.18V80h-9.18V2.672Zm43.77 79.056c-5.256 0-9.972-1.26-14.148-3.78-4.176-2.52-7.452-5.976-9.828-10.368-2.304-4.464-3.456-9.468-3.456-15.012 0-5.328 1.08-10.224 3.24-14.688 2.232-4.464 5.364-8.028 9.396-10.692 4.032-2.664 8.712-3.996 14.04-3.996 5.4 0 10.08 1.224 14.04 3.672 3.96 2.376 6.984 5.688 9.072 9.936 2.16 4.248 3.24 9.108 3.24 14.58 0 1.08-.108 2.016-.324 2.808h-43.524c.216 4.176 1.224 7.704 3.024 10.584s4.068 5.04 6.804 6.48c2.808 1.44 5.724 2.16 8.748 2.16 7.056 0 12.492-3.312 16.308-9.936l7.776 3.78c-2.376 4.464-5.616 7.992-9.72 10.584-4.032 2.592-8.928 3.888-14.688 3.888Zm15.876-35.1c-.144-2.304-.792-4.608-1.944-6.912-1.152-2.304-2.988-4.248-5.508-5.832-2.448-1.584-5.544-2.376-9.288-2.376-4.32 0-7.992 1.404-11.016 4.212-2.952 2.736-4.896 6.372-5.832 10.908h33.588Zm60.379-43.956h10.368L430.173 80h-10.044l-7.776-21.276h-33.372L371.205 80h-10.044l29.268-77.328Zm18.792 47.412-10.26-27.864-3.132-8.316h-.432l-3.024 8.316-10.26 27.864h27.108Zm28.206-47.412h9.072V80h-9.072V2.672Zm45.469 0h44.712v8.748h-35.64v25.596h32.184v8.64h-32.184v25.596h35.64V80h-44.712V2.672Zm75.867 79.056c-4.896 0-9.36-1.26-13.392-3.78-3.96-2.52-7.092-5.976-9.396-10.368-2.304-4.464-3.456-9.504-3.456-15.12 0-5.544 1.152-10.548 3.456-15.012 2.304-4.464 5.436-7.956 9.396-10.476 4.032-2.52 8.496-3.78 13.392-3.78 4.248 0 8.028.972 11.34 2.916 3.312 1.872 5.832 4.212 7.56 7.02h.432l-.432-7.668V2.672h9.18V80h-8.748v-8.1h-.432c-1.728 2.736-4.248 5.076-7.56 7.02-3.312 1.872-7.092 2.808-11.34 2.808Zm1.08-8.316c3.24 0 6.228-.864 8.964-2.592 2.808-1.728 5.04-4.176 6.696-7.344 1.728-3.168 2.592-6.84 2.592-11.016s-.864-7.848-2.592-11.016c-1.656-3.168-3.888-5.616-6.696-7.344-2.736-1.728-5.724-2.592-8.964-2.592-3.168 0-6.156.864-8.964 2.592-2.808 1.728-5.076 4.212-6.804 7.452-1.656 3.168-2.484 6.804-2.484 10.908 0 4.104.828 7.776 2.484 11.016 1.728 3.168 3.996 5.616 6.804 7.344 2.808 1.728 5.796 2.592 8.964 2.592Zm61.225 31.644c-6.48 0-11.844-1.512-16.092-4.536-4.176-3.024-6.984-6.66-8.424-10.908l8.316-3.564c1.224 3.168 3.24 5.724 6.048 7.668 2.88 2.016 6.264 3.024 10.152 3.024 5.688 0 10.08-1.656 13.176-4.968 3.168-3.312 4.752-7.992 4.752-14.04v-6.156h-.432c-1.8 2.808-4.356 5.112-7.668 6.912-3.24 1.728-6.948 2.592-11.124 2.592-4.752 0-9.108-1.224-13.068-3.672-3.888-2.448-6.984-5.868-9.288-10.26-2.304-4.392-3.456-9.396-3.456-15.012 0-5.544 1.152-10.512 3.456-14.904 2.304-4.464 5.4-7.92 9.288-10.368 3.96-2.448 8.316-3.672 13.068-3.672 4.176 0 7.884.9 11.124 2.7 3.312 1.728 5.868 4.032 7.668 6.912h.432V24.92h8.748v52.92c0 8.856-2.448 15.588-7.344 20.196-4.896 4.68-11.34 7.02-19.332 7.02Zm0-32.292c3.312 0 6.3-.828 8.964-2.484 2.736-1.656 4.896-4.032 6.48-7.128 1.656-3.096 2.484-6.768 2.484-11.016 0-4.32-.828-8.028-2.484-11.124-1.584-3.096-3.744-5.436-6.48-7.02-2.664-1.656-5.652-2.484-8.964-2.484-3.312 0-6.336.828-9.072 2.484-2.736 1.656-4.932 4.032-6.588 7.128-1.584 3.096-2.376 6.768-2.376 11.016s.792 7.956 2.376 11.124c1.656 3.096 3.852 5.472 6.588 7.128 2.736 1.584 5.76 2.376 9.072 2.376Zm61.238 8.964c-5.256 0-9.972-1.26-14.148-3.78-4.176-2.52-7.452-5.976-9.828-10.368-2.304-4.464-3.456-9.468-3.456-15.012 0-5.328 1.08-10.224 3.24-14.688 2.232-4.464 5.364-8.028 9.396-10.692 4.032-2.664 8.712-3.996 14.04-3.996 5.4 0 10.08 1.224 14.04 3.672 3.96 2.376 6.984 5.688 9.072 9.936 2.16 4.248 3.24 9.108 3.24 14.58 0 1.08-.108 2.016-.324 2.808h-43.524c.216 4.176 1.224 7.704 3.024 10.584s4.068 5.04 6.804 6.48c2.808 1.44 5.724 2.16 8.748 2.16 7.056 0 12.492-3.312 16.308-9.936l7.776 3.78c-2.376 4.464-5.616 7.992-9.72 10.584-4.032 2.592-8.928 3.888-14.688 3.888Zm15.876-35.1c-.144-2.304-.792-4.608-1.944-6.912-1.152-2.304-2.988-4.248-5.508-5.832-2.448-1.584-5.544-2.376-9.288-2.376-4.32 0-7.992 1.404-11.016 4.212-2.952 2.736-4.896 6.372-5.832 10.908h33.588Z" fill="url(#a)"></path>
        <defs>
          <linearGradient id="a" x1="-69" y1="44" x2="776" y2="44" gradientUnits="userSpaceOnUse">
            <stop stop-color="#217BFE"></stop>
            <stop offset=".335" stop-color="#078EFB"></stop>
            <stop offset=".7" stop-color="#AC87EB"></stop>
            <stop offset="1" stop-color="#EE4D5D"></stop>
          </linearGradient>
        </defs>
      </svg>
      <svg viewBox="0 0 220 109" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M20.114 39.832c-3.5 0-6.725-.85-9.672-2.548-2.946-1.699-5.286-4.039-7.02-7.02-1.733-2.981-2.6-6.275-2.6-9.88 0-3.605.867-6.899 2.6-9.88 1.734-2.981 4.074-5.321 7.02-7.02C13.39 1.785 16.613.936 20.114.936c2.74 0 5.304.485 7.696 1.456 2.427.97 4.42 2.34 5.98 4.108l-3.172 3.172c-1.178-1.456-2.686-2.565-4.524-3.328-1.837-.797-3.813-1.196-5.928-1.196-2.634 0-5.096.641-7.384 1.924-2.253 1.248-4.073 3.033-5.46 5.356-1.352 2.288-2.028 4.94-2.028 7.956s.676 5.685 2.028 8.008c1.387 2.288 3.224 4.073 5.512 5.356a15.08 15.08 0 0 0 7.332 1.872c2.496 0 4.576-.381 6.24-1.144 1.7-.797 3.155-1.837 4.368-3.12.902-.97 1.63-2.132 2.184-3.484.555-1.387.902-2.912 1.04-4.576h-13.78v-4.108h17.888c.174.97.26 1.872.26 2.704 0 2.288-.364 4.524-1.092 6.708a14.789 14.789 0 0 1-3.484 5.668c-3.432 3.71-7.99 5.564-13.676 5.564Zm34.911 0c-2.6 0-4.94-.624-7.02-1.872-2.08-1.248-3.709-2.947-4.888-5.096-1.144-2.15-1.716-4.524-1.716-7.124 0-2.6.572-4.975 1.716-7.124 1.179-2.15 2.808-3.848 4.888-5.096 2.08-1.248 4.42-1.872 7.02-1.872 2.6 0 4.94.624 7.02 1.872a12.905 12.905 0 0 1 4.836 5.096c1.179 2.15 1.768 4.524 1.768 7.124 0 2.6-.589 4.975-1.768 7.124a12.905 12.905 0 0 1-4.836 5.096c-2.08 1.248-4.42 1.872-7.02 1.872Zm0-4.004a8.92 8.92 0 0 0 4.524-1.196c1.422-.832 2.548-2.01 3.38-3.536.867-1.525 1.3-3.31 1.3-5.356 0-2.045-.433-3.83-1.3-5.356-.832-1.525-1.958-2.687-3.38-3.484a8.634 8.634 0 0 0-4.524-1.248c-1.629 0-3.154.416-4.576 1.248-1.421.797-2.565 1.959-3.432 3.484-.832 1.525-1.248 3.31-1.248 5.356 0 2.045.416 3.83 1.248 5.356.867 1.525 2.011 2.704 3.432 3.536a9.204 9.204 0 0 0 4.576 1.196Zm29.977 4.004c-2.6 0-4.94-.624-7.02-1.872-2.08-1.248-3.71-2.947-4.888-5.096-1.144-2.15-1.716-4.524-1.716-7.124 0-2.6.572-4.975 1.716-7.124 1.178-2.15 2.808-3.848 4.888-5.096 2.08-1.248 4.42-1.872 7.02-1.872 2.6 0 4.94.624 7.02 1.872a12.905 12.905 0 0 1 4.836 5.096c1.178 2.15 1.768 4.524 1.768 7.124 0 2.6-.59 4.975-1.768 7.124a12.905 12.905 0 0 1-4.836 5.096c-2.08 1.248-4.42 1.872-7.02 1.872Zm0-4.004a8.92 8.92 0 0 0 4.524-1.196c1.421-.832 2.548-2.01 3.38-3.536.866-1.525 1.3-3.31 1.3-5.356 0-2.045-.434-3.83-1.3-5.356-.832-1.525-1.959-2.687-3.38-3.484a8.634 8.634 0 0 0-4.524-1.248c-1.63 0-3.155.416-4.576 1.248-1.422.797-2.566 1.959-3.432 3.484-.832 1.525-1.248 3.31-1.248 5.356 0 2.045.416 3.83 1.248 5.356.867 1.525 2.01 2.704 3.432 3.536a9.204 9.204 0 0 0 4.576 1.196Zm29.404 15.236c-3.12 0-5.702-.728-7.748-2.184-2.01-1.456-3.362-3.207-4.056-5.252l4.004-1.716a7.832 7.832 0 0 0 2.912 3.692c1.387.97 3.016 1.456 4.888 1.456 2.739 0 4.854-.797 6.344-2.392 1.526-1.595 2.288-3.848 2.288-6.76v-2.964h-.208c-.866 1.352-2.097 2.461-3.692 3.328-1.56.832-3.345 1.248-5.356 1.248-2.288 0-4.385-.59-6.292-1.768-1.872-1.179-3.362-2.825-4.472-4.94-1.109-2.115-1.664-4.524-1.664-7.228 0-2.67.555-5.061 1.664-7.176 1.11-2.15 2.6-3.813 4.472-4.992 1.907-1.179 4.004-1.768 6.292-1.768 2.011 0 3.796.433 5.356 1.3 1.595.832 2.826 1.941 3.692 3.328h.208V12.48h4.212v25.48c0 4.264-1.178 7.505-3.536 9.724-2.357 2.253-5.46 3.38-9.308 3.38Zm0-15.548c1.595 0 3.034-.399 4.316-1.196 1.318-.797 2.358-1.941 3.12-3.432.798-1.49 1.196-3.259 1.196-5.304 0-2.08-.398-3.865-1.196-5.356-.762-1.49-1.802-2.617-3.12-3.38-1.282-.797-2.721-1.196-4.316-1.196-1.594 0-3.05.399-4.368 1.196-1.317.797-2.374 1.941-3.172 3.432-.762 1.49-1.144 3.259-1.144 5.304 0 2.045.382 3.83 1.144 5.356.798 1.49 1.855 2.635 3.172 3.432 1.318.763 2.774 1.144 4.368 1.144Zm18.38-33.748h4.42V39h-4.42V1.768Zm21.519 38.064c-2.531 0-4.802-.607-6.812-1.82-2.011-1.213-3.588-2.877-4.732-4.992-1.11-2.15-1.664-4.559-1.664-7.228 0-2.565.52-4.923 1.56-7.072 1.074-2.15 2.582-3.865 4.524-5.148 1.941-1.283 4.194-1.924 6.76-1.924 2.6 0 4.853.59 6.76 1.768 1.906 1.144 3.362 2.739 4.368 4.784 1.04 2.045 1.56 4.385 1.56 7.02 0 .52-.052.97-.156 1.352h-20.956c.104 2.01.589 3.71 1.456 5.096.866 1.387 1.958 2.427 3.276 3.12 1.352.693 2.756 1.04 4.212 1.04 3.397 0 6.014-1.595 7.852-4.784l3.744 1.82c-1.144 2.15-2.704 3.848-4.68 5.096-1.942 1.248-4.299 1.872-7.072 1.872Zm7.644-16.9c-.07-1.11-.382-2.219-.936-3.328-.555-1.11-1.439-2.045-2.652-2.808-1.179-.763-2.67-1.144-4.472-1.144-2.08 0-3.848.676-5.304 2.028-1.422 1.317-2.358 3.068-2.808 5.252h16.172Zm29.96-21.164h4.992L211.045 39h-4.836l-3.744-10.244h-16.068L182.653 39h-4.836l14.092-37.232Zm9.048 22.828-4.94-13.416-1.508-4.004h-.208l-1.456 4.004-4.94 13.416h13.052Zm14.025-22.828h4.368V39h-4.368V1.768Zm-158.185 57h21.528v4.212h-17.16v12.324H76.66v4.16H61.165v12.324h17.16V96H56.797V58.768ZM93.77 96.832c-2.358 0-4.507-.607-6.448-1.82-1.907-1.213-3.415-2.877-4.524-4.992-1.11-2.15-1.664-4.576-1.664-7.28 0-2.67.554-5.079 1.664-7.228 1.11-2.15 2.617-3.83 4.524-5.044 1.941-1.213 4.09-1.82 6.448-1.82 2.045 0 3.865.468 5.46 1.404 1.594.901 2.808 2.028 3.64 3.38h.208l-.208-3.692V58.768h4.42V96h-4.212v-3.9h-.208c-.832 1.317-2.046 2.444-3.64 3.38-1.595.901-3.415 1.352-5.46 1.352Zm.52-4.004c1.56 0 2.998-.416 4.316-1.248 1.352-.832 2.426-2.01 3.224-3.536.832-1.525 1.248-3.293 1.248-5.304 0-2.01-.416-3.779-1.248-5.304-.798-1.525-1.872-2.704-3.224-3.536a7.926 7.926 0 0 0-4.316-1.248c-1.526 0-2.964.416-4.316 1.248-1.352.832-2.444 2.028-3.276 3.588-.798 1.525-1.196 3.276-1.196 5.252s.398 3.744 1.196 5.304c.832 1.525 1.924 2.704 3.276 3.536 1.352.832 2.79 1.248 4.316 1.248Zm29.923 15.236c-3.12 0-5.703-.728-7.748-2.184-2.011-1.456-3.363-3.207-4.056-5.252l4.004-1.716a7.827 7.827 0 0 0 2.912 3.692c1.387.971 3.016 1.456 4.888 1.456 2.739 0 4.853-.797 6.344-2.392 1.525-1.595 2.288-3.848 2.288-6.76v-2.964h-.208c-.867 1.352-2.097 2.461-3.692 3.328-1.56.832-3.345 1.248-5.356 1.248-2.288 0-4.385-.59-6.292-1.768-1.872-1.179-3.363-2.825-4.472-4.94-1.109-2.115-1.664-4.524-1.664-7.228 0-2.67.555-5.061 1.664-7.176 1.109-2.15 2.6-3.813 4.472-4.992 1.907-1.179 4.004-1.768 6.292-1.768 2.011 0 3.796.433 5.356 1.3 1.595.832 2.825 1.941 3.692 3.328h.208V69.48h4.212v25.48c0 4.264-1.179 7.505-3.536 9.724-2.357 2.253-5.46 3.38-9.308 3.38Zm0-15.548c1.595 0 3.033-.399 4.316-1.196 1.317-.797 2.357-1.941 3.12-3.432.797-1.49 1.196-3.259 1.196-5.304 0-2.08-.399-3.865-1.196-5.356-.763-1.49-1.803-2.617-3.12-3.38-1.283-.797-2.721-1.196-4.316-1.196-1.595 0-3.051.399-4.368 1.196-1.317.797-2.375 1.941-3.172 3.432-.763 1.49-1.144 3.259-1.144 5.304 0 2.045.381 3.83 1.144 5.356.797 1.49 1.855 2.635 3.172 3.432 1.317.763 2.773 1.144 4.368 1.144Zm29.93 4.316c-2.531 0-4.802-.607-6.812-1.82-2.011-1.213-3.588-2.877-4.732-4.992-1.11-2.15-1.664-4.559-1.664-7.228 0-2.565.52-4.923 1.559-7.072 1.075-2.15 2.583-3.865 4.525-5.148 1.941-1.283 4.194-1.924 6.76-1.924 2.6 0 4.853.59 6.76 1.768 1.906 1.144 3.362 2.739 4.368 4.784 1.04 2.045 1.56 4.385 1.56 7.02 0 .52-.052.97-.156 1.352h-20.956c.104 2.01.589 3.71 1.456 5.096.866 1.387 1.958 2.427 3.276 3.12 1.352.693 2.756 1.04 4.212 1.04 3.397 0 6.014-1.595 7.852-4.784l3.744 1.82c-1.144 2.15-2.704 3.848-4.68 5.096-1.942 1.248-4.299 1.872-7.072 1.872Zm7.644-16.9c-.07-1.11-.382-2.219-.936-3.328-.555-1.11-1.439-2.045-2.652-2.808-1.179-.763-2.67-1.144-4.472-1.144-2.08 0-3.848.676-5.304 2.028-1.422 1.317-2.358 3.068-2.808 5.252h16.172Z" fill="url(#b)"></path>
        <defs>
          <linearGradient id="b" x1="-57" y1="50" x2="278" y2="50" gradientUnits="userSpaceOnUse">
            <stop stop-color="#217BFE"></stop>
            <stop offset=".335" stop-color="#078EFB"></stop>
            <stop offset=".7" stop-color="#AC87EB"></stop>
            <stop offset="1" stop-color="#EE4D5D"></stop>
          </linearGradient>
        </defs>
      </svg>

      <h3 id="deploy-ai-across-mobile,-web,-and-embedded-applications" data-text="         Deploy AI across mobile, web, and embedded applications       " tabindex="-1">
        Deploy AI across mobile, web, and embedded applications
      </h3>
    </p>
    <ul data-viewport="once" data-viewport-threshold=".3">
      <li>
        
        <div>
          <h4 id="on-device" data-text="On device" tabindex="-1">On device</h4>
          <p>Reduce latency. Work offline. Keep your data local &amp; private.</p>
        </div>
      </li>
      <li></li>
      <li>
        
        <div>
          <h4 id="cross-platform" data-text="Cross-platform" tabindex="-1">Cross-platform</h4>
          <p>Run the same model across Android, iOS, web, and embedded.</p>
        </div>
      </li>
      <li></li>
      <li>
        
        <div>
          <h4 id="multi-framework" data-text="Multi-framework" tabindex="-1">Multi-framework</h4>
          <p>Compatible with JAX, Keras, PyTorch, and TensorFlow models.
          </p>
        </div>
      </li>
      <li></li>
      <li>
        
        <div>
          <h4 id="full-ai-edge-stack" data-text="Full AI edge stack" tabindex="-1">Full AI edge stack</h4>
          <p>Flexible frameworks, turnkey solutions, hardware accelerators</p>
        </div>
      </li>
    </ul>
  </div>
    

<div>
      <h2 data-viewport="once" data-viewport-threshold=".3" id="ready-made-solutions-and-flexible-frameworks" data-text="Ready-made solutions and flexible frameworks" tabindex="-1">Ready-made solutions and flexible frameworks</h2>
      <div data-viewport="once" data-viewport-threshold=".1">
      <h3 id="low-code-apis-for-common-ai-tasks" data-text="Low-code APIs for common AI tasks" tabindex="-1">Low-code APIs for common AI tasks</h3>
      <p>Cross-platform APIs to tackle common generative AI, vision, text, and audio tasks.</p>
      <p><a href="https://ai.google.dev/edge/mediapipe/solutions/guide" alt="Get started with MediaPipe tasks" data-category="content_click" data-label="https://ai.google.dev/edge/mediapipe/solutions/guide" data-action="Get started with MediaPipe tasks">
        <span>
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <mask id="mask0_28372_1334" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="16" height="16">
            <rect width="16" height="16" fill="url(#pattern0_28372_1334)"></rect>
            </mask>
            <g mask="url(#mask0_28372_1334)">
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            </g>
            <defs>
            <pattern id="pattern0_28372_1334" patternContentUnits="objectBoundingBox" width="1" height="1">
            <use xlink:href="#image0_28372_1334" transform="scale(0.00416667)"></use>
            </pattern>
            <image id="image0_28372_1334" width="240" height="240" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPAAAADwCAYAAAA+VemSAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAA8KADAAQAAAABAAAA8AAAAADV6CrLAAAR8UlEQVR4Ae2dT4gdRR7Hfz2z40RdB2VZSEI0CKsHJZ4MxFu8eJiAEAhzWRD2IGJOrqCX4ByiXj0aBpGcAvFhCCoZyMkcZaMXhRx2VQiIuizJysSsmejr3t+vX95kTOZPd1V1V3W/T8Ewb96r35/6VH+nqvt1V4lQIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQqEsgq2sQrf5gcI/8VDwrefG8FMWTkmW7pJDdZT6ZfK+/7eeSTGUfy4PZp7KwcDNarhsF7lP+Ik9oE3fDf6OObve99AV8crBTVvM39GB5QaT4Y0U8KyLZKdkxdVz+tvBjRZtmqvUh/xv5orL/qwKaqwgJ/hVB+VZLV8A2Yv1XD5xC/q4j7n1ODc3kuhRT78jeB96W+flVJx+uRl3Pf3l5Vi5fOyZZ/qr2wf1OGODvhK2OUZoCXjqj0+ObZ/TAeaZOYzavm32mo/Hh1kbjrudvs4Yb+VkddQ9szrTOJ/CvQ6tO3fQEvDTYJ8XwvIp3V52GbFs3k+8km56Xlxa+2rauT4Wu5//eh0/J8Ndzyn+PD4a7bOF/F5IQb6Ql4NF//n/of/6HQzTuLh92EM1O729sJO5D/qvDi8HFO+4I+I9JBPs9FcyTr6PB4F5ZHX7SmHgtPxtVbGpo53ehS9fzNybGJvTIu54z/NfTCPI6HQFfzV/Tg+fpIK3a0ome19nFmdCl6/mXTEKd824FF/5b0an7WRpT6Pc/2i2//u+fKmC3q531W31dp9J/CTaV7nr+5Vddw6/hX/FAsqvrIY+fimE3qpbGCPzbL/Z1UTviNQoWq/xucyMkDu91PX9jAf/qHR/6+Kke+a6a8Ufgpc9nJP/XvzWzh+7Krtk3VuRP03/2vmOr6/nb99VXhv9R1FVv0gjVK/APQDKBEfjb57QdbYvX0M2Vt2Z6Q+x4/nZ7avvihb/3cTdyEF/AeX4oUFvqu7H7qn1L5/MPwMCVIfxdya3ZxRewyGNr2bT/wm7K9y1dzz8EA1eGIWJ3nb8ru9IuvoAzedSrBX7Go6eZfHx0PX97qihe8Y/dff5e9OMLWIpHvFrgYzx+HNHHR9fzD8LAEWCQ2F0/fhzZ3TKLL+BCZvya4GNd+fHEzYN0Pf/NW9b8J5l+eeVbJpx/fAH7diD2fgRGiyH4+XC3/sHdFEsjgIA5Dmwlk1glZuxYbQ4aFwEHxdlJZ5ciZh0zdsRmhwuNgMOx7KYnW0MsVokZO1abA8dFwIGBds6dLQAoomtYtV5WysUHWw/br4AIuF/9Wb815eqdugBg60VjprZyaOsM/AMiYH+G3fdgq3faI3JtFYtlMSneBBCwN8IeOLCld231zraKxYq93G9bbW04DgJuGHBn3NvSu6KrRzZeNEYZq/FAExEAAU9EN1dopK2bbUvv2sJzTRXzbTHaXqO7qfYk4BcBJ9AJyaRg09rpmUONiNjEa8v6MnUO2t0IOCjOHjh78ciX5dK7QafTOm225XybXpO7B/jrNgEB1yU2CfVtlNw7d1DvtH3T6+p0eWVbfZgvRt5Gjpw/NOIVp90nMDpPXZSTg3dHCwA6bG42m8Dmct3viS1bgIC3xMOHt0bOozIYvLK2vWsXtxftaVci4J52bPBmje6aOq9+7YeSCAHOgRPpCNKAgAsBBOxCDRsIJEIAASfSEaQBARcCCNiFGjYQSIQAAk6kI0gDAi4EELALNWwgkAgBBJxIR5AGBFwIIGAXathAIBECCDiRjiANCLgQQMAu1LCBQCIEEHAiHUEaEHAhgIBdqGEDgUQIIOBEOoI0IOBCAAG7UMMGAokQQMCJdARpQMCFAM8Du1CbRJvB4J61B/qL4knJsl26u+/uEsVoi1LbafCS2H5Htl0Luy60cpQg4FYwdzjIycFOWc3fkCv5CyLrNkQv1u3NXcjj2kL7OSjD4qhc0b2WTpw+Ve6+wFpYjXY+U+hG8XbYuY24S6ffUvF+I4WKcr14t2/WnNZ/WVaHX8uJD47L8vLs9ibUcCGAgF2o9d1m6cwuuTq8IHlxTMV7n3NzC7lfREfvyysXdHG8nc5+MNyUAALeFM2EfrA02CfFzS/0/PaZcASKAzoaXxTzTQlKAAEHxdlxZzZK5vk5Fe+u4C0pZI8Uw2VG4rBkEXBYnt31Nhjcq6PkJ3ru+nBjjTAR38jPck4cjjACDsey256u5q/pyPt0843Q6fTla8eajzMZERDwZPTz1q18/yP9Prd4fetKAT/N8leZSofhiYDDcOy2l99+WdTRV68Yt1Qs1o18saVovQ6DgHvdvRUat/T5jH5dtFChZuAquteSfddM8SKAgL3w9cH42+e0FQ9FaMlceWtmhMB9ComA+9SbLm3J80MuZkFs8uL5IH4m2AkCnuDOv9X0xyIieCJi7F6ERsC96EaPRmTyqIe1r+noaSZfLxNsj4AnuPNHTS8eiYZg/DhitAS6HzgBAWc/R8OYZdeixU4lcCEz8VJZ93hivCQ6HTm+gEcPg8eC+EOswMSFQAgC8QUs8n2Ihjj6iBnbMWXMIHCbQAoCvnQ7ndZfxYzdemMJ2D8C8QVsayjFKjFjx2ozcXtFIL6AbQE00TWU2i8r5eJr7cclIgSCEYgv4HL1wuxUsBZVdqQxWTmxMi0qpkkgvoCNy46p45LJ9dYQWSyLSYFAxwmkIWBberSYeqc1lhaL5U5bw02g5gikIWBr394H3hbJPmuuqWPPGqOMNf6b3xDoLoF0BDw/v6rT2sM6lf6uMZzm22JYLAoEekAgHQEbTJvWTs8cakTEJt5sep6pcw+OWpqwRiAtAVtaLx75Uman94edTuu02Xy+tPDVWst5AYEeEEhPwAbVRuK9cwdFpt70ujpdXtlWH+aLi1Y9OFxpwp0E0t3cbHSeuqirF747WgBN11AS0T13KhW9MUS/553Vr4oQbiVgVOomgXQFPOY5EuBRXQDtlbXtLUVsJYfdupIi21uOOfF7IgmkL+Bxt4zumjqvf9oPBQIQUAJpngPTNRCAQCUCCLgSJipBIE0CCDjNfiErCFQigIArYaISBNIkgIDT7BeygkAlAgi4EiYqQSBNAgg4zX4hKwhUIoCAK2GiEgTSJICA0+wXsoJAJQIIuBImKkEgTQIIOM1+ISsIVCKAgCthohIE0iSAgNPsF7KCQCUCCLgSJipBIE0CCDjNfiErCFQi0J3ngQeDe9Ye6C+KJyXLdvFAf6U+plKPCaQv4JODnbKavyFX8hdE1m0IXRS3u6WQx/UP+zkow+KoXNG9lk6cPlXuvsCSOrc58ap3BNKdQtuIu3T6LRXvN1KoKNeLd/tu0LWzipdldfi1nPjguCwvz25vQg0IdI9AmgJeOrNLrg4vSF4cU/He54y1kPtFdPS+vHJBF8fb6ewHQwgkSiA9AS8N9klx8ws9v30mHLPigI7GF8V8UyDQIwJpCdhGyTw/p+LdFZxxIXukGC4zEgcni8OIBNIR8GBwr46Sn+i568ON8TAR38jPck7cGGEct0wgHQFfzV/Tkffp5tuv0+nL1441H4cIEGieQBoCfv8jXaC9eL355t6KkOWvMpVujTaBGiSQhoB/+2VRR1+9YtxSsVg38sWWohEGAo0RiC/gpc9n9OuihcZauKlj3WvJvmumQKDDBOILWL59Tvk9FIHhXHlrZoTAhIRAKALxBZznh0I1prafvHi+tg0GEEiIQHwBizwWkYftckiBQGcJxBdwJo9GpDfanjRiAoSGgA+B+AKW4hGfBnjZjvcX9nKCMQTiEYgv4EJm4jV/3eOJzklkPzub+hpm2TVfF9h3m0B8AXebn0gm30dswg8RYxM6AQII2L8TYgo4Zmx/cnjwJoCAvRHKJX8Xzh5ixnZOGsNwBBCwL8up7GNfF872MWM7J41hSAII2Jfmg9mn6mLF142D/YqMYjuYYtIXAgjYtycXFm7qlaxTvm7q22vMMnZ9Syz6QwABh+jLHVPH9Wr09RCuKvmwWBaTMvEEEHCIQ8CWri2m3gnhqpIPi8VyuZVQ9b0SAg7Vw3sfeFun0p+Fcre5H41Rxtq8Bp9MDgEEHKqv5+dXdVp7WKfS34VyeZcf820xLBYFAkoAAYc8DGxaOz1zqBERm3iz6XmmziE7rPu+EHDoPnzxyJcyO70/7HRap83m86WFr0Kni79uE0DATfSfjcR75w7qBOdNr6vT5ZVt9WG+uGjVRE913mf6m5t1FfHoPHVRV798d7SAnq7BJaJ7NlUqemOIfs87q18VIdxKwCa1EgJuuudHAjyqC+i9srY9qoitBLJbV+IcLSgweqLJHky4JHZ7pN1hxU0aTfdML/wj4La6cSTI8xrOfigQCEKAc+AgGHECgTgEEHAc7kSFQBACCDgIRpxAIA4BBByHO1EhEIQAAg6CEScQiEMAAcfhTlQIBCGAgINgxAkE4hBAwHG4ExUCQQgg4CAYcQKBOAQQcBzuRIVAEAIIOAhGnEAgDgEEHIc7USEQhAACDoIRJxCIQwABx+FOVAgEIYCAg2DECQTiEOB54La4Dwb38EB/W7AnJw4CbrqvTw52lkvqXBluvqROIY9rGvZzUIbFUbmiey2dOH2q3H2BJXWa7qFO+2cK3VT3LS/PyokPjsvq8GuR4mUNU3U9LMtI66qN2ZoP80WBwAYEEPAGULzfslH38soFkfwNXffqfmd/pa36MF/mkwKBOwgg4DuAeP/53odP6ch5UUfQA96+1hyoL/O5NNi39hYvIKAEEHDIw8BGyeGv53TU3RPSbenLfBbDZUbi4GQ77RABh+o+O0+9kZ9tRLzjHE3EFoNz4jGRif+NgEMdApevHQs7bd4sMZ1Ol7E2+5z3J4kAAg7R2zZ1zvJXQ7iq5MNicVGrEqq+V0LAIXr4Rr7odbW5bg52ddpiUiaeAAL2PQTsDisp9z3y9VTTXmOWsWuaUb1XBBCwb3f+VDyrLurcpOEbcWw/V96aOf6L3xNJAAH7dntePO/rwtk+ZmznpDEMSQAB+9O0nQZjlZixY7WZuOsIIOB1MBxfjrYIdTT2NIsZ2zN1zEMQQMC+FMd7/Pr6cbGPGdslX2yCE0hAwNnPwVtV1WGWXataNcl6mX555V26zr/r+ft1YHwBj3an92uFu/UP7qa3LMnfByH8feipbXwBi3zv2QYf8xCxQ/hwbUOI2CF8kL8rAU+7FAR8ybMNPuYhYofw4dqGELFD+CB/VwKedvEFPJV97NkGd/MQsUP4cG1BiNghfJC/KwFvu/gCfjD7VFux4t2S+g5WZBS7vuV6C/JfT6POa/jXobVJ3fgCXli4KZKd2iS/Bt/WmGVszxDk7wgQ/o7gfmcWX8CWzo6p45LJ9d9l1uQfFstihirkX48k/Ovx2qJ2GgK2pVOLqXe2yDPsRxYr5HKt5F+vf+Bfj9cWtdMQsCW494G3dSr92Ra5BvpIY5SxArkbuyH/MYltfsN/G0C1Pk5HwPPzqzqtPaxT6e9qtaBOZfNtMSxW6EL+2xOF//aMatZIR8CWuE1Fp2cONSJiO3iy6fmgU+c7YZP/nURu/w3/2ywCvkpLwNawF498KbPT+8NOp3XaZj5fWvgqILuNXZH/BlzgvwGUIG9lQbw04cSWTrXVF20BN9fdDexqp10wsfPTJqbNW7Wb/PV/MPy3OkRCfJaugMets9UXywXcynWnqi5dozeG6PeM9vVOyKvN45zq/Cb/H+vgCl636/y3AZK+gMcN6Pr2nOQ/7sk4v7vOPw41okIAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEekng/2YQf9qNUIDmAAAAAElFTkSuQmCC"></image>
            </defs>
          </svg>
          <span>Get started with MediaPipe tasks</span>
        </span>
      </a>
    </p></div>

      <div data-viewport="once" data-viewport-threshold=".1">
    <div>
      <h3 id="deploy-custom-models-cross-platform" data-text="Deploy custom models cross-platform" tabindex="-1">Deploy custom models cross-platform</h3>
      <p>Performantly run JAX, Keras, PyTorch, and TensorFlow models on Android, iOS, web, and embedded devices, optimized for traditional ML and generative AI.</p>
      <p><a href="https://ai.google.dev/edge/litert" alt="Get started with LiteRT" data-category="content_click" data-label="https://ai.google.dev/edge/litert" data-action="Get started with LiteRT">
        <span>
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <mask id="mask0_28372_1334" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="16" height="16">
            <rect width="16" height="16" fill="url(#pattern0_28372_1334)"></rect>
            </mask>
            <g mask="url(#mask0_28372_1334)">
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            </g>
            <defs>
            <pattern id="pattern0_28372_1334" patternContentUnits="objectBoundingBox" width="1" height="1">
            <use xlink:href="#image0_28372_1334" transform="scale(0.00416667)"></use>
            </pattern>
            <image id="image0_28372_1334" width="240" height="240" xlink:href="data:image/png;base64,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"></image>
            </defs>
          </svg>
          <span>Get started with LiteRT</span>
        </span>
      </a>
    </p></div>
    <p><img src="https://ai.google.dev/static/images/edge/feature-2.jpg" alt="Feature 2">
      
    </p>
  </div>

      <div data-viewport="once" data-viewport-threshold=".1">
    <div>
      <h3 id="shorten-development-cycles-with-visualization" data-text="Shorten development cycles with visualization" tabindex="-1">Shorten development cycles with visualization</h3>
      <p>Visualize your model’s transformation through conversion and quantization. Debug hotspots by
        overlaying benchmarks results.</p>
      <p><a href="https://ai.google.dev/edge/model-explorer" alt="Get started with Model Explorer" data-category="content_click" data-label="https://ai.google.dev/edge/model-explorer" data-action="Get started with Model Explorer">
        <span>
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <mask id="mask0_28372_1334" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="16" height="16">
            <rect width="16" height="16" fill="url(#pattern0_28372_1334)"></rect>
            </mask>
            <g mask="url(#mask0_28372_1334)">
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            </g>
            <defs>
            <pattern id="pattern0_28372_1334" patternContentUnits="objectBoundingBox" width="1" height="1">
            <use xlink:href="#image0_28372_1334" transform="scale(0.00416667)"></use>
            </pattern>
            <image id="image0_28372_1334" width="240" height="240" xlink:href="data:image/png;base64,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"></image>
            </defs>
          </svg>
          <span>Get started with Model Explorer</span>
        </span>
      </a>
    </p></div>
    <p><img src="https://ai.google.dev/static/images/edge/feature-3.jpg" alt="Feature 1">
      
    </p>
  </div>

      <div data-viewport="once" data-viewport-threshold=".1">
    <div>
      <h3 id="build-custom-pipelines-for-complex-ml-features" data-text="Build custom pipelines for complex ML features" tabindex="-1">Build custom pipelines for complex ML features</h3>
      <p>Build your own task by performantly chaining multiple ML models along with pre and post processing
      logic. Run accelerated (GPU &amp; NPU) pipelines without blocking on the CPU.&nbsp;</p>
      <p><a href="https://ai.google.dev/edge/mediapipe/framework" alt="Get started with MediaPipe Framework" data-category="content_click" data-label="https://ai.google.dev/edge/mediapipe/framework" data-action="Get started with MediaPipe Framework">
        <span>
          <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <mask id="mask0_28372_1334" style="mask-type:alpha" maskUnits="userSpaceOnUse" x="0" y="0" width="16" height="16">
            <rect width="16" height="16" fill="url(#pattern0_28372_1334)"></rect>
            </mask>
            <g mask="url(#mask0_28372_1334)">
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            <rect x="0.53125" y="1.06641" width="14.1333" height="13.3333" fill="white"></rect>
            </g>
            <defs>
            <pattern id="pattern0_28372_1334" patternContentUnits="objectBoundingBox" width="1" height="1">
            <use xlink:href="#image0_28372_1334" transform="scale(0.00416667)"></use>
            </pattern>
            <image id="image0_28372_1334" width="240" height="240" xlink:href="data:image/png;base64,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"></image>
            </defs>
          </svg>
          <span>Get started with MediaPipe Framework</span>
        </span>
      </a>
    </p></div>
    <p><img src="https://ai.google.dev/static/images/edge/feature-4.jpg" alt="Feature 2">
      
    </p>
  </div>
    </div>
    



<section>
  
  
  
  

  <div>
  <article data-viewport="once" data-viewport-threshold="0.3">
    
    
    <p>A low level framework used to build high performance accelerated ML pipelines, often including multiple ML models combined with pre and post processing.</p>
    
  </article>
</div>
  


  <div>
  <article data-viewport="once" data-viewport-threshold="0.3">
    
    
    <p>Visually explore, debug, and compare your models. Overlay performance benchmarks and numerics to pinpoint troublesome hotspots.</p>
    
  </article>
</div>
</section>
    <div>
    <div>
  <article data-viewport="once" data-viewport-threshold="0.3">
    
    <p>
      <h2 id="gemini-nano-in-android-chrome" data-text="Gemini Nano in Android &amp; Chrome" tabindex="-1">Gemini Nano in Android &amp; Chrome</h2>
    </p>
    <p>Build generative AI experiences using Google's most powerful, on-device model</p>
    
  </article>
</div>
    <div>
        <picture>
          <img src="https://ai.google.dev/static/images/edge/nano-characters.png" alt="nano characters">
        </picture>
      </div>
  </div>
    




<div>
  <article data-viewport="once" data-viewport-threshold="0.3">
    
    <p>
      <h2 id="recent-videos-and-blog-posts" data-text="Recent videos and blog posts" tabindex="-1">Recent videos and blog posts</h2>
    </p>
    
    
  </article>
</div>
  </devsite-gemini-page>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Figma Slides Is a Beautiful Disaster (310 pts)]]></title>
            <link>https://allenpike.com/2025/figma-slides-beautiful-disaster</link>
            <guid>44148933</guid>
            <pubDate>Sun, 01 Jun 2025 05:59:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://allenpike.com/2025/figma-slides-beautiful-disaster">https://allenpike.com/2025/figma-slides-beautiful-disaster</a>, See on <a href="https://news.ycombinator.com/item?id=44148933">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Some highlights and lowlights.</p><div>
<p>I think of presentation slides as having 3 main jobs:</p>

<ol>
  <li><strong>Emphasize key points</strong>, so people remember</li>
  <li><strong>Break up complex concepts</strong>, so people learn</li>
  <li><strong>Entertain</strong>, so people pay attention</li>
</ol>

<p>This calls for slides that are mostly images or very short phrases. A minority of slides justify designing something to match the vibe or illustrate a point – a diagram, for example. This can mean going back and forth between Keynote and, say, Figma, but it’s worth the effort.</p>

<p>This year I’ve been spending a lot of time in Figma, so when I was recently asked to speak at an event I thought, “Why not try <a href="https://www.figma.com/slides/">Figma Slides</a>?” Figma Slides launched a year back, and <a href="https://www.linkedin.com/posts/paigecostello_figma-slides-came-out-of-beta-today-30-activity-7308211241049591811-Z_-U/">graduated from beta in March</a>. I’ve used Keynote for almost 20 years now – it seemed like time for me to finally upgrade my presentation tool.</p>

<p>So I gave it a whirl.</p>

<p>Pretty quickly, I liked building slides right in Figma. The Grid view made it easy for me to structure my ideas. Features like Auto Layout and Components made building slides that adapt to different text and images a snap.</p>

<p>For example, a key point in my talk was that selecting a JavaScript framework can be overwhelming. It was easy to build a quick visual of this right in the deck.</p>

<p><img src="https://allenpike.com/images/2025/figma-smash.jpg" alt="A Choose Your Fighter screen of JS frameworks.">
Components and Auto Layout made this 10x faster to build in Figma than Keynote.
</p>

<p>Admittedly, Figma Slides is missing some Keynote features that I think of as pretty essential.</p>

<p>For example, Keynote has long had a toggle to “Autosize Text”, which will set an object’s font size to just big enough to fill its container. There’s no way to do this in Figma’s auto layout because Figma only wants to support layouts that CSS Grid can implement. That constraint makes sense when designing for the web, but shows how hard it is to expand a design tool to other domains.</p>

<p>Another odd omission is that you can’t easily create a slide where items (e.g. bullets, sections of a diagram) appear with each click. The closest you can do is to split them into different layers, add a “Fade” animation of 1 millisecond on each, then <em>reorder the animation order to match the order they appear on the slide</em> since it will default to the order the layers were created. 🥴</p>

<p>I’m not a fan of bullet-laden decks, but sometimes you just want to make 4 words appear one by one, okay?</p>

<p>Whining aside, building in Figma Slides got me excited. It was powerful, it was fun, and it gave me hope that Figma would successfully make the transition from single-product company about to be acquired by Adobe, to multi-product company <a href="https://www.cnbc.com/2025/04/15/figma-confidentially-files-for-ipo-a-year-after-ditching-adobe-deal.html">ready to IPO</a>.</p>

<p>Then I made a fatal mistake: I tried to actually give the presentation.</p>

<p><img src="https://allenpike.com/images/2025/figma-offline.jpg" alt="Figma error -106">
</p>

<p>I’ve given enough talks that I know to do at least one rehearsal with representative conditions: on an external display, clicker in hand, cued by the presenter view. I know to make sure it works offline, save a backup copy of the deck, and so on.</p>

<p>It was during this dry run I noticed some bad omens:</p>

<ol>
  <li>You can “Save Local Copy” of the presentation, but that doesn’t allow you to present it locally.</li>
  <li>Just because you have a presentation open and loaded, doesn’t mean you can present it. If you are offline when you actually click Present, it will barf.</li>
  <li>Once you are presenting, you can click to “download” the presentation to be available offline – but be careful not to close the tab or it will undownload!</li>
  <li>When you do click Present, you will not get a full-screen audience view, nor a <a href="https://support.apple.com/en-ca/guide/keynote/tanfde4a3e6d/mac">keyboard shortcut to swap which display the audience can see</a>. You just get a pop-up window you need to manually drag to the projector – hope it’s somewhere intuitive! – then maximize it. At a pro event you can do this with the AV crew, but at a meetup this is just visible to the audience.</li>
  <li>Make sure you then move your mouse to the edge of the screen. Otherwise, it will stay there on top of your slides like it’s 1999.</li>
  <li>Even beyond this, the functionality around managing Present and the Audience view is just a little… flaky?</li>
</ol>

<div>
<video src="https://allenpike.com/images/2025/figma-x.mp4" autoplay="" loop="" muted="" playsinline="">
  Your browser doesn't support HTML5 video.
</video><p>
Figma will decide when your presentation is over.
</p></div>

<p>This all made me uneasy, so I practiced the flow more often than usual. I made sure I clicked the right things to make it work offline. Then, I went to go present.</p>

<p>At the venue, I immediatley noticed something once the slides were up on their giant 40 foot display: I needed to click twice to advance each slide. Odd, but I’m an adult. I can click twice for each slide.</p>

<p>Unfortunately, halfway into my talk I noticed a much bigger problem.</p>

<p>The animations – which I had used only on my most complex slides, which needed to be explained in parts – weren’t advancing at all. Even if I clicked twice, the audience display would continue to show a blank slide.</p>

<p><a href="https://www.youtube.com/watch?v=j7_o-YiwGwo">
<img src="https://allenpike.com/images/2025/clicker-fail.jpg" alt="A man looks in confusion at his own clicker.">
</a>
Looking at my clicker in desperation: the exact moment I realized that I’d been hoisted on my own petard.
</p>

<p>After some exploratory clicking, I found that once all the builds on a blank slide had been triggered, it would then jump to the next slide. At that point, you could click back to show the fully built skipped slide.</p>

<p>So that’s what I did: on a slide with 7 builds, I would dutifully click 14 times to get Figma through to the next slide, then then click back, and attempt to explain all seven parts at once.</p>

<p>I stumbled through. The audience was gracious – thankfully it was only 100 people and they were friendly. But it stung a little.</p>

<p>A silver lining was that the core point of my talk was that <a href="https://boringtechnology.club/">boring technology is underrated</a>. Watching me struggle with Figma Slides helped prove the point. <strong>Here lies Allen, murdered by not-boring technology</strong>.</p>

<p>While I was able to reproduce the animation bug the next day, I haven’t been able to since restarting Figma. Their forums have stories of <a href="https://forum.figma.com/ask-the-community-7/fixed-figma-slides-show-smart-animate-transition-only-in-presenter-view-and-not-in-audience-view-39081?utm_source=chatgpt.com">other</a> <a href="https://forum.figma.com/ask-the-community-7/fixed-figma-slides-show-smart-animate-transition-only-in-presenter-view-and-not-in-audience-view-39081?utm_source=chatgpt.com">similar</a> <a href="https://forum.figma.com/ask-the-community-7/figma-slides-with-multiple-videos-in-present-mode-have-to-click-arrow-key-loads-before-next-slide-28477?utm_source=chatgpt.com">bugs</a>, none quite the same. So maybe it’s fixed.</p>

<p>Even if this specific bug is fixed, though, I get the strong impression that Figma doesn’t treat presenting Slides as mission-critical. A team that uses Figma Slides at meetups or conferences wouldn’t let clicking Present offline just vomit “Error -106”.</p>

<p>While Keynote is old, and pasting things in from other design tools feels janky, the OG still benefits from Apple institutionally <a href="https://allenpike.com/2022/giving-a-shit">giving a shit</a> about giving a presentation to an audience. You can tell they feel it should be bulletproof.</p>

<p>So, in the end, I learned the lesson I was trying to teach. Boring technology is good, actually.</p>

<p>Here’s to Keynote: it works. 🍻</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Father Ted Kilnettle Shrine Tape Dispenser (177 pts)]]></title>
            <link>https://stephencoyle.net/kilnettle</link>
            <guid>44148853</guid>
            <pubDate>Sun, 01 Jun 2025 05:31:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stephencoyle.net/kilnettle">https://stephencoyle.net/kilnettle</a>, See on <a href="https://news.ycombinator.com/item?id=44148853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><meta property="og:image" content="/media/kilnettle1.jpg">
<img src="https://stephencoyle.net/media/kilnettle1.jpg"></p>

<p>A couple of years ago I wrote <a href="https://stephencoyle.net/father-ted-tape-dispenser">this</a> post, wherein I detail how I built a working version of the tape dispenser from Father Ted (<a href="https://youtu.be/c_ojhPi7lH4">the one that says</a> <em>you have used two inches of sticky tape, god bless you</em>). While it worked surprisingly well, that version had a number of flaws. It used unnecessarily overpowered hardware, was fragile, and didn’t sound very good.</p>

<p>Since making that one, I’d toyed with the idea of creating a better, more repeatable design. So, in very procrastinate-y fits and starts over the last few months, I’ve been doing just that.</p>

<p>This new one is much smaller, sounds better, and I daresay looks a touch more <em>professional</em> than its predecessor. Read on to find out how to get one of your own!</p>

<p>
        <iframe width="420pt" height="315pt" src="//www.youtube.com/embed/zKlZgHTyWA4" frameborder="0" allowfullscreen="">
            </iframe>
        </p>

<p><img src="https://stephencoyle.net/media/kilnettle3.jpg"></p>

<p>More importantly, it’s also much easier to make and I’ve got actual documentation for doing so. The case is 3D-printable, and all the parts print with no supports. Instead of a rotary encoder to measure the tape rotation, it now uses an IR led and sensor. The logic runs on an ESP8266 microcontroller instead of a Raspberry Pi Zero, all of which means the electronics inside can be purchased for less than €10.</p>

<p>It was a lot of fun improving on my 3D modelling, electronics, design and general planning skills in the process of making it. The initial body design was more complex, as was the circuitry, and I enjoyed iterating on and simplifying those significantly by the final product.</p>

<p>I did nurture the idea of actually selling these, but ultimately I think I’ve decided against it for now, for a few reasons:</p>

<ul>
<li>Making one is fun, and a nice evening or weekend project. Making lots is more effort than I have time for at the moment.</li>
<li>I’ve got several earmarked for people already, and when I subtract those from the dozen I’ve made, I’m left with only a tiny batch to sell.</li>
<li>Factoring in the cost of components, time to assemble, and shipping, I don’t think there’s a compelling price point that doesn’t run at a loss. In which case I’d rather just give them away on my own terms.</li>
</ul>

<h2>How can you get one?</h2>

<p><img src="https://stephencoyle.net/media/kilnettle4.jpg"></p>

<p>The good news, though, is that you can build one yourself! I’ve made the software, 3D-printable models, and instructions public on <a href="https://github.com/stephenjdc/TapeDispenser">GitHub</a>. You can also view the print files on <a href="https://www.printables.com/model/935093-kilnettle-shrine-commemorative-talking-tape-dispen">Printables</a>. It’s not a super complex build, so if you can manage basic soldering and have a 3D printer it’s probably a one-day project. I could potentially make kits, which contain all the required parts, that folks could assemble by themselves. If that's something you'd be interested in, do let me know. And if you do make one, or make some kind of remix, please send me pics!</p>

<p>Finally, and perhaps most importantly, if you do make one for yourself I'd appreciate if you made a donation to a charity that supports trans people. It's an awful shame that the creator of Father Ted is doing so much damage to an already vulnerable group, so anything that can help offset that is appreciated.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Structured Errors in Go (2022) (102 pts)]]></title>
            <link>https://southcla.ws/structured-errors-in-go</link>
            <guid>44148734</guid>
            <pubDate>Sun, 01 Jun 2025 04:54:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://southcla.ws/structured-errors-in-go">https://southcla.ws/structured-errors-in-go</a>, See on <a href="https://news.ycombinator.com/item?id=44148734">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><time>written<!-- --> <!-- -->almost 3 years ago</time></p><hr></div><p>This post documents a catalogue of experiments on the topic of error management in medium-sized Go programs, specifically HTTP APIs, with certain end goals in mind around ergonomics, syntactic salt and making the lives of everyone involved easier. The final result is a simple approach and a new library that I've been using in production for a couple of months now.</p>
<p>Error handling (or, more accurately, error <em>management</em>) has always ended up being a hot topic with Go. Most likely because of two things. Firstly, Go's early ideas about error handling – as with a lot of Go's design – chose to steer away from commonly found implementations such as exception handling and opted for a vastly simpler (at least from an implementation perspective) approach: return values. Secondly, the smallest, simplest form of an error in Go is a string of text. This leaves a lot of room for building on top of the simple concept of a string of text.</p>
<pre><code><span>type</span> <span>error</span> <span>interface</span> <span>{</span>
    <span>Error</span><span>(</span><span>)</span> <span>string</span>
<span>}</span>
</code></pre>
<p>Every Go programmer is familiar with this interface. The beauty in simplicity here is that any type that satisfies this interface is an error. This allows us to construct domain-specific types that satisfy our needs if those needs stretch beyond a simple string of text.</p>
<p>Okay, so you know that. I did the one thing I hate in articles about some topic and described the absolute basics of said topic despite the fact that whoever is reading this already knows all of that before getting to the <em>actual</em> content.</p>
<h2>Structured logging</h2>
<blockquote>
<p>(unrelated to lumberjack unions)</p>
</blockquote>
<p>I won't do that here, promise. You all know what structured logging is. It's basically a standard concept in most medium and larger systems. As <a target="_blank" href="https://www.willett.io/posts/precepts/">Brandon Willett writes</a>, <strong>Structured logs are non-negotiable</strong>.</p>
<p>So what does this have to do with error handling? Well, first of all, let's define <em>error handling</em> by showing what it's not:</p>
<pre><code>data<span>,</span> err <span>:=</span> service<span>.</span><span>GetAccount</span><span>(</span><span>...</span><span>)</span>
<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
    <span>return</span> <span>nil</span><span>,</span> err
<span>}</span>
</code></pre>
<p>This is not error handling. Nothing about this code is handling anything. This is I-don't-know-what-to-do-with-this-error-so-you-deal-with-it.</p>
<p>And this is fine and this is still one part of what I call error <em>management</em> because there are a few different key events during the lifecycle of an error.</p>
<pre><code>data<span>,</span> err <span>:=</span> service<span>.</span><span>GetAccount</span><span>(</span><span>...</span><span>)</span>
<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
    <span>return</span> <span>nil</span><span>,</span> errors<span>.</span><span>Wrap</span><span>(</span>err<span>,</span> <span>"failed to get account"</span><span>)</span>
<span>}</span>
</code></pre>
<p>This is somewhat more involved, and arguably, more useful error management. We're now annotating the error with a bit more context. So when we finally <em>handle</em> the error, we will see the original error that <code>GetAccount</code> produced as well as some <em>context</em> relevant to the caller of <code>GetAccount</code>.</p>
<p>And this is <em>error management</em> for a lot of Go code. Glueing strings together is – a lot of the time – <em>all you need</em>.</p>
<pre><code>data<span>,</span> err <span>:=</span> service<span>.</span><span>GetThing</span><span>(</span>id<span>)</span>
<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
    <span>return</span> <span>nil</span><span>,</span> errors<span>.</span><span>Wrapf</span><span>(</span>err<span>,</span> <span>"failed to get account: %v"</span><span>,</span> id<span>)</span>
<span>}</span>
</code></pre>
<p>So now, let's say the implementation of <code>GetAccount</code> accesses a database and the database is offline, we would see something like:</p>
<pre><code>failed to get account: 59: database connection refused
</code></pre>
<p>The actual <em>handling</em> of this error occurs elsewhere. I'm willing to bet that within the majority of cases, where the service is some form of HTTP based API, the error handling happens near the service boundary.</p>
<p>The service boundary for a HTTP server is a route handler. The typical approach is:</p>
<ol>
<li>there's an error! oh no</li>
<li>log.Error</li>
<li>http.StatusInternalServerError</li>
<li>the user tweets the error screen to your company account and everyone is slightly less happy</li>
</ol>
<p>Step 2 is what we're interested in here. <em>Handling</em> an error in this example means we tell someone about it. That someone is, again, in the majority of cases, probably a log aggregator and an alerting system and, eventually, a human.</p>
<p>Now, log aggregators are incredibly powerful. You can filter based on the service, the account that made the request, the trace ID, the filename that the log entry originated from, the region the service is running in, and infinitely more options. This is extremely useful for diagnosing issues at scale.</p>
<p>But you can only do all of that stuff if you <em>actually</em> structure your log entries. And this is not very structured:</p>
<pre><code><span>{</span>
  <span>"message"</span><span>:</span> <span>"failed to get account: 59: database connection refused"</span><span>,</span>
  <span>"level"</span><span>:</span> <span>"error"</span><span>,</span>
  <span>"file"</span><span>:</span> <span>"api/get_user.go:56"</span>
<span>}</span>
</code></pre>
<p>I can't filter all logs by account "59". Well, I can, I can search for the string of text "account: 59" but that won't help when developers use slightly different wording in their messages. Strings of text are for humans to read after all.</p>
<p>In a perfect world, it would be nice to do this:</p>
<pre><code><span>{</span>
  <span>"message"</span><span>:</span> <span>"failed to get account: 59: database connection refused"</span><span>,</span>
  <span>"level"</span><span>:</span> <span>"error"</span><span>,</span>
  <span>"file"</span><span>:</span> <span>"api/get_user.go:56"</span><span>,</span>
  <span>"account_id"</span><span>:</span> <span>"59"</span>
<span>}</span>
</code></pre>
<p>Which you can do with any structured logging library, but there's a wide gap between your error value and your <code>log.Info</code> call. You can solve this by creating a custom type that satisfies the <code>error</code> interface:</p>
<pre><code><span>type</span> AccountError <span>{</span>
  message   <span>string</span>
  AccountID <span>int</span>
<span>}</span>

<span>func</span> <span>(</span>a <span>*</span>AccountError<span>)</span> <span>Error</span><span>(</span><span>)</span> <span>string</span> <span>{</span>
  <span>return</span> a<span>.</span>message
<span>}</span>
</code></pre>
<p>Then, when you handle the error, you can check if the error is of this type and pull out the account ID for logging.</p>
<pre><code><span>var</span> ae <span>*</span>AccountError
<span>if</span> errors<span>.</span><span>As</span><span>(</span>err<span>,</span> <span>&amp;</span>ae<span>)</span> <span>{</span>
  <span>// add ae.AccountID to the structured logging call</span>
<span>}</span>
</code></pre>
<p>That's a lot of work, imagine doing that on a large system with many different kinds of resources, many different kinds of metadata you want to include with logs. It would be easier if you could just store a <code>map[string]string</code> of metadata and log that, right?</p>
<h2>The importance of being ergonomic</h2>
<blockquote>
<p>(not a yoga ad)</p>
</blockquote>
<p>One thing I've noticed throughout my career in myself, coworkers and open source collaborators I've worked with is that if you make good things hard to do, people won't do them.</p>
<p>Take testing as an example. I wrote a bunch of C++ early on in my software authorship and writing tests for C++ is a whole ordeal. You need a completely separate independent tool to handle the dance of compiling and linking your code as a separate binary with a bunch of extra tooling, macros and functions to facilitate telling the test system where your test functions are.</p>
<p>Compare that to Go where 1. it's built into the toolchain and 2. you literally just add "_test" to your filename. This is so simple that it removes all barriers to writing tests. Easy to write means developers actually do it early instead of ending every PR with a commit titled "add tests".</p>
<p>I've still never written a single C++ test, I couldn't get the tooling to work with my CMake setup and my compiler configuration so I gave up. Not ergonomic.</p>
<h2>The ergonomics of errors</h2>
<p>When it comes to errors, Go does have a fairly ergonomic mechanism. You write a message, maybe add another message, you read it later in the logs and figure out what went wrong. It's easy to create errors and it's easy to add context to error chains.</p>
<p>With custom error structs however, it's a lot of writing to create your own error type and thus it becomes more of a burden to encourage your team members to do this. I've worked in codebases where there are a couple of custom errors for special cases but most of the time it ends up just being <code>errors.Wrap(err, "thing failed to do stuff")</code> which, the majority of the time, I do not find that it adds value to the diagnostic process when things go wrong.</p>
<p>The key is to build this concept into your entire codebase so it doesn't get forgotten. If every single instance of error handling does X, Y and Z then when new developers join your team, they will notice that and follow convention.</p>
<h2>Design</h2>
<p>So we want ergonomics? Let's start from the top. Forget implementation details, we care about the <code>user experience</code> of our error management solution. Treat your libraries like products and your team as end-users and you'll have a happy team.</p>
<p>What we want is:</p>
<ol>
<li>to decorate errors with metadata</li>
<li>to easily access that metadata when it comes to logging the problem or sending a response to the client.</li>
<li>make it stupid simple</li>
</ol>
<p>So I'm going to copy the popular Go structured logging library, Logrus, for this example. It's widely used so it's probably doing something right when it comes to ease-of-use.</p>
<pre><code>log<span>.</span><span>WithFields</span><span>(</span>log<span>.</span>Fields<span>{</span>
  <span>"event"</span><span>:</span> event<span>,</span>
  <span>"topic"</span><span>:</span> topic<span>,</span>
  <span>"key"</span><span>:</span> key<span>,</span>
<span>}</span><span>)</span><span>.</span><span>Fatal</span><span>(</span><span>"Failed to send event"</span><span>)</span>
</code></pre>
<p>What we want is something similar to this. Let's sketch it out:</p>
<pre><code><span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
  <span>return</span> <span>Wrap</span><span>(</span>err<span>,</span> Fields<span>{</span>
    <span>"event"</span><span>:</span> event<span>,</span>
    <span>"topic"</span><span>:</span> topic<span>,</span>
    <span>"key"</span><span>:</span> key<span>,</span>
  <span>}</span><span>)</span>
<span>}</span>
</code></pre>
<p>Looks good! We're wrapping the error and the wrapper function takes a second argument that a developer <em>must</em> fill with a <code>Fields</code> object.</p>
<p>And at the other end, we just need to easily pull out the data in a form that's simple to pass directly to a call to our logging library:</p>
<pre><code><span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
  metadata <span>:=</span> <span>GetErrorFields</span><span>(</span>err<span>)</span>
  log<span>.</span><span>WithFields</span><span>(</span>metadata<span>)</span><span>.</span><span>Error</span><span>(</span><span>"failed to handle request"</span><span>)</span>
  w<span>.</span><span>Write</span><span>(</span>http<span>.</span>StatusInternalServerError<span>)</span>
  <span>return</span>
<span>}</span>
</code></pre>
<p>Perfect! We now have structured errors, compatible with structured logging. It's low effort to use and the benefits when diagnosing issues are huge.</p>
<p>And I actually wrote this in early 2022 for <a target="_blank" href="https://joinodin.com/">Odin's</a> codebase because problems were becoming increasingly difficult to diagnose due to poor error messages. The logging helper was added to one slice of the application to try it out. It made diagnosing bugs both at development time and in production vastly simpler since I had decorated many of our error returns with additional contextual metadata to aid searching in Datadog.</p>
<p>But the problem here is there was still an immense amount of manual work required to add all these bits of metadata to error returns. One call to <code>Wrap</code> is fine but if you have a complex piece of business logic with 10 or more <code>if err != nil</code>'s scattered around, it suddenly becomes a lot of work and that hurts the ergonomics.</p>
<h2>Bridging the context gap</h2>
<p>I've been using this word <em>context</em> quite a bit already throughout this article. This word is defined by Oxford as:</p>
<blockquote>
<p>the circumstances that form the setting for an event, statement, or idea, and in terms of which it can be fully understood.</p>
</blockquote>
<p>Go's adequately named "context" library does exactly this for call trees. It's a tool that's widely used for cancelling or giving deadlines to long-running operations but it can also be used to store information and carry that information throughout a call tree.</p>
<p>Now this powerful functionality is not to be taken lightly. Contexts involve a few allocations when reading or writing information (many more than passing arguments or accessing receivers) and from a core software engineering standpoint, contexts can serve to hide information as it's essentially a black box. You cannot iterate contexts and print out their contents. You must <em>know</em> what you're looking for via a "context key."</p>
<p>So while I'm advocating for careful and considerate use of Go's context library, I'm also saying <a target="_blank" href="https://isburmistrov.substack.com/p/all-you-need-is-wide-events-not-metrics">stuff it full of as much metadata as you can find</a>. But seriously, as long as your actual business logic isn't dependent on the content of a context, I have found the tool quite useful for storing metadata about a call tree or an operation that can be accessed for non-business logic related operations such as logging and error management.</p>
<p>In reality, this looks like this:</p>
<pre><code>ctx <span>=</span> <span>WithMetadata</span><span>(</span>ctx<span>,</span> <span>"account_id"</span><span>,</span> accountID<span>)</span>

<span>...</span>

<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
  <span>return</span> <span>Wrap</span><span>(</span>err<span>,</span> ctx<span>)</span>
<span>}</span>

<span>...</span>

metadata <span>:=</span> <span>GetErrorFields</span><span>(</span>err<span>)</span>
<span>// map[string]any{"account_id": 59}</span>
</code></pre>
<p>Contexts only accrue information as they descend into a call tree. Once you return from a function where a context has been wrapped with any of the "With" APIs you often see in Go (<code>WithDeadline</code>, <code>WithTimeout</code> and my addition above, <code>WithMetadata</code>) that information is freed. So you can't do this:</p>
<pre><code><span>func</span> <span>Y</span><span>(</span>ctx context<span>.</span>Context<span>)</span> <span>{</span>
  ctx <span>=</span> context<span>.</span><span>WithValue</span><span>(</span>ctx<span>,</span> <span>"key"</span><span>,</span> <span>"value"</span><span>)</span>
  <span>...</span>
<span>}</span>

<span>func</span> <span>X</span><span>(</span>ctx context<span>.</span>Context<span>)</span> <span>{</span>
  <span>Y</span><span>(</span>ctx<span>)</span>

  ctx<span>.</span><span>Value</span><span>(</span><span>"key"</span><span>)</span> <span>// nothing, nil, zilch, it's gone</span>
<span>}</span>
</code></pre>
<p>This is because when you "wrap" a context, you're creating a copy which is only sticking around for the duration of that stack frame. Once you return from a function you lose your modified context. There's no two-way state manipulation (a good thing) so you can't decorate a call tree on the way down and then read that information back at the top.</p>
<h2>Errors are context in reverse</h2>
<p>Hot take. But if you squint a bit, errors are basically contexts but they operate on the ascending side of the call tree, not the descending side.</p>
<p>Contexts accrue information as they descend the call tree and errors accrue information as they ascend the call tree.</p>
<p>This means that, if you want to store structured metadata in a context, that information is only useful if you pass it to an error so that metadata can ride the call tree ascension like a rollercoaster all the way up to the top where it's actually useful.</p>
<p><img aria-describedby="image-caption-a4e06b75-e006-42f4-97d4-cde9609bef5d" src="https://southcla.ws/images/2024-04-07-17-08-55.png"></p>
<blockquote>
<p>The Von-Neumann architecture is basically a rollercoaster. You can either go up or down and usually, if you're going backwards there's a small chance shit hit the fan. (by <a target="_blank" href="https://unsplash.com/@mboulden?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Meg Boulden / Unsplash</a>)</p>
</blockquote>
<h2>Putting it all together</h2>
<p>Okay it's about time we had some code. We need a context key, a helper to top up a context with some metadata and another helper to get it out again.</p>
<pre><code><span>type</span> contextKey <span>struct</span><span>{</span><span>}</span>

<span>func</span> <span>WithMeta</span><span>(</span>ctx context<span>.</span>Context<span>,</span> kv <span>...</span><span>string</span><span>)</span> context<span>.</span>Context <span>{</span>
	<span>var</span> data <span>map</span><span>[</span><span>string</span><span>]</span><span>string</span>

	<span>// overwrite any existing context metadata</span>
	<span>if</span> meta<span>,</span> ok <span>:=</span> ctx<span>.</span><span>Value</span><span>(</span>contextKey<span>{</span><span>}</span><span>)</span><span>.</span><span>(</span><span>map</span><span>[</span><span>string</span><span>]</span><span>string</span><span>)</span><span>;</span> ok <span>{</span>
		data <span>=</span> meta
	<span>}</span> <span>else</span> <span>{</span>
		data <span>=</span> <span>make</span><span>(</span><span>map</span><span>[</span><span>string</span><span>]</span><span>string</span><span>)</span>
	<span>}</span>

	l <span>:=</span> <span>len</span><span>(</span>kv<span>)</span>
	<span>for</span> i <span>:=</span> <span>0</span><span>;</span> i <span>&lt;</span> l<span>;</span> i <span>+=</span> <span>2</span> <span>{</span>
		k <span>:=</span> kv<span>[</span>i<span>]</span>
		v <span>:=</span> kv<span>[</span>i<span>+</span><span>1</span><span>]</span>

		data<span>[</span>k<span>]</span> <span>=</span> v
	<span>}</span>

	<span>return</span> context<span>.</span><span>WithValue</span><span>(</span>ctx<span>,</span> contextKey<span>{</span><span>}</span><span>,</span> data<span>)</span>
<span>}</span>
</code></pre>
<p>Because we intend to call this multiple times during a call tree, any successive calls to wrapped contexts will already contain metadata. Now I've opted for an extremely simple approach of essentially copying all the fields, if there are any, into the newly wrapped context. This could be an opportunity for memory usage optimisation but that's for another day.</p>
<p>Now we need to wrap errors and store the context's metadata hash table into the error, simple stuff.</p>
<pre><code><span>type</span> withContext <span>struct</span> <span>{</span>
	cause <span>error</span>
	meta  <span>map</span><span>[</span><span>string</span><span>]</span><span>string</span>
<span>}</span>

<span>// Omitted: the error implementation that actually makes this an error.</span>

<span>func</span> <span>Wrap</span><span>(</span>err <span>error</span><span>,</span> ctx context<span>.</span>Context<span>)</span> <span>error</span> <span>{</span>
	meta<span>,</span> ok <span>:=</span> ctx<span>.</span><span>Value</span><span>(</span>contextKey<span>{</span><span>}</span><span>)</span><span>.</span><span>(</span><span>map</span><span>[</span><span>string</span><span>]</span><span>string</span><span>)</span>
	<span>if</span> <span>!</span>ok <span>{</span>
		<span>return</span> err
	<span>}</span>

	<span>return</span> <span>&amp;</span>withContext<span>{</span>err<span>,</span> meta<span>}</span>
<span>}</span>
</code></pre>
<p>And finally, a way to get the metadata information (or a zero-value) out of any error value.</p>
<pre><code><span>func</span> <span>Unwrap</span><span>(</span>err <span>error</span><span>)</span> <span>map</span><span>[</span><span>string</span><span>]</span><span>string</span> <span>{</span>
	values <span>:=</span> <span>map</span><span>[</span><span>string</span><span>]</span><span>string</span><span>{</span><span>}</span>

	<span>for</span> err <span>!=</span> <span>nil</span> <span>{</span>
		<span>if</span> f<span>,</span> ok <span>:=</span> err<span>.</span><span>(</span><span>*</span>withContext<span>)</span><span>;</span> ok <span>{</span>
			<span>if</span> m <span>:=</span> f<span>.</span>meta<span>;</span> m <span>!=</span> <span>nil</span> <span>{</span>
				<span>for</span> k<span>,</span> v <span>:=</span> <span>range</span> m <span>{</span>
					values<span>[</span>k<span>]</span> <span>=</span> v
				<span>}</span>
			<span>}</span>
		<span>}</span>

		err <span>=</span> errors<span>.</span><span>Unwrap</span><span>(</span>err<span>)</span>
	<span>}</span>

	<span>if</span> <span>len</span><span>(</span>values<span>)</span> <span>==</span> <span>0</span> <span>{</span>
		<span>return</span> <span>nil</span>
	<span>}</span>

	<span>return</span> values
<span>}</span>
</code></pre>
<p>Because errors in Go are essentially nested within each other like Russian dolls, we need to iterate the tree of errors by repeatedly calling <code>errors.Unwrap(err)</code> which is pretty much how many error management code works. We're basically searching through the chain for an instance of the type we're interested in.</p>
<p>Lots of error code does this repeated unwrapping, which sounds like it could be a performance concern in larger applications but there's an argument to be made that if your error handling code is your biggest performance concern then you need to either 1. stop causing so many errors or 2. use a different language!</p>
<h2>Things to watch out for</h2>
<p>Great power, great responsibility and all that. As with any kind of logging system that's moving information from your application to a third party you must <em>must</em> <strong>must</strong> avoid including any sort of PII (personally identifiable information) in your context, errors and logs unless you want to fail your SOC2 certification.</p>
<p>When decorating errors, the same rules as using logging applies. There are many great articles on this topic, but in summary, avoid:</p>
<ul>
<li>Email addresses</li>
<li>Names, even if just first name</li>
<li>Any form of institutional identifier such as social security, financial, healthcare numbers.</li>
<li>Any kind of public social media handle</li>
<li>(hopefully extremely obvious) bank information like credit card numbers or routing+account numbers</li>
</ul>
<p>Some tips for what metadata can be included and will help you when diagnosing issues:</p>
<ul>
<li>Anonymous internal unique identifiers of any kind (user IDs, account IDs, company IDs...) any of them you have available, stick them in your context. You'll thank yourself later.</li>
<li>Relevant timestamps. Obviously you have the timestamp of the log entry already but if you have any other timestamps available that are relevant to your business domain, include them so you can perform time-range queries.</li>
<li>Slugs are often useful if you have non-technical/QA looking at your logs. Slugs are human-readable URL friendly names often used for things like blog posts or public resources (like a GitHub repository or a customer's subdomain)</li>
</ul>
<p>And of course, you'll still run the risk of some developers using different naming conventions to other developers in key names, <code>user_id</code> vs <code>userID</code> for example. So establish a convention and stick to it. Use a centralised package of const strings and custom types to cause compiler errors if you need to. Make mistakes difficult.</p>
<h2>Here's one I made earlier</h2>
<blockquote>
<p>(brits older than 25 who used to watch cbbc, iykyk)</p>
</blockquote>
<p>Of course, it wouldn't be a technical article if I wasn't going to shill something I made. All of the above was essentially the experimental journey I went through to improve how errors work in a number of codebases I maintain and operate in production settings.</p>
<p>I've used those learnings to write (and rewrite about 3 times, based on feedback from many a Go engineer) a fairly simple but also pluggable error management library which includes the structured errors functionality discussed in this post as well as some other useful tools.</p>
<p><a target="_blank" href="https://github.com/Southclaws/fault">https://github.com/Southclaws/fault</a></p>
<p>Now, because of the way this library is architected, you can just use the error context utility called <a target="_blank" href="https://github.com/Southclaws/fault/blob/master/README.md#fctx">fctx</a> on its own as it satisfies the Fault wrapper interface as well as the commonly used <code>Wrap(error, stuff...)</code> pattern used in many libraries.</p>
<p>There will also be a future post diving deeper into some of the technical bits and rationale behind Fault and how I use it in production systems. So keep an eye out for that!</p>
<p>If you made it this far, thank you for reading! I hope this helps you navigate Go errors in larger applications. Feedback on the concepts and writing is greatly appreciated so <a target="_blank" href="https://twitter.com/Southclaws">feel free to tweet me</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RenderFormer: Neural rendering of triangle meshes with global illumination (226 pts)]]></title>
            <link>https://microsoft.github.io/renderformer/</link>
            <guid>44148524</guid>
            <pubDate>Sun, 01 Jun 2025 03:43:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://microsoft.github.io/renderformer/">https://microsoft.github.io/renderformer/</a>, See on <a href="https://news.ycombinator.com/item?id=44148524">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><nav><div><p><img src="https://microsoft.github.io/renderformer/imgs/rf.png" alt="RF"></p></div></nav><header></header><div id="intro"><h2>Introduction</h2><p>We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that <strong>does not require per-scene training or fine-tuning</strong>.</p><h3>Mesh to Image, End to End</h3><p>Instead of taking a physics-centric approach to rendering, we formulate rendering as a <strong>sequence-to-sequence transformation</strong> where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels.</p><h3>Simple Transformer Architecture with Minimal Prior Constraints</h3><p>RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. <strong>No rasterization, no ray tracing.</strong></p><p><img src="https://microsoft.github.io/renderformer/imgs/model-arch.jpg" alt="Model architecture"></p></div><div id="gallery"><h2>Rendering Gallery</h2><p>Examples of scenes rendered with RenderFormer demonstrating various lighting conditions, materials, and geometric complexity, without any per-scene training or fine-tuning. Check out the <a href="https://microsoft.github.io/renderformer/error-images">reference images</a> for more details.</p><div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox.png" alt="Cornell Box"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox-bunny.png" alt="Cornell Box with Bunny"></p><div><p>Stanford Bunny in Cornell Box</p></div></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox-lucy.png" alt="Cornell Box with Lucy"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/cbox-teapot.png" alt="Cornell Box with Teapot"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/compose-scene.png" alt="Composed Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/constant-width.png" alt="Constant Width Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/crystals.png" alt="Crystal Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/fox-in-the-wild.png" alt="Fox Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/horse-and-heart.png" alt="Horse and Heart"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/renderformer-logo.png" alt="RenderFormer Logo"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/room.png" alt="Room Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/shader-ball.png" alt="Shader Ball"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/tree.png" alt="Tree Scene"></p></div><div><p><img src="https://microsoft.github.io/renderformer/gallery/veach-mis.png" alt="Veach MIS Scene"></p></div></div></div><div id="videos"><h2>Videos</h2><p>Check out extra video results including <a href="https://microsoft.github.io/renderformer/uncompressed-videos">uncompressed videos</a> and <a href="https://microsoft.github.io/renderformer/reference-videos">some reference videos</a>.</p><div><h3>Teaser Scenes</h3><p>Dynamic demonstrations of RenderFormer's capabilities, showing object rotations, lighting changes, and material adjustments.</p><div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/teaser-scenes/cbox-roughness.mp4" type="video/mp4"></video><div><p>Cornell Box Roughness Adjustment</p></div></div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/teaser-scenes/cbox-bunny-roughness.mp4" type="video/mp4"></video><div><p>Bunny Roughness Adjustment</p></div></div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/teaser-scenes/compose-change-light.mp4" type="video/mp4"></video><div><p>Composed Scene View Change</p></div></div></div></div><div><h3>Animations</h3><p>RenderFormer can render animations of scenes.</p></div><div><h3>Physical-Based Simulations</h3><p>RenderFormer can render physically simulated scenes with complex dynamics and interactions.</p><div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/simulations/bowling.mp4" type="video/mp4"></video><div><p>Bowling Ball Physics Simulation</p></div></div><div><video autoplay="" muted="" loop="" playsinline=""><source src="https://microsoft.github.io/renderformer/videos/simulations/constant-width-sim.mp4" type="video/mp4"></video><div><p>Constant Width Body Simulation</p></div></div></div></div></div><div id="BibTeX"><h2>BibTeX</h2><div><pre><code>@inproceedings {zeng2025renderformer,
    title      = {RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination},
    author     = {Chong Zeng and Yue Dong and Pieter Peers and Hongzhi Wu and Xin Tong},
    booktitle  = {ACM SIGGRAPH 2025 Conference Papers},
    year       = {2025}
}</code></pre></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stepping Back (125 pts)]]></title>
            <link>https://rjp.io/blog/2025-05-31-stepping-back</link>
            <guid>44147966</guid>
            <pubDate>Sun, 01 Jun 2025 01:04:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rjp.io/blog/2025-05-31-stepping-back">https://rjp.io/blog/2025-05-31-stepping-back</a>, See on <a href="https://news.ycombinator.com/item?id=44147966">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

<section>
<p>The other day I was playing around with Claude Code, experimenting with porting
some C code to Rust - not for any particular reason, just because I was curious
how well it could do. As these things happen, I got more and more invested in
the process, instead of just letting Claude do something and noting what
happened, I kept poking in and adjusting the behavior, "no you can't pretend the
result is 0, try again" - the usual stuff when you're trying to get an LLM to do
something large or novel.</p>
<p>But I got so invested in seeing the port of the module work that I forgot that
my original goal was to see how well the LLM could do on its own. Instead of
testing a potentially interesting hypothesis, I was poorly porting a module of a
project that I had no interest in continuing. I was only rescued in this case
from my pains by the LLM itself: Claude hit some kind of rate-limiting
error, forcing me to take a break from the project.</p>
<p>Once I was forced away, I found myself posessed with a sudden clarity: why on
earth had I 5 minutes ago become so invested in this particular task?  Given the
forced break, my brain "reset" and I could step away from it, and go back to
thinking about the original problem and what my goals and conclusions were. <em>But
there was no way in the moment I could do so.</em> I just couldn't let it go.</p>
<p>This isn't of course the first time that this has happened. Sometimes the breaks
are imposed: my wife and I are off to get lunch, and while walking to the door I
realize the last 2 hours spent feverishly trying to get some RPC from A to B to
work are actually pointless and there's some much simpler solution. Other times
I find myself sitting back and catching myself getting too deep into a problem.</p>
<p>Of course, in any big enough problem, you're never sure what to do initially.
You have to just... explore, and correct yourself if you're off. There's an
unresolvable tension between trying to get through the problem at hand and
contemplating "maybe this isn't the right direction after all". At least I can't
seem to hold both of them in my head at the same time: if you're trying to debug
some subtle problem involving 10 layers of software and hardware, you can't
afford the brain space for questioning whether you'd be better off trying a
different approach. But even more, the tenancity that makes you stick on
problems, it's almost the thing that makes you a good engineer. You need to be
stubborn and willing to push through stupid problems in order to get things
done.</p>
<p>I don't think there's an easy solution: how do you know when you're in too deep?
In RL we'd probably characterize this as a variation on the
<a href="https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma">Exploration/Explotation dilemma</a></p>
<ul>
<li>how much energy should we invest into getting "just this next bug fixed and
working" versus taking a sip of tea and then you should change your approach or change your job?</li>
</ul>
<p>Spending too much time fixating on one thing leads you towards a sort of
monomanical obsession with it: "functional programming is the only way" sort of
thing. On the other hand, you can fixate the other way and spend all your time
contemplating what the best thing is to do, leading to a weird sort of nihilism
as you question everything while you watch life go by. And yet stepping away
from our problems can be such a powerful tool for us, giving us the space to
understand if we're doing the right thing: you can't see the forest for the
trees and all that.</p>
<p>Recently I've found myself adopting a sort of ritual for stepping back and
reflecting. I find if I align it to natural time boundaries: hours, days, weeks,
months, years, I can sort of set a mental schedule for myself and it's not too
much of a distraction. When I take a break, I can ask myself the usual
questions: "what am I doing?", "why am I doing it?", "what could I be doing
instead?". I find this can be cathartic as well, letting me note progress as
well as struggles.</p>
<p>(It should be taken for given that of course it's really hard to switch your
mindset from "debugging this thing is really important" to "should I debug this
thing at all", <em>while you're in the middle of debugging said thing</em>. But you
know, you just try your best, sometimes your brain refuses to let go, and other
times, maybe you get enough space to decide going off for a beer is the better
bet.)</p>
<p>Of course, the amount of time deliberating and the scope of the questions
changes with the time duration: I don't want to question why I'm working at my
job 8 times at day, but maybe once a week is okay? Conversely, I can afford 1
minute an hour to jot down what I'm trying to do: I'm not contemplating throwing
away a project at that point, but maybe the scope of a feature or how I'm
debugging an issue.</p>
<p>But once a year, well then maybe it's worth taking a day off from everything and
contemplating where you're going in life: are you happy with your decisions? In
a way you're paying 1% of your time and energy as a sort of insurance against
wandering too far in a bad direction. Not a bad trade.</p>

</section>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Progressive JSON (411 pts)]]></title>
            <link>https://overreacted.io/progressive-json/</link>
            <guid>44147945</guid>
            <pubDate>Sun, 01 Jun 2025 00:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overreacted.io/progressive-json/">https://overreacted.io/progressive-json/</a>, See on <a href="https://news.ycombinator.com/item?id=44147945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Do you know about Progressive JPEGs? Here’s a <a target="_blank" href="https://www.liquidweb.com/blog/what-is-a-progressive-jpeg/">nice explanation</a> of what a Progressive JPEG is. The idea is that instead of loading the image top to bottom, the image instead is fuzzy at first and then progressively becomes more crisp.</p>
<p>What if we apply the same idea to transferring JSON?</p>
<p>Suppose you have a JSON tree with some data:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>'</span><span>Welcome to my blog</span><span>'</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>'</span><span>This is my article</span><span>'</span><span>,</span></span>
<span data-line=""><span>    comments: </span><span>[</span></span>
<span data-line=""><span>      '</span><span>First comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      '</span><span>Second comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      // ...</span></span>
<span data-line=""><span>    ]</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>'</span><span>Hope you like it</span><span>'</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Now imagine you want to transfer it over the wire. Because the format is JSON, you’re not going to have a valid object tree until the last byte loads. You have to wait for the <em>entire</em> thing to load, then call <code>JSON.parse</code>, and then process it.</p>
<p>The client can’t do <em>anything</em> with JSON until the server sends the <em>last</em> byte. If a part of the JSON was slow to generate on the server (e.g. loading <code>comments</code> took a slow database trip), <strong>the client can’t <em>start any</em> work until the server <em>finishes all</em> the work.</strong></p>
<p>Would you call that good engineering? And yet it’s the status quo—that’s how 99.9999%<sup>*</sup> of apps send and process JSON. Do we dare to improve on that?</p>
<p><small>* I made it up</small></p>
<hr>
<h3 id="streaming-json"><a target="_self" href="#streaming-json">Streaming JSON</a></h3>
<p>We can try to improve this by implementing a <em>streaming</em> JSON parser. A streaming JSON parser would be able to produce an object tree from an incomplete input:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>'</span><span>Welcome to my blog</span><span>'</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>'</span><span>This is my article</span><span>'</span><span>,</span></span>
<span data-line=""><span>    comments: </span><span>[</span></span>
<span data-line=""><span>      '</span><span>First comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      '</span><span>Second comment</span><span>'</span></span></code></pre></figure>
<p>If you ask for the result at this point, a streaming parser would hand you this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>'</span><span>Welcome to my blog</span><span>'</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>'</span><span>This is my article</span><span>'</span><span>,</span></span>
<span data-line=""><span>    comments: </span><span>[</span></span>
<span data-line=""><span>      '</span><span>First comment</span><span>'</span><span>,</span></span>
<span data-line=""><span>      '</span><span>Second comment</span><span>'</span></span>
<span data-line=""><span>      // (The rest of the comments are missing)</span></span>
<span data-line=""><span>    ]</span></span>
<span data-line=""><span>  }</span></span>
<span data-line=""><span>  // (The footer property is missing)</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>However, this isn’t too great either.</p>
<p>One downside of this approach is that the objects are kind of malformed. For example, the top-level object was supposed to have three properties (<code>header</code>, <code>post</code>, and <code>footer</code>), but the <code>footer</code> is missing because it hasn’t appeared in the stream yet. The <code>post</code> was supposed to have three <code>comments</code>, but you <em>can’t actually tell</em> whether more <code>comments</code> are coming or if this was the last one.</p>
<p>In a way, this is inherent to streaming—didn’t we <em>want</em> to get incomplete data?—but <strong>this makes it very difficult to actually <em>use</em> this data on the client.</strong> None of the types “match up” due to missing fields. We don’t know what’s complete and what’s not. That’s why streaming JSON isn’t popular aside from niche use cases. It’s just too hard to actually take advantage of it in the application logic which generally assumes the types are correct, “ready” means “complete”, and so on.</p>
<p>In the analogy with JPEG, this naïve approach to streaming matches the default “top-down” loading mechanism. The picture you see is crisp but you only see the top 10%. So despite the high fidelity, you don’t actually see <em>what’s</em> on the picture.</p>
<p>Curiously, this is also how streaming <em>HTML itself</em> works by default. If you load an HTML page on a slow connection, it will be streamed in the document order:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;</span><span>This is my article</span><span>&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>ul</span><span> class=</span><span>"</span><span>comments</span><span>"</span><span>&gt;</span></span>
<span data-line=""><span>          &lt;</span><span>li</span><span>&gt;</span><span>First comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line=""><span>          &lt;</span><span>li</span><span>&gt;</span><span>Second comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span></code></pre></figure>
<p>This has some upsides—the browser is able to display the page partially—but it has the same issues. The cutoff point is arbitrary and can be visually jarring or even mess up the page layout. It’s unclear if more content is coming. Whatever’s below—like the footer—is cut off, even if it <em>was</em> ready on the server and <em>could</em> have been sent earlier. When we stream data <em>in order</em>, <strong>one slow part delays <em>everything</em>.</strong></p>
<p>Let’s repeat that: when we stream things in order they appear, a <em>single</em> slow part delays <em>everything</em> that comes after it. Can you think of some way to fix this?</p>
<hr>
<h3 id="progressive-json"><a target="_self" href="#progressive-json">Progressive JSON</a></h3>
<p>There is another way to approach streaming.</p>
<p>So far we’ve been sending things <em>depth-first</em>. We start with the top-level object’s properties, we go into that object’s <code>post</code> property, then we go into <em>that</em> object’s <code>comments</code> property, and so on. If something is slow, everything else gets held up.</p>
<p>However, we could also send data <em>breadth-first</em>.</p>
<p>Suppose we send the top-level object like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Here, <code>"$1"</code>, <code>"$2"</code>, <code>"$3"</code> refer to pieces of information that <em>have not been sent yet</em>. These are placeholders that can progressively be filled in later in the stream.</p>
<p>For example, suppose the server sends a few more rows of data to the stream:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $1 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $3 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>Hope you like it</span><span>"</span></span></code></pre></figure>
<p>Notice that we’re not obligated to send the rows in any particular order. In the above example, we’ve just sent both <code>$1</code> and <code>$3</code>—but the <code>$2</code> row is still pending!</p>
<p>If the client tried to reconstruct the tree at this point, it could look like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>We’ll represent the parts that haven’t loaded yet as Promises.</p>
<p>Then suppose the server could stream in a few more rows:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line=""><span>/* $3 */</span></span>
<span data-line=""><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $2 */</span></span>
<span data-line="" data-highlighted-line=""><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>  content: </span><span>"</span><span>$4</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  comments: </span><span>"</span><span>$5</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $4 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is my article</span><span>"</span></span></code></pre></figure>
<p>This would “fill in” some of the missing pieces from the client’s perspective:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  post: </span><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line="" data-highlighted-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>The Promise for the <code>post</code> would now resolve to an object. However, we still don’t know what’s inside the <code>comments</code>, so now <em>those</em> are represented as a Promise.</p>
<p>Finally, the comments could stream in:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line=""><span>/* $3 */</span></span>
<span data-line=""><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>/* $2 */</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  content: </span><span>"</span><span>$4</span><span>"</span><span>,</span></span>
<span data-line=""><span>  comments: </span><span>"</span><span>$5</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $4 */</span></span>
<span data-line=""><span>"</span><span>This is my article</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $5 */</span></span>
<span data-line="" data-highlighted-line=""><span>[</span><span>"</span><span>$6</span><span>"</span><span>,</span><span> "</span><span>$7</span><span>"</span><span>,</span><span> "</span><span>$8</span><span>"</span><span>]</span></span>
<span data-line="" data-highlighted-line=""><span>/* $6 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is the first comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $7 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is the second comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>/* $8 */</span></span>
<span data-line="" data-highlighted-line=""><span>"</span><span>This is the third comment</span><span>"</span></span></code></pre></figure>
<p>Now, from the client’s perspective, the entire tree would be complete:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>[</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the first comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the second comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the third comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>    ]</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>By sending data breadth-first in chunks, we gained the ability to progressively handle it on the client. As long as the client can deal with some parts being “not ready” (represented as Promises) and process the rest, this is an improvement!</p>
<hr>
<h3 id="inlining"><a target="_self" href="#inlining">Inlining</a></h3>
<p>Now that we have the basic mechanism, we’ll adjust it for more efficient output. Let’s have another look at the entire streaming sequence from the last example:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$2</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>$3</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>"</span><span>Welcome to my blog</span><span>"</span></span>
<span data-line=""><span>/* $3 */</span></span>
<span data-line=""><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>/* $2 */</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  content: </span><span>"</span><span>$4</span><span>"</span><span>,</span></span>
<span data-line=""><span>  comments: </span><span>"</span><span>$5</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $4 */</span></span>
<span data-line=""><span>"</span><span>This is my article</span><span>"</span></span>
<span data-line=""><span>/* $5 */</span></span>
<span data-line=""><span>[</span><span>"</span><span>$6</span><span>"</span><span>,</span><span> "</span><span>$7</span><span>"</span><span>,</span><span> "</span><span>$8</span><span>"</span><span>]</span></span>
<span data-line=""><span>/* $6 */</span></span>
<span data-line=""><span>"</span><span>This is the first comment</span><span>"</span></span>
<span data-line=""><span>/* $7 */</span></span>
<span data-line=""><span>"</span><span>This is the second comment</span><span>"</span></span>
<span data-line=""><span>/* $8 */</span></span>
<span data-line=""><span>"</span><span>This is the third comment</span><span>"</span></span></code></pre></figure>
<p>We may have gone a <em>little</em> too far with streaming here. Unless generating some parts actually <em>is</em> slow, we don’t gain anything from sending them as separate rows.</p>
<p>Suppose that we have two different slow operations: loading a post and loading a post’s comments. In that case, it would make sense to send three chunks in total.</p>
<p>First, we would send the outer shell:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>On the client, this would immediately become:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Then we’d send the <code>post</code> data (but without the <code>comments</code>):</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $1 */</span></span>
<span data-line="" data-highlighted-line=""><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>  content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  comments: </span><span>"</span><span>$2</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>}</span></span></code></pre></figure>
<p>From the client’s perspective:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  post: </span><span>{</span></span>
<span data-line="" data-highlighted-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>new</span><span> Promise</span><span>(</span><span>/* ... not yet resolved ... */</span><span>),</span></span>
<span data-line="" data-highlighted-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Finally, we’d send the comments in a single chunk:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>"</span><span>$1</span><span>"</span><span>,</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>/* $1 */</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line=""><span>  comments: </span><span>"</span><span>$2</span><span>"</span></span>
<span data-line=""><span>}</span></span>
<span data-line="" data-highlighted-line=""><span>/* $2 */</span></span>
<span data-line="" data-highlighted-line=""><span>[</span></span>
<span data-line="" data-highlighted-line=""><span>  "</span><span>This is the first comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  "</span><span>This is the second comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>  "</span><span>This is the third comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>]</span></span></code></pre></figure>
<p>That would give us the whole tree on the client:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>{</span></span>
<span data-line=""><span>  header: </span><span>"</span><span>Welcome to my blog</span><span>"</span><span>,</span></span>
<span data-line=""><span>  post: </span><span>{</span></span>
<span data-line=""><span>    content: </span><span>"</span><span>This is my article</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>    comments: </span><span>[</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the first comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the second comment</span><span>"</span><span>,</span></span>
<span data-line="" data-highlighted-line=""><span>      "</span><span>This is the third comment</span><span>"</span></span>
<span data-line="" data-highlighted-line=""><span>    ]</span></span>
<span data-line=""><span>  },</span></span>
<span data-line=""><span>  footer: </span><span>"</span><span>Hope you like it</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>This is more compact and achieves the same purpose.</p>
<p>In general, this format gives us leeway to decide when to send things as a single chunks vs. multiple chunks. As long as the client is resilient to chunks arriving out-of-order, the server can pick different batching and chunking heuristics.</p>
<hr>
<h3 id="outlining"><a target="_self" href="#outlining">Outlining</a></h3>
<p>One interesting consequence of this approach is that it <em>also</em> gives us a natural way to reduce repetition in the output stream. If we’re serializing an object we’ve already seen before, we can just outline it as a separate row, and reuse it.</p>
<p>For example, suppose we have an object tree like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>const </span><span>userInfo</span><span> =</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> };</span></span>
<span data-line=""> </span>
<span data-line=""><span>[</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>header</span><span>'</span><span>,</span><span> user</span><span>:</span><span> userInfo </span><span>},</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>sidebar</span><span>'</span><span>,</span><span> user</span><span>:</span><span> userInfo </span><span>},</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>footer</span><span>'</span><span>,</span><span> user</span><span>:</span><span> userInfo </span><span>}</span></span>
<span data-line=""><span>]</span></span></code></pre></figure>
<p>If we were to serialize it to plain JSON, we’d end up repeating <code>{ name: 'Dan' }</code>:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>[</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>header</span><span>'</span><span>,</span><span> user</span><span>:</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> }</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>sidebar</span><span>'</span><span>,</span><span> user</span><span>:</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> }</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>footer</span><span>'</span><span>,</span><span> user</span><span>:</span><span> {</span><span> name</span><span>:</span><span> '</span><span>Dan</span><span>'</span><span> }</span><span> }</span></span>
<span data-line=""><span>]</span></span></code></pre></figure>
<p>However, if we’re serving JSON progressively, we could choose to outline it:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>[</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>header</span><span>'</span><span>,</span><span> user</span><span>:</span><span> "</span><span>$1</span><span>"</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>sidebar</span><span>'</span><span>,</span><span> user</span><span>:</span><span> "</span><span>$1</span><span>"</span><span> },</span></span>
<span data-line=""><span>  {</span><span> type</span><span>:</span><span> '</span><span>footer</span><span>'</span><span>,</span><span> user</span><span>:</span><span> "</span><span>$1</span><span>"</span><span> }</span></span>
<span data-line=""><span>]</span></span>
<span data-line="" data-highlighted-line=""><span>/* $1 */</span></span>
<span data-line="" data-highlighted-line=""><span>{</span><span> name: </span><span>"</span><span>Dan</span><span>"</span><span> }</span></span></code></pre></figure>
<p>We could also pursue a more balanced strategy—for example, to inline objects by default (for compactness) until we see some object being used two or more times, at which point we’ll emit it separately and dedupe the rest of them in the stream.</p>
<p>This also means that, unlike with plain JSON, we can support serializing cyclic objects. A cyclic object just has a property that points to its own stream “row”.</p>
<hr>
<h3 id="streaming-data-vs-streaming-ui"><a target="_self" href="#streaming-data-vs-streaming-ui">Streaming Data vs Streaming UI</a></h3>
<p>The approach described above is essentially how React Server Components work.</p>
<p>Suppose you write a page with React Server Components:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>function</span><span> Page</span><span>()</span><span> {</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>Post</span><span> /&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Post</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>post</span><span> =</span><span> await </span><span>loadPost</span><span>();</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;{</span><span>post</span><span>.</span><span>text</span><span>}&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>Comments</span><span> /&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Comments</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>comments</span><span> =</span><span> await </span><span>loadComments</span><span>();</span></span>
<span data-line=""><span>  return</span><span> &lt;</span><span>ul</span><span>&gt;{</span><span>comments</span><span>.</span><span>map</span><span>(</span><span>c </span><span>=&gt;</span><span> &lt;</span><span>li</span><span> key={</span><span>c</span><span>.</span><span>id</span><span>}&gt;{</span><span>c</span><span>.</span><span>text</span><span>}&lt;/</span><span>li</span><span>&gt;)}&lt;/</span><span>ul</span><span>&gt;;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>React will serve the output of the <code>Page</code> as a progressive JSON stream. On the client, it will be reconstructed as a progressively loaded React tree.</p>
<p>Initially, the React tree on the client will appear like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>    {new</span><span> Promise</span><span>(</span><span>/* ... not resolved yet */</span><span>)}</span></span>
<span data-line=""><span>    &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></figure>
<p>Then, as <code>loadPost</code> resolves on the server, more will stream in:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;</span><span>p</span><span>&gt;</span><span>This is my post</span><span>&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      {new</span><span> Promise</span><span>(</span><span>/* ... not resolved yet */</span><span>)}</span></span>
<span data-line="" data-highlighted-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></figure>
<p>Finally, when <code>loadComments</code> resolves on the server, the client receives the rest:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line=""><span>&lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;</span><span>This is my post</span><span>&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;</span><span>ul</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>        &lt;</span><span>li</span><span> key=</span><span>"</span><span>1</span><span>"</span><span>&gt;</span><span>This is the first comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>        &lt;</span><span>li</span><span> key=</span><span>"</span><span>2</span><span>"</span><span>&gt;</span><span>This is the second comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>        &lt;</span><span>li</span><span> key=</span><span>"</span><span>3</span><span>"</span><span>&gt;</span><span>This is the third comment</span><span>&lt;/</span><span>li</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;/</span><span>ul</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>  &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>&lt;/</span><span>html</span><span>&gt;</span></span></code></pre></figure>
<p>However, here’s the kicker.</p>
<p>You don’t actually <em>want</em> the page to jump arbitrarily as the data streams in. For example, maybe you never want to show the page <em>without</em> the post’s content.</p>
<p><strong>This is why React doesn’t display “holes” for pending Promises. Instead, it displays the closest declarative loading state, indicated by <a target="_blank" href="https://react.dev/reference/react/Suspense"><code>&lt;Suspense&gt;</code></a>.</strong></p>
<p>In the above example, there are no <code>&lt;Suspense&gt;</code> boundaries in the tree. This means that, although React will receive the <em>data</em> as a stream, it will not actually display a “jumping” page to the user. It will wait for the <em>entire</em> page to be ready.</p>
<p>However, you can <em>opt into</em> a progressively revealed loading state by wrapping a part of the UI tree into <code>&lt;Suspense&gt;</code>. This doesn’t change how the data is sent (it’s still as “streaming” as possible), but it changes <em>when</em> React reveals it to the user.</p>
<p>For example:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="js" data-theme="Overnight"><code data-language="js" data-theme="Overnight"><span data-line="" data-highlighted-line=""><span>import</span><span> {</span><span> Suspense </span><span>}</span><span> from</span><span> '</span><span>react</span><span>'</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>function</span><span> Page</span><span>()</span><span> {</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>header</span><span>&gt;</span><span>Welcome to my blog</span><span>&lt;/</span><span>header</span><span>&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>Post</span><span> /&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>footer</span><span>&gt;</span><span>Hope you like it</span><span>&lt;/</span><span>footer</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;/</span><span>body</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>html</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Post</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>post</span><span> =</span><span> await </span><span>loadPost</span><span>();</span></span>
<span data-line=""><span>  return</span><span> (</span></span>
<span data-line=""><span>    &lt;</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>      &lt;</span><span>p</span><span>&gt;{</span><span>post</span><span>.</span><span>text</span><span>}&lt;/</span><span>p</span><span>&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;</span><span>Suspense</span><span> fallback={&lt;</span><span>CommentsGlimmer</span><span> /&gt;}&gt;</span></span>
<span data-line=""><span>        &lt;</span><span>Comments</span><span> /&gt;</span></span>
<span data-line="" data-highlighted-line=""><span>      &lt;/</span><span>Suspense</span><span>&gt;</span></span>
<span data-line=""><span>    &lt;/</span><span>article</span><span>&gt;</span></span>
<span data-line=""><span>  );</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> function</span><span> Comments</span><span>()</span><span> {</span></span>
<span data-line=""><span>  const </span><span>comments</span><span> =</span><span> await </span><span>loadComments</span><span>();</span></span>
<span data-line=""><span>  return</span><span> &lt;</span><span>ul</span><span>&gt;{</span><span>comments</span><span>.</span><span>map</span><span>(</span><span>c </span><span>=&gt;</span><span> &lt;</span><span>li</span><span> key={</span><span>c</span><span>.</span><span>id</span><span>}&gt;{</span><span>c</span><span>.</span><span>text</span><span>}&lt;/</span><span>li</span><span>&gt;)}&lt;/</span><span>ul</span><span>&gt;;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>Now the user will perceive the loading sequence in two stages:</p>
<ul>
<li>First, the post “pops in” together with the header, the footer, and a glimmer for comments. The header and the footer never appear on their own.</li>
<li>Then, the comments “pop in” on their own.</li>
</ul>
<p><strong>In other words, the stages in which the UI gets revealed are decoupled from how the data arrives. The data is streamed as it becomes available, but we only want to <em>reveal</em> things to the user according to intentionally designed loading states.</strong></p>
<p>In a way, you can see those Promises in the React tree acting almost like a <code>throw</code>, while <code>&lt;Suspense&gt;</code> acts almost like a <code>catch</code>. The data arrives as fast as it can in whatever order the server is ready to send it, but React takes care to present the loading sequence gracefully and let the developer control the visual reveal.</p>
<p>Note that what I described so far has nothing to do with “SSR” or HTML. I was describing a general mechanism for streaming a UI tree represented as JSON. You can <em>turn</em> that JSON tree into progressively revealed HTML (and <a target="_blank" href="https://gal.hagever.com/posts/out-of-order-streaming-from-scratch">React can do that</a>), but the idea is broader than HTML and applies to SPA-like navigations as well.</p>
<hr>
<h3 id="in-conclusion"><a target="_self" href="#in-conclusion">In Conclusion</a></h3>
<p>In this post, I’ve sketched out one of the core innovations of RSC. Instead of sending data as a single big chunk, it sends the props for your component tree outside-in. As a result, as soon as there’s an intentional loading state to display, React can do that while the rest of the data for your page is being streamed in.</p>
<p>I’d like to challenge more tools to adopt progressive streaming of data. If you have a situation where you can’t <em>start</em> doing something on the client until the server <em>stops</em> doing something, that’s a clear example of where streaming can help. If a <em>single slow thing</em> can slow down <em>everything after it,</em> that’s another warning sign.</p>
<p>Like I showed in this post, streaming <em>alone</em> is not enough—you also need a programming model that can <em>take advantage</em> of streaming and gracefully handle incomplete information. React solves that with intentional <code>&lt;Suspense&gt;</code> loading states. If you know systems that solve this differently, I’d love to hear about them!</p><p><a href="https://ko-fi.com/gaearon" target="_blank"><span></span>Pay what you like</a></p><hr><p><a target="_blank" href="https://bsky.app/search?q=https%3A%2F%2Foverreacted.io%2Fprogressive-json%2F">Discuss on Bluesky</a>&nbsp;&nbsp;·&nbsp;&nbsp;<a target="_blank" href="https://github.com/gaearon/overreacted.io/edit/main/public/progressive-json/index.md">Edit on GitHub</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Patio – Rent tools, learn DIY, reduce waste (181 pts)]]></title>
            <link>https://patio.so</link>
            <guid>44147803</guid>
            <pubDate>Sun, 01 Jun 2025 00:17:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://patio.so">https://patio.so</a>, See on <a href="https://news.ycombinator.com/item?id=44147803">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The NFS 4 Freezer Spacer In Science Fiction Sets (104 pts)]]></title>
            <link>https://kolektiva.social/@beka_valentine/114600567753999701</link>
            <guid>44147631</guid>
            <pubDate>Sat, 31 May 2025 23:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kolektiva.social/@beka_valentine/114600567753999701">https://kolektiva.social/@beka_valentine/114600567753999701</a>, See on <a href="https://news.ycombinator.com/item?id=44147631">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[New adaptive optics shows details of our star's atmosphere (141 pts)]]></title>
            <link>https://nso.edu/press-release/new-adaptive-optics-shows-stunning-details-of-our-stars-atmosphere/</link>
            <guid>44147573</guid>
            <pubDate>Sat, 31 May 2025 23:08:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nso.edu/press-release/new-adaptive-optics-shows-stunning-details-of-our-stars-atmosphere/">https://nso.edu/press-release/new-adaptive-optics-shows-stunning-details-of-our-stars-atmosphere/</a>, See on <a href="https://news.ycombinator.com/item?id=44147573">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2><i>Scientists develop new optical system that removes blur over fine-structure in the Sun’s corona, revealing clearest images to date</i></h2>
<p><b>BOULDER, CO, Tuesday, May 27, 2025 </b><b><i>– </i></b>The Sun’s corona—the outermost layer of its atmosphere, visible only during a total solar eclipse—has long intrigued scientists due to its extreme temperatures, violent eruptions, and large prominences. However, turbulence in the Earth’s atmosphere has caused image blur and hindered observations of the corona. A ground-breaking recent development by scientists from the U.S. National Science Foundation (NSF) National Solar Observatory (NSO), and New Jersey Institute of Technology (NJIT), is changing that by using adaptive optics to remove the blur.</p>
<p>As published in <a href="https://www.nature.com/articles/s41550-025-02564-0" target="_blank" rel="noopener"><i>Nature Astronomy</i></a>, this pioneering ‘coronal adaptive optics’ technology has produced the most astonishing, clearest images and videos of fine-structure in the corona to date. This development will open the door for deeper insights into the corona’s enigmatic behavior and the processes driving space weather.</p>
<h3><b>Most Detailed Coronal Images to Date Revealed</b><b></b></h3>
<p>Funded by the NSF and installed at the 1.6-meter Goode Solar Telescope (GST), operated by NJIT’s Center for Solar-Terrestrial Research (CSTR) at Big Bear Solar Observatory (BBSO) in California, “Cona”—the adaptive optics system responsible for these new images—compensates for the blur caused by air turbulence in the Earth’s atmosphere —similar to the bumpy air passengers feel during a flight.</p>
<p><i>“The turbulence in the air severely degrades images of objects in space, like our Sun, seen through our telescopes. But we can correct for that,”</i> says Dirk Schmidt, NSO Adaptive Optics Scientist who led the development.</p>
<p>Among the team’s remarkable observations is a movie of a quickly restructuring solar prominence unveiling fine, turbulent internal flows. Solar prominences are large, bright features, often appearing as arches or loops, extending outward from the Sun’s surface.</p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-1392942-1" width="1080" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Video.mp4?_=1"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Prominence-1-Video.mp4</a></video></p>
<p>This image of a prominence above the solar surface is a snapshot of a 4-minute time-lapse movie that reveals its rapid, fine, and turbulent restructuring with unprecedented detail. The Sun’s fluffy-looking surface is covered by “spicules”, short-lived plasma jets, whose creation is still the subject of scientific debate. The streaks on the right of this image are coronal rain falling down onto the Sun’s surface. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>

<p>A second movie replays the rapid formation and collapse of a finely structured plasma stream. <i>“These are by far the most detailed observations of this kind, showing features not previously observed, and it’s not quite clear what they are,”</i> says Vasyl Yurchyshyn, co-author of the study and NJIT-CSTR research professor. “<i>It is super exciting to build an instrument that shows us the Sun like never before,</i>” Schmidt adds.</p>
<p><video id="video-1392942-2" width="1080" height="739" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Video.mp4?_=2"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Plasmoid-Video.mp4</a></video></p>
<p>This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light.<br>
<b>Credit:</b> Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>

<p>A third movie shows fine strands of coronal rain—a phenomenon where cooling plasma condenses and falls back toward the Sun’s surface. “<i>Raindrops in the Sun’s corona can be narrower than 20 kilometers,” </i>NSO Astronomer Thomas Schad concludes from the most detailed images of coronal rain to date, “<i>These findings offer new invaluable observational insight that is vital to test computer models of coronal processes.</i>”</p>
<p><video id="video-1392942-3" width="1080" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4?_=3"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4</a></video></p>
<p>Coronal rain forms when hotter plasma in the Sun’s corona cools down and becomes denser. Like raindrops on Earth, coronal rain is pulled down to the surface by gravity. Because the plasma is electrically charged, it follows the magnetic field lines, which make huge arches/loops, instead of falling in a straight line.</p>
<p>This image is a snapshot from a 23-minute time-lapse video which is comprised of the highest resolution images ever made of coronal rain. The scientists show in the paper that the strands can be narrower than 20 kilometers.</p>
<p>This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light.<br>
<b>Credit: </b>Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>
<p>Another movie shows the dramatic motion of a solar prominence being shaped by the Sun’s magnetism.</p>
<p><video id="video-1392942-4" width="1080" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Video.mp4?_=4"><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Video.mp4">https://nso.edu/wp-content/uploads/2025/05/Prominence-2-Video.mp4</a></video></p>
<p>This image of a solar prominence is a snapshot of a 19-minute time-lapse movie showing how plasma “dances” and twists with the Sun’s magnetic field.</p>
<p>This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light.<br>
<b>Credit:</b> Schmidt et al./NJIT/NSO/AURA/NSF</p>
<hr>
<h3><b>A Breakthrough in Solar Adaptive Optics</b><b></b></h3>
<p>The corona is heated to millions of degrees–much hotter than the Sun’s surface–by mechanisms unknown to scientists. It is also home to dynamic phenomena of much cooler solar plasma that appears reddish-pink during eclipses. Scientists believe that resolving the structure and dynamics of the cooler plasma at small scales holds a key to answering the coronal heating mystery and improving our understanding of eruptions that eject plasma into space driving space weather—i.e., the conditions in Earth’s near-space environment primarily influenced by the Sun’s activity (e.g.,&nbsp;solar flares,&nbsp;coronal mass ejections,&nbsp;and the solar wind) that can impact technology and systems on Earth and in space. The precision required demands large telescopes and adaptive optics systems like the one developed by this team.</p>
<p>The GST system Cona uses a mirror that continuously reshapes itself 2,200 times per second to counteract the image degradation caused by turbulent air. “<i>Adaptive optics is like a pumped-up autofocus and optical image stabilization in your smartphone camera, but correcting for the errors in the atmosphere rather than the user’s shaky hands</i>,” says BBSO Optical Engineer and Chief Observer, Nicolas Gorceix.</p>
<p>Since the early 2000s, adaptive optics have been used in large solar telescopes to restore images of the Sun’s surface to their full potential, enabling telescopes to reach their theoretical diffraction limits—i.e., the theoretical maximum resolution of an optical system. These systems have since revolutionized observing the Sun’s surface, but until now, have not been useful for observations in the corona; and the resolution of features beyond the solar limb stagnated at an order of 1,000 kilometers or worse—levels achieved 80 years ago.</p>
<p>“<i>The new coronal adaptive optics system closes this decades-old gap and delivers images of coronal features at 63 kilometers resolution—the theoretical limit of the 1.6-meter Goode Solar Telescope,</i>” says Thomas Rimmele, NSO Chief Technologist who built the first operational adaptive optics for the Sun’s surface, and motivated the development.</p>
<h3><b>Implications for the Future</b><b><br>
</b></h3>
<p>Coronal adaptive optics is now available at the GST. <i>“This technological advancement is a game-changer, there is a lot to discover when you boost your resolution by a factor of 10</i>,” Schmidt says.</p>
<p>The team now knows how to overcome the resolution limit imposed by the Earth’s lowest region of the atmosphere—i.e., the troposphere—on observations beyond the solar limb and is working to apply the technology at the 4-meter NSF Daniel K. Inouye Solar Telescope, built and operated by the NSO in Maui, Hawaiʻi. The world’s largest solar telescope would see even smaller details in the Sun’s atmosphere.</p>
<p>“<i>This transformative technology, which is likely to be adopted at observatories world-wide, is poised to reshape ground-based solar astronomy,</i>” says Philip R. Goode, distinguished research professor of physics at NJIT-CSTR and former director at BBSO, who co-authored the study. “<i>With coronal adaptive optics now in operation, this marks the beginning of a new era in solar physics, promising many more discoveries in the years and decades to come.</i>”</p>
<p>The <a href="https://www.nature.com/articles/s41550-025-02564-0">paper</a> describing this study, titled<i> “Observations of fine coronal structures with high-order solar adaptive optics,”</i> is now available in <i>Nature Astronomy</i>.</p>
<p>The authors are: Dirk Schmidt (NSO), Thomas A. Schad (NSO), Vasyl Yurchyshyn (NJIT), Nicolas Gorceix (NJIT), Thomas R. Rimmele (NSO), and Philip R. Goode (NJIT).</p>
<p><iframe loading="lazy" title="“Raindrops in the Sun’s Corona”: New Adaptive Optics Shows Stunning Details of our Star’s Atmosphere" src="about:blank" width="1080" height="608" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media" data-rocket-lazyload="fitvidscompatible" data-lazy-src="https://player.vimeo.com/video/1088466060?h=12164cb284&amp;dnt=1&amp;app_id=122963"></iframe><br>
Credit: Dirk Schmidt</p>

<h3>About the U.S. NSF National Solar Observatory</h3>
<p>The mission of the NSF National Solar Observatory (NSO) is to advance knowledge of the Sun, both as an astronomical object and as the dominant external influence on Earth, by providing forefront observational opportunities to the research community.</p>
<p>NSO built and operates the world’s most extensive collection of ground-based optical and infrared solar telescopes and auxiliary instrumentation— including the NSF GONG network of six stations around the world, and the world’s largest solar telescope, the NSF Daniel K. Inouye Solar Telescope—allowing solar physicists to probe all aspects of the Sun, from the deep solar interior to the photosphere, chromosphere, the outer corona, and out into the interplanetary medium. These assets also provide data for heliospheric modeling, space weather forecasting, and stellar astrophysics research, putting our Sun in the context of other stars and their environments.</p>
<p>Besides the operation of cutting-edge facilities, the mission includes the continued development of advanced instrumentation both in-house and through partnerships, conducting solar research, and educational and public outreach. NSO is managed by the Association of Universities for Research in Astronomy, Inc. (AURA) under a cooperative agreement with NSF. For more information, visit <a href="https://nso.edu/">nso.edu</a>.</p>
<h3>About the Big Bear Solar Observatory</h3>
<p>The Center for Solar-Terrestrial Research (CSTR) at New Jersey Institute of Technology (NJIT) is an international leader in ground- and space-based solar and terrestrial physics, with interest in understanding the effects of the Sun on the geospace environment. CSTR operates, along with a number of other observatories, the Big Bear Solar Observatory (BBSO).</p>
<p>BBSO is located on the north side of Big Bear Lake in the San Bernardino Mountains of southwestern San Bernardino County, California, approximately 75 miles East of downtown Los Angeles. BBSO has a 1.6-meter clear-aperture Goode Solar Telescope (GST), which has no obscuration in the optical train. The telescopes and instruments at the observatory are designed and employed specifically for studying the activities and phenomena of the Sun. GST was the largest and highest-resolution solar telescope in the world for ten years, and is a leading facility for solar physics research.</p>
<p>The NSF National Solar Observatory and BBSO have collaborated for over two decades to develop and to advance adaptive optics technologies for solar observations. The GST has been a critical facility to develop and test prototypes for the 4-meter NSF Daniel K. Inouye Solar Telescope, which took over as the world’s largest solar telescope in 2022. GST’s first-ever coronal adaptive optics system Cona is the latest product of this successful and pioneering collaboration. For more information, visit <a href="http://bbso.njit.edu/">bbso.njit.edu</a>.</p>
<h3>Contact</h3>
<p>For media inquiries, please contact:</p>
<p>Evan Pascual<br>
Communications Specialist<br>
U.S. NSF National Solar Observatory<br>
<a href="mailto:media@nso.edu">media@nso.edu</a></p>
<p>For technical inquiries, please contact:</p>
<p>Dirk Schmidt<br>
U.S. NSF National Solar Observatory<br>
<a href="mailto:dschmidt@nso.edu">dschmidt@nso.edu</a></p></div><div>
				
				
				
				
				<div><h4>Related Images</h4>
<p><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Raindrops-in-the-Suns-Corona.zip" target="_blank">Download Complete Media Package</a></p><div id="attachment_1392975"><picture fetchpriority="high" decoding="async" aria-describedby="caption-attachment-1392975">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1024x1024.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-150x150.jpg.webp 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-768x768.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1536x1536.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-610x610.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1080x1080.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-440x440.jpg.webp 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-1392975" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg" alt="This image of a prominence above the solar surface is a snapshot of a 4-minute time-lapse movie that reveals its rapid, fine, and turbulent restructuring with unprecedented detail. The Sun’s fluffy-looking surface is covered by “spicules”, short-lived plasma jets, whose creation is still the subject of scientific debate. The streaks on the right of this image are coronal rain falling down onto the Sun’s surface. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="300" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Scale_optimized-300x300.jpg">
</picture>
<p id="caption-attachment-1392975">This image of a prominence above the solar surface is a snapshot of a 4-minute time-lapse movie that reveals its rapid, fine, and turbulent restructuring with unprecedented detail. The Sun’s fluffy-looking surface is covered by “spicules”, short-lived plasma jets, whose creation is still the subject of scientific debate. The streaks on the right of this image are coronal rain falling down onto the Sun’s surface. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>
<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-1.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”)</li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Scale.png" target="_blank">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392972"><picture decoding="async" aria-describedby="caption-attachment-1392972">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1024x700.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-768x525.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1536x1050.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-610x417.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1080x738.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392972" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg" alt="This image is a snapshot from a 16-minute time-lapse movie showing the formation and collapse of a complexly shaped plasma stream traveling at almost 100 kilometers per seconds in front of a coronal loop system. This is likely the first time such a stream, which the scientists refer to as “plasmoid”, has been observed, leaving them wondering about the physical explanation of the observed dynamics. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="205" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1024x700.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-768x525.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1536x1050.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-610x417.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1080x738.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20205'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1024x700.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-768x525.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1536x1050.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-610x417.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-1080x738.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid_optimized-300x205.jpg">
</picture>
<p id="caption-attachment-1392972">This image is a snapshot from a 16-minute time-lapse movie showing the formation and collapse of a complexly shaped plasma stream traveling at almost 100 kilometers per seconds in front of a coronal loop system. This is likely the first time such a stream, which the scientists refer to as “plasmoid”, has been observed, leaving them wondering about the physical explanation of the observed dynamics. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”)</li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Scale.png" target="_blank">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392966"><picture decoding="async" aria-describedby="caption-attachment-1392966">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1024x1024.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-150x150.jpg.webp 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-768x768.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1536x1536.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-610x610.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1080x1080.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-440x440.jpg.webp 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392966" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg" alt="Coronal rain forms when hotter plasma in the Sun’s corona cools down and becomes denser. Like raindrops on Earth, coronal rain is pulled down to the surface by gravity. Because the plasma is electrically charged, it follows the magnetic field lines, which make huge arches/loops, instead of falling in a straight line. This image is a snapshot from a 23-minute time-lapse video which is comprised of the highest resolution images ever made of coronal rain. The scientists show in the paper that the strands can be narrower than 20 kilometers. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="300" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain_optimized-300x300.jpg">
</picture>
<p id="caption-attachment-1392966">Coronal rain forms when hotter plasma in the Sun’s corona cools down and becomes denser. Like raindrops on Earth, coronal rain is pulled down to the surface by gravity. Because the plasma is electrically charged, it follows the magnetic field lines, which make huge arches/loops, instead of falling in a straight line. This image is a snapshot from a 23-minute time-lapse video which is comprised of the highest resolution images ever made of coronal rain. The scientists show in the paper that the strands can be narrower than 20 kilometers. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”) </li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-2.png" target="_blank">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”) </li>
</ul>

<div id="attachment_1392951"><picture decoding="async" aria-describedby="caption-attachment-1392951">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1024x1024.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-150x150.jpg.webp 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-768x768.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1536x1536.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-610x610.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1080x1080.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-440x440.jpg.webp 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized.jpg.webp 1786w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392951" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg" alt="This image of a solar prominence is a snapshot of a 19-minute time-lapse movie showing how plasma “dances” and twists with the Sun’s magnetic field. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF" width="300" height="300" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized.jpg 1786w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1024x1024.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-150x150.jpg 150w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-768x768.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1536x1536.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-610x610.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-1080x1080.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-440x440.jpg 440w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized.jpg 1786w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2_optimized-300x300.jpg">
</picture>
<p id="caption-attachment-1392951">This image of a solar prominence is a snapshot of a 19-minute time-lapse movie showing how plasma “dances” and twists with the Sun’s magnetic field. This image was taken by the Goode Solar Telescope at Big Bear Solar Observatory using the new coronal adaptive optics system Cona. The image shows the hydrogen-alpha light emitted by the solar plasma. The image is artificially colorized, yet based on the color of hydrogen-alpha light, and darker color is brighter light. Credit: Schmidt et al./NJIT/NSO/AURA/NSF</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2.png" target="_blank">Download Hi-res Version</a> (Right-mouse click and “Save Image As.”)</li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-2-Scale.png">Download Hi-res Version with Scale</a> (Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392968"><picture decoding="async" aria-describedby="caption-attachment-1392968">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1024x819.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-768x614.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1536x1229.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-610x488.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1080x864.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized.jpg.webp 1920w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392968" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg" alt="The 1.6-meter Goode Solar Telescope (GST), located in Big Bear Lake, California, observing the Sun. The steady temperature of the water surface helps keep the air around the telescope calm, reducing the optical effects of turbulent air that degrades the telescope’s images of the Sun and that the adaptive optics further removes to achieve the maximum image detail. The GST is the second-largest solar telescope in the world and home to several instruments that scientists use to analyze the light from the Sun to infer physical processes in the Sun. The NSF National Solar Observatory and the Big Bear Solar Observatory have collaborated for over two decades to develop and advance adaptive optics technologies for solar observations. The GST has been a critical facility to develop and test prototypes for the U.S. National Science Foundation’s 4-meter Daniel K. Inouye Solar Telescope, which took over as the world’s largest solar telescope in 2019. GST’s first-ever coronal adaptive optics system is the latest product of this successful and pioneering collaboration. Credit: Sergey Shumko" width="300" height="240" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1024x819.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-768x614.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1536x1229.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-610x488.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1080x864.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized.jpg 1920w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20240'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1024x819.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-768x614.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1536x1229.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-610x488.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-1080x864.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized.jpg 1920w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST_optimized-300x240.jpg">
</picture>
<p id="caption-attachment-1392968">The 1.6-meter Goode Solar Telescope (GST), located in Big Bear Lake, California, observing the Sun. The steady temperature of the water surface helps keep the air around the telescope calm, reducing the optical effects of turbulent air that degrades the telescope’s images of the Sun and that the adaptive optics further removes to achieve the maximum image detail. The GST is the second-largest solar telescope in the world and home to several instruments that scientists use to analyze the light from the Sun to infer physical processes in the Sun. The NSF National Solar Observatory and the Big Bear Solar Observatory have collaborated for over two decades to develop and advance adaptive optics technologies for solar observations. The GST has been a critical facility to develop and test prototypes for the U.S. National Science Foundation’s 4-meter Daniel K. Inouye Solar Telescope, which took over as the world’s largest solar telescope in 2019. GST’s first-ever coronal adaptive optics system is the latest product of this successful and pioneering collaboration. Credit: Sergey Shumko</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/GST.jpg" target="_blank">Download Hi-res Version</a>(Right-mouse click and “Save Image As.”)</li>
</ul>

<div id="attachment_1392959"><picture decoding="async" aria-describedby="caption-attachment-1392959">
<source type="image/webp" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg.webp 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1024x683.jpg.webp 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-768x512.jpg.webp 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1536x1024.jpg.webp 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-610x407.jpg.webp 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1080x720.jpg.webp 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized.jpg.webp 1920w" sizes="(max-width: 300px) 100vw, 300px">
<img decoding="async" aria-describedby="caption-attachment-1392959" src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg" alt="The coronal adaptive optics system Cona at the Goode Solar Telescope. The black square box in the center illuminated and reflecting the sunlight is the adaptive mirror that corrects the images of the Sun. Credit: Dirk Schmidt" width="300" height="200" srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1024x683.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-768x512.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1536x1024.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-610x407.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1080x720.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized.jpg 1920w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20200'%3E%3C/svg%3E" data-lazy-srcset="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg 300w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1024x683.jpg 1024w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-768x512.jpg 768w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1536x1024.jpg 1536w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-610x407.jpg 610w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-1080x720.jpg 1080w, https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized.jpg 1920w" data-lazy-src="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System_optimized-300x200.jpg">
</picture>
<p id="caption-attachment-1392959">The coronal adaptive optics system Cona at the Goode Solar Telescope. The black square box in the center illuminated and reflecting the sunlight is the adaptive mirror that corrects the images of the Sun. Credit: Dirk Schmidt</p></div>

<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Cona-Coronal-Adaptive-Optics-System.jpg" target="_blank">Download Hi-res Version</a>(Right-mouse click and “Save Image As.”)</li>
</ul>
</div><div><h4>Related Video</h4>
<p><strong>Instructions:</strong> To download, right mouse-click, and select “Save Link As” or “Download Video As.”</p>
<ul>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Plasmoid-Video.mp4">Plasmoid Video</a></li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Prominence-1-Video.mp4">Prominence 1 Video</a></li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4">Coronal Rain</a></li>
<li><a href="https://nso1.b-cdn.net/wp-content/uploads/2025/05/Coronal-Rain-Video.mp4">Prominence 2</a></li>
</ul></div><div><h3>Credits</h3><p>
Schmidt et al./NJIT/NSO/AURA/NSF</p></div><div>
				
				
				
				
				<p><h3>Contacts</h3>
</p>
			</div><div>
				
				
				
				
				<p><strong>Media Contact:</strong><b></b><br>Evan Pascual<br>National Solar Observatory Communications Officer<br><a href="mailto:media@nso.edu">media@nso.edu<b></b><b></b></a></p>
			</div>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YOLO-World: Real-Time Open-Vocabulary Object Detection (134 pts)]]></title>
            <link>https://arxiv.org/abs/2401.17270</link>
            <guid>44146858</guid>
            <pubDate>Sat, 31 May 2025 20:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2401.17270">https://arxiv.org/abs/2401.17270</a>, See on <a href="https://news.ycombinator.com/item?id=44146858">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2401.17270">View PDF</a>
    <a href="https://arxiv.org/html/2401.17270v3">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Tianheng Cheng [<a href="https://arxiv.org/show-email/d19800c9/2401.17270" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2401.17270v1" rel="nofollow">[v1]</a></strong>
        Tue, 30 Jan 2024 18:59:38 UTC (5,276 KB)<br>
            <strong><a href="https://arxiv.org/abs/2401.17270v2" rel="nofollow">[v2]</a></strong>
        Fri, 2 Feb 2024 10:06:24 UTC (5,276 KB)<br>
    <strong>[v3]</strong>
        Thu, 22 Feb 2024 13:05:52 UTC (5,277 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oniux: Kernel-level Tor isolation for any Linux app (174 pts)]]></title>
            <link>https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/</link>
            <guid>44146830</guid>
            <pubDate>Sat, 31 May 2025 20:46:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/">https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/</a>, See on <a href="https://news.ycombinator.com/item?id=44146830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When launching privacy-critical apps and services, developers want to make sure that every packet really only goes through Tor. One mistyped proxy setting–or a single system-call outside the SOCKS wrapper–and your data is suddenly on the line.</p>
<p>That's why today, we are excited to introduce <em>oniux</em>: a small command-line utility providing Tor network isolation for third-party applications using Linux namespaces. Built on Arti, and onionmasq, oniux drop-ships any Linux program into its own network namespace to route it through Tor and strips away the potential for data leaks. If your work, activism, or research demands rock-solid traffic isolation, oniux delivers it.</p>
<h2>What are Linux namespaces? 🐧</h2>
<p>Namespaces are an isolation feature found in the Linux kernel that were introduced around the year 2000. They provide a secure way of isolating a certain part of an application from the rest of the system. Namespaces come in various forms and shapes. Some examples include network namespaces, mount namespaces, process namespaces, and a few more; each of them isolating a certain amount of system resources from an application.</p>
<p>What do we mean by <strong>system resources</strong>? In Linux, system resources are available globally by all applications on the system. The most notable example of this is probably your operating system clock, but there are many other areas as well, such as the list of all processes, the file system, and the list of users.</p>
<p>Namespaces <em>containerize</em> a certain part of an application from the rest of the operating system; this is exactly what Docker uses in order to provide its isolation primitives.</p>
<h2>Tor + Namespaces = ❤️</h2>
<p>As outlined above, namespaces are a powerful feature that gives us the ability to isolate Tor network access of an arbitrary application. We put each application in a network namespace that doesn't provide access to system-wide network interfaces (such as eth0), and instead provides a custom network interface onion0.</p>
<p>This allows us to isolate an arbitrary application over Tor in the most secure way possible software-wise, namely by relying on a security primitive offered by the operating system kernel. Unlike SOCKS, the application cannot accidentally leak data by failing to make some connection via the configured SOCKS, which may happen due to a mistake by the developer.</p>
<h2>oniux vs. torsocks</h2>
<p>You may have also heard of a tool with a similar goal, known as
<a href="https://gitlab.torproject.org/tpo/core/torsocks"><code>torsocks</code></a>, which works by
overwriting all network-related libc functions in a way to route traffic over a
SOCKS proxy offered by Tor.  While this approach is a bit more cross-platform,
it has the notable downside that applications making system calls <em>not</em> through
a dynamically linked libc, either with malicious intent or not, will leak data.
Most notably, this excludes support for purely static binaries and applications
from the Zig ecosystem.</p>
<p>The following provides a basic comparison on <em>oniux</em> vs <em>torsocks</em>:</p>
<table>
<thead><tr>
<th><em>oniux</em></th>
<th><em>torsocks</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>Standalone application</td>
<td>Requires running Tor daemon</td>
</tr>
<tr>
<td>Uses Linux namespaces</td>
<td>Uses an ld.so preload hack</td>
</tr>
<tr>
<td>Works on all applications</td>
<td>Only works on applications making system calls through libc</td>
</tr>
<tr>
<td>Malicious application cannot leak</td>
<td>Malicious application can leak by making a system call through raw assembly</td>
</tr>
<tr>
<td>Linux only</td>
<td>Cross-platform</td>
</tr>
<tr>
<td>New and experimental</td>
<td>Battle-proven for over 15 years</td>
</tr>
<tr>
<td>Uses Arti as its engine</td>
<td>Uses CTor as its engine</td>
</tr>
<tr>
<td>Written in Rust</td>
<td>Written in C</td>
</tr>
</tbody>
</table>
<h2>How can I use <em>oniux</em>? 🧅</h2>
<p>First, you need a Linux system with a Rust toolchain installed.
Afterwards, you can install <em>oniux</em> with the following command:</p>
<div><pre><span></span>$ cargo install --git https://gitlab.torproject.org/tpo/core/oniux oniux@0.4.0
</pre></div>
<p>Once that is done, you are ready to go for using <em>oniux</em>!  🙂</p>
<p>Using <em>oniux</em> is straightforward:</p>
<div><pre><span></span><span># Perform a simple HTTPS query using oniux!</span>
$ oniux curl https://icanhazip.com
&lt;A TOR EXIT NODE IP ADDRESS&gt;

<span># oniux also supports IPv6 of course!</span>
$ oniux curl -6 https://ipv6.icanhazip.com
&lt;A TOR EXIT NODE IPv6 ADDRESS&gt;

<span># Tor without onion services is like a car without an engine ...</span>
$ oniux curl http://2gzyxa5ihm7nsggfxnu52rck2vv4rvmdlkiu3zzui5du4xyclen53wid.onion/index.html

<span># You can also enable logging if you are a nerd. 🤓</span>
$ <span>RUST_LOG</span><span>=</span>debug oniux curl https://icanhazip.com

<span># If you want, you can "torify" your entire shell, isolating all processes within!</span>
$ oniux bash

<span># If you are in a desktop environment, you can isolate graphical applications too!</span>
$ oniux hexchat
</pre></div>
<h2>How does this work internally? ⚙️</h2>
<p><em>oniux</em> works by immediately spawning a child process using the <code>clone(2)</code>
system call, which is isolated in its own network, mount, PID, and user
namespace.  This process then mounts its own copy of <code>/proc</code> followed by UID
and GID mappings to the respective UID and GID of the parent process.</p>
<p>Afterwards, it creates a temporary file with nameserver entries which will then
be bind mounted onto <code>/etc/resolv.conf</code>, so that applications running within
will use a custom name resolver that supports resolving through Tor.</p>
<p>Next, the child process utilizes
<a href="https://gitlab.torproject.org/tpo/core/onionmasq">onionmasq</a> to create a TUN
interface named <code>onion0</code> followed by some <code>rtnetlink(7)</code> operations required to
set up the interface, such as assigning IP addresses.</p>
<p>Then, the child process sends the file descriptor of the TUN interface over
a Unix Domain socket to the parent process, who has been waiting for this
message ever since executing the initial <code>clone(2)</code>.</p>
<p>Once that is done, the child process drops all of its capabilities which
were acquired as part of being the root process in the user namespace.</p>
<p>Finally, the command supplied by the user is executed using facilities
provided by the Rust standard library.</p>
<h2><em>oniux</em> is experimental ⚠️</h2>
<p>Although this section should not discourage you from using <em>oniux</em>, you should
keep in mind that this is a relatively new feature which uses new Tor software,
such as <em>Arti</em> and <em>onionmasq</em>.</p>
<p>While things are already working as expected at the moment, tools such as
<em>torsocks</em> have been around for over 15 years, giving them more experience on
the battlefield.</p>
<p>But we do want to reach a similar state with oniux, so please go ahead and
check it out!</p>
<h2>Credits</h2>
<p>Many thanks to the developers of
<a href="https://github.com/smoltcp-rs/smoltcp"><code>smoltcp</code></a>, which is a Rust crate that
implements a full IP stack in Rust -- something, we make heavy use of.</p>
<p>Also many thanks go to <code>7ppKb5bW</code>, who taught us on how this can implemented
without the use of <code>capabilities(7)</code> by using <code>user_namespaces(7)</code> properly.</p>
<p>Last but not least, many thanks to all people and organizations who support Tor financially. The Tor Project, Inc. is a 501(c)(3) nonprofit advancing human rights and defending privacy online through free software and open networks. The oniux release is powered by a community of supporters. Please consider donating today to continue advancing our work that makes privacy possible.</p>
<p><a href="https://torproject.org/donate/donate-bp2-sc2025"><img src="https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/button-large-black.png" alt="Donate Button"></a></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CCD co-inventor George E. Smith dies at 95 (140 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html</link>
            <guid>44146619</guid>
            <pubDate>Sat, 31 May 2025 19:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html">https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=44146619">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/30/science/george-e-smith-dead.html: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>