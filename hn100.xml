<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 30 Nov 2025 09:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Zigbook Is Plagiarizing the Zigtools Playground (216 pts)]]></title>
            <link>https://zigtools.org/blog/zigbook-plagiarizing-playground/</link>
            <guid>46093518</guid>
            <pubDate>Sun, 30 Nov 2025 03:54:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zigtools.org/blog/zigbook-plagiarizing-playground/">https://zigtools.org/blog/zigbook-plagiarizing-playground/</a>, See on <a href="https://news.ycombinator.com/item?id=46093518">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://github.com/SuperAuguste" target="_blank">Auguste Rame</a>, <a href="https://github.com/Techatrix" target="_blank">Techatrix</a> — 30 November 2025</p><h2>Introduction</h2><p>For those unfamiliar, Zigtools was founded to support the <a href="https://ziglang.org/" target="_blank">Zig</a> community, especially newcomers, by creating editor tooling such as <a href="https://github.com/zigtools/zls" target="_blank">ZLS</a>, providing building blocks for language servers written in Zig with <a href="https://github.com/zigtools/lsp-kit" target="_blank">lsp-kit</a>, working on tools like the <a href="https://github.com/zigtools/playground" target="_blank">Zigtools Playground</a>, and contributing to Zig editor extensions like <a href="https://codeberg.org/ziglang/vscode-zig" target="_blank">vscode-zig</a>.</p><h2>The Plagiarism</h2><p>A couple weeks ago, a Zig resource called Zigbook was released with a bold claim of “zero AI” and an original “project-based” structure.</p><p>Unfortunately, even a cursory look at the nonsense chapter structure, book content, examples, generic website, or post-backlash issue-disabled repo reveals that the book is wholly LLM slop and the project itself is structured like some sort of sycophantic psy-op, with botted accounts and fake reactions.</p><p>We’re leaving out all direct links to Zigbook to not give them any more SEO traction.</p><p>We thought that the <a href="https://news.ycombinator.com/item?id=45947810" target="_blank">broad community backlash</a> would be the end of the project, but Zigbook persevered, releasing just last week a brand new feature, a “high-voltage beta” Zig playground.</p><p>As we at Zigtools have our own Zig playground (<a href="https://github.com/zigtools/playground" target="_blank">repo</a>, <a href="https://playground.zigtools.org/" target="_blank">website</a>), our interest was immediately piqued. The form and functionality looked pretty similar and Zigbook even integrated (in a non-functional manner) ZLS into their playground to provide all the fancy editor bells-and-whistles, like code completions and goto definition.</p><p>Knowing Zigbook’s history of deception, we immediately investigated the WASM blobs. Unfortunately, the WASM blobs are byte-for-byte identical to ours. This cannot be a coincidence given the two blobs (<code>zig.wasm</code>, a lightly modified version of the Zig compiler, and <code>zls.wasm</code>, ZLS with a modified entry point for WASI) are entirely custom-made for the Zigtools Playground.</p><p>We archived the WASM files for your convenience, courtesy of the great Internet Archive:</p><ul><li><code>zls.wasm</code> (<code>sha256sum</code>: <code>3a63e5092e8f90172716977af5c88b4f49e546f730f25e9bafb47f4ac9a2ee1d</code>)<ul><li><a href="https://web.archive.org/web/20251129224054/https://playground.zigtools.org/assets/zls-Dxe5uroE.wasm" target="_blank">Original</a></li><li><a href="https://web.archive.org/web/20251128234504/https://www.zigbook.net/zig/zls.wasm" target="_blank">Plagiarized</a></li></ul></li><li><code>zig.wasm</code> (<code>sha256sum</code>: <code>d3fe6b8a6b1db84a914eaa1f4a80ca5dcfd3b0948a35f2b1e78432a392eace96</code>)<ul><li><a href="https://web.archive.org/web/20251129224210/https://playground.zigtools.org/assets/zig-n5fT0I4M.wasm" target="_blank">Original</a></li><li><a href="https://web.archive.org/web/20251128234607/https://www.zigbook.net/zig/zig.wasm" target="_blank">Plagiarized</a></li></ul></li></ul><p>We proceeded to look at the JavaScript code, which we quickly determined was similarly copied, but with LLM distortions, likely to prevent the code from being completely identical. Still, certain sections were copied one-to-one, like the JavaScript worker data-passing structure and logging (<a href="https://github.com/zigtools/playground/blob/0ae8aa03fa38e8ce38e95f57cccfcf9aa4db610a/src/workers/runner.ts#L12-L41" target="_blank">original ZLS playground code</a>, <a href="https://archive.ph/4LZ2X" target="_blank">plagiarized Zigbook code</a>).</p><p>The following code from both files is identical:</p><pre><code>    <span>try</span> <span>{</span>
        <span>// @ts-ignore</span>
        <span>const</span> <span>exitCode</span> <span>=</span> <span>wasi</span><span>.</span><span>start</span><span>(</span><span>instance</span><span>)</span><span>;</span>

        <span>postMessage</span><span>(</span><span>{</span>
            <span>stderr</span>: <span>`\n\n---\nexit with exit code </span><span>${</span><span>exitCode</span><span>}</span><span>\n---\n`</span><span>,</span>
        <span>}</span><span>)</span><span>;</span>
    <span>}</span> <span>catch</span> <span>(</span><span>err</span><span>)</span> <span>{</span>
        <span>postMessage</span><span>(</span><span>{</span> <span>stderr</span>: <span>`</span><span>${</span><span>err</span><span>}</span><span>`</span> <span>}</span><span>)</span><span>;</span>
    <span>}</span>

    <span>postMessage</span><span>(</span><span>{</span>
        <span>done</span>: <span>true</span><span>,</span>
    <span>}</span><span>)</span><span>;</span>

    <span>// ...</span>

    <span>onmessage</span> <span>=</span> <span>(</span><span>event</span><span>)</span> <span>=&gt;</span> <span>{</span>
        <span>if</span> <span>(</span><span>event</span><span>.</span><span>data</span><span>.</span><span>run</span><span>)</span> <span>{</span>
            <span>run</span><span>(</span><span>event</span><span>.</span><span>data</span><span>.</span><span>run</span><span>)</span><span>;</span>
        <span>}</span>
    <span>}</span><span>;</span>
</code></pre>
<p>The <code>\n\n---\nexit with exit code ${exitCode}\n---\n</code> is perhaps the most obviously copied string.</p><p>Funnily enough, despite copying many parts of our code, Zigbook didn’t copy the most important part of the ZLS integration code, the JavaScript ZLS API designed to work with the <a href="https://github.com/zigtools/playground/blob/227bcd3775baeb60cbf2f1ffb1ad0566516a64fc/src/zls.zig" target="_blank">ZLS WASM binary’s API</a>. That JavaScript code is absolutely required to interact with the ZLS binary which they <em>did</em> plagiarize. Zigbook either avoided copying that JavaScript code because they knew it would be too glaringly obvious, because they fundamentally do not understand how the Zigtools Playground works, or because they plan to copy more of our code.</p><p>To be clear, copying our code and WASM blobs is entirely permissible given that the playground and Zig are MIT licensed. Unfortunately, Zigbook has not complied with the terms of the MIT license at all, and seemingly claims the code and blobs as their own without correctly reproducing the license.</p><p>We sent Zigbook a neutral <a href="https://github.com/zigbook/zigbook/pull/43" target="_blank">PR correcting the license violations</a>, but they quickly closed it and deleted the description, seemingly to hide their misdeeds.</p><p>The original description (also available in the “edits” dropdown of the original PR comment) is reproduced below:</p><blockquote><p>We (@zigtools) noticed you were using code from the Zigtools Playground, including byte-by-byte copies of our WASM blobs and excerpts of our JavaScript source code.</p><p>This is a violation of the MIT license that the Zigtools Playground is licensed under alongside a violation of the Zig MIT license (for the zig.wasm blob).</p><p>As the MIT license states:</p><pre><code>The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.
</code></pre><p>We’ve fixed this by adding the licenses in question to your repository. As your repository does not include a direct link to the *.wasm dependencies, we’ve added a license disclaimer on the playground page as well that mentions the licenses.</p></blockquote><p>Zigbook’s aforementioned bad behavior and their continued violation of our license and unwillingness to fix the violation motivated us to write this blog post.</p><h2>Our Vision for the Zigtools Playground</h2><p>It’s sad that our first blog post is about the plagiarism of our coolest subproject. We challenged ourselves by creating a WASM-based client-side playground to enable offline usage, code privacy, and no server costs.</p><p>This incident has motivated us to invest more time into our playground and has generated a couple of ideas:</p><ul><li>We’d like to enable multifile support to allow more complex Zig projects to be run in the browser</li><li>We’d like to collaborate with fellow Ziguanas to integrate the playground into their excellent Zig tutorials, books, and blogposts<ul><li>A perfect example usecase would be enabling folks to hop into <a href="https://codeberg.org/ziglings/exercises" target="_blank">Ziglings</a> online with the playground</li><li>The Zig website itself would be a great target as well!</li></ul></li><li>We’d like to support stack traces using DWARF debug info which is not yet emitted by the self-hosted Zig compiler</li></ul><h2>Conclusion</h2><p>As Zig community members, <strong>we advise all other members of the Zig community to steer clear of Zigbook</strong>.</p><p>If you’re looking to learn Zig, we strongly recommend looking at the excellent official <a href="https://ziglang.org/learn/" target="_blank">Zig learn page</a> which contains excellent resources from the previously mentioned <a href="https://codeberg.org/ziglings/exercises" target="_blank">Ziglings</a> to <a href="https://www.openmymind.net/learning_zig/" target="_blank">Karl Seguin’s Learning Zig</a>.</p><p>We’re also using this opportunity to mention that we’re fundraising to keep ZLS sustainable for our only full-time maintainer, Techatrix. We’d be thrilled if you’d be willing to give just $5 a month. You can check out our <a href="https://opencollective.com/zigtools" target="_blank">OpenCollective</a> or <a href="https://github.com/sponsors/zigtools" target="_blank">GitHub Sponsors</a>.</p><p>Thanks for reading! \(^-^)/</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Boing (297 pts)]]></title>
            <link>https://boing.greg.technology/</link>
            <guid>46093473</guid>
            <pubDate>Sun, 30 Nov 2025 03:46:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boing.greg.technology/">https://boing.greg.technology/</a>, See on <a href="https://news.ycombinator.com/item?id=46093473">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Regrets. Think carefully about how you spend your time (147 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=46092688</link>
            <guid>46092688</guid>
            <pubDate>Sun, 30 Nov 2025 01:33:27 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=46092688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bigbox"><td><table><tbody><tr id="46092688"><td><span></span></td><td><center><a id="up_46092688" href="https://news.ycombinator.com/vote?id=46092688&amp;how=up&amp;goto=item%3Fid%3D46092688"></a></center></td><td><span> [flagged] <a href="https://news.ycombinator.com/item?id=46092688">Tell HN: Regrets. Think carefully about how you spend your time</a></span></td></tr><tr><td colspan="2"></td><td><span><span id="score_46092688">119 points</span> by <a href="https://news.ycombinator.com/user?id=anonymous_ibex">anonymous_ibex</a> <span title="2025-11-30T01:33:27 1764466407"><a href="https://news.ycombinator.com/item?id=46092688">2 hours ago</a></span> <span id="unv_46092688"></span> | <a href="https://news.ycombinator.com/hide?id=46092688&amp;goto=item%3Fid%3D46092688">hide</a> | <a href="https://hn.algolia.com/?query=Tell%20HN%3A%20Regrets.%20Think%20carefully%20about%20how%20you%20spend%20your%20time&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=46092688&amp;auth=52c71c6dcad859121c032feb3a61f66041a5e749">favorite</a> | <a href="https://news.ycombinator.com/item?id=46092688">33&nbsp;comments</a></span></td></tr><tr><td colspan="2"></td><td><div><p>I'm writing this 38 hours before I go into a surgery on Monday that I may not survive, and while I am told I have a better than 50/50 chance of making it to this time next year, I still feel, though I am too young (early 50s) to deal with these things, that I have wasted too much time. I'd like to impart some lessons.</p><p>1. A small number of accomplishments really mean something, but you often won't know which ones. I started three companies and two were successes, and even though they comprised more than two decades of my life, I feel like I remember a grand total of six important hours between them. Meanwhile, I still remember the shed I built for my father in the first summer after college. Whatever seems unimportant, you will care about the most.</p><p>2. The thing you do today, you will probably do tomorrow. I've wasted a lot of time, but most people I know have wasted lots of time, and it's because of the tendency to make an exception of the present day, which either excuses laziness or pathological busyness, which is a form of the same thing. "I'll do it tomorrow." But tomorrow it will be today. It's always today.</p><p>3. Ethics matter. I don't believe there's any life after this one, but I find myself ruminating on what I've done. In 2015, I had a lot of interaction with a startup incubator you know well, and ended up sitting in the discussions and planning around banning and erasing a young programmer we considered a threat to our financial interests, due to his concerns about authoritarianism in technology. In retrospect, he was harmless, but an example had to be made. The decision was made to ban him here, try to get him fired though I don't know if we succeeded, and attack him with sockpuppets on Reddit, and it seems to have worked because you don't hear his name much.</p><p>Ten years later, I'm still stuck thinking about this. Am I the kind of person who does shitty things? I was. Am I still? How would I even know?</p><p>I don't believe that faith is an out, or that you can apologize or donate your way out of past behaviors. You will always be the person who has done what you have done.</p><p>4. Be kind to animals. There are few joys like having a dog. I always refused when my ex-wife wanted one, and she got one after we separated. For her, it was probably an upgrade.</p><p>5. I developed a knack for founding companies, but I never learned how to build communities. They aren't the same thing. You might have three hundred people at your company and you truly feel like they are your village, but they're not. Circumstances will change, and people will move, and in five years, most of them will not remember your name.</p><p>That's probably enough for now. My mind goes between periods of racing and long spells of languid acceptance. All humans end up in the place where I am, and I hope you reach it with fewer regrets than I have.</p></div></td></tr><tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr></tbody></table><br>
</td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meshtastic (141 pts)]]></title>
            <link>https://meshtastic.org/</link>
            <guid>46092558</guid>
            <pubDate>Sun, 30 Nov 2025 01:15:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://meshtastic.org/">https://meshtastic.org/</a>, See on <a href="https://news.ycombinator.com/item?id=46092558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><header><div><h2></h2><p>An open source, off-grid, decentralized, mesh network built to run on affordable, low-power devices</p></div></header><main><div><h3>Getting Started</h3><div><p><h4>Step 1</h4></p><p><a href="https://meshtastic.org/docs/hardware/devices/">Choose a Device →</a></p><p>Meshtastic devices are available in a variety of configurations to suit your needs.</p></div></div><div><h3 id="get-connected">Get Connected</h3><p>Connect and control your Meshtastic devices through various platforms. Choose the client that best fits your needs and device ecosystem.</p></div><div><div><p>Manage your Meshtastic network on-the-go with our iOS application.</p></div><div><p>Connect and control your Meshtastic devices using our Android application.</p></div><div><p>Access your Meshtastic network from any device with our web-based client.</p></div><div><p>Command-line interface and software development kit for Python developers and power users.</p></div></div><br></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Americans no longer see four-year college degrees as worth the cost (253 pts)]]></title>
            <link>https://www.nbcnews.com/politics/politics-news/poll-dramatic-shift-americans-no-longer-see-four-year-college-degrees-rcna243672</link>
            <guid>46091591</guid>
            <pubDate>Sat, 29 Nov 2025 22:56:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/politics-news/poll-dramatic-shift-americans-no-longer-see-four-year-college-degrees-rcna243672">https://www.nbcnews.com/politics/politics-news/poll-dramatic-shift-americans-no-longer-see-four-year-college-degrees-rcna243672</a>, See on <a href="https://news.ycombinator.com/item?id=46091591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p id="anchor-46ad49">Americans have grown sour on one of the longtime key ingredients of the American dream.</p><p id="anchor-21cce4">Almost two-thirds of registered voters say that a four-year college degree isn’t worth the cost, according to a new <a href="https://www.documentcloud.org/documents/26310998-nbc-news-october-2025-poll/" target="_blank">NBC News poll</a>, a dramatic decline over the last decade.</p><p id="anchor-f810a7">Just 33% agree a four-year college degree is “worth the cost because people have a better chance to get a good job and earn more money over their lifetime,” while 63% agree more with the concept that it’s “not worth the cost because people often graduate without specific job skills and with a large amount of debt to pay off.”</p><p id="anchor-628a42">In 2017, U.S. adults surveyed were virtually split on the question — 49% said a degree was worth the cost and 47% said it wasn’t. When CNBC asked the same question in 2013 as part of its All American Economic Survey, 53% said a degree was worth it and 40% said it was not.</p><p id="anchor-86586a">The eye-popping shift over the last 12 years comes against the backdrop of several major trends shaping the job market and the education world, from exploding college tuition prices to rapid changes in the modern economy — which seems once again poised for radical transformation alongside advances in AI.</p><p id="anchor-20699a">“It’s just remarkable to see attitudes on any issue shift this dramatically, and particularly on a central tenet of the American dream, which is a college degree. Americans used to view a college degree as aspirational — it provided an opportunity for a better life. And now that promise is really in doubt,” said Democratic pollster Jeff Horwitt of Hart Research Associates, who conducted the poll along with the Republican pollster Bill McInturff of Public Opinion Strategies.</p><p id="anchor-d693f1">“What is really surprising about it is that everybody has moved. It’s not just people who don’t have a college degree,” Horwitt added.</p><p id="anchor-9e5b5b">National <a href="https://www.bls.gov/emp/chart-unemployment-earnings-education.htm" target="_blank">data from the Bureau of Labor Statistics</a> shows that those with advanced degrees earn more and have lower unemployment rates than those with lower levels of education. That’s been true for years.</p><p id="anchor-5cfa48">But what has shifted is the price of college. While there have been some small declines in tuition prices over the last decade, when adjusted for inflation, <a href="https://research.collegeboard.org/media/pdf/Trends-in-College-Pricing-and-Student-Aid-2025-final_0.pdf" target="_blank">College Board data shows</a> that the average, inflation-adjusted cost of public four-year college tuition for in-state students has doubled since 1995. Tuition at private, four-year colleges is up 75% over the same period.</p><p id="anchor-8c8a8a">Poll respondents who spoke with NBC News all emphasized those rising costs as a major reason why the value of a four-year degree has been undercut.</p><p id="anchor-3853e4">Jacob Kennedy, a 28-year-old server and bartender living in Detroit, told NBC News that while he believes “an educated populace is the most important thing for a country to have,” if people can’t use those degrees because of the debt they’re carrying, it undercuts the value.</p><p id="anchor-f78999">Kennedy, who has a two-year degree, reflected on “the number of people who I’ve met working in the service industry who have four-year degrees and then within a year of graduating immediately quit their ‘grown-up jobs’ to go back to the jobs they had.”</p><p id="anchor-c9c32a">“The cost overwhelms the value,” he continued. “You go to school with all that student debt — the jobs you get out of college don’t pay that debt, so you have to go find something else that can pay that debt.”</p><p id="anchor-eaa980">The 20-point decline over the last 12 years among those who say a degree is worth it — from 53% in 2013 to 33% now — is reflected across virtually every demographic group. But the shift in sentiment is especially striking among Republicans.</p><p id="anchor-3f0f4d">In 2013, 55% of Republicans called a college degree worth it, while 38% said it wasn’t worth it. In the new poll, just 22% of Republicans say the four-year degree is worth it, while 74% say it’s not.</p><p id="anchor-388362">Democrats have seen a significant shift too, but not to the same extent — a decline from 61% who said a degree was worth it in 2013 to 47% this year.</p><p id="anchor-38b2b0"><a href="https://catalist.us/wh-national/" target="_blank">Over the same period</a>, the composition of both parties has changed, with the Republican Party garnering new and deeper support from voters without college degrees, while the Democratic Party drew in more degree-holders.</p><p id="anchor-a6d7dc">Remarkably, less than half of voters with college degrees see those degrees as worth the cost: 46% now, down from 63% in 2013.</p><p id="anchor-5eb933">Those without a college degree were about split on the question in 2013. Now, 71% say a four-year degree is not worth the cost, while 26% say it is.</p><p id="anchor-6dec9e">Preston Cooper, a senior fellow at the right-leaning American Enterprise Institute, said enough cracks have proliferated under the long-standing narrative that a college degree always pays off to create a serious rupture.</p><p id="anchor-0c81e4">“Some people drop out, or sometimes people end up with a degree that is not worth a whole lot in the labor market, and sometimes people pay way too much for a degree relative to the value of what that credential is,” he said. “These cases have created enough exceptions to the rule that a bachelor’s degree always pays off, so that people are now more skeptical.”</p><p id="anchor-103baf">The upshot is that interest in technical, vocational and two-year degree programs has soared.</p><p id="anchor-47f9a3">“I think students are more wary about taking on the risk of a four-year or even a two-year degree,” he said. “They’re now more interested in any pathway that can get them into the labor force more quickly.”</p><p id="anchor-2f0c4e">Josiah Garcia, a 24-year-old in Virginia, said he recently enrolled in a program to receive a four-year engineering degree after working as an electrician’s apprentice. He said he was motivated to go back to school because he saw the degree as having a direct effect on his future earning potential.</p><p id="anchor-603e73">But he added that he didn’t feel that those who sought other degrees in areas like art or theater could say the same.</p><p id="anchor-df4664">“A lot of my friends who went to school for art or dance didn’t get the job they thought they could get after graduating,” he said, arguing that degrees for “softer skills” should be cheaper than those in STEM fields.</p><p id="anchor-550c9a">Jessica Burns, a 38-year-old Iowa resident and bachelor’s degree-holder who works for an insurance company, told NBC News that for her, the worth of a four-year-degree largely depends on the cost.</p><p id="anchor-96ad64">She went to a community college and then a state school to earn her degree, so she said she graduated without having to spend an “insane” amount of money.</p><p id="anchor-ae8558">But her husband went to a private college for his degree, and she quipped: “We are going to have student loan debt for him forever.”</p><p id="anchor-7f91f2">Burns said she believes a college degree is “essential for a lot of jobs. You’re not going to get an interview if you don’t have a four-year degree for a lot of jobs in my field.”</p><p id="anchor-9e7cff">But she framed the value of degrees more in terms of how society views them instead of intrinsic value.</p><p id="anchor-125272">“It’s not valuable because it’s brought a bunch of value added, it’s valuable because it’s the key to even getting in the door,” she said. “Our society needs to figure out that if we value it, we need to make it affordable.”</p><p id="anchor-fb8a5b">Burns said she believes that a lot more people in her millennial generation are “now saddled with a huge amount of debt, even as successful business professionals,” which will influence how her peers approach paying for college for their children.</p><p id="anchor-4d8602">There hasn’t just been a decline in the cost-benefit analysis of a degree. <a href="https://news.gallup.com/poll/692519/public-trust-higher-rises-recent-low.aspx" target="_blank">Gallup polling also shows</a> a marked decline in public confidence in higher education over the last decade, albeit with a slight increase over the last year.</p><p id="anchor-145a1d">“This is a political problem. It’s also a real problem for higher education. Colleges and universities have lost that connection they’ve had with a large swath of the American people based on affordability,” Horwitt said. “They’re now seen as out of touch and not accessible to many Americans.”</p><p id="anchor-941684"><em>The NBC News poll surveyed 1,000 registered voters Oct. 24-28 via a mix of telephone interviews and an online survey sent via text message. The margin of error is plus or minus 3.1 percentage points.</em></p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/ben-kamisar-ncpn891986" tabindex="-1"><picture data-testid="picture" data-flavor="focal" data-original-height="48" data-original-width="48"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_26/3653062/240627-ben-kamisar.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_26/3653062/240627-ben-kamisar.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_26/3653062/240627-ben-kamisar.jpg" alt="" height="48" width="48"></picture></a></div><p>Ben Kamisar is a national political reporter for NBC News</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bazzite: The next generation of Linux gaming (394 pts)]]></title>
            <link>https://bazzite.gg/</link>
            <guid>46091362</guid>
            <pubDate>Sat, 29 Nov 2025 22:22:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bazzite.gg/">https://bazzite.gg/</a>, See on <a href="https://news.ycombinator.com/item?id=46091362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<div id="features">
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
												
												<div>
													<p>
														<h2>Play your favorite games</h2>
													</p>
												</div>
												<div><p><strong>Bazzite is designed for Linux newcomers and enthusiasts alike</strong> with <a href="https://steampowered.com/" target="_blank">Steam</a> pre-installed, HDR &amp; VRR support, improved CPU schedulers for responsive gameplay, and numerous community-developed tools and tweaks to streamline your gaming and streaming experience.</p><p><em>Own games outside of Steam?</em> Lutris <em>(Pre-installed)</em> <a href="https://docs.bazzite.gg/Gaming/Game_Launchers/#non-steam-games" target="_blank">and other launchers</a> can not only run games from most game stores, but cleanly integrate them into Steam Gaming Mode, including: <span><span>Xbox Game Pass</span> <em>(via battle.net)</em>, <span>EA App</span>, <span>Epic Games Store</span>, GOG.com, itch.io, <span>Rockstar Games Launcher</span>, <span>Ubisoft Connect</span>, your dad's old CDs, and more!</span></p></div>
											</div>
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
												
												<div data-tilt="" data-tilt-max="10" data-tilt-scale="1.1" data-tilt-glare="" data-tilt-max-glare="0.25" data-tilt-gyroscope="false" data-anim-type="fadeInUp">
															<p><img src="https://bazzite.gg/content/uploads/2025/02/sot_birds.webp" alt="Sea of Thieves" width="1400" height="1400" decoding="async" loading="lazy"></p><div>
																<p>Sea of Thieves</p>
																<p>Shot on Framework</p>
																
															</div>
														</div>
												<div data-tilt="" data-tilt-max="10" data-tilt-scale="1.1" data-tilt-glare="" data-tilt-max-glare="0.25" data-tilt-gyroscope="false" data-anim-type="fadeInUp">
															<p><img src="https://bazzite.gg/content/uploads/2024/02/fh5.webp" alt="Forza Horizon 5" width="1440" height="1440" decoding="async" loading="lazy"></p><div>
																<p>Forza Horizon 5</p>
																<p>Shot on Steam Deck</p>
																
															</div>
														</div>
												<div data-tilt="" data-tilt-max="10" data-tilt-scale="1.1" data-tilt-glare="" data-tilt-max-glare="0.25" data-tilt-gyroscope="false" data-anim-type="fadeInUp">
															<p><img src="https://bazzite.gg/content/uploads/2024/11/stalker2.webp" alt="Sea of Thieves" width="1440" height="1440" decoding="async" loading="lazy"></p>
														</div>
												<div data-tilt="" data-tilt-max="10" data-tilt-scale="1.1" data-tilt-glare="" data-tilt-max-glare="0.25" data-tilt-gyroscope="false" data-anim-type="fadeInUp">
															<p><img src="https://bazzite.gg/content/uploads/2025/02/rivals.webp" alt="Marvel Rivals" width="1080" height="1080" decoding="async" loading="lazy"></p>
														</div>
											</div>
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
																<p><strong>Wondering if your favorite games are compatible?</strong> Check the community-maintained compatibility lists:
																</p>
															</div>
										
									</div>
								<div id="devices">
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
													<p>
														<h2>On all your <span>favorite devices</span></h2>
													</p>
												</div>
										<div>
											<div data-anim-type="zoomIn" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
																<p><a href="https://docs.bazzite.gg/Gaming/Hardware_compatibility_for_gaming/" target="_blank" title="View Compatible Hardware">
																		<img width="1300" height="1300" src="https://bazzite.gg/content/uploads/2024/09/convergence_small_nl3.webp" alt="A sample of the devices Bazzite can run on" decoding="async" loading="lazy">
																	</a>
																</p>
															</div>
											<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
																<p><strong>Bazzite works for you</strong> whether you prefer to game on your <strong>handheld</strong>, chill on the couch with your <strong>home theater PC</strong>, lug your <strong>laptop</strong> to the LAN party, surf the subway with your <strong>tablet</strong>, or pretend you're getting work done on your <strong>desktop</strong> - <em>no judgement, we're hopelessly addicted to Deadlock too.</em></p>
																<p><a href="https://docs.bazzite.gg/Gaming/Hardware_compatibility_for_gaming/" target="_blank" title="">
																	<span>View Compatible Hardware</span>
																</a>
															</p></div>
										</div>
										<div id="library" data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
													<p>
														<h3>Take your game library <em>anywhere</em></h3>
													</p>
												</div>
										<div data-anim-type="zoomIn" data-desktop-col="one" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p><img width="800" height="400" src="https://bazzite.gg/content/uploads/2024/09/convergence_library_lq.webp" alt="A game library being shared with a Framework 13 and Legion Go - Northstar.tf is depicted" decoding="async" loading="lazy">
															</p>
														</div>
										<div data-desktop-col="one" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
														<p>Your MicroSD card game library can be shared between multiple Bazzite installs <em>no matter which devices you're using</em>.</p>
													</div>
									</div>
								<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one" id="stability">
												
												
												<div>
														<h2><p><span>Upgrade</span><span>Rollback</span></p><span>Upgrade</span>&nbsp;fearlessly</h2>
													</div>
												<div>
															<div><p><strong>Bazzite is image based</strong> meaning that after every update the previous version of the operating system is retained on your machine. Should an update cause any issues, <strong><a title="View our Updates, Rollbacks, and Rebasing documentation" href="https://docs.bazzite.gg/Installing_and_Managing_Software/Updates_Rollbacks_and_Rebasing/" target="_blank">you can select the previous image at boot time</a></strong>.</p><p>
																Images of the operating system are retained in our repositories for <b>ninety days</b> and can be switched to via the terminal. <i>Nvidia driver update broke something you needed?</i> No worries, rebase to the last known good release and pin it so that it's retained as long as needed.</p><p>
																
																<i>󰌾</i></p></div><h3>Secure by default</h3><p>
																Experience enterprise class security with <i>out-of-the-box</i> SELinux, Secure Boot support, <strong><a href="https://github.com/sigstore/cosign" target="_blank">signed container images</a></strong>, and LUKS disk encryption with optional automatic TPM unlocking.</p><p>
																
																Modern app stores provide attestation, sandboxing, and the most officially verified applications of <i>any</i> Linux application repository.
															</p>
														</div>
											</div>
								<div id="hardware">
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
												
												<div>
													<p>
														<h2>Work with your hardware, <span>not for it</span></h2>
													</p>
												</div>
												<div><p><strong>Bazzite focuses on hardware compatibility out of the box</strong>, with full support for accelerated video encoding and decoding, built in Nvidia drivers, additional HID drivers, and just about every udev rule you could need.</p><p><strong>Let your operating system work with your hardware so you don't have to.</strong></p></div>
											</div>
										<div data-desktop-col="one-third" data-laptop-col="laptop-one-third" data-tablet-col="tablet-one-third" data-mobile-col="mobile-one" data-anim-type="fadeInUp">
																<h5>Game Controllers</h5>
																<p>Out-of-the-box support for Xbox, Wii, Switch, PS3, PS4, PS5, and numerous other controllers.</p>
															</div>
										<div data-desktop-col="one-third" data-laptop-col="laptop-one-third" data-tablet-col="tablet-one-third" data-mobile-col="mobile-one" data-anim-type="fadeInUp">
																<h5>GPU Support</h5>
																<p>Nvidia drivers and the latest Mesa for AMD &amp; Intel pre-installed, with tweaks applied as needed</p>
															</div>
										<div data-desktop-col="one-third" data-laptop-col="laptop-one-third" data-tablet-col="tablet-one-third" data-mobile-col="mobile-one" data-anim-type="fadeInUp">
																<h5>Additional Drivers</h5>
																<p>Bazzite ships with support for additional Wi-Fi adapters, display standards like DisplayLink, and more!</p>
															</div>
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
												
												<div>
													<p>
														<h3>Complete Handheld PC Support</h3>
													</p>
												</div>
												<div>
														<p>Bazzite features <a href="https://hhd.dev/" target="_blank">Handheld Daemon</a>, offering enhanced functionality and support for handhelds from manufacturers such as ASUS, Ayn, GPD, and Lenovo - <strong>all accessible by double tapping the quick access menu button</strong>.</p>
													</div>
											</div>
										<div>
											<div data-anim-type="fadeInLeft" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p>Customize your handheld experience with in-depth controller emulation, including paddles, touchpad, rumble, face buttons, RGB lighting, and more.</p>
														</div>
											<div data-anim-type="fadeInRight" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
																<p><img width="600" height="300" src="https://bazzite.gg/content/uploads/2024/09/hhd-rgb4.webp" alt="HHD RGB Control" decoding="async" loading="lazy">
																</p>
															</div>
											<div data-anim-type="fadeInLeft" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
																<p><img width="600" height="300" src="https://bazzite.gg/content/uploads/2024/09/hhd-tdp3.webp" alt="HHD Hardware and TDP Control" decoding="async" loading="lazy">
																</p>
															</div>
											<div data-anim-type="fadeInRight" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p>Control all aspects of your hardware from a clean and intuitive UI, including GPU frequency, TDP controls, and CPU scheduler.</p>
														</div>
										</div>
									</div>
								<div id="desktop">
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
														<h2>Use your favorite<span>&nbsp;</span><p><span>desktop&nbsp;environment</span><span>handheld&nbsp;experience</span><span>couch&nbsp;gaming&nbsp;setup</span></p><span>desktop environment</span></h2>
													</div>
										<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
												
												<div>
															<p><a href="https://kde.org/" target="_blank">
																	<img src="https://bazzite.gg/content/uploads/2024/02/kde.svg" width="48" height="48" alt="KDE" decoding="async" loading="lazy">
																</a>
															</p>
														</div>
												<div>
														<p>The latest and greatest by the KDE community built from <a href="https://fedoraproject.org/atomic-desktops/kinoite/" target="_blank">Fedora Kinoite</a>. KDE offers a highly customizable and modern UI that Windows users would find right at home, with a bottom taskbar, start menu, and widgets. Valve's themes and customizations present in SteamOS come pre-installed.</p>
													</div>
											</div>
										<div data-anim-type="fadeInRight" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p><img width="1863" height="1178" src="https://bazzite.gg/content/uploads/2025/07/kde.webp" alt="Screenshots of the KDE desktop" decoding="async" loading="lazy">
															</p>
														</div>
										<div data-anim-type="fadeInLeft" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p><img width="1863" height="1178" src="https://bazzite.gg/content/uploads/2025/07/gnome2.webp" alt="Screenshots of the GNOME desktop" decoding="async" loading="lazy">
															</p>
														</div>
										<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
												
												<div>
															<p><a href="https://www.gnome.org/" target="_blank">
																	<img src="https://bazzite.gg/content/uploads/2024/02/Gnomelogo.svg" width="402" height="488" alt="GNOME" decoding="async" loading="lazy">
																</a>
															</p>
														</div>
												<div>
														<p>A modern and beautiful desktop by the GNOME Foundation built from <a href="https://fedoraproject.org/atomic-desktops/silverblue/" target="_blank">Fedora Silverblue</a>. Optimized for touch input, this desktop environment feels right at home on handhelds and tablets, with rounded corners and thoughtful design choices that would make even a die-hard Apple user blush. We provide a lightly customized GNOME experience with tweaks that can easily be undone if desired.</p>
													</div>
											</div>
										<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
												
												<div>
															<p><a href="https://store.steampowered.com/" target="_blank">
																	<img src="https://bazzite.gg/content/uploads/2024/02/Steam_icon_logo.svg" width="68" height="68" alt="Steam" decoding="async" loading="lazy">
																</a>
															</p>
														</div>
												<div>
														<p>From your handheld to your home theater PC, Steam Gaming Mode offers the premier console-like experience, and can be extended with community-developed plugins and themes thanks to <a href="https://decky.xyz/" target="_blank">Decky Loader</a>. </p>
													</div>
											</div>
										<div data-anim-type="fadeInRight" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p><img width="1863" height="1178" src="https://bazzite.gg/content/uploads/2024/03/steam.webp" alt="Steam Gaming Mode screenshot" decoding="async" loading="lazy">
															</p>
														</div>
										<div data-anim-type="fadeInLeft" data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
															<p><img width="1863" height="1178" src="https://bazzite.gg/content/uploads/2024/03/waydroid_preview.webp" alt="Waydroid screenshot" decoding="async" loading="lazy">
															</p>
														</div>
										<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one-second" data-mobile-col="mobile-one">
												
												<div>
															<p><a href="https://waydro.id/" target="_blank">
																	<img src="https://bazzite.gg/content/uploads/2024/03/waydroid.webp" width="68" height="68" alt="Waydroid" decoding="async" loading="lazy">
																</a>
															</p>
														</div>
												<div>
														<p>Waydroid brings the Android apps and games you love to Bazzite, working side by side with your other Linux applications. Visit our <a href="https://docs.bazzite.gg/Installing_and_Managing_Software/Waydroid_Setup_Guide/" target="_blank">Waydroid setup guide</a> for more information.</p>
													</div>
											</div>
									</div>
								<div id="apps">
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
														<h2>Run your favorite<span>&nbsp;</span><p><span>applications</span><span>containers</span><span>everything</span></p><span>applications</span></h2>
													</div>
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
															<p>Bazzite utilizes the <a href="https://github.com/kolunmi/bazaar" target="_blank">Bazaar</a> app store, allowing for easy installation and management of all your favorite applications from Flathub <strong>and a curated list of apps we think you'll love</strong>.</p>
															<p><img width="600" height="415" src="https://bazzite.gg/content/uploads/2025/07/bazaar3.webp" alt="Screenshot of Bazzite's Flatpak Store, Bazaar" decoding="async" loading="lazy">
																<img width="64" height="64" src="https://bazzite.gg/content/uploads/2025/07/bazaar.svg" alt="Bazaar's Icon" decoding="async" loading="lazy">
															</p>
														</div>
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one"><p>Additionally, <strong>Bazzite brings <a href="https://docs.brew.sh/Homebrew-on-Linux" target="_blank">Brew</a> and the tools and techniques created by the cloud native community to your desktop.</strong> Built in container support means packages for <em>any</em> Linux distribution can be installed and used as if they were native applications. The included <a href="https://gitlab.gnome.org/chergert/ptyxis" target="_blank" alt="Ptyxis terminal (Pronounced Ticksus)">Ptyxis terminal</a> and <a href="https://flathub.org/apps/com.ranfdev.DistroShelf" target="_blank">DistroShelf</a> provide <span>first-class</span> access to your Distrobox containers, letting you focus on what matters - your software.</p><p>Running a game, a development environment, a container for your Jellyfin server, or a utility only available on the Arch User Repository? You can rest assured it works here. <strong>Bazzite is developed on Bazzite.</strong></p></div>
										
										
									</div>
								<section id="video">
									<video preload="none" poster="https://bazzite.gg/content/uploads/2025/01/bazzite_preview.webp" width="1920" height="1080" controls="">
										<source src="https://bazzite.gg/content/uploads/2025/01/bazzite.mp4" type="video/mp4">
									</video>
									<span>
										Video provided by Brandon Lester and the <a href="https://oreonproject.org/" target="_blank">Oreon Project</a>
									</span>
								</section>
								
								<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one" id="our-team">
													<p>
														<h2>Our team</h2>
													</p>
												</div>
								<section id="image-picker">
									
									<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
												
												<div>
													<p>
														<h2>Download <span>Bazzite</span></h2>
													</p>
												</div>
												
												<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
																	<form id="image-builder" data-anim-type="fadeInUp" autocomplete="off">
																		<p><img src="https://bazzite.gg/content/uploads/2024/03/download3.svg" alt="Fill in the form on the left to download Bazzite" width="250" height="250" decoding="async" loading="lazy">
																		</p>
																		<div>
																			<p><label for="selectedHardware">What hardware are you using?</label></p>
																		</div>
																		<p><span>If you're having issues booting this ISO, ensure the <span>Setup Utility -&gt; USB Configuration -&gt; USB Dual Role Device</span> setting in your BIOS is set to <span>XHCI</span> and not <span>DRD</span>.</span>
																		</p>
																		
																		<p><span>Your hardware has issues booting the installation media and requires Ventoy in GRUB2 mode to boot properly. Please use <a href="https://www.ventoy.net/en/index.html" target="_blank">Ventoy</a> and <a href="https://www.ventoy.net/en/doc_grub2boot.html" target="_blank">follow these instructions</a>. Secure boot must be disabled for the installation, but can be re-enabled after our MOK key is enrolled and installation is complete.</span>
																		</p>
																		
																		<p><span>Your selected hardware does not support Steam Gaming Mode at this time. The <a href="https://docs.bazzite.gg/General/FAQ/#1-desktop-edition" target="_blank">Desktop version of Bazzite</a> is still available.</span>
																		</p>
																		
																		
																		<p><span>When using Steam Gaming Mode in a virtual machine, you <strong>must</strong> pass-through a GPU.</span>
																		</p>
																		<p><span>Steam Gaming Mode support is available for your hardware in beta, but <a href="https://docs.bazzite.gg/Handheld_and_HTPC_edition/quirks/#nvidia-exclusive-issues" target="_blank">multiple known issues exist in these builds</a>. Please note that the majority of bugs cannot be fixed except by your GPU manufacturer.</span>
																		</p>
																		<p><span>Some devices with two AMD GPUs (iGPU and dGPU) cause gamescope to be unable to detect your display resulting in a black screen. If you are affected, use an image without Steam Gaming Mode or disable your iGPU. <a href="https://github.com/ValveSoftware/gamescope/issues/1469" target="_blank">See this issue</a> for more information.</span>
																		</p>
																	</form>
																</div>
												<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
																	<p><span>
																			
																		</span>
																		<span>
																			<h3>Lenovo Legion Go &amp; Go 2 &amp; Go S<a href="https://www.lenovo.com/us/en/p/handheld/legion-go/len106g0001" aria-label="Lenovo Legion Go" target="_blank"><i></i></a></h3>
																			<div><p>A high resolution screen, detachable controllers with dual gyros, sturdy hinge, 
																			and hall effect joysticks and triggers, make the Legion Go the highest engineered
																			handheld in the market.
																			With dual per-controller gyro, 4 paddles, RGB, and touchpad that are usable in Steam 
																			Input, it matches and exceeds the Steam Deck, there's nothing to miss from 
																			Valve's offerings.</p><p>
																			
																			Even better, detach the controllers and enjoy the same experience on the couch
																			and bump to 30W to give that extra oomph to new titles.
																			Pop the stand open, dock, and use yours as a desktop replacement, as many of us
																			are doing.</p><p>
																			
																			Having the fastest Laptop CPU AMD makes, you'll forget you even had a laptop.
																			The Legion Go also makes for a great tablet, utilizing our built-in Android container
																			support to take full advantage of that touch screen.
																			Unfortunately, Windows may need to be dual booted to keep the BIOS updated.</p></div>
																		</span>
																		<span>
																			<h3>ASUS ROG Allies<a href="https://www.asus.com/us/site/gaming/rog/gaming-handheld/rog-ally.html" aria-label="ASUS ROG Ally" target="_blank"><i></i></a></h3>
																			<div><p>Being ASUS' little 'hot rod', the ROG Ally runs at over 40W stock, with a cooling
																			system to match.</p><p>
																			
																			Combined with its 120hz VRR display and 800Hz gyro, the Ally is a joy to use
																			and a must for the most discerning gamers.
																			Experience 0 latency with your actions and the most consistent frame pacing in
																			the market.</p><p>
																			
																			With granular TDP controls, fan curves, RGB, and paddle support in Steam Input,
																			Bazzite offers a hand-tuned experience for the Ally, that will have you leaving
																			Windows behind.</p></div>
																		</span>
																		<span>
																			<h3>Valve Steam Deck <a href="https://store.steampowered.com/steamdeck/" aria-label="Valve Steam Deck" target="_blank"><i></i></a></h3>
																			<p>Bazzite offers a solid alternative to SteamOS, with up-to-date kernel,
																			packages, and a first class desktop experience. Choose between
																			KDE Plasma and GNOME, and layer system packages to turn your Steam Deck into a mini
																			computer while still retaining feature parity with SteamOS, including firmware and BIOS
																			updates.</p>
																		</span>
																		<span>
																			<h3>GPD Devices <a href="https://gpd.hk/" aria-label="GPD Devices" target="_blank"><i></i></a></h3>
																			<div><p>GPD devices offer the smallest form factor laptops out in the market, with high quality controllers built-in.
																			It is intended for the seasoned road warrior.
																			Great for gaming, work, and even light machine learning. <br>
																			The GPD devices are a joy to use, and fully supported by Bazzite. It has out-of-the-box support for the L4, R4 buttons, touchpad, and gyro to be used in
																			Steam Input, and bundled ROCm for your machine learning needs.</p><p>
																			
																			We support all of GPD devices (2023/2024), including GPD Win 4, GPD Win Mini, Max 2.</p></div>
																		</span>
																		<span>
																			<h3>OneXPlayer <a href="https://onexplayerstore.com/" aria-label="OneXPlayer Devices" target="_blank"><i></i></a></h3>
																			<div><p>OneXPlayer devices are the best 3-in-1 devices on the market, with great build quality, displays, and performance. Snap on the controllers, game, and then replace with a folio keyboard to take your work on the go.</p><p>
																			
																			We offer great for all OneXPlayer devices. The exception is TDP Control for Intel devices, where the turbo button can be used to switch between 15W and 25W, and the 2 Pro not having speaker support.</p><p>
																			
																			Supported devices enjoy Gyro, RGB, back button support, fan curves, and charge limitting out of the box.</p></div>
																		</span>
																		<span>
																			<h3>AOKZOE<a href="https://aokzoestore.com/" aria-label="AOKZOE Devices" target="_blank"><i></i></a></h3>
																			<div><p>
																			The AOKZOE A1X offers a premium experience, with a nice large 8in 1200p VRR screen, great speakers, and incredible performance.
																			</p><p>
																			It is fully supported on Bazzite, with Gyro, RGB, fan curves, and charge limitting working out of the box.
																			</p><p>
																			A1, A1 Pro, and A2 are also partly supported, with basic controls and fan curves. We are looking into improving support for these devices in the future.
																			</p></div>
																		</span>
																		<span>
																			<h3>Ayn Loki</h3>
																			<div><p>The Ayn Loki is our pick for the most portable handheld in the market right now.</p><p>
																			With a great high resolution screen, controls that feel great, and sick looking RGB handles, there's a lot to love. We offer RGB control out-of-the-box, and the handheld experience you would expect with such a handheld. We have verified support for the Loki Max, but if you own one of the other variants then get in touch with us.
																			We will make sure the next Bazzite version has great support for your unit too.</p></div>
																		</span>
																		<span>
																			<h3>MSI Claw</h3>
																			<div><p>The MSI Claw is a powerful Intel-based handheld gaming PC with impressive performance and features.
																				With its sleek design, responsive controls, and vibrant display, the Claw delivers an excellent gaming experience. </p><p>
																				Note that Gyro is not currently supported.</p></div>
																		</span>
																		<span>
																			<h3>Home Theater PCs</h3>
																			<div><p>What makes Bazzite great for handhelds transfers over to Home Theater PCs. Enjoy the PC gaming console-like experience that a setup like that should have from the comfort of your couch. Feel free to bump up the frame-rate and resolution, without the power limitations of a handheld.</p><p>
																			
																			This experience is powered by Valve's window manager, Gamescope, which is included in our Handheld/HTPC images. Gamescope has certain hardware requirements with fantastic support for recent AMD GPUs, Intel ARC GPUs, and support for Nvidia GPUs in beta.</p></div>
																		</span>
																		<span>
																			<h3>Desktops &amp; Laptops</h3>
																			<div><p>Bazzite marries Cloud technology, gaming, and developer tools into a compelling package.
																			With Bazzite, you can work and game on the same OS, regardless of what your hardware is
																			or what your work consists of.</p><p>
																			
																			We make sure our desktop images work on ASUS Laptops, Lenovo Legion Laptops, and on AMD, Intel, and Nvidia GPUs.
																			Out-of-the-box tools such as Distrobox, Flatpak, and package layering, you
																			can install that one missing package like a VPN client you bought
																			a year ago, all while keeping the benefits of an image based operating system.</p><p>
																			
																			<em><strong>What benefits does Cloud Native bring?</strong></em> A scenario where an update breaks a package occurs, but now you can completely rollback
																			safely and quickly to any Bazzite build that came out within the last 90 days.
																			Delivered straight to you from our image repositories.
																			Rollbacks and rebasing almost act like a cloud save for your operating system.</p></div>
																		</span>
																		<span>
																			<a href="https://frame.work/" target="_blank"><img src="https://bazzite.gg/content/uploads/2024/03/framework.svg" alt="Framework" width="884" height="116" decoding="async" loading="lazy"></a>
																			<div><p><a href="https://frame.work/linux" target="_blank">Bazzite is officially supported by Framework</a>, with out-of-the-box support for all of Framework's hardware,
																			including the custom handhelds community members keep showing off!</p><p>
																			
																			Enjoy enhanced <a href="https://frame.work/products/16-graphics-module-amd-radeon-rx-7700s" target="_blank">dGPU</a> support on the <a href="https://frame.work/products/laptop16-diy-amd-7040" target="_blank">Framework 16</a>, no setup, no launch options, no fuss. Applications like Steam and Lutris are automatically configured to run on your dGPU for optimal performance.
																			Thanks to Framework's modules you can even add a MicroSD card slot and share games with your other Bazzite running hardware - plug and play just like any gaming handheld.</p><p>
																			
																			<em><strong>Using a Framework Desktop?</strong></em> Be sure to select Steam Gaming Mode for the ultimate console-like gaming experience.</p><p>
																			
																			<em><strong>Looking to do more than game?</strong></em> Bazzite comes with full support for SELinux &amp; Secure Boot, and contains helpful ujust commands for setting up TPM unlock for LUKS drive encryption and full Ollama AI workload support.</p></div>
																		</span>
																		<span>
																			<h3>Other Handheld PCs <a href="https://docs.bazzite.gg/Handheld_and_HTPC_edition/Steam_Gaming_Mode/index.html" aria-label="Alternative Handheld PCs" target="_blank"><i></i></a></h3>
																			<div><p><strong>Note</strong>: Intel and Ayaneo handheld support is still limited.</p><p>
																			
																			For most Ayaneo handhelds we offer controller support, but certain models either have stability issues or are missing a speaker driver.
																			<br>
																			If your handheld hardware is not listed, then you can still give Bazzite a try with our Handheld/HTPC image.</p><p>
																			
																			Your mileage may vary with untested hardware.</p></div>
																		</span>
																	</p>
																</div>
												
												
											</div>
									<div>
										<div data-desktop-col="one" data-laptop-col="laptop-one" data-tablet-col="tablet-one" data-mobile-col="mobile-one">
													<p>
														<h2 id="diy">Build your own</h2>
													</p>
												</div>
										<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one" data-mobile-col="mobile-one"><p>As an <a href="https://github.com/opencontainers/image-spec/blob/main/spec.md" target="_blank"><span>󱋩</span>image</a>, Bazzite is able to be consumed, repackaged, and shared. Dislike our choices or want more software pre-installed than we offer? Creating your own image is easy and builds can be scheduled to keep in-sync with us, or forge your own path. </p><p>
															Bazzite, its RPM packages, and downstream images are updated autonomously through GitHub Actions and <a href="https://copr.fedorainfracloud.org/" target="_blank">Fedora Copr</a> webhooks.
														</p></div>
										<div data-desktop-col="one-second" data-laptop-col="laptop-one-second" data-tablet-col="tablet-one" data-mobile-col="mobile-one" data-anim-type="fadeInRight">
															<p><img width="679" height="518" src="https://bazzite.gg/content/uploads/2025/02/pizzazzite2.webp" alt="Sublime screenshot showing a Bazzite Containerfile" decoding="async" loading="lazy">
															</p>
														</div>
									</div>
								</section>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Landlock-Ing Linux (185 pts)]]></title>
            <link>https://blog.prizrak.me/post/landlock/</link>
            <guid>46090969</guid>
            <pubDate>Sat, 29 Nov 2025 21:30:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.prizrak.me/post/landlock/">https://blog.prizrak.me/post/landlock/</a>, See on <a href="https://news.ycombinator.com/item?id=46090969">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h3 id="landlock-what-is-it">Landlock: What Is It?</h3>
<p>Landlock is a Linux API that lets applications explicitly declare which resources they are allowed to access. Its philosophy is similar to OpenBSD’s <code>unveil()</code> and (less so) <code>pledge()</code>: programs can make a contract with the kernel stating, “I only need these files or resources — deny me everything else if I’m compromised.”</p>
<p>It provides a simple, developer-friendly way to add defense-in-depth to applications. Compared to traditional Linux security mechanisms, Landlock is vastly easier to understand and integrate.</p>
<p>This post is meant to be an accessible introduction, and hopefully persuade you to give Landlock a try.</p>
<hr>
<h3 id="how-does-it-work">How Does It Work?</h3>
<p>Landlock is a Linux Security Module (LSM) available since Linux 5.13. Unlike MAC frameworks such as SELinux or AppArmor, Landlock applies <em>transient</em> restrictions: policies are created at runtime, enforced on the current thread and its future descendants, and disappear when the process exits.</p>
<p>You don’t tag files with labels or extended attributes. Instead, applications create policies dynamically.</p>
<p>A Landlock policy consists of two pieces:</p>
<ol>
<li><strong>Handled accesses</strong> — the categories of operations you want to restrict (e.g., filesystem read/write).</li>
<li><strong>Access grants</strong> — an explicit allowlist of which objects are permitted for those operations.</li>
</ol>
<p>For example, you could create a policy that handles all filesystem reads/writes and network binds, and grants:</p>
<ul>
<li>read-only access to <code>/home/user</code></li>
<li>read/write access to <code>/tmp</code></li>
<li>permission to bind to port <code>2222</code></li>
</ul>
<p>The application then calls <code>landlock_restrict_self()</code> to enter the restricted domain. From that point on, that thread’s child threads and child processes are permanently constrained. Restrictions cannot be revoked.</p>
<p>Policies can be layered (up to 16 layers). A child layer may further <em>reduce</em> access, but cannot reintroduce permissions the parent layer removed. For example, a child thread may add a layer to this policy to restrict itself to only reading <code>/home/user</code>, but it cannot regain permission to bind to port <code>2222</code> once a layer omits this grant.</p>
<p>Landlock is unprivileged — any application can sandbox itself. It also uses ABI versioning, allowing programs to apply best-effort sandboxing even on older kernels lacking newer features.</p>
<p>It’s also a stackable LSM, meaning you can combine it with selinux or apparmor in a supplemental layer.</p>
<hr>
<h3 id="why-should-you-use-it">Why Should You Use It?</h3>
<p>Landlock shines when an application has a predictable set of files or directories it needs. For example, a web server could restrict itself to accessing only <code>/var/www/html</code> and <code>/tmp</code>.</p>
<p>Unlike SELinux or AppArmor, Landlock policies don’t require administrator involvement or system-wide configuration. Developers can embed policies directly in application code, making sandboxing a natural part of the development process.</p>
<p>Because Landlock requires <em>no privileges</em> to use, adding it to most programs is straightforward.</p>
<p>Bindings exist for languages such as Rust, Go, and Haskell, and several projects provide user-friendly <code>unveil</code>-style wrappers.</p>
<p>A official c library doesn’t exist yet unfortunately, but there’s several out there you can try.</p>
<p>Here’s a quick rust example:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>use</span> landlock::{
</span></span><span><span>    <span>ABI</span>, Access, AccessFs, Ruleset, RulesetAttr, RulesetCreatedAttr, RulesetStatus, RulesetError,
</span></span><span><span>    path_beneath_rules,
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>fn</span> <span>restrict_thread</span>() -&gt; Result<span>&lt;</span>(), RulesetError<span>&gt;</span> {
</span></span><span><span>    <span>let</span> abi <span>=</span> <span>ABI</span>::V1;
</span></span><span><span>    <span>let</span> status <span>=</span> Ruleset::default()
</span></span><span><span>        .handle_access(AccessFs::from_all(abi))<span>?</span>
</span></span><span><span>        .create()<span>?</span>
</span></span><span><span>        <span>// Read-only access to /usr, /etc and /dev.
</span></span></span><span><span><span></span>        .add_rules(path_beneath_rules(<span>&amp;</span>[<span>"/usr"</span>, <span>"/etc"</span>, <span>"/dev"</span>], AccessFs::from_read(abi)))<span>?</span>
</span></span><span><span>        <span>// Read-write access to /home and /tmp.
</span></span></span><span><span><span></span>        .add_rules(path_beneath_rules(<span>&amp;</span>[<span>"/home"</span>, <span>"/tmp"</span>], AccessFs::from_all(abi)))<span>?</span>
</span></span><span><span>        .restrict_self()<span>?</span>;
</span></span><span><span>
</span></span><span><span>    <span>match</span> status.ruleset {
</span></span><span><span>        RulesetStatus::FullyEnforced <span>=&gt;</span> <span>println!</span>(<span>"Fully sandboxed."</span>),
</span></span><span><span>        RulesetStatus::PartiallyEnforced <span>=&gt;</span> <span>println!</span>(<span>"Partially sandboxed."</span>),
</span></span><span><span>        RulesetStatus::NotEnforced <span>=&gt;</span> <span>println!</span>(<span>"Not sandboxed! Please update your kernel."</span>),
</span></span><span><span>    }
</span></span><span><span>    Ok(())
</span></span><span><span>}
</span></span></code></pre></div><hr>
<h3 id="the-state-of-linux-sandboxing-why-this-matters">The State of Linux Sandboxing: Why This Matters</h3>
<p>As Linux adoption grows, so does the amount of malware targeting desktop users. While Linux has historically enjoyed relative safety, this is largely due to smaller market share and higher technical barriers compared to Windows — not because Linux is inherently safer.</p>
<p>Linux is not a security panacea. For example, on most major distributions:</p>
<ul>
<li>Users can download and execute untrusted binaries with no warnings.</li>
<li>Shell scripts can be piped from the internet and executed blindly.</li>
<li>Many users run passwordless sudo, giving them root access on demand.</li>
<li>Unprivileged applications can typically:
<ul>
<li>Read <code>~/.ssh</code>, <code>~/.bashrc</code>, browser cookies, and anything else in <code>$HOME</code></li>
<li>Modify environment variables and <code>$PATH</code></li>
<li>Create systemd user services</li>
<li>(on X11) log keystrokes and read input devices</li>
<li>Bind to arbitrary network ports</li>
</ul>
</li>
</ul>
<p>Several tools try to improve the state of security on linux, but each has significant drawbacks:</p>
<h4 id="containerization-docker-podman">Containerization (docker, podman)</h4>
<ul>
<li>Designed for service isolation, not desktop apps.</li>
<li>Managing home directory access is clunky.</li>
<li>Many users break isolation by using <code>--privileged</code> or <code>--network host</code>.</li>
</ul>
<h4 id="flatpak--snap">Flatpak / Snap</h4>
<ul>
<li>Great for graphical applications (Flatpak especially).</li>
<li>Often require overly broad permissions.</li>
<li>Less suitable for CLI tools.</li>
</ul>
<h4 id="firejail">Firejail</h4>
<ul>
<li>Requires per-application profiles.</li>
<li>Must be explicitly invoked each time, or you need a wrapper script.</li>
</ul>
<p>From the developer side:</p>
<h4 id="seccomp">seccomp</h4>
<ul>
<li>Powerful syscall filtering.</li>
<li>Tedious and error-prone to configure.</li>
<li>Blacklists are fragile; new syscalls can break things.</li>
<li>Argument filtering is difficult and full of TOCTOU hazards.</li>
</ul>
<h4 id="selinux">SELinux</h4>
<ul>
<li>Extremely powerful, but difficult to understand.</li>
<li>Requires system-wide policies and admin involvement.</li>
<li>Many users disable it due to complexity.</li>
<li>Not enabled on most distributions by default. (used a lot in android)</li>
</ul>
<h4 id="apparmor">AppArmor</h4>
<ul>
<li>Easier than SELinux, but still requires admin-defined profiles.</li>
<li>Applies system-wide and lacks per-process namespacing.</li>
<li>Gets disabled by many distributions, but is more commonly used in the desktop.</li>
</ul>
<h4 id="landlock">Landlock</h4>
<ul>
<li>Unprivileged</li>
<li>Application-centric</li>
<li>Easy to integrate</li>
<li>Deny-by-default</li>
<li>Widely supported since 5.13</li>
<li>Backward and forward compatibility mechanisms.</li>
</ul>
<p>Landlock isn’t perfect, but it fills a major gap: a simple, self-contained unprivileged sandboxing tool.</p>
<h3 id="what-landlock-could-bring-to-the-table">What landlock could bring to the table:</h3>
<p>Long-running system daemons that run with elevated privileges could benefit from landlock restrictions.</p>
<p>Desktop applications dealing with binary formats, like pdf readers, image viewers web browsers, and word processors can be
restricted to accessing the files they originally opened.</p>
<p>FTP and HTTP servers can be bound to the files they need. Even if nginx is running as root, if an attacker gets a
full reverse shell, they won’t be able to see access files outside the policy.</p>
<p>If the supervisor proposal gets added, we could bring an android-like permissions system to the linux desktop. Flatpak does a
decent job at this, but imagine if every process in your desktop would need to explicitly ask (at least once) before accessing
sensitive files or resources.</p>
<p>Pair that with an accessible GUI and a system for handling updates and saving permission grants, and we have potential for
a safer, more secure linux user experience on the desktop.</p>
<hr>
<h3 id="ongoing-work-in-landlock">Ongoing Work in Landlock</h3>
<p>Several promising features are under active development:</p>
<ul>
<li>
<p><a href="https://marc.info/?l=linux-fsdevel&amp;m=174105064226536&amp;w=2"><strong>Supervise Mode</strong></a><br>
Lets a userspace “supervisor” interactively allow or deny access — similar to Android-style permission prompts.</p>
</li>
<li>
<p><a href="https://github.com/landlock-lsm/linux/issues/6"><strong>Socket Restrictions</strong></a>
Fine-grained control over which types of sockets or ports processes may use.</p>
</li>
<li>
<p><a href="https://lore.kernel.org/all/20250221184417.27954-2-gnoack3000@gmail.com/"><strong>LANDLOCK_RESTRICT_SELF_TSYNC</strong></a>
Ensures restrictions propagate to all threads in a process.</p>
</li>
<li>
<p><a href="https://lore.kernel.org/linux-security-module/cover.1763931318.git.m@maowtm.org/T/#t"><strong>LANDLOCK_ADD_RULE_QUIET</strong></a>
Allows suppressing audit messages for certain objects.</p>
</li>
<li>
<p><a href="https://lore.kernel.org/linux-security-module/20251126122039.3832162-1-utilityemal77@gmail.com/T/#t"><strong>LANDLOCK_ADD_RULE_NO_INHERIT</strong> <em>(disclosure: this is my patch series)</em></a>
Prevents rules from unintentionally inheriting permissions from parent directories, giving finer-grained filesystem control.</p>
</li>
</ul>
<hr>
<h3 id="tldr">TL;DR</h3>
<p>Landlock is a simple, unprivileged, deny-by-default sandboxing mechanism for Linux.<br>
It’s easy to understand, easy to integrate, and has tremendous potential for improving desktop and application security.</p>
<p>Give it a try in your application.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Europe's New War on Privacy (141 pts)]]></title>
            <link>https://unherd.com/2025/11/europes-new-war-on-privacy/</link>
            <guid>46090794</guid>
            <pubDate>Sat, 29 Nov 2025 21:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unherd.com/2025/11/europes-new-war-on-privacy/">https://unherd.com/2025/11/europes-new-war-on-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=46090794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                            <p><span>In theory, Chat Control should have been buried last month. The EU’s ominous plan to mass-scan citizens’ private messages was met with overwhelming public resistance in Germany, with the country’s government refusing to approve it. But Brussels rarely retreats merely because the public demands it. And so, true to form, a reworked version of the text is already being pushed forward — this time out of sight, behind closed doors.&nbsp;</span></p>
<p><span>Chat Control, formally known as the Child Sexual Abuse Regulation, was first proposed by the European Commission in 2022. The original plan would have made it mandatory for email and messenger providers to scan private, even encrypted, communications — with the purported aim of detecting child sexual abuse material.&nbsp;</span></p>
<p><span>The tool was sold as a noble crusade against some of the world’s most horrific crimes. But critics argued that the tool risked becoming a blueprint for generalised surveillance, by essentially giving states and EU institutions the ability to scan every private message. Indeed, a </span><a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12726-Child-sexual-abuse-online-detection-removal-and-reporting-/public-consultation_en"><span>public consultation</span></a><span> preceding the proposal revealed that a majority of respondents opposed such obligations, with over 80% explicitly rejecting its application to end-to-end encrypted communications.</span></p>
<p><span>Yet despite repeated blockages, and widespread criticism for violating privacy and fundamental rights, the text was never abandoned. Instead, it was repackaged, and continually pushed forward from one Council presidency to the next. Each time democratic resistance stopped the original plan, it kept returning in new forms, under new labels, each time dressed up as a “necessary” and “urgent” tool to protect children online, yet always preserving its core logic: normalising government-mandated monitoring of private communications on an unprecedented scale.&nbsp;</span></p>
<p>“The tool was sold as a noble crusade against some of the world’s most horrific crimes.”</p>
<p><span>In May, the European Commission once again presented its proposal. Yet several states objected. That included Germany, but also Poland, Austria and the Netherlands. As a result, Denmark, which currently holds the rotating presidency of the European Council, immediately began drafting a new version, known as “Chat Control 2.0” and unveiled earlier this month, which removed the requirement for general monitoring of private chats; the searches would now remain formally voluntary for providers. All this happened under the auspices of Coreper, the Committee of Permanent Representatives — one of the most powerful, but least visible, institutions in the EU decision-making process. It is where most EU legislation is actually negotiated; if Coreper agrees on a legislative file, member states almost always rubber-stamp it.&nbsp;</span></p>
<p><span>The gamble worked. Yesterday, this revised version was quietly greenlit by Coreper, essentially paving the way for the text’s adoption by the Council, possibly as early as December. As digital rights campaigner and former MEP Patrick Breyer </span><a href="https://www.patrick-breyer.de/en/chat-control-2-0-through-the-back-door-breyer-warns-the-eu-is-playing-us-for-fools-now-theyre-scanning-our-texts-and-banning-teens/" target="_blank" rel="noopener"><span>put it</span></a><span>, this manoeuvre amounts to “a deceptive sleight of hand” aimed at bypassing meaningful democratic debate and oversight.&nbsp;</span></p>
<p><span>While the removal of mandatory on-device detection is an improvement on the first draft, the new text still contains two extremely problematic features. First, it encourages “voluntary” mass scanning by online platforms — a practice already allowed in “temporary” form, which would now become a lasting feature of EU law. Second, it effectively outlaws anonymous communication by introducing mandatory age-verification systems.&nbsp;</span></p>

<p><span>An </span><a href="https://csa-scientist-open-letter.org/Nov2025" target="_blank" rel="noopener"><span>open letter</span></a><span> signed by 18 of Europe’s leading cybersecurity and privacy academics warned that the latest proposal poses “high risks to society without clear benefits for children”. The first, in their view, is the expansion of “voluntary” scanning, including automated text analysis using AI to identify ambiguous “grooming” behaviours. This approach, they argue, is deeply flawed. Current AI systems are incapable of properly distinguishing between innocent conversation and abusive behaviour. As the experts explain, AI-driven grooming detection risks sweeping vast numbers of normal, private conversations into a dragnet, overwhelming investigators with false positives and exposing intimate communications to third parties.&nbsp;</span></p>
<p><span>Breyer further emphasised this danger by noting that no AI can reliably distinguish between innocent flirtation, humorous sarcasm — and criminal grooming. He warned that this amounts to a form of digital witch-hunt, whereby the mere appearance of words like “love” or “meet” in a conversation between family members, partners or friends could trigger intrusive scrutiny. This is not child protection, Breyer has argued, but mass suspicion directed at the entire population. Even under the existing voluntary regime, German federal police warn that roughly half of all reports received are criminally irrelevant, representing tens of thousands of leaked legal chats annually. According the Swiss Federal Police, meanwhile,&nbsp;</span><a href="https://www.republik.ch/2022/12/08/die-dunklen-schatten-der-chatkontrolle"><span>80%</span></a><span> of machine-reported content is not illegal. It might, for example, encompass harmless holiday photos showing nude children playing at a beach. The new text would expand these risks dramatically.&nbsp;</span></p>
<p><span>Further concerns arise from Article 4 of the new compromise proposal, which requires providers to implement “all appropriate risk mitigation measures”. This clause could allow authorities to pressure encrypted messaging services to enable scanning, even if this undermines their core security model. In practice, this could mean requiring providers such as WhatsApp, Signal or Telegram to scan messages on users’ devices before encryption is applied.&nbsp;</span></p>
<p><span>The Electronic Frontier Foundation has </span><a href="https://www.eff.org/deeplinks/2025/09/chat-control-back-menu-eu-it-still-must-be-stopped-0?utm_source=chatgpt.com" target="_blank" rel="noopener"><span>noted</span></a><span> that this approach risks creating a permanent security infrastructure, one which could gradually become universal. Meta, Google and Microsoft already scan unencrypted content voluntarily; extending this practice to encrypted content would merely require technical changes. Moreover, what begins as a voluntary option can easily become compulsory in practice, as platforms face reputational, legal and market pressure to “cooperate” with the authorities. Furthermore, this doesn’t affect just people in the EU, but everyone around the world, including the United States. If platforms decide to stay in the EU, they would be forced to scan the conversations of everyone in the bloc. If you’re not in the EU, but you chat with someone who is, then your privacy is compromised too.&nbsp;</span></p>
<p><span>Another major danger is the introduction of mandatory age-verification systems for app stores and private messaging services. Though the Council claims these systems can be designed to “preserve privacy”, critics </span><a href="https://law.stanford.edu/wp-content/uploads/2025/07/Segregate-and-Suppress.pdf" target="_blank" rel="noopener"><span>insist</span></a><span> that the very concept is technologically unworkable. Age assessments inevitably rely on biometric and behavioural data, both of which require invasive data collection. Far from protecting children, these systems would increase the volume of sensitive personal information being stored and potentially exploited.&nbsp;</span></p>

<p><span>Requiring official identity documents for online verification would exclude millions of people who lack easy access to digital IDs or who won’t provide such sensitive documentation merely to use a messaging service. In practice, this would spell the end of anonymous communication online, forcing users to present ID or face scans simply to open an email or messaging account. Breyer has warned that such measures would be particularly disastrous for whistleblowers, journalists, political activists and others reliant on online anonymity. It would also push under-16s towards less safe, poorly regulated alternatives that lack encryption or basic safety protections.&nbsp;</span></p>
<p><span>Ultimately, critics </span><a href="https://www.patrick-breyer.de/en/posts/messaging-and-chat-control/" target="_blank" rel="noopener"><span>argue</span></a><span> that mass surveillance is simply the wrong approach to combating child sexual exploitation. Scanning private messages does not stop the circulation of child abuse material. Platforms such as Facebook have used scanning technologies for years, yet the number of automated reports continues to rise. Moreover, mandatory scanning would still fail to detect perpetrators who distribute material through decentralised secret forums or via encrypted archives shared using only links and passwords — methods that scanning algorithms cannot successfully penetrate. The most effective strategy would be to delete known abuse material from online hosts, something Europol has repeatedly failed to do.&nbsp;</span></p>
<p><span>Chat Control, in short, would do little to actually help victims of child sexual exploitation while harming everyone else. Every message would become subject to surveillance, without any judicial oversight, contrary to long-standing guarantees of private correspondence. There’s a legal question here too. The EU Court of Justice has previously ruled that general and automatic analysis of private communications violates fundamental rights, yet the EU is now poised to adopt legislation that contravenes this precedent. Once adopted, it could take years for a new judicial challenge to overturn it.&nbsp;</span></p>
<p><span>The confidentiality of electronic communication — essential for personal privacy, business secrecy and democratic participation — would be sacrificed. Sensitive conversations could be read, analysed, wrongly flagged or even misused, as past scandals involving intelligence officials and tech employees have shown. One of the most notorious cases of intelligence abuse came from the US National Security Agency, in which multiple NSA employees were </span><a href="https://www.reuters.com/article/world/uk/nsa-staff-used-spy-tools-on-spouses-ex-lovers-watchdog-idUSBRE98Q14H/" target="_blank" rel="noopener"><span>caught</span></a><span> using the agency’s powerful surveillance tools to spy on romantic partners and ex-lovers. Leaked documents have also </span><a href="https://www.bbc.com/news/technology-26367781" target="_blank" rel="noopener"><span>shown</span></a><span>&nbsp;that the UK intelligence agency GCHQ captured and stored images from Yahoo webcam chats, including millions of sexually explicit images of completely innocent users. There have also been several cases of Big Tech employees — from </span><a href="https://www.wired.com/2010/09/google-spy/" target="_blank" rel="noopener"><span>Google</span></a><span> to </span><a href="https://www.vice.com/en/article/facebook-employees-look-at-user-data/" target="_blank" rel="noopener"><span>Facebook</span></a><span> — using internal tools to spy on unsuspecting users.&nbsp;</span></p>
<p><span>Furthermore, secure encryption, a foundation of cybersecurity, would be compromised by introducing backdoors or client-side scanning tools that foreign intelligence services or criminal actors could exploit. At the same time, the responsibility for criminal investigations would shift from democratically accountable authorities to opaque corporate algorithms, with minimal transparency or oversight.&nbsp;</span></p>
<p><a href="https://stopchatcontrol.eu/" target="_blank" rel="noopener"><span>Opponents</span></a><span> therefore argue that the EU should instead adopt a fundamentally different approach: one that protects children without undermining fundamental rights. They propose ending the current voluntary scanning of private messages by US internet companies — restoring the principle that targeted surveillance requires a judicial warrant and must be limited to individuals reasonably suspected of wrongdoing — and maintain that secure end-to-end encryption, and the right to anonymous communication, must be preserved.&nbsp;</span></p>
<p><span>Particularly worrying is the issue of function creep, the process by which a technology introduced for a narrowly defined purpose gradually expands to serve broader, and sometimes entirely different, purposes over time. The UK’s Online Safety Act, passed in October 2023, obliges firms to develop child sexual abuse detection systems, even though the British government itself admits that such infrastructure is not yet technically available, creating legal authority awaiting technical capability. In the United States, “temporary” surveillance measures introduced under the post-9/11 Patriot Act became permanent, and indeed expanded in scope. Once a technological infrastructure for comprehensive online surveillance exists, it can easily be repurposed and is hard to dismantle. Technologies designed to detect harmful content can quickly be extended to political repression; examples from authoritarian states demonstrate how similar systems are used to identify and target dissidents.&nbsp;</span></p>
<p><span>Breyer </span><a href="https://www.patrick-breyer.de/en/chat-control-2-0-through-the-back-door-breyer-warns-the-eu-is-playing-us-for-fools-now-theyre-scanning-our-texts-and-banning-teens/" target="_blank" rel="noopener"><span>summarised</span></a><span> this pattern starkly: “They are selling us security but delivering a total surveillance machine. They promise child protection but punish our children and criminalise privacy.” The implications are ominous. Europe effectively stands on the threshold of building a machine that can see everything. Once constructed, it will serve not only the current political authorities — the idea of Ursula von der Leyen spying on everyone’s messages is disturbing enough — but whoever wields power next. With yet another vote approaching, the window to stop Chat Control is narrowing.&nbsp;</span></p>
                            
                            
                            <hr>
                            
                            
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Nano PDF – A CLI Tool to Edit PDFs with Gemini's Nano Banana (126 pts)]]></title>
            <link>https://github.com/gavrielc/Nano-PDF</link>
            <guid>46090619</guid>
            <pubDate>Sat, 29 Nov 2025 20:44:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gavrielc/Nano-PDF">https://github.com/gavrielc/Nano-PDF</a>, See on <a href="https://news.ycombinator.com/item?id=46090619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/gavrielc/Nano-PDF/main/assets/Nano%20PDF.png"><img src="https://raw.githubusercontent.com/gavrielc/Nano-PDF/main/assets/Nano%20PDF.png" alt="Nano PDF Logo" width="300"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Nano PDF Editor</h2><a id="user-content-nano-pdf-editor" aria-label="Permalink: Nano PDF Editor" href="#nano-pdf-editor"></a></p>
<p dir="auto"><a href="https://badge.fury.io/py/nano-pdf" rel="nofollow"><img src="https://camo.githubusercontent.com/2e2dcef7e2e2fd1932b94eb20013a05f63898fa96bf61739923bc5e4fc89870c/68747470733a2f2f62616467652e667572792e696f2f70792f6e616e6f2d7064662e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/nano-pdf.svg"></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/93a33cfc2339ec3fa9be792576576fbaafc42b0c7031285662b02f3aca1e1c59/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d626c75652e737667" alt="Python 3.10+" data-canonical-src="https://img.shields.io/badge/python-3.10+-blue.svg"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/fdf2982b9f5d7489dcf44570e714e3a15fce6253e0cc6b5aa61a075aac2ff71b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a></p>
<p dir="auto">A CLI tool to edit PDF slides using natural language prompts, powered by Google's <strong>Gemini 3 Pro Image</strong> ("Nano Banana") model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Natural Language Editing</strong>: "Update the graph to include data from 2025", "Change the chart to a bar graph".</li>
<li><strong>Add New Slides</strong>: Generate entirely new slides that match your deck's visual style.</li>
<li><strong>Non-Destructive</strong>: Preserves the searchable text layer of your PDF using OCR re-hydration.</li>
<li><strong>Multi-page &amp; Parallel</strong>: Edit multiple pages in a single command with concurrent processing.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<p dir="auto">Nano PDF uses Gemini 3 Pro Image (aka Nano Banana) and PDF manipulation to enable quick edits of PDFs with natural language editing:</p>
<ol dir="auto">
<li><strong>Page Rendering</strong>: Converts target PDF pages to images using Poppler</li>
<li><strong>Style References</strong>: Optionally includes style reference pages with generation request to understand visual style (fonts, colors, layout)</li>
<li><strong>AI Generation</strong>: Sends images + prompts to Gemini 3 Pro Image, which generates edited versions</li>
<li><strong>OCR Re-hydration</strong>: Uses Tesseract to restore searchable text layer to generated images</li>
<li><strong>PDF Stitching</strong>: Replaces original pages with AI-edited versions while preserving document structure</li>
</ol>
<p dir="auto">The tool processes multiple pages in parallel for speed, with configurable resolution (4K/2K/1K) to balance quality vs. cost.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">You need a <strong>paid</strong> Google Gemini API key with billing enabled. Free tier keys do not support image generation.</p>
<ol dir="auto">
<li>Get an API key from <a href="https://aistudio.google.com/api-keys" rel="nofollow">Google AI Studio</a></li>
<li>Enable billing on your Google Cloud project</li>
<li>Set it as an environment variable:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="export GEMINI_API_KEY=&quot;your_api_key_here&quot;"><pre><span>export</span> GEMINI_API_KEY=<span><span>"</span>your_api_key_here<span>"</span></span></pre></div>
<p dir="auto"><strong>Note:</strong> This tool uses Gemini 3 Pro Image which requires a paid API tier. See <a href="https://ai.google.dev/pricing" rel="nofollow">pricing</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Edit</h3><a id="user-content-basic-edit" aria-label="Permalink: Basic Edit" href="#basic-edit"></a></p>
<p dir="auto">Edit a single page (e.g., Page 2):</p>
<div dir="auto" data-snippet-clipboard-copy-content="nano-pdf edit my_deck.pdf 2 &quot;Change the title to 'Q3 Results'&quot;"><pre>nano-pdf edit my_deck.pdf 2 <span><span>"</span>Change the title to 'Q3 Results'<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Multi-page Edit</h3><a id="user-content-multi-page-edit" aria-label="Permalink: Multi-page Edit" href="#multi-page-edit"></a></p>
<p dir="auto">Edit multiple pages in one go:</p>
<div dir="auto" data-snippet-clipboard-copy-content="nano-pdf edit my_deck.pdf \
  1 &quot;Update date to Oct 2025&quot; \
  5 &quot;Add company logo&quot; \
  10 &quot;Fix typo in footer&quot;"><pre>nano-pdf edit my_deck.pdf \
  1 <span><span>"</span>Update date to Oct 2025<span>"</span></span> \
  5 <span><span>"</span>Add company logo<span>"</span></span> \
  10 <span><span>"</span>Fix typo in footer<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Add New Slides</h3><a id="user-content-add-new-slides" aria-label="Permalink: Add New Slides" href="#add-new-slides"></a></p>
<p dir="auto">Insert a new AI-generated slide into your deck:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add a title slide at the beginning
nano-pdf add my_deck.pdf 0 &quot;Title slide with 'Q3 2025 Review'&quot;

# Add a slide after page 5
nano-pdf add my_deck.pdf 5 &quot;Summary slide with key takeaways as bullet points&quot;"><pre><span><span>#</span> Add a title slide at the beginning</span>
nano-pdf add my_deck.pdf 0 <span><span>"</span>Title slide with 'Q3 2025 Review'<span>"</span></span>

<span><span>#</span> Add a slide after page 5</span>
nano-pdf add my_deck.pdf 5 <span><span>"</span>Summary slide with key takeaways as bullet points<span>"</span></span></pre></div>
<p dir="auto">The new slide will automatically match the visual style of your existing slides and uses document context by default for better relevance.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<ul dir="auto">
<li><code>--use-context</code> / <code>--no-use-context</code>: Include the full text of the PDF as context for the model. Disabled by default for <code>edit</code>, <strong>enabled by default for <code>add</code></strong>. Use <code>--no-use-context</code> to disable.</li>
<li><code>--style-refs "1,5"</code>: Manually specify which pages to use as style references.</li>
<li><code>--output "new.pdf"</code>: Specify the output filename.</li>
<li><code>--resolution "4K"</code>: Image resolution - "4K" (default), "2K", or "1K". Higher quality = slower processing.</li>
<li><code>--disable-google-search</code>: Prevents the model from using Google Search to find information before generating (enabled by default).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fixing Presentation Errors</h3><a id="user-content-fixing-presentation-errors" aria-label="Permalink: Fixing Presentation Errors" href="#fixing-presentation-errors"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Fix typos across multiple slides
nano-pdf edit pitch_deck.pdf \
  3 &quot;Fix the typo 'recieve' to 'receive'&quot; \
  7 &quot;Change 'Q4 2024' to 'Q1 2025'&quot;"><pre><span><span>#</span> Fix typos across multiple slides</span>
nano-pdf edit pitch_deck.pdf \
  3 <span><span>"</span>Fix the typo 'recieve' to 'receive'<span>"</span></span> \
  7 <span><span>"</span>Change 'Q4 2024' to 'Q1 2025'<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Visual Design Changes</h3><a id="user-content-visual-design-changes" aria-label="Permalink: Visual Design Changes" href="#visual-design-changes"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Update branding and colors
nano-pdf edit slides.pdf 1 &quot;Make the header background blue and text white&quot; \
  --style-refs &quot;2,3&quot; --output branded_slides.pdf"><pre><span><span>#</span> Update branding and colors</span>
nano-pdf edit slides.pdf 1 <span><span>"</span>Make the header background blue and text white<span>"</span></span> \
  --style-refs <span><span>"</span>2,3<span>"</span></span> --output branded_slides.pdf</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Content Updates</h3><a id="user-content-content-updates" aria-label="Permalink: Content Updates" href="#content-updates"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Update financial data
nano-pdf edit report.pdf 12 &quot;Update the revenue chart to show Q3 at $2.5M instead of $2.1M&quot;"><pre><span><span>#</span> Update financial data</span>
nano-pdf edit report.pdf 12 <span><span>"</span>Update the revenue chart to show Q3 at <span>$2</span>.5M instead of <span>$2</span>.1M<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Batch Processing with Context</h3><a id="user-content-batch-processing-with-context" aria-label="Permalink: Batch Processing with Context" href="#batch-processing-with-context"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Use full document context for consistency
nano-pdf edit presentation.pdf \
  5 &quot;Update the chart colors to match the theme&quot; \
  8 &quot;Add the company logo in the bottom right&quot; \
  --use-context"><pre><span><span>#</span> Use full document context for consistency</span>
nano-pdf edit presentation.pdf \
  5 <span><span>"</span>Update the chart colors to match the theme<span>"</span></span> \
  8 <span><span>"</span>Add the company logo in the bottom right<span>"</span></span> \
  --use-context</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding New Slides</h3><a id="user-content-adding-new-slides" aria-label="Permalink: Adding New Slides" href="#adding-new-slides"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add a new agenda slide at the beginning
nano-pdf add quarterly_report.pdf 0 &quot;Agenda slide with: Overview, Financial Results, Q4 Outlook&quot;"><pre><span><span>#</span> Add a new agenda slide at the beginning</span>
nano-pdf add quarterly_report.pdf 0 <span><span>"</span>Agenda slide with: Overview, Financial Results, Q4 Outlook<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Google Search</h3><a id="user-content-using-google-search" aria-label="Permalink: Using Google Search" href="#using-google-search"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Google Search is enabled by default - the model can look up current information
nano-pdf edit deck.pdf 5 &quot;Update the market share data to latest figures&quot;

# Disable Google Search if you want the model to only use provided context
nano-pdf add deck.pdf 3 &quot;Add a summary slide&quot; --disable-google-search"><pre><span><span>#</span> Google Search is enabled by default - the model can look up current information</span>
nano-pdf edit deck.pdf 5 <span><span>"</span>Update the market share data to latest figures<span>"</span></span>

<span><span>#</span> Disable Google Search if you want the model to only use provided context</span>
nano-pdf add deck.pdf 3 <span><span>"</span>Add a summary slide<span>"</span></span> --disable-google-search</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3.10+</li>
<li><code>poppler</code> (for PDF rendering)</li>
<li><code>tesseract</code> (for OCR)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">System Dependencies</h3><a id="user-content-system-dependencies" aria-label="Permalink: System Dependencies" href="#system-dependencies"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">macOS</h4><a id="user-content-macos" aria-label="Permalink: macOS" href="#macos"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install poppler tesseract"><pre>brew install poppler tesseract</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Windows</h4><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="choco install poppler tesseract"><pre>choco install poppler tesseract</pre></div>
<p dir="auto"><strong>Note:</strong> After installation, you may need to restart your terminal or add the installation directory to your PATH.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Linux (Ubuntu/Debian)</h4><a id="user-content-linux-ubuntudebian" aria-label="Permalink: Linux (Ubuntu/Debian)" href="#linux-ubuntudebian"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get install poppler-utils tesseract-ocr"><pre>sudo apt-get install poppler-utils tesseract-ocr</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">"Missing system dependencies" error</h3><a id="user-content-missing-system-dependencies-error" aria-label="Permalink: &quot;Missing system dependencies&quot; error" href="#missing-system-dependencies-error"></a></p>
<p dir="auto">Make sure you've installed poppler and tesseract for your platform. After installation, restart your terminal to refresh PATH. Run <code>which pdftotext</code> and <code>which tesseract</code> to verify they're accessible.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">"GEMINI_API_KEY not found" error</h3><a id="user-content-gemini_api_key-not-found-error" aria-label="Permalink: &quot;GEMINI_API_KEY not found&quot; error" href="#gemini_api_key-not-found-error"></a></p>
<p dir="auto">Set your API key as an environment variable:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export GEMINI_API_KEY=&quot;your_key_here&quot;"><pre><span>export</span> GEMINI_API_KEY=<span><span>"</span>your_key_here<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">"Gemini API Error: PAID API key required" error</h3><a id="user-content-gemini-api-error-paid-api-key-required-error" aria-label="Permalink: &quot;Gemini API Error: PAID API key required&quot; error" href="#gemini-api-error-paid-api-key-required-error"></a></p>
<p dir="auto">Gemini 3 Pro Image requires a paid API tier. Visit <a href="https://aistudio.google.com/api-keys" rel="nofollow">Google AI Studio</a> to enable billing on your project.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generated images don't match the style</h3><a id="user-content-generated-images-dont-match-the-style" aria-label="Permalink: Generated images don't match the style" href="#generated-images-dont-match-the-style"></a></p>
<p dir="auto">Try using <code>--style-refs</code> to specify reference pages that have the desired visual style. The model will analyze these pages to better match fonts, colors, and layout.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text layer is missing or incorrect after editing</h3><a id="user-content-text-layer-is-missing-or-incorrect-after-editing" aria-label="Permalink: Text layer is missing or incorrect after editing" href="#text-layer-is-missing-or-incorrect-after-editing"></a></p>
<p dir="auto">The tool uses Tesseract OCR to restore searchable text. For best results, ensure your generated images are high resolution (<code>--resolution "4K"</code>). Note that OCR may not be perfect for stylized fonts or small text.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pages are processing slowly</h3><a id="user-content-pages-are-processing-slowly" aria-label="Permalink: Pages are processing slowly" href="#pages-are-processing-slowly"></a></p>
<ul dir="auto">
<li>Use <code>--resolution "2K"</code> or <code>--resolution "1K"</code> for faster processing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running from Source</h2><a id="user-content-running-from-source" aria-label="Permalink: Running from Source" href="#running-from-source"></a></p>
<p dir="auto">If you want to run the latest development version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/gavrielc/Nano-PDF.git
cd Nano-PDF

# Install dependencies
pip install -e .

# Run the tool
nano-pdf edit my_deck.pdf 2 &quot;Your edit here&quot;"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/gavrielc/Nano-PDF.git
<span>cd</span> Nano-PDF

<span><span>#</span> Install dependencies</span>
pip install -e <span>.</span>

<span><span>#</span> Run the tool</span>
nano-pdf edit my_deck.pdf 2 <span><span>"</span>Your edit here<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All it takes is for one to work out (511 pts)]]></title>
            <link>https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</link>
            <guid>46090433</guid>
            <pubDate>Sat, 29 Nov 2025 20:22:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/">https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</a>, See on <a href="https://news.ycombinator.com/item?id=46090433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
<article id="post-24017">
	
	<!-- .entry-header -->

	<div>
		
<p>More than a decade ago, when I was applying to graduate school, I went through a period of deep uncertainty. I had tried the previous year and hadn’t gotten in anywhere. I wanted to try again, but I had a lot going against me.</p>



<p>I’d spent most of my undergrad building a student job-portal startup and hadn’t balanced it well with academics. My GPA needed explaining. My GMAT score was just okay. I didn’t come from a big-brand employer. And there was no shortage of people with similar or stronger profiles applying to the same schools.</p>



<p>Even though I had learned a few things from the first round, the second attempt was still difficult. There were multiple points after I submitted applications where I lost hope.</p>



<p>But during that stretch, a friend and colleague kept repeating one line to me:</p>



<p><em>“All it takes is for one to work out.”</em></p>



<p>He’d say it every time I spiraled. And as much as it made me smile, a big part of me didn’t fully believe it. Still, it became a little maxim between us. And eventually, he was right – that one did work out. And it changed my life.</p>



<p>I’ve thought about that framing so many times since then.</p>



<p>It’s unbelievably powerful in any high-stakes search:</p>



<p>You don’t need every job to choose you. You just need the one that’s the right fit. </p>



<p>You don’t need every house to accept your offer. You just need the one that feels like home. </p>



<p>You don’t need every person to want to build a life with you. You just need the one.</p>



<p>You don’t need ten universities to say yes. You just need the one that opens the right door.</p>



<p>These processes – college admissions, job searches, home buying, finding a partner – can be emotionally brutal. They can get you down in ways that feel personal. But in those moments, that truth can be grounding.</p>



<p>All it takes is for one to work out.</p>



<p>And that one is all you need.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-24017 -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning Feynman's Trick for Integrals (184 pts)]]></title>
            <link>https://zackyzz.github.io/feynman.html</link>
            <guid>46090269</guid>
            <pubDate>Sat, 29 Nov 2025 19:55:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zackyzz.github.io/feynman.html">https://zackyzz.github.io/feynman.html</a>, See on <a href="https://news.ycombinator.com/item?id=46090269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
        

        <h3>
            a.k.a. <span>Differentiation under the Integral Sign</span> &amp; <span>Leibniz Integral Rule</span>
        </h3>
    </p>

        <hr>

        <p>
            Among a few other integral tricks and techniques, Feynman's trick was a strong reason that made me love evaluating integrals, and although the technique itself goes back to Leibniz being commonly known as the Leibniz integral rule, it was Richard Feynman who popularized it, which is why it is also referred to as Feynman's trick.
            Here's an excerpt from his book, Surely You're Joking, Mr. Feynman:
        </p>

        <p>
            "One thing I never did learn was contour integration. I had learned to do integrals 
            by various methods shown in a book that my high school physics teacher Mr. Bader had given me.
        </p>

        <p>
            One day he told me to stay after class. "Feynman," he said, "you talk too much and you make too much noise.
            I know why. You're bored. So I'm going to give you a book. You go up there in the back, in the corner, and
            study this book, and when you know everything that's in this book, you can talk again."
        </p>

        <p>
            So every physics class, I paid no attention to what was going on with Pascal's Law, or whatever they were
            doing. I was up in the back with this book: Advanced Calculus, by Woods. Bader knew I had studied Calculus
            for the Practical Man a little bit, so he gave me the real works -- it was for a junior or senior course
            in college. It had Fourier series, Bessel functions, determinants, elliptic functions -- all kinds of
            wonderful stuff that I didn't know anything about.
        </p>

        <p>
            That book also showed how to differentiate parameters under the integral sign -- it's a certain operation.
            It turns out that's not taught very much in the universities; they don't emphasize it. But I caught on
            how to use that method, and I used that one damn tool again and again. So because I was self-taught using
            that book, I had peculiar methods of doing integrals.
        </p>

        <p>
            The result was, when guys at MIT or Princeton had trouble doing a certain integral, it was because they
            couldn't do it with the standard methods they had learned in school. If it was contour integration, they
            would have found it; if it was a simple series expansion, they would have found it. Then I come along and
            try differentiating under the integral sign, and often it worked. So I got a great reputation for doing
            integrals, only because my box of tools was different from everybody else's, and they had tried all their
            tools on it before giving the problem to me."         
        </p>

        <p>
            For me, employing this trick felt like I was using cheat codes to deal with integrals. At the same time, it enabled a lot of creativity and wishful thinking, which transformed integrals into puzzles. Unfortunately, this also means that there is no clear path on how and when to use this technique. In addition, what Feynman wrote still applies today since the method isn't taught much, if at all, in universities. Therefore, the trick can seem obscure and difficult to grasp for newcomers.
        </p>

        <p>
            In the following section, we will embark on a journey to develop some rules of thumb to have at our disposal when using Feynman's trick. These are merely some heuristics that I tend to use, so deviating from them can be perfectly acceptable. However, I hope that they can provide a path to follow when nothing obvious or intuitive occurs when someone tries to use this trick, or even better, so that they can serve as motivation for someone to start using the method.
        </p>

        <hr>
        <section id="chapter1"> <h2>Hello, World!</h2> </section>
        <hr>

        <p>
            Feynman already provided a significant hint about the trick when he mentioned differentiating under the integral sign, which is also an alternative name for the technique. More explicitly, if \(f(x,t)\) and \(\frac{\partial f(x,t)}{\partial t}\) is continuous with respect to both variables over the \([a,b]\) interval, then the following holds:
        </p>
        <p>
            \[\frac{d}{dt} \int_a^b f(x, t)dx = \int_a^b \frac{\partial f(x, t)}{\partial t}dx\]
        </p>

        <p>
            This is nice, but not so useful by itself since it doesn't say anything about how and when to apply it. Moreover, learning is not a spectator sport and one has to get their hands dirty as there are no shortcuts to it.
            Take for example chess, most people could read and understand the rules in a few minutes, however, if they would go on to play a game then most likely they would get stomped by a more experienced player. This is because the other player, through practice, learned some strategies to use when playing.
        </p>

        <p>
            Thus, with the goal to develop some strategies here as well, we will dive straight into action and approach Feynman's trick using practical examples. As a "Hello, World!" introduction, let's take a look at the following integral:
        </p>

        <p>
            \[I=\int_0^1 \frac{x-1}{\ln x}dx\]
        </p>

        <p>
            You are encouraged to try and evaluate the integral using basic methods, but the logarithm being in the
            denominator makes this integral quite stubborn to deal with. Feynman's trick aims to get rid of this issue by differentiating under the intgeral sign, with respect to a parameter, in order to obtain an
            integral that is easier to evaluate.
        </p>

        <p>
            Unfortunately in the integral from above we lack a parameter, therefore the first step is to parameterise the integral, which can even mean introducing a whole function, but for this example we will simply consider:
        </p>

        <p>
            \[I(t)=\int_0^1 \frac{x^t-1}{\ln x}dx\]
        </p>

        <p>
            Keep in mind that our original integral is just \(I(1)\). Also, surely we could've placed a parameter in many
            different places, such as:
        </p>

        <p>
            \[I(a)=\int_0^1 \frac{x-a}{\ln x}dx, \quad I(b)=\int_0^1 \frac{x-1}{b-\ln x}dx, \quad I(c)=\int_0^1 \frac{x-1}{\ln (cx)}dx\ ...\]
        </p>

        <p>
            However, the main idea behind the trick is to obtain an integral that we can evaluate easier, after differentiating with respect to the new parameter. Let's put this in action and see what happens to \(I(t)\).
        </p>

        <p>
            \[I'(t)=\int_0^1 \frac{\partial}{\partial t}\left(\frac{x^t-1}{\ln x}\right)dx  = \int_0^1x^tdx = \frac{x^{t+1}}{t+1}\bigg|_0^1 = \frac{1}{t+1}\]
        </p>

        <p>
            Notice how easy it was to evaluate the integral \(I'(t)=\int_0^1x^tdx\) from above, had we kept \(I(a)\), \(I(b)\)
            or \(I(c)\) the things wouldn't had simplified at all after differentiating, and most significantly is that we would still
            have the \(\ln x\) in the denominator, a thing which made the integral hard to deal with in the first place.
        </p>

        <p>
            We can already sense that the following might be an important question in the future:
            <span>How to parameterise the integral when using Feynman's Trick?</span>
        </p>

        <p>
            We will worry about that a bit later, for now let's finish the integral as we only found \(I'(t)\).
            Since we are looking to find \(I(1)\) we need to integrate \(I'(t)\) back and set \(t=1\) in order to arrive there.
            Here it's useful to recall that:
        </p>

        <p>
            \[\int_a^b f'(x) dx = f(b)-f(a)\]
        </p>

        <p>
            For us, \(f(x)\) is just \(I(t)\) in the above expression. Luckily \(I(0)=0\), and as we are looking for
            \(I=I(1)\) we have:
        </p>

        <p>
            \[I= I(1)-I(0)=\int_0^1 I'(t)dt = \int_0^1 \frac{1}{1+t}dt=\ln(1+t)\bigg|_0^1 = \ln 2\]
        </p>
        
        <p>
            So that is the big picture of Feynman's trick - we have an integral that is hard to evaluate
            in it's original form, therefore by differentiating under the integral sign
            we attempt to transform the integral so that it can be easier integrated, and in the end we go back to
            undo the differentiation step.
        </p>

        <hr>
        <section id="chapter2"> <h2>The parameter</h2> </section>
        <hr>

        <p>
            As emphasized above, the main goal of the technique is to obtain an integral that is easier to evaluate after
            differentiating with respect to a parameter, and one issue is that it is not always obvious how to parameterise the integral.
            In order to make things more intuitively we will play around with the integral from below.
        </p>

        <p>
            \[I=\int_0^1 \frac{\ln(1+x)}{1+x^2}dx\]
        </p>

        <p>
            The most annoying thing is the logarithm, so if we get rid of it everything should be straightforward.
            There are a few parameter possibilities which makes sense to consider, namely:
        </p>

        <p>
            \[I(a)=\int_0^1\frac{\ln(a+x)}{1+x^2}dx, \quad I(b) = \int_0^1 \frac{\ln(1+bx)}{1+x^2}dx\]
        </p>

        <p>
            With the first one we are out of luck, as differentiating with respect to \(a\) gives:
        </p>

        <p>
            \[I'(a)=\int_0^1 \frac{1}{(a+x)(1+x^2)}dx = \frac{1}{1+a^2}\int_0^1\left(\frac{1}{a+x}+\frac{a}{1+x^2}-\frac{x}{1+x^2}\right)dx\]
            \[=\frac{\ln(1+a)}{1+a^2}-\frac{\ln a}{1+a^2} +\frac{\pi}{4}\frac{a}{1+a^2}-\frac{\ln 2}{2}\frac{1}{1+a^2}\]
        </p>

        <p>
            Therefore, if we would try to go back to what we're looking, which is \(I=I(1)-I(0)\), we would end up with
            \(I=I+\text{other stuff}\). This cancels out \(I\) and we wouldn't be able to recover it.
            Unfortunately, there's no magic formula that tells a priori whether placing a parameter in a specific place would
            succeed or fail in evaluating an integral - and sometimes we are simply unlucky.
        </p>

        <p>
            In contrast, things work out nicely with the second choice from above.
        </p>

        <p>
            \[I'(b)=\int_0^1\frac{x}{(1+bx)(1+x^2)}dx=\frac{1}{1+b^2}\int_0^1\left(\frac{b}{1+x^2}+\frac{x}{1+x^2}-\frac{b}{1+bx}\right)dx\]
            \[=\frac{\pi}{4}\frac{b}{1+b^2}+\frac{\ln 2}{2}\frac{1}{1+b^2}-\frac{\ln(1+b)}{1+b^2}\]
        </p>

        <p>
            Again, we are looking to find \(I(1)\), and as \(I(0) = 0\), we have:
        </p>

        <p>
            \[I=\int_0^1 I'(b)db \Rightarrow 2I=\int_0^1 \left(\frac{\pi}{4}\frac{b}{1+b^2}+\frac{\ln 2}{2}\frac{1}{1+b^2}\right)db \Rightarrow I=\frac{\pi}{8}\ln 2\]
        </p>

        <p>
            This works, but we can do even better. Looking at the <a href="#chapter1">Hello, World!</a> integral we can see that there we simplified the logarithm in the denominator while performing \(\frac{\partial}{\partial t}x^t\). This is also the first thing that I always attempt to look for when using this technique - namely, to simplify something from the integrand which is independent to the parameter when differentiating. Surely for the current integral we got rid of the logarithm, but the denominator remained intact.
        </p>

        <p>
            In short this will be our first rule of thumb: <span>if possible, place the parameter so that something from the integral, which is not related to the parameter, gets simplified</span>.
        </p>

        <p>
            In order to achieve this with our integral we would need to get rid of \(1+x^2\), and by using \(\ln x=\frac12\ln(x^2)\) we can rewrite the integral as:
        </p>

        <p>
            \[I=\int_0^1\frac{\ln(1+x)}{1+x^2}dx=\frac12\int_0^1\frac{\ln(1+2x+x^2)}{1+x^2}dx\]
        </p>

        <p>
            Finally, in this form it's more natural to place the parameter so that it simplifies \(1+x^2\) when differentiating with respect to \(t\), namely we can consider:
        </p>

        <p>
            \[I(t)=\frac12\int_0^1 \frac{\ln(2x+t(1+x^2))}{1+x^2}dx\Rightarrow I'(t)=\frac12\int_0^1 \frac{1}{2x+t(1+x^2)}dx=\frac14\frac{\ln\left(\frac{1+\sqrt{1-t^2}}{t}\right)}{\sqrt{1-t^2}}\]
        </p>

        <p>
            Like for \(I(b)\) we are looking to find \(I(1)\), however here \(I(0)\) is equal to \(\frac12\int_0^1\frac{\ln(2x)}{1+x^2}dx\) not \(0\).
        </p>

        <p>
            \[\Rightarrow I=I(1)-I(0)+I(0)=\frac14\int_0^1\frac{\ln\left(\frac{1+\sqrt{1-t^2}}{t}\right)}{\sqrt{1-t^2}}dt+\frac12\int_0^1\frac{\ln(2x)}{1+x^2}dx\]
            \[\int_0^1\frac{\ln\left(\frac{1+\sqrt{1-t^2}}{t}\right)}{\sqrt{1-t^2}}dt\overset{\frac{1+\sqrt{1-t^2}}{t}=x}=-2\int_0^1 \frac{\ln x}{1+x^2}dx\Rightarrow I = \frac12\int_0^1 \frac{\ln 2}{1+x^2}dx=\frac{\pi}{8}\ln 2\]
        </p>

        <p>
            For this specific integral we only avoided performing partial fractions so there wasn't really a big improvement by simplifying the denominator. However I want to emphasize the importance of this because it will make things come way more natural when deciding where place the parameter. <span>Of course, in case there's not an appropiate or immediate way to achieve this, it's perfectly fine to place the parameter elsewhere too.</span>
        </p>

        <p>
            As mentioned previously, practicing is the best approach to get along with new techniques, therefore below are more integrals to evaluate alongside some hidden steps in case those will be needed. However, I strongly recommend to try and deal with the integrals before looking at any hints, and only check them afterwards for correctness.
        </p>

        <p>
                \[\int_0^\frac{\pi}{2} \frac{\ln(1-\sin x)}{\sin x}dx = ?\]
        </p>

        <p>
                Consider introducing the following parameter:
                \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-t\sin x)}{\sin x}dx \Rightarrow I'(t)= -\frac{2\arctan\left(\sqrt{\frac{1+t}{1-t}}\right)}{\sqrt{1-t^2}}\]
                This should lead to:
                \[\int_0^\frac{\pi}{2} \frac{\ln(1-\sin x)}{\sin x}dx = I(1) - I(0)=\int_0^1 I'(t) dt \overset{\sqrt{\frac{1-t}{1+t}}=x} = -\frac{3\pi^2}{8}\]
                But it would be even better if the integral would be parameterised as:
                \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-\sin t\sin x)}{\sin x}dx\]
                That is because usually when having trigonometric functions, parameterising the integral with another trigonometric function, leads to a more smoother result.
            </p>

        <p>
            \[\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx = ?\]
        </p>

        <p>
                Consider introducing the following parameter:
                \[I(t)=\int_0^1 \frac{\ln(1-t(x-x^2))}{x-x^2}dx\Rightarrow I'(t) = \frac{4\arctan\left(\sqrt{\frac{t}{4-t}}\right)}{\sqrt{t(4-t)}}\]
                This should lead to:
                \[I(1)=\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx = I(1) - I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{4-t}{t}}= x}= -\frac{\pi^2}{9}\]
            </p>

        <p>
            \[\int_0^\frac{\pi}{2} \frac{\arctan(\sin x)}{\sin x}dx = ?\]
        </p>

        <p>
                Consider introducing the following parameter:
                \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{1+t^2}}\]
                This should lead to:
                \[I(1)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx = I(1)-I(0) = \int_0^1 I'(t)dt = \frac{\pi}{2}\ln(1+\sqrt 2)\]
                It will also work if the integral is parameterised as:
                \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(\tan t\sin x)}{\sin x}dx\]
                However, in this case the first variant is simple enough to integrate back.
            </p>

        <p>
                \[\int_0^\infty x^2e^{-\left(4x^2+\frac{9}{x^2}\right)}dx = ?\]
        </p>

        <p>
                Consider introducing the following parameter:
                \[I(t)=\int_0^\infty x^2e^{-\left(4x^2+\frac{t}{x^2}\right)}dx\Rightarrow I'(t)=-\frac{\sqrt \pi}{4} e^{-4\sqrt t}\]
                Where the above result follows by using Glasser's master theorem alongside the Gaussian integral.
                This should lead to:
                \[\int_0^\infty x^2e^{-\left(4x^2+\frac{9}{x^2}\right)}dx = I(9)- I(0) + I(0) = \int_0^9 I'(t) dt +\frac{\sqrt \pi}{32}=\frac{13}{32}\frac{\sqrt \pi}{e^{12}}\]
            </p>

        <p>
                \[\int_0^1 \frac{\ln x}{1-x^2}dx = ?\]
        </p>

        <p>
                Consider parameterising the integral as:
                \[I(t)=\frac12\int_0^1\frac{\ln(1-t(1-x^2))}{1-x^2}dx\Rightarrow I'(t)=\frac{\arctan\left(\sqrt{\frac{t}{1-t}}\right)}{2\sqrt{t(1-t)}}\]
                This should lead to:
                \[\int_0^1 \frac{\ln x}{1-x^2}dx = I(1)- I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{1-t}{t}} = x}= -\frac{\pi^2}{8}\]
            </p>

        <p>
                \[\int_0^\infty \frac{e^{-x^2}}{1+x^2}dx = ?\]
        </p>

        <p>
                Consider parameterising the integral as:
                \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t) = -\frac{\sqrt \pi}{2\sqrt t}e^{-t}\]
                This should lead to:
                \[\int_0^\infty \frac{e^{-x^2}}{1+x^2}dx = e\left(I(1)-I(\infty)\right) = -e\int_1^\infty I'(t)dt= \frac{\pi e}{2}\operatorname{erfc}(1)\]
                Where \(\operatorname{erfc}(x)\) is the complementary error function.
            </p>

        <p>
                \[\int_0^\infty \frac{\ln\left(\frac{1-x^2+x^4}{(1-x^2)^2}\right)}{(1+x^2)^2}dx = ?\]
        </p>

        <p>
                Since \(1-x^2+x^4=(1+x^2)^2-3x^2\), consider parameterising the integral as:
                \[I(t)=\int_0^\infty \frac{\ln\left(\frac{t(1+x^2)^2-3x^2}{(1-x^2)^2}\right)}{(1+x^2)^2}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{t(4t-3)}}\]
                And in order to go back it should be observed that \(\frac34(1+x^2)^2-3x^2=\frac34(1-x^2)^2\).
                \[\int_0^\infty \frac{\ln\left(\frac{1-x^2+x^4}{(1-x^2)^2}\right)}{(1+x^2)^2}dx=I(1)- I\left(\frac34\right)+ I\left(\frac34\right)\]
                \[=\int_\frac34^1 I'(t)dt + \frac{\pi}{4}\ln\left(\frac{3}{4}\right) = \frac{\pi}{2}\ln\left(\frac32\right)\]
            </p>

        <hr>
        <section id="chapter3"> <h2>Accelerated Feynman's trick</h2> </section>
        <hr>

        <p>
            The previous chapter emphasized to parameterise integrals so that something from the integral, which is not related to the parameter, gets simplified when differentiating (if possible). However there are times when even though we can introduce a parameter to accomplish that, it wouldn't be enough to finish the integral.
        </p>

        <p>
            In this chapter we will look at a different way to obtain this simplification. Let's start by looking at a modified version of an integral that was previously given as an exercise.
        </p>

        <p>
            \[I=\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^4}dx\]
        </p>

        <p>
            With \(\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^2}dx\) it was quite direct to parameterise the integral as \(\int_{-\infty}^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\) since it simplifies the denominator, however the similar way to do that for our integral, \(\int_{-\infty}^\infty \frac{e^{-x^2-t(1+x^4)}}{1+x^4}dx\), doesn't seem to work as it complicates things a bit too much.
        </p>

        <p>
            There is however a way to simplify the denominator and in the same time to obtain a decent integral afterwards. Without getting into too much details I will parameterise the integral as:
        </p>

        <p>
            \[I(t)=\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^4} e^{-tx^2}(x^2\sin t+\cos t) dx\]
        </p>

        <p>
            This will seem obscure, but fear not as we will never use this approach again.
            The whole point is to simplify \(1+x^4\), and the above function was created explicitly to achieve that, as \(\frac{\partial}{\partial t}e^{-tx^2}(x^2\sin t+\cos t)\) is \(-(1+x^4)e^{-tx^2}\sin t\). Note that even though we introduced a couple other terms, those aren't disturbing.
        </p>
        
        <p>
            \[I'(t)=-\sin t\int_{-\infty}^\infty e^{-x^2}e^{-tx^2}dx \overset{(1+t)x^2\to x^2}= -\frac{\sin t}{\sqrt{1+t}} \int_{-\infty}^\infty e^{-x^2}dx=-\sqrt \pi \frac{\sin t}{\sqrt{1+t}}\]
        </p>

        <p>
            Here we are looking to find \(I=I(0)\), and we also have \(I(\infty)=0\), therefore:
        </p>

        <p>
            \[I=-\int_0^\infty I'(t)dt=\sqrt \pi \int_0^\infty \frac{\sin t}{\sqrt{1+t}}dt\overset{\sqrt{1+t}=x}=2\sqrt \pi \int_1^\infty \sin(x^2-1)dx\]
            \[=2\sqrt{\pi} \cos 1 \int_1^\infty\sin(x^2)dx-2\sqrt{\pi} \sin  1 \int_1^\infty\cos(x^2)dx\]
            \[=\pi\cos 1\frac{1-2S\left(\sqrt{\frac{2}{\pi}}\right)}{\sqrt 2}-\pi\sin 1\frac{1-2C\left(\sqrt{\frac{2}{\pi}}\right)}{\sqrt 2}\]
        </p>

        <p>
            Where \(S(x)\) and \(C(x)\) are the Fresnel integrals. However, the approach is important here, not the result itself.
        </p>

        <p>
            We can avoid the parametrisation from above by directly using \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t \, dt\), and then switch to double integrals, or put in other words: employ the accelerated Feynman's trick (in which we skip the usual parameterisation step).
        </p>

        <p>
            \[I=\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^4}dx = \int_{-\infty}^\infty e^{-x^2}\int_0^\infty e^{-tx^2}\sin t \, dtdx\]
            \[=\int_{0}^\infty \sin t\int_{-\infty}^\infty e^{-x^2}e^{-tx^2}dxdt\overset{(1+t)x^2\to x^2}=\sqrt{\pi}\int_0^\infty \frac{\sin t}{\sqrt{1+t}}dt\]
        </p>

        <p>
            The rest goes exactly as with the previous method, as all we did here was to skip differentiation step and instead we switched to double integrals.
        </p>

        <p>
            A natural question that arises here is how did \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t\, dt\) appear? Or even better, how can someone come up with similar results for other integrals? In the case from above, simply the Laplace transform of the sine function was used, however in general it's useful to have a list of such identities. There are tables of integral results that can be used - for example: Table of Integrals, Series, and Products by Gradshteyn and Ryzhik - but alternatively one can build up their own list of results which tend to appear often while evaluating other integrals.
        </p>

        <hr>

        <p>
            Let's conclude this chapter by evaluating one of the most popular integrals that appears when Feynman's trick gets into the conversation.
        </p>
            
        <p>
            \[I=\int_0^\infty \frac{\sin x}{x}dx\]
        </p>

        <p>
            Since \(\int_0^\infty e^{-xt} dt = \frac{1}{x}\), we can make use of this to rewrite the integral as:
        </p>

        <p>
            \[I=\int_0^\infty \int_0^\infty \sin x e^{-xt}dtdx = \int_0^\infty \int_0^\infty \sin x e^{-xt}dxdt=\int_0^\infty \frac{1}{1+t^2}dt =\frac{\pi}{2}\]
        </p>

        <p>
            Alternatively, we can also consider the parameter version of this integral, \(\int_0^\infty \frac{\sin x}{x}e^{-xt}dx\), however I feel like switching to double integrals is way more intuitively.
        </p>

        <p>
            It might be worth to highlight again that this method should be used preferable when parameterising the integral leads to nowhere. For the above integral, the natural introduction of \(\int_0^\infty \frac{\sin(tx)}{x}dx\) unfortunatelly does fail, as we obtain a divergent integral after differentiating under the integral sign.
        </p>

        <p>
            Like in the previous chapter below are more integrals alongside some hints in order to practice with the accelerated variation of Feynman's trick. However in this case I do recommend to peek at hints faster in case nothing obvious comes to mind, and afterwards to attempt and understand why the mentioned identity can be used.
        </p>

        <p>
                \[\int_0^\infty \sin(x^2)dx=?\]
        </p>

        <p>
                Start by substituting \(x^2\to x\) and then switch to double integrals using:
                \[\int_0^\infty e^{-xt^2}dt = \frac{\sqrt \pi}{2\sqrt x}\]
                Where the latter result is due to the Gaussian integral. Also, this integral is one particular case of the Fresnel integral. 
            </p>

        <p>
            \[\int_0^\infty \frac{\operatorname{Li}_2(x)}{x\sqrt{1-x^2}}dx=?\]
        </p>

        <p>
                Switch directly to double integrals by using:
                \[\int_0^1 \frac{\ln t}{t-\frac{1}{x}}dt = \operatorname{Li}_2(x)\]
            </p>

        <p>
            \[\int_0^1 \frac{\arctan x\ln(1+x^2)}{x(1+x)}dx = ?\]
        </p>

        <p>
                Switch to double integrals by using the following result:
                \[\int_0^x \frac{\arctan t}{1+xt}dt = \frac{\arctan x \ln(1+x^2)}{2x}\]
            </p>

        <p>
            \[\int_{-\infty}^\infty \frac{x}{\pi^2+x^2}\frac{e^{2x}}{(1+e^{x})^3}dx\]
        </p>

        <p>
                Consider switching to double integrals with:
                \[\frac{x}{\pi^2+x^2}=\Im\left(-\frac{1}{\pi+ix}\right)=-\Im\int_0^\infty e^{-(\pi+ix)t}dt\]
                It's also really useful to try and see what happens when the Laplace transform of the cosine function is used instead, or the equivalent:
                \[\frac{x}{\pi^2+x^2}=\Re\left(\frac{1}{i\pi+x}\right)=\Re\int_0^\infty e^{-(i\pi+x)t}dt\]
            </p>

        <p>
            \[\int_0^\infty x\left(\operatorname{Ci}^2(x)+\operatorname{si}^2(x)\right)\operatorname{Ci}(x)dx\]
        </p>

        <p>
                Consider switching to double integrals using:
                \[\operatorname{Ci}^2(x)+\operatorname{si}^2(x)=\int_0^\infty \frac{e^{-xy}\ln(1+y^2)}{y}dy\]
            </p>


        <p>
            Above \(\operatorname{Li}_2(x)\) denotes the dilogarithm function and \(\operatorname{Ci}(x)\), \(\operatorname{si}(x)\) are the cosine and the sine integral functions, defined as:
        </p>


        <p>
            \[\operatorname{Li}_2(x)=\sum_{n=1}^\infty \frac{x^n}{n^2},\ \operatorname{Ci}(x) = - \int_x^\infty \frac{\cos t}{t}dt,\ \operatorname{si}(x) = - \int_x^\infty \frac{\sin t}{t}dt\]
        </p>

        <hr>
        <section id="chapter4"> <h2>More Feynman's trick variants</h2> </section>
        <hr>

        <p>
            We already got familiar with a popular version of Feynman's trick in the previous chapter. Similarly, now we will take a look at other interesting variants of Feynman's trick, which although might appear less often, they can still help to expand the applicability of the technique.
        </p>

        <hr>

        <h3>
            Differentiating under the integral sign
        </h3>

        <p>
            We will start by taking a look at a much simpler case of Feynman's trick, namely, in the situation when it would be enough to simply differentiate under the integral sign without performing that "undo" step to integrate back.
        </p>

        <p>
            As a small note, it's true that "differentiating under the integral sign" tends to be used as an alternative name for Feynman's trick, however I prefer to keep this for the variant where only the differentiating process takes part, or as mentioned above, when there's no need to integrate back the result, and the name describes quite literally what we are doing.
        </p>

        <p>
            Let's make this more clear by looking at the following integral:
        </p>

        <p>
            \[I=\int_0^1 x^3 \ln^2 x \, dx \]
        </p>

        <p>
            We are already aware from the <a href="#chapter1">Hello, World!</a> integral how \(\ln x\) can be simplified, since \(\frac{\partial}{\partial a}x^a = x^a \ln x\). However, by introducing the parameter in that original form as \(x^a \ln^2 x\), we would just produce a third logarithm, so that's going in the opposite direction.
        </p>

        <p>
            Fortunatelly, if we take a step back, we can observe that after we find the result of \(\int_0^1 x^a dx\), then differentiating it w.r.t.\(a\) would give us as many logarithms as we want. So, let's put that integral to use.
        </p>

        <p>
            \[\mathcal J(a)=\int_0^1 x^a dx  = \frac{1}{a+1}\]
            \[\Rightarrow \mathcal J'(a) = \int_0^1 x^a \ln x \, dx = \left(\frac{1}{a+1}\right)' = - \frac{1}{(a+1)^2}\]
            \[\Rightarrow \mathcal J''(a) = \int_0^1 x^a \ln^2 x \, dx = \left(- \frac{1}{(a+1)^2}\right)' = \frac{2}{(a+1)^3}\]
            \[\Rightarrow I= \mathcal J''(3) = \int_0^1 x^3 \ln^2 x \, dx = \frac{1}{32}\]
        </p>

        <p>
            Of course the integral itself was quite simple this time, however the important part that should be highlighted is that not always we need to perform that "undo" step after differentiating under the integral sign - and sometimes knowing a general integral result can provide us more useful integrals by differentiating it.
        </p>

        <hr>

        <h3>
            Feynman's trick &amp; indefinite integrals
        </h3>

        <p>
            Further, we will take a look at how Feynman's trick can be applied to indefinite integrals. Let's consider:
        </p>

        <p>
            \[\int \frac{1}{\sqrt{x^3}} \exp\left({-\frac{(a-bx)^2}{2x}}\right) dx\]
        </p>

        <p>
            In this form it makes no sense to differentiate the integral with respect to any parameter, but we can extend the integral with temporary bounds by writing:
        </p>

        <p>
            \[\int f(x)dx = F(x) + C = F(x) - F(0) = \int_0^x f(t)dt\]
            \[\Rightarrow I(a,b,t)=\int_0^t\frac{1}{\sqrt{x^3}} \exp\left({-\frac{(a-bx)^2}{2x}}\right) dx\]
        </p>

        <p>
            After this we can go on apply Feynman's trick, however, first we are going get rid of the square root via the substitution \(\frac{1}{\sqrt x}\to x\).
        </p>

        <p>
            \[I(a,b,t)=2\int_\frac{1}{\sqrt t}^\infty \exp\left(-\frac12 \left(ax-b/x\right)^2\right)dx\]
        </p>

        <p>
            Here, we can notice that the derivative of \(ax-\frac{b}{x}\) is \(a+\frac{b}{x^2}\) so it would be quite helpful if we had that additional term.
            In the same time if we differentiate the integrand with respect to \(b\) we'll produce \(a-\frac{b}{x^2}\), which is really useful as \((ax-b/x)^2\) is equal to \((ax+b/x)^2+4ab\) and the derivative of \(ax+\frac{b}{x}\) is \(a-\frac{b}{x^2}\). So let's differentiate as mentioned above:
        </p>
           
        <p>
            \[\frac{\partial}{\partial b}I(a,b,t)=2\int_\frac{1}{\sqrt t}^\infty \exp\left(-\frac12 \left(ax-b/x\right)^2\right)\left(a-\frac{b}{x^2}\right)dx\]
            \[=2e^{2ab}\int_\frac{1}{\sqrt t}^\infty \exp\left(-\frac12 (ax+b/x)^2\right)\left(a-\frac{b}{x^2}\right)dx\]
            \[\overset{ax+b/x\to x}=2e^{2ab}\int_{\frac{a}{\sqrt t}+b\sqrt t}^\infty \exp{\left(-\frac{x^2}{2}\right)}dx=\sqrt{2\pi}e^{2ab}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}+b\sqrt t}{\sqrt 2}\right)\]
        </p>

        <p>
            Where \(\operatorname{erfc}(x)\) is the complementary error function. Now we'll go back to \(I(a,b,t)\), but we should be careful to replace the dummy variable \(b\), with something else as the \(b\) parameter does also appear in the bounds.
        </p>
            
        <p>
            \[\lim_{b\to -\infty}I(a,b,t)=0\Rightarrow I(a,b,t)=\sqrt{2\pi}\int_{-\infty}^be^{2ax}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}+x\sqrt t}{\sqrt 2}\right)dx\]
            \[\overset{IBP}=\frac{\sqrt{\pi}}{\sqrt 2a}e^{2ax}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}+x\sqrt t}{\sqrt 2}\right)\bigg|_{-\infty}^b+\frac{\sqrt t}{a}\int_{-\infty}^b e^{2ax}\exp\left(-\frac{(a+xt)^2}{2t}\right)dx\]
            \[=\sqrt{\frac{\pi}{2}}\frac{1}{a}e^{2ab}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}+b\sqrt t}{\sqrt 2}\right)+\frac{\sqrt t}{a}\int_{-\infty}^b \exp\left(-\frac12\left(x\sqrt{t}-\frac{a}{\sqrt{t}}\right)^2\right)dx\]
            \[\overset{x\sqrt t-\frac{a}{\sqrt t}\to -x}=\sqrt{\frac{\pi}{2}}\frac{1}{a}e^{2ab}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}+b\sqrt t}{\sqrt 2}\right)+\frac{1}{a}\int^{\infty}_{\frac{a}{\sqrt t}-b\sqrt t}\exp\left(-\frac{x^2}{2}\right)dx\]
            \[=\sqrt{\frac{\pi}{2}}\frac{1}{a}\left(e^{2ab}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}+b\sqrt t}{\sqrt 2}\right)+\operatorname{erfc}\left(\frac{\frac{a}{\sqrt t}-b\sqrt t}{\sqrt 2}\right)\right)\]
        </p>

        <p>
            Or for the indefinite integral, this would lead to:
        </p>
        
        <p>
            \[\small \int \frac{1}{\sqrt{x^3}} \exp\left({-\frac{(a-bx)^2}{2x}}\right) dx=\sqrt{\frac{\pi}{2}}\frac{1}{a}\left(e^{2ab}\operatorname{erfc}\left(\frac{\frac{a}{\sqrt x}+b\sqrt x}{\sqrt 2}\right)+\operatorname{erfc}\left(\frac{\frac{a}{\sqrt x}-b\sqrt x}{\sqrt 2}\right)\right)+C\]
        </p>

        <hr>
        
        <h3>
            Feynman's trick &amp; power series
        </h3>

        <p>
            Next, we will take a look at how to combine Feynman's trick with power series. For this we are going to look at:
        </p>

        <p>
            \[I=\int_{0}^{1}\int_{0}^{1}\frac{x\ln x\ln y}{1-xy}\frac{dxdy}{\ln(xy)}\]
        </p>

        <p>
            We are already got familiar with what to do when there is a logarithm in the denominator as we saw that we can get rid of them by using \(\frac{d}{dt} x^t = x^t\ln x\), however here also the \(1-xy\) term appears. In order to solve this issue we'll make use of the geoemtric series, namely \(\frac{1}{1-x}=\sum_{n=0}^\infty x^n\), but we will expand into series a bit later and for now continue with the following integral:
        </p>

        <p>
            \[I(n)=\int_{0}^{1}\int_{0}^{1}\frac{(xy)^{n-1}x\ln x\ln y}{\ln(xy)}dxdy\Rightarrow I'(n)=\int_0^1\int_0^1  (xy)^{n-1} x \ln x \ln y dxdy\]
            \[=\int_0^1 x^{n} \ln x \, dx \int_0^1 y^{n-1} \ln y\, dy=\frac{1}{(n+1)^2}\frac{1}{n^2}\]
        </p>

        <p>
            Now we have to to get back to \(I(n)\):
        </p>

        <p>
            \[I(n)=-(I(\infty)-I(n))=-\int_n^\infty \frac{1}{(x+1)^2 x^2 } dx=-\frac{1}{n}-\frac{1}{n+1}+2\ln\left(1+\frac{1}{n}\right)\]
        </p>

        <p>
            And finally, we'll put the geometric series to use.
        </p>

        <p>
            \[\int_0^1\int_0^1 \frac{ x\ln x\ln y}{(1-xy)\ln(xy)}dx\ dy=\sum_{n=1}^\infty \int_0^1\int_0^1 \frac{(xy)^{n-1} x\ln x\ln y}{\ln(xy)}dxdy\]
            \[\small =\sum_{n=1}^\infty \left(-\frac{1}{n}-\frac{1}{n+1}+2\ln\left(1+\frac{1}{n}\right)\right)=\sum_{n=1}^\infty \left(\underbrace{\frac{1}{n}-\frac{1}{n+1}}_{1}-2\left(\underbrace{\frac{1}{n}-\ln\left(1+\frac{1}{n}\right)}_{\gamma}\right)\right)\]
        </p>

        <p>
            So the result is simply \(1-2\gamma\), where \(\gamma\) is the Euler-Mascheroni constant.
        </p>

        <hr>

        <h3>
            Feynman's trick &amp; differential equations
        </h3>

        <p>
            In what's to come we are going to take a look at a combination between Feynman's trick and differential equations. Let's consider the following integral:
        </p>

        <p>
            \[I=\int_0^\infty \frac{\cos x}{1+x^2}dx\]
        </p>

        <p>
            We can start by parameterising the cosine function and then employ the accelerated Feynman's trick:
        </p>

        <p>
            \[I(t)=\int_0^\infty \frac{\cos(tx)}{1+x^2}dx=\int_0^\infty \int_0^\infty \cos(tx)\sin y e^{-xy}dydx\]
            \[\small =\int_0^\infty \sin y \int_0^\infty \cos(tx)e^{-xy}dxdy=\int_0^\infty \frac{y\sin y}{t^2+y^2}dy\overset{y\to tx}=\int_0^\infty \frac{x\sin(tx)}{1+x^2}dx\]
        </p>

        <p>
            We haven't made much progress above, since we simply arrived at another integral with \(x\sin(tx)\) instead of \(\cos(tx)\), thus  complexity is the same. However, as \(\frac{\partial}{\partial t}\cos(tx)\) is \(x\sin(tx)\), differentiating \(I(t)\) gives us a differential equation to work with, namely:
        </p>

        <p>
            \[I'(t)=- \int_0^\infty \frac{x\sin(tx)}{1+x^2}dx = - I(t) \Rightarrow \frac{I'(t)}{I(t)}=-1\Rightarrow I(t) = C e^{-t}\]
            \[I(0)=\int_0^\infty \frac{1}{1+x^2}dx=\frac{\pi}{2} \Rightarrow I(t)=\frac{\pi}{2}e^{-t}\]
            \[ I = I(1) \Rightarrow I = \frac{\pi}{2e}\]
        </p>

        <p>
            As a small note for the starting step, although employing the accelerated Feynman's trick was rather obvious as to get rid of the denominator, the additional introduction of the \(t\) parameter might be weird first. However performing the same steps without this parameter gives us:
        </p>

        <p>
            \[\int_0^\infty \frac{\cos x}{1+x^2}dx=\int_0^\infty \frac{x\sin x}{1+x^2}dx\]
        </p>

        <p>
            Which indicates that one might put to use the fact that \(I(1)=-I'(1)\), by adding the additional \(t\) parameter.
        </p>

        <hr>

        <h3>
            Generalizing Feynman's trick
        </h3>

        <p>
            So far we've seen the Feynman's trick applied only when the parameter was inside the integrand, however it can also be used when the bounds are parameterised as well. More generally, the following holds:
        </p>

        <p>
            \[\frac{d}{dt} \int_{a(t)}^{b(t)} f(x, t)dx = \frac{d}{dt}b(t)f(x,b(t))-\frac{d}{dt}a(t)f(x,a(t))+\int_{a(t)}^{b(t)} \frac{\partial f(x, t)}{\partial t}dx\]
        </p>

        <p>
            We'll put this to use with the integral from below.
        </p>

        <p>
            \[I=\int_\frac{1}{\sqrt 2}^1 \frac{\operatorname{arccosh}\left(\sqrt 2 x\right)}{\sqrt{1-x^2}}dx\]
        </p>

        <p>
            Above we can see that the same \(\sqrt 2\) appears in both the lower bound and the \(\operatorname{arccosh}\) function, so we'll parameterise the integral as:
        </p>
        
        <p>
            \[I(t)=\int_\frac{1}{t}^1 \frac{\operatorname{arccosh}\left(t x\right)}{\sqrt{1-x^2}}dx
            \Rightarrow I'(t) = \frac{1}{t^2}\frac{\cancelto{0}{\operatorname{arccosh}\left(t\frac{1}{t}\right)}}{\sqrt{1-\frac{1}{t^2}}}+\int_\frac{1}{t}^1\frac{x}{\sqrt{1-t^2x^2}\sqrt{1-x^2}}dx\]
            \[\overset{1-x^2\to x^2}=\frac1t\int_0^{\sqrt{1-\frac{1}{t^2}}}\frac{1}{\sqrt{1-\frac{1}{t^2}-x^2}}dx=\frac1t\arcsin\left(\frac{x}{\sqrt{1-\frac{1}{t^2}}}\right)\bigg|_0^\sqrt{1-\frac1{t^2}}=\frac{\pi}{2t}\]
        </p>

        <p>
            We're looking to find \(I=I\left(\sqrt 2\right)\), and since \(I\left(1\right)=0\), we have:
        </p>

        <p>
            \[I=\int_1^\sqrt 2I'(t)dt = \frac{\pi}{2} \int_1^\sqrt 2 \frac{1}{t}dt=\frac{\pi}{4}\ln 2\]
        </p>

        <hr>

        <h3>
            Generating integrals using Feynman's trick
        </h3>

        <p>
            Now we'll take a look at a fancier way to use Feynman's trick, especially in order to generate new integrals, for this we're considering:
        </p>

        <p>
            \[I(t)=\int_0^\frac{\pi}{2} \arctan\left(\frac{\sin x-\tan\frac{t}{2}}{\cos x}\right)dx\]
        </p>

        <p>
            Note that we are not trying to evaluate the above integral, instead we are simply using it in order to build up new integrals with the result that follows after differentiating w.r.t. \(t\).
        </p>

        <p>
            \[I'(t)=-\frac12\int_0^\frac{\pi}{2}\frac{\cos x}{1-\sin t\sin x}dx=\frac12 \frac{\ln(1-\sin t)}{\sin t}\]
        </p>

        <p>
            We also have that \(I(\pi)=-\frac{\pi^2}{4}\) and \(I(0)=\frac{\pi^2}{8}\), therefore:
        </p>

        <p>
            \[I(\pi)-I(0)=-\frac{\pi^2}{4}-\frac{\pi^2}{8}=\frac12\int_0^\pi \frac{\ln(1-\sin x)}{\sin x}dx\]
            \[\Rightarrow \int_0^\pi \frac{\ln(1-\sin x)}{\sin x}dx=-\frac{3\pi^2}{4}\]
        </p>

        <p>
            In retrospect, this integral also appeared as an exercise in the  <a href="#chapter2">second chapter</a>, and with the same suggestion from there, we can evaluate the integral by applying Feynman's trick to:
        </p>
        
        <p>
            \[I(t)=\int_0^\pi \frac{\ln(1-\sin t\sin x)}{\sin x}dx\]
        </p>

        <p>
            Admittedly, following this parameterisation is much more intuitevely than what we've shown with the new variation, however it's also useful to have this trick in the bag.
        </p>

        <hr>

        <p>
            To keep the practice going, underneath are listed some integrals that can be evaluated with one version of Feynman's trick described in this chapter.
        </p>

        <p>
            \[\int_1^\infty \int_1^\infty (x+y)^2 e^{-(x+y)}dxdy = ?\]
        </p>

        <p>
                Start by showing that:
                \[I(t)=\int_1^\infty \int_1^\infty e^{-t(x+y)}dxdy = \left(\frac{e^{-t}}{t}\right)^2\]
                Then differentiate both sides two times with respect to \(t\) and set \(t=1\).
            </p>

        <p>
            \[\int x^4\cos(nx)dx = ?\]
        </p>

        <p>
                Differentiate four times with respect to \(n\) the following extended indefinite integral:
                \[
                    I(n,t) = \int_0^t \cos(nx) dx
                \]
            </p>

        <p>
            \[\int_0^1 \int_0^1 \frac{dxdy}{(1+xy)\ln(xy)} = ?\]
        </p>

        <p>
                Combine the geometric series \(\sum\limits_{n=0}^\infty (-1)^n x^n = \frac{1}{1+x}\) with:
                \[I(t)=\int_0^1 \int_0^1 \frac{(xy)^t}{\ln(xy)}dxdy\]
            </p>

        <p>
            \[\int_0^\infty \frac{\sin^2 x}{x^2(1+x^2)}dx = ?\]
        </p>

        <p>
                Solve the resulting differential equation after differentiating twice the following integral:
                \[
                    I(t) = \int_0^\infty \frac{\sin^2 (tx)}{x^2(1+x^2)}dx
                \]
            </p>

        <p>
            \[\int_0^1 \ln x \left(\frac{1}{t+x}+\frac{1}{\frac{1}{t}+x}\right)dx = ?\]
        </p>

        <div>
            <p>
                Split the integral in two parts, then substitute \(x\to tx\) and respectively \(x\to \frac{x}{t}\) in the resulting integrals to obtain:
                \[I(t)= \int_0^1 \ln x \left(\frac{1}{t+x}+\frac{1}{\frac{1}{t}+x}\right)dx = \int_0^\frac{1}{t} \frac{\ln(tx)}{1+x}dx + \int_0^t \frac{\ln\left(\frac{x}{t}\right)}{1+x}dx\]
                Now employ the Feynman's trick (in the generalized variant).
            </p>
            <hr>
            <p> 
                In disguise this exercise is a reformulation of the following identity:
                \[
                    \operatorname{Li}_2(-t)+\operatorname{Li}_2\left(-\frac{1}{t}\right) = -\frac{\pi^2}{6} - \frac{1}{2}\ln^2 t
                \]
                Where \(\operatorname{Li}_2(x)\) is the dilogarithm function.
            </p>
        </div>

        <p>
            \[\int_0^\infty e^{-x^2}dx = ?\]
        </p>

        <p>
                This integral can be generated starting from:
                \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t)=-e^{-t}\int_0^\infty e^{-tx^2} dx\overset{\sqrt t x \to x}= -\frac{e^{-t}}{\sqrt t}\int_0^\infty e^{-x^2} dx\]
                Afterwards it should be observed that \(I(0)=\frac{\pi}{2}\) and \(I(\infty)=0\), therefore:
                \[\frac{\pi}{2} = \int_0^\infty \frac{e^{-t}}{\sqrt t} dt \int_0^\infty e^{-x^2} dx \overset{t\to t^2} = 2\left(\int_0^\infty e^{-x^2}dx\right)^2 \]
            </p>


        <hr>
        <section id="chapter5"> <h2>Feynman's trick in practice</h2> </section>
        <hr>

        <p>
            In this last chapter we'll dive into some "real-world" integrals and observe how Feynman's trick can be adapted for them.
            Additionally, we will also attempt to build up some heuristics that'll help us manipulate the integrals up to a point where we can introduce an useful parameter. This is due to the fact that with most of the previous examples we had the luxury to parameterise the integrals as they appeared, however often this might not be the case.
        </p>

        <hr>

        <h3>
            Breaking the rules
        </h3>

        <p>
            In the <a href="#chapter2">second chapter</a> we've observed how parameterising the integral so that it also simplifies some parts of the integral can make things more intuitively. However, we can always look for better, especially when our approach doesn't seem elegant enough.
        </p>

        <p>
            Let's see how can we break this rule with the following integral:
        </p>

        <p>
            \[I=\int_0^1 \frac{\ln(1-x^2+x^4)}{1-x^2} dx\]
        </p>

        <p>
            As with our first rule of thumb it's straightforward to introduce the new parameter as:
        </p>

        <p>
            \[I(a)=\int_0^1 \frac{\ln(a(1-x^2)+x^4)}{1-x^2} dx \Rightarrow I'(a)= \int_0^1 \frac{1}{a(1-x^2)+x^4}dx\]
        </p>

        <p>
            You are encouraged to proceed forward with the above integral, however computing \(I'(a)\) might not give the most pleasant result. So to overcome this, we'll manipulate the integral a bit before parametersing it.
        </p>

        <p>
            One extremely useful substitution (which deserves its own special chapter) is \(x\to \frac{1-x}{1+x}\). This one is great here especially since it has the following property (among many others):
        </p>

        <p>
            \[\int_0^1 \frac{f(x)}{1-x^2}dx \overset{x\to\frac{1-x}{1+x}}= \frac12\int_0^1 \frac{f\left(\frac{1-x}{1+x}\right)}{x}dx\]
            \[\Rightarrow I \overset{x\to\frac{1-x}{1+x}}= \frac12\underbrace{\int_0^1 \frac{\ln(1+14x^2+x^4)}{x}dx}_{x^2\to x} - \frac12 \int_0^1 \frac{\ln((1+x)^4)}{x}dx\]
            \[= \frac14\int_0^1 \frac{\ln(1+14x+x^2)}{x}dx - 2 \int_0^1 \frac{\ln(1+x)}{x}dx = \frac14 I(\operatorname{arccosh} 7) -  I(0)\]
        </p>
    
        <p>
            Above, although we could have introduced a simple parameter, for a smoother result we parameterised the integral as:
        </p>

        <p>
            \[I(t)=\int_0^1 \frac{\ln(1+2\cosh t+x^2)}{x} dx \Rightarrow I'(t)= \int_0^1 \frac{2\sinh t}{1+2\cosh t+x^2}dx\]
            \[= \int_0^1 \frac{2\sinh t}{(x+\cosh t)^2-\sinh^2 t}dx = -2\operatorname{arctanh}\left(\frac{x+\cosh t}{\sinh t}\right)\bigg|_0^1 = t\]
        </p>

        <p>
            Finally, to find to find \(I\) we can combine \(I(\operatorname{arccosh} 7)\) with \(I(0)\).
        </p>

        <p>
            \[I = \frac14 \int_0^{\operatorname{arccosh 7}} t \, dt - \frac34 I(0) = \frac{\ln^2(2+\sqrt 3)}{2} - \frac{\pi^2}{8}\]
        </p>

        <hr>

        <p>
            The result from above follows since includes \(I(0) = \frac{\pi^2}{6}\), which is also the Basel problem in disguise, but you're further encouraged to approach it by employing Feynman's trick.
        </p>

        <p>
            \[\int_0^1 \frac{\ln(1+x)}{x}dx=\frac{\pi^2}{12}\]
        </p>

        <p>
                One idea is to rewrite the integral as:
                \[ I=\int_0^1 \frac{\ln(1+x)}{x}dx\overset{x\to x^3}=3\int_0^1 \frac{\ln(1+x^3)}{x}dx\]
                \[\Rightarrow \frac13I-I=\int_0^1 \frac{\ln\left(\frac{1+x^3}{1+x}\right)}{x}dx\Rightarrow I = -\frac32 \int_0^1 \frac{\ln(1-x+x^2)}{x}dx \]
                Now put Feynman's trick to use for:
                \[I(t)=\int_0^1 \frac{\ln(1-tx+x^2)}{x}dx\]
                But also take into account that:
                \[I(0)=\int_0^1 \frac{\ln(1+x^2)}{x}dx\overset{x^2\to x}=\frac12 \int_0^1 \frac{\ln(1+x)}{x}dx=\frac12I\]
            </p>

        <hr>

        <h3>
            Switching to rational functions
        </h3>

        <p>
            Maybe it's just a personal preference, but for me working with rational functions tends to give a higher visibility on how to parameterise the integrals. We'll see what is meant by this when dealing with the following integral:
        </p>

        <p>
            \[I=\int_0^\frac{\pi}{2}\ln(2+\tan^2 x)dx\]
        </p>

        <p>
            In this form, two immediate ways to parameterise the integral are as follows:
        </p>

        <p>
            \[I(a)=\int_0^\frac{\pi}{2}\ln(a+\tan^2 x)dx,\quad I(b)=\int_0^\frac{\pi}{2}\ln(2+b\tan^2 x)dx\]
        </p>

        <p>
            It turns out that both variants work, but we can do better. Another rule of thumb that I follow is to use almost exclusive rational functions as they often provide the most visibility to work with. Let's do that for our integral too.
        </p>
            
        <p>
            \[I=\int_0^\frac{\pi}{2}\ln(2+\tan^2 x)dx\overset{\tan x\to x}=\int_0^\infty \frac{\ln(2+x^2)}{1+x^2}dx\]
        </p>

        <p>
            And at this point it should be obvious where to place the parameter so that we can simplify the denominator.
        </p>

        <p>
            \[I(t)=\int_0^\infty \frac{\ln(1+t(1+x^2))}{1+x^2}dx\Rightarrow I'(t)=\int_0^\infty \frac{1}{1+t+tx^2}dx=\frac{\pi}{2}\frac{1}{\sqrt t\sqrt{1+t}}\]
            \[\Rightarrow I=\int_0^1 I'(t)dt=\frac{\pi}{2}\int_0^1 \frac{1}{\sqrt t\sqrt{1+t}}dt\overset{t=x^2}=\pi\int_0^1\frac{1}{\sqrt{1+x^2}}dx=\pi\ln(1+\sqrt 2)\]
        </p>

        <p>
            Obviously, if you're already more used to trigonometric functions, hyperbolic functions or anything else, then this can be ignored. I however recommend switching to rational functions, a few exceptions being when there's a clear way to parameterise the integral directly or when at least one of the bounds is \(\infty\).
        </p>

        <hr>

        <p>
            For more practice you can also attempt to tackle the following integral:
        </p>

        <p>
            \[\int_0^\frac{\pi}{2} \ln(\sec^2 x +\tan^4 x)dx=?\]
        </p>

        <p>
                Start by substituting \(\tan x \to x\) then employ Feynman's trick.
            </p>

        <hr>
        
        <h3>
            Cleaning up the functions
        </h3>

        <p>
            Before parameterising the integrals it's quite useful to clean the disturbing functions as much as possible before any parameterisation. Let's demonstrate this by considering:
        </p>

        <p>
            \[I=\int_0^\frac{\pi}{2}\arctan\left(\frac{2\sin x}{2\cos x -1}\right)\frac{\sin\left(\frac{x}{2}\right)}{\sqrt{\cos x}}dx\]
        </p>

        <p>
            As we have trigonometric functions all over the place, we will perform the Weierstrass substitution \(\tan\left(\frac{x}{2}\right)\to x\) in order to obtain only rational functions, as per the previous heuristic.
        </p>
            
        <p>
            \[\Rightarrow I = 2\int_0^1 \frac{x\arctan\left(\frac{4x}{1-3x^2}\right)}{\sqrt{1-x^2}(1+x^2)}dx\]
        </p>

        <p>
            It would be great to parameterise the integral so that we get rid of the arctangent function, but unfortunately here it is a bit overloaded, therefore we'll clean it by splitting it into two. In case it's not obvious how to do that directly, we can differentiate it, perform partial fractions and then integrate back.
        </p>

        <p>
            \[\frac{d}{dx}\arctan\left(\frac{4x}{1-3x^2}\right)=\frac{12x^2+4}{9x^4+10x^2+1}=\frac{1}{1+x^2}+\frac{3}{1+9x^2}\]
            \[\Rightarrow \int \left(\frac{1}{1+x^2}+\frac{3}{1+9x^2}\right) dx = \arctan x + \arctan(3x) + C\]
        </p>

        <p>
            However since we're integrating over the \((0,1)\) interval we have that \(x \cdot 3 x&gt;1\) for \(x &gt; \frac{1}{\sqrt 3}\), so we need to rewrite the integral as:
        </p>

        <p>
            \[I=2\int_0^1 \frac{x(\arctan x + \arctan(3x))}{\sqrt{1-x^2}(1+x^2)}dx - 2 \int_\frac{1}{\sqrt 3}^1 \frac{x}{\sqrt{1-x^2}(1+x^2)}dx\]
            \[= 2 \mathcal J - \frac{\pi}{\sqrt 2}\ln(2+\sqrt 3)\]
        </p>

        <p>
            At this point there's only left to evaluate the first integral, \(\mathcal J\), for which we'll employ Feynman's trick. There isn't any way to place the parameter so that we get rid of anything from the denominator so we'll simply introduce the following parameterisation:
        </p>

        <p>
            \[\mathcal J(t)=\int_0^1 \frac{x(\arctan x + \arctan(tx))}{\sqrt{1-x^2}(1+x^2)}dx\Rightarrow \mathcal J'(t) = \int_0^1 \frac{x^2}{\sqrt{1-x^2}(1+x^2)(1+t^2 x^2)}dx\]
        </p>

        <p>
            In general when we have rational functions it is prefered to integrate over \((0,\infty)\), if possible, as it drastically reduces the result, and when there's the derivative of \(\arcsin x\) in the denominator one way to map \((0,1)\) to \((0,\infty)\) is to directly substitute \(x \to \frac{1}{\sqrt{1+x^2}}\).
        </p>
            
        <p>
            \[\Rightarrow \mathcal J=\int_0^\infty \frac{1}{(2+x^2)(1+t^2+x^2)}dx = \frac{\pi}{2(1-t^2)}\left(\frac{1}{\sqrt{1+t^2}}-\frac{1}{\sqrt 2}\right)\]
        </p>

        <p>
            Now in order to go back to \(\mathcal J(3)\), we'll make use of \(\mathcal J(-1)=0\), so:
        </p>

        <p>
            \[\mathcal J = \frac{\pi}{2}\int_{-1}^3 \frac{1}{(1-t^2)}\left(\frac{1}{\sqrt{1+t^2}}-\frac{1}{\sqrt 2}\right)dt\]
        </p>

        <p>
            Finally, to finish this integral we'll subsitute \(t = \frac{1-x}{1+x}\).
        </p>
    
        <p>
            \[\Rightarrow \mathcal J = \frac{\pi}{4\sqrt 2}\int_{-\frac12}^\infty \frac{1}{x}\left(\frac{1+x}{\sqrt{1+x^2}}-1\right)dx=\frac{\pi}{\sqrt 2}\ln \left(\frac{1+\sqrt 5}{2}\right)\]
        </p>

        <p>
            As such, we can conclude that:
        </p>

        <p>
            \[\int_0^\frac{\pi}{2}\arctan\left(\frac{2\sin x}{2\cos x -1}\right)\frac{\sin\left(\frac{x}{2}\right)}{\sqrt{\cos x}}dx=\sqrt 2 \pi \ln \left(\frac{1+\sqrt 5}{2}\right)- \frac{\pi}{\sqrt 2}\ln(2+\sqrt 3)\]
        </p>

        <hr>

        <p>
            Similarly you can try and tackle the following integral:
        </p>

        <p>
            \[\int_0^\pi \arctan\left(\frac{1+2\cos x}{\sqrt 3}\right)dx=?\]
        </p>

        <p>
                Start by substituting \(\tan\left(\frac{x}{2}\right) \to x\), then split the arctangent function into two parts and use Feynman's trick.
            </p>

        <hr>

        <h3>
            Preparing better integral bounds
        </h3>

        <p>
            Another useful thing to consider before using Feynman's trick is to manipulate the bounds prior to parameterising the integral so that they get rid of any complicated functions for the differentiated integral, \(I'(t)\). This will almost always give a smoother and easier integral that we have to undo. We will show this in action with the integral from below.
        </p>
        
        <p>
            \[I=\int_0^1 \frac{x^2 \ln(1-x^2)}{1+x^4}dx\]
        </p>

        <p>
            With the previous heuristic we've already seen that it is useful to integrate over \((0,\infty)\) when there are some rational functions - therefore we will attempt to do the same with this integral. You are encouraged to try and see what kind of mess it would be produced by parameterising the integral as it is, when the bounds are \((0,1)\). However we will jump straightforward to get our bounds to \((0,\infty)\).
        </p>

        <p>
            In order to obtain that we can notice that the integrand is even, so we can move the bounds to \((-1,1)\) and then substitute \(x\to \frac{1-x}{1+x}\) again, which is another useful way to get our bounds at \((0,\infty)\).
        </p>

        <p>
            \[I=\frac12\int_{-1}^1 \frac{x^2\ln(1-x^2)}{1+x^4}dx\overset{x\to\frac{1-x}{1+x}}=\frac12\int_0^\infty \frac{(1-x)^2\ln\left(\frac{4x}{(1+x)^2}\right)}{1+6x^2+x^4}dx\]
        </p>

        <p>
            Now we can split the logarithm into three parts and use that:
        </p>
        
        <p>
            \[\int_0^\infty \frac{(1-x)^2}{1+6x^2+x^4}dx = \frac{\pi}{2\sqrt 2}-\frac{\ln(1+\sqrt 2)}{\sqrt 2}\]
            \[\int_0^\infty \frac{(1-x)^2\ln(x)}{1+6x^2+x^4}dx\overset{x\to\frac{1}{x}} = -\int_0^\infty \frac{(1-x)^2\ln(x)}{1+6x^2+x^4}dx = 0\]
        </p>

        <p>
            Therefore our integral is:
        </p>
        
        <p>
            \[I=\frac{\pi\ln 2}{2\sqrt 2}-\frac{\ln 2\ln(1+\sqrt 2)}{\sqrt 2}-\mathcal J, \quad \mathcal J = \int_0^\infty \frac{(1-x)^2\ln(1+x)}{1+6x^2+x^4}dx\]
        </p>

        <p>
            To evaluate the emerging integral we will perform Feynman's trick. There's not an obvious way to place the parameter as to simplify the denominator, so we'll parameterise the integral as:
        </p>

        <p>
            \[\mathcal J(a) = \int_0^\infty \frac{(1-x)^2\ln(1+ax)}{1+6x^2+x^4}dx\Rightarrow \mathcal J'(a)\int_0^\infty \frac{x(1-x)^2}{(1+ax)(1+6x^2+x^4)}dx\]
            \[=-\frac{(1+a)^2\ln a}{1+6a^2+a^4}-\frac{\ln(1+\sqrt 2)}{\sqrt 2}\frac{a(3-a+a^2)+1}{1+6a^2+a^4}+\frac{\pi}{2\sqrt 2}\frac{a(3+a+a^2)-1}{1+6a^2+a^4}\]
        </p>

        <p>
            The partial fraction was ommited above, as what's really important here is that we're left only with a simple \(\ln a\) as a "disturbing" function. In contrast, if the bounds were \((0,1)\) things would have been way more complicated.
        </p>

        <p>
            Let's finish this integral as we still have to undo the differentiating step.
        </p>

        <p>
            \[\mathcal J(1)=\mathcal J(1)-\mathcal J(0)=\int_0^1 \mathcal J'(a)da\]
            \[=-\int_0^1\frac{(1+a)^2\ln a}{1+6a^2+a^4}da-\frac{\pi\ln(1+\sqrt 2)}{8\sqrt 2}-\frac{3\ln 2\ln(1+\sqrt 2)}{4\sqrt 2}-\frac{\pi^2}{16\sqrt 2}+\frac{3\pi\ln 2}{8\sqrt 2}\]
            \[\int_0^1\frac{(1+x)^2\ln x}{1+6x^2+x^4}dx=\int_0^1\left(\frac{1}{2\sqrt 2}\frac{(1+\sqrt 2)-x}{(1+\sqrt 2)^2+x^2}-\frac{1}{2\sqrt 2}\frac{(1-\sqrt 2)-x}{(1-\sqrt 2)^2+x^2}\right)\ln x \, dx\]
            \[\small =\frac{1}{8\sqrt 2}\left(\operatorname{Li}_2\left(-(1+\sqrt 2)^2\right)-\operatorname{Li}_2\left(-(1-\sqrt 2)^2\right)\right)+\frac{1}{2\sqrt 2}\left(\operatorname{Ti}_2\left(-(1+\sqrt 2)\right)-\operatorname{Ti}_2\left(-(1-\sqrt 2)\right)\right)\]
        </p>

        <p>
            The result from above follows since:
        </p>

        <p>
            \[\int_0^1 \frac{x\ln x}{a^2+x^2}dx\overset{x^2\to x}=\frac14\int_0^1 \frac{\ln x}{a^2+x}dx=\frac14\operatorname{Li}_2\left(-\frac{1}{a^2}\right)\]
            \[\int_0^1 \frac{a\ln x}{a^2+x^2}dx\overset{x\to ax}=\int_0^\frac1a\frac{\ln a +\ln x}{1+x^2}dx\overset{IBP}=-\operatorname{Ti}_2\left(\frac1a\right)\]
        </p>

        <p>
            Where \(\operatorname{Li}_2(x)\) is the dilogarithm and \(\operatorname{Ti}_2(x)\) is the inverse tangent integral.
        </p>

        <p>
            Finally, collecting all the results yields:
        </p>

        <p>
            \[I=\frac{\pi^2}{16\sqrt 2}+\frac{\pi \ln 2}{8\sqrt 2}+\frac{\pi\ln(1+\sqrt 2)}{8\sqrt 2}-\frac{\ln 2\ln(1+\sqrt 2)}{4\sqrt 2}\]
            \[\small +\frac{1}{8\sqrt 2}\left(\operatorname{Li}_2\left(-(1+\sqrt 2)^2\right)-\operatorname{Li}_2\left(-(1-\sqrt 2)^2\right)\right)+\frac{1}{2\sqrt 2}\left(\operatorname{Ti}_2\left(-(1+\sqrt 2)\right)-\operatorname{Ti}_2\left(-(1-\sqrt 2)\right)\right)\]
        </p>

        <hr>

        <p>
            With the same idea one can attempt to calculate the following integral:
        </p>        

        <p>
            \[\int_0^1 \frac{\ln^2 x\ln(1+x)}{1+x^2}dx=?\]
        </p>

        <p>
                Map the bounds from \((0,1)\) to \((0,\infty)\) by substituting \(x\to\frac{1}{x}\) - here it is necessary to add the resulting integral with the original one - afterwards use Feynman's trick.
            </p>

        <hr>

        <h3>
            Multiple parameters
        </h3>

        <p>
            We mostly got familiar to apply Feynman's trick by introducing a parameter somewhere, however sometimes even multiple parameters can be used when encountering new integrals. To exemplify such a situation, let's take a look at the following unit square integral arising in geometric probability:
        </p>

        <p>
            \[I=\int_0^1\int_0^1 \left(\frac{-\ln(xy)}{1-xy}\right)^mdxdy\]
        </p>

        <p>
            In order to generate the \(\ln(xy)\) part it's straightforward to consider the following integral:
        </p>

        <p>
            \[I(a)=\int_0^1\int_0^1 \frac{(xy)^a}{(1-xy)^m}dxdy\]
        </p>

        <p>
            Differentiating with respect to \(a\), \(m\) times followed by setting \(a=0\) will gives us the desired integral, ignoring the \((-1)^m\) term. However the denominator is still troublesome, and to deal with that we will introduce one more parameter:
        </p>
            
        <p>
            \[I(a,z)=\int_0^1\int_0^1 \frac{(xy)^a}{z-xy}dxdy\]
        </p>

        <p>
            This is perfect now, as we can recover our original integral by differentiating with respect to \(a\), \(m\) times, and with respect to \(z\), \(m-1\) times, followed by setting \(a=0\) and respectively \(z=1\) - ignoring some coefficients.
        </p>

        <p>
            One way to evaluate \(I(a,z)\) is to expand the denominator into geometric series as:
        </p>

        <p>
            \[I(a,z)=\int_0^1\int_0^1 \frac{(xy)^a}{z-xy}dxdy=\frac{1}{z}\sum_{n=0}^\infty \int_0^1\int_0^1(xy)^a\left(\frac{xy}{z}\right)^ndxdy\]
            \[=\sum_{n=0}^\infty \frac{1}{z^{n+1}}\int_0^1x^{n+a}dx\int_0^1y^{n+a}dy=\sum_{n=1}^\infty \frac{1}{z^{n+2}}\frac{1}{(n+a)^2}\]
        </p>

        <p>
            Now we will take \(m\) derivatives with respect to \(a\) and then set it to \(0\).
        </p>

        <p>
            \[\frac{\partial^m}{\partial a^m}I(a,z) = \int_0^1\int_0^1 \frac{\ln^m(xy)}{z-xy} dxdy =(-1)^m (m+1)!\sum_{n=1}^\infty \frac{1}{z^{n+2}}\frac{1}{n^{m+2}}\]
        </p>

        <p>
            This can be also rewriten in terms of the polylogarithm function as:
        </p>

        <p>
            \[\int_0^1\int_0^1 \frac{(-\ln(xy))^m}{z-xy} dxdy = (m+1)!\operatorname{Li}_{m+2}\left(\frac{1}{z}\right)\]
        </p>

        <p>
            Finally, we can arrive at our original integral by taking \(m-1\) derivatives with respect to \(z\) and setting it to \(1\).
        </p>

        <p>
                \[(-1)^m(m-1)!\int_0^1\int_0^1 \frac{(-\ln(xy))^m}{(1-xy)^m}dxdy=(m+1)!\frac{d^{m-1}}{dz^{m-1}}\operatorname{Li}_{m+2}\left(\frac{1}{z}\right)\bigg|_{z=1}\]
        </p>

        <p>
            Although the derivation from above was the important part since it shows the main idea on how to differentiate in order to produce the desired integral, by using \(\frac{\partial}{\partial z} \operatorname{Li}_n(z) = \frac{\operatorname{Li}_{n-1}(z)}{z}\) the result can be also written, with the help of <a href="https://oeis.org/A094638">OEIS</a>, as:
        </p>

        <p>
            \[\int_0^1\int_0^1 \left(\frac{-\ln(xy)}{1-xy}\right)^m dxdy =m(m+1)\sum_{k=1}^{m-1}|s(m-1,m-k)|\zeta(k+2)\]
        </p>

        <p>
            Where \(s(n,m)\) is the Stirling number of the first kind and \(\zeta(z)\) is the Riemann zeta function.
        </p>

        <hr>

        <p>
            A similar idea can be applied for the following integral:
        </p>        

        <p>
            \[\int_0^\infty \frac{\cos(3x)}{(1+x^2)^4}dx=?\]
        </p>

        <p>
                Make use of a more general integral that was evaluated in the <a href="#chapter4">fourth chapter</a>, namely:
                \[\int_0^\infty \frac{\cos(tx)}{a^2+x^2}dx = \frac{\pi}{2a}e^{-at}\]
                Then differentiate \(3\) times w.r.t. \(a\).
            </p>

        <hr>

        <h3>
            Cascaded Feynman's trick
        </h3>

        <p>
            Sometimes to enable an application of Feynman's trick we needed to actually apply another Feynman's trick. Let's look at the following integral:
        </p>

        <p>
            \[I = \int_0^\frac{\pi}{2}\int_0^\frac{\pi}{2}\cot x \csc^2 y\ln(\cos y)\ln(1-2\sin x+\sin^2 x\csc^2y)dxdy\] 
        </p>

        <p>
            The first step should be pretty obvious by now, namely to get rid of the trigonometric functions.
        </p>

        <p>
            \[I \overset{\large \sin x\to x \atop  \large \cot y \to y}=-\frac12\int_0^\infty \int_0^1 \frac{\ln\left(1+\frac{1}{y^2}\right)\ln((1-x)^2+x^2y^2)}{x}dxdy\]
        </p>

        <p>
            Now we can notice that we have two logarithms and only one of them contains the \(x\) term, however since the bounds are \((0,1)\) dealing with that integral won't produce much success (as the result would be quite complicated).
        </p>

        <p>
            However we also have the bounds as \((0,\infty)\) for the \(y\) integral, and it would be even better if we could have a single logarithm. To further obtain such a favorable integral form, we can use the following result:
        </p>

        <p>
            \[\int_0^1 \frac{t}{t^2+y^2}dt=\frac12\ln\left(1+\frac{1}{y^2}\right)\] 
            \[\Rightarrow I=-\int_0^\infty \int_0^1 \int_0^1 \frac{t\ln((1-x)^2+x^2y^2)}{x(t^2+y^2)}dtdxdy \]
            \[\overset{y\to ty}=-\int_0^1 \frac{1}{x}\int_0^1 \int_0^\infty \frac{\ln((1-x)^2+x^2t^2y^2)}{1+y^2} dydtdx\]
        </p>

        <hr>

        <p>
            In the <a href="#chapter3">third chapter</a> we saw how it's useful to have a list of integral results. One more such useful integral that tend to appear quite often is:
        </p>

        <p>
            \[\int_0^\infty \frac{\ln(a^2+b^2 x^2)}{1+x^2}dx= \pi \ln(|a|+|b|)\]
        </p>

        <p>
                You can differentiate either \(I(a)\) or \(I(b)\), and even directly employ the accelerated Feynman's trick by writing the logarithm as an integral.
            </p>

        <hr>

        <p>
            Now back to our integral, by using the above result, we can easily finish the integral.
        </p>

        <p>
            \[I=-\pi\int_0^1 \frac{1}{x}\int_0^1 \ln((1-x)+xt)dtdx\]
            \[=\pi\underbrace{\int_0^1 \left(\frac1x+\frac{\ln(1-x)}{x^2}\right)dx}_{\large -1}-\pi\underbrace{\int_0^1\frac{\ln(1-x)}{x}dx}_{\large -\frac{\pi^2}{6}}=\frac{\pi^3}{6}-\pi\]
        </p>

        <hr>

        <p>
            Although this marks the conclusion of the essay, this isn't a static website, and I might update it when I encounter new interesting integrals that are worth to be shown. So far, the integrals comes from <a href="https://math.stackexchange.com/users/515527/zacky?tab=answers">my posts</a> on Mathematics Stack Exchange, combined with some of the most popular integrals - thus you can also check them directly there.
        </p>

        <p>
            For further exercises, I can recommend you to explore math forums and magazines such as Art of Problem Solving, Mathematics Stack Exchange, The American Mathematical Monthly, Crux Mathematicorum, or the Romanian Mathematical Magazine, where dozens of fascinating integrals are often posted or published. Additionally, delving into other fields like Statistics, Physics, or Quantum Physics will present you with many remarkable integrals — some of which might be computed using Feynman's trick.
        </p>

        <hr>

        <p>
            I would appreciate a notice on my email address (<a href="mailto:rxzacky@gmail.com">rxzacky@gmail.com</a>) in case you find any mistakes or if something feels unclear in this essay - and even better if you have some further ideas or suggestions.
        </p>

        <p>
            This work is licensed under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution 4.0 International License</a>, and it can be cited as:
        <br>
            Zaharia Burghelea, "Feynman's Trick," <cite><a href="https://zackyzz.github.io/feynman" target="_blank" rel="noopener noreferrer">https://zackyzz.github.io/feynman</a></cite>.
        </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be Like Clippy (281 pts)]]></title>
            <link>https://be-clippy.com/</link>
            <guid>46090172</guid>
            <pubDate>Sat, 29 Nov 2025 19:41:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://be-clippy.com/">https://be-clippy.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46090172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p><strong>"Clippy didn’t sell your data. Clippy didn’t hold your data hostage. Clippy was there to help you."</strong></p><p>Fed up with trillion-dollar companies exploiting your data?
                Forced to use their services?
                Your data held for ransom?
                Your data used to train their AI models?
                Opt-outs for data collection instead of opt-ins?</p> <p>Join the movement to make companies more like Clippy. <span>Set your profile picture to Clippy</span>, make your voice heard.</p> <p>Below is a video that explains the Be Like Clippy movement. It’s a call to action for developers, companies, and users alike to embrace a more open, transparent, and user-friendly approach to technology.</p></div><div><p>Licensed under <a href="https://github.com/NotReeceHarris/be-clippy.com/blob/main/LICENSE">GPL-3.0</a></p> <p>Share your own on <a href="https://github.com/NotReeceHarris/be-clippy.com">Github</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework Computer Now Sponsoring LVFS / Fwupd Development (141 pts)]]></title>
            <link>https://www.phoronix.com/news/Framework-Sponsoring-LVFS</link>
            <guid>46089980</guid>
            <pubDate>Sat, 29 Nov 2025 19:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Framework-Sponsoring-LVFS">https://www.phoronix.com/news/Framework-Sponsoring-LVFS</a>, See on <a href="https://news.ycombinator.com/item?id=46089980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="HARDWARE" src="https://www.phoronix.com/assets/categories/hardware.webp" width="100" height="100"></p><p>
With <a href="https://www.phoronix.com/news/LVFS-Fwupd-135-Million-Download">the Linux Vendor Firmware Service serving more than 135 million downloads</a> for Linux users updating their system and device firmware, LVFS has been <a href="https://www.phoronix.com/news/LVFS-Fair-Use-Quota">working to get more hardware vendors to contribute</a> either engineering resources or directly contributing annual dues as sponsors. Framework Computer is now the first one to have executed an agreement under these new sponsorship efforts.
</p><p>
Red Hat in employing lead developer Richard Hughes has contributed the most to LVFS/Fwupd's success, the Linux Foundation has also hosted the project since it has shifted into their umbrella, AMD's Mario Limonciello is among the significant contributors, and now Framework Computer is a new sponsor to the project.
</p><p>
Richard Hughes posted on <a href="https://mastodon.social/@hughsie/115628170779065489">Mastodon</a> on Friday:
</p><blockquote>"I'm also happy to announce we've got a new sponsor for the LVFS: #Framework
<p>
Although there are about a half a dozen OEMs that have promised to sponsor LVFS, Framework is the first to have actually signed the paperwork."</p></blockquote>
<p>The graphic on the <a href="https://fwupd.org/">Fwupd.org page</a> shows Framework as a Startup Sponsor, which puts them in the ballpark of around $10k USD in annual dues to the project.
</p><p><img src="https://www.phoronix.net/image.php?id=2025&amp;image=lvfs_framework" alt="LVFS Framework sponsorship"></p>
<p>Richard Hughes also followed up with a <a href="https://mastodon.social/@hughsie/115628171746383319">comment</a> on the positive impact that Framework has also applied on their suppliers around Fwupd/LVFS support too:
</p><blockquote>"I also wanted to say a huge thanks to Framework, not just for the sponsorship -- but also for the pressure they've put on their suppliers to support fwupd and the LVFS."</blockquote>
<p>Framework has already been <a href="https://www.phoronix.com/news/Fwupd-2.0.14-Released">supporting LVFS/Fwupd with their devices</a> and ensuring good Linux support in general, such as with <a href="https://www.phoronix.com/review/framework-16-ryzen-ai-300-series">the recent Framework 16 laptop upgrade</a>.
</p><p><img src="https://www.phoronix.net/image.php?id=framework-16-ryzen-ai-300-series&amp;image=framework_16_13_med" alt="Framework 16 upgrade"></p>
<p>Hopefully the sponsorship agreements with the other major OEMs pan out and that these other vendors also continue ramping up in mandating LVFS/Fwupd hardware support for ensuring a better firmware updating experience for Linux customers.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electric vehicle sales are booming in South America – without Tesla (148 pts)]]></title>
            <link>https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/</link>
            <guid>46089971</guid>
            <pubDate>Sat, 29 Nov 2025 19:12:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/">https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/</a>, See on <a href="https://news.ycombinator.com/item?id=46089971">Hacker News</a></p>
Couldn't get https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Zero knowlege proof of compositeness (104 pts)]]></title>
            <link>https://www.johndcook.com/blog/2025/11/29/zkp-composite/</link>
            <guid>46089394</guid>
            <pubDate>Sat, 29 Nov 2025 17:53:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.johndcook.com/blog/2025/11/29/zkp-composite/">https://www.johndcook.com/blog/2025/11/29/zkp-composite/</a>, See on <a href="https://news.ycombinator.com/item?id=46089394">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>A zero knowledge proof (ZKP) answers a question without revealing anything more than answer. For example, a digital signature proves your possession of a private key without revealing that key.</p>
<p>Here’s another example, one that’s more concrete than a digital signature. Suppose you have a deck of 52 cards, 13 of each of spades, hearts, diamonds, and clubs. If I draw a spade from the deck, I can prove that I drew a spade without showing which card I drew. If I show you that all the hearts, diamonds, and clubs are still in the deck, then you know that the missing card must be a spade.</p>
<h2>Composite numbers</h2>
<p>You can think of Fermat’s primality test as a zero knowledge proof. For example, I can convince you that the following number is composite without telling you what its factors are.</p>
<p><em>n</em> = 244948974278317817239218684105179099697841253232749877148554952030873515325678914498692765804485233435199358326742674280590888061039570247306980857239550402418179621896817000856571932268313970451989041</p>
<p>Fermat’s little theorem&nbsp;says that if&nbsp;<em>n</em>&nbsp;is a prime and&nbsp;<em>b</em>&nbsp;is not a multiple of&nbsp;<em>n</em>, then</p>
<p><em>b</em><sup><em>n</em>−1</sup>&nbsp;= 1 (mod&nbsp;<em>n</em>).</p>
<p>A number&nbsp;<em>b</em> such that <em>b</em><sup><em>n</em>−1</sup>&nbsp;≠ 1 (mod&nbsp;<em>n</em>) is a proof that&nbsp;<em>n</em> is not prime, i.e. <em>n</em> is composite. So, for example,&nbsp;<em>b</em> = 2 is a proof that&nbsp;<em>n</em> above is composite. This can be verified very quickly using Python:</p>
<pre>    &gt;&gt;&gt; pow(2, n-1, n)
    10282 ... 4299
</pre>
<p>I tried the smallest possible base [1] and it worked. In general you may have to try a few bases. And for a few rare numbers (Carmichael numbers) you won’t be able to find a base. But if you do find a base <em>b</em> such that <em>b</em><sup><em>n</em>−1</sup> is not congruent to 1 mod&nbsp;<em>n</em>, you know with&nbsp;<strong>certainty</strong> that&nbsp;<em>b</em> is composite.</p>
<h2>Prime numbers</h2>
<p>The converse of Fermat’s little theorem is false. It can be used to prove a number is&nbsp;<em>not</em> prime, but it cannot prove that a number&nbsp;<em>is</em> prime. But it can be used to show that a number is&nbsp;<strong>probably</strong> prime. (There’s some subtlety as to what it means for a number to probably be prime. See <a href="https://www.johndcook.com/blog/2010/10/06/probability-a-number-is-prime/">here</a>.)</p>
<p>Fermat’s little theorem can give you a zero knowledge proof that a number is composite. Can it give you a zero knowledge proof that a number is prime? There are a couple oddities in this question.</p>
<p>First, what would it mean to have a zero knowledge proof that a number is prime? What knowledge are you keeping secret? When you prove that a number is composite, the prime factors are secret (or even unknown), but what’s the secret when you say a number is prime? Strictly speaking a ZKP doesn’t have to keep anything secret, but in practice it always does.</p>
<p>Second, what about the probability of error? Zero knowledge proofs do not have to be infallible. A ZKP can have some negligible probability of error, and usually do.</p>
<p>It’s not part of the definition, but n practice ZKPs are supposed to be easier to verify than the direct approach to what they prove. So you could have something like a <a href="https://www.johndcook.com/blog/2023/01/03/pratt-certificate/">primality certificate</a> that takes far less computation to verify than the computation needed to determine from scratch that a number is prime.</p>
<h2>Proving other things</h2>
<p>You could think of non-constructive proofs as ZKPs. For example, you could think of the intermediate value theorem as a ZKP: it proves that a function has a zero in an interval without giving you any information about where that zero may be located.</p>
<p>What makes ZKPs interesting in application is that they can prove things of more general interest than mathematical statements [2]. For example, cryptocurrencies can provide ZKPs that accounting constraints hold without revealing the inputs or outputs of the transaction. You could prove that nobody tried to spend a negative amount and that the sum of the inputs equals the sum of the outputs.</p>
<h2>Related posts</h2>
<ul>
<li><a href="https://www.johndcook.com/blog/2025/08/01/jubjub/">Lewis Carroll and ZKP</a></li>
<li><a href="https://www.johndcook.com/blog/2023/01/03/pratt-certificate/">Primality certificates</a></li>
<li><a href="https://www.johndcook.com/blog/2023/01/13/proof-of-optimization/">Proof of optimization</a></li>
</ul>
<p>[1] You could try&nbsp;<em>b</em> = 1, but then <em>b</em><sup><em>n</em>−1</sup> is always 1. This example shows that the existence of a base for which <em>b</em><sup><em>n</em>−1</sup> = 1 (mod&nbsp;<em>n</em>) doesn’t prove anything.</p>
<p>[2] You might object that accounting rules are mathematical statements, and of course they are. But they’re of little interest to mathematicians and of great interest to the parties in a transaction.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're learning more about what Vitamin D does to our bodies (167 pts)]]></title>
            <link>https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/</link>
            <guid>46088998</guid>
            <pubDate>Sat, 29 Nov 2025 16:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/">https://www.technologyreview.com/2025/11/21/1128206/vitamin-d-bodies-bone-health-immune/</a>, See on <a href="https://news.ycombinator.com/item?id=46088998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content--body"><div> <p>It has started to get really wintry here in London over the last few days. The mornings are frosty, the wind is biting, and it’s already dark by the time I pick my kids up from school. The darkness in particular has got me thinking about vitamin D, a.k.a. the sunshine vitamin.</p>  <p>At a checkup a few years ago, a doctor told me I was deficient in vitamin D. But he wouldn’t write me a prescription for supplements, simply because, as he put it, <em>everyone</em> in the UK is deficient. Putting the entire population on vitamin D supplements would be too expensive for the country’s national health service, he told me.</p> </div><div> <p><strong>But supplementation—whether covered by a health-care provider or not—can be important.</strong> As those of us living in the Northern Hemisphere spend fewer of our waking hours in sunlight, let’s consider the importance of vitamin D.</p>  <p>Yes, it is important for bone health. But recent research is also uncovering surprising new insights into how the vitamin might influence other parts of our bodies, including our immune systems and heart health.</p> 
 <p><strong>Vitamin D was discovered just over 100 years ago,</strong> when health professionals were looking for ways to treat what was then called “the English disease.” Today, we know that rickets, a weakening of bones in children, is caused by vitamin D deficiency. And vitamin D is best known for its importance in bone health.</p>  <p>That’s because it helps our bodies absorb calcium. Our bones are continually being broken down and rebuilt, and they need calcium for that rebuilding process. Without enough calcium, bones can become weak and brittle. (Depressingly, rickets is still a global health issue, which is why there is&nbsp;<a href="https://www.who.int/tools/elena/bbc/vitamind-infants">global consensus that infants should receive a vitamin D supplement</a> at least until they are one year old.)</p> 
 <p>In the decades since then, scientists have learned that vitamin D has effects beyond our bones. There’s some evidence to suggest, for example, that being deficient in vitamin D puts people at risk of high blood pressure. Daily or weekly supplements&nbsp;<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9509305/">can help those individuals</a> lower their blood pressure.</p> </div><div> <p>A vitamin D deficiency has also been linked to&nbsp;<a href="https://link.springer.com/content/pdf/10.1007/s40265-023-01875-8.pdf">a greater risk of “cardiovascular events”</a> like heart attacks, although it’s not clear whether supplements can reduce this risk; the evidence is pretty mixed.</p>  <p>Vitamin D appears to influence our immune health, too.&nbsp;<a href="https://www.sciencedirect.com/science/article/pii/S0002916523302831?via%3Dihub">Studies</a>&nbsp;<a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/414815">have</a> found a link between low vitamin D levels and incidence of the common cold, for example. And other&nbsp;<a href="https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2022.790444/full">research</a> has shown that vitamin D supplements can influence the way our genes make proteins that play important roles in the way our immune systems work.</p>  <p>We don’t yet know exactly how these relationships work, however. And, unfortunately, a recent&nbsp;<a href="https://www.thelancet.com/action/showPdf?pii=S2213-8587%2824%2900348-6">study</a> that assessed the results of 37 clinical trials found that overall, vitamin D supplements aren’t likely to stop you from getting an “acute respiratory infection.”</p> </div><div> <p><strong>Other studies have linked vitamin D levels to mental health, pregnancy outcomes, and even how long people survive after a cancer diagnosis.</strong> It’s tantalizing to imagine that a cheap supplement could benefit so many aspects of our health.</p>  <p>But, as you might have gathered if you’ve got this far, we’re not quite there yet. The evidence on the effects of vitamin D supplementation for those various conditions is mixed at best.</p> </div><div><p>In fairness to researchers, it can be difficult to run a randomized clinical trial for vitamin D supplements. That’s because most of us get the bulk of our vitamin D from sunlight. Our skin converts UVB rays into a form of the vitamin that our bodies can use. We get it in our diets, too, but not much. (The main sources are oily fish, egg yolks, mushrooms, and some fortified cereals and milk alternatives.)</p>  <p>The standard way to measure a person’s vitamin D status is to look at blood levels of 25-hydroxycholecalciferol (25(OH)D), which is formed when the liver metabolizes vitamin D. But not everyone can agree on what the “ideal” level is.</p> 

 <p>Even if everyone did agree on a figure, it isn’t obvious how much vitamin D a person would need to consume to reach this target, or how much sunlight exposure it would take. One complicating factor is that people respond to UV rays in different ways—a lot of that can depend on how much melanin is in your skin. Similarly, if you’re sitting down to a meal of oily fish and mushrooms and washing it down with a glass of fortified milk, it’s hard to know how much more you might need.</p>  <p><strong>There is more consensus on the definition of vitamin D <em>deficiency</em>, though.</strong> (It’s a blood level below 30 nanomoles per liter, in case you were wondering.) And until we know more about what vitamin D is doing in our bodies, our focus should be on avoiding that.</p>  <p>For me, that means topping up with a supplement. The UK government&nbsp;<a href="https://www.nhs.uk/conditions/vitamins-and-minerals/vitamin-d/">advises</a> everyone in the country to take a 10-microgram vitamin D supplement over autumn and winter. That advice doesn’t factor in my age, my blood levels, or the amount of melanin in my skin. But it’s all I’ve got for now. </p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Testing shows automotive glassbreakers can't break modern automotive glass (120 pts)]]></title>
            <link>https://www.core77.com/posts/138925/Testing-Shows-Automotive-Glassbreakers-Cant-Break-Modern-Automotive-Glass</link>
            <guid>46088379</guid>
            <pubDate>Sat, 29 Nov 2025 15:43:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.core77.com/posts/138925/Testing-Shows-Automotive-Glassbreakers-Cant-Break-Modern-Automotive-Glass">https://www.core77.com/posts/138925/Testing-Shows-Automotive-Glassbreakers-Cant-Break-Modern-Automotive-Glass</a>, See on <a href="https://news.ycombinator.com/item?id=46088379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-post-id="138925" id="c77_content">
            
    <div id="post_header">
        

            <ul>
                


                            </ul>
        

            <h2>Testing Shows Automotive Glassbreakers Can't Break Modern Automotive Glass </h2>
                    <h2>And seatbelt cutters are useful mostly for first responders</h2>
                    

        
    </div>

    




            <section id="post">




<p id="278b64_7556" data-ic-marker="7971b9_1294">It's easy to convince EDC people to buy EDC things. But how do you convince non-EDC folks to buy your product?</p><p id="6e511a_615" data-ic-marker="247809_3295">Simple: Fear.</p><p id="1c10a4_436" data-ic-marker="693048_6883">The global "car escape tool market," according to market research firm <a href="https://www.datainsightsmarket.com/reports/car-escape-tools-130264" rel="">Data Insights Market</a>, is valued at $500 million this year, projected to grow 7% a year and hit $900 million in 2033. The trend is being driven by "heightened safety concerns among consumers."</p><p id="9d9e5e_841" data-ic-marker="735b3_1904">A big seller in this category is the automotive window breaker and seat belt cutter. The fantasy being peddled by the toolmakers is: You will crash, remain conscious, find that your car has burst into flame or is slowly sinking in water, find that you cannot undo your seatbelt, yet are still able to reach for this specialty tool, slice through your seatbelt, then smash the window open and climb free to safety.</p><p id="c23e61_7137" data-ic-marker="4b2628_4755"><img data-image-width="800" data-image-height="800" data-image-id="1767465" src="https://s3files.core77.com/blog/images/1767465_81_138925_2epp5yrPt.jpg"><em id="95e1e5_18">This image sure looks real to us! </em></p><p id="f2f2a7_3824" data-ic-marker="846f03_1965"><img data-image-width="880" data-image-height="880" data-image-id="1767466" src="https://s3files.core77.com/blog/images/1767466_81_138925_vyDpz6IZS.jpeg"></p><p id="9c4fbd_1347" data-ic-marker="b4751c_3473">Accidents that involve fire or water are <i>less than half of one percent</i> of all accidents, according to the <a href="https://www.nhtsa.gov/vehicle-safety/seat-belts" rel="">NHTSA</a>. And the amount of accidents where the above scenario actually occurred, and that tool saved lives, is not recorded as a statistic. Similarly, seat belt jamming is so rare that neither the NHTSA nor the AAA track it as a statistic.</p><p id="3b99c7_788" data-ic-marker="917759_6889">As for the glassbreakers, here's the big thing that most people don't realize: They're designed to break <i>tempered</i> glass, which is what most cars <i>used to have</i> for the side windows. However, modern safety regulations—specifically, the "Ejection Mitigation Rule" in the 2013 Federal Motor Vehicle Safety Standard 226 (FMVSS 226), mean that most manufacturers have transitioned to <i>laminated</i> glass for the side windows. </p><p id="433e3d_917" data-ic-marker="c91fb_696">Laminated glass (which is what the windshields were already made of) is tougher to break, and is now used to prevent occupants from being ejected through the side glass.</p><p id="16a20a_287" data-ic-marker="ef2963_6215">An <a href="https://www.aaa.com/AAA/common/AAR/files/Research-Report-Vehicle-Escape-Tools.pdf" rel="">AAA research report</a> tested six commonly-available glassbreakers. Not a single one of them was capable of breaking through laminated glass—and two of the tools couldn't even break through tempered glass, but instead broke themselves. On the glass.</p><p id="b3c672_4105" data-ic-marker="305f1c_990"><img data-image-width="880" data-image-height="482" data-image-id="1767468" src="https://s3files.core77.com/blog/images/1767468_81_138925_09_e0lERA.jpg"></p><p id="45bc3e_3572" data-ic-marker="1103be_2176"><img data-image-width="880" data-image-height="636" data-image-id="1767469" src="https://s3files.core77.com/blog/images/1767469_81_138925_4SOe4rgbj.jpg"></p><p id="8beb06_1543" data-ic-marker="ecdef3_1419">It's true that not all automakers have switched over to laminated glass for the side windows; the FMVSS 226 law stipulates that you can get around it if you install elaborate side airbags that also prevent ejection.</p><p id="94e41a_1203" data-ic-marker="11fc32_3648">The automakers that <i>are</i> using laminated side glass are only: Acura, Audi, BMW, Cadillac, Chevrolet, Chrysler, Dodge, Ford, Genesis, GMC, Honda, Hyundai, Infiniti, Jaguar, Jeep, Kia, Land Rover, Lexus, Lincoln, Mercedes-Benz, Nissan, Porsche, Ram, Subaru, Toyota, Volkswagen and Volvo. Some of them, like Chevy and BMW, have been using laminated glass since the '70s and '80s. Glassbreakers might only be useful if you're driving around in a classic car and believe you'll become submerged.</p><p id="8e2327_1738" data-ic-marker="31cddd_6816">That said, seatbelt cutters are of supreme use to firefighters, EMTs and other first responders who may not be able to reach an unconscious accident victim's seatbelt release. So there might be a case for them if you see yourself in a situation where you need to free an unconscious person, and have the training to safely extricate them. </p>                

            </section>

                


                


<div>
    <ul>
        <li data-this-post-id="138925" data-this-author-id="0">
            
            
            <p>Favorite This</p>
        </li>
       <li data-this-post-title="Testing Shows Automotive Glassbreakers Can't Break Modern Automotive Glass " data-this-post-id="138925" data-this-author-id="0">
            
            
            <p>Comment</p>
        </li>
        
    </ul> 
</div>






           


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Major AI conference flooded with peer reviews written by AI (194 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-03506-6</link>
            <guid>46088236</guid>
            <pubDate>Sat, 29 Nov 2025 15:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-03506-6">https://www.nature.com/articles/d41586-025-03506-6</a>, See on <a href="https://news.ycombinator.com/item?id=46088236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="access-teaser"> <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Close up view of a red toy robot sat amongst a stack of books." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_51767010.jpg"><figcaption><p><span>An AI-detection tool developed by Pangram labs found that peer reviewers are increasingly using chatbots to draft responses to authors.</span><span>Credit: breakermaximus/iStock via Getty</span></p></figcaption></picture></figure><p>What can researchers do if they suspect that their manuscripts have been peer reviewed using artificial intelligence (AI)? Dozens of academics have raised concerns on social media about manuscripts and peer reviews submitted to the organizers of next year’s International Conference on Learning Representations (ICLR), an annual gathering of specialists in machine learning. Among other things, they flagged <a href="https://www.nature.com/articles/d41586-025-02853-8" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02853-8" data-track-category="body text link">hallucinated citations</a> and suspiciously long and vague feedback on their work.</p><p>Graham Neubig, an AI researcher at Carnegie Mellon University in Pittsburgh, Pennsylvania, was one of those who received peer reviews that seemed to have been<a href="https://www.nature.com/articles/d41586-025-03390-0" data-track="click" data-label="https://www.nature.com/articles/d41586-025-03390-0" data-track-category="body text link"> produced using large language models (LLMs)</a>. The reports, he says, were “very verbose with lots of bullet points” and requested analyses that were not “the standard statistical analyses that reviewers ask for in typical AI or machine-learning papers.”</p><p>But Neubig needed help proving that the reports were AI-generated. So, he posted on X (formerly Twitter) and offered a reward for anyone who could scan all the conference submissions and their peer reviews for AI-generated text. The next day, he got a response from Max Spero, chief executive of Pangram Labs in New York City, which develops tools to detect AI-generated text. Pangram screened all 19,490 studies and 75,800 peer reviews submitted for ICLR 2026, which will take place in Rio de Janeiro, Brazil, in April. Neubig and more than 11,000 other AI researchers will be attending.</p><p>Pangram’s analysis revealed that around 21% of the ICLR peer reviews were fully AI-generated, and more than half contained signs of AI use. The findings were <a href="https://www.pangram.com/blog/pangram-predicts-21-of-iclr-reviews-are-ai-generated" data-track="click" data-label="https://www.pangram.com/blog/pangram-predicts-21-of-iclr-reviews-are-ai-generated" data-track-category="body text link">posted online by Pangram Labs</a>. “People were suspicious, but they didn’t have any concrete proof,” says Spero. “Over the course of 12 hours, we wrote some code to parse out all of the text content from these paper submissions,” he adds. </p><p>The conference organizers say they will now use automated tools to assess whether submissions and peer reviews breached policies on using <a href="https://www.nature.com/articles/d41586-025-01839-w" data-track="click" data-label="https://www.nature.com/articles/d41586-025-01839-w" data-track-category="body text link">AI in submissions and peer reviews</a>. This is the first time that the conference has faced this issue at scale, says Bharath Hariharan, a computer scientist at Cornell University in Ithaca, New York, and senior programme chair for ICLR 2026. “After we go through all this process … that will give us a better notion of trust.” </p><h2>AI-written peer review</h2><p>The Pangram team used one of its own tools, which <a href="https://www.nature.com/articles/d41586-025-02936-6" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02936-6" data-track-category="body text link">predicts whether text is generated or edited by LLMs</a>. Pangram’s analysis flagged 15,899 peer reviews that were fully AI-generated. But it also identified many manuscripts that had been submitted to the conference with suspected cases of AI-generated text: 199 manuscripts (1%) were found to be fully AI-generated; 61% of submissions were mostly human-written; but 9% contained more than 50% AI-generated text. </p><p>Pangram described the model in a preprint<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>, which it submitted to ICLR 2026. Of the four peer reviews received for the manuscript, one was flagged as fully AI-generated and another as lightly AI-edited, the team’s analysis found.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-025-00894-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-025-03506-6/d41586-025-03506-6_50959900.jpg"><p>AI is transforming peer review — and many scientists are worried</p></a></article><p>For many researchers who received peer reviews for their submissions to ICLR, the Pangram analysis confirmed what they had suspected. Desmond Elliott, a computer scientist at the University of Copenhagen, says that one of three reviews he received seemed to have missed “the point of the paper”. His PhD student who led the work suspected that the review was generated by LLMs, because it mentioned numerical results from the manuscript that were incorrect and contained odd expressions. </p><p>When Pangram released its findings, Elliott adds, “the first thing I did was I typed in the title of our paper because I wanted to know whether my student’s gut instinct was correct”. The suspect peer review, which Pangram’s analysis flagged as fully AI-generated, gave the manuscript the lowest rating, leaving it “on the borderline between accept and reject”, says Elliott. “It's deeply frustrating”.</p><h2>Repercussions</h2></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iceland declares ocean-current instability a national security risk (344 pts)]]></title>
            <link>https://www.dagens.com/news/iceland-declares-ocean-current-instability-a-national-security-risk</link>
            <guid>46088192</guid>
            <pubDate>Sat, 29 Nov 2025 15:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dagens.com/news/iceland-declares-ocean-current-instability-a-national-security-risk">https://www.dagens.com/news/iceland-declares-ocean-current-instability-a-national-security-risk</a>, See on <a href="https://news.ycombinator.com/item?id=46088192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Iceland has classified the potential collapse of a major Atlantic current system as a national security threat, citing new scientific warnings that such a change could radically alter the country’s climate and economy. Officials say the risk represents an “existential” challenge that demands a coordinated, top-level response.</p>

			<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(0);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Others are reading now</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>

<p>Iceland has taken the rare step of treating a climate-linked ocean threat as a matter of national survival, launching a coordinated government response to one of the most feared potential tipping points in the climate system.</p>



<p><br>Officials say the shift reflects mounting evidence that a key Atlantic current system could be heading toward dangerous instability.</p>



<h2>Security shift in Reykjavík</h2>



<p>According to <em><a href="https://edition.cnn.com/2025/11/15/climate/iceland-warming-current-amoc-collapse-threat" target="_blank" rel="noopener">CNN</a></em>, Iceland’s National Security Council formally labelled the possible collapse of the Atlantic Meridional Overturning Circulation (AMOC) a national security risk in September — the first time the country has applied such a designation to a climate impact.</p>



<p><br>The move followed a government briefing on new research that raised “grave concerns” about the system’s future stability.</p>



<p>Jóhann Páll Jóhannsson, Iceland’s minister for environment, energy and climate, said the risks extend far beyond weather. </p>
<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(2);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Also read</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>



<p>“Our climate, economy and security are deeply tied to the stability of the ocean currents around us,” he told <em>CNN</em>.</p>



<p>He later described the threat as “an existential threat,” warning that a breakdown could disrupt transport, damage infrastructure and hit the country’s fishing industry.</p>



<h2>Why the AMOC matters</h2>



<p>The AMOC — often compared to a giant conveyor belt — carries warm water northward before it cools and sinks, helping regulate weather across the Atlantic basin. </p>



<p><em>CNN </em>reported that scientists increasingly worry that warming temperatures and disrupted salinity levels are slowing the system.</p>



<p>Some studies suggest a tipping point could be reached this century, though the exact timeline remains uncertain.</p>
<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(4);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Also read</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>



<p>Stefan Rahmstorf, an oceanographer at Potsdam University, told <em>CNN </em>that a collapse “cannot be considered a low likelihood risk anymore.” </p>



<p>The consequences, he said, would be dramatic: surging sea levels along US and European coasts, major monsoon disruptions across Africa and Asia, and a deep freeze across parts of Europe.</p>



<p>For Iceland, he said, the country “would be close to the center of a serious regional cooling,” with sea ice potentially surrounding the island.</p>



<h2>Preparing for the worst</h2>



<p>The security designation means Iceland will now pursue a high-level, cross-government effort to analyse the threat and consider how to manage or reduce the consequences. Jóhannsson said the decision </p>



<p>“reflects the seriousness of the issue and ensures that the matter gets the attention it deserves.”</p>
<div x-data="{
        posts: [],
        loadPosts() {
            try {
                const relatedPostData = JSON.parse(document.getElementById('related-post-data').textContent);
                if (!relatedPostData.length) return;
                let postsToShow = relatedPostData.slice(6);
                                postsToShow = postsToShow.slice(0, 2);
                                this.posts = postsToShow;
            } catch (error) {
                console.error('Error loading related posts:', error);
            }
        }
	}" x-init="loadPosts()">
		<p x-show="posts.length > 0">
			<h2>Also read</h2>
			<template x-for="(post, index) in posts" :key="index">
				<div id="related-post" class="not-prose sm:gap-sm group flex w-full items-center">
					<a :href="post.link">
						<img class="my-0 aspect-square h-full max-h-16 w-auto rounded object-cover hidden sm:block" :src="post.featured_media_url" alt="">
					</a>
					<h3 class="m-0 flex flex-1 flex-col text-base">
						<a class="text-body-text group-hover:text-body-accent font-normal" :href="post.link" x-html="post.title"></a>
					</h3>
				</div>
			</template>
		</p>
	</div>



<p>Rahmstorf praised Iceland’s stance, telling <em>CNN </em>that other nations should treat the risk with similar urgency.</p>



<p>Jóhannsson said the country is confronting a stark possibility: “What we do know is that the current climate might change so drastically that it could become impossible for us to adapt… this is not just a scientific concern — it’s a matter of national survival and security.” </p>



<p><strong>sources</strong>:CNN</p>
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's Always the Process, Stupid (261 pts)]]></title>
            <link>https://its.promp.td/its-always-the-process-stupid/</link>
            <guid>46087737</guid>
            <pubDate>Sat, 29 Nov 2025 14:20:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://its.promp.td/its-always-the-process-stupid/">https://its.promp.td/its-always-the-process-stupid/</a>, See on <a href="https://news.ycombinator.com/item?id=46087737">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                <a href="https://its.promp.td/tag/article/">Article</a>
            
                <p>Why AI Won’t Save Your Broken Workflow</p>

            <div>
                <p><a href="https://its.promp.td/author/dsb/">
                                <img src="https://its.promp.td/content/images/size/w160/2025/08/dsb-slide2016.jpg" alt="DocIsInDaHouse">
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2025-11-29">29 Nov. 2025</time>
                            <span><span>—</span> 3 min read</span>
                    </p>
                </div>
            </div>

                <figure>
        <img srcset="https://its.promp.td/content/images/size/w320/2025/11/MessyWorkflow.jpeg 320w,
                    https://its.promp.td/content/images/size/w600/2025/11/MessyWorkflow.jpeg 600w,
                    https://its.promp.td/content/images/size/w960/2025/11/MessyWorkflow.jpeg 960w,
                    https://its.promp.td/content/images/size/w1200/2025/11/MessyWorkflow.jpeg 1200w,
                    https://its.promp.td/content/images/size/w2000/2025/11/MessyWorkflow.jpeg 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://its.promp.td/content/images/size/w1200/2025/11/MessyWorkflow.jpeg" alt="It’s Always the Process, Stupid!">
    </figure>

        </div><section>
            <p>Let’s rip the Band-Aid off immediately: If your underlying business process is a mess, sprinkling "AI dust" on it won’t turn it into gold. It will just speed up the rate at which you generate garbage.</p><p>In the world of Business IT, we get seduced by the shiny new toy. Right now, that toy is Artificial Intelligence. Boardrooms are buzzing with buzzwords like LLMs, agentic workflows, and generative reasoning. Executives are frantically asking, <em>"What is our AI strategy?"</em></p><p>But here is the hard truth: </p><blockquote><strong>There is no such thing as an AI strategy. <br>There is only Business Process Optimization (BPO).</strong></blockquote><h3 id="the-magic-wand-fallacy">The "Magic Wand" Fallacy</h3><p>Too many enterprises treat AI like a magic wand. They believe that by implementing a sophisticated neural network, their structural inefficiencies will vanish. They think AI brings intelligence.</p><p>It doesn’t.</p><p>Like every major technological shift before it—from the steam engine to the spreadsheet—AI does not inherently make an organization smarter. <strong>AI, like any other tool, only makes faster.</strong></p><p>If you automate a stupid decision, you just make stupid decisions at light speed. If you apply an agentic AI workflow to a bureaucratic nightmare of an approval chain, you haven't fixed the bureaucracy; you’ve just built a robot that hates its job as much as your employees do.</p><h3 id="the-unstructured-data-trap">The Unstructured Data Trap</h3><p>There is, however, one superpower AI possesses that previous tools lacked: <strong>It is the first technology that is truly useful for handling unstructured data.</strong></p><p>For decades, traditional software demanded structure. Rows, columns, booleans, and fixed fields. If data didn't fit the box, the computer couldn't read it.</p><p>AI changes this. It can read messy emails, interpret vague Slack messages, parse PDFs, and analyze images. But this capability exposes a massive, hidden problem in most enterprises.</p><p><strong>Processes that rely on unstructured data are usually unstructured processes.</strong></p><p>Because computers couldn't handle the mess, humans handled it (before AI). And humans don't always follow a flow chart. These processes—like "handling a complex customer complaint" or "brainstorming a marketing campaign"—are often ad-hoc, intuitive, and completely undocumented. They live in the heads of your senior staff, not in your SOPs.</p><h3 id="you-can%E2%80%99t-automate-what-you-haven%E2%80%99t-designed">You Can’t Automate What You Haven’t Designed</h3><p>This brings us back to BPO. You cannot apply AI to these "hidden" processes until you drag them into the light.</p><p>If you want to use AI to process unstructured data, you must first bring structure to the workflow itself. You need to improve your process design to account for the ambiguity that AI handles.</p><p>Ask yourself:</p><ol><li><strong>What is the trigger?</strong> (Where does the unstructured mess come from?)</li><li><strong>What is the transformation?</strong> (What exactly is the human—or now the AI—supposed to extract or deduce from that mess?)</li><li><strong>What is the structured output?</strong> (How does this flow back into your rigid ERP or CRM systems?)</li></ol><h3 id="speed-vs-intelligence">Speed vs. Intelligence</h3><p>Let’s clarify the distinction between "smarter" and "faster."</p><p>Intelligence implies wisdom, context, and nuance. While AI models are simulating reasoning better every day, in a business context, they are fundamentally pattern-matching engines. They excel at acceleration.</p><ul><li><strong>The Old Way:</strong> An analyst reads 50 contracts (unstructured), highlights risks based on gut feeling (unstructured process), and summarizes them in 3 days.</li><li><strong>The AI Way:</strong> An AI scans 50 contracts and extracts specific risk clauses based on defined parameters in 3 minutes.</li></ul><p>The <em>process</em> (Review Contracts -&gt; Identify Risk -&gt; Summarize) hasn't changed, but it had to be rigorously defined for the AI to work. The <em>intelligence</em> (knowing what a "risk" actually means) still requires human governance. What has changed is the <strong>velocity</strong>.</p><h3 id="the-bottom-line">The Bottom Line</h3><p>Stop chasing the hype. Stop looking for a specialized "AI Savior."</p><p>Go back to the whiteboard. Map out your value chain—especially the messy, human-centric parts involving unstructured data that you previously ignored. Find the bottlenecks. Identify the waste.</p><p>Once you have a streamlined, logical, and robust business process, <em>then</em> apply AI to hit the accelerator.</p><div><p>Technology changes. <br>The rules of business efficiency do not. <br>It’s always the process, stupid!</p><p>And that's where actual AI Tools are missing that point, because they weren't build for that </p></div><figure><a href="https://its.promp.td/the-great-it-divide-why-ai-adoption-in-enterprises-is-failing/"><div><p>The Great IT-Divide: Why AI-Adoption in enterprises is failing</p><p>IT innovation moved from business tools to social tech, creating two distinct IT worlds: Business-IT (compliance, efficiency) and Social-IT (social interaction). Grasping this divide is crucial for enterprise adoption and explains why businesses struggle with AI uptake.</p><p><img src="https://its.promp.td/content/images/icon/promptdIconRoundBlue-1-16.png" alt=""><span>it's promp.td</span><span>DocIsInDaHouse</span></p></div><p><img src="https://its.promp.td/content/images/thumbnail/ITDivide-2.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><hr><p>Live long and prosper 😉🖖</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Datacenters in space aren't going to work (223 pts)]]></title>
            <link>https://taranis.ie/datacenters-in-space-are-a-terrible-horrible-no-good-idea/</link>
            <guid>46087616</guid>
            <pubDate>Sat, 29 Nov 2025 14:05:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taranis.ie/datacenters-in-space-are-a-terrible-horrible-no-good-idea/">https://taranis.ie/datacenters-in-space-are-a-terrible-horrible-no-good-idea/</a>, See on <a href="https://news.ycombinator.com/item?id=46087616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>In the interests of clarity, I am a former NASA engineer/scientist with a PhD in space electronics. I also worked at Google for 10 years, in various parts of the company including YouTube and the bit of Cloud responsible for deploying AI capacity, so I'm quite well placed to have an opinion here.</p><p>The short version: this is an absolutely terrible idea, and really makes zero sense whatsoever. There are multiple reasons for this, but they all amount to saying that the kind of electronics needed to make a datacenter work, particularly a datacenter deploying AI capacity in the form of GPUs and TPUs, is exactly the opposite of what works in space. If you've not worked specifically in this area before, I'll caution against making gut assumptions, because the reality of making space hardware actually function in space is not necessarily intuitively obvious.</p><p><strong>Power</strong></p><figure><img src="https://taranis.ie/content/images/2025/11/image-6.png" alt="Part of the ISS solar array" loading="lazy" width="1041" height="693" srcset="https://taranis.ie/content/images/size/w600/2025/11/image-6.png 600w, https://taranis.ie/content/images/size/w1000/2025/11/image-6.png 1000w, https://taranis.ie/content/images/2025/11/image-6.png 1041w" sizes="(min-width: 720px) 720px"></figure><p>The first reason for doing this that seems to come up is abundant access to power in space. This really isn't the case. You basically have two options: solar and nuclear. Solar means deploying a solar array with photovoltaic cells – something essentially equivalent to what I have on the roof of my house here in Ireland, just in space. It works, but it isn't somehow magically better than installing solar panels on the ground – you don't lose <em>that</em> much power through the atmosphere, so intuition about the area needed transfers pretty well. The biggest solar array ever deployed in space is that of the International Space Station (ISS), which at peak can deliver a bit over 200kW of power. It is important to mention that it took several Shuttle flights and a lot of work to deploy <a href="https://www.nasa.gov/image-article/solar-arrays-international-space-station-2/?ref=taranis.ie" rel="noreferrer">this system</a> – it measures about 2500 square metres, over half the size of an American football field.</p><p>Taking the NVIDIA H200 as a reference, the per-GPU-device power requirements are  on the order of 0.7kW per chip. These won't work on their own, and power conversion isn't 100% efficient, so in practice 1kW per GPU might be a better baseline. A huge, ISS-sized, array could therefore power roughly 200 GPUs. This sounds like a lot, but lets keep some perspective: OpenAI's upcoming Norway datacenter is intending to house 100,000 GPUs, probably each more power hungry than the H200. To equal this capacity, you'd need to launch 500 ISS-sized satellites. In contrast, a single server rack (as sold by NVIDIA preconfigured) will house 72 GPUs, so each monster satellite is only equivalent to roughly three racks.</p><p>Nuclear won't help. We are not talking nuclear <em>reactors</em> here – we are talking about <a href="https://en.wikipedia.org/wiki/Radioisotope_thermoelectric_generator?ref=taranis.ie" rel="noreferrer">radioisotope thermal generators (RTGs)</a>, which typically have a power output of about 50W - 150W. So not enough to even run a single GPU, even if you can persuade someone to give you a subcritical lump of plutonium and not mind you having hundreds of chances to scatter it across a wide area when your launch vehicle explosively self-disassembles.</p><p><strong>Thermal Regulation</strong></p><figure><img src="https://taranis.ie/content/images/2025/11/image-7.png" alt="" loading="lazy" width="1144" height="920" srcset="https://taranis.ie/content/images/size/w600/2025/11/image-7.png 600w, https://taranis.ie/content/images/size/w1000/2025/11/image-7.png 1000w, https://taranis.ie/content/images/2025/11/image-7.png 1144w" sizes="(min-width: 720px) 720px"><figcaption><span>ISS Advanced Thermal Control System (Boeing)</span></figcaption></figure><p>I've seen quite a few comments about this concept where people are saying things like, "Well, space is cold, so that will make cooling really easy, right?"</p><p>Um...</p><p>No.</p><p>Really, really no.</p><p>Cooling on Earth is relatively straightforward. Air convection works pretty well – blow air across a surface, particularly one designed to have a large surface area to volume ratio like a heatsink, will transfer heat from the heatsink to the air quite effectively. If you need more power density than can be directly cooled in this way (and higher power GPUs are definitely in that category), you can use liquid cooling to transfer heat from the chip to a larger radiator/heatsink elsewhere. In datacenters on Earth, it is common to set up cooling loops where machines are cooled via chilled coolant (usually water) that is pumped around racks, with the heat extracted and cold coolant returned to the loop. Typically the coolant is cooled via convective cooling to the air, so one way or another this is how things work on Earth.</p><p>In space, there is no air. The environment is close enough to a hard, total vacuum as makes no practical difference, so convection just doesn't happen. On the space engineering side, we typically think about <em>thermal management</em>, not just cooling. Thing is, space doesn't really <em>have</em> a temperature as-such. Only materials have a temperature. It may come as a surprise, but in the Earth-Moon system the average temperature of pretty much anything is basically the same as the average temperature of Earth, because this is why Earth has that particular temperature. If a satellite is rotating, a bit like a chicken on a rotisserie, it will tend toward having a consistent temperature that's roughly similar to that of the Earth surface. If it isn't rotating, the side pointing away from the sun will tend to get progressively colder, with a limit due to the cosmic microwave background, around 4 Kelvin, just a little bit above absolute zero. On the sunward side, things can get a bit cooked, hitting hundreds of centigrade. Thermal management therefore requires very careful design, making sure that heat is carefully directed where it needs to go. Because there is no convection in a vacuum, this can only be achieved by conduction, or via some kind of heat pump.</p><p>I've designed space hardware that has flown in space. In one particular case, I designed a camera system that needed to be very small and lightweight, whilst still providing science-grade imaging capabilities. Thermal management was front and centre in the design process – it had to be, because power is scarce in small spacecraft, and thermal management has to be achieved whilst keeping mass to a minimum. So no heat pumps or fancy stuff for me – I went in the other direction, designing the system to draw a maximum of about 1 watt at peak, dropping to around 10% of that when the camera was idle. All this electrical power turns into heat, so if I can draw 1 watt only while capturing an image, then turn the image sensor off as soon as the data is in RAM, I can halve the consumption, then when the image has been downloaded to the flight computer I can turn the RAM off and drop the power down to a comparative trickle. The only thermal management needed was bolting the edge of the board to the chassis so the internal copper planes in the board could transfer any heat generated.</p><p>Cooling even a single H200 will be an absolute nightmare. Clearly a heatsink and fan won't do anything at all, but there is a liquid cooled H200 variant. Let's say this was used. This heat would need to be transferred to a radiator panel – this isn't like the radiator in your car, no convection, remember? – which needs to radiate heat into space. Let's assume that we can point this away from the sun.</p><p>The <a href="https://www.nasa.gov/wp-content/uploads/2021/02/473486main_iss_atcs_overview.pdf?ref=taranis.ie" rel="noreferrer">Active Thermal Control System (ATCS)</a> on the ISS is an example of such a thermal control system. This is a <em>very</em> complex system, using an ammonia cooling loop and a large thermal radiator panel system. It has a dissipation limit of 16kW, so roughly 16 H200 GPUs, a bit over the equivalent to a quarter of a ground-based rack. The thermal radiator panel system measures 13.6m x 3.12 m, i.e., roughly 42.5 square metres. If we use 200kW as a baseline and assume all of that power will be fed to GPUs, we'd need a system 12.5 times bigger, i.e., roughly 531 square metres, or about 2.6 times the size of the relevant solar array. This is now going to be a <em>very</em> large satellite, dwarfing the ISS in area, all for the equivalent of three standard server racks on Earth.</p><p><strong>Radiation Tolerance</strong></p><p>This is getting into my PhD work now. Assuming you can both power and cool your electronics in space, you have the further problem of radiation tolerance.</p><figure><img src="https://taranis.ie/content/images/2025/11/image-8.png" alt="" loading="lazy" width="2000" height="1100" srcset="https://taranis.ie/content/images/size/w600/2025/11/image-8.png 600w, https://taranis.ie/content/images/size/w1000/2025/11/image-8.png 1000w, https://taranis.ie/content/images/size/w1600/2025/11/image-8.png 1600w, https://taranis.ie/content/images/size/w2400/2025/11/image-8.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>By Booyabazooka at English Wikipedia - Transferred from en.wikipedia to Commons., Public Domain, </span><a href="https://commons.wikimedia.org/w/index.php?curid=1715297&amp;ref=taranis.ie"><span>https://commons.wikimedia.org/w/index.php?curid=1715297</span></a></figcaption></figure><p>The first question is <em>where</em> in space?</p><p>If you are in low Earth orbit (LEO), you are inside the inner radiation belt, where radiation dose is similar to that experienced by high altitude aircraft – more than an airliner, but not terrible. Further out, in <a href="https://en.wikipedia.org/wiki/Medium_Earth_orbit?ref=taranis.ie" rel="noreferrer">mid Earth orbit</a> (MEO), where the GPS satellites live, they are not protected by the Van Allen belts – worse, this orbit is literally inside them. Outside the belts, you are essentially in deep space (details vary with how close to the Sun you happen to be, but the principles are similar).</p><p>There are two main sources of radiation in space – from our own star, the Sun, and from deep space. This basically involves charged particles moving at a substantial percentage of the speed of light, from electrons to the nuclei of atoms with masses up to roughly that of oxygen. These can cause direct damage, by smashing into the material from which chips are made, or indirectly, by travelling through the silicon die without hitting anything but still leaving a trail of charge behind them.</p><p>The most common conseqence of this happening is a single-event upset (SEU), where a direct impact or (more commonly) a particle passing through a transistor briefly (approx 600 picoseconds) causes a pulse to happen where it shouldn't have. If this causes a bit to be flipped, we call this a SEU. Other than damage to data, they don't cause permanent damage.</p><p>Worse is single-event latch-up. This happens when a pulse from a charged particle causes a voltage to go outside the power rails powering the chip, causing a transistor essentially to turn on and stay on indefinitely. I'll skip the semiconductor physics involved, but the short version is that if this happens in a bad way, you can get a pathway connected between the power rails that shouldn't be there, burning out a gate permanently. This may or may not destroy the chip, but without mitigation it can make it unusable.</p><p>For longer duration missions, which would be the case with space based datacenters because they would be so expensive that they would have to fly for a long time in order to be economically viable, it's also necessary to consider <em>total dose effects</em>. Over time, the performance of chips in space degrades, because repeated particle impacts make the tiny field-effect transistors switch more slowly and turn on and off less completely. In practice, this causes maximum viable clock rates to decay over time, and for power consumption to increase. Though not the hardest issue to deal with, this must still be mitigated or you tend to run into a situation where a chip that was working fine at launch stops working because either the power supply or cooling has become inadequate, or the clock is running faster than the chip can cope with. It's therefore necessary to have a clock generator that can throttle down to a lower speed as needed – this can also be used to control power consumption, so rather than a chip ceasing to function it will just get slower.</p><p>The next FAQ is, can't you just use shielding? No, not really, or maybe up to a point. Some kinds of shielding can make the problem worse – an impact to the shield can cause a shower of particles that then cause multiple impact at once, which is far harder to mitigate. The very strongest cosmic rays can go through an astonishing amount of solid lead – since mass is always at a premium, it's rarely possible to deploy significant amounts of shielding, so radiation tolerance must be built into the system (this is often described as Radiation Hardness By Design, RHBD).</p><p>GPUs and TPUs and the high bandwidth RAM they depend on are absolutely worst case for radiation tolerance purposes. Small geometry transistors are inherently much more prone both to SEUs and latch-up. The very large silicon die area also makes the frequency of impacts higher, since that scales with area.</p><p>Chips genuinely designed to work in space are taped out with different gate structures and much larger geometries. The processors that are typically used have the performance of roughly a 20-year-old PowerPC from 2005. Bigger geometries are inherently more tolerant, both to SEUs and total dose, and the different gate topologies are immune to latch up, whilst providing some degree of SEU mitigation via fine-grained redundancy at the circuit level. Taping out a GPU or TPU with this kind of approach is certainly possible, but the performance would be a tiny fraction of that of a current generation Earth-based GPU/TPU.</p><p>There is a you-only-live-once (my terminology!) approach, where you launch the thing and hope for the best. This is commonplace in small cubesats, and also why small cubesats often fail after a few weeks on orbit. Caveat emptor!</p><p><strong>Communications</strong></p><p>Most satellites communicate with the ground via radio. It is difficult to get much more than about 1Gbps reliably. There is some interesting work using lasers to communicate with satellites, but this depends on good atmospheric conditions to be feasible. Contrasting this with a typical server rack on Earth, where 100Gbps rack-to-rack interconnect would be considered at the low end, and it's easy to see that this is also a significant gap.</p><p><strong>Conclusions</strong></p><p>I suppose this is just about possible if you <em>really</em> want to do it, but I think I've demonstrated above that it would firstly be extremely difficult to achieve, disproportionately costly in comparison with Earth-based datacenters, and offer mediocre performance at best.</p><p>If you still think this is worth doing, good luck, space is hard. Myself, I think it's a catastrophically bad idea, but you do you.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS LOC Record (2014) (134 pts)]]></title>
            <link>https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/</link>
            <guid>46087596</guid>
            <pubDate>Sat, 29 Nov 2025 14:02:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/">https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/</a>, See on <a href="https://news.ycombinator.com/item?id=46087596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>2014-04-01</p><section><p>2 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7lCDvfhTzFcVMB5PveKJco/9283bc24e4a11eddef1aa631a61960e9/the-weird-and-wonderful-world-of-dns-loc-records.jpg" alt=""><div><p>A cornerstone of CloudFlare's infrastructure is our ability to serve DNS requests quickly and <a href="https://blog.cloudflare.com/deep-inside-a-dns-amplification-ddos-attack">handle DNS attacks</a>. To do both those things we wrote our own authoritative DNS server called <a href="https://blog.cloudflare.com/the-story-of-a-little-dns-easter-egg">RRDNS</a> in Go. Because of it we've been able to fight off DNS attacks, and be consistenly one of the <a href="https://blog.cloudflare.com/cloudflare-fastest-free-dns-among-fastest-dns">fastest</a> DNS providers on the web.</p><p>Implementing an authoritative DNS server is a large task. That's in part because DNS is a very old standard (<a href="http://www.ietf.org/rfc/rfc1035.txt">RFC 1035</a> dates to 1987), in part because as DNS has developed it has grown into a more and more complex system, and in part because what's written in the RFCs and what happens in the real-world aren't always the same thing.</p><p>One little used type of DNS record is the LOC (or location). It allows you to specify a physical location. CloudFlare handles millions of DNS records; of those just 743 are LOCs. Nevertheless, it's possible to set up a LOC record in the CloudFlare DNS editor.</p>
            <figure>
            
            <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/SE3YNqdqCWTZjCq2X3204/6350156b41c21658e2c1cafe7780ae4b/480px-Trinity_Site_Obelisk_National_Historic_Landmark.jpg" alt="Trinity" width="480" height="480" loading="lazy">
            
            </figure><p>My site <a href="https://web.archive.org/web/20140329092052/http://geekatlas.com/">geekatlas.com</a> has a LOC record as an Easter Egg. Here's how it's configured in the CloudFlare DNS settings:</p>
            <figure>
            
            <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2Qe1yA3xIACsdf36XEi1oQ/1a078bd649abd88e5b382cab36f748b9/Screen_Shot_2014-03-27_at_11.34.24.png" alt="LOC Example" width="1894" height="96" loading="lazy">
            
            </figure><p>When you operate at CloudFlare scale the little-used nooks and crannies turn out to be important. And even though there are only 743 LOC records in our entire database, at least one customer contacted support to find out why their LOC record wasn't being served.</p><p>And that sent me into the RRDNS source code to find out why.</p><p>The answer was simple. Although RRDNS had code for receiving requests for LOC records, creating response packets containing LOC data, there was a missing link. The CloudFlare DNS server stores the LOC record as a string (such as the <code>33 40 31 N 106 28 29 W 10m</code> above) and no one had written the code to parse that and turn it into the internal format. Oops.</p><p>The textual LOC format and the binary, on-the-wire, format are described in <a href="https://tools.ietf.org/rfc/rfc1876.txt">RFC 1876</a> and it's one of many RFCs that updated the original 1987 standard. RFC 1876 is from 1996.</p><p>The textual format is fairly simple. Here's what the RFC says:</p>
            <pre><code>The LOC record is expressed in a primary file in the following format:

owner TTL class LOC ( d1 [m1 [s1]] {"N"|"S"} d2 [m2 [s2]]
                           {"E"|"W"} alt["m"] [siz["m"] [hp["m"]
                           [vp["m"]]]] )

where:

   d1:     [0 .. 90]            (degrees latitude)
   d2:     [0 .. 180]           (degrees longitude)
   m1, m2: [0 .. 59]            (minutes latitude/longitude)
   s1, s2: [0 .. 59.999]        (seconds latitude/longitude)
   alt:    [-100000.00 .. 42849672.95] BY .01 (altitude in meters)
   siz, hp, vp: [0 .. 90000000.00] (size/precision in meters)

If omitted, minutes and seconds default to zero, size defaults to 1m,
horizontal precision defaults to 10000m, and vertical precision
defaults to 10m.  These defaults are chosen to represent typical
ZIP/postal code area sizes, since it is often easy to find
approximate geographical location by ZIP/postal code.</code></pre>
            <p>So, there are required latitude, longitude and altitude and three optional values for the size of the location and precision information. Pretty simple.</p><p>Then there's the on-the-wire format. Unlike a TXT record the LOC record data is parsed and turned into a fixed size binary format. Back to RFC 1876:</p>
            <pre><code>  MSB                                           LSB
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  0|        VERSION        |         SIZE          |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  2|       HORIZ PRE       |       VERT PRE        |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  4|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  6|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  8|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 10|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 12|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 14|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+</code></pre>
            <p>So, 32 bits of latitude, longitude and altitude and then three 8 bit values for the size and precision. The latitude and longitude values have a pretty simple encoding that treats the 32 bits as an unsigned integer:</p>
            <pre><code>The latitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in thousandths of a second of arc.  2^31 represents the equator; numbers above that are north latitude.</code></pre>
            <p>And the altitude can be below sea-level but still unsigned:</p>
            <pre><code>The altitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in centimeters, from a base of 100,000m below the [WGS 84] reference spheroid used by GPS.</code></pre>
            <p>But the 8 bit values use a very special encoding that allows a wide range of approximate values to be packed into 8 bits and also be human-readable when dumped out in hex!</p>
            <pre><code>The diameter of a sphere enclosing the described entity, in centimeters, expressed as a pair of four-bit unsigned integers, each ranging from zero to nine, with the most significant four bits representing the base and the second number representing the power of ten by which to multiply the base.  This allows sizes from 0e0 (&lt;1cm) to 9e9 (90,000km) to be expressed.  This representation was chosen such that the hexadecimal representation can be read by eye; 0x15 = 1e5.</code></pre>
            <p>For example, the value <code>0x12</code> means <code>1 * 10^2</code> or 100cm. <code>0x99</code> means <code>9 * 10^9</code> or 90,000,000m. The smallest value that can be represented is 1cm (it's <code>0x10</code>). So, in just 8 bits there's a range values from 1cm to larger than the diameter of Jupiter.</p><p>To fix this I wrote a parser for the LOC text record type (and associated tests). It can be found <a href="https://gist.github.com/jgrahamc/9807839">here</a>.</p><p>We've now rolled out the fix and all the existing LOC records are being served by RRDNS. For example, my <code>geekatlas.com</code> LOC record can be queried like this:</p>
            <pre><code>$ dig geekatlas.com LOC
; &lt;&lt;&gt;&gt; DiG 9.8.3-P1 &lt;&lt;&gt;&gt; geekatlas.com LOC
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2997
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:    
;geekatlas.com.         IN  LOC

;; ANSWER SECTION:
geekatlas.com.      299 IN  LOC 33 40 31.000 N 106 28 29.000 W 10.00m 1m 10000m 10m

;; Query time: 104 msec
;; SERVER: 192.168.14.1#53(192.168.14.1)
;; WHEN: Tue Apr  1 14:13:48 2014
;; MSG SIZE  rcvd: 59</code></pre>
            </div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><astro-slot> <!--[if astro]>server-island-start<![endif]--> </astro-slot><a href="https://blog.cloudflare.com/tag/rrdns/">RRDNS</a><a href="https://blog.cloudflare.com/tag/dns/">DNS</a><a href="https://blog.cloudflare.com/tag/reliability/">Reliability</a><a href="https://blog.cloudflare.com/tag/attacks/">Attacks</a><a href="https://blog.cloudflare.com/tag/go/">Go</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hachi: An Image Search Engine (132 pts)]]></title>
            <link>https://eagledot.xyz/hachi.md.html</link>
            <guid>46087549</guid>
            <pubDate>Sat, 29 Nov 2025 13:56:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eagledot.xyz/hachi.md.html">https://eagledot.xyz/hachi.md.html</a>, See on <a href="https://news.ycombinator.com/item?id=46087549">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<h2>Hachi: An (Image) Search engine</h2>
<blockquote>
<p>Only the dead have seen the end of war ..   George Santayana
<br></p>
</blockquote>
<p>For quite some time now, i have been working on and off on a fully self-hosted <a href="https://github.com/eagledot/hachi">search engine</a>, in hope to make it easier to search across Personal data in an <code>end to end</code> manner. Even as individuals, we are hoarding and generating more and more data with no end in sight. Such "personal" data is being stored from local hard-disks to corporate controlled cloud-centers which makes it distributed in nature. So for following discussion, "Personal" meaning would be flexible enough to accommodate resources on a remote server and/or on different devices, as long the user could prove authentication and/or authorization to that data.
Current implementation supports only "images", but eventual goal is also to support other modalities like <code>video</code>, <code>text</code> and <code>audio</code>, some code would be shared, while some new code would be required to better extract <code>Features</code> for each modality.</p>
<p>Such distributed nature of data and potential capabilities of current self-hosted Machine learning models to extract semantic information, only to be queried through a single interface seemed enticing enough for me start this experiment in the first place. Following post at times may seem in-coherent, as i try to articulate my thoughts on the journey of development, challenges faced and future ideas. I hope to treat this as a personal essay with multiple themes, anecdotes and even random thoughts aiming to provide a higher level view of the journey and philosophy so far in more concrete terms.<br>
Also, following post doesn't aim to cover every technical choice and implementation in finer details, such discussions would instead be part of dedicated future posts!</p>
<h2>Motivation:</h2>
<p>As Humans we tend to remember different attributes/parts of an entity/information at different times, and most of search engines' interfaces refuse to accomodate that. User generally end up with an unidirectional flow of information, with no recourse of providing feedback to improve upon the on-going query. Even most advanced interfaces fail to handle the stochastic nature of queries and humans' pre-disposition towards partial information to keep moving, it should be default for search-engines to present best-effort suggestions for queries even if they couldn't be <em>fully</em> resolved. </p>
<p>I also note that, it is not always easy to model the imperfect information like handelling a mis-spelling, which itself could be mis-spelled in many ways. It would require a conscious effort to put in a better search interface, as most digital machines make it easy to model when "something" is "correct" or when something is "incorrect". Conveying "Why" something is incorrect takes a lot more code, effort and time, hence indicating that economic realities are more to blame for such cases than bad intentions!</p>
<p>It also presents an opportunity to analyze the capabilities of a <em>good</em> interface, as personal data would make it very easy to notice its limitations, which couldn't be observed through seemingly complete interfaces exposed by many e-commerce companies. </p>
<p>Inspired by above stated ideas, My try has been to expose multiple (if not all) attributes for a resource directly to user and then letting user recursively refine query to get to desired result. Implementation is still far from complete, but this theme has served me well to set a basic roadmap for the project. Other themes such as self-hosting, hostile behaviour towards users in terms of privacy-invading features, limited or no options to refine a search by google, github etc has contributed to evolution of this experiment. Distributed queries being served by a cluster of (refurbished) smart-phones or single-board-computers remains a lofty goal of this experiment too! </p>
<p>Despite all the good intentions and ideas, any search interface should pass that threshold of being fast enough to not end up as another impractical experiment. Efforts were involved from the beginning to embrace the inevitable complexity such projects come to include despite many refactorings. 
Below is a minimal video to help visualize the current state and capabilities of the project.</p>
<video id="video-player" width="100%" controls="" preload="metadata" poster="https://eagledot.xyz/assets/hachi_demo.png">
        <source id="video-source" src="https://eagledot.xyz/assets/hachi_demo.mp4" type="video/mp4">
        Your browser does not support the video tag.
</video>

<h2>Broader Ideas:</h2>
<ul>
<li>
<p>Minimalism:
Minimalism in terms of number of external dependencies required for this project to be bootstraped, could explain a lot about downstream choices and evolution of the project to its current form. This has  of any existing (source) code if possible or writing it from scratch which itself would require reading of a lot of existing code before i could port it to extend the project in a pure source sense.
If it would be practical to reuse some code from existing capable projects/databases, i would have done so but most of such projects are designed to be de-coupled from application code for good reasons, as they are supposed to offer much more guarantees and stay robust even under heavy load.
Being an (embedded) part of personal application we can choose to do away with such guarantees and yet expose much more information by tightly integrating ML models pipeline. In the end, application would handle much more complex indexing and inferencing pipelines, which would require a lot more code apart from search and storage interface generally expose!</p>
</li>
<li>
<p>Experimentation:
Thinking more about in terms of augmenting the existing information, rather than to duplicate it, while fusing traditional (deterministic) attributes with semantic(ML) attributes. I think this is an interesting problem and which have not been fully utilized/explored for personal applications. Most of traditional databases were written to only handle "text" modality, but current ML models allow us to query semantic information too, which opens up a new space to experiment in.
I treat semantic information as necessary and independent, but not the only signal useful to implement great search interfaces.</p>
</li>
<li>
<p>Hackability:
For this project i wanted it be very easy for someone to start modifying it according to their needs, and this mostly co-relates with the first point about minimalism, lesser the number of dependencies, lesser is the amount of configuration required to bootstrap the developing environment. Both Python and Nim are stable, cross-platform languages and are easier to extend just using a C compiler. Nim source code it easy to compile and/or cross-compile to on almost all platforms. There are already python bridges for many languages, so all such languages are fair game to extend the codebase in any desired way!<br>
Python environments (in)famously have the reputation of being difficult to bootstrap, whole parallel ecosystem is there to do so which itself creates another dependency. But i think project has made great progress in this regard, with now having a requirement of just 3 dependencies as <code>numpy</code> , <code>regex</code> and <code>markupsafe</code> and optionally <code>requests</code>, with no hard-dependence on versioning. Almost all python environments could be used to run the project with no changes, which also removes any need to bootstrap dev environment using Docker like huge dependency or any complex unwarranted build-systems plaguing many of the interesting projects. If i had money, i would pay someone to just make such projects easier to install and start with, by removing any redundant configuration or making it possible to use one common build-system !</p>
</li>
</ul>
<p>Even though above ideas may seem worthy to follow on, there is always an on-going fight to prevent dilution of agreed upon principals. Counter-intuitively i think there is some kind of <code>activation-enery</code> (<a href="https://en.wikipedia.org/wiki/Activation_energy">https://en.wikipedia.org/wiki/Activation_energy</a>) requirement for each project, past that it actually is much easier to extend, modify, optimize the codebase somewhat like paying a debt to live debt free:)     </p>
<p>There are already very capable projects like <code>Sqlite</code>, <code>Lucene</code> offering full-text search capabilities, but they implement their own storage backends which require all data to be transformed to the compatible format which leads to duplication of data . This is something i wanted to avoid, as we would be continuously transforming every newer data and this would become computationally expensive when such data wouldn't even reside on same physical machine/node. If we could get away with fast-enough queries through a much simpler index/database, that seems like something worthy to pursue further.<br> 
Most of such projects were created to handle only text queries, But current ML models expose semantic information through "vectors" or "embeddings", generated after a series of linear and non-linear operations on some text or/and an image. <code>Top-k</code> matching results are later retrived through a "Comparison" procedure with user query (embedding) as one of inputs. 
Such extensions are being gradually added in many older engines, so a hackable codebase like this project may offer more flexibilities while accomodating future ideas in this rapidly evolving field!  </p>
<p>It leads to a design comprising a meta-data indexing engine, coupled with vector-search engines for semantic search. We never intend to duplicate the original-data and don't care where it actually resides, once indexing is done. As i think search is more about reaching to a desired file/resource before that resource could be used! Pin-pointing that resource location quickly is the major motivation by incorporating the user intentions and context recursively!</p>
<p>(C)Python is used as the major language for backend and Nim (and C) is used to speed up the bottleneck portions of the codebase where-ever warranted. 
Writing from scratch allows me to update the api as i fit to handle a bottleneck portion of the pipeline (querying or indexing), without asking or waiting for a change in some upstream dependency.
Nim itself is a language with relatively smaller community, so i am getting a bit comfortable porting code from other languages to my projects with only standard library and even experimenting with my own data-structures based on (protected) reference semantics than default value semantics that Nim use!</p>
<h2>Meta-Index:</h2>
<div><p>Its a minimal module/database to handle (store and query) meta-data being extracted from resources(images) and has been written in Nim. Currently it is single-threaded, column-oriented database using Json as data-exchange mechanism between python and Nim. In future idea is to shift to leveraging multiple threads for workloads/size greater than a threshold, to better use the current hardware capabilities. It is possible to generate an <code>auxilary</code> index to speed up queries for a column/attribute on demand, which internally would use cache-friendly and hierichal data-structures to achieve so for most of scenarios! </p><p>
Through development of this module, it has been easier to note that why most of databases end-up with some kind of dedicated <code>query language</code>, as situations arise requiring composing multiple operations in one go which seems like a cleaner way to model such intentions. (and this also seems to validate the requirement of a  <code>query-planner</code> to better execute a query by analyzing the order and nature of operations and some internal details).
Since it would be written for <code>hachi</code> itself, it remains possible for me to  speed up a frequent operation by sharing a pointer/memory directly across Nim and python to prevent costly <code>copy</code> operations, or to directly serve <code>raw json</code> to the frontend in some cases without serializing and de-serializing at python boundary.</p></div>
<p>I have also experimented with multi-versioning storage design as <strong>Lmdb</strong>, to protect the original information created by code itself from user revisions. But current implementation instead favours creation of a dedicated field/attribute for user to modify/update.
For example  during face clustering process, backend will assign an unique Id for each new <code>cluster</code>, to which user may want to change to a more descriptive name, this leads to presence of attributes like <code>personML</code> and <code>person</code> in the final schema. By default, any attribute/information generated through during indexing pipeline is supposed to be immutable to be easily reset to genesis state.<br>
It still is a bit rigid implementation, as schema is locked once initialized (lazily or explicit), as adding new columns dynamically will require me to reallocate data in the memory and more syncing logic which i am off-putting for now and will work on in the future!
Current iteration supports <code>string</code>, <code>int32</code>, <code>float32</code>, <code>bool</code>, <code>array[string]</code> data-types, which seems to be enough for the application needs, but could be evolved in the future. I am not particularly content with current "string" querying, one reason is that Nim  by default does not have a concept of no-copy slice, and it is difficult to even expose such a user-defined type. As <code>strings</code> are null-terminated, so most of other composed data-structures with string as one of fields have that underlying assumption which that user-defined type will break. Also i think for a lot of meta-data attributes, i could use <code>ShortString</code>    kind of data-type to speed up scanning/querying by better leveraging the cache. Some of these issues are being experimented through an independent project and if found to improve performance could be implemented in this codebase too!    </p>
<p>There are also Simd opportunities inside the "querying" code, but since its design is being guided by overall needs for the product itself, i hope to add those architecture specific optimizations only after system-design becomes stable enough for most of the features supposed to be supported! </p>
<h2>Face-Recognition:</h2>
<p>Being able to group same person(s) with a high probability, as another attribute to search for or mix with other attributes, would be a very quality addition to any search interface. Current DL models for some-time now have been able to distinguish faces with a high accuracy. But being able to distinguish real-life faces still requires a conformance to the pipeline such models would have been trained with.<br>
There are multiple architectures for such models that have been proposed to tackle this problem, but most pipelines could be assumed to follow a generic flow, which begins with detection of facial bounding boxes from a given image or camera frame, then followed by detection of facial-landmarks for each such face, and ends with generation of  <code>embeddings/vectors</code> which figuratively would represent some kind of latent representation of that face.  At this point, this would be reduced to a Vector Spaces problem and hence much easier to deal with traditional tools like nearest neighbour search !<br></p>
<p>It almost always overwhelming to decide on a particular Implementation to build upon, while accommodating various factors like <code>latency</code>, <code>accuracy</code> , <code>hardware requirements</code>, and most of such intensive pro-bono work would never even be visible to the end-user. For me atleast this goes much further, as i would be implementing each such model using an independent ML framework, which would require me to understand also all the pre-processing and post-processing code, to be  faithfully ported to Nim.<br>
Spending time on reading papers and existing implementations helps me to get an idea about overall "capability" of the model and potential requirements during fine-tuning of the model in future. Sometimes it has been enough for me to come across an interesting concept through a paper or some nice optimization trick, even if i end up not using that particular implementation. <br>
<label for="face-rec-0">⊕</label>

<span>
Most of face embeddings generation models are trained on a <code>Siamese-loss</code> like objective to try to explicitly distinguish both positive-positive and positive-negative pairs. This generally involves manually collecting such pairs and hence prone to bias ! Such features predictors are also very sensitive to <code>face-alignment</code> code used, and hence may require you to faithfully follow the training code!
</span>
Dataset being used for training and choice of the objective function are two very major factors influencing the performance of any model. Leakage of evaluation data into training set has been a real issue in recent years for many experiments. Face-recognition itself is a <em>very</em> subjective problem and generally require more "visual" testing apart from (mathematical) metrics proposed for this problem/sub-domain.  </p>
<p>Current pipeline uses <code>retina-face</code> model to predict faces and landmarks in one go which helps producing stable facial-landmarks and speeding up the pipeline. (As predicting facial-landmarks would be much cheaper from internal features than through a dedicated model, and it also helps stabilizing  the training of the model).
Though it could make sense to argue about a model's ability to internalize learning <em>correlated</em> features without adding an explicit loss, but in practice it is always (very) beneficial to use multiple losses explicitly.
Interestingly, <code>residual</code> connection in <code>ResNets</code> was an important innovation making it possible to train much deeper networks at that time, even though it would be just mimicing an <code>identity</code> function.
<label for="residual-0">⊕</label>

<span>
  <img src="https://eagledot.xyz/assets/residual_0.png" alt="Residual component">
  Residual block, see <a href="https://en.wikipedia.org/wiki/Residual_neural_network">https://en.wikipedia.org/wiki/Residual_neural_network</a>
</span>
Explicit multiple losses decrease the chances of over-fitting by large. There could be other auxiliary objectives that are used during training only by means of an smaller auxiliary network and then not used/required during inference, just like training wheels :)</p>
<p>In my experience, dataset being used for training and choice of the objective function are two very major factors influencing the performance of your model on real-life (bit out-of-distribution datasets). I find it a good practice to always visually debug some of the random samples to get a "feel" for the dataset!</p>
<p>Even after having a good pipeline to generate "embeddings" for a face, <code>clustering</code> remains a very challenging problem, due to various reasons.
Like with almost all clustering algorithms, we start out with no prior information about of the underlying (number) distribution of the data (faces). (as this is what we would be trying to estimate). As we keep encountering the newer information, possible <code>updates</code> through <code>back-filling</code> are required for the underlying index, which somewhat resembles of an auto-regressive operation and hence the error-accumulation rate is relatively high. We would also need to wait for some "initial" amount of data/information to be collected, to estimate initial stable centroids. This difficulty is further compounded by the choices for various thresholds like face-detection, some measure for blurness in the detected face, and a dependence on order of information being encountered.</p>
<p><label for="face-models-info">⊕</label>

<span>
As indicated, choosing same model to predict landmarks and face-bounding boxes, helps reduce the impedance mismatch that occurs when output of one model is being fed through another model. We would need to a dedicate model for facial-features though as earlier features may not be dense enough to distinguish among individual faces!
</span></p>
<p>Currently Implementation works by collecting some minimal amount of information before <code>Cluster</code> creation process could begin. 
Each Cluster is a union of a set of main/master embeddings and a set of follower/slave embeddings. Selection of main embeddings is a crucial part to maintain the stability of a cluster even when new information would be encountered. Initial filtering of unfeasible (master) embeddings is done through some static criterias, for example we strive to filter any of <code>blurred</code> faces, face-profile is estimated through facial-landmarks, stable forward-facing profiles make face-alignment easier further in the pipeline. Such (static) criterias definitely help to reduce the number of invalid candidates, but may not be enough for many real-life datasets. A further minimal module comparing the <code>hog-features</code> with a set of pre-saved hog-features is introduced to help invalidate faces with <code>sunglasses</code> and some false positives not caught by earlier criterias!</p>
<p><label for="hog-compare">⊕</label>

<span>
  <img src="https://eagledot.xyz/assets/hog_compare.png" height="120" alt="hog comparison">
  Hog features are finally compared at pixel level, after applying normalization!
</span>
After experimenting with other approaches like SIFT-features, i found it easier to compare hog-features generated from <em>aligned</em> faces/eyes. Alignment part of the pipeline is crucial to generate rich embeddings, even minor deviation from reference landmarks end up producing bad-embeddings rendering the pipeline useless. All feasible candidates/embeddings are then compared sequentially to create final clusters conditioned on some threshold. Note for now this is not exhaustive and hence order in which information is being encountered would have some effect on final clusters! Remaining follower ids are also then assigned (sequentially) to one of the clusters or to a special cluster like <code>no-categorical-info</code>, when not able to being fit into any of the clusters.
Note that a lot of empirical data comes into effect as multiple decisions would be required while choosing many thresholds and may require multiple runs .</p>
<p><img alt="alt ML codebase sample" src="https://eagledot.xyz/assets/face_rec_1.png"></p>
<p>Since face-recognition is very subjective and i myself have to compare other features to make sure that indeed the <em>correct</em> person(s) have been grouped together by the pipeline. But with a latency of around 25 ms, it seems to do very good on a held out dataset of persons with close up faces, (Zen-Z) selfies and sunglasses occluded eyes. Personal photos are much easier to classify/recognize compared to such a dataset!  </p>
<p>For any practical ML integrated product, We would need to have a very performant concurrent pipeline to keep feeding the model while being constantly aware of any data-distribution impedance mismatch, to reach anywhere near the 'accuracy' and <code>speed</code> promised in a research paper. This touches upon the issue of having good understanding of software engineering basics, while being aware of possible regressions resulting from a newer functionality like ML.<br>
Though bigger VLM/LLM (base) models have potential to handle data-impedance mismatch issues due to their sheer size, their usage would definitely hamper the application responsiveness and have proven to be relatively rigid to be fine-tuned for a specific domain! </p>
<h2>Indexing:</h2>
<p>Indexing pipeline begins with desired data location as its input to recursively scanning <code>directories</code> to collect <code>raw-data</code> in batches.
Multiple meta attributes such as <code>exif-data</code>, <code>size</code>, <code>mount location</code>, <code>name</code> are extracted to be later queried through the Meta-indexing engine. Focus has been on designing a good schema to accomodate future use-cases, but since we would be collecting only meta-information without ever modifying the original or duplicating the original data, it remains relatively easier to shift to a newer version/schema even through automatic means.<br>
ML models extract semantic information which can be later queried through a vector-indexing engine. By default resources to be indexed are assumed to be residing on a local-disk but any protocol could be leveraged, if proper authorization and authentication could be provided.<br>
 Monolithic nature of the code helps me to share <code>raw-data</code> read/collected once for various components like <code>hash generation</code>, <code>preprocessing</code> code for ML models, reducing the number of costly I/O calls. This pipeline has come a long way from a blocking implementation to its current (almost) fully async nature, resulting in very high saturation of computing resources. Apart from running multiple threads, dedicated kernels/functions are used to speed up pipeline by <code>fusion</code> of operations wherever possible.
 One such example/scenario  has been shown below.</p>
<div><pre><span></span><code><span>def</span> <span>preprocess_kernel</span><span>(</span>
    <span>image</span><span>:</span><span>Tensor</span><span>[</span><span>uint8</span><span>],</span>
    <span>new_shape</span><span>:</span><span>tuple</span><span>[</span><span>int</span><span>,</span><span>int</span><span>],</span> 
    <span>rgb_to_bgr</span><span>:</span><span>bool</span> <span>=</span> <span>True</span><span>,</span> 
    <span>normalize</span><span>:</span><span>bool</span> <span>=</span> <span>True</span><span>):</span>
    <span># Preprocess kernel, may fuse resize, color_conversion and normalization into one function!</span>

    <span># Pseudo-code!</span>

    <span>result</span> <span>=</span> <span>newEmptyTensor</span><span>[</span><span>uint8</span><span>](</span><span>new_shape</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>new_height</span><span>:</span>
        <span>for</span> <span>j</span> <span>in</span> <span>new_width</span><span>:</span>
            <span>inp_h</span><span>,</span> <span>inp_w</span> <span>=</span> <span>get_corresponding_pixel</span><span>(</span><span>image</span><span>,</span> <span>i</span><span>,</span> <span>j</span><span>)</span>
            <span>for</span> <span>k</span> <span>in</span> <span>0.</span><span>.&lt;</span><span>3</span><span>:</span>
                <span>if</span> <span>rgb_to_bgr</span><span>:</span>
                    <span>result</span><span>[</span><span>i</span><span>,</span><span>j</span> <span>,</span> <span>3</span><span>-</span><span>k</span><span>-</span><span>1</span><span>]</span> <span>=</span> <span>image</span><span>[</span><span>inp_h</span><span>,</span> <span>inp_w</span><span>,</span> <span>k</span><span>]</span>
                    <span># normalize based on mean and deviation used for training dataset further...</span>
                <span>else</span><span>:</span>
                    <span>result</span><span>[</span> <span>i</span><span>,</span><span>j</span><span>,</span><span>k</span><span>]</span> <span>=</span> <span>image</span><span>[</span><span>inp_h</span><span>,</span> <span>inp_w</span><span>,</span> <span>k</span><span>]</span>
</code></pre></div>

<p>Each <code>resource</code> could be assumed to go through a flow like this:</p>
<div><pre><span></span><code><span>resource_location</span> <span>=</span> <span>"file://xyz.jpg"</span>
<span># OR</span>
<span>resource_location</span> <span>=</span> <span>"remoteProtocol://xyz.jpg"</span>

<span>raw_data</span> <span>=</span> <span>download_raw_data</span><span>(</span><span>resource_location</span><span>)</span>

<span>embeddings</span> <span>=</span> <span>ML_model</span><span>(</span> <span>preprocess</span><span>(</span><span>raw_data</span><span>))</span>
<span>exif_data</span> <span>=</span> <span>extract_exif_data</span><span>(</span><span>raw_data</span><span>)</span>
<span>preview</span> <span>=</span> <span>generate_preview</span><span>(</span><span>raw_data</span><span>)</span>
<span>write_preview</span><span>(</span><span>preview</span><span>)</span>
<span>....</span>
</code></pre></div>

<h2>Vector Index:</h2>
<p>It is another minimal module to store vector-embeddings as <strong>shards</strong> on the disk. Necessary meta-data is stashed along with that shard, to make it self-contained, which in future will help in distributed/parallel retrieval. For now each shard is just a numpy (float32) Tensor, and comparison routine is a <code>np.dot</code> operator, which itself use the <code>blas/openblas</code> library to speed up this operation! Each shard is loaded from the Disk during a <em>query</em>, and <code>top-k</code> candidates are collected to be fused together with other deterministic meta-attributes. Loading from Disk do add some latency, but it allows me to regulate RAM usage through <code>shard-size</code> hyper-parameter, to allow running this on different platforms  with diverse specifications including single-board computers. <code>Shard-size</code> could be kept relatively high for higher RAM systems to speed up shard querying.</p>
<p><code>Matmul</code> is one of the most optimized algorithms which run at almost 90% of theoretical capacity on most of intel/amd Cpus when leveraging <code>Blas</code> like libraries. So every further optimization from here-on would involve some kind of information loss. There is a whole literature now to speed up this comparison/retrieval process through <code>quantization</code> and/or <code>nearest neighbour</code> indices like HNSW. Fast SSDs are also leveraged to run such comparisons at very high speed for upto billion vectors on just a single node in near real time!</p>
<p>But such all techniques involve compression of information (which itself is best-effort being the result of modeling a large amount of biased data) through out-of-band mechanisms, for example creating centroids/clusters is just based on the vector values and taking some mean without a way to pass back the information to the model which produced those vectors in the first place. This way is quick and you would get great speed-ups, and there is an active debate among vector-database vendors across various metrics and implementations. In my experience only visual results on a <em>personal</em> data would be a good metric a user should test for. Product-quantization is something i would be implementing if were to choose one, as i think coupled with <code>top-k</code>, it should work reasonably well to include (subjectively) correct results (high recall!) .</p>
<p>Another worthy and very effective solution i think is to instead <em>train</em> a linear layer to <em>finetune</em> the original model depending upon the task. ML Features/embeddings from a big enough model, could assumed to have a <em>knowledge</em> about diverse topics, but for example, a user may be trying to distinguish between different plants. A linear layer could easily be trained with just few thousand samples, to achieve so with much higher accuracy than original models, and even with half the size/dimension of original embeddings. Intuitively it could be thought that we freed the information channels to just focus on plants, decreasing the entropy model earlier had to deal with. Any such layer could be trained even without any framework, as it would just be one <code>backward</code> operation to implement.
OpenAI has a nice cookbook if a reader would want to explore this further!
<label for="embed-playbook">⊕</label>

<span>
  <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb</a></span></p>

<p>An interesting thing sharding allows is to use any available <code>hardware</code> to speed up retrieval. Since we need just <code>comparison</code> routine and corresponding shard(s) to return top-k candidates, it de-couples it from any of application code. A new smartphone could be detected, and some <code>shards</code> could be transferred during initial set-up, optimal percentage/number of shards could be easily calculated by running same <code>comparsion</code> operation on new device. Like running a <code>2048 x 2048</code> , inner-product op and comparing latency with <code>master/main</code> device, would tell us the capacity of the new device and so that number of shards would be transferred to speed up retrieval process!</p>
<p>There are performance gains to be have in the current implementation, would like to atleast start using <code>float16</code> data-type, but its a bit tricky on intel cpus with no compiler support for this type. Printing of CPU capabilities <em>do</em> show the presence of float16 hardware support on my system ! 
ARM(v8 64) seems to offer native float16/floatH types, there seems to be difference in that type either supported natively by compiler or as an intrinsics/assembly code. I have not been able to get expected speed up for now! Such code is still being experimented upon in the limited time i have.</p>
<h2>Backend:</h2>
<p>Backend is written in python, which exposes a pure API server, to let the client/frontend to make API calls to. Starting with very naive code to just return all the meta-data for a <code>directory</code> to current pagination support it have gone through many revisions and design iterations and now i have much clearer idea about how to architect/wrap a big enough piece of functionality. I wanted the app to be end to end, but this also put extra pressure on app to be responsive enough for all user events. Current indexing code is capable of providing rich details such as directory currently being scanned, estimated time (eta) and allows robust <em>Cancellation</em> of an ongoing task/threads. 
It has not been easy to model such communication b/w concurrent tasks and touches upon much discussed <code>structured-concurrency</code> debate i.e how to run multiple tasks asynchronously, while being able to robustly cancel them at any point in time, all while being able to collect all errors cleanly!<br></p>
<p>From C days, i have been a user of (Posix) <code>threads</code> type implementations, since major OSes provide those minimal but stable APIs, it helps me during context switching to different languages. Both C and Nim expose that, Python itself let the OS manage threads without its own runtime implementation, but bypassing the GIL when makes sense is something user have to do to fully utilize the resources! Also this kind of code requires user to handle a lot of code as to communicate b/w threads but atleast i (think) understand the basic ideas to prevent deadlocking if occurs and iron out initial bugs. As you run such threads deeper and deeper inside application stack , it keeps getting harder to communicate information back to the client. But when it starts working, it is really cool to have a central interface to see all the stuff backend is doing and predict very good ETA !</p>
<p><code>Flask</code> was initially used to easily map <code>functions</code> to a particular route/url to wrap up initial implementation, current implementation now just uses <code>werkzeug</code> (main engine behind flask) directly, hence doing away with a lot of unrequired dependencies like a template engines that Flask ships with. Even though this would not effect the end user in any visible way, this has been a very nice quality-of-life improvement like stuff for me as a developer. Since werkzeug is pure python, it can now be shipped/bundled directly as source code. Also each request is now handled by an available thread (from a pool) by reading <code>http environment</code> from a shared queue following conventional model. By default for multi-threaded option, werkzeug would create a new fresh thread for handling that request. This does away with lots of OS/system calls for each new request and latency now seems more consistent and predictive. I have also  stumbled upon a pattern to actually make it easier to <code>mount</code> multiple <code>apps</code>  cleanly given i never liked and even understood the <code>blueprint</code> that flask offers to make it easier to distribute the logic of your app to other modules too.
Since WSGI protocol just expect a callable python object, it should be much easier to develop independent <em>apps</em> without having any knowledge where it would be called/used. It also makes it quite fun to actually write/expose python code to handle client inputs. </p>
<div><pre><span></span><code><span>class</span> <span>SimpleApp</span><span>():</span>
    <span>"""Each instance could be used a WSGI compatible callable"""</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>allow_local_cors</span><span>:</span><span>bool</span> <span>=</span> <span>False</span><span>):</span>
        <span>self</span><span>.</span><span>initialized</span> <span>=</span> <span>False</span>
        <span>self</span><span>.</span><span>http_methods</span> <span>=</span> <span>[</span><span>"GET"</span><span>,</span> <span>"POST"</span><span>,</span> <span>"PUT"</span><span>,</span> <span>"DELETE"</span><span>,</span> <span>"OPTIONS"</span><span>]</span> 
        <span>self</span><span>.</span><span>url_map</span> <span>=</span> <span>None</span> <span># we will lazily initialize it!</span>
        <span>self</span><span>.</span><span>extension_prefix</span> <span>=</span> <span>"ext"</span> <span># as apps would be registered/</span>
        <span>self</span><span>.</span><span>registered_extensions</span><span>:</span><span>dict</span><span>[</span><span>str</span><span>,</span> <span>SimpleApp</span><span>]</span> <span>=</span> <span>{}</span>

        <span>....</span>

    <span>def</span> <span>add_url_rule</span><span>(</span><span>self</span>
                     <span>rule</span><span>:</span><span>str</span><span>,</span> 
                     <span>view_function</span><span>:</span><span>Callable</span><span>,</span> <span># corresponding view.</span>
                     <span>endpoint</span><span>:</span><span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span> <span># set to view_function</span>
                     <span>methods</span><span>:</span><span>list</span><span>[</span><span>str</span><span>]</span><span>=</span> <span>[</span><span>"GET"</span><span>]):</span>

        <span>...</span> <span># some validation code.</span>

        <span>self</span><span>.</span><span>endpoint_2_uri</span><span>[</span><span>endpoint</span><span>]</span> <span>=</span> <span>(</span><span>Rule</span>
            <span>(</span><span>rule</span><span>,</span> <span>endpoint</span> <span>=</span> <span>endpoint</span><span>),</span> <span>methods</span>
            <span>)</span>
        <span>self</span><span>.</span><span>endpoint_2_viewFunction</span><span>[</span><span>endpoint</span><span>]</span>  <span>=</span> <span>view_function</span>
        <span>self</span><span>.</span><span>initialized</span> <span>=</span> <span>False</span>

    <span>def</span> <span>register</span><span>(</span><span>self</span><span>,</span> <span>app</span><span>:</span><span>SimpleApp</span><span>,</span> <span>name</span><span>:</span><span>str</span><span>):</span>
        <span>"""</span>
<span>        Here we register another such `app`.</span>
<span>        It would be mounted at `/ext/&lt;name&gt;` , so all requests to /ext/&lt;name&gt;/&lt;route&gt;, would be forwarded to this `app` .</span>
<span>        """</span>

        <span>...</span> <span># some validation code.</span>
        <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>name</span><span>]</span> <span>=</span> <span>app</span>
        <span>print</span><span>(</span><span>"Extension registered at: </span><span>{}</span><span>/</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>self</span><span>.</span><span>extension_prefix</span><span>,</span> <span>name</span><span>))</span>


    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>environ</span><span>,</span> <span>start_response</span><span>)</span> <span>-&gt;</span> <span>Iterable</span><span>[</span><span>bytes</span><span>]:</span>
        <span># This is called </span>
        <span>if</span> <span>not</span> <span>(</span><span>self</span><span>.</span><span>initialized</span><span>):</span>
            <span>print</span><span>(</span><span>"[Initializing]: Parent"</span><span>)</span>
            <span>self</span><span>.</span><span>initialize</span><span>()</span>

        <span>for</span> <span>ext</span> <span>in</span> <span>self</span><span>.</span><span>registered_extensions</span><span>:</span>
            <span>if</span> <span>not</span> <span>(</span><span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>ext</span><span>]</span><span>.</span><span>initialized</span><span>):</span>
                <span>print</span><span>(</span><span>"[Initializing]: </span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>ext</span><span>))</span>
                <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>ext</span><span>]</span><span>.</span><span>initialize</span><span>()</span>

        <span># If a call to such an extension.. we modify the environment a bit.</span>
        <span>active_app</span> <span>=</span> <span>self</span>
        <span>extension_name</span> <span>=</span> <span>None</span>
        <span>temp_path</span> <span>=</span> <span>environ</span><span>[</span><span>'PATH_INFO'</span><span>]</span>
        <span>temp_split</span> <span>=</span> <span>temp_path</span><span>.</span><span>split</span><span>(</span><span>"/"</span><span>)</span>
        <span>if</span> <span>temp_split</span><span>[</span><span>1</span><span>]</span> <span>==</span> <span>self</span><span>.</span><span>extension_prefix</span><span>:</span>

            <span>extension_name</span> <span>=</span> <span>temp_split</span><span>[</span><span>2</span><span>]</span>
            <span>assert</span> <span>extension_name</span> <span>in</span> <span>self</span><span>.</span><span>registered_extensions</span><span>,</span> 
            <span>extension_path</span> <span>=</span> <span>temp_path</span><span>.</span><span>replace</span><span>(</span><span>"/</span><span>{}</span><span>/</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>self</span><span>.</span><span>extension_prefix</span><span>,</span> <span>extension_name</span><span>),</span> <span>""</span><span>)</span>


            <span>environ</span><span>[</span><span>'PATH_INFO'</span><span>]</span> <span>=</span> <span>extension_path</span>
            <span>environ</span><span>[</span><span>'REQUEST_URI'</span><span>]</span> <span>=</span> <span>extension_path</span>
            <span>environ</span><span>[</span><span>'RAW_URI'</span><span>]</span> <span>=</span> <span>extension_path</span>

            <span>active_app</span> <span>=</span> <span>self</span><span>.</span><span>registered_extensions</span><span>[</span><span>extension_name</span><span>]</span>

    <span>## -----------------------------------------------</span>
    <span># NOTE: only werkzeug specific code is here!</span>
    <span># ---------------------------------------------</span>
    <span>request</span> <span>=</span> <span>Request</span><span>(</span><span>environ</span> <span>=</span> <span>environ</span><span>)</span> <span># minimal wrapping code!</span>
    <span>urls</span> <span>=</span> <span>active_app</span><span>.</span><span>url_map</span><span>.</span><span>bind_to_environ</span><span>(</span><span>environ</span><span>)</span>
    <span>endpoint</span><span>,</span> <span>args</span> <span>=</span> <span>urls</span><span>.</span><span>match</span><span>()</span>

    <span># view function can choose to return iterable[bytes] are the result of view function or call , or further wrap it to be as expected by werkzeug!</span>
    <span>iterable_bytes</span> <span>=</span> <span>active_app</span><span>.</span><span>endpoint_2_viewFunction</span><span>[</span><span>endpoint</span><span>](</span><span>request</span><span>,</span> <span>**</span><span>args</span><span>)</span> 
    <span>return</span> <span>iterable_bytes</span>  <span># as WSGI protocol expects!</span>
    <span># ---------------------------------------------------------</span>
</code></pre></div>

<p>Note that, any existing Python object, can be made to accept <code>client</code> requests on demand by adding very minimal code and could be done for selective functionality. For example, during setup of a new android device, i may have to ask user to choose one of the existing <code>devices</code>, this kind of interactive input can be modeled easily now, as i just add a new routine in the Corresponding class to accept requests on a route such as <code>/ext/android/beginSetup</code>, once i get that, all the existing logic already written could be used to finish setup. It is as easy as <code>parent_app.register(app = thisApp, name = "android")</code> to start routing corresponding requests to this app!</p>
<h2>ML:</h2>
<p>Machine learning is being powered by a framework written completely in Nim, most of work was done on that framework before i even stared working on this project. This has allowed me to wrap CLIP and Face-Recognition Pipeline along with the application while only depending on OneDNN for some routines. OneDNN (mkldnn) (<a href="https://github.com/uxlfoundation/oneDNN">https://github.com/uxlfoundation/oneDNN</a>) is one of the libraries to speed up various Deep learning operations with great documentation. </p>
<p>Ported models run faster  on intel/Amd Cpus than pytorch counterparts, owing to fusion of operations like Batch Normalization and Convolution, and high re-use of pre-allocated memory (similar to in-place operations). Current <code>torch.compile</code> like engine would end up making some of those optimizations after analyzing the graph, but for at-least 2.0 version it is not supported on Windows for me to compare against!</p>
<p>It took a lot of effort during one-two years i was working on it to be complete enough for me to start porting Deep-learning models using it. Also OneDNN shifted to V3 during that time, and only some code was updated to newer API and this has left the project in a unstable state with no visible stable APIs for users to work with. For each model i have to manually analyze the locations/requirements for fusion of operations, port quite a lot of pre-processing and post-processing code to make it end to end. These reasons contributed to a lot of technical debt, which i have not found the resources to tackle yet. Without removing that debt it never made sense to open-source it, besides there are now projects like GGML, and tiny-grad to serve inference only needs with minimal resources! </p>
<p><img alt="alt ML codebase sample" src="https://eagledot.xyz/assets/ml_1.png"></p>
<p><img alt="alt ML codebase sample" src="https://eagledot.xyz/assets/ml_2.png"></p>
<p>Porting of each model is quite an involved task, as you have to read enough papers to understand ideas about model if want to later fine-tune that model too. You may want to find first find or create a simpler implementation in pytorch to make it easier to port to a new language. All experimentation could be done in pytorch/python, for example i experimented with alternate quantized attention layers for CLIP model, and it indeed had a better performance for eval datasets mentioned in CLIP paper. Tangentially it was really cool to read through Open-AI implementations and papers, papers were written in an approachable manner to let the read indulge in hypothesis, codebases were clean with minimal dependencies. Its really a shame what that company/organisation chose to become under the guise of "user-safety" effectively clipping the (open) ethos of this field, but at same time i am grateful for all the researchers' work in this current DL/ML era and seeing the evolution of this field in such an open manner!   </p>
<p>I would like to work on the project though atleast enough to tackle that debt and open-source it in state for users to extend upon, if found useful.
Even though i am using OneDNN for some routines, i think it is better to have a common and easier to extend codebase to allow more experimentation and aggressive optimizations , but this itself is a huge-task and now with multiple GPU architectures its just something that couldn't be tackled without a lot of time and money. Even in this age where H100 is the baseline for benchmarks in testing, i find it worthwhile to work on a minimal DL Compiler to just tackle ARM/Intel/Risc Cpus to start taking advantage of these cheaper machines. Being able to pin-point a tennis ball in a 3D space remains the dream !</p>
<h2>Frontend / App:</h2>
<p>Current front-end is completely written in Html, Js(Ts) and (tailwind) css as multi page webapp. Earlier frontend was written in Svelte, but lack of internal documentation and too much "magic" became too "viral" for me to handle. For me, abstractions and APIs exposed by Browsers are more than enough to maintain required precision during DOM updates. Care is taken to use batch updates, prevent redundant rendering, judicial usage of resources to prevent unrequired pressure through pagination, even for a local backend server. It has passed our litmus test for search over 180 Gb of indexed Pexels dataset on a (minimal) remote server. My friend <a href="https://github.com/akshaymalik1995">Akshay</a> helped a lot in frontend development, testing various datasets and offering detailed bug reports which helped uncover a lot of edge cases during development of the project. There would always be room for improvements on the UX/UI side, but we have found it is much easier to extend and improve frontend with a stable backend! </p>
<p><label for="pexel-link">⊕</label>

<span>
  Pexels dataset: <a href="https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf">https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf</a></span></p>

<p>Apart from webapp, there is also a Windows App, which under the hood uses the <code>webview</code> to render the frontend. All native Windows APIs remain available to use from the Nim code, which puts it into a hybrid category. It is not ideal, but atleast it doesn't require me to ship a full web-browser, which i think is waste of compute resources, but at the same time leaves me wondering how current GUI development became so resource intensive for a single developer to manage while offering little benefits! I have been looking into forks of earlier GTK versions for linux to keep the complexity/learning contained, but that also seems nothing less than an adventure!  </p>
<h2>Tools/libraries:</h2>
<ul>
<li>
<p>Nimpy (<a href="https://github.com/yglukhov/nimpy">https://github.com/yglukhov/nimpy</a>) : A minimal python-Nim bridge to make it easier to write extensions in Nim to be called from python and to use python modules in Nim. Unlike many such bridges which includes a lot of boiler-plate code, there are no complex classes/interfaces to be included in the extension. It targets necessary features like marshaling of native python types to and from Nim, targets the minimal Python API to not depend on python versions, finding underlying python.dll at runtime.</p>
</li>
<li>
<p>Stb Image (<a href="https://github.com/nothings/stb">https://github.com/nothings/stb</a>): A big fan of such single header libraries, this one implements encoders for most of image formats in pure C. Its very easy to modify it pass pointer to the raw-data and writing raw-data to a pre-allocated memory saving costly memory copying particularly visible for 4k photos! It helps remove dependency on OpenCV for image reading ! Nim made it very easy to just compile this along with other Nim code.</p>
</li>
<li>
<p>LibWebp (<a href="https://github.com/webmproject/libwebp">https://github.com/webmproject/libwebp</a>): Allows decoding and encoding for webp formats,  Though documentation is a bit sparse on some internal API usage, lot of examples are included in the repository to read. I managed to use <code>argb</code> field directly to pass <code>argb</code> format data to do away with transformation logic and some (memory) allocations. It follows callback passing convention to implement custom behaviour like a progress bar and to write encoded data to a user provided buffer. Written completely in C and very easy to compile and read, it is being used for writing image previews, helping remove dependency on OpenCV.</p>
</li>
<li>
<p>Zig-cc (<a href="https://ziglang.org/">https://ziglang.org</a>): Using <code>zig/clang</code> as a C compiler, allowed me to easily cross-compile a lot of Nim code for Linux, targeting <code>2.27 libc</code>. Making it easier to set a LibC target has proved very useful to bypass that <code>libC</code> mismatching stuff!
Really cool work by Zig community to tackle a lot of such technical debt to make software development much easier !</p>
</li>
</ul>
<p>As mentioned earlier i try to use a lot of existing open-source code if i can, even it would be for reading/understanding purposes only. It still blows my mind even after many years, to just read/understand some complex implementation and modify it for personal use-case for <em>Free</em>.
For example even though <code>OpenCV</code> is a big/complex dependency, its still has a very readable codebase and i read code from it a few times during this project to understand differences b/w my port and OpenCV one.</p>
<p>Being able to integrate multiple languages has its own challenges too, as it would require us to understand boundaries, internal details, assumptions that each runtime would want developer to respect. It gets complex to reproduce and understand bugs while running multiple multi-threaded runtimes as  debugging gets more difficult. Debugging is one of things i would like to get better at, i have very limited knowledge of GDB as of now, which is expected to be table stakes for debugging in such environments. I have had some nasty bugs , but being able to compile all required pieces made it a bit easier to debug even with print-style debugging :)</p>
<h2>Current State:</h2>
<p>A lot of functionality is working, than not and having tested over 500k images i could be a bit satisfied about internals' performance and robustness. I would like to say that it can easily handle 10 millions of images/resources, and there is nothing to suggest that it won't, but it is different from using a production database to extrapolate the performance confidently. Despite writing from (almost) scratch in a number of languages, both indexing and inferencing pipeline are more expressive, robust and faster than many similar images search apps, but benchmarking for such complex apps could be subjective and more so when you mix in semantic search.</p>
<p>There are still some hardcoded constants and also intentionally some low performing components, like using ViT B/32 variant of CLIP model, which are acting as placeholders, and would be replaced easily with better counterparts in the future. </p>
<p>It has been tested on Windows 10/11 and on Fedora 42/43 with an assumption of <code>x64</code> architecture. Compiled extensions are also packaged to quickly test the application, but users are free to compile code as they see fit. Linux shared objects target <code>LibC 2.27</code>, so should work on most of recent distributions out of the box. Except some ML code there is main requirement of any/a C compiler to further extend the codebase by the user. Most of testing is done on my Laptop with  <code>i5-8300H</code> processor and 8 GB memory. I don't have a MacOS to test on, ML code would need to be modified to target ARM architecture, except that very minor modifications should be needed if any. It is quite possible for initial users to encounter minor bugs, due to its limited run in diverse dev environments, but installation and usage on Cloud servers during testing has been quite smooth.</p>
<p>Below is a video showcasing workflow to index data from multiple MTP/Android devices. (Still a WIP). </p>
<video id="video-player" width="100%" controls="" preload="auto" poster="https://eagledot.xyz/assets/extension_demo.png">
        <source id="video-source" src="https://eagledot.xyz/assets/extension_demo.mp4" type="video/mp4">
        Your browser does not support the video tag.
</video>

<h2>Random Thoughts/Philosophy:</h2>
<p>I think it gets easier with time to grok larger codebases to isolate/find the functionality/implementation reader would be interested in. Most of mature codebases are organized to help navigating the source-tree anyway, and have detailed documentation. Being able to have enough patience to make yourself comfortable is a necessary part of growing as a developer, as initial documentation/codebase would always seem alien and big enough to trigger that flight reaction! </p>
<p>Batching and Caching are two generic strategies that could be applied to speed up most of bottleneck portions. Both strategies lead to better/denser utilization of CPUs by (trying to) minimise the costly load/store instructions during a hot loop. Batching for example could do it by allocating necessary memory up-front for a batch and de-allocating all at once when no longer required, reducing the number of costly system-calls. Caching may involve designing or using a (hardware)cache friendly data-structure, when it is possible to do so.</p>
<p>Each optimization would involve assumptions and each subsequent optimization would become harder and harder to implement, may preventing the clean refactoring of code when future functionalities may need to be accommodated. It itself is a kind of rabbit-hole, and user should know when to stop as there would always be something <em>else</em> to be optimized! </p>
<p>With (coding) tools involving AI/LLMs it is easier than ever to get a piece of desired functionality, as a developer i understand it is another useful tool in a long-history of improvements, that most of developers would come to use in their workflow. Current LLMs have undeniable ability to handle complex instructions, explain non-trivial code and that so for various mixed modalities! It has been a bit <em>unreasonable</em> to end up with such abilities with just next token prediction as primary objective, even for a researcher working in this field.
My usage for such tools is only through a (free) search engine(s), Although for now there has been <em>no</em> scenario in such tools have helped me, that i wouldn't have got to using traditional means. But i can admit such tools/engines are really effective in helping us to get unstuck in a variety of situations, arguably helping us to <em>learn</em> faster. <code>Functions/routines</code> are nice and enough abstractions to provide enough context to such engines, to get the required <em>help</em>, without ever needing <code>review/edit/rewrite</code> cycle.<br>
I have <em>always</em> been benefited from visiting the original documentation, if AI is spitting out good enough arguments, there must be a good documentation out there for that topic . Our minds capability to extract abstract patterns resulted from <em>studying</em> one topic and applying it to another seemingly unrelated domain is uncanny to say the least. Also tone/motivation for developer writing about a topic matters to me, and many times i have visited a concept further just because writer himself/herself was very excited about it . Again, these are just personal factors and biases and people should be free to choose workflow they feel most comfortable in , without any judgments from either side.<br>
<label for="llm-note">⊕</label>

<span> 
It has been difficult to access SOTA models actual abilities, with fewer and fewer details being published for each newer version, but it has been a wild-ride for me to see the evolution from RNNs to bi-directional RNNs to LSTMs to Transformer architecture (finally founding atleast one stable architecture be able to support training on whole internet without exploding or vanishing gradients).
Arguably there are also more <em>more</em> open family of models like <code>Qwen</code> or <code>Deepseek</code> from other labs which could run on <em>local</em> infrastructure. Even at this stage, ideas behind LLMs are simple enough for anybody to understand without burdening them with terms like AGI . There is already great work from <a href="https://allenai.org/olmo">OLMO</a> and <a href="https://github.com/huggingface/smollm">Smollm</a>  to build upon and start with, for personal needs, without spending a lot of money. On technical front  there is still much more to explore and it comes down to doing more experiments by smaller companies to prevent ending up with another monopoly/duopoly in this field only to later blame such for their incompetence!<br>
</span>
I literally have no idea what would be the end game with this ever increasing ability of AI models and what social consequences we would end up with in an already fragmented and struggling world.
But it would be a mistake to abandon <em>learning</em>, however inconvenient it may seem at any time, if we were to survive !  <br>
Thing that really boils my blood is these (AI) companies <strong>lawless</strong> laundering of <em>all</em> the open-source code, art, poetry without any attribution only to be packaged as a product for users to pay for. Constant attacks on all the infrastructure even run by very small or single-developer companies/communities, not respecting any of the <code>robots.txt</code>, proxying through residential networks, damaging the very core of the information-sharing/internet while coming up with ironical headlines is bordering on criminal-behaviour for me! Waiting for tens of seconds just for a (community written) stack-overflow post through many layers of <em>security</em>, for wanting to understand various perspectives for some concept without all the bullshit summarization, is new bleak reality with nothing for end-users to have a say in.  </p>
<p>Despite the dominant usage of LLMs there exist equally interesting smaller models/architectures representing the huge potential that this field of deep-learning holds. Neural-networks allow us to (good enough)model any arbitrary function/flow using an iterative framework from a few thousand samples representing the function space, effectively equipping us with a very power statistical tool 
<label for="ssl-0">⊕</label>

<span>
Self-supervised learning don't even need explicit outputs, how cool is that..
See <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/">https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/</a> this work for more information.
</span>
to introduce a new independent signal to reduce the entropy of the problem in many domains. I am a fan of smaller personalized models' potential to tackle everyday problems, and myself uses cheap off-the-self cameras coupled with a DL model to detect those Damn Monkeys, and for local voice-synthesis.
<label for="monkey-trivia">⊕</label>

<span>
  Monkey Capturing was even on the manifesto of one of the candidates at city-level elections!
</span>
In country like India, where even (traditional) Automation is limited to products of very few big companies, I can't help smiling whenever i point remote at my "AI" controlled AC :) </p>
<p>Living in a two-tier town in northern India with very minimal fixed-costs has allowed me to work on this for quite a long time without any savings or continuous financial freedom. But i cannot be a hypocrite about it, as it was a conscious decision to learn, explore and implement some of the ideas i had for some time. In return, this has allowed me to stay in touch with friends, played a lot of outdoor games, and help me in reflecting on the things i would want to spend more time in future.</p>
<p>Timely financial grants during the last one and half year from <a href="https://samagata.org/">Samagata foundation</a> and <a href="https://fossunited.org/">FossUnited</a> has allowed me to complete a bulk of work to point, where i am satisfied with the current state of the project, for which i will always be grateful.</p>
<p>I would very much like to continue on this or adjacent projects, as there are still a lot of ideas and code pending, to make it a very stable everyday engine for users to use . But for that i will have to figure out a way to sustain this , without ever compromising the Core features/functionality in any way, As those were some of reasons i started working on it in the first place! Extensions to allow indexing remote storage like Google Drive or Android devices smoothly from the app itself seems like a good direction in that regard for now!</p>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types (175 pts)]]></title>
            <link>https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</link>
            <guid>46087022</guid>
            <pubDate>Sat, 29 Nov 2025 12:25:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/">https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</a>, See on <a href="https://news.ycombinator.com/item?id=46087022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>   <p>Back around 2014, I kept hearing about this cool database called <a href="https://riak.com/">Riak</a>, a distributed database that could survive network partitions and keep accepting writes. Some really interesting companies were using it at massive scale, and I was curious about it. One of the big selling points was that it could handle concurrent writes without any coordination or consensus. I was intrigued, and I started reading about it. Underlying all of this was the concept of CRDTs, Conflict-free Replicated Data Types.<sup><a href="#user-content-fn-crdt-origin" id="user-content-fnref-crdt-origin" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<p>At the time, I was working on a beer startup called Brewtown with a friend: a beer review social site and delivery subscription service. It failed for other reasons, but I was a little too enamored with shiny tech back then, and CRDTs and Riak fit the bill for shiny tech. I kept trying to find excuses to shoehorn CRDT stuff into our codebase when, honestly, we didn’t need any of it. Postgres would’ve been fine. Live and learn.</p>
<p>Anyways, the idea sounded like pure sorcery: data structures that replicate across nodes and merge deterministically, without coordination, without losing information. I got excited, read a few papers, played with some toy implementations… and then we gave up on the beer startup. I didn’t really have a reason to mess with CRDTs for a while.</p>
<p>Fast forward to 2025, I’ve just had Thanksgiving dinner, and I’m curious again. What’s the state of the art? What have I forgotten? Which CRDT should I reach for when? So I’m writing this, both as a refresher for myself and a reference for the next time I need to remember why OR-Sets exist or what WOOT stands for. (“WithOut Operational Transformation.” Yes, really.<sup><a href="#user-content-fn-woot-paper" id="user-content-fnref-woot-paper" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>)</p>
<p>So, grab a coffee.</p>
<p>Commutative. Replicated. Data Types.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/g92-vC9iTn8?si=egtFeVe21g8SVrOm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p>In isolation, all of the words make sense. But when you look at the literature, it’s overwhelming:</p>
<p>Suddenly you’ve moved beyond the simple terms and start seeing things like G-Counters, PN-Counters, LWW-Sets, OR-Sets, 2P-Sets, RGAs, WOOTs, Logoots (wtf?)… Each with subtle tradeoffs. Each paper assuming you’ve read the previous five. It’s overwhelming.</p>
<p>This guide will hopefully cut through that. We’ll build intuition through <strong>interactive demos</strong> and concrete examples. You’ll see how merges actually work, watch conflicts resolve (or not resolve), and develop a feel for which CRDT fits which problem.</p>
<h2 id="what-you-need-to-know">What You Need to Know</h2>
<p>You don’t need a PhD in distributed systems. If you understand:</p>
<ul>
<li>Why network failures happen</li>
<li>What “eventual consistency” means</li>
<li>Basic set operations (union, intersection)</li>
</ul>
<p>…you’re good.</p>
<h2 id="the-problem">The Problem</h2>
<p>Picture this: Alice and Bob are both editing a shared counter. Alice increments it. Bob increments it. The network is flaky, so neither sees the other’s change immediately. Later, they reconnect. What should the counter show?</p>
<p><strong>Option 1: Consensus</strong> - Use Paxos/Raft to agree on who went first. Works great! Until the network partitions and half your users can’t write because they can’t reach a quorum. Not ideal for offline-first apps.</p>
<p><strong>Option 2: Last-Write-Wins</strong> - Use timestamps. Whoever wrote last “wins.” Easy to implement! Except Bob’s increment gets completely erased if Alice’s timestamp was later. Data loss.</p>
<p><strong>Option 3: CRDTs</strong> - Design the data structure so that merging is deterministic. Both increments survive. No coordination needed. No data loss. However, you have to be okay with some level of eventual consistency.</p>
<p>What’s the trick? How do CRDTs achieve this?</p>
<p>Roughly speaking, you are working with a CRDT if your merge operation is:</p>
<ul>
<li>commutative (order doesn’t matter)</li>
<li>associative (grouping doesn’t matter)</li>
<li>idempotent (duplicates don’t matter)</li>
</ul>
<p>Once you achieve these properties, then you can use your merge operation to ensure that replicas automatically converge to the same state.</p>
<h3 id="a-quick-detour-lattices-and-why-they-matter">A Quick Detour: Lattices and Why They Matter</h3>
<p>Before we dive into specific CRDTs, let’s build some intuition about what makes merging work. In CRDT literature, this is often referred to as a “lattice”.</p>
<p>Think about natural numbers with <code>max</code> as the merge operation. If you have <code>3</code> and <code>5</code>, taking <code>max(3, 5) = 5</code> makes sense. It doesn’t matter if you compute <code>max(3, max(5, 7))</code> or <code>max(max(3, 5), 7)</code> - you get <code>7</code> either way. And <code>max(5, 5) = 5</code>, so duplicates are harmless.</p>
<p>This forms a <strong>partial order</strong>: some values are “greater than” others (<code>5 &gt; 3</code>), and there’s a <strong>join</strong> operation (<code>max</code>) that gives you the least upper bound. The fancy math term is “join-semilattice,” but think of it as: <strong>a way to consistently pick “more recent” or “more complete” information</strong>.</p>
<p>Here’s the key insight: if your data structure’s states form a lattice, and updates only move “upward” in the ordering, then:</p>
<ul>
<li>You can apply updates in any order</li>
<li>You can apply the same update twice</li>
<li>Eventually, everyone agrees on the maximum state</li>
</ul>
<p>Consider a counter where each replica tracks its own count: <code>{A: 3, B: 5}</code>. The partial order is <strong>pointwise</strong>: <code>{A: 3, B: 5} ≥ {A: 2, B: 5}</code> because each component is greater-or-equal. To join, take the <code>max</code> of each component. This is exactly how the G-Counter CRDT works!</p>
<p><strong>Why does this matter?</strong> Because if you can design your data structure so that:</p>
<ol>
<li>States form a lattice (there’s always a sensible “join”)</li>
<li>Operations only move upward (you can’t un-increment a counter)</li>
</ol>
<p>Then merging becomes trivial: just take the join. No coordination needed. No conflicts possible. The math guarantees convergence.</p>
<p>Not all CRDTs fit this clean model (some need timestamps or version vectors to determine what’s “greater”), but the lattice intuition often guides the design. When you see <code>merge = unionWith max</code> or <code>merge = union</code>, you’re seeing some pure, beautiful math-brained lattice thinking.</p>
<h3 id="state-based-vs-operation-based">State-Based vs Operation-Based</h3>
<p>Moving on…</p>
<p>There are two fundamental approaches to CRDTs:</p>
<p><strong>State-based CRDTs (CvRDTs)</strong> send the entire state to other replicas, which merge it with their local state using a join operation. The state must form a join-semilattice.<sup><a href="#user-content-fn-cvcrdt" id="user-content-fnref-cvcrdt" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></p>
<p><strong>Operation-based CRDTs (CmRDTs)</strong> send operations to other replicas, which apply them to their local state. Operations must be commutative when applied concurrently.<sup><a href="#user-content-fn-cmcrdt" id="user-content-fnref-cmcrdt" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<p>In this guide, we’ll primarily discuss state-based CRDTs, as they’re conceptually simpler and the ideas translate naturally to the operation-based variants.</p>
<h3 id="the-core-laws">The Core Laws</h3>
<p>For a data structure to be a state-based CRDT, its merge operation must satisfy:</p>
<p><strong>Associativity</strong>: <code>(a ⊔ b) ⊔ c = a ⊔ (b ⊔ c)</code> where <code>⊔</code> denotes the merge/join operation</p>
<p><strong>Commutativity</strong>: <code>a ⊔ b = b ⊔ a</code></p>
<p><strong>Idempotence</strong>: <code>a ⊔ a = a</code></p>
<p>These properties ensure that:</p>
<ul>
<li>Merging in any order produces the same result</li>
<li>Re-receiving the same state is harmless</li>
<li>Partial merges can be composed</li>
</ul>
<p>Additionally, the state must form a <strong>monotonic semilattice</strong>: updates only move “upward” in the partial order, never downward. This ensures convergence: once all updates have been delivered, all replicas reach the same state.</p>
<p>For the curious, The symbol ⊔ is called (square cup) or square union. I have no idea why regular union symbol isn’t used. Pointy-headed researchers, I guess.</p>
<p>Anyways, it’s commonly used to denote:</p>
<ul>
<li>Disjoint union - union of sets treated as disjoint</li>
<li>Join operation in lattice theory - the least upper bound (supremum) of two elements</li>
<li>Merge operation in CRDTs - combining two states by taking their least upper bound</li>
</ul>
<p>With these foundations in place, let’s explore the CRDT zoo.</p>
<h2 id="g-counter-grow-only-counter">G-Counter: Grow-Only Counter</h2>
<p>Let’s start with the simplest CRDT: a counter that only goes up.<sup><a href="#user-content-fn-gcounter-origin" id="user-content-fnref-gcounter-origin" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup></p>
<h3 id="the-idea">The Idea</h3>
<p>Instead of storing one global count, each replica tracks its own count. The total is the sum of all replica counts. When replicas merge, they take the <code>max</code> of each replica’s count.</p>
<p>Why <code>max</code>? Because counts only increase. If replica A shows that replica B has counted to 5, and replica B shows it’s counted to 3, we know A has seen newer information. Taking the max ensures we never lose increments.<sup><a href="#user-content-fn-gcounter-space" id="user-content-fnref-gcounter-space" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup></p>
<h3 id="implementation">Implementation</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> GCounter</span><span> =</span><span> Map</span><span> ReplicaId</span><span> Nat</span></span>
<span></span>
<span><span>value</span><span> ::</span><span> GCounter</span><span> -&gt;</span><span> Nat</span></span>
<span><span>value counter </span><span>=</span><span> sum (Map.elems counter)</span></span>
<span></span>
<span><span>increment</span><span> ::</span><span> ReplicaId</span><span> -&gt;</span><span> GCounter</span><span> -&gt;</span><span> GCounter</span></span>
<span><span>increment r counter </span><span>=</span><span> Map.insertWith (</span><span>+</span><span>) r </span><span>1</span><span> counter</span></span>
<span></span>
<span><span>merge</span><span> ::</span><span> GCounter</span><span> -&gt;</span><span> GCounter</span><span> -&gt;</span><span> GCounter</span></span>
<span><span>merge </span><span>=</span><span> Map.unionWith max</span></span></code></pre>
<h3 id="laws-and-invariants">Laws and Invariants</h3>
<p>The merge operation forms a join-semilattice where the partial order is defined pointwise: <code>c1 ≤ c2</code> if for all replicas <code>r</code>, <code>c1[r] ≤ c2[r]</code>.</p>
<ul>
<li><strong>Associative</strong>: <code>max</code> is associative</li>
<li><strong>Commutative</strong>: <code>max</code> is commutative</li>
<li><strong>Idempotent</strong>: <code>max(x, x) = x</code></li>
<li><strong>Monotonic</strong>: Each replica’s count only increases</li>
</ul>
<h3 id="intuition">Intuition</h3>
<p>Think of each replica as having its own tally marks. When replicas sync, they each adopt the maximum tally for each replica they’ve seen. Since tallies only grow, taking the maximum ensures we never lose increments.</p>
<h3 id="tradeoffs">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and efficient</li>
<li>No metadata overhead beyond replica counts</li>
<li>Perfect for increment-only scenarios (page views, likes, etc.)</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot decrement</li>
<li>Size grows with number of replicas (though typically small)</li>
<li>No garbage collection (all replica counts retained forever)</li>
</ul>
<h3 id="when-to-use">When to Use</h3>
<p>Use G-Counter when you need to count upward-only events in a distributed system: analytics counters, like counts, view counts, or any monotonically increasing metric. (If you need to decrement, well… keep reading.)</p>
<h3 id="interactive-demo">Interactive Demo</h3>
<p>Try it yourself! Increment counters on different replicas and see how the merge operation works:</p>
<astro-island uid="Z1h8Xr8" prefix="r0" component-url="/_astro/GCounterDemo.BSvnoCcc.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;GCounterDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="pn-counter-positive-negative-counter">PN-Counter: Positive-Negative Counter</h2>
<p>What if we need to decrement? Enter the PN-Counter. The trick is beautifully simple.</p>
<h3 id="definition">Definition</h3>
<p>A PN-Counter contains two G-Counters: one for increments, one for decrements:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> PNCounter</span><span> =</span><span> PNCounter</span></span>
<span><span>  { increments </span><span>::</span><span> GCounter</span></span>
<span><span>  , decrements </span><span>::</span><span> GCounter</span></span>
<span><span>  }</span></span></code></pre>
<p>The value is the difference:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>value :: PNCounter -&gt; Int</span></span>
<span><span>value (PNCounter inc dec) = value inc - value dec</span></span></code></pre>
<p>What I love about PN-Counters as a broader insight for CRDTs is that you can often build more complex CRDTs by combining simpler ones.</p>
<h3 id="operations">Operations</h3>
<p><strong>Increment</strong> (on replica <code>r</code>):</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>increment r (PNCounter inc dec) = PNCounter (increment r inc) dec</span></span></code></pre>
<p><strong>Decrement</strong> (on replica <code>r</code>):</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>decrement r (PNCounter inc dec) = PNCounter inc (increment r dec)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>merge (PNCounter i1 d1) (PNCounter i2 d2) =</span></span>
<span><span>  PNCounter (merge i1 i2) (merge d1 d2)</span></span></code></pre>
<h3 id="laws-and-invariants-1">Laws and Invariants</h3>
<p>Since both components are G-Counters with valid merge operations, the PN-Counter’s merge inherits their properties and forms a semilattice.</p>
<h3 id="intuition-1">Intuition</h3>
<p>A PN-Counter is like having two separate tally sheets: one for additions, one for subtractions. The current value is the difference between them. When replicas sync, they merge both sheets independently.</p>
<h3 id="tradeoffs-1">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports both increment and decrement</li>
<li>Deterministic convergence</li>
<li>Simple to understand and implement</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Double the space of a G-Counter</li>
<li>Can never truly garbage collect old replica entries</li>
<li>No bound on the value range (can overflow)</li>
<li>Cannot reset the counter atomically</li>
</ul>
<h3 id="when-to-use-1">When to Use</h3>
<p>Use PN-Counter for any metric that can increase or decrease over time: inventory counts, resource pools, etc.</p>
<h3 id="variants">Variants</h3>
<p>Some implementations use a single map with integer values instead of two separate maps, but the principle is the same.</p>
<h2 id="g-set-grow-only-set">G-Set: Grow-Only Set</h2>
<p>Moving from numbers to collections, we consider the simplest CRDT set.</p>
<h3 id="definition-1">Definition</h3>
<p>A G-Set is simply a set that supports addition but not removal:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> GSet</span><span> a </span><span>=</span><span> Set</span><span> a</span></span></code></pre>
<h3 id="operations-1">Operations</h3>
<p><strong>Add</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a</span></span>
<span><span>add </span><span>=</span><span> insert</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> GSet</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a </span><span>-&gt;</span><span> GSet</span><span> a</span></span>
<span><span>merge </span><span>=</span><span> union</span></span></code></pre>
<h3 id="laws-and-invariants-2">Laws and Invariants</h3>
<p>Sets with union form a semilattice under the subset relation.</p>
<ul>
<li><strong>Associative</strong>: Set union is associative</li>
<li><strong>Commutative</strong>: Set union is commutative</li>
<li><strong>Idempotent</strong>: <code>A ∪ A = A</code></li>
<li><strong>Monotonic</strong>: Sets only grow</li>
</ul>
<h3 id="intuition-2">Intuition</h3>
<p>Once an element is added to any replica, it will eventually appear in all replicas. There’s no way to remove it.</p>
<h3 id="tradeoffs-2">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Minimal overhead (just the set elements)</li>
<li>Simple and efficient</li>
<li>Familiar set semantics</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot remove elements</li>
<li>Grows unbounded</li>
<li>No garbage collection</li>
</ul>
<h3 id="when-to-use-2">When to Use</h3>
<p>Use G-Set for append-only collections where removal is never needed: event logs, collected tags, or immutable registries.</p>
<h2 id="2p-set-two-phase-set">2P-Set: Two-Phase Set</h2>
<p>The natural extension of G-Set to support removal.</p>
<h3 id="definition-2">Definition</h3>
<p>A 2P-Set (Two-Phase Set) contains two G-Sets: one for added elements, one for removed elements:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> TwoPhaseSet</span><span> a </span><span>=</span><span> TwoPhaseSet</span></span>
<span><span>  { added </span><span>::</span><span> GSet</span><span> a</span></span>
<span><span>  , removed </span><span>::</span><span> GSet</span><span> a</span></span>
<span><span>  }</span></span></code></pre>
<p>An element is in the set if it’s been added but not removed:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>member</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> TwoPhaseSet</span><span> a </span><span>-&gt;</span><span> Bool</span></span>
<span><span>member x (TwoPhaseSet a r) </span><span>=</span><span> x </span><span>`Set.member`</span><span> a </span><span>&amp;&amp;</span><span> x </span><span>`Set.notMember`</span><span> r</span></span></code></pre>
<h3 id="operations-2">Operations</h3>
<p><strong>Add</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add x (TwoPhaseSet a r) </span><span>=</span><span> TwoPhaseSet (insert x a) r</span></span></code></pre>
<p><strong>Remove</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>remove x (TwoPhaseSet a r) </span><span>=</span><span> TwoPhaseSet a (insert x r)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge (TwoPhaseSet a1 r1) (TwoPhaseSet a2 r2) </span><span>=</span></span>
<span><span>  TwoPhaseSet (union a1 a2) (union r1 r2)</span></span></code></pre>
<h3 id="laws-and-invariants-3">Laws and Invariants</h3>
<p><strong>Bias toward removal</strong>: If an element appears in the removed set, it’s not in the 2P-Set, even if it’s also in the added set.</p>
<p><strong>Once removed, forever removed</strong>: Once an element is removed at any replica, it will eventually be removed from all replicas and cannot be re-added.</p>
<h3 id="intuition-3">Intuition</h3>
<p>The 2P-Set is like marking items in a ledger: you can add entries and you can cross them out, but you can’t un-cross-out an entry. Once something is crossed out (removed), that decision is permanent.</p>
<h3 id="tradeoffs-3">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports both add and remove</li>
<li>Simple to understand</li>
<li>Deterministic convergence</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Cannot re-add removed elements (the “2P” means two-phase: add, then remove, no going back)</li>
<li>Both sets grow monotonically (removed items never truly disappear)</li>
<li>No garbage collection</li>
<li>Not suitable for scenarios where elements might be removed and re-added</li>
</ul>
<h3 id="when-to-use-3">When to Use</h3>
<p>Use 2P-Set when elements have a lifecycle of “not present → added → removed” and never need to be re-added: task completion tracking, tombstones, or revoked permissions.</p>
<h2 id="lww-element-set-last-write-wins-element-set">LWW-Element-Set: Last-Write-Wins Element Set</h2>
<p>What if we want to re-add elements? We need timestamps.</p>
<h3 id="definition-3">Definition</h3>
<p>An LWW-Element-Set associates each element with a timestamp for additions and removals:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> LWWSet</span><span> a </span><span>=</span><span> LWWSet</span></span>
<span><span>  { addTimes </span><span>::</span><span> Map</span><span> a </span><span>Timestamp</span></span>
<span><span>  , removeTimes </span><span>::</span><span> Map</span><span> a </span><span>Timestamp</span></span>
<span><span>  }</span></span></code></pre>
<p>An element is in the set if its most recent operation was an add:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>member</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> LWWSet</span><span> a </span><span>-&gt;</span><span> Bool</span></span>
<span><span>member x (LWWSet adds removes) </span><span>=</span></span>
<span><span>  case</span><span> (Map.lookup x adds</span><span>,</span><span> Map.lookup x removes) </span><span>of</span></span>
<span><span>    (Just t1</span><span>,</span><span> Just t2) </span><span>-&gt;</span><span> t1 </span><span>&gt;</span><span> t2</span></span>
<span><span>    (Just _</span><span>,</span><span> Nothing) </span><span>-&gt;</span><span> True</span></span>
<span><span>    _ </span><span>-&gt;</span><span> False</span></span></code></pre>
<h3 id="operations-3">Operations</h3>
<p><strong>Add</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add x t (LWWSet adds removes) </span><span>=</span><span> LWWSet (insert x t adds) removes</span></span></code></pre>
<p><strong>Remove</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>remove x t (LWWSet adds removes) </span><span>=</span><span> LWWSet adds (insert x t removes)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge (LWWSet a1 r1) (LWWSet a2 r2) </span><span>=</span></span>
<span><span>  LWWSet (unionWith max a1 a2) (unionWith max r1 r2)</span></span></code></pre>
<h3 id="laws-and-invariants-4">Laws and Invariants</h3>
<p>The merge operation is well-defined because <code>max</code> over timestamps forms a semilattice.</p>
<p><strong>Timestamp monotonicity</strong>: Each replica must generate increasing timestamps (typically using wall clocks plus replica IDs as tiebreakers).</p>
<p><strong>Bias</strong>: We must decide what happens when add and remove timestamps are equal. Common choices: bias toward add, or bias toward remove.</p>
<h3 id="intuition-4">Intuition</h3>
<p>Each element has a timestamp for when it was last added and when it was last removed. The most recent operation wins. When merging, we take the latest add timestamp and latest remove timestamp we’ve seen.</p>
<h3 id="tradeoffs-4">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports add, remove, and re-add</li>
<li>Can garbage collect old timestamps (carefully)</li>
<li>Natural semantics for many applications</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Requires synchronized clocks (or logical clocks with careful replica ID handling)</li>
<li>Concurrent add/remove on the same element may surprise users (one operation is discarded)</li>
<li>Loses information: if two users concurrently add the same element, only one timestamp survives</li>
<li>The “last write wins” semantics mean data loss is possible</li>
</ul>
<h3 id="when-to-use-4">When to Use</h3>
<p>Use LWW-Element-Set when you need a set with add/remove/re-add capability and can tolerate last-write-wins semantics: user preferences, feature flags, or cached collections where perfect consistency isn’t critical.</p>
<h3 id="clock-considerations">Clock Considerations</h3>
<p>The biggest pitfall of LWW-Element-Set is clock skew. If replica A’s clock is ahead of replica B’s, then A’s operations will always “win” over B’s, even if B’s operations happened later in real time. Solutions include:</p>
<ul>
<li>Use hybrid logical clocks (HLC) instead of wall clocks</li>
<li>Use replica IDs as tiebreakers (e.g., timestamps are <code>(wall_time, replica_id)</code> pairs)</li>
<li>Accept the inconsistency as a tradeoff</li>
</ul>
<h2 id="or-set-observed-remove-set">OR-Set: Observed-Remove Set</h2>
<p>The most sophisticated set CRDT, solving the re-add problem without LWW semantics.<sup><a href="#user-content-fn-orset-origin" id="user-content-fnref-orset-origin" data-footnote-ref="" aria-describedby="footnote-label">7</a></sup></p>
<h3 id="definition-4">Definition</h3>
<p>An OR-Set (Observed-Remove Set) associates each element with a set of unique tags:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ORSet</span><span> a </span><span>=</span><span> Map</span><span> a (</span><span>Set</span><span> Tag</span><span>)</span></span></code></pre>
<p>Tags are unique identifiers generated when adding an element (e.g., <code>(replica_id, sequence_number)</code> pairs).</p>
<p>An element is in the set if it has any tags:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>member</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> ORSet</span><span> a </span><span>-&gt;</span><span> Bool</span></span>
<span><span>member x set </span><span>=</span><span> case</span><span> Map.lookup x set </span><span>of</span></span>
<span><span>  Just tags </span><span>-&gt;</span><span> not (Set.null tags)</span></span>
<span><span>  Nothing </span><span>-&gt;</span><span> False</span></span></code></pre>
<h3 id="operations-4">Operations</h3>
<p><strong>Add</strong> (with fresh tag <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>add x t set </span><span>=</span><span> Map.insertWith union x (singleton t) set</span></span></code></pre>
<p><strong>Remove</strong> (with observed tags <code>ts</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>remove x ts set </span><span>=</span><span> Map.update (</span><span>\\</span><span>tags </span><span>-&gt;</span></span>
<span><span>  let</span><span> remaining </span><span>=</span><span> tags </span><span>\\</span><span> ts</span></span>
<span><span>  in</span><span> if</span><span> Set.null remaining </span><span>then</span><span> Nothing </span><span>else</span><span> Just remaining) x set</span></span></code></pre>
<p>The critical insight: removal removes only the tags that were observed. If concurrent adds create new tags, those survive.</p>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge </span><span>=</span><span> Map.unionWith union</span></span></code></pre>
<h3 id="laws-and-invariants-5">Laws and Invariants</h3>
<p>The merge operation forms a semilattice where <code>s1 ≤ s2</code> if for all elements <code>x</code>, <code>s1[x] ⊆ s2[x]</code>.</p>
<p><strong>Add wins</strong>: If an add and remove happen concurrently (the add’s tag wasn’t observed by the remove), the add wins.</p>
<p><strong>Causal consistency</strong>: You can only remove tags you’ve observed (seen in a prior state).</p>
<h3 id="intuition-5">Intuition</h3>
<p>Think of each addition as dropping a unique token into a bucket for that element. Removal takes specific tokens out of the bucket. If someone concurrently added a new token you haven’t seen, your removal doesn’t affect it. An element is present if its bucket has any tokens.</p>
<p>This gives us <strong>add-wins semantics</strong>: concurrent add and remove means the element stays in the set (because the remove didn’t observe the add’s tag).</p>
<h3 id="tradeoffs-5">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports add, remove, and re-add with intuitive semantics</li>
<li>No timestamp requirements</li>
<li>Add-wins semantics are often more desirable than LWW</li>
<li>Properly handles concurrent operations</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Larger space overhead (tags per element)</li>
<li>More complex implementation</li>
<li>Need garbage collection strategy for tags</li>
<li>Remove operations need to carry the observed tags (larger messages)</li>
</ul>
<h3 id="when-to-use-5">When to Use</h3>
<p>Use OR-Set when you need a set with full add/remove/re-add support and can’t tolerate LWW’s data loss: collaborative editing, shopping carts, or any scenario where concurrent adds should be preserved.</p>
<h3 id="garbage-collection">Garbage Collection</h3>
<p>Old tags can accumulate. Strategies include:</p>
<ul>
<li><strong>Tombstones</strong>: Keep removed tags for a grace period before discarding</li>
<li><strong>Version vectors</strong>: Use causal history to determine which tags are safe to remove</li>
<li><strong>Bounded tags</strong>: Limit the number of tags per element, using LWW within that bound</li>
</ul>
<h3 id="interactive-demo-1">Interactive Demo</h3>
<p>Experience the add-wins semantics of OR-Set:</p>
<astro-island uid="1UCY2P" prefix="r1" component-url="/_astro/ORSetDemo.DVgELLPP.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;ORSetDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="lww-register-last-write-wins-register">LWW-Register: Last-Write-Wins Register</h2>
<p>Registers store single values. The simplest register CRDT uses last-write-wins.</p>
<h3 id="definition-5">Definition</h3>
<p>An LWW-Register pairs a value with a timestamp:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> LWWRegister</span><span> a </span><span>=</span><span> LWWRegister</span></span>
<span><span>  { value </span><span>::</span><span> a</span></span>
<span><span>  , timestamp </span><span>::</span><span> Timestamp</span></span>
<span><span>  }</span></span></code></pre>
<h3 id="operations-5">Operations</h3>
<p><strong>Write</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>write x t _ </span><span>=</span><span> LWWRegister x t</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge r1</span><span>@</span><span>(LWWRegister v1 t1) r2</span><span>@</span><span>(LWWRegister v2 t2)</span></span>
<span><span>  |</span><span> t1 </span><span>&gt;</span><span> t2 </span><span>=</span><span> r1</span></span>
<span><span>  |</span><span> t1 </span><span>&lt;</span><span> t2 </span><span>=</span><span> r2</span></span>
<span><span>  |</span><span> otherwise </span><span>=</span><span> r1  </span><span>-- tiebreaker (could use replica ID)</span></span></code></pre>
<h3 id="laws-and-invariants-6">Laws and Invariants</h3>
<p>The merge operation is a semilattice with partial order defined by timestamps.</p>
<p><strong>One value wins</strong>: When concurrent writes occur, only one survives (the one with the higher timestamp).</p>
<h3 id="tradeoffs-6">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and efficient</li>
<li>Small size (just value + timestamp)</li>
<li>Easy to understand</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Loses concurrent updates</li>
<li>Requires clock synchronization</li>
<li>No way to detect or recover lost updates</li>
</ul>
<h3 id="when-to-use-6">When to Use</h3>
<p>Use LWW-Register for single-value cells where you can tolerate lost updates: user profile fields, configuration settings, or cached computed values.</p>
<h3 id="interactive-demo-2">Interactive Demo</h3>
<p>See data loss in action with last-write-wins semantics:</p>
<astro-island uid="2g6pnS" prefix="r2" component-url="/_astro/LWWRegisterDemo.xg1zuJ1y.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;LWWRegisterDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="mv-register-multi-value-register">MV-Register: Multi-Value Register</h2>
<p>What if we want to preserve concurrent writes instead of discarding them?</p>
<h3 id="definition-6">Definition</h3>
<p>An MV-Register stores a set of value-timestamp pairs:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> MVRegister</span><span> a </span><span>=</span><span> Set</span><span> (a</span><span>,</span><span> Timestamp</span><span>)</span></span></code></pre>
<p>When reading, you get back all concurrently written values (values with incomparable timestamps).</p>
<h3 id="operations-6">Operations</h3>
<p><strong>Write</strong> (with timestamp <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>write x t reg </span><span>=</span><span> Set.singleton (x</span><span>,</span><span> t)</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge reg1 reg2 </span><span>=</span></span>
<span><span>  let</span><span> combined </span><span>=</span><span> union reg1 reg2</span></span>
<span><span>      maxTime </span><span>=</span><span> maximum (map snd combined)</span></span>
<span><span>      concurrent </span><span>=</span><span> filter (</span><span>\\</span><span>(_</span><span>,</span><span> t) </span><span>-&gt;</span><span> t </span><span>==</span><span> maxTime) combined</span></span>
<span><span>  in</span><span> fromList concurrent</span></span></code></pre>
<p>More sophisticated: keep values with causally concurrent timestamps, not just the maximum.</p>
<h3 id="laws-and-invariants-7">Laws and Invariants</h3>
<p>The merge preserves all values that might be “current” from different replicas’ perspectives.</p>
<p><strong>Concurrent values preserved</strong>: If two writes happened concurrently, both values appear until a subsequent write supersedes them.</p>
<h3 id="tradeoffs-7">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No data loss on concurrent updates</li>
<li>Application can detect and resolve conflicts</li>
<li>More information available for conflict resolution</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Returns sets of values, not single values</li>
<li>Application must handle conflict resolution</li>
<li>More complex semantics</li>
<li>Slightly larger space overhead</li>
</ul>
<h3 id="when-to-use-7">When to Use</h3>
<p>Use MV-Register when concurrent updates must be detected and resolved by application logic: collaborative text fields, conflict-aware configuration, or any scenario where losing an update is unacceptable.</p>
<h3 id="conflict-resolution">Conflict Resolution</h3>
<p>When reading an MV-Register returns multiple values, the application must resolve the conflict. Strategies include:</p>
<ul>
<li>Present all values to the user (collaborative editing)</li>
<li>Apply a deterministic merge function (e.g., union of tags)</li>
<li>Use application-specific semantics (e.g., prefer non-empty values)</li>
</ul>
<h2 id="or-map-observed-remove-map">OR-Map: Observed-Remove Map</h2>
<p>Maps are common. How do we make them CRDTs?</p>
<h3 id="definition-7">Definition</h3>
<p>An OR-Map is a map where each key is associated with an OR-Set of tagged values:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ORMap</span><span> k v </span><span>=</span><span> Map</span><span> k (</span><span>ORSet</span><span> (v</span><span>,</span><span> Tag</span><span>))</span></span></code></pre>
<p>Alternatively, implement as a composition of OR-Set (for keys) with per-key CRDTs (for values).</p>
<h3 id="operations-7">Operations</h3>
<p><strong>Put</strong> (with fresh tag <code>t</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>put k v t map </span><span>=</span><span> Map.insertWith union k (singleton (v</span><span>,</span><span> t)) map</span></span></code></pre>
<p><strong>Remove key</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>removeKey k map </span><span>=</span><span> Map.delete k map</span></span></code></pre>
<p><strong>Remove specific value</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>removeValue k v tags map </span><span>=</span><span> -- similar to OR-Set remove</span></span></code></pre>
<p><strong>Merge</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>merge </span><span>=</span><span> Map.unionWith (OR</span><span>-</span><span>Set merge)</span></span></code></pre>
<h3 id="tradeoffs-8">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Full map operations with CRDT semantics</li>
<li>Can nest other CRDTs as values</li>
<li>Compositional</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Complex metadata management</li>
<li>Garbage collection challenges</li>
<li>Larger overhead</li>
</ul>
<h3 id="when-to-use-8">When to Use</h3>
<p>Use OR-Map when you need a distributed key-value store with CRDT guarantees: collaborative JSON documents, distributed configuration, or nested data structures.</p>
<h2 id="rga-replicated-growable-array">RGA: Replicated Growable Array</h2>
<p>Sequences are hard. How do you handle insertions in the middle when replicas disagree on positions?<sup><a href="#user-content-fn-sequence-crdt-challenge" id="user-content-fnref-sequence-crdt-challenge" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup></p>
<h3 id="definition-8">Definition</h3>
<p>RGA (Replicated Growable Array) assigns each element a unique ID and stores the sequence as a tree structure based on insertion order and causality.<sup><a href="#user-content-fn-rga-paper" id="user-content-fnref-rga-paper" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup></p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> RGA</span><span> a </span><span>=</span><span> RGA</span></span>
<span><span>  { elements </span><span>::</span><span> Map</span><span> UID</span><span> (a</span><span>,</span><span> UID</span><span>)  </span><span>-- element ID -&gt; (value, parent ID)</span></span>
<span><span>  , root </span><span>::</span><span> UID</span></span>
<span><span>  }</span></span></code></pre>
<p>Each element knows its “parent” (the element after which it was inserted).</p>
<h3 id="operations-8">Operations</h3>
<p><strong>Insert</strong> (after element with ID <code>p</code>, with fresh ID <code>uid</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>insert p x uid rga </span><span>=</span><span> -- complex tree manipulation</span></span></code></pre>
<p><strong>Delete</strong> (element with ID <code>uid</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>delete uid rga </span><span>=</span><span> -- mark as tombstone, don't actually remove</span></span></code></pre>
<p><strong>Merge</strong>: Merge trees by reconciling insertion orders.</p>
<h3 id="laws-and-invariants-8">Laws and Invariants</h3>
<p>The challenge is that positional indices change as elements are inserted/removed. RGA solves this by using immutable IDs and causal relationships.</p>
<p><strong>Causal order preserved</strong>: If element A was inserted before element B on the same replica, that relationship is preserved globally.</p>
<h3 id="intuition-6">Intuition</h3>
<p>Instead of “insert at position 5,” you say “insert after element X.” Since X has a unique ID, this instruction is unambiguous even when other replicas are concurrently inserting elsewhere.</p>
<h3 id="tradeoffs-9">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Supports arbitrary insertions and deletions</li>
<li>Eventual consistency for sequences</li>
<li>Handles concurrent edits intuitively</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Complex implementation</li>
<li>Large overhead (IDs, tombstones)</li>
<li>No compaction without coordination</li>
<li>Performance degrades with many deletes (tombstones accumulate)</li>
</ul>
<h3 id="when-to-use-9">When to Use</h3>
<p>Use RGA for collaborative text editing or any replicated sequence where insertions at arbitrary positions must be supported: shared lists, collaborative documents, or distributed queues.</p>
<h3 id="alternatives">Alternatives</h3>
<p>Other sequence CRDTs include:</p>
<ul>
<li><strong>WOOT</strong> (Without Operational Transformation): similar idea, different structure</li>
<li><strong>Logoot</strong>: uses position identifiers between elements</li>
<li><strong>LSEQ</strong>: adaptive allocation of position identifiers</li>
<li><strong>YATA</strong>: optimizations for text editing workloads<sup><a href="#user-content-fn-yata-yjs" id="user-content-fnref-yata-yjs" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup></li>
</ul>
<p>Each has different tradeoffs in space overhead, time complexity, and behavior under specific edit patterns.</p>
<h2 id="causal-crdts-adding-causality">Causal CRDTs: Adding Causality</h2>
<p>Advanced CRDTs incorporate causal tracking using version vectors or similar mechanisms. This enables more sophisticated semantics.</p>
<h3 id="version-vectors">Version Vectors</h3>
<p>A version vector tracks the logical clock for each replica:<sup><a href="#user-content-fn-version-vectors" id="user-content-fnref-version-vectors" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup></p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> VersionVector</span><span> =</span><span> Map</span><span> ReplicaId</span><span> Nat</span></span></code></pre>
<p>Operations include the version vector, allowing replicas to determine causality: whether one operation happened-before another, or whether they were concurrent.</p>
<h3 id="causal-register">Causal Register</h3>
<p>Pairs an MV-Register with version vectors:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> CausalRegister</span><span> a </span><span>=</span><span> CausalRegister</span></span>
<span><span>  { values </span><span>::</span><span> Map</span><span> VersionVector</span><span> a</span></span>
<span><span>  }</span></span></code></pre>
<p>Only keeps values with concurrent version vectors, discarding those that are causally dominated.</p>
<h3 id="advantages-of-causality">Advantages of Causality</h3>
<ul>
<li>More precise conflict detection (concurrent vs. causally ordered)</li>
<li>Better garbage collection (can discard superseded operations)</li>
<li>Foundation for stronger consistency guarantees</li>
</ul>
<h3 id="disadvantages-of-causality">Disadvantages of Causality</h3>
<ul>
<li>Larger metadata (version vectors grow with number of replicas)</li>
<li>More complex logic</li>
<li>Still doesn’t eliminate conflicts, just detects them more precisely</li>
</ul>
<h3 id="interactive-demo-3">Interactive Demo</h3>
<p>Explore how version vectors track causality:</p>
<astro-island uid="Z1AQVOM" prefix="r3" component-url="/_astro/VectorClockDemo.CoZCB5dm.js" component-export="default" renderer-url="/_astro/client.JDWrnR5R.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;VectorClockDemo&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>
<h2 id="delta-crdts-efficient-state-transmission">Delta CRDTs: Efficient State Transmission</h2>
<p>State-based CRDTs have a problem: sending the entire state on every sync is wasteful. Delta CRDTs solve this.<sup><a href="#user-content-fn-delta-crdt-paper" id="user-content-fnref-delta-crdt-paper" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup></p>
<h3 id="the-problem-1">The Problem</h3>
<p>Consider a G-Counter with 1000 replicas. If replica A increments its count, must it send all 1000 entries to replica B? That’s inefficient: only one entry changed!</p>
<h3 id="the-solution">The Solution</h3>
<p>Instead of sending full state, send only the <strong>delta</strong>: the part of the state that changed since the last sync.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> Delta</span><span> a </span><span>=</span><span> a  </span><span>-- same type as state, but represents only changes</span></span>
<span></span>
<span><span>merge</span><span> ::</span><span> CRDT</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> Delta</span><span> a </span><span>-&gt;</span><span> a</span></span></code></pre>
<p>For G-Counter, a delta might be just <code>{A: 1}</code> instead of the full map.</p>
<h3 id="definition-9">Definition</h3>
<p>A Delta CRDT extends a state-based CRDT with delta operations:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> DeltaCRDT</span><span> a </span><span>=</span><span> DeltaCRDT</span></span>
<span><span>  { state </span><span>::</span><span> a</span></span>
<span><span>  , lastSent </span><span>::</span><span> Map</span><span> ReplicaId</span><span> a  </span><span>-- track what we've sent to each replica</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>delta</span><span> ::</span><span> ReplicaId</span><span> -&gt;</span><span> DeltaCRDT</span><span> a </span><span>-&gt;</span><span> Delta</span><span> a</span></span>
<span><span>delta replica crdt </span><span>=</span><span> state crdt </span><span>`since`</span><span> lastSent[replica]</span></span></code></pre>
<h3 id="laws-and-invariants-9">Laws and Invariants</h3>
<p>Delta CRDTs must satisfy the same semilattice properties as regular state-based CRDTs, plus:</p>
<p><strong>Delta-state equivalence</strong>: Merging deltas incrementally must be equivalent to merging full states.</p>
<p><strong>Delta composition</strong>: Deltas can be composed: <code>delta1 ⊔ delta2</code> is itself a valid delta.</p>
<h3 id="tradeoffs-10">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Dramatically reduced bandwidth (send only changes)</li>
<li>Same convergence guarantees as state-based CRDTs</li>
<li>Can batch multiple deltas together</li>
<li>Easier to implement than operation-based CRDTs</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Must track what has been sent to each replica</li>
<li>Slightly more complex than pure state-based</li>
<li>Still need full state for new replicas joining</li>
</ul>
<h3 id="when-to-use-10">When to Use</h3>
<p>Use Delta CRDTs when network bandwidth is a concern or state size is large. Most production CRDT systems use delta-state internally (<a href="https://riak.com/">Riak</a>, <a href="https://automerge.org/">Automerge</a>). If you’re implementing your own CRDT system from scratch, start with deltas. Your future self will thank you.</p>
<h3 id="example-delta-g-counter">Example: Delta G-Counter</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>increment</span><span> ::</span><span> ReplicaId</span><span> -&gt;</span><span> GCounter</span><span> -&gt;</span><span> (</span><span>GCounter</span><span>,</span><span> Delta</span><span> GCounter</span><span>)</span></span>
<span><span>increment r counter </span><span>=</span></span>
<span><span>  let</span><span> newCounter </span><span>=</span><span> insertWith (</span><span>+</span><span>) r </span><span>1</span><span> counter</span></span>
<span><span>      delta </span><span>=</span><span> singleton r </span><span>1</span><span>  -- only the change!</span></span>
<span><span>  in</span><span> (newCounter</span><span>,</span><span> delta)</span></span></code></pre>
<p>The delta is just the single updated entry, not the entire counter.</p>
<h2 id="woot-without-operational-transformation">WOOT: Without Operational Transformation</h2>
<p>WOOT is a sequence CRDT that predates RGA, with different design choices.<sup><a href="#user-content-fn-woot-paper" id="user-content-fnref-woot-paper-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
<h3 id="definition-10">Definition</h3>
<p>WOOT represents a sequence as a set of character objects with unique IDs, where each character stores references to its previous and next characters:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> WChar</span><span> a </span><span>=</span><span> WChar</span></span>
<span><span>  { charId </span><span>::</span><span> UID</span></span>
<span><span>  , value </span><span>::</span><span> a</span></span>
<span><span>  , prevId </span><span>::</span><span> UID</span></span>
<span><span>  , nextId </span><span>::</span><span> UID</span></span>
<span><span>  , isVisible </span><span>::</span><span> Bool</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>type</span><span> WOOT</span><span> a </span><span>=</span><span> Set</span><span> (</span><span>WChar</span><span> a)</span></span></code></pre>
<h3 id="key-insight">Key Insight</h3>
<p>Instead of storing a linear sequence, WOOT stores constraints: “this character comes after X and before Y.” When multiple characters claim to be between X and Y, a deterministic ordering (based on UID) resolves the conflict.</p>
<h3 id="operations-9">Operations</h3>
<p><strong>Insert</strong> (after character with ID <code>prev</code>, before character with ID <code>next</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>insert</span><span> ::</span><span> a </span><span>-&gt;</span><span> UID</span><span> -&gt;</span><span> UID</span><span> -&gt;</span><span> UID</span><span> -&gt;</span><span> WOOT</span><span> a </span><span>-&gt;</span><span> WOOT</span><span> a</span></span>
<span><span>insert val uid prev next woot </span><span>=</span></span>
<span><span>  insert (WChar uid val prev next True) woot</span></span></code></pre>
<p><strong>Delete</strong> (character with ID <code>uid</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>delete</span><span> ::</span><span> UID</span><span> -&gt;</span><span> WOOT</span><span> a </span><span>-&gt;</span><span> WOOT</span><span> a</span></span>
<span><span>delete uid woot </span><span>=</span><span> -- mark character as invisible, don't remove</span></span></code></pre>
<h3 id="linearization">Linearization</h3>
<p>To read the sequence, perform a topological sort respecting the prev/next constraints, filtering out invisible characters.</p>
<h3 id="tradeoffs-11">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Strong eventual consistency</li>
<li>No need for causal delivery (constraints handle ordering)</li>
<li>Intuitive model (characters reference neighbors)</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Tombstones accumulate (deleted characters remain)</li>
<li>Linearization has O(n²) worst case</li>
<li>More complex than RGA</li>
<li>UIDs must be globally unique and ordered</li>
</ul>
<h3 id="comparison-with-rga">Comparison with RGA</h3>
<ul>
<li><strong>RGA</strong>: Uses a tree structure, parent-child relationships</li>
<li><strong>WOOT</strong>: Uses bidirectional constraints, more flexible but slower linearization</li>
</ul>
<h3 id="when-to-use-11">When to Use</h3>
<p>WOOT is primarily of historical interest. Modern implementations prefer RGA or YATA for better performance. But it’s a neat design, and the name alone makes it worth knowing about.</p>
<h2 id="logoot-scalable-position-identifiers">Logoot: Scalable Position Identifiers</h2>
<p>Logoot takes a different approach to sequences: instead of linking elements, assign each element a position in a dense order.<sup><a href="#user-content-fn-logoot-paper" id="user-content-fnref-logoot-paper" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup></p>
<h3 id="definition-11">Definition</h3>
<p>Each element has a position identifier that is a sequence of (digit, replicaId) pairs:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> Position</span><span> =</span><span> [(</span><span>Int</span><span>,</span><span> ReplicaId</span><span>)]</span></span>
<span></span>
<span><span>data</span><span> LogootElement</span><span> a </span><span>=</span><span> LogootElement</span></span>
<span><span>  { position </span><span>::</span><span> Position</span></span>
<span><span>  , value </span><span>::</span><span> a</span></span>
<span><span>  , isDeleted </span><span>::</span><span> Bool</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>type</span><span> Logoot</span><span> a </span><span>=</span><span> Set</span><span> (</span><span>LogootElement</span><span> a)</span></span></code></pre>
<p>Positions are ordered lexicographically.</p>
<h3 id="key-insight-1">Key Insight</h3>
<p>Positions form a dense order: between any two positions, you can always allocate a new position. To insert between positions <code>p1</code> and <code>p2</code>, generate a new position <code>p</code> such that <code>p1 &lt; p &lt; p2</code>.</p>
<h3 id="operations-10">Operations</h3>
<p><strong>Insert</strong> (between positions <code>before</code> and <code>after</code>):</p>
<pre tabindex="0" data-language="haskell"><code><span><span>insert</span><span> ::</span><span> a </span><span>-&gt;</span><span> Position</span><span> -&gt;</span><span> Position</span><span> -&gt;</span><span> Logoot</span><span> a </span><span>-&gt;</span><span> Logoot</span><span> a</span></span>
<span><span>insert val before after logoot </span><span>=</span></span>
<span><span>  let</span><span> newPos </span><span>=</span><span> allocatePosition before after currentReplicaId</span></span>
<span><span>      element </span><span>=</span><span> LogootElement newPos val False</span></span>
<span><span>  in</span><span> insert element logoot</span></span></code></pre>
<p><strong>Position Allocation</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>allocatePosition</span><span> ::</span><span> Position</span><span> -&gt;</span><span> Position</span><span> -&gt;</span><span> ReplicaId</span><span> -&gt;</span><span> Position</span></span>
<span><span>allocatePosition before after replicaId </span><span>=</span></span>
<span><span>  -- Find a position between before and after</span></span>
<span><span>  -- Use replicaId as tiebreaker for deterministic ordering</span></span></code></pre>
<p><strong>Delete</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>delete</span><span> ::</span><span> Position</span><span> -&gt;</span><span> Logoot</span><span> a </span><span>-&gt;</span><span> Logoot</span><span> a</span></span>
<span><span>delete pos logoot </span><span>=</span><span> -- mark element at pos as deleted</span></span></code></pre>
<h3 id="laws-and-invariants-10">Laws and Invariants</h3>
<p><strong>Deterministic ordering</strong>: Elements are always ordered by their positions.</p>
<p><strong>Unique positions</strong>: Each insert generates a unique position (using replica ID in the position).</p>
<h3 id="tradeoffs-12">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>No need to reference other elements by ID</li>
<li>Simpler merge than WOOT</li>
<li>Positions are self-describing (no need to look up IDs)</li>
<li>Can insert without knowing the full document structure</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Position identifiers grow over time (especially with many edits)</li>
<li>Still accumulates tombstones</li>
<li>Position allocation algorithm is complex</li>
<li>Pathological cases where positions become very long</li>
</ul>
<h3 id="lseq-adaptive-positions">LSEQ: Adaptive Positions</h3>
<p>LSEQ improves on Logoot by using an adaptive allocation strategy. Instead of always allocating positions the same way, LSEQ alternates between strategies to keep positions shorter on average.<sup><a href="#user-content-fn-lseq-paper" id="user-content-fnref-lseq-paper" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup></p>
<h3 id="when-to-use-12">When to Use</h3>
<p>Use Logoot/LSEQ when you need a sequence CRDT and want simpler semantics than RGA/WOOT. The tradeoff is position identifier growth.</p>
<h2 id="tree-crdts-hierarchical-data">Tree CRDTs: Hierarchical Data</h2>
<p>Extending CRDTs to trees is challenging because parent-child relationships must be maintained consistently.</p>
<h3 id="the-problem-2">The Problem</h3>
<p>Trees have structural constraints:</p>
<ul>
<li>Each node has exactly one parent (except root)</li>
<li>No cycles allowed</li>
<li>Moving a node changes parent-child relationships</li>
</ul>
<p>How do we handle concurrent operations like:</p>
<ul>
<li>Two replicas move the same node to different parents?</li>
<li>One replica moves node A under node B while another moves B under A?</li>
</ul>
<h3 id="approaches">Approaches</h3>
<p><strong>OR-Tree</strong>: Combine OR-Set with parent pointers, using conflict resolution strategies when multiple parents are observed.</p>
<p><strong>CRDT-Tree</strong>: Use causal ordering to determine which move operations take precedence.</p>
<p><strong>Log-based Trees</strong>: Store operations in a replicated log and rebuild tree structure on read.</p>
<h3 id="or-tree-definition">OR-Tree Definition</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ORTree</span><span> a </span><span>=</span><span> Map</span><span> NodeId</span><span> (</span><span>ORSet</span><span> ParentId</span><span>,</span><span> a)</span></span></code></pre>
<p>Each node stores an OR-Set of potential parents. Conflict resolution:</p>
<ul>
<li><strong>Last-write-wins</strong>: Use timestamps to pick winning parent</li>
<li><strong>First-wins</strong>: The first parent observed wins</li>
<li><strong>Merge</strong>: Allow nodes to have multiple parents temporarily, application resolves</li>
</ul>
<h3 id="tradeoffs-13">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Can represent hierarchical data distributedly</li>
<li>Handles concurrent structural changes</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Complex conflict resolution strategies</li>
<li>Must prevent cycles (may require rejecting some operations)</li>
<li>Moving subtrees is complicated</li>
<li>High metadata overhead</li>
</ul>
<h3 id="when-to-use-13">When to Use</h3>
<p>Use Tree CRDTs for file systems, organizational charts, or document outlines where the hierarchy must be replicated. Be prepared for complexity in handling concurrent structural changes.</p>
<h3 id="alternatives-1">Alternatives</h3>
<p>For many use cases, an OR-Map with explicit parent fields is simpler than a full Tree CRDT, even if it doesn’t enforce tree constraints at the CRDT level.</p>

<p>A practical example combining multiple CRDT concepts.</p>
<h3 id="the-domain">The Domain</h3>
<p>An e-commerce shopping cart must support:</p>
<ul>
<li>Add product to cart</li>
<li>Remove product from cart</li>
<li>Change quantity</li>
<li>Work offline and sync later</li>
</ul>
<h3 id="naive-approach-lww-map">Naive Approach: LWW Map</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> CartLWW</span><span> =</span><span> Map</span><span> ProductId</span><span> (</span><span>Int</span><span>,</span><span> Timestamp</span><span>)</span></span></code></pre>
<p>Problems:</p>
<ul>
<li>Concurrent additions of the same product (one wins)</li>
<li>Remove on one device, add on another (one wins, data loss)</li>
</ul>
<h3 id="better-or-set--pn-counter">Better: OR-Set + PN-Counter</h3>
<pre tabindex="0" data-language="haskell"><code><span><span>type</span><span> ShoppingCart</span><span> =</span><span> Map</span><span> ProductId</span><span> PNCounter</span></span></code></pre>
<ul>
<li>Use OR-Set semantics for which products are in cart</li>
<li>Use PN-Counter for quantities</li>
<li>Add-wins semantics for products (if concurrently added and removed, item stays)</li>
<li>Quantities merge correctly (concurrent +1 and +2 becomes +3)</li>
</ul>
<h3 id="operations-11">Operations</h3>
<p><strong>Add product with quantity</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>addToCart</span><span> ::</span><span> ProductId</span><span> -&gt;</span><span> Int</span><span> -&gt;</span><span> ReplicaId</span><span> -&gt;</span><span> ShoppingCart</span><span> -&gt;</span><span> ShoppingCart</span></span>
<span><span>addToCart pid qty replica cart </span><span>=</span></span>
<span><span>  let</span><span> counter </span><span>=</span><span> lookupOr emptyCounter pid cart</span></span>
<span><span>      incremented </span><span>=</span><span> incrementN replica qty counter</span></span>
<span><span>  in</span><span> insert pid incremented cart</span></span></code></pre>
<p><strong>Remove product</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>removeFromCart</span><span> ::</span><span> ProductId</span><span> -&gt;</span><span> ShoppingCart</span><span> -&gt;</span><span> ShoppingCart</span></span>
<span><span>removeFromCart pid cart </span><span>=</span><span> delete pid cart</span></span></code></pre>
<p><strong>Change quantity</strong>:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>changeQuantity</span><span> ::</span><span> ProductId</span><span> -&gt;</span><span> Int</span><span> -&gt;</span><span> ReplicaId</span><span> -&gt;</span><span> ShoppingCart</span><span> -&gt;</span><span> ShoppingCart</span></span>
<span><span>changeQuantity pid delta replica cart </span><span>=</span></span>
<span><span>  let</span><span> counter </span><span>=</span><span> lookupOr emptyCounter pid cart</span></span>
<span><span>      updated </span><span>=</span><span> if</span><span> delta </span><span>&gt;</span><span> 0</span></span>
<span><span>                then</span><span> incrementN replica delta counter</span></span>
<span><span>                else</span><span> decrementN replica (</span><span>-</span><span>delta) counter</span></span>
<span><span>  in</span><span> insert pid updated cart</span></span></code></pre>
<h3 id="tradeoffs-14">Tradeoffs</h3>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Handles all operations correctly</li>
<li>No data loss on concurrent modifications</li>
<li>Intuitive semantics for users</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>PN-Counters can go negative (need validation)</li>
<li>Must track all replicas (for PN-Counter)</li>
<li>Slightly more overhead than simple LWW</li>
</ul>
<p>This example shows how combining basic CRDTs creates sophisticated application-level data structures.</p>
<h2 id="practical-considerations">Practical Considerations</h2>
<h3 id="choosing-a-crdt">Choosing a CRDT</h3>
<p>The choice of CRDT depends on your requirements:</p>
<p><strong>Do you need only additions?</strong> Use G-Counter or G-Set.</p>
<p><strong>Do you need removals but not re-additions?</strong> Use 2P-Set.</p>
<p><strong>Can you tolerate last-write-wins?</strong> Use LWW-Element-Set or LWW-Register.</p>
<p><strong>Do you need to preserve concurrent operations?</strong> Use OR-Set or MV-Register.</p>
<p><strong>Do you have sequences?</strong> Use RGA or similar sequence CRDT.</p>
<p><strong>Do you need nested structures?</strong> Use OR-Map with nested CRDTs.</p>
<h3 id="garbage-collection-1">Garbage Collection</h3>
<p>Garbage collection is one of the most challenging practical problems with CRDTs. The fundamental tension: CRDTs achieve convergence by monotonically accumulating information, but production systems can’t grow unbounded forever.</p>
<h4 id="the-problem-in-detail">The Problem in Detail</h4>
<p>Consider an OR-Set used for a collaborative todo list. Each time someone adds a task and removes it, we accumulate:</p>
<ul>
<li>A unique tag for the addition (never removed)</li>
<li>A tombstone tracking the removal (never removed)</li>
</ul>
<p>After 10,000 tasks have been created and completed, our “empty” todo list still contains 10,000 tags worth of metadata. In a G-Counter tracking page views, we keep a separate count for every replica that has ever incremented the counter—even if that replica hasn’t been online in years. For sequence CRDTs like RGA or WOOT, every deleted character becomes a tombstone that must be retained indefinitely. A 1000-character document that’s been heavily edited might internally contain 50,000 tombstones.</p>
<p>The core issue: <strong>CRDTs converge by retaining enough information to handle any possible merge</strong>. If replica A discards metadata about some operation, and replica B (which has been offline for weeks) later tries to merge its state—which still references that metadata—the merge may produce incorrect results.</p>
<h4 id="why-cant-we-just-delete-old-data">Why Can’t We Just Delete Old Data?</h4>
<p>Let’s make this concrete with an OR-Set example:</p>
<pre tabindex="0" data-language="haskell"><code><span><span>-- Replica A's state</span></span>
<span><span>orset_a </span><span>=</span><span> {"todo</span><span>-</span><span>1</span><span>": {tag_1, tag_2}}</span></span>
<span></span>
<span><span>-- Replica B's state (has been offline)</span></span>
<span><span>orset_b = {"</span><span>todo</span><span>-</span><span>1</span><span>": {tag_1, tag_2, tag_3}}</span></span>
<span></span>
<span><span>-- Replica A removes todo-1, observing tags {tag_1, tag_2}</span></span>
<span><span>-- Now A's state is:</span></span>
<span><span>orset_a = {}</span></span>
<span></span>
<span><span>-- If A garbage collects and forgets about tags {tag_1, tag_2},</span></span>
<span><span>-- then later merges with B:</span></span>
<span><span>merge(orset_a, orset_b) = {"</span><span>todo</span><span>-</span><span>1</span><span>": {tag_3}}</span></span>
<span></span>
<span><span>-- The element reappears! (Zombie resurrection)</span></span></code></pre>
<p>The element we removed comes back because we lost the causal information about which tags we had observed and removed. This is the fundamental safety problem with CRDT garbage collection.</p>
<h4 id="strategies-and-tradeoffs">Strategies and Tradeoffs</h4>
<p><strong>Time-Based Expiry</strong></p>
<p>The simplest approach: discard metadata older than some threshold (e.g., 30 days). This works well when you can guarantee all replicas sync within that window.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>gcTombstones</span><span> ::</span><span> Timestamp</span><span> -&gt;</span><span> ORSet</span><span> a </span><span>-&gt;</span><span> ORSet</span><span> a</span></span>
<span><span>gcTombstones cutoff set </span><span>=</span></span>
<span><span>  -- Remove tags older than cutoff</span></span>
<span><span>  Map.mapMaybe (</span><span>\\</span><span>tags </span><span>-&gt;</span></span>
<span><span>    let</span><span> recent </span><span>=</span><span> Set.filter (</span><span>\\</span><span>t </span><span>-&gt;</span><span> tagTime t </span><span>&gt;</span><span> cutoff) tags</span></span>
<span><span>    in</span><span> if</span><span> Set.null recent </span><span>then</span><span> Nothing </span><span>else</span><span> Just recent) set</span></span></code></pre>
<p><em>Advantages</em>:</p>
<ul>
<li>Simple to implement</li>
<li>No coordination required</li>
<li>Works well for frequently-syncing systems</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Unsafe if replicas can be offline longer than the grace period</li>
<li>Must choose grace period conservatively (wasted space)</li>
<li>Zombie resurrection if threshold is too aggressive</li>
</ul>
<p><em>When to use</em>: Mobile apps where you can bound offline time (e.g., “you must sync at least once per week”).</p>
<p><strong>Coordinated Garbage Collection</strong></p>
<p>Use distributed consensus to agree on what’s safe to discard. Once all replicas acknowledge they’ve received a particular update, the corresponding metadata can be safely removed.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> GCState</span><span> =</span><span> GCState</span></span>
<span><span>  { pendingGC </span><span>::</span><span> Set</span><span> Tag</span><span>  -- Tags eligible for GC</span></span>
<span><span>  , replicaAcks </span><span>::</span><span> Map</span><span> ReplicaId</span><span> (</span><span>Set</span><span> Tag</span><span>)  </span><span>-- What each replica has seen</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>-- When all replicas have acked a tag, it's safe to remove</span></span>
<span><span>safeToDiscard</span><span> ::</span><span> GCState</span><span> -&gt;</span><span> Set</span><span> Tag</span></span>
<span><span>safeToDiscard (GCState pending acks) </span><span>=</span></span>
<span><span>  -- Tags that all known replicas have acknowledged</span></span>
<span><span>  Set.filter (</span><span>\\</span><span>tag </span><span>-&gt;</span><span> all (Set.member tag) (Map.elems acks)) pending</span></span></code></pre>
<p><em>Advantages</em>:</p>
<ul>
<li>Completely safe (no zombie resurrections)</li>
<li>Can garbage collect aggressively once consensus is reached</li>
<li>Works with arbitrary offline periods</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Requires coordination (defeats CRDT’s main selling point!)</li>
<li>Slow convergence if some replicas are rarely online</li>
<li>Must track all replicas (what about replicas that never come back?)</li>
</ul>
<p><em>When to use</em>: When you have a bounded, known set of replicas and can tolerate periodic coordination rounds.</p>
<p><strong>Version Vectors for Causal Tracking</strong></p>
<p>Use version vectors to track causal history. Metadata can be discarded once it’s been causally superseded at all replicas.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> CausalORSet</span><span> a </span><span>=</span><span> CausalORSet</span></span>
<span><span>  { elements </span><span>::</span><span> Map</span><span> a (</span><span>Set</span><span> (</span><span>Tag</span><span>,</span><span> VersionVector</span><span>))</span></span>
<span><span>  , replicaVersions </span><span>::</span><span> Map</span><span> ReplicaId</span><span> VersionVector</span><span>  -- Last known VV per replica</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>-- A tag can be GC'd if its version vector is dominated by all known replicas</span></span>
<span><span>canDiscardTag</span><span> ::</span><span> (</span><span>Tag</span><span>,</span><span> VersionVector</span><span>) </span><span>-&gt;</span><span> Map</span><span> ReplicaId</span><span> VersionVector</span><span> -&gt;</span><span> Bool</span></span>
<span><span>canDiscardTag (_</span><span>,</span><span> tagVV) replicaVVs </span><span>=</span></span>
<span><span>  all (</span><span>\\</span><span>replicaVV </span><span>-&gt;</span><span> tagVV </span><span>`happenedBefore`</span><span> replicaVV) (Map.elems replicaVVs)</span></span></code></pre>
<p>This is more sophisticated: we track causality explicitly and can safely discard tags that are in the causal past of all known replicas.</p>
<p><em>Advantages</em>:</p>
<ul>
<li>More precise than time-based expiry</li>
<li>No coordination needed for the happy path</li>
<li>Safe as long as causal tracking is correct</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Version vectors add significant overhead (O(replicas) per operation)</li>
<li>Still requires tracking all replicas</li>
<li>Complex to implement correctly</li>
<li>What about new replicas that join later?</li>
</ul>
<p><em>When to use</em>: Systems already using version vectors for causal consistency (Riak, Cassandra-style systems).</p>
<p><strong>Bounded Structures with Fallback</strong></p>
<p>Limit metadata size and use LWW semantics when bounds are exceeded. For example, keep at most 1000 tags per element in an OR-Set. If we exceed that, discard the oldest tags and accept potential anomalies.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>addWithBound</span><span> ::</span><span> Ord</span><span> a </span><span>=&gt;</span><span> a </span><span>-&gt;</span><span> Tag</span><span> -&gt;</span><span> Int</span><span> -&gt;</span><span> ORSet</span><span> a </span><span>-&gt;</span><span> ORSet</span><span> a</span></span>
<span><span>addWithBound x tag maxTags set </span><span>=</span></span>
<span><span>  let</span><span> currentTags </span><span>=</span><span> Map.findWithDefault Set.empty x set</span></span>
<span><span>      newTags </span><span>=</span><span> Set.insert tag currentTags</span></span>
<span><span>      boundedTags </span><span>=</span><span> if</span><span> Set.size newTags </span><span>&gt;</span><span> maxTags</span></span>
<span><span>                    then</span><span> Set.fromList </span><span>$</span><span> take maxTags </span><span>$</span><span> </span></span>
<span><span>                         sortBy (comparing tagTimestamp) (Set.toList newTags)</span></span>
<span><span>                    else</span><span> newTags</span></span>
<span><span>  in</span><span> Map.insert x boundedTags set</span></span></code></pre>
<p><em>Advantages</em>:</p>
<ul>
<li>Bounded space overhead (guaranteed)</li>
<li>No coordination needed</li>
<li>Graceful degradation (becomes LWW-ish when bounded)</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Correctness sacrificed for space</li>
<li>May lose concurrent operations</li>
<li>Choosing the bound is difficult (too small = frequent anomalies, too large = still wasteful)</li>
</ul>
<p><em>When to use</em>: When you must have bounded space (embedded systems, strict SLAs) and can tolerate occasional anomalies.</p>
<p><strong>Checkpoint and Rebase</strong></p>
<p>Periodically create a “checkpoint” snapshot and discard history before that point. New replicas joining after the checkpoint start from the snapshot.</p>
<pre tabindex="0" data-language="haskell"><code><span><span>data</span><span> CheckpointedCRDT</span><span> a </span><span>=</span><span> CheckpointedCRDT</span></span>
<span><span>  { baselineState </span><span>::</span><span> a  </span><span>-- Snapshot at checkpoint</span></span>
<span><span>  , checkpointTime </span><span>::</span><span> Timestamp</span></span>
<span><span>  , deltaSince </span><span>::</span><span> [</span><span>Delta</span><span> a]  </span><span>-- Operations since checkpoint</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>-- Create a new checkpoint, discarding old deltas</span></span>
<span><span>checkpoint</span><span> ::</span><span> CheckpointedCRDT</span><span> a </span><span>-&gt;</span><span> CheckpointedCRDT</span><span> a</span></span>
<span><span>checkpoint crdt </span><span>=</span><span> CheckpointedCRDT</span></span>
<span><span>  { baselineState </span><span>=</span><span> foldl merge (baselineState crdt) (deltaSince crdt)</span></span>
<span><span>  , checkpointTime </span><span>=</span><span> currentTime</span></span>
<span><span>  , deltaSince </span><span>=</span><span> []</span></span>
<span><span>  }</span></span></code></pre>
<p>Replicas that haven’t synced since before the checkpoint must do a full state sync rather than incremental merge.</p>
<p><em>Advantages</em>:</p>
<ul>
<li>Can aggressively prune old history</li>
<li>Conceptually clean (like Git’s shallow clones)</li>
<li>Works well with mostly-online systems</li>
</ul>
<p><em>Disadvantages</em>:</p>
<ul>
<li>Replicas offline during checkpoint period lose incremental sync</li>
<li>Need to track which replicas are pre-checkpoint</li>
<li>Full state sync is expensive</li>
</ul>
<p><em>When to use</em>: Collaborative editing systems where most users are online most of the time (Google Docs, Figma).</p>
<h4 id="practical-recommendations">Practical Recommendations</h4>
<p>For most applications, a <strong>hybrid approach</strong> works best:</p>
<ol>
<li>Use time-based expiry with a conservative grace period (90 days)</li>
<li>Track the oldest unsynced replica timestamp</li>
<li>Only discard metadata older than: <code>min(graceperiod, oldest_unsynced - safety_margin)</code></li>
<li>Provide manual “compact” operations for administrators</li>
<li>Use bounded structures for untrusted/public replicas</li>
</ol>
<p>Without some form of garbage collection, CRDT state grows unbounded and will eventually exhaust memory or storage. The question isn’t whether to implement GC, but which tradeoffs you’re willing to accept.</p>
<p>And, realistically speaking, you’re unlikely to implement a system that only uses CRDTs and no other data storage. You’ll almost certainly have some sort of traditional database to store your data, which
you can probably use to periodically coordinate garbage collection.</p>
<h3 id="a-note-on-causal-consistency">A note on Causal Consistency</h3>
<p>CRDTs themselves don’t enforce causal delivery. You need a causal broadcast protocol to ensure operations are delivered respecting happens-before relationships. Without causal delivery, some CRDTs (especially operation-based ones) may behave incorrectly.</p>
<h3 id="performance">Performance</h3>
<p>Different CRDTs have different performance characteristics. Consider your read/write ratio, expected contention, and replica count when choosing:</p>


























































































































<table><thead><tr><th>CRDT Type</th><th>Space Complexity</th><th>Add/Insert</th><th>Remove/Delete</th><th>Merge</th><th>Read/Query</th><th>Notes</th></tr></thead><tbody><tr><td><strong>G-Counter</strong></td><td>O(r)</td><td>O(1)</td><td>N/A</td><td>O(r)</td><td>O(r)</td><td>Space: one counter per replica</td></tr><tr><td><strong>PN-Counter</strong></td><td>O(r)</td><td>O(1)</td><td>O(1)</td><td>O(r)</td><td>O(r)</td><td>Double the space of G-Counter</td></tr><tr><td><strong>G-Set</strong></td><td>O(e)</td><td>O(1)</td><td>N/A</td><td>O(e)</td><td>O(1)</td><td>Standard set operations</td></tr><tr><td><strong>2P-Set</strong></td><td>O(e)</td><td>O(1)</td><td>O(1)</td><td>O(e)</td><td>O(1)</td><td>Both added and removed sets grow</td></tr><tr><td><strong>LWW-Element-Set</strong></td><td>O(e)</td><td>O(1)</td><td>O(1)</td><td>O(e)</td><td>O(1)</td><td>Can GC old timestamps carefully</td></tr><tr><td><strong>OR-Set</strong></td><td>O(e × t)</td><td>O(1)</td><td>O(t)</td><td>O(e × t)</td><td>O(1)</td><td>Tags accumulate, needs GC</td></tr><tr><td><strong>LWW-Register</strong></td><td>O(1)</td><td>O(1)</td><td>N/A</td><td>O(1)</td><td>O(1)</td><td>Minimal overhead</td></tr><tr><td><strong>MV-Register</strong></td><td>O(concurrent)</td><td>O(1)</td><td>N/A</td><td>O(c)</td><td>O(c)</td><td>Returns set of concurrent values</td></tr><tr><td><strong>OR-Map</strong></td><td>O(k × t)</td><td>O(1)</td><td>O(t)</td><td>O(k × t)</td><td>O(1)</td><td>Per-key OR-Set overhead</td></tr><tr><td><strong>RGA</strong></td><td>O(n + d)</td><td>O(log n)</td><td>O(log n)</td><td>O(n + d)</td><td>O(n)</td><td>Tombstones accumulate</td></tr><tr><td><strong>WOOT</strong></td><td>O(n + d)</td><td>O(n²) worst</td><td>O(log n)</td><td>O(n + d)</td><td>O(n²) worst</td><td>Linearization is expensive</td></tr><tr><td><strong>Logoot/LSEQ</strong></td><td>O(n × p)</td><td>O(log n)</td><td>O(log n)</td><td>O(n)</td><td>O(n log n)</td><td>Position identifiers grow</td></tr></tbody></table>
<p><strong>Legend:</strong></p>
<ul>
<li><code>r</code> = number of replicas</li>
<li><code>e</code> = number of elements in set</li>
<li><code>t</code> = average tags per element (OR-Set)</li>
<li><code>k</code> = number of keys in map</li>
<li><code>n</code> = number of visible elements in sequence</li>
<li><code>d</code> = number of deleted elements (tombstones)</li>
<li><code>c</code> = number of concurrent writes</li>
<li><code>p</code> = average position identifier length</li>
</ul>
<p><strong>Key Observations:</strong></p>
<ul>
<li><strong>Counter CRDTs</strong> scale with replica count, not operation count. A billion increments still cost O(replicas) space.</li>
<li><strong>Set CRDTs</strong> generally have constant-time operations, but OR-Set’s space grows with tags unless garbage collected.</li>
<li><strong>Sequence CRDTs</strong> suffer from tombstone accumulation. RGA is typically faster than WOOT in practice despite similar asymptotic complexity.</li>
<li><strong>Position-based sequences</strong> (Logoot/LSEQ) trade time complexity for avoiding explicit parent pointers, but position identifiers can grow pathologically.</li>
<li><strong>Merge operations</strong> are often the bottleneck in high-throughput systems. Delta CRDTs dramatically improve merge performance by sending only changes.</li>
</ul>
<h3 id="libraries-and-implementations">Libraries and Implementations</h3>
<p>Many CRDT libraries exist:</p>
<ul>
<li><strong>Automerge</strong>: Full-featured CRDT library for JSON-like documents<sup><a href="#user-content-fn-automerge" id="user-content-fnref-automerge" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup></li>
<li><strong>Yjs</strong>: Optimized for collaborative editing<sup><a href="#user-content-fn-yjs" id="user-content-fnref-yjs" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup></li>
<li><strong>Riak</strong>: Database with built-in CRDT support<sup><a href="#user-content-fn-riak" id="user-content-fnref-riak" data-footnote-ref="" aria-describedby="footnote-label">17</a></sup></li>
<li><strong>Redis Enterprise</strong>: CRDT-enabled Redis<sup><a href="#user-content-fn-redis-crdt" id="user-content-fnref-redis-crdt" data-footnote-ref="" aria-describedby="footnote-label">18</a></sup></li>
<li><strong>AntidoteDB</strong>: CRDT-native database<sup><a href="#user-content-fn-antidote" id="user-content-fnref-antidote" data-footnote-ref="" aria-describedby="footnote-label">19</a></sup></li>
</ul>
<p>Each makes different tradeoff decisions.</p>
<h2 id="further-reading">Further Reading</h2>
<p>The CRDT literature is vast and honestly a bit scattered across conference proceedings. Here are the key papers worth reading:</p>
<p><strong>Foundational</strong>:</p>
<ul>
<li>Shapiro et al., <a href="https://hal.inria.fr/inria-00609399v1/document">“Conflict-Free Replicated Data Types”</a> (2011): The original CRDT paper, defining state-based and operation-based CRDTs.</li>
<li>Shapiro et al., <a href="https://hal.inria.fr/inria-00555588/document">“A Comprehensive Study of Convergent and Commutative Replicated Data Types”</a> (2011): Detailed technical report covering many CRDTs. This is the one you want to bookmark.</li>
</ul>
<p><strong>Sequence CRDTs</strong>:</p>
<ul>
<li>Oster et al., <a href="https://hal.inria.fr/inria-00108523/document">“Data Consistency for P2P Collaborative Editing”</a> (2006): Introduces WOOT.</li>
<li>Roh et al., <a href="http://csl.skku.edu/papers/jpdc11.pdf">“Replicated Abstract Data Types: Building Blocks for Collaborative Applications”</a> (2011): Introduces RGA.</li>
<li>Weiss et al., <a href="https://hal.inria.fr/inria-00432368/document">“Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing”</a> (2009): Introduces Logoot.</li>
<li>Nédelec et al., <a href="https://hal.archives-ouvertes.fr/hal-00921633/document">“LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing”</a> (2013): Introduces LSEQ.</li>
</ul>
<p><strong>Advanced Topics</strong>:</p>
<ul>
<li>Baquero et al., <a href="https://inria.hal.science/hal-01287738v1/document">“Making Operation-based CRDTs Operation-based”</a> (2014): Pure operation-based CRDTs without state.</li>
<li>Almeida et al., <a href="https://arxiv.org/abs/1603.01529">“Delta State Replicated Data Types”</a> (2018): Efficiency improvements for state-based CRDTs.</li>
<li>Kleppmann et al., <a href="https://arxiv.org/abs/1608.03960">“A Conflict-Free Replicated JSON Datatype”</a> (2017): Automerge’s design.</li>
</ul>
<p><strong>Surveys</strong>:</p>
<ul>
<li>Shapiro et al., <a href="https://inria.hal.science/inria-00555588/en/">“Convergent and Commutative Replicated Data Types”</a> (2011): The comprehensive technical report. Start here if you want depth.</li>
</ul>
<h2 id="wrapping-up">Wrapping up</h2>
<p>CRDTs are not a silver bullet. They trade coordination for metadata, strong consistency for eventual consistency, and simplicity for convergence guarantees. But in scenarios where availability matters more than immediate consistency, they’re remarkably powerful.</p>
<p>There is no “best” CRDT, only CRDTs suited to different problems; the CRDT you choose depends entirely on your application’s semantics:</p>
<ul>
<li>What operations do you need (add, remove, re-add)?</li>
<li>Can you tolerate lost updates?</li>
<li>Do you need to detect conflicts or resolve them automatically?</li>
<li>What’s your tolerance for metadata overhead?</li>
</ul>
<p>The CRDT abstraction is elegant in theory, but bewildering in practice because there are so many instances with subtle differences. Hopefully this guide has cut through some of the confusion, and given you a good intuition for how they work and when to use them.</p>
<p>I honestly still haven’t hit a use case for CRDTs that I couldn’t solve with a traditional database and some custom coordination logic. But sometimes we just want to learn for the sake of learning. If you beat me to it, let me know!</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-crdt-origin">
<p>The term “Conflict-free Replicated Data Type” was coined by Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski in their 2011 paper <a href="https://hal.inria.fr/inria-00609399v1/document">“Conflict-free Replicated Data Types”</a> (technical report) and the 2011 SSS conference paper <a href="https://hal.inria.fr/inria-00555588/document">“A comprehensive study of Convergent and Commutative Replicated Data Types”</a>. The theoretical foundations draw from earlier work on commutative replicated data types and optimistic replication. <a href="#user-content-fnref-crdt-origin" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-woot-paper">
<p>WOOT was introduced by Oster, Urso, Molli, and Imine in <a href="https://hal.inria.fr/inria-00108523/document">“Data Consistency for P2P Collaborative Editing”</a> (2006). The name is a play on “OT” (Operational Transformation), emphasizing that it achieves similar goals “WithOut OT.” WOOT was one of the first practical sequence CRDTs and influenced many subsequent designs. <a href="#user-content-fnref-woot-paper" data-footnote-backref="" aria-label="Back to reference 2">↩</a> <a href="#user-content-fnref-woot-paper-2" data-footnote-backref="" aria-label="Back to reference 2-2">↩<sup>2</sup></a></p>
</li>
<li id="user-content-fn-cvcrdt">
<p>State-based CRDTs are also called “convergent” replicated data types (CvRDT). The “Cv” stands for “convergent” - emphasizing that replicas converge to the same state by repeatedly applying the join operation. <a href="#user-content-fnref-cvcrdt" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-cmcrdt">
<p>Operation-based CRDTs are also called “commutative” replicated data types (CmRDT). They require causal delivery of operations - if operation A happened before operation B on the same replica, B must not be delivered before A at any other replica. <a href="#user-content-fnref-cmcrdt" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
<li id="user-content-fn-gcounter-origin">
<p>The G-Counter appears in Shapiro et al.’s 2011 technical report <a href="https://hal.inria.fr/inria-00555588/document">“A Comprehensive Study of Convergent and Commutative Replicated Data Types”</a> as one of the foundational examples demonstrating CRDT principles. <a href="#user-content-fnref-gcounter-origin" data-footnote-backref="" aria-label="Back to reference 5">↩</a></p>
</li>
<li id="user-content-fn-gcounter-space">
<p>The space complexity is O(n) where n is the number of replicas, not the number of increments. This means G-Counters scale well with the number of operations but require tracking all replicas that have ever incremented the counter. <a href="#user-content-fnref-gcounter-space" data-footnote-backref="" aria-label="Back to reference 6">↩</a></p>
</li>
<li id="user-content-fn-orset-origin">
<p>The OR-Set (Observed-Remove Set) was introduced by Shapiro et al. in their <a href="https://hal.inria.fr/inria-00555588/document">2011 technical report</a>. It’s also known as the “Add-Wins Set” because concurrent add and remove operations result in the element remaining in the set. The key innovation is using unique tags to distinguish between different additions of the same element. <a href="#user-content-fnref-orset-origin" data-footnote-backref="" aria-label="Back to reference 7">↩</a></p>
</li>
<li id="user-content-fn-sequence-crdt-challenge">
<p>Sequence CRDTs are particularly challenging because positional indices change as elements are inserted or deleted. Unlike sets or counters where elements have stable identity, sequences must maintain ordering despite concurrent modifications at arbitrary positions. <a href="#user-content-fnref-sequence-crdt-challenge" data-footnote-backref="" aria-label="Back to reference 8">↩</a></p>
</li>
<li id="user-content-fn-rga-paper">
<p>RGA was introduced by Roh et al. in <a href="http://csl.skku.edu/papers/jpdc11.pdf">“Replicated Abstract Data Types: Building Blocks for Collaborative Applications”</a> (2011). The name “Replicated Growable Array” emphasizes that it’s an array-like structure that can grow through replication. <a href="#user-content-fnref-rga-paper" data-footnote-backref="" aria-label="Back to reference 9">↩</a></p>
</li>
<li id="user-content-fn-yata-yjs">
<p>YATA (Yet Another Transformation Approach) was developed by Kevin Jahns for the <a href="https://yjs.dev/">Yjs</a> collaborative editing library. It combines ideas from RGA and WOOT while optimizing for the common case of sequential insertions (typing). Yjs is used in production by companies like Braid, Row Zero, and others for real-time collaboration. <a href="#user-content-fnref-yata-yjs" data-footnote-backref="" aria-label="Back to reference 10">↩</a></p>
</li>
<li id="user-content-fn-version-vectors">
<p>Version vectors were introduced by Parker et al. in <a href="https://www.cs.purdue.edu/homes/bb/cs542-11Spr/Parker_TSE83.pdf">“Detection of Mutual Inconsistency in Distributed Systems”</a> (1983). They extend <a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf">Lamport’s logical clocks</a> to track causality in distributed systems. Each replica maintains a vector of logical clocks (one for each replica), enabling precise causal ordering without requiring synchronized physical clocks. <a href="#user-content-fnref-version-vectors" data-footnote-backref="" aria-label="Back to reference 11">↩</a></p>
</li>
<li id="user-content-fn-delta-crdt-paper">
<p>Delta CRDTs were introduced by Almeida, Shoker, and Baquero in <a href="https://arxiv.org/abs/1603.01529">“Delta State Replicated Data Types”</a> (2018). They bridge the gap between state-based and operation-based CRDTs, achieving operation-based bandwidth efficiency while maintaining state-based simplicity. Most production CRDT systems (<a href="https://riak.com/">Riak</a>, <a href="https://automerge.org/">Automerge</a>) use delta-state internally. <a href="#user-content-fnref-delta-crdt-paper" data-footnote-backref="" aria-label="Back to reference 12">↩</a></p>
</li>
<li id="user-content-fn-logoot-paper">
<p>Logoot was introduced by Weiss, Urso, and Molli in <a href="https://hal.inria.fr/inria-00432368/document">“Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing”</a> (2009). The name combines “log” (logarithmic complexity) with “oot” from WOOT, its predecessor. Logoot’s position-based approach influenced many subsequent CRDTs including LSEQ and Treedoc. <a href="#user-content-fnref-logoot-paper" data-footnote-backref="" aria-label="Back to reference 13">↩</a></p>
</li>
<li id="user-content-fn-lseq-paper">
<p>LSEQ was introduced by Nédelec, Molli, Mostéfaoui, and Desmontils in <a href="https://hal.archives-ouvertes.fr/hal-00921633/document">“LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing”</a> (2013). The key innovation is using different allocation strategies (boundary+ vs boundary-) based on tree depth, which keeps position identifiers shorter in practice compared to Logoot’s fixed strategy. <a href="#user-content-fnref-lseq-paper" data-footnote-backref="" aria-label="Back to reference 14">↩</a></p>
</li>
<li id="user-content-fn-automerge">
<p><a href="https://automerge.org/">Automerge</a>, created by Martin Kleppmann and collaborators, implements a JSON CRDT described in <a href="https://arxiv.org/abs/1608.03960">“A Conflict-Free Replicated JSON Datatype”</a> (2017). It uses a columnar encoding for efficiency and has been <a href="https://github.com/automerge/automerge">rewritten in Rust</a> for performance. Used by production apps like <a href="https://www.inkandswitch.com/pushpin/">Inkandswitch’s Pushpin</a>. <a href="#user-content-fnref-automerge" data-footnote-backref="" aria-label="Back to reference 15">↩</a></p>
</li>
<li id="user-content-fn-yjs">
<p><a href="https://yjs.dev/">Yjs</a>, created by Kevin Jahns, is optimized for text editing and uses the YATA algorithm. It’s notably faster than Automerge for text operations and includes bindings for popular editors like <a href="https://codemirror.net/">CodeMirror</a>, <a href="https://microsoft.github.io/monaco-editor/">Monaco</a>, <a href="https://quilljs.com/">Quill</a>, and <a href="https://prosemirror.net/">ProseMirror</a>. <a href="#user-content-fnref-yjs" data-footnote-backref="" aria-label="Back to reference 16">↩</a></p>
</li>
<li id="user-content-fn-riak">
<p><a href="https://riak.com/">Riak</a>, a distributed database from Basho, was one of the first production systems to adopt CRDTs (2012). It implements counters, sets, and maps as <a href="https://docs.riak.com/riak/kv/latest/developing/data-types/index.html">native data types</a>, using Delta CRDTs internally to minimize bandwidth. Sadly, the company collapsed dramatically, and the project was abandoned for quite some time. I think it’s still around in a diminished form, but haven’t tried it in a while. <a href="#user-content-fnref-riak" data-footnote-backref="" aria-label="Back to reference 17">↩</a></p>
</li>
<li id="user-content-fn-redis-crdt">
<p><a href="https://redis.io/docs/latest/operate/rc/databases/configuration/active-active-redis/">Redis Enterprise’s CRDT support</a> (Active-Active deployment) uses operation-based CRDTs with causal consistency. It supports strings, hashes, sets, and sorted sets with CRDT semantics, enabling multi-master Redis deployments. <a href="#user-content-fnref-redis-crdt" data-footnote-backref="" aria-label="Back to reference 18">↩</a></p>
</li>
<li id="user-content-fn-antidote">
<p><a href="https://www.antidotedb.eu/">AntidoteDB</a> is a research database from the <a href="https://syncfree.lip6.fr/">SyncFree project</a> that makes CRDTs the primary abstraction. Unlike other databases where CRDTs are a feature, AntidoteDB is designed from the ground up around CRDT semantics, providing highly available transactions over CRDTs. <a href="#user-content-fnref-antidote" data-footnote-backref="" aria-label="Back to reference 19">↩</a></p>
</li>
</ol>
</section>   </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leak confirms OpenAI is preparing ads on ChatGPT for public roll out (557 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/</link>
            <guid>46086771</guid>
            <pubDate>Sat, 29 Nov 2025 11:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/">https://www.bleepingcomputer.com/news/artificial-intelligence/leak-confirms-openai-is-preparing-ads-on-chatgpt-for-public-roll-out/</a>, See on <a href="https://news.ycombinator.com/item?id=46086771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	   
<p><img alt="GPT" height="900" src="https://www.bleepstatic.com/content/hl-images/2023/03/24/ChatGPT-logo.jpg" width="1600"></p>


<p>OpenAI is now internally testing 'ads' inside ChatGPT that could redefine the web economy.</p>


<p>Up until now, the ChatGPT experience has been completely&nbsp;free. While there are premium plans and models, you don't see GPT sell you products or show ads.</p>


<p>As <a href="https://x.com/btibor91/status/1994714152636690834" target="_blank" rel="nofollow noopener">spotted </a>by Tibor on X,&nbsp;ChatGPT Android app 1.2025.329 beta includes new references to an "ads feature" with "bazaar content", "search ad" and "search ads carousel."</p>

<p><a href="https://www.wiz.io/lp/ai-data-security-best-practices-cheat-sheet?utm_source=bleepingcomputer&amp;utm_medium=display&amp;utm_campaign=FY26Q3_INB_Form_AI-Data-Security-Best-Practices&amp;sfcid=701Py00000SmgsrIAB&amp;utm_term=FY26Q4-bleepingcomputer-970x250&amp;utm_content=AI-Data-Security-BP" rel="nofollow noopener" target="_blank"><img src="https://www.bleepstatic.com/c/w/wiz/AI-Data-Security-970x250.png" alt="Wiz"></a>
</p>


<p>It is likely that ads will be limited to the search experience only, but that might change soon.</p>


<p>My understanding is that GPT ads could be highly personalised as the AI knows everything about you unless you disable the feature,</p>


<p><em>This is a developing story...</em></p>



	   


<div>
    <p><a href="https://www.wiz.io/lp/secrets-security-cheat-sheet?utm_source=bleepingcomputer&amp;utm_medium=display&amp;utm_campaign=FY26Q3_INB_FORM_Secret-Security-Sprawl-to-Control&amp;sfcid=701Py00000T0tF9IAJ&amp;utm_term=FY26Q4-bleepingcomputer-article-ad&amp;utm_content=Secrets-Security" target="_blank" rel="noopener sponsored">
            <img src="https://www.bleepstatic.com/c/w/wiz/Secrets-Security-512x512.png" alt="Wiz">
        </a>
    </p>
    <div>
        <h2><a href="https://www.wiz.io/lp/secrets-security-cheat-sheet?utm_source=bleepingcomputer&amp;utm_medium=display&amp;utm_campaign=FY26Q3_INB_FORM_Secret-Security-Sprawl-to-Control&amp;sfcid=701Py00000T0tF9IAJ&amp;utm_term=FY26Q4-bleepingcomputer-article-ad&amp;utm_content=Secrets-Security" target="_blank" rel="noopener sponsored">Secrets Security Cheat Sheet: From Sprawl to Control </a></h2>
<p>Whether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.</p>
<p>Get the cheat sheet and take the guesswork out of secrets management.</p>

        </div>
</div>

           
          
 
	  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Belgian Police exposed using botnets to manipulate EU data law impact assessment (178 pts)]]></title>
            <link>https://old.reddit.com/r/europe/comments/1p9kxhm/belgian_federal_police_forgot_to_turn_their_vpn/</link>
            <guid>46086681</guid>
            <pubDate>Sat, 29 Nov 2025 11:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/europe/comments/1p9kxhm/belgian_federal_police_forgot_to_turn_their_vpn/">https://old.reddit.com/r/europe/comments/1p9kxhm/belgian_federal_police_forgot_to_turn_their_vpn/</a>, See on <a href="https://news.ycombinator.com/item?id=46086681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p>EU is not a federation</p>
</blockquote>

<p>This part is arguable, because it isn't a federation indeed, but it's not <em>not</em> one either. </p>

<p>The EU doesn't check any box of existing institutions; it's a <em>sui generis</em> entity that mixes intergovernmental, confederal &amp; federal attributes. It's pretty much unique, and cannot really be compared to anything else.</p>

<p>But you're technically correct.</p>



<blockquote>
<p>EU has actually quite little power. </p>
</blockquote>

<p>This part is just downright bullshit, however.</p>

<p>The EU literally has more exclusive regulatory power than the US federal government in several key areas, notably market regulation, labour legislation, environment regulation, and a few others, which are all very much central to the attributes of sovereign states.</p>



<p>That being said, your answer is pretty much irrelevant to the question the OP asked, though. You're missing his point entirely, and it feels like you just wanted to take a jab at the EU.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[High air pollution could diminish exercise benefits by half – study (175 pts)]]></title>
            <link>https://scienceclock.com/exercise-may-protect-less-when-air-pollution-is-high-study-finds/</link>
            <guid>46086624</guid>
            <pubDate>Sat, 29 Nov 2025 10:54:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scienceclock.com/exercise-may-protect-less-when-air-pollution-is-high-study-finds/">https://scienceclock.com/exercise-may-protect-less-when-air-pollution-is-high-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=46086624">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p>We all know exercise is good for us. It lowers the risk of heart disease, cancer, and early death, and keeps the body and mind in shape. But when the air you breathe is polluted, a <a href="https://doi.org/10.1186/s12916-025-04496-y" target="_blank" rel="noopener">new study</a> suggests that exercise might not provide the same benefits it normally would.</p>



<p>An international team, including researchers from University College London (UCL) analyzed health data from over 1.5 million adults, collected over more than a decade in countries including the UK, Taiwan, China, Denmark, and the United States.</p>



<p>The researchers focused on levels of fine <a href="https://en.wikipedia.org/wiki/Particulate_matter" target="_blank" rel="noopener">particulate matter</a>, specifically tiny particles known as <a href="https://link.springer.com/article/10.1186/s12916-025-04496-y" target="_blank" rel="noopener">PM<sub>2.5</sub></a>. These fine particles are smaller than 2.5 micrometers; they can get stuck in the lungs and enter the bloodstream, where they can trigger inflammation and long-term damage.</p>



<p><strong>Also Read: </strong><a href="https://scienceclock.com/humans-have-titled-the-earth-31-5-inches-since-1993-study-finds/"><strong>Humans Have Tilted the Earth 31.5 Inches Since 1993, Study Finds</strong></a></p>




<p>Researchers found that adults who exercised at least two and a half hours a week—moderate to vigorous activity like jogging or other sports—typically had a 30% lower risk of dying during the study period than less active people. But in areas where the yearly average PM<sub>2.5</sub> exceeded 25 μg/m³, the protective effects of exercise dropped to just 12–15%.</p>



<p>The protective effects of <a href="https://scienceclock.com/why-audiences-doubt-the-most-attractive-fitness-influencers-according-to-new-research/">exercise</a> weaken even further in more polluted regions. At PM<sub>2.5</sub> levels above 35 μg/m³, where about a third (<a href="https://www.eurekalert.org/news-releases/1107581#:~:text=At%20higher%20levels%20of%20fine%20particle%20pollution%2C%20above%2035%20%CE%BCg/m%C2%B3%2C%20the%20benefits%20of%20exercise%20weakened%20further%2C%20particularly%20for%20risk%20of%20death%20from%20cancer%2C%20where%20the%20benefits%20were%20no%20longer%20robust.%20About%20a%20third%20of%20the%20world%E2%80%99s%20population%20(36%25)%20live%20in%20areas%20whose%20yearly%20average%20PM2.5%20levels%20exceed%2035%20%CE%BCg/m%C2%B3." target="_blank" rel="noopener">36%</a>) of the global population lives, exercise offered even less protection, particularly against the risk of death from cancer.</p>



<p>“Our findings emphasise that exercise remains beneficial even in polluted <a href="https://scienceclock.com/category/environment/">environments</a>,” lead researcher Professor Po-Wen Ku <a href="https://www.eurekalert.org/news-releases/1107581" target="_blank" rel="noopener">said in a statement</a>. “But improving air quality can significantly enhance these health gains.”</p>



<figure><img decoding="async" width="1024" height="682" src="https://scienceclock.com/wp-content/uploads/2025/11/5530768-1024x682.jpg" alt="man, training, lazy, tired, fitness, exercise, motivation, gym, workout, male, sport, active, activity, fit, stress, stressed, burnout, lazy, lazy, lazy, lazy, lazy" srcset="https://scienceclock.com/wp-content/uploads/2025/11/5530768-1024x682.jpg 1024w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-300x200.jpg 300w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-768x512.jpg 768w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-330x220.jpg 330w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-420x280.jpg 420w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-615x410.jpg 615w, https://scienceclock.com/wp-content/uploads/2025/11/5530768-860x573.jpg 860w, https://scienceclock.com/wp-content/uploads/2025/11/5530768.jpg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>(Photo by <a href="https://pixabay.com/users/Sammy-Sander-10634669/?utm_source=instant-images&amp;utm_medium=referral" target="_blank" rel="noopener noreferrer">Sammy-Sander</a> on <a href="https://pixabay.com/" target="_blank" rel="noopener noreferrer">Pixabay</a>)</figcaption></figure>



<p>Co-author Professor Andrew Steptoe <a href="https://www.eurekalert.org/news-releases/1107581" target="_blank" rel="noopener">said</a>, “Our study shows that toxic air can, to some extent, block the benefits of exercise, although not eliminate them. The findings are further evidence of the damage that fine particle pollution can do to <a href="https://scienceclock.com/category/health/">our health</a>.</p>




<p>“We believe clean air and physical activity are both important for healthy ageing, and so we encourage greater efforts to curb health-harming pollution levels.”</p>



<p>The study looked at the data from seven existing studies, including three previously unpublished datasets, combining both summary statistics and raw participant-level data. Researchers carefully accounted for a wide range of other factors, including income, education, smoking, and pre-existing chronic conditions. </p>



<p><strong>Also Read: </strong><a href="https://scienceclock.com/men-need-more-exercise-than-women-to-lower-the-heart-disease-risk/"><strong>Men Need More Exercise Than Women to Lower the Heart Disease Risk</strong></a></p>



<p>However, the team points out that their data mostly comes from high-income countries, so the impact could be greater in low-income regions, where PM<sub>2.5</sub> levels often exceed 50 μg/m³. They also mention the lack of indoor air quality data and limited information on participants’ diets as part of the study’s caveats.</p>




<p>“We don’t want to discourage people from exercising outdoors,” <a href="https://www.eurekalert.org/news-releases/1107581" target="_blank" rel="noopener">said </a>Co-author Professor Paola Zaninotto. “Checking air quality, choosing cleaner routes, or easing off intensity on polluted days can help you get the most health benefits from your exercise.”</p>



<p>The study reminds us of one of the world’s most serious problems: <a href="https://scienceclock.com/carbon-offsets-play-a-negligible-role-in-corporate-climate-action-and-emission-reductions/">air pollution</a>. Staying active is not enough to protect your health if the air around you is toxic. Cleaner air and regular exercise go hand in hand, and tackling pollution is not just about the environment—it’s about our bodies too.</p>



<p>The study was published in <a href="https://doi.org/10.1186/s12916-025-04496-y" target="_blank" rel="noopener"><em>BMC Medicine</em></a>. </p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
    </channel>
</rss>