<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 16 Mar 2024 09:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The United States has its first large offshore wind farm, with more to come (132 pts)]]></title>
            <link>https://apnews.com/article/orsted-offshore-wind-new-york-south-fork-climate-cbb9360388d91be1368dd91ba35aa384</link>
            <guid>39721158</guid>
            <pubDate>Fri, 15 Mar 2024 22:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/orsted-offshore-wind-new-york-south-fork-climate-cbb9360388d91be1368dd91ba35aa384">https://apnews.com/article/orsted-offshore-wind-new-york-south-fork-climate-cbb9360388d91be1368dd91ba35aa384</a>, See on <a href="https://news.ycombinator.com/item?id=39721158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>America’s first commercial-scale offshore wind farm is officially open, a long-awaited moment that helps pave the way for a succession of large wind farms.</p><p>Danish wind energy developer Ørsted and the utility Eversource built a 12-turbine wind farm called South Fork Wind 35 miles (56 kilometers) east of Montauk Point, New York. New York Gov. Kathy Hochul went to Long Island Thursday to announce that the turbines are delivering clean power to the local electric grid, flipping a massive light switch to “turn on the future.” Interior Secretary Deb Haaland was also on hand.</p><p>Achieving commercial scale is a turning point for the industry, but what’s next? Experts say the nation needs a major buildout of this type of clean electricity to address climate change. </p><p>Offshore wind is central to both <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/binde-offshore-wind-power-plan-announcement-1cfa8537ac0dafa87f2d388502f53e18">national</a></span> and <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/new-york-offshore-wind-projects-f712af2d09f896b2dba04c3bc1edaa06">state plans</a></span> to transition to a carbon-free electricity system. The Biden administration has approved six commercial-scale offshore wind energy projects, and auctioned lease areas for offshore wind for the first time off the Pacific and Gulf of Mexico coasts. New York picked two more projects last month to power more than 1 million homes. </p>
    

<p>This is just the beginning, Hochul said. She said the completion of South Fork shows that New York will aggressively pursue climate change solutions to save future generations from a world that otherwise could be dangerous. <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/offshore-wind-farm-orsted-eversource-biden-south-fork-new-york-a94722b3f4a52e93580ad15a2de257a0">South Fork can generate 132 megawatts</a></span> of offshore wind energy to power more than 70,000 homes. </p>



<p>“It’s great to be first, we want to make sure we’re not the last. That’s why we’re showing other states how it can be done, why we’re moving forward, on to other projects,” Hochul told The Associated Press in an exclusive interview before the announcement. </p>
    
<p>“This is the date and the time that people will look back in the history of our nation and say, ‘This is when it changed,’” Hochul added.</p>
    

<p>South Fork will generate more than four times the power of a five-turbine pilot project developed earlier off the coast of Rhode Island, and unlike that subsidized test project, was developed after Orsted and Eversource were chosen in a competitive bidding process to supply power to Long Island. The Long Island Power Authority first approved this project in 2017. The blades for the 12 Siemens Gamesa turbines reach speeds of more than 200 miles per hour (350 kilometers per hour). </p><p>Ørsted CEO Mads Nipper called the opening a major milestone that proves large offshore wind farms can be built, both in the United States and in other countries with little or no offshore wind energy currently. </p><p>With South Fork finished, Ørsted and Eversource are turning their attention to the work they will do offshore beginning this spring for a wind farm more than five times its size. Revolution Wind will be Rhode Island and Connecticut’s first commercial-scale offshore wind farm, capable of powering more than 350,000 homes next year. The site where the cable will connect in Rhode Island is already under construction.</p><p>In New York, the state said last month it would negotiate a contract with Ørsted and Eversource for an even larger wind farm, Sunrise Wind, to power 600,000 homes. The Norwegian company Equinor was picked for its Empire Wind 1 project to power more than 500,000 New York homes. Both aim to start providing power in 2026. </p>
    

<p>After years of planning and development, 2024 is a year of action— building projects that will deliver sizeable amounts of clean power to the grid, said David Hardy, group executive vice president and CEO Americas at Ørsted.</p><p>Ørsted, formerly DONG Energy, for Danish Oil and Natural Gas, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/offshore-wind-power-orsted-mads-nipper-eed40962a99ad867e090fd90a15cbff6">started aggressively building wind farms</a></span> off the coast of Denmark, the U.K. and Germany in 2008. The company sold off the North Sea oil and gas assets on which it had built its identity to focus on clean energy, becoming Ørsted. It’s now one of the biggest wind power developers.</p><p>The first U.S. offshore wind farm was supposed to be a project off the coast of Massachusetts known as Cape Wind. A Massachusetts developer proposed the project in 2001. It failed after years of local opposition and litigation. </p>
    

<p>Turbines began spinning off Rhode Island’s Block Island as a pilot project in 2016. But with just five of them, it’s not a commercial-scale wind farm.</p><p><span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/offshore-wind-orsted-cancellation-biden-new-jersey-3f2ff7c9832210ce862f6e7179fae439">Last year brought challenges</a></span> for the nascent U.S. offshore wind industry, as Ørsted and other developers canceled projects in the Northeast that they said were no longer financially feasible. High inflation, supply chain disruptions and the rising cost of capital and building materials were making projects more expensive as developers were trying to get the first large U.S. offshore wind farms opened. </p><p>Industry leaders expect 2024 to be a better year, as interest rates come down and states ask for more offshore wind to meet their climate goals. </p><p>The nation’s second large offshore wind farm, Vineyard Wind, is expected to open later this year off the coast of Massachusetts, too. <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/offshore-wind-electricity-vineyard-turbines-power-grid-181842ca65a801b2cdea5e434e8a5f28">The first five turbines are providing power</a></span> for about 30,000 homes and businesses in Massachusetts. When all 62 turbines are spinning, they’ll generate enough electricity for 400,000 homes and businesses. Avangrid and Copenhagen Infrastructure Partners are the joint owners of that project.</p>
    

<p>The Biden administration wants enough offshore wind energy to power 10 million homes by 2030. Interior Secretary Haaland said that “America’s clean energy transition is not a dream for a distant future— it’s happening right here and right now.” </p><h2>___</h2><p>The Associated Press’ climate and environmental coverage receives financial support from multiple private foundations. AP is solely responsible for all content. Find AP’s <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.ap.org/about/standards-for-working-with-outside-groups/" target="_blank" rel="noopener">standards</a></span> for working with philanthropies, a list of supporters and funded coverage areas at <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.ap.org/discover/Supporting-AP" target="_blank" rel="noopener">AP.org</a></span>.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GhostRace: Exploiting and mitigating speculative race conditions (104 pts)]]></title>
            <link>https://www.vusec.net/projects/ghostrace/</link>
            <guid>39720508</guid>
            <pubDate>Fri, 15 Mar 2024 20:43:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vusec.net/projects/ghostrace/">https://www.vusec.net/projects/ghostrace/</a>, See on <a href="https://news.ycombinator.com/item?id=39720508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" role="main">

			
<article id="post-2940" class="page">
	<!-- .entry-header -->
	<div>
		
<p><strong>Exploiting and Mitigating Speculative Race Conditions</strong></p>



<h3><a href="https://www.cve.org/CVERecord?id=CVE-2024-2193" target="_blank" rel="noreferrer noopener">GhostRace: CVE-2024-2193</a></h3>



<p>Race conditions arise when multiple threads attempt to access a shared resource without proper synchronization, often leading to vulnerabilities such as concurrent use-after-free. To mitigate their occurrence, operating systems rely on synchronization primitives such as mutexes, spinlocks, etc.</p>



<p><strong>In this work, we present GhostRace, the first security analysis of these primitives on speculatively executed code paths.</strong></p>



<p><strong>Our key finding is that all the common synchronization primitives implemented using conditional branches (<a href="#mutex-lock">Figure 1</a>) can be microarchitecturally bypassed on speculative paths using a Spectre-v1 attack, turning all architecturally race-free critical regions into Speculative Race Conditions (SRCs), allowing attackers to leak information from the target software.</strong></p>


<div>
<figure id="mutex-lock"><img decoding="async" src="https://lh7-us.googleusercontent.com/A5mtFcjtk0myeLsZ-QqmBi7iFv_h_3wrIdzAVSwGwRiFe35KkzdgkIIiCjko7ORcEoCf7B8-PvbAlpf8nJ92oFNDS3uFdaVSm24VCzdK5dSBPSK-WUKDkXSRf-bWNxK6-Bru6g4W0iC8zA8x3UFcy9Q" alt="Figure 1: Top part: The core implementation of the mutex_lock synchronization primitive in the Linux x86-64 kernel, with the conditional branch that can be abused to craft SRCs in red. Bottom part: The branch ultimately checks the outcome of the lock cmpxchgq instruction, which does not serialize the execution." title="Figure 1"><figcaption><em>Figure 1: Top part: The core implementation of the mutex_lock synchronization primitive in the Linux x86-64 kernel, with the conditional branch that can be abused to craft SRCs in red. Bottom part: The branch ultimately checks the outcome of the lock cmpxchgq instruction, which does not serialize the execution.</em></figcaption></figure></div>


<hr>



<p><strong>Our <a href="http://download.vusec.net/papers/ghostrace_sec24.pdf" target="_blank" rel="noreferrer noopener">GhostRace Paper (PDF)</a> is accepted for publication at the 33rd USENIX Security Symposium 2024.</strong> <strong>This is a joint project with the <a href="https://ibm.github.io/system-security-research-updates/2024/03/12/ghostrace" target="_blank" rel="noreferrer noopener">Systems Security Research Group</a> at IBM Research Europe</strong>.</p>







<hr>



<h2>Speculative Synchronization Primitives</h2>



<p><strong>Our analysis shows <em>all </em>the other common write-side synchronization primitives in the Linux kernel are ultimately implemented through a conditional branch and are therefore vulnerable to speculative race conditions. </strong></p>



<p>To experimentally confirm this intuition, we tested all such synchronization primitives under speculative execution after mistraining the vulnerable branch. <strong>In all cases, we confirmed transient execution of the guarded critical region despite another victim thread already architecturally executing in the region.</strong> To determine the transient window size, we measured the maximum number of speculative load instructions we could speculatively execute inside the critical region (<a href="#speculative-window">Figure 2</a>).</p>


<div>
<figure id="speculative-window"><img decoding="async" src="https://lh7-us.googleusercontent.com/W2JoafY-Chy0JQdu9ihuVmr0G2gq3U4P4RAjvjVbZXGSqvG6Qy2MDPB6SUWb1UnYEHwM0rWRbl33eND2PK9oIS0mGWPxvPy7KfOca9oGK5Ewfe8MFPMYkz2TM6rjmAX6x4pZA7O_aGcACISPdxgYo1U" alt="Figure 2: Transient window size for different write-side synchronization mechanisms, i.e., the number of speculative loads that leave an observable microarchitectural trace." title="Figure 2"><figcaption><em>Figure 2: Speculative window size for different write-side synchronization mechanisms, i.e., the number of speculative loads that leave an observable microarchitectural trace.</em></figcaption></figure></div>


<h2>SCUAF Gadget Scanner</h2>



<p>To investigate the severity of SRCs, <strong>we concentrate on Speculative Concurrent Use-After-Free (SCUAF) and statically scan the Linux kernel with Coccinelle (<a href="#cocci-script">Figure 3</a>),</strong> <strong>discovering 1,283 potentially exploitable gadgets</strong>.</p>


<div>
<figure id="cocci-script"><img decoding="async" src="https://lh7-us.googleusercontent.com/k-oVgKvuKIRcC7XY9rj2RwuZKN54Dri-xlgnKnGXf5fDlNeptlXJI8F-yZSkpfi_1r3x2RnsCxjyAynYRgt_jCOBKOUskRZfC2Rxpy1ko3hWRSVZo_mr0JdMV34y235LlLOVpOQ9K01R_rckQ-0AFWE" alt="Figure 3: Simplified Cocci scripts (left Free and right Use) scanning for SCUAF gadgets in the Linux kernel." title="Figure 3"><figcaption><em>Figure 3: Simplified Cocci scripts (left Free and right Use) scanning for SCUAF gadgets in the Linux kernel.</em></figcaption></figure></div>


<h3><a href="https://lore.kernel.org/lkml/2024022614-unhappily-python-2cd0@gregkh/" target="_blank" rel="noreferrer noopener">IPI Storming: CVE-2024-26602</a></h3>



<p>To win an SRC, we need to interrupt the execution of the victim process at the right point (i.e., when the dangling pointer is created), and keep the victim there forever so that the attacker can perform the SCUAF attack. In order to achieve this, we created a new exploitation technique called Inter-Process Interrupt (IPI) Storming, which consists of infinitely flooding the victim process’s CPU core with IPIs once interrupted so that it never finishes handling the incoming interrupts, <strong>resulting in creating an unbounded exploitation window that allows the attacker to execute an arbitrary number of SCUAF invocations to mount an end-to-end attack within a single race window.</strong> In <a href="#IPI-storming">Figure 4</a> we show how the increasing number of storming SMTs widens the UAF exploitation window.</p>


<div>
<figure id="IPI-storming"><img decoding="async" src="https://lh7-us.googleusercontent.com/MKaHNmZBeMuo9s0UQxTYuDHsvBHGF5ESIx38LAav9IXwcfpG_CG3UULkDrWFNzyjjrydVIHnFz6O172oktSAiegAkwsqeSHiv_NsTjtdFYM1SVsBsStW4TpdawvWe12sXRrIuhBoiBjINO969rMiI4k" alt="Figure 4: Size of the UAF exploitation window vs. number of IPI storming cores targeting the victim core." title="Figure 4"><figcaption><em>Figure 4: Size of the UAF exploitation window vs. number of IPI storming cores targeting the victim core.</em> Our test CPU contains 16 cores and 24 SMTs.</figcaption></figure></div>


<h3>SCUAF Information Disclosure Attacks</h3>



<p>Furthermore, we show that SCUAF information disclosure attacks (<a href="#SCUAF-attack">Figure 5</a>) on the kernel are feasible and can match the reliability of typical Spectre attacks, with <strong>our proof of concept leaking kernel memory at 12 KB/s</strong>.</p>


<div>
<figure id="SCUAF-attack"><img decoding="async" src="https://lh7-us.googleusercontent.com/fwbNm1aDJw8BM6BTUTesMeblQ3_umrNPB86olhS8i8HDwionIkY1lXfnzn6DhPXp4SDQRjdYeqO1YK2ry_j-A5DPqYOrhlQRBbEEzVgq9P-OikA5ZBc6gTx0FKHoP86RgRCvlQ_q1MhE8PdPZE0lEbg" alt="Figure 5: Speculative information disclosure attack exploiting a speculative race condition. Steps 1-4 and 8-10 run in user mode, issuing syscalls to trigger the relevant kernel code. The other steps run in kernel mode. Our gadget scanner identified the nfc_hci_msg_tx_work function as a SCUAF gadget in the Linux kernel." title="Figure 5"><figcaption><em>Figure 5: Speculative information disclosure attack exploiting a speculative race condition. Steps 1-4 and 8-10 run in user mode, issuing syscalls to trigger the relevant kernel code. The other steps run in kernel mode. Our gadget scanner identified the nfc_hci_msg_tx_work function as a SCUAF gadget in the Linux kernel.</em></figcaption></figure></div>


<h2 id="code">Code</h2>



<p>You can find a minimalistic PoC exemplifying the concept of SRC in a step-by-step single-threaded fashion, Coccinelle SCUAF-scanning scripts, and 1200+ SCUAF gadgets found in the Linux kernel at <a href="https://github.com/vusec/ghostrace" target="_blank" rel="noreferrer noopener">https://github.com/vusec/ghostrace</a></p>



<h2 id="affected-hw-sw">Affected Hardware &amp; Software</h2>



<p>While we have explicitly focused on x86 and Linux in the paper, SRCs also affect other hardware and software targets.</p>



<p><strong>Hardware</strong>: We have confirmed that all the major hardware vendors are affected by SRCs since, regardless of the particular compare-and-exchange instruction implementation, the conditional branch that follows is subject to branch (mis)prediction. In other words, all the microarchitectures affected by Spectre-v1 are also affected by SRCs.</p>



<p><strong>Software</strong>: Any target relying on conditional branches to determine whether to enter critical regions—a common design pattern that extends well beyond Linux—is vulnerable to SRCs.</p>



<p><strong>In summary, any software, e.g., operating system, hypervisor, etc., implementing synchronization primitives through conditional branches without any serializing instruction on that path and running on any microarchitecture (e.g., x86, ARM, RISC-V, etc.), which allows conditional branches to be speculatively executed, is vulnerable to SRCs. As in other speculative execution attacks, this allows leaking data from the target software.</strong></p>



<h2 id="mitigation">Mitigation</h2>



<p>To address the new attack surface,<strong> we also propose a generic SRC mitigation to serialize all the affected synchronization primitives on Linux </strong>(i.e., adding an <code>lfence</code> instruction after the <code>lock cmpxchq</code> in <a href="#mutex-lock">Figure 1</a>)<strong>.</strong> Our mitigation requires minimal kernel changes <strong>(i.e., 2 LoC)</strong> <strong>and incurs only ≈5% geomean performance overhead on LMBench.</strong></p>



<h2 id="disclosure">Disclosure</h2>



<p>We disclosed Speculative Race Conditions to the major hardware vendors (Intel, AMD, ARM, and IBM) and the Linux kernel in late 2023.</p>



<p>Hardware vendors have further notified other affected software (OS/hypervisors) vendors, and all parties have acknowledged the reported issue (<a href="https://www.cve.org/CVERecord?id=CVE-2024-2193" target="_blank" rel="noreferrer noopener"><strong>CVE-2024-2193</strong></a>). Specifically, AMD responded with an explicit impact statement (i.e., “existing [Spectre-v1] mitigations apply”), pointing to the attacks relying on conditional branch mis-speculation, like Spectre-v1.</p>



<p><strong>The Linux kernel developers have no immediate plans to implement our proposed serialization of synchronization primitives due to performance concerns</strong>. However, they confirmed the IPI storming issue (<a href="https://lore.kernel.org/lkml/2024022614-unhappily-python-2cd0@gregkh/" target="_blank" rel="noreferrer noopener"><strong>CVE-2024-26602</strong></a>) and <strong>implemented an </strong><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=944d5fe50f3f03daacfea16300e656a1691c4a23" target="_blank" rel="noreferrer noopener"><strong>IPI rate-limiting feature</strong></a> to address the CPU saturation issue by adding a synchronization mutex on the path of sys_membarrier and avoiding its concurrent execution on multiple cores. <strong>Unfortunately, as our experiments show (Figure 4), hindering IPI storming primitives (i.e., 0 storming cores) is insufficient to close the attack surface completely.</strong></p>



<h2>Acknowledgments</h2>



<p>We would like to thank the anonymous reviewers for their feedback, Andrew Cooper for his early comments on the paper, Julia Lawall for the Coccinelle clarifications, and Alessandro Sorniotti for the early discussions about the project. This work was partially supported by Intel Corporation through the “Allocamelus” project, by the Dutch Research Council (NWO) through project “INTERSECT”, and by the European Union’s Horizon Europe program under grant agreement No. 101120962 (“Rescale”).</p>
	</div><!-- .entry-content -->
</article><!-- #post-2940 -->

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Great Ideas in Theoretical Computer Science (220 pts)]]></title>
            <link>https://www.cs251.com</link>
            <guid>39720388</guid>
            <pubDate>Fri, 15 Mar 2024 20:30:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs251.com">https://www.cs251.com</a>, See on <a href="https://news.ycombinator.com/item?id=39720388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
<h2>Great Ideas in Theoretical Computer Science</h2>
<h2>Great Ideas in Theoretical Computer Science</h2>
<h2>Great Ideas in Theoretical Computer Science</h2>
<h2>Great Ideas in Theoretical Computer Science</h2>
<h2>Great Ideas in Theoretical Computer Science</h2>
</p>
</div><div>
<p><img src="https://www.cs251.com/static/images/cs251_pic.jpg">
</p>
<div>
<p>Welcome to <span>CS251</span> at <a href="https://www.cmu.edu/">CMU</a>!</p>
<p>This course is about the rigorous study of computation, which is a fundamental component of our universe, the societies we live in, the new technologies we discover, as well as the minds we use to understand these things. Therefore, having the right language and tools to study computation is important. In this course, we explore some of the central results and questions regarding the nature of computation.</p>
</div>
</div><div>



<div>
<p><span>MODULE 1</span>
</p>
<p><span>MODULE 1</span>
<span>Introduction</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Introduction.jpg">
</p>
</div>
<div>
<p>Welcome to CS251! In this module, our main goal is to explain at a high-level what theoretical computer science is about and set the right context for the material covered in the future.</p>
<p>In the first part of the course, we want to build up formally/mathematically, the important notions related to computation and algorithms. We start this journey here by discussing how to formally represent data and how to formally define the concept of a computational problem.</p>
</div>
</div>
<div>
<p><span>MODULE 2</span>
</p>
<p><span>MODULE 2</span>
<span>Finite Automata</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Finite_Automata.jpg">
</p>
</div>
<div>
<p>The goal of this module is to introduce you to a simple (and restricted) model of computation known as <em>deterministic finite automata</em> (DFA). This model is interesting to study in its own right, and has very nice applications, however, our main motivation to study this model is to use it as a stepping stone towards formally defining the notion of an <em>algorithm</em> in its full generality. Treating deterministic finite automata as a warm-up, we would like you to get comfortable with how one formally defines a model of computation, and then proves interesting theorems related to the model. Along the way, you will start getting comfortable with using a bit more sophisticated mathematical notation than you might be used to. You will see how mathematical notation helps us express ideas and concepts accurately, succinctly and clearly.</p>
</div>
</div>
<div>
<p><span>MODULE 3</span>
</p>
<p><span>MODULE 3</span>
<span>Formalizing Computation</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Formalizing_Computation.jpg">
</p>
</div>
<div>
<p>In this module, our main goal is to introduce the definition of a Turing machine, which is the standard mathematical model for any kind of computational device. As such, this definition is very foundational. As we discuss in lecture, the physical Church-Turing thesis asserts that any kind of physical device or phenomenon, when viewed as a computational process mapping input data to output data, can be simulated by some Turing machine. Thus, rigorously studying Turing machines does not just give us insights about what our laptops can or cannot do, but also tells us what the universe can and cannot do computationally. This module kicks things off with examples of computable problems. In the next module, we will start exploring the limitations of computation.</p>
</div>
</div>
<div>
<p><span>MODULE 4</span>
</p>
<p><span>MODULE 4</span>
<span>Limits of Computation</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Limits_of_Computation.jpg">
</p>
</div>
<div>
<p>In this module, we prove that most problems are undecidable, and give some explicit examples of undecidable problems. The two key techniques we use are diagonalization and reductions. These are two of the most fundamental concepts in mathematics and computer science.</p>
</div>
</div>
<div>
<p><span>MODULE 5</span>
</p>
<p><span>MODULE 5</span>
<span>Limits of Human Reasoning</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Limits_of_Human_Reasoning.jpg">
</p>
</div>
<div>
<p>The late 19th to early 20th century was an important time in mathematics. With various problems arising with the usual way of doing mathematics and proving things, it became clear that there was a need to put mathematical reasoning on a secure foundation. In other words, there was a need to mathematically formalize mathematical reasoning itself. As mathematicians took on the task of formalizing mathematics, two things started to become clear. First, a complete formalization of mathematics was not going to be possible. Second, formalization of mathematics involves formalizing what we informally understand as “algorithm” or “computation”. This is because one of the defining features of mathematical reasoning is that it is a computation. In this module we will make this connection explicit and see how the language of theoretical computer science can be effectively used to answer important questions in the foundations of mathematics.</p>
</div>
</div>



<div>
<p><span>MODULE 6</span>
</p>
<p><span>MODULE 6</span>
<span>Time Complexity</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Time_Complexity.jpg">
</p>
</div>
<div>
<div>
<p>So far, we have formally defined what a computational/decision problem is, what an algorithm is, and saw that most (decision) problems are undecidable. We also saw some explicit and interesting examples of undecidable problems. Nevertheless, it turns out that many problems that we care about are actually decidable. So the next natural thing to study is the computational complexity of problems. If a problem is decidable, but the most efficient algorithm solving it takes vigintillion computational steps even for reasonably sized inputs, then practically speaking, that problem is still undecidable. In a sense, computational complexity is the study of practical computability.</p>
<p>Even though computational complexity can be with respect to various resources like time, memory, randomness, and so on, we will be focusing on arguably the most important one: time complexity. In this module, we will set the right context and language to study time complexity.</p>
</div>

<div>
<p>
To be added (under construction).
</p>
</div>
</div>
</div>
<div>
<p><span>MODULE 7</span>
</p>
<p><span>MODULE 7</span>
<span>Graph Theory</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Graph_Theory.jpg">
</p>
</div>
<div>
<div>
<p>In the study of computational complexity of languages and computational problems, graphs play a very fundamental role. This is because an enormous number of computational problems that arise in computer science can be abstracted away as problems on graphs, which model pairwise relations between objects. This is great for various reasons. For one, this kind of abstraction removes unnecessary distractions about the problem and allows us to focus on its essence. Second, there is a huge literature on graph theory, so we can use this arsenal to better understand the computational complexity of graph problems. Applications of graphs are too many and diverse to list here, but we’ll name a few to give you an idea: communication networks, finding shortest routes in various settings, finding matchings between two sets of objects, social network analysis, kidney exchange protocols, linguistics, topology of atoms, and compiler optimization.</p>
<p>This module introduces basic graph theoretic concepts as well as some of the fundamental graph algorithms.</p>
</div>

<div>
<p>
To be added (under construction).
</p>
</div>
</div>
</div>
<div>
<p><span>MODULE 8</span>
</p>
<p><span>MODULE 8</span>
<span>P vs NP</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_P_vs_NP.jpg">
</p>
</div>
<div>
<div>
<p>In this module, we introduce the complexity class NP and discuss the most important open problem in computer science: the P vs NP problem. The class NP contains many natural and well-studied languages that we would love to decide in polynomial time. In particular, if we could decide the languages in NP efficiently, this would lead to amazing applications. For instance, in mathematics, proofs to theorems with reasonable length proofs would be found automatically by computers. In artificial intelligence, many machine learning tasks we struggle with would be easy to solve (like vision recognition, speech recognition, language translation and comprehension, etc). Many optimization tasks would become efficiently solvable, which would affect the economy in a major way. Another main impact would happen in privacy and security. We would say “bye” to public-key cryptography which is being used heavily on the internet today. (We will learn about public-key cryptography in a later module.) These are just a few examples; there are many more.</p>
<p>Our goal in this module is to present the formal definition of NP, and discuss how it relates to P. We also discuss the notion of NP-completeness (which is intimately related to the question of whether NP equals P) and give several examples of NP-complete languages.</p>
</div>

<div>
<p>
To be added (under construction).
</p>
</div>
</div>
</div>
<div>
<p><span>MODULE 9</span>
</p>
<p><span>MODULE 9</span>
<span>Randomized Algorithms</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Randomized_Algorithms.jpg">
</p>
</div>
<div>
<div>
<p>Randomness is an essential concept and tool in modeling and analyzing nature. Therefore, it should not be surprising that it also plays a foundational role in computer science. For many problems, solutions that make use of randomness are the simplest, most efficient and most elegant solutions. And in many settings, one can prove that randomness is absolutely required to achieve a solution. (We mention some concrete examples in lecture.)</p>
<p>One of the primary applications of randomness to computer science is randomized algorithms. A randomized algorithm is an algorithm that has access to a randomness source like a random number generator, and a randomized algorithm is allowed to err with a very small probability of error. There are computational problems that we know how to solve efficiently using a randomized algorithms, however, we do not know how to solve those problems efficiently with a deterministic algorithm (i.e. an algorithm that does not make use of randomness). In fact, one of the most important open problems in computer science asks whether every efficient randomized algorithm has a deterministic counterpart solving the same problem. In this module, we start by reviewing probability theory, and then introduce the concept of randomized algorithms.</p>
</div>

<div>
<p>
To be added (under construction).
</p>
</div>
</div>
</div>
<div>
<p><span>MODULE 10</span>
</p>
<p><span>MODULE 10</span>
<span>Cryptography</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Cryptography.jpg">
</p>
</div>
<div>
<div>
<p>The quest for secure communication in the presence of adversaries is an ancient one. From Caesar shift to the sophisticated Enigma machines used by Germans during World War 2, there have been a variety of interesting cryptographic protocols used in history. But it wasn’t until the computer science revolution in the mid 20th century when the field of cryptography really started to flourish. In fact, it is fair to say that the study of computational complexity completely revolutionized cryptography. The key idea is to observe that any adversary would be computationally bounded just like anyone else. And we can exploit the computational hardness of certain problems to design beautiful cryptographic protocols for many different tasks. In this module, we will first review the mathematical background needed (modular arithmetic), and then present some of the fundamental cryptographic protocols to achieve secure communication.</p>
</div>

<div>
<p>
To be added (under construction).
</p>
</div>
</div>
</div>

<div>
<p>
Highlights of Theoretical Computer Science
</p>
</div>

<div>
<p><span>MODULE 11</span>
</p>
<p><span>MODULE 11</span>
<span>Extra Topics</span>
</p>
</div>
<div>
<div>
<p><img src="https://www.cs251.com/static/images/Module_Extra_Topics.jpg">
</p>
</div>
<div>
<div>
<p>In this module, we present a selection of highlights from theoretical computer science.</p>
</div>

<div>
<p>
To be added (under construction).
</p>
</div>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig, Rust, and Other Languages (102 pts)]]></title>
            <link>https://notes.eatonphil.com/2024-03-15-zig-rust-and-other-languages.html</link>
            <guid>39720187</guid>
            <pubDate>Fri, 15 Mar 2024 20:11:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.eatonphil.com/2024-03-15-zig-rust-and-other-languages.html">https://notes.eatonphil.com/2024-03-15-zig-rust-and-other-languages.html</a>, See on <a href="https://news.ycombinator.com/item?id=39720187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>Having worked a bit in Zig, Rust, Go and now C, I think there are a
few common topics worth having a fresh conversation on: automatic
memory management, the standard library, and explicit allocation.</p>
<p>Zig is not a mature language. But it has made enough useful choices
for a number of companies to invest in it and run it in
production. The useful choices make Zig worth talking about.</p>
<p>Go and Rust are mature languages. But they have both made questionable
choices that seem worth talking about.</p>
<p>All of these languages are developed by highly intelligent folks I
personally look up to. And your choice to use any one of these is
certainly fine, whichever it is.</p>
<p>The positive and negative choices particular languages made, though, are
worth talking about as we consider what a systems programming language
10 years from now would look like. Or how these languages themselves
might evolve in the next 10 years.</p>
<p>My perspective is mostly building distributed databases. So the points
that I bring up may have no relevance to the kind of work you do, and
that's alright. Moreover, I'm already aware most of these opinions are
not shared by the language maintainers, and that's ok too. I am not
writing to convince anyone.</p>
<h3 id="automatic-memory-management">Automatic memory management</h3><p>One of my bigger issues with Zig is that it doesn't support RAII. You
can defer cleanup to the end of a block; and this is half of the
problem. But only RAII will allow for smart pointers and automatic
(not manual) reference counting. RAII is an excellent option to
default to, but in Zig you aren't allowed to. In contrast, even C
"supports" automatic cleanup (via compiler extensions).</p>
<p>But most of the time, arenas are fine. Postgres is written in C and
memory is almost entirely managed through nested arenas (called
"memory contexts") that get cleaned up when some subset of a task
finishes, recursively. Zig has builtin support for arenas, which is
great.</p>
<h3 id="standard-library">Standard library</h3><p>It seems regrettable that some languages have been shipping smaller
standard libraries. Smaller standard libraries seem to encourage users
of the language to install more transitively-unvetted third-party
libraries, which increases build time and build flakiness, and which
increases bitrot over time as unnecessary breaking changes occur.</p>
<p>People have been making jokes about <code>node_modules</code> for a decade now, but
this problem is just as bad in Rust codebases I've seen. And to a
degree it happens in Java and Go as well, though their larger standard
libraries allow you to get further without dependencies.</p>
<p>Zig has a good standard library, which may be Go and Java tier in a
few years. But one goal of their package manager seemed to be
to allow the standard library to be broken up; made smaller. For
example, JSON support moving out of the standard library into a
package. I don't know if that is actually the planned direction. I
hope not.</p>
<p>Having a large standard library doesn't mean that the programmer
shouldn't be able to swap out implementations easily as needed. But
all that is required is for the standard library to define an
<strong>interface</strong> along with the standard library implementation.</p>
<p>The small size of the standard library doesn't just affect developers
using the language, it even encourages developers of the language
itself to depend on libraries owned by individuals.</p>
<p>Take a look at the transitive dependencies of an official Node.js
package like
<a href="https://github.com/nodejs/node-gyp/blob/main/package.json#L25">node-gyp</a>. Is
it really the ideal outcome of a small standard library to encourage
dependence in official libraries on libraries owned by individuals,
like <a href="https://github.com/sindresorhus/env-paths">env-paths</a>, that
haven't been modified in 3 years? 68 lines of code. Is it not safer at
this point to vendor that code? i.e. copy the <code>env-paths</code> code into
<code>node-gyp</code>.</p>
<p>Similarly, if you go looking for compression support in Rust, there's
none in the standard library. But you may notice the
<a href="https://github.com/rust-lang/flate2-rs">flate2-rs</a> repo under the
official <a href="https://github.com/rust-lang">rust-lang</a> GitHub
namespace. If you look at its transitive dependencies:
<a href="https://github.com/rust-lang/flate2-rs/blob/main/Cargo.toml#L23">flate2-rs</a>
depends on (an individual's)
<a href="https://github.com/Frommi/miniz_oxide/blob/master/miniz_oxide/Cargo.toml#L20">miniz_oxide</a>
which depends on (an individual's)
<a href="https://github.com/jonas-schievink/adler">adler</a> that hasn't been
updated in 4 years. 300 lines of code including tests. Why not vendor
this code? It's the habits a small standard library builds that seem
to encourage everyone not to.</p>
<p>I don't mean these necessarily constitute a supply-chain risk. I'm not
talking about
<a href="https://www.theregister.com/2016/03/23/npm_left_pad_chaos/">left-pad</a>. But
the pattern is sort of clear. Even official packages may end up
depending on external party packages, because the commitment to a
small standard library meant omitting stuff like compression,
checksums, and common OS paths.</p>
<p>It's a tradeoff and maybe makes the job of the standard library
maintainer easier. But I don't think this is the ideal
situation. Dependencies are useful but should be kept to a reasonable
minimum.</p>
<p>Hopefully languages end up more like Go than like Rust in
this regard.</p>
<h3 id="explicit-allocation">Explicit allocation</h3><p>When folk discuss the Zig standard library's pattern of requiring an
allocator argument for every method that allocates, they often talk
about the benefit of swapping out allocators or the benefit of being
able to handle OOM failures.</p>
<p>Both of these seem pretty niche to me. For example, in Zig tests you
are encouraged to pass around a debug allocator that tells you about
memory leaks. But this doesn't seem too different from compiling a C
project with a debug allocator or compiling with different sanitizers
on and running tests against the binary produced. In both cases you
mostly deal with allocators at a global level depending on the
environment you're running the code in (production or tests).</p>
<p>The real benefit of explicit allocations to me is much more
trivial. You basically can't code a method in Zig without
acknowledging allocations.</p>
<p>This is particularly useful for hotpath code. Take an iterator for
example. It has a <code>new()</code> method, a <code>next()</code> method, and a <code>done()</code>
method. In most languages, it's basically impossible at the syntax or
compiler-level to know if you are allocating in the <code>next()</code> method. You
may know because you know the behavior of all the code in <code>next()</code> by
heart. But that won't happen all the time.</p>
<p>Zig is practically alone in that if you write the <code>next()</code> method and
and don't pass an allocator to any method in the <code>next()</code> body,
nothing in that <code>next()</code> method will allocate.</p>
<p>In any other language it might not be until you run a profiler that
you notice an allocation that should have been done once in <code>new()</code>
accidentally ended up in <code>next()</code> instead.</p>
<p>On the other hand, for all the same reasons, writing Zig is kind of a
pain because everything takes an allocator!</p>
<p>Explicit allocation is not intrinsic to Zig, the language. It is a
convention that is prevalent in the standard library. There is still a
global allocator and any user of Zig could decide to use the global
allocator. At which point you've got implicit allocation. So explicit
allocation as a convention isn't a perfect solution.</p>
<p>But it, by default, gives you a level of awareness of allocations you
just can't get from typical Go or Rust or C code, depending on the
project's practices. Perhaps it's possible to switch off the Go, Rust
and C standard library and use one where all functions that allocate
do require an allocator.</p>
<p>But explicitly passing allocators is still sort of a visual hack.</p>
<p>I think the ideal situation in the future will be that every language
supports annotating blocks of code as <code>must-not-allocate</code> or something
along those lines. Either the compiler will enforce this and fail if
you seem to allocate in a block marked <code>must-not-allocate</code>, or it will
panic during runtime so you can catch this in tests.</p>
<p>This would be useful beyond static programming languages. It would be
as interesting to annotate blocks in JavaScript or Python as
<code>must-not-allocate</code> too.</p>
<p>Otherwise the current state of things is that you'd normally configure
this sort of thing at the global level. Saying "there must not be
any allocations in this entire program" just doesn't seem as useful in
general as being able to say "there must not be any allocations in
this one block".</p>
<h4 id="optional,-not-required,-allocator-arguments">Optional, not required, allocator arguments</h4><p>Rust has nascent support for passing an allocator to methods that
allocate. But it's optional. From what I understand, C++ STL is like
this too.</p>
<p>These are both super useful for programming extensions. And it's one
of the reasons I think Zig makes a ton of sense for Postgres
extensions specifically. Because it was only and always ever built for
running in an environment with someone else's allocator.</p>
<h3 id="praise-for-zig,-rust,-and-go-tooling">Praise for Zig, Rust, and Go tooling</h3><p>All three of these have really great first-party tooling including
build system, package management, test runners and formatters. The
idea that the language should provide a great environment to code in
(end-to-end) makes things simpler and nicer for programmers.</p>
<h3 id="meandering-non-conclusion">Meandering non-conclusion</h3><p>Use the language you want to use. Zig and Rust are both nice
alternatives to writing vanilla C.</p>
<p>On the other hand, I've been pleasantly surprised writing Postgres C.
How high level it is. It's almost a separate language since you're
often dealing with user-facing constructs, like Postgres's Datum
objects which represent what you might think of as a cell in a
Postgres database. And you can use all the same functions provided for
Postgres SQL for working with Datums, but from C.</p>
<p>I've also been able work a bit on Postgres extensions in Rust with
<a href="https://github.com/pgcentralfoundation/pgrx">pgrx</a> lately, which I
hope to write about soon. And when I saw
<a href="https://github.com/xataio/pgzx">pgzx</a> for writing Postgres extensions in Zig
I was excited to spend some time with that too.</p>
<blockquote><p lang="en" dir="ltr">Wrote a post on my wishlist for Zig and Rust. Focused on automatic memory management, the standard library, and explicit allocation.<a href="https://t.co/dvynizU9V2">https://t.co/dvynizU9V2</a> <a href="https://t.co/iTXp5QVxj0">pic.twitter.com/iTXp5QVxj0</a></p>— Phil Eaton (@eatonphil) <a href="https://twitter.com/eatonphil/status/1768725864923931033?ref_src=twsrc%5Etfw">March 15, 2024</a></blockquote> 

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nix is a better Docker image builder than Docker's image builder (285 pts)]]></title>
            <link>https://xeiaso.net/talks/2024/nix-docker-build/</link>
            <guid>39720007</guid>
            <pubDate>Fri, 15 Mar 2024 19:56:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xeiaso.net/talks/2024/nix-docker-build/">https://xeiaso.net/talks/2024/nix-docker-build/</a>, See on <a href="https://news.ycombinator.com/item?id=39720007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    

    


<article>
    
    <p>
        Fri Mar 15 2024
    </p>

    

    <div><div><h2><big>$50 of Fly.io Credits</big></h2><p>Coupon code <a href="http://fly.io/ref/go-fly-nix"><code>go-fly-nix</code></a>. Only valid for new accounts that have not used a DevRel coupon code before.</p></div><div><h2><big>Slides and Video</big></h2><p>Slides: <a href="https://drive.google.com/file/d/18-Bz9422oyQH1KKkguHirWr_dEsbB6pT/view?usp=sharing">Google Drive</a><br>
Script: <a href="https://drive.google.com/file/d/1sNhkcT1IlqtFYTj-gg8-gneC604PcTrH/view?usp=sharing">Google Drive</a><br>
Video: <strong>Coming Soon!</strong></p></div></div>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/64"></p><div><p>&lt;<a href="https://xeiaso.net/characters#cadey"><b>Cadey</b></a>&gt; </p><p>A full copy of the talk will be available later today. The video may take
longer. Conference wifi is horrible.</p></div></div>
<h2>The Talk</h2>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/001.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/001.webp"><img alt="The title slide of the talk. It features a hot air balloon breaking into a shipping container with a crowbar. Art by Annie Rugyt." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/001.jpg"></picture><figcaption>The title slide of the talk. It features a hot air balloon breaking into a shipping container with a crowbar. Art by Annie Rugyt.</figcaption></figure>
<p>Hi, I'm Xe Iaso and today I'm gonna get to talk with you about one of my favourite tools: Nix. Nix is many things, but my super hot take is that it's a much better Docker image builder than Docker's image builder.</p>
<p>As many of you know, Nix is a tool that makes it easy to build packages based on the instructions you give it using its little domain-specific language. For reasons which are an exercise to the listener, this language is also called Nix.</p>
<p>A Nix package can be just about anything, but usually you'll see Nix being used to build software packages, your own custom python tooling, OS hard drive image, or container images.</p>
<p>If you've never used it before, Nix is gonna seem a bit weird. It's going to feel like you're doing a lot of work up front, and that's because at some level it is. You're doing work today that you would have done in a few months anyways. I'll get into more detail about this as the talk goes on.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/005.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/005.webp"><img alt="Slide 2024/nix-docker-builder/005" loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/005.jpg"></picture></figure>
<p>As I said, I'm Xe Iaso. I'm the Senior Technophilosopher at Fly.io where I do developer relations. My friends and loved ones can attest that I have a slight tendency to write on my blog. I've been using Nix and NixOS across all of my personal and professional projects for the last four years. I live in Ottawa with my husband.</p>
<p>It's the morning and I know we're all waiting for that precious bean juice to kick in. Let's get that blood pumping with a little exercise. If you've read my blog before, can you raise your hand?</p>
<p>(Ad-lib on the number of hands raised)</p>
<p>Okay, that's good. Raise your hand if this is your first introduction to Nix or NixOS.</p>
<p>(Ad-lib again)</p>
<p>How about if you're a Nix or NixOS expert? Raise your hand if you'd call yourself a Nix or NixOS expert.</p>
<p>(Ad-lib again)</p>
<p>Finally, Raise your hand if you got into Nix or NixOS because of my blog.</p>
<p>(Ad-lib again)</p>
<p>Alright thanks, you can lower your hands now.</p>
<p>This talk is a bit more introductory. There's a mixed audience here of people that are gonna be more hardcore Nix users and people that have probably never heard of Nix before. I want this talk to be a bridge so that those of you who are brand new to Nix can understand what it's about and why you should care. For those of you who have ascended the mortal plane with NixOS powers, maybe this can help you realize where we're needed most. Today I'm gonna cover what Nix is, why it's better than Docker at making Docker images, and some neat second-order properties of Nix that makes it so much more efficient in the long run.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/008.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/008.webp"><img alt="The holy trinity of Nix, showing that Nix the language, the package manager, and the OS are different facets of the same thing." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/008.jpg"></picture><figcaption>The holy trinity of Nix, showing that Nix the language, the package manager, and the OS are different facets of the same thing.</figcaption></figure>
<p>Nix is just a package manager, right? Well, it's a bit more. It's a package manager, a language, and an operating system. It's kind of a weird balance because they all have the name "Nix", but you can use this handy diagram to split the differences. You use Nix the language to make Nix the package manager build packages. Those packages can be anything from software to entire NixOS images.</p>
<p>This is compounded by the difficulty of adopting Nix at work if you have anything but a brand new startup or homelab that's willing to burn down everything and start anew with Nix. Nix is really different than what most developers are used to, which makes it difficult to cram into existing battle-worn CI/CD pipelines.</p>
<p>This is not sustainable. I'm afraid that if there's not a bridge like this, Nix will wilt and die because of the lack of adoption.</p>
<p>I want to show you how to take advantage of Nix today somewhere that it's desperately needed: building and deploying container images.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/012.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/012.webp"><img alt="The docker logo on a sky background." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/012.jpg"></picture><figcaption>The docker logo on a sky background.</figcaption></figure>
<p>To say that Docker won would be an understatement. My career started just about the same time that Docker left public beta. Docker and containerization has been adopted so widely that I'd say that Docker containers have become the de-facto universal package format of the Internet. Modern platforms like Fly.io, Railway, and Render could let people run arbitrary VM images or Linux programs in tarball slugs, but they use Docker images because that works out better for everyone.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/013.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/013.webp"><img alt="The docker logo with a badly photoshopped muscle-bound beefy arm on a sky background." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/013.jpg"></picture><figcaption>The docker logo with a badly photoshopped muscle-bound beefy arm on a sky background.</figcaption></figure>
<p>This gives people a lot of infrastructure superpowers and the advantages make the thing sell itself. It's popular for a reason. It solves real-world problems that previously required complicated cross-team coordination. No more arguing with your sysadmin or SRE team over upgrading your local fork of Ubuntu to chase the dragon with package dependencies!</p>
<p>However, there's just one fatal flaw:</p>
<p>Docker builds are not deterministic. Not even slightly. Sure, the average docker file you find on the internet will build 99.99% of the time, but that last 0.01% is where the real issues come into play.</p>
<p>Speaking as a former wielder of the SRE shouting pager, that last 0.01% of problems ends up coming into play at 4am. Always 4am, never while you are at work.</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/64"></p><div><p>&lt;<a href="https://xeiaso.net/characters#cadey"><b>Cadey</b></a>&gt; </p><p>Ask me how I know.</p></div></div>
<p>One of the biggest problems that doesn't sound like a problem at first is that Docker builds have access to the public Internet. This is needed to download packages from the Ubuntu repositories, but that also means that it's hard to go back and recreate the exact state of the Ubuntu repositories when you inevitably need to recreate an image at a future date.</p>
<p>Remember, Ubuntu 18.04 is going out of support this year! You're going to have a flag day finding out what depends on that version of Ubuntu when things break and not any sooner.</p>
<p>Even more fun, adding packages to a docker image the naïve way means that you get wasted space. If you run <code>apt-get upgrade</code> at the beginning of your docker build, you can end up replacing files in the container image. Those extra files end up being some "wasted space" shadow copies that will add up over time, especially with AWS charging you per millibyte of disk space and network transfer or whatever.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/64"></p><div><p>&lt;<a href="https://xeiaso.net/characters#aoi"><b>Aoi</b></a>&gt; </p><p>What if we had the ability to know all of the dependencies that are needed
ahead of time and then just use those? What if your builds didn't need an
internet connection at all?</p></div></div>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/021.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/021.webp"><img alt="The Nix/NixOS logo on a purple and black gradient background." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/021.jpg"></picture><figcaption>The Nix/NixOS logo on a purple and black gradient background.</figcaption></figure>
<p>This is the real advantage of Nix when compared to docker builds. Nix lets you know exactly what you're depending on ahead of time and then can break that into the fewest docker layers possible. This means that pushing updates to your programs only means that the minimal number of changes are actually made. You don't need to wait for apt or npm to install your dependencies yet again just to change a single line of code in your service.</p>
<p>I think one of the best ways to adopt it is to use it to build docker images. This helps you bridge the gap so that you can experiment with new tools without breaking too much of your existing workflows.</p>
<p>As an example, let's say I have a Go program that gives you quotes from Douglas Adams. I want to deploy it to a platform that only takes Docker images, like Fly.io, Railway, or Google Cloud Functions.</p>
<p>In order to do this, I'd need to do a few things: First, I'd need to build the program into a package with Nix and make sure it works. Then I'd need to turn that into a docker image, load it into my docker daemon, and push it to their registry. Finally I can deploy my application and everyone can benefit from the wisdom of days gone past.</p>
<p>Here's what that package definition looks like in my project's Nix flake. Let's break this down into parts.</p>
<pre><code><span>bin <span>=</span> pkgs<span>.</span>buildGoModule <span>{</span>
</span><span>  pname <span>=</span> <span>"douglas-adams-quotes"</span><span>;</span>
</span><span>  <span>inherit</span> version<span>;</span>
</span><span>  src <span>=</span> <span>./.</span><span>;</span>
</span><span>  vendorHash <span>=</span> <span>null</span><span>;</span>
</span><span><span>}</span><span>;</span>
</span></code></pre>
<p>This project is in a Go module, so <code>pkgs.buildGoModule</code> tells Nix to use the Go module template. That template will set everything up for us: mainly the Go compiler, a C compiler for CGo code, and downloading any external dependencies for you.</p>
<p>Here are the arguments to the <code>buildGoModule</code> function: a package name, the version, the path to the source code, and the hash of the external dependencies.</p>
<p>The name of the package is "Douglas Adams Quotes" in kebab case, the version is automagically generated from the git commit of the service, the source code is in the current working directory, and I don't need anything beyond Go's standard library. If you need external dependencies, you can specify the hash of all the dependencies here or use <a href="https://github.com/nix-community/gomod2nix"><code>gomod2nix</code></a> to automate this (it's linked in the description at the end of the talk).</p>
<pre><code><span># nix build .#bin
</span></code></pre>
<p>Now that we have a package definition, you can build it with nix build dot hash bin. That makes Nix build the bin package in your flake and put the result in dot slash result.</p>
<p>Next comes building that into a Docker image with the dockerTools family of helpers. dockerTools lets you take that Nix package you just made and put it and all its dependencies into a Docker image so you can deploy it.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/031.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/031.webp"><img alt="An onion and an onion with an X over it. An onion is a visual metaphor for layered Docker images." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/031.jpg"></picture><figcaption>An onion and an onion with an X over it. An onion is a visual metaphor for layered Docker images.</figcaption></figure>
<p>There's two basic ways to use it, making a layered image and a non-layered image.</p>
<p>A non-layered image is the simplest way to use Nix to build a docker image. It takes the program, its dependencies, any additional things like TLS root certificates and puts it all into a folder to be exposed as a single-layer docker image.</p>
<p>This works, but it doesn't really let us take advantage of the benefits of Nix. Making any change to a non-layered image means you have to push all of the things that haven't changed. Nix knows what all your dependencies are, so it should be able to take advantage of that when building a container image. Why should you have to upload new copies of glibc and the python interpreter over and over?</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/034.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/034.webp"><img alt="An onion pointing to a bunch of folders with Nix packages in its layers." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/034.jpg"></picture><figcaption>An onion pointing to a bunch of folders with Nix packages in its layers.</figcaption></figure>
<p>Nix also lets you make a layered image. A layered image puts every dependency into its own image layer so you only upload the parts of your image that have actually changed. Made an update to the webp library to fix a trivial bounds checking vulnerability because nobody writes those libraries in memory-safe languages? The only thing that'd need to be uploaded is that single webp library layer.</p>
<p>The reason why this works is that there's a dirty secret deep into Docker that nobody can really take advantage of: Docker has a content-aware store baked into the heart of it, but because <code>docker build</code> isn't made with it in mind, nothing is really able to take advantage of it.</p>
<p>Except Nix! A layered image means that every package is in its own layer, so glibc only needs to get uploaded once...</p>
<div><p><img alt="Cadey is coffee" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/coffee/64"></p><div><p>&lt;<a href="https://xeiaso.net/characters#cadey"><b>Cadey</b></a>&gt; </p><p>...until we find yet another trivial memory safety vulnerability in glibc
that's been ignored for my entire time on this planet and need to have a fire
day rebuilding everything to cope.</p></div></div>
<p>Here's what a layered docker image build for that Douglas Adams quotes service would look like:</p>
<pre><code><span>docker <span>=</span> pkgs<span>.</span>dockerTools<span>.</span>buildLayeredImage <span>{</span>
</span><span>  name <span>=</span> <span>"registry.fly.io/douglas-adams-quotes"</span><span>;</span>
</span><span>  tag <span>=</span> <span>"latest"</span><span>;</span>
</span><span>  config<span>.</span>Cmd <span>=</span> <span>"<span><span>$</span><span>{</span>bin<span>}</span></span>/bin/douglas-adams-quotes"</span><span>;</span>
</span><span><span>}</span><span>;</span>
</span></code></pre>
<p>Again, let's break it down.</p>
<p>You start by saying that you want to build a layered image by calling the <code>dockerTools.buildLayeredImage</code> function with the image name and tag, just like you would with <code>docker build</code>. Now comes the fun part: the rest of the container image.</p>
<pre><code><span>config<span>.</span>Cmd <span>=</span> <span>"<span><span>$</span><span>{</span>bin<span>}</span></span>/bin/douglas-adams-quotes"</span><span>;</span>
</span></code></pre>
<p>Just tell Nix that the container should run the built version of the Douglas Adams quotes server and bam, everything'll be copied over for you. Glibc will make it over as well as whatever detritus you need to make Glibc happy these days.</p>
<p>If you need to add something like the CA certificate root, you can specify it with the <code>contents</code> argument. You can use this to add any package from nixpkgs into your image. My website uses this to add Typst, Deno, and Dhall tools to the container.</p>
<pre><code><span>docker <span>=</span> pkgs<span>.</span>dockerTools<span>.</span>buildLayeredImage <span>{</span>
</span><span>  name <span>=</span> <span>"registry.fly.io/douglas-adams-quotes"</span><span>;</span>
</span><span>  tag <span>=</span> <span>"latest"</span><span>;</span>
</span><span>  contents <span>=</span> <span>with</span> pkgs<span>;</span> <span>[</span> cacert <span>]</span><span>;</span> <span># &lt;--</span>
</span><span>  config<span>.</span>Cmd <span>=</span> <span>"<span><span>$</span><span>{</span>bin<span>}</span></span>/bin/douglas-adams-quotes"</span><span>;</span>
</span><span><span>}</span><span>;</span>
</span></code></pre>
<p>Then you type in <code>nix build .#docker</code> and whack enter. A shiny new image will show up in <code>./result</code>.</p>
<pre><code><span>nix build .#docker
</span></code></pre>
<p>Load it using <code>docker load &lt; ./result</code> and it'll be ready for deployment.</p>
<pre><code><span>docker load &lt; ./result
</span></code></pre>
<p>TODO: embed video</p>
<p>Opening the image in <code>dive</code>, we see that every layer adds another package from nixpkgs until you get to the end where it all gets tied together and any contents are symlinked to the root of the image.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/045.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/045.webp"><img alt="A successful slide with a lot of cheery imagery." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/045.jpg"></picture><figcaption>A successful slide with a lot of cheery imagery.</figcaption></figure>
<p>And that's it! All that's left is to deploy it to the cloud and find out if you just broke production. It should be fine, right?</p>
<p>The really cool part is that this will work for the cases where you have single images exposed from a code repository, but that content-aware hackery doesn't end at making just one of your services faster to upload.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/050.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/050.webp"><img alt="A diagram showing several programs sharing the same layers." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/050.jpg"></picture><figcaption>A diagram showing several programs sharing the same layers.</figcaption></figure>
<p>If you have multiple services in the same repository, they'll share docker layers between each other. For free. Without any extra configuration. I don't think you can even dream of doing this with Docker without making a bunch of common base images that have a bunch of tools and bloat that some of your services will never make use of.</p>
<p>As a practical example, I have a repo I call <a href="https://github.com/Xe/x">"x"</a>. It's full of a decade's worth of side projects, experiments, and tools that help me explore various bits of technology. It's also a monorepo for a bunch of other projects:</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/049.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/049.webp"><img alt="A diagram showing several programs sharing the same layers." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/049.jpg"></picture><figcaption>A diagram showing several programs sharing the same layers.</figcaption></figure>
<p>This is a lot of stuff and I don't expect anyone to read that, so I made the text small enough to discourage it. Most of it is deployed across like three platforms too, but I've been slowly converging on one common deployment backbone by shoving everything into Docker images.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/050.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/050.webp"><img alt="A diagram showing several programs sharing the same layers." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/050.jpg"></picture><figcaption>A diagram showing several programs sharing the same layers.</figcaption></figure>
<p>Pushing updates to any one of these services also pushes parts of the updates to most of the other ones. This saves me a lot of time and money across my plethora of projects. Take that, Managed NAT Gateway!</p>
<p>Oh no, I think I sense it, you do too right? It's the pedantry alert! Yes in theory I could take advantage of Docker caching to build the images just as efficiently as Nix, but then my build steps would have to look like this:</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/052.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/052.webp"><img alt="A giant depressing mess of wires." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/052.jpg"></picture><figcaption>A giant depressing mess of wires.</figcaption></figure>
<p>Sure, you can do it, but you'd end up with unmaintainable balls of mud that would have you install shared libraries into their own layers and then you risk invoking the wrath of general protection fault. Not only would you have to turn the network stack back on during builds (there goes reproducibility!), I'd have to rejigger search paths, compiler flags, CGO-related goat sacrifices and more. It'd just be a mess.</p>
<pre><code><span>docker <span>=</span> pkgs<span>.</span>dockerTools<span>.</span>buildLayeredImage <span>{</span>
</span><span>  name <span>=</span> <span>"registry.fly.io/douglas-adams-quotes"</span><span>;</span>
</span><span>  tag <span>=</span> <span>"latest"</span><span>;</span>
</span><span>  contents <span>=</span> <span>with</span> pkgs<span>;</span> <span>[</span> cacert <span>]</span><span>;</span>
</span><span>  config<span>.</span>Cmd <span>=</span> <span>"<span><span>$</span><span>{</span>bin<span>}</span></span>/bin/douglas-adams-quotes"</span><span>;</span>
</span><span><span>}</span><span>;</span>
</span></code></pre>
<p>Look at this though, it's just so much simpler. It takes the package and shoves it into a container for you so you don't need to care about the details. It's so much more beautiful in comparison.</p>
<p>Above all though, the biggest advantage Nix gives you is the ability to travel back in time and build software exactly as it was in the past. This lets you recreate a docker image exactly at a later point in the future when facts and circumstances demand because that one on-prem customer had apparently never updated their software and was experiencing a weird bug.</p>
<p>This means that in theory, when you write package builds today, you're taking from that time you would have spent in the future to recreate it. You don't just build your software though, you crystallize a point in time that describes the entire state of the world including your software to get the resulting packages and docker images.</p>
<p>I've been working on a project called <a href="https://cdn.xeiaso.net/">XeDN</a> for a few years. Here's how easy it is to build a version from 14 months ago:</p>
<pre><code><span>nix build github:Xe/x/567fdc2#xedn-docker
</span></code></pre>
<p>That's it. That's the entire command. I say that I want to build the <a href="https://github.com/Xe/x">GitHub repo Xe/x</a> at an arbitrary commit hash and get the xedn-docker target. I can then load it into my docker daemon and then I have the exact same bytes I had back then, Go 1.19 and all.</p>
<p>This party trick isn't as easy to pull off with vanilla docker builds unless you pay a lot for storage.</p>
<p>An even cooler part of that is that most of the code didn't even need to be rebuilt thanks to the fact that I upload all of my builds into a Nix cache. A Nix cache lets you put the output of Nix commands into a safe place so that they don't need to be run again in the future. This means that developer laptops don't all need to build new versions of nokogiri every time it's bumped ever so slightly. It'll already be built for you with the power of the cloud.</p>
<p>I have that uploaded into a cache through <a href="https://garnix.io/">Garnix</a>, which I use to do CI on all of my flakes projects. Garnix is effortless. Turn it on and then wait for it to report build status on every commit. It's super great because I don't have to think about it.</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/060.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/060.webp"><img alt="A terrible picture of my homelab." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/060.jpg"></picture><figcaption>A terrible picture of my homelab.</figcaption></figure>
<p>I even have all of my homelab machine configurations built with Garnix so that when they update every evening, they just pull the newest versions of their config from the Garnix cache instead of building it themselves. Around 7pm or so I hear them reboot after the day of a kernel upgrade. It's really great.</p>
<p>Not to mention never having to ever wait for my custom variant of Iosevka to build on my MacBook or shellbox.</p>
<p>In conclusion:</p>
<ul>
<li>Nix is a better docker image builder than docker's image builder.</li>
<li>Nix makes you specify the results, not the steps you take to get there.</li>
<li>Building Docker images with Nix makes adopting Nix easy if you already use Docker.</li>
<li>Nix makes docker images that share layers between parts of your monorepo.</li>
<li>Nix lets you avoid building code that was built in the past thanks to binary caches.</li>
<li>And you end up with normal, ordinary container images that you can deploy anywhere. Even platforms like AWS, Google Cloud, or Fly.io.</li>
</ul>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/068.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/068.webp"><img alt="A slide listing everyone I have to thank for the talk." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/068.jpg"></picture><figcaption>A slide listing everyone I have to thank for the talk.</figcaption></figure>
<p>Before I get all of this wrapped up, I want to thank everyone on this list for their input, feedback, and more to help this talk shine. Thank you so much!</p>
<figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/069.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/069.webp"><img alt="A conclusion slide showing information about me and the link to this page." loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/talks/2024/nix-docker-builder/069.jpg"></picture><figcaption>A conclusion slide showing information about me and the link to this page.</figcaption></figure>
<p>And thank you for watching! I've been Xe Iaso and I'm gonna linger around afterwards for questions. If I don't get to you and you really want a question answered, please email <a href="mailto:dockerimage@xeserv.us">dockerimage@xeserv.us</a>. I promise I'll get back to you as soon as possible.</p>
<p>If you want to work with me to make developer relations better, my employer Fly.io is hiring. Catch up with me if you want stickers!</p>
<p>I have some extra information linked at the QR code on screen. This includes the source code for the Douglas Adams quotes server so you can clone it on your laptop and play around with it.</p>
<p>Be well, all.</p>

    <hr>

    

    

    <p>Facts and circumstances may have changed since publication. Please contact me before jumping to conclusions if something seems wrong or unclear.</p>

    <p>Tags: </p>

    <a href="">View slides</a>
</article>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing whistleblower before death: "If anything happens, it's not suicide" (192 pts)]]></title>
            <link>https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide</link>
            <guid>39718672</guid>
            <pubDate>Fri, 15 Mar 2024 17:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide">https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide</a>, See on <a href="https://news.ycombinator.com/item?id=39718672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="incArticle"><h2>"I know that he did not commit suicide."</h2><h2>Curious Causes</h2><p>As Boeing continues <a href="https://futurism.com/the-byte/jet-tire-falls-off">to be in the news</a> for its <a href="https://futurism.com/the-byte/plane-wing-breaking-footage">repeatedly</a> <a href="https://futurism.com/the-byte/boeing-plane-fire">malfunctioning</a> <a href="https://futurism.com/the-byte/boeing-trouble-crack-cockpit">planes</a>, the fallout from one ex-employee's death continues — and new reports complicate the coroner's initial suicide ruling.</p><p>In an <a href="https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024">interview with Charleston, South Carolina's&nbsp;<em>ABC4 News</em></a>, a friend of Boeing whistleblower John Barnett, whose body was found dead in a car parked in a hotel lot amid his testimony against his former employer last weekend, said that he warned her that something might happen to him.</p><p>"I said, 'Aren't you scared?'" the woman, who gave only her first name Jennifer, told the local broadcaster. "And he said, 'No, I ain't scared, but if anything happens to me, it's not suicide.'"</p><p>Jennifer said that Barnett's words were spoken ahead of his deposition against Boeing. At the time, he'd mentioned that the company had retaliated against him for raising safety concerns before — which was, indeed, the subject of his Occupational Safety and Health Administration (OSHA) <a href="https://www.corporatecrimereporter.com/news/200/john-barnett-on-why-he-wont-fly-on-a-boeing-787-dreamliner/">complaint</a> that led to his now-unfinished deposition.</p><p>The woman, who lives in the whistleblower's home state of Louisiana where he'd moved in recent years to take care of his aging mother, said that that cryptic warning has come back to haunt her since Barnett's death, which a coroner in Charleston says was self-inflicted.</p><p>"I know that he did not commit suicide," Jennifer told <em>ABC4</em>. "There's no way. He loved life too much. He loved his family too much. He loved his brothers too much to put them through what they're going through right now."</p><h2>Hearsay</h2><p>She's not alone in that sentiment either, it seems.</p><p>In a statement to&nbsp;<em>Futurism</em>, Barnett's attorneys said that they also "<a href="https://futurism.com/boeing-whistleblowers-lawyers-statement">didn't see any indication</a>" that the whistleblower may have been planning to take his own life, and that he'd seemed in "good spirits" as his deposition was coming to a close.</p><p>Not everyone close to the longtime Boeing quality control manager agrees with that sentiment, however.</p><p>In an <a href="https://www.seattletimes.com/business/boeing-787-whistleblower-found-dead-in-apparent-suicide/">interview with the&nbsp;<em>Seattles Times</em></a>, Barnett's niece, Katelyn Gillespie, said that her "fun uncle" had become "stressed and depressed" in recent months as his ex-employer was in the news amid the same kinds of safety concerns he'd raised and ultimately resigned over.</p><p>"He battled a lot due to the Boeing stuff," the whistleblower's niece said. "It took a major toll on him."</p><p>Despite the Charleston County coroner's preliminary autopsy report on the cause of death being a self-inflicted gunshot wound and that local police found, <a href="https://www.live5news.com/2024/03/12/brave-honest-man-boeing-whistleblowers-attorneys-release-statement-his-death/">per the city's <em>Live 5 News</em></a>, "some sort of note," authorities said they are still actively investigating the case while awaiting an official coroner's ruling.</p><p>As in other <a href="https://www.theguardian.com/politics/2013/jul/16/david-kelly-death-10-years-on">suspicious</a> <a href="https://www.cbsnews.com/sacramento/news/dhs-whistleblower-philip-haney-death-ruled-suicide/">whistleblower</a> <a href="https://foreignpolicy.com/2022/05/17/robert-mcfarlane-death-national-security-advisor-iran-contra/">suicides</a>, it's almost impossible for anyone to know exactly what happened when Barnett died — but with this new claim from someone close to Barnett, things just got a lot more complicated.</p><p><strong>More on Boeing:</strong> <a href="https://futurism.com/the-byte/pilot-boeing-gauges-nosedive"><em>Pilot Lost Control of Boeing Jet Because Gauges “Went Blank," Causing Nosedive</em></a></p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama now supports AMD graphics cards (575 pts)]]></title>
            <link>https://ollama.com/blog/amd-preview</link>
            <guid>39718558</guid>
            <pubDate>Fri, 15 Mar 2024 17:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/blog/amd-preview">https://ollama.com/blog/amd-preview</a>, See on <a href="https://news.ycombinator.com/item?id=39718558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <h2>March 14, 2024</h2>
      <section>
        <p><a href="https://ollama.com/download"><img src="https://ollama.com/public/blog/amd-preview.png" alt="Ollama AMD"></a></p>

<p>Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for <a href="https://ollama.com/download/linux">Linux</a> and <a href="https://ollama.com/download/windows">Windows</a>.</p>

<video autoplay="" controls="">
  <source src="https://github.com/ollama/ollama/assets/3325447/671a8031-1915-448e-b033-16b367b359d9" type="video/mp4">
</video>

<h2>Supported graphics cards</h2>

<table>
<thead>
<tr>
<th>Family</th>
<th>Supported cards and accelerators</th>
</tr>
</thead>

<tbody>
<tr>
<td>AMD Radeon RX</td>
<td><code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <br><code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code><br><code>Vega 64</code> <code>Vega 56</code></td>
</tr>

<tr>
<td>AMD Radeon PRO</td>
<td><code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <br><code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code><br><code>V620</code> <code>V420</code> <code>V340</code> <code>V320</code><br><code>Vega II Duo</code> <code>Vega II</code> <code>VII</code> <code>SSG</code></td>
</tr>

<tr>
<td>AMD Instinct</td>
<td><code>MI300X</code> <code>MI300A</code> <code>MI300</code><br><code>MI250X</code> <code>MI250</code> <code>MI210</code> <code>MI200</code><br><code>MI100</code> <code>MI60</code> <code>MI50</code></td>
</tr>
</tbody>
</table>
<p>Support for more AMD graphics cards is coming soon.</p>

<h2>Get started</h2>

<p>To get started with Ollama with support for AMD graphics cards, download Ollama for <a href="https://ollama.com/download/linux">Linux</a> or <a href="https://ollama.com/download/windows">Windows</a>.</p>

      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenVPN Is Open to VPN Fingerprinting (120 pts)]]></title>
            <link>https://arxiv.org/abs/2403.03998</link>
            <guid>39718389</guid>
            <pubDate>Fri, 15 Mar 2024 17:34:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.03998">https://arxiv.org/abs/2403.03998</a>, See on <a href="https://news.ycombinator.com/item?id=39718389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.03998">Download PDF</a>
    <a href="https://arxiv.org/html/2403.03998v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>VPN adoption has seen steady growth over the past decade due to increased public awareness of privacy and surveillance threats. In response, certain governments are attempting to restrict VPN access by identifying connections using "dual use" DPI technology. To investigate the potential for VPN blocking, we develop mechanisms for accurately fingerprinting connections using OpenVPN, the most popular protocol for commercial VPN services. We identify three fingerprints based on protocol features such as byte pattern, packet size, and server response. Playing the role of an attacker who controls the network, we design a two-phase framework that performs passive fingerprinting and active probing in sequence. We evaluate our framework in partnership with a million-user ISP and find that we identify over 85% of OpenVPN flows with only negligible false positives, suggesting that OpenVPN-based services can be effectively blocked with little collateral damage. Although some commercial VPNs implement countermeasures to avoid detection, our framework successfully identified connections to 34 out of 41 "obfuscated" VPN configurations. We discuss the implications of the VPN fingerprintability for different threat models and propose short-term defenses. In the longer term, we urge commercial VPN providers to be more transparent about their obfuscation approaches and to adopt more principled detection countermeasures, such as those developed in censorship circumvention research.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Diwen Xue [<a href="https://arxiv.org/show-email/af7c5f54/2403.03998">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 6 Mar 2024 19:15:02 UTC (25,370 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's worked in Computer Science: 1999 vs. 2015 (2015) (119 pts)]]></title>
            <link>http://danluu.com/butler-lampson-1999/</link>
            <guid>39717838</guid>
            <pubDate>Fri, 15 Mar 2024 16:53:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://danluu.com/butler-lampson-1999/">http://danluu.com/butler-lampson-1999/</a>, See on <a href="https://news.ycombinator.com/item?id=39717838">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>In 1999, Butler Lampson gave a talk about the <a href="http://research.microsoft.com/pubs/68591/computersystemsresearch.pdf">past and future of “computer systems research”</a>. Here are his opinions from 1999 on "what worked".</p> <table> <thead> <tr> <th>Yes</th> <th>Maybe</th> <th>No</th> </tr> </thead> <tbody> <tr> <td>Virtual memory</td> <td>Parallelism</td> <td>Capabilities</td> </tr> <tr> <td>Address spaces</td> <td>RISC</td> <td>Fancy type systems</td> </tr> <tr> <td>Packet nets</td> <td>Garbage collection</td> <td>Functional programming</td> </tr> <tr> <td>Objects / subtypes</td> <td>Reuse</td> <td>Formal methods</td> </tr> <tr> <td>RDB and SQL</td>  <td>Software engineering</td> </tr> <tr> <td>Transactions</td>  <td>RPC</td> </tr> <tr> <td>Bitmaps and GUIs</td>  <td>Distributed computing</td> </tr> <tr> <td>Web</td>  <td>Security</td> </tr> <tr> <td>Algorithms</td>   </tr> </tbody> </table>  <p><br> Basically everything that was a Yes in 1999 is still important today. Looking at the Maybe category, we have:</p> <h3 id="parallelism">Parallelism</h3> <p>This is, unfortunately, still a Maybe. Between the <a href="https://en.wikipedia.org/wiki/Dennard_scaling">end of Dennard scaling</a> and the continued demand for compute, chips now expose plenty of the parallelism to the programmer. Concurrency has gotten much easier to deal with, but really extracting anything close to the full performance available isn't much easier than it was in 1999.</p> <p>In 2009, <a href="https://channel9.msdn.com/Shows/Going+Deep/E2E-Erik-Meijer-and-Butler-Lampson-Abstraction-Security-Embodiment">Erik Meijer and Butler Lampson talked about this</a>, and Lampson's comment was that when they came up with threading, locks, and conditional variables at PARC, they thought they were creating something that programmers could use to take advantage of parallelism, but that they now have decades of evidence that they were wrong. Lampson further remarks that to do parallel programming, what you need to do is put all your parallelism into a little box and then have a wizard go write the code in that box. Not much has changed since 2009.</p> <p>Also, note that I'm using the same criteria to judge all of these. Whenever you say something doesn't work, someone will drop in say that, no wait, here's a PhD that demonstrates that someone has once done this thing, or here are nine programs that demonstrate that Idris is, in fact, widely used in large scale production systems. I take Lampson's view, which is that if the vast majority of programmers are literally incapable of using a certain class of technologies, that class of technologies has probably not succeeded.</p> <p>On recent advancements in parallelism, Intel <a href="http://danluu.com/intel-cat/">recently added features that make it easier to take advantage of trivial parallelism by co-scheduling multiple applications on the same machine without interference</a>, but outside of a couple big companies, no one's really taking advantage of this yet. They also added hardware support for STM recently, but it's still not clear how much STM helps with usability when designing large scale systems.</p> <h3 id="risc-danluu-com-risc-definition"><a href="http://danluu.com/risc-definition/">RISC</a></h3> <p>If this was a Maybe in 1999 it's certainly a No now. In the 80s and 90s a lot of folks, probably the majority of folks, believed RISC was going to take over the world and x86 was doomed. In 1991, Apple, IBM, and Motorola got together to create PowerPC (PPC) chips that were going to demolish Intel in the consumer market. They opened the Somerset facility for chip design, and collected a lot of their best folks for what was going to be a world changing effort. At the upper end of the market, DEC's Alpha chips were getting twice the performance of Intel's, and their threat to the workstation market was serious enough that Microsoft ported Windows NT to the Alpha. DEC started a project to do dynamic translation from x86 to Alpha; at the time the project started, the projected performance of x86 basically running in emulation on Alpha was substantially better than native x86 on Intel chips.</p> <p>In 1995, Intel released the Pentium Pro. At the time, it had better workstation integer performance than anything else out there, including much more expensive chips targeted at workstations, and its floating point performance was within a factor of 2 of high-end chips. That immediately destroyed the viability of the mainstream Apple/IBM/Moto PPC chips, and in 1998 IBM pulled out of the Somerset venture<sup id="fnref:I"><a rel="footnote" href="#fn:I">1</a></sup> and everyone gave up on really trying to produce desktop class PPC chips. Apple continued to sell PPC chips for a while, but they had to cook up bogus benchmarks to make the chips look even remotely competitive. By the time DEC finished their dynamic translation efforts, x86 in translation was barely faster than native x86 in floating point code, and substantially slower in integer code. While that was a very impressive technical feat, it wasn't enough to convince people to switch from x86 to Alpha, which killed DEC's attempts to move into the low-end workstation and high-end PC market.</p> <p>In 1999, high-end workstations were still mostly RISC machines, and supercomputers were a mix of custom chips, RISC chips, and x86 chips. Today, Intel dominates the workstation market with x86, and the supercomputer market has also moved towards x86. Other than POWER, RISC ISAs were mostly wiped out (like PA-RISC) or managed to survive by moving to the low-margin embedded market (like MIPS), which wasn't profitable enough for Intel to pursue with any vigor. You can see a kind of instruction set arbitrage that MIPS and ARM have been able to take advantage of because of this. Cavium and ARM will sell you a network card that offloads a lot of processing to the NIC, which have a bunch of cheap MIPS and ARM processors, respectively, on board. The low-end processors aren't inherently better at processing packets than Intel CPUS; they're just priced low enough that Intel won't compete on price because they don't want to cannibalize their higher margin chips with sales of lower margin chips. MIPS and ARM have no such concerns because MIPS flunked out of the high-end processor market and ARM has yet to get there. If the best thing you can say about RISC chips is that they manage to exist in areas where the profit margins are too low for Intel to care, that's not exactly great evidence of a RISC victory. That Intel ceded the low end of the market might seem ironic considering Intel's origins, but they've always been aggressive about moving upmarket (they did the same thing when they transitioned from DRAM to SRAM to flash, ceding the barely profitable DRAM market to their competitors).</p> <p>If there's any threat to x86, it's ARM, and it's their business model that's a threat, not their ISA. And as for their ISA, ARM's biggest inroads into mobile and personal computing came with ARMv7 and earlier ISAs, which aren't really more RISC-like than x86<sup id="fnref:A"><a rel="footnote" href="#fn:A">2</a></sup>. In the area in which they dominated, their "modern" RISC-y ISA, ARMv8, is hopeless and will continue to be hopeless for years, and they'll continue to dominate with their non-RISC ISAs.</p> <p>In retrospect, the reason RISC chips looked so good in the 80s was that you could fit a complete high-performance RISC microprocessor onto a single chip, which wasn't true of x86 chips at the time. But as we got more transistors, this mattered less.</p> <p>It's possible to nitpick RISC being a no by saying that modern processors translate x86 ops into RISC micro-ops internally, but if you listened to talk at the time, people thought that having an external RISC ISA would be so much lower overhead that RISC would win, which has clearly not happened. Moreover, modern chips also do micro-op fusion in order to fuse operations into decidedly un-RISC-y operations. A clean RISC ISA is a beautiful thing. I sometimes re-read Dick Sites's <a href="http://www.hpl.hp.com/hpjournal/dtj/vol4num4/vol4num4art1.pdf">explanation of the Alpha design</a> just to admire it, but it turns out beauty isn't really critical for the commercial success of an ISA.</p> <h3 id="garbage-collection">Garbage collection</h3> <p>This is a huge Yes now. Every language that's become successful since 1999 has GC and is designed for all normal users to use it to manage all memory. In five years, Rust or D might make that last sentence untrue, but even if that happens, GC will still be in the yes category.</p> <h3 id="reuse">Reuse</h3> <p>Yes, I think, although I'm not 100% sure what Lampson was referring to here. Lampson said that reuse was a maybe because it sometimes works (for UNIX filters, OS, DB, browser) but was also flaky (for OLE/COM). There are now widely used substitutes for OLE; service oriented architectures also seem to fit his definition of re-use.</p> <p>Looking at the No category, we have:</p> <h3 id="capabilities">Capabilities</h3> <p>Yes. Widely used on mobile operating systems.</p> <h3 id="fancy-type-systems">Fancy type systems</h3> <p>It depends on what qualifies as a fancy type system, but if “fancy” means something at least as fancy as Scala or Haskell, this is a No. That's even true if you relax the standard to an ML-like type system. Boy, would I love to be able to do everyday programming in an ML (F# seems particularly nice to me), but we're pretty far from that.</p> <p>In 1999 C, and C++ were mainstream, along with maybe Visual Basic and Pascal, with Java on the rise. And maybe Perl, but at the time most people thought of it as a scripting language, not something you'd use for "real" development. PHP, Python, Ruby, and JavaScript all existed, but were mostly used in small niches. Back then, Tcl was one of the most widely used scripting languages, and it wasn't exactly widely used. Now, PHP, Python, Ruby, and JavaScript are not only more mainstream than Tcl, but more mainstream than C and C++. C# is probably the only other language in the same league as those languages in terms of popularity, and Go looks like the only language that's growing fast enough to catch up in the foreseeable future. Since 1999, we have a bunch of dynamic languages, and a few languages with type systems that are specifically designed not to be fancy.</p> <p>Maybe I'll get to use F# for non-hobby projects in another 16 years, but things don't look promising.</p> <h3 id="functional-programming">Functional programming</h3> <p>I'd lean towards Maybe on this one, although this is arguably a No. Functional languages are still quite niche, but functional programming ideas are now mainstream, at least for the HN/reddit/twitter crowd.</p> <p>You might say that I'm being too generous to functional programming here because I have a soft spot for immutability. That's fair. In 1982, <a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/other-authors/morris-real-programming.pdf">James Morris wrote</a>:</p> <blockquote> <p>Functional languages are unnatural to use; but so are knives and forks, diplomatic protocols, double-entry bookkeeping, and a host of other things modern civilization has found useful. Any discipline is unnatural, in that it takes a while to master, and can break down in extreme situations. That is no reason to reject a particular discipline. The important question is whether functional programming in unnatural the way Haiku is unnatural or the way Karate is unnatural.</p> <p>Haiku is a rigid form poetry in which each poem must have precisely three lines and seventeen syllables. As with poetry, writing a purely functional program often gives one a feeling of great aesthetic pleasure. It is often very enlightening to read or write such a program. These are undoubted benefits, but real programmers are more results-oriented and are not interested in laboring over a program that already works.</p> <p>They will not accept a language discipline unless it can be used to write programs to solve problems the first time -- just as Karate is occasionally used to deal with real problems as they present themselves. A person who has learned the discipline of Karate finds it directly applicable even in bar-room brawls where no one else knows Karate. Can the same be said of the functional programmer in today's computing environments? No.</p> </blockquote> <p>Many people would make the same case today. I don't agree, but that's a matter of opinion, not a matter of fact.</p> <h3 id="formal-methods">Formal methods</h3> <p>Maybe? Formal methods have had high impact in a few areas. Model checking is omnipresent in chip design. Microsoft's <a href="http://research.microsoft.com/pubs/70038/tr-2004-08.pdf">driver verification tool</a> has probably had more impact than all formal chip design tools combined, clang now has a fair amount of static analysis built in, and so on and so forth. But, formal methods are still quite niche, and the vast majority of developers don't apply formal methods.</p> <h3 id="software-engineering">Software engineering</h3> <p>No. In 1995, David Parnas had a talk at ICSE (the premier software engineering conference) about the fact that even the ICSE papers that won their “most influential paper award” (including two of Parnas's papers) had <a href="http://danluu.com/empirical-pl/">very little impact on industry</a>.</p> <p>Basically all of Parnas's criticisms are still true today. One of his suggestions, that there should be distinct conferences for researchers and for practitioners has been taken up, but there's not much cross-pollination between academic conferences like ICSE and FSE and practitioner-focused conferences like StrangeLoop and PyCon.</p> <h3 id="rpc">RPC</h3> <p>Yes. In fact RPCs are now so widely used that I've seen multiple RPCs considered harmful talks.</p> <h3 id="distributed-systems">Distributed systems</h3> <p>Yes. These are so ubiquitous that startups with zero distributed systems expertise regularly use distributed systems provided by Amazon or Microsoft, and it's totally fine. The systems aren't perfect and there are some infamous downtime incidents, but if you compare the bit error rate of random storage from 1999 to something like EBS or Azure Blob Storage, distributed systems don't look so bad.</p> <h3 id="security">Security</h3> <p>Maybe? As with formal methods, a handful of projects with very high real world impact get a lot of mileage out of security research. But security still isn't a first class concern for most programmers.</p> <h3 id="conclusion">Conclusion</h3> <p>What's worked in computer systems research?</p> <table> <thead> <tr> <th>Topic</th> <th>1999</th> <th>2015</th> </tr> </thead> <tbody> <tr> <td>Virtual memory</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Address spaces</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Packet nets</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Objects / subtypes</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>RDB and SQL</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Transactions</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Bitmaps and GUIs</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Web</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Algorithms</td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>Parallelism</td> <td>Maybe</td> <td>Maybe</td> </tr> <tr> <td>RISC</td> <td>Maybe</td> <td>No</td> </tr> <tr> <td>Garbage collection</td> <td>Maybe</td> <td>Yes</td> </tr> <tr> <td>Reuse</td> <td>Maybe</td> <td>Yes</td> </tr> <tr> <td>Capabilities</td> <td>No</td> <td>Yes</td> </tr> <tr> <td>Fancy type systems</td> <td>No</td> <td>No</td> </tr> <tr> <td>Functional programming</td> <td>No</td> <td>Maybe</td> </tr> <tr> <td>Formal methods</td> <td>No</td> <td>Maybe</td> </tr> <tr> <td>Software engineering</td> <td>No</td> <td>No</td> </tr> <tr> <td>RPC</td> <td>No</td> <td>Yes</td> </tr> <tr> <td>Distributed computing</td> <td>No</td> <td>Yes</td> </tr> <tr> <td>Security</td> <td>No</td> <td>Maybe</td> </tr> </tbody> </table> <p><br> Not only is every Yes from 1999 still Yes today, seven of the Maybes and Nos were upgraded, and only one was downgraded. And on top of that, there are a lot of topics like neural networks that weren't even worth adding to the list as a No that are an unambiguous Yes today.</p> <p>In 1999, I was taking the SATs and applying to colleges. Today, I'm not really all that far into my career, and the landscape has changed substantially; many previously impractical academic topics are now widely used in industry. I probably have twice again as much time until the end of my career and <a href="http://danluu.com/infinite-disk/">things are changing faster now than they were in 1999</a>. After reviewing Lampson's 1999 talk, I'm much more optimistic about research areas that haven't yielded much real-world impact (yet), like capability based computing and fancy type systems. It seems basically impossible to predict what areas will become valuable over the next thirty years.</p> <h3 id="correction">Correction</h3> <p>This post originally had Capabilities as a No in 2015. In retrospect, I think that was a mistake and it should have been a Yes due to use on mobile.</p> <p><small> Thanks to Seth Holloway, Leah Hanson, Ian Whitlock, Lindsey Kuper, Chris Ball, Steven McCarthy, Joe Wilder, David Wragg and Alex Clemmer for comments/discussion. </small></p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compressing Chess Moves for Fun and Profit (159 pts)]]></title>
            <link>https://mbuffett.com/posts/compressing-chess-moves/</link>
            <guid>39717615</guid>
            <pubDate>Fri, 15 Mar 2024 16:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mbuffett.com/posts/compressing-chess-moves/">https://mbuffett.com/posts/compressing-chess-moves/</a>, See on <a href="https://news.ycombinator.com/item?id=39717615">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Chess notation has come a long way since <a href="https://en.wikipedia.org/wiki/Descriptive_notation">descriptive
notation</a>, now we have nice
and decipherable Standard Algebraic Notation, like <code>Qxf7</code> (queen takes on
f7) or <code>Nf3</code> (knight takes on f3).</p><p>This is a great text format, but a massive waste of
space if you’re trying to store a lot of these. <code>Qxf7</code> takes 4 bytes, or 32 bits. Let’s do some rough back-of-the-envelope math of how much information is actually being transmitted though.
This move affects one of 6 pieces (3 bits), this move is also a capture (1
bit), and it specifies a destination squuare (64 possibilities == 6 bits). Add those up, you get 10 bits. Far from the 32 bits that the textual representation needs.</p><p>Why do I care? I run a site that stores a <em>ton</em> of
chess lines, something like 100 million in total. Assume an average of 6 moves for each line, that’s 600 million moves. The database is growing large
enough that querying it is IO-constrained. I want to speed up the reads from this database when I’m fetching thousands of lines.</p><h2 id="a-first-pass">A first pass</h2><p>First some general numbers:</p><ul><li>Encoding a file or rank (a-h or 1-8) takes 3 bits (8 possibilities)</li><li>Encoding a piece (k, q, r, b, n, p) takes 3 bits (6 possibilities)</li><li>Encoding a square takes 6 bits (64 possibilities)</li></ul><p>So let’s see the pieces we’re working with for encoding a SAN.</p><p>So let’s go with the most naive approach.</p><ul><li>Which piece was it? 3 bits</li><li>Is it a capture? 1 bit</li><li>Do we have to disabiguate it (ie <code>Ngf3</code>)? Maximum of 2 bits + 6 bits (this is explained more further down)</li><li>Where did it go? 6 bits</li><li>Is it a promotion, and to which piece? 7 bits</li><li>Is it a check? 1 bit</li><li>Is it a checkmate? 1 bit</li><li>Is it a castle? Short or long? 2 bits</li></ul><p>This gives us a total of <code>3+1+2+6+6+7+1+1+2 = 29</code> bits, or about 3.5 bytes per move. That’s not great though. A lot of moves actually take up less bits than that in text format.</p><h2 id="getting-smarter">Getting smarter</h2><h3 id="the-first-few-bits">The first few bits</h3><p>In the first pass, we just encoded the piece that moved using 3 bits, but that
leaves 2 unused permutations available, since there are only 6 pieces in chess.
Luckily, there are two very common moves that fit neatly into this hole, those
being short castles (<code>O-O</code>) and long castles (<code>O-O-O</code>).</p><p>So we can encode the first 3 bits like so:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>match</span> first_bytes {
</span></span><span><span>    FirstBytes::Pawn <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>false</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::Knight <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>false</span>, <span>true</span>]),
</span></span><span><span>    FirstBytes::Bishop <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>true</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::Rook <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>true</span>, <span>true</span>]),
</span></span><span><span>    FirstBytes::Queen <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>false</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::King <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>false</span>, <span>true</span>]),
</span></span><span><span>    FirstBytes::ShortCastling <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>true</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::LongCastling <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>true</span>, <span>true</span>]),
</span></span><span><span>}
</span></span></code></pre></div><h3 id="the-destination-square">The destination square</h3><p>In all cases except castling, you need to know the square that the piece is moving to, to reproduce the SAN. So we’ll skip this entirely when castling, but otherwise always include 6 bits for the destination square.</p><h4 id="just-kidding-pawn-moves">Just kidding, pawn moves!</h4><p>There’s another exception to the destination square rule that might not seem so
obvious. Let’s take the move <code>exf6</code>. We know it’s a pawn move from the e-file,
so we don’t really need to encode the file it’s capturing using 6 bits. After
all, you’ll never see <code>exa6</code>. So in these cases instead of 6 bits for the
destination square, we only need 4 (one for the direction, and one for the
rank).</p><p>But we can get even more clever here. Take <code>hxg6</code> for example. You know as soon as you see <code>hx</code>, that the file is going to be the g-file. So we don’t even need the extra bit to encode direction, we can just encode the file once, and the rank once.</p><p>So here’s the setup:</p><ul><li>Pawn capture from b-g files: 3 bits for the file you’re capturing from, 1 bit for the direction of the capture, and 3 bits for the rank<ul><li>Total: 10 bits for movement</li></ul></li><li>Pawn capture from a and h files: 3 bits for the file you’re capturing from, 3 bits for rank<ul><li>Total: 6 bits for movement</li></ul></li></ul><h3 id="special-moves">“Special” moves</h3><p>It’s a bit of a waste to encode for each move, whether it’s a promotion, check,
checkmate, capture, etc. After all, the vast majority of chess moves in a game are not any of these.</p><p>So we’ll devote one bit to determining whether a move is “special”. It means we
have to use an extra bit for moves that are promotions/checks/captures, but it
also means that we save a whole lot of bits for “regular” moves.</p><h3 id="promotions">Promotions</h3><p>Promotions are nice, because even though there are 6 chess pieces, there are
only 4 valid pieces that you can promote to (after all, you can’t promote to a
King or Pawn). So we only need 2 bits instead of 3, to encode the promotion
piece.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>if</span> <span>let</span> Some(promotion) <span>=</span> promotion {
</span></span><span><span>    bits.push(<span>true</span>);
</span></span><span><span>    bits.extend(<span>match</span> promotion {
</span></span><span><span>        PromotionPiece::Queen <span>=&gt;</span> <span>&amp;</span>[<span>false</span>, <span>false</span>],
</span></span><span><span>        PromotionPiece::Rook <span>=&gt;</span> <span>&amp;</span>[<span>false</span>, <span>true</span>],
</span></span><span><span>        PromotionPiece::Bishop <span>=&gt;</span> <span>&amp;</span>[<span>true</span>, <span>false</span>],
</span></span><span><span>        PromotionPiece::Knight <span>=&gt;</span> <span>&amp;</span>[<span>true</span>, <span>true</span>],
</span></span><span><span>    });
</span></span><span><span>} <span>else</span> {
</span></span><span><span>    bits.push(<span>false</span>);
</span></span><span><span>}
</span></span></code></pre></div><h3 id="disambiguation">Disambiguation</h3><p>Disambiguation is a bit thorny. You have to encode a surprising amount of information, to be able to decode the SAN exactly as you received it.</p><p>Take <code>Ngf3</code> for example. Besides the usual stuff you also need to encode the
disambiguation (file=g). There are 3 different disambiguation possibilities (rank, file, or whole square), so
we need 2 bits. One goes to waste but disambiguated moves aren’t that common
and I can’t think of a nice way to take advantage of that last permutation.</p><p>So we always use 2 bits, then either 3 bits for the rank, 3 bits for the file, or 6 bits for a whole square (very rare).</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>if</span> <span>let</span> Some(disambiguate) <span>=</span> disambiguate {
</span></span><span><span>    bits.push(<span>true</span>);
</span></span><span><span>    <span>match</span> disambiguate {
</span></span><span><span>        Disambiguation::File(file) <span>=&gt;</span> {
</span></span><span><span>            bits.extend(<span>&amp;</span>[<span>false</span>, <span>true</span>]);
</span></span><span><span>            bits.extend(square_component_to_bits(file));
</span></span><span><span>        }
</span></span><span><span>        Disambiguation::Rank(rank) <span>=&gt;</span> {
</span></span><span><span>            bits.extend(<span>&amp;</span>[<span>true</span>, <span>false</span>]);
</span></span><span><span>            bits.extend(square_component_to_bits(rank));
</span></span><span><span>        }
</span></span><span><span>        Disambiguation::Square(square) <span>=&gt;</span> {
</span></span><span><span>            bits.extend(<span>&amp;</span>[<span>true</span>, <span>true</span>]);
</span></span><span><span>            bits.extend(square_component_to_bits(square.<span>0</span>));
</span></span><span><span>            bits.extend(square_component_to_bits(square.<span>1</span>));
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>} <span>else</span> {
</span></span><span><span>    bits.push(<span>false</span>);
</span></span><span><span>}
</span></span></code></pre></div><h2 id="how-does-this-do">How does this do?</h2><table><thead><tr><th>Move</th><th>Original bits</th><th>Encoded bits</th><th>Savings</th></tr></thead><tbody><tr><td>e4</td><td>16</td><td>10</td><td>37.5%</td></tr><tr><td>exd5</td><td>32</td><td>12</td><td>62.5%</td></tr><tr><td>Nf3+</td><td>24</td><td>10</td><td>58.33%</td></tr><tr><td>Qxa5+</td><td>40</td><td>16</td><td>60%</td></tr><tr><td>cxd8=Q#</td><td>56</td><td>16</td><td>71.43%</td></tr></tbody></table><p>Not bad! We’re saving anywhere from 37.5% to 71.43% of the bits.</p><h2 id="pgns">PGNs</h2><p>You may be thinking something like this: “Isn’t is sorta cheating to measure these in bits? Since you can only address one byte at a time, needing 10 bits for a move is virtually the same as 16 bits”</p><p>Well yes, that’s true and means that we get less savings when storing individual SANs. But they don’t account for the majority of what I’m storing, which are PGNs.</p><p>PGNs are a way to store lines or games, and they look something like this:</p><pre tabindex="0"><code>1.e4 d5 2.exd5 Nf6 3.d4 Bg4 4.Be2 Bxe2 5.Nxe2 Qxd5 6.O-O Nc6 7.c3 O-O-O 8.Qb3 Qh5 9.Nf4 Qh4 10.Qxf7 Ng4 11.h3 Nf6 12.Be3 e6 13.Qxe6+ Kb8 14.Nd2 Bd6 15.g3 Qh6 16.Qf5 Ne7 17.Qa5 b6 18.Qa6 Bxf4 19.Bxf4 Qxh3 20.Nf3 Ned5 21.Be5 Qh5 22.Kg2 Qg6 23.Rae1 Nh5 24.Nh4 Nhf4+ 25.Bxf4 Nxf4+ 26.Kh2 b5 27.Qxg6 hxg6 28.gxf4 Rxh4+ 29.Kg3 Rdh8 30.Re5 Rh3+ 31.Kg4 R8h4+ 32.Kg5 Rh5+ 33.Kxg6 Rh6+ 34.Kf7 Rh7 35.Rxb5+ Kc8 36.Rg5 g6+ 37.Kxg6 R3h6+ 38.Kf5 Rf7+ 39.Kg4 Rhf6 40.f5 Kd7 41.Re1 Kd6 42.Re8 Kd7 43.Ra8 Rh6 44.Rxa7 Rfh7 45.Kf4 Rh4+ 46.Rg4 Rh2 47.f3 Rxb2 48.f6 Rf7 49.Rg7 Ke6 50.Rxf7 Kxf7 51.Rxc7+ Kxf6 52.a4 Ra2 53.Rc4 Ra3 54.d5 Ke7 55.Ke5 Kd7 56.f4 Ra2 57.Kd4 Rd2+ 58.Kc5 Ra2 59.Kb5 Kd6 60.Rd4 Rb2+ 61.Ka6 Ra2 62.a5 Kc5 63.d6 Ra3 64.d7 Kc6 65.d8=Q Rxa5+ 66.Kxa5 Kc5 67.Qc7#
</code></pre><p>This PGN is 759 bytes. There’s a ton of wasted space though. One byte between each move (the space). Then <strong>at least 3 bytes</strong> between full moves (<code> 2.</code>). This is sort of crazy, and if we combine our SAN encoding, we can compress this to be way smaller.</p><p>If we encode the whole PGN using our SAN encoding, with no space between moves because we know once we’ve reached the end of a move, we can compress this specific 759-byte PGN down to <strong>195 bytes</strong>, for a savings of <strong>74%</strong>.</p><h2 id="impact">Impact</h2><p>This hasn’t been deployed yet, I’m working on a ton of other performance
improvements. But I anticipate this along with EPD compression (which I may
write another at article on) will reduce the size of the database by about 70%.
We’re almost entierly read-constrained, which should mean a 3x speedup for the
most expensive queries we run.</p><h2 id="speed">Speed</h2><p>Another consideration here is the speed of doing this encoding/decoding; will
it just cancel out the gains from having a much smaller database? Turns out,
computers are really fast at this stuff, and conversion to and from this
encoding is a rounding error.</p><p>I’m using Rust + the <code>bitvec</code> library. Encoding and then decoding 1000 moves
takes about 600,000ns, or 0.6ms. I haven’t taken a performance pass at all
either, and there’s a few places I know are very inefficient. I’m guessing I’m
not even within 10x of optimal, but it should be good enough.</p></div></article><div><p>Thanks for reading! If you have any questions, comments, or just want to say hi,
please email me at <a href="mailto:me@mbuffett.com">me@mbuffett.com</a>. I'm not
very active <a href="https://twitter.com/MarcusBuffett">on twitter</a>,
but you can choose to follow me in case that changes.</p><p>If you're into chess, I've made a <a href="https://chessbook.com/">repertoire builder</a>. It uses statistics from
hundreds of millions of games at your level to find the gaps in your
repertoire, and uses spaced repetition to quiz you on them.</p><p>Samar Haroon, my girlfriend, has started a podcast where she talks about the
South Asian community, from the perspective of a psychotherapist.
<a href="https://open.spotify.com/show/7teSzaHt5I3r9s5PPLZFrF?si=J1-h-uFCTLyXGPbZnYSIGQ" target="_blank">Go check it out!</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC and DOJ want to free McDonald's ice cream machines from DMCA repair rules (258 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/</link>
            <guid>39717558</guid>
            <pubDate>Fri, 15 Mar 2024 16:30:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/">https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=39717558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      I scream, you scream, we all scream for 1201(c)3 exemptions    —
</h4>
            
            <h2 itemprop="description">McFlurries are a notable part of petition for commercial and industrial repairs.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/03/ice_cream_fix-800x536.jpg" alt="Taylor ice cream machine, with churning spindle removed by hand.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/03/ice_cream_fix-scaled.jpg" data-height="1714" data-width="2560">Enlarge</a> <span>/</span> Taylor's C709 Soft Serve Freezer isn't so much mechanically complicated as it is a software and diagnostic trap for anyone without authorized access.</p></figcaption>  </figure>

  




<!-- cache hit 149:single/related:70bb8fbbd06d2f1057c24bd5f3501ac0 --><!-- empty -->
<p>Many devices have been made difficult or financially nonviable to repair, whether by design or because of a lack of parts, manuals, or specialty tools. Machines that make ice cream, however, seem to have a special place in the hearts of lawmakers. Those machines are often broken and locked down for only the most profitable repairs.</p>
<p>The Federal Trade Commission and the antitrust division of the Department of Justice have <a href="https://www.ftc.gov/system/files/ftc_gov/pdf/ATR-FTC-JointComment.pdf">asked the US Copyright Office</a> (PDF) to exempt "commercial soft serve machines" from the anti-circumvention rules of <a href="https://www.copyright.gov/1201/2018/">Section 1201</a> of the Digital Millennium Copyright Act (DMCA). The governing bodies also submitted proprietary diagnostic kits, programmable logic controllers, and enterprise IT devices for DMCA exemptions.</p>
<p>"In each case, an exemption would give users more choices for third-party and self-repair and would likely lead to cost savings and a better return on investment in commercial and industrial equipment," the joint comment states. Those markets would also see greater competition in the repair market, and companies would be prevented from using DMCA laws to enforce monopolies on repair, according to the comment.</p>
<p>The joint comment builds upon a petition filed by repair vendor and advocate iFixit and interest group Public Knowledge, which advocated for broad reforms while keeping a relatable, ingestible example at its center. McDonald's soft serve ice cream machines, which are <a href="https://mcbroken.com/">famously frequently broken</a>, are supplied by industrial vendor Taylor. <a href="https://publicknowledge.org/public-knowledge-petitions-copyright-office-for-dmca-exemption-for-ice-cream-machines/">Taylor's C709 Soft Serve Freezer</a> requires lengthy, finicky warm-up and cleaning cycles, produces obtuse error codes, and, perhaps not coincidentally, costs $350 per 15 minutes of service for a Taylor technician to fix. iFixit <a href="https://www.ifixit.com/News/80215/whats-inside-that-mcdonalds-ice-cream-machine-broken-copyright-law">tore down such a machine</a>, confirming the lengthy process between plugging in and soft serving.
</p><p>After one company built a Raspberry Pi-powered device, the <a href="https://www.kytch.com/landing">Kytch</a>, that could provide better diagnostics and insights, Taylor moved to ban franchisees from installing the device, then offered up its own competing product. Kytch has <a href="https://www.wired.com/story/kytch-ice-cream-machine-hackers-sue-mcdonalds-900-million/">sued Taylor for $900 million</a>&nbsp;in a case that is still pending.</p>                                            
                                                        
<p>Beyond ice cream, the petitions to the Copyright Office would provide more broad exemptions for industrial and commercial repairs that require some kind of workaround, decryption, or other software tinkering. Going past technological protection measures (TPMs) was made illegal by the 1998 DMCA, which was put in place largely because of the concerns of media firms facing what they considered rampant piracy.</p>
<p>Every three years, the Copyright Office allows for petitions to exempt certain exceptions to DMCA violations (and renew prior exemptions). Repair advocates have won exemptions for farm equipment repair, <a href="https://arstechnica.com/tech-policy/2021/10/us-copyright-office-oks-right-to-repair-for-video-game-console-optical-drives/">video game consoles</a>, cars, and certain medical gear. The exemption is often granted for device fixing if a repair person can work past its locks, but not for the distribution of tools that would make such a repair far easier. The esoteric nature of such "release valve" offerings has led groups like the EFF to <a href="https://arstechnica.com/tech-policy/2016/07/eff-sues-us-government-saying-copyright-rules-on-drm-are-unconstitutional/">push for the DMCA's abolishment</a>.</p>
<p>DMCA exemptions occur on a parallel track to <a href="https://arstechnica.com/tech-policy/2024/03/oregon-oks-right-to-repair-bill-that-bans-the-blocking-of-aftermarket-parts/">state right-to-repair bills</a> and broader federal action. President Biden issued <a href="https://arstechnica.com/tech-policy/2021/07/bidens-right-to-repair-order-could-stop-companies-from-blocking-diy-fixes/">an executive order</a> that included a push for repair reforms. The FTC has issued studies that call out <a href="https://www.ftc.gov/system/files/documents/reports/nixing-fix-ftc-report-congress-repair-restrictions/nixing_the_fix_report_final_5521_630pm-508_002.pdf">unnecessary repair restrictions</a> and has taken <a href="https://www.ftc.gov/news-events/news/press-releases/2022/10/ftc-approves-final-orders-right-repair-cases-against-harley-davidson-mwe-investments-weber">action</a> against firms like Harley-Davidson, Westinghouse, and grill maker Weber for tying warranties to an authorized repair service.</p>
<p><i>Disclosure: Kevin Purdy previously worked for iFixit. He has no financial ties to the company.</i></p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open-source, browser-local data exploration using DuckDB-WASM and PRQL (169 pts)]]></title>
            <link>https://github.com/pretzelai/pretzelai</link>
            <guid>39717268</guid>
            <pubDate>Fri, 15 Mar 2024 16:02:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pretzelai/pretzelai">https://github.com/pretzelai/pretzelai</a>, See on <a href="https://news.ycombinator.com/item?id=39717268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">🥨 Pretzel</h2><a id="user-content--pretzel" aria-label="Permalink: 🥨 Pretzel" href="#-pretzel"></a></p>
<p dir="auto"><a href="https://github.com/pretzelai/pretzelai/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/c74c83918e248d250e94ac5b8c93771ebb0d3127155fc17220595489455e643d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f707265747a656c61692f707265747a656c6169" alt="License" data-canonical-src="https://img.shields.io/github/license/pretzelai/pretzelai"></a>
<a href="https://github.com/pretzelai/pretzelai"><img src="https://camo.githubusercontent.com/93e4c3cd9a6b630a89aaa2c6a9309d696c0ca515a050ac7ea5245a5b1122af91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f707265747a656c61692f707265747a656c61693f7374796c653d736f6369616c" alt="GitHub Stars" data-canonical-src="https://img.shields.io/github/stars/pretzelai/pretzelai?style=social"></a></p>
<p dir="auto">Live deployed build: <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></p>
<p dir="auto">Pretzel is an open-source, offline browser-based tool for fast and intuitive data exploration and visualization. It can handle large data files, runs locally in your browser, and requires no backend setup. Pretzel makes it easy to manipulate data via visual chained data transform blocks. It's also reactive - chaging an tranform block in the chain automatically updates all transform blocks and charts that follow.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121360087/313066071-e7f20a16-b19c-4a29-b468-88d42eaa9b43.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MDcxLWU3ZjIwYTE2LWIxOWMtNGEyOS1iNDY4LTg4ZDQyZWFhOWI0My5naWY_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xNGIzOWM4ZDRmMTRhMDEyMWIzMmVjMGFlOGQ0ZDNkMjRhYzIyNjNhMjBlOTdhZGJlYjhmMTI5ZTM3ZjYxNjE4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.gcFiE42OSFIMDrDV7Z6IdBzOBgxHjnEOVfO4V2X5aSU"><img src="https://private-user-images.githubusercontent.com/121360087/313066071-e7f20a16-b19c-4a29-b468-88d42eaa9b43.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MDcxLWU3ZjIwYTE2LWIxOWMtNGEyOS1iNDY4LTg4ZDQyZWFhOWI0My5naWY_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xNGIzOWM4ZDRmMTRhMDEyMWIzMmVjMGFlOGQ0ZDNkMjRhYzIyNjNhMjBlOTdhZGJlYjhmMTI5ZTM3ZjYxNjE4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.gcFiE42OSFIMDrDV7Z6IdBzOBgxHjnEOVfO4V2X5aSU" alt="demo.gif" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🚀 Blazing-fast performance with WebAssembly-based <a href="https://duckdb.org/" rel="nofollow">DuckDB</a> and <a href="https://prql-lang.org/" rel="nofollow">PRQL</a></li>
<li>🔍 Intuitive data exploration with a visual, top-down pipeline of data transformations and visualizations</li>
<li>🧠 AI-powered transformation block to help with fast data manipulation</li>
<li>🔒 Privacy-first design: run Pretzel AI locally or host it yourself for full control over your data</li>
<li>📊 Upcoming features: Local LLM support, API calls, in-browser Python support with <a href="https://github.com/pyodide/pyodide">Pyodide</a>, save and share workflows securely and canvas based table rendering</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#demo-video">Demo video</a></li>
<li><a href="#getting-started">Getting started</a>
<ul dir="auto">
<li><a href="#website-easiest">Website (Easiest)</a></li>
<li><a href="#offline-standalone-app">Offline standalone app</a></li>
<li><a href="#developers">Developers</a>
<ul dir="auto">
<li><a href="#run-locally">Run locally</a></li>
<li><a href="#host-pretzel">Host Pretzel</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#optional-configuration">Optional Configuration</a></li>
<li><a href="#implemented-transformation-blocks">Implemented Transformation Blocks</a></li>
<li><a href="#known-bugs">Known Bugs</a></li>
<li><a href="#contact">Contact</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo video</h2><a id="user-content-demo-video" aria-label="Permalink: Demo video" href="#demo-video"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description Pretzel.mp4">Pretzel.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/161899563/313245661-cb5b0f00-4add-40e8-b0c8-f59a0186e3ff.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xNjE4OTk1NjMvMzEzMjQ1NjYxLWNiNWIwZjAwLTRhZGQtNDBlOC1iMGM4LWY1OWEwMTg2ZTNmZi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zMjU1N2M2YzNmMTg5YmE1NDljZjRiZTcwNTE2NzI0YmY4YWI0YmYzODBlMjc4NTNlOTg4ZjQ3OWNkMjcxMTU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.p9Cwumfo8cZKz_5umvOXFKLi_sVhkhB5q0S65SARwZs" data-canonical-src="https://private-user-images.githubusercontent.com/161899563/313245661-cb5b0f00-4add-40e8-b0c8-f59a0186e3ff.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xNjE4OTk1NjMvMzEzMjQ1NjYxLWNiNWIwZjAwLTRhZGQtNDBlOC1iMGM4LWY1OWEwMTg2ZTNmZi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zMjU1N2M2YzNmMTg5YmE1NDljZjRiZTcwNTE2NzI0YmY4YWI0YmYzODBlMjc4NTNlOTg4ZjQ3OWNkMjcxMTU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.p9Cwumfo8cZKz_5umvOXFKLi_sVhkhB5q0S65SARwZs" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Website (Easiest)</h3><a id="user-content-website-easiest" aria-label="Permalink: Website (Easiest)" href="#website-easiest"></a></p>
<p dir="auto">The easiest way to use Pretzel is to visit <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Offline standalone app</h3><a id="user-content-offline-standalone-app" aria-label="Permalink: Offline standalone app" href="#offline-standalone-app"></a></p>
<p dir="auto">Since Pretzel doesn't have a backend you can easily install it as a Chrome app and it will work even without internet (for those long flights!)</p>
<ol dir="auto">
<li>
<p dir="auto">Visit <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a> in Chrome</p>
</li>
<li>
<p dir="auto">Click the install app icon</p>
</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121360087/313066187-c6276699-5109-4e59-8bf5-2858c51cb4c3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MTg3LWM2Mjc2Njk5LTUxMDktNGU1OS04YmY1LTI4NThjNTFjYjRjMy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00OWNhNTYyZWM4NDNkYmFkYzc2ZDRiMmIyM2Y4NjRhZTFhYWVhODgwODM0MTNlNGQ3NzBiMjI3ZTc1Y2IzYzU4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.GbmRczgdy_3OGVxJiejCrCV8iI2cODu7sqC7w45rRFA"><img width="521" alt="pretzel_chrome_install" src="https://private-user-images.githubusercontent.com/121360087/313066187-c6276699-5109-4e59-8bf5-2858c51cb4c3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MTg3LWM2Mjc2Njk5LTUxMDktNGU1OS04YmY1LTI4NThjNTFjYjRjMy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00OWNhNTYyZWM4NDNkYmFkYzc2ZDRiMmIyM2Y4NjRhZTFhYWVhODgwODM0MTNlNGQ3NzBiMjI3ZTc1Y2IzYzU4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.GbmRczgdy_3OGVxJiejCrCV8iI2cODu7sqC7w45rRFA"></a>
<ol start="3" dir="auto">
<li>Now you can launch Pretzel as a standalone app. It will also work offline, it may error if you try to use some internet feature (like the AI Block), just close it and open it again to fix it</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121360087/313066562-cc13e552-d93a-4990-be22-1f6b5d906b15.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2NTYyLWNjMTNlNTUyLWQ5M2EtNDk5MC1iZTIyLTFmNmI1ZDkwNmIxNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYzQzMzg0MzRjY2MyYTFiMjVlN2EwMzg3NTNhYTRiYjIwMGM1YTMxYzY4ZmM5MTY4OGFmMDYwYWFkNmVhZTNlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.6kReT3qm1l4_zW6J34SEr9DyUpEEYNDXSKkbJaA-Smo"><img width="268" alt="pretzel_app_icon" src="https://private-user-images.githubusercontent.com/121360087/313066562-cc13e552-d93a-4990-be22-1f6b5d906b15.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2NTYyLWNjMTNlNTUyLWQ5M2EtNDk5MC1iZTIyLTFmNmI1ZDkwNmIxNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYzQzMzg0MzRjY2MyYTFiMjVlN2EwMzg3NTNhYTRiYjIwMGM1YTMxYzY4ZmM5MTY4OGFmMDYwYWFkNmVhZTNlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.6kReT3qm1l4_zW6J34SEr9DyUpEEYNDXSKkbJaA-Smo"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Developers</h3><a id="user-content-developers" aria-label="Permalink: Developers" href="#developers"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Run locally</h4><a id="user-content-run-locally" aria-label="Permalink: Run locally" href="#run-locally"></a></p>
<p dir="auto">To run Pretzel locally, follow these steps:</p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repository:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/pretzelai/pretzelai.git"><pre><code>git clone https://github.com/pretzelai/pretzelai.git
</code></pre></div>
</li>
<li>
<p dir="auto">Install dependencies:</p>

</li>
<li>
<p dir="auto">Start the development server:</p>

</li>
<li>
<p dir="auto">Open your browser and navigate to <code>http://localhost:3000</code></p>
</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">Host Pretzel</h4><a id="user-content-host-pretzel" aria-label="Permalink: Host Pretzel" href="#host-pretzel"></a></p>
<p dir="auto">To host Pretzel, follow these steps (it's just a static website!):</p>
<ol dir="auto">
<li>Build the app</li>
</ol>

<ol start="2" dir="auto">
<li>Upload the contents of the <code>build</code> folder to your hosting. This is what you can find live at <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Optional configuration</h2><a id="user-content-optional-configuration" aria-label="Permalink: Optional configuration" href="#optional-configuration"></a></p>
<ul dir="auto">
<li>Bug report box: Update <code>/src/lib/config.ts</code> with your PostHog config to let users report bugs directly on the website</li>
<li>AI Endpoint: Deploy a cloud function to provide an AI endpoint for users without an OpenAI API key. Check the <code>cloud</code> folder for instructions.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implemented transformation blocks</h2><a id="user-content-implemented-transformation-blocks" aria-label="Permalink: Implemented transformation blocks" href="#implemented-transformation-blocks"></a></p>
<ul dir="auto">
<li><strong>Upload:</strong> accepts CSV / Excel (XLSX) files</li>
<li><strong>Filter</strong>: string/number/date filtering including nested filters</li>
<li><strong>Ask AI</strong>: connects to OpenAI to transform user command to SQL</li>
<li><strong>Pivot</strong>: to create a pivot table (you can also go group-by using this - only use the <code>Rows</code> and <code>Values</code> fields)</li>
<li><strong>Sort</strong>: sorts ascending or descending on multiple columns</li>
<li><strong>Chart</strong>: supports line (including multi-line) charts, bar charts (grouped and stacked) and scatter plot</li>
<li><strong>Create column</strong>: make a new column with basic math or use <a href="https://prql-lang.org/book/reference/declarations/functions.html" rel="nofollow">PRQL functions</a></li>
<li><strong>Remove columns</strong>: easily add/remove columns with visual toggles</li>
<li><strong>Table</strong>: add a table in the middle of your workflow to visualize data in a intermediate step</li>
<li><strong>Download</strong>: export your transformed data in CSV</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known Bugs</h2><a id="user-content-known-bugs" aria-label="Permalink: Known Bugs" href="#known-bugs"></a></p>
<ul dir="auto">
<li>Dates are sometimes parsed incorrectly - existing GH issue <a href="https://github.com/pretzelai/pretzelai/issues/23" data-hovercard-type="issue" data-hovercard-url="/pretzelai/pretzelai/issues/23/hovercard">here</a></li>
<li>Table panel is slow for large datasets. We're planning on moving to a canvas based table</li>
<li>[Rare] Charts axes can sometimes not be ordered correctly</li>
</ul>
<p dir="auto">Please report any bugs you find in <a href="https://github.com/pretzelai/pretzelai">GitHub issues</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">You can email us at founders [at] withpretzel [dot] com.</p>
<p dir="auto">We also read all the feedback and bugs you report at the top left of <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reversing for dummies – x86 assembly and C code (Beginner/ADHD friendly) (115 pts)]]></title>
            <link>https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html</link>
            <guid>39716494</guid>
            <pubDate>Fri, 15 Mar 2024 14:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html">https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html</a>, See on <a href="https://news.ycombinator.com/item?id=39716494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3 id="context">Context</h3>

          <p>
            Before I got into reverse engineering, executables always seemed
            like black magic to me. I always wondered how stuff worked under the
            hood, and how binary code is represented inside .exe files, and how
            hard it is to modify this ‘compiled code’ without access to the
            original source code.
          </p>

          <p>
            But one of the main intimidating hurdles always seemed to be the
            assembly language, it’s the thing that scares most people away from
            trying to learn about this field.
          </p>

          <p>
            That’s the main reason why I thought of writing this
            straight-to-the-point article that only contains the essential stuff
            that you encounter the most when reversing, albeit missing crucial
            details for the sake of brevity, and assumes the reader has a reflex
            of finding answers online, looking up definitions, and more
            importantly, coming up with examples/ideas/projects to practice on.
          </p>

          <p>
            The goal is to hopefully guide an aspiring reverse engineer and
            arouse motivation towards learning more about this seemingly elusive
            passion.
          </p>

          <p>
            <strong><em>Note</em></strong>: This article assumes the reader has elementary knowledge
            regarding the
            <a href="https://en.wikipedia.org/wiki/Hexadecimal" rel="noopener noreferrer" target="_blank">hexadecimal numeral system</a>, as well as the
            <a href="https://en.wikipedia.org/wiki/C_(programming_language)" rel="noopener noreferrer" target="_blank">C programming language</a>, and is based on a 32-bit Windows executable case study - results
            might differ across different OSes/architectures.
          </p>

          <h3 id="introduction">Introduction</h3>

          <h4 id="compilation">Compilation</h4>

          <p>
            After writing code using a
            <a href="https://en.wikipedia.org/wiki/Compiled_language" rel="noopener noreferrer" target="_blank">compiled language</a>, a compilation takes place <del>(duh)</del>, in order to generate
            the output binary file (an example of such is an .exe file).
          </p>

          <p>
            <img src="https://i.0x44.cc/b/compilation-c-to-exe-file.png">
          </p>

          <p>
            Compilers are sophisticated programs which do this task. They make
            sure the syntax of your <del>ugly</del> code is correct, before
            compiling and optimizing the resulting machine code by minimizing
            its size and improving its performance, whenever applicable.
          </p>

          <h4 id="binary-code">Binary code</h4>

          <p>
            As we were saying, the resulting output file contains binary code,
            which can only be ‘understood’ by a CPU, it’s essentially a
            succession of varying-length instructions to be executed in order -
            here’s what some of them look like:
          </p>

          <table>
            <thead>
              <tr>
                <th>CPU-readable instruction data (in hex)</th>
                <th>Human-readable interpretation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>55</td>
                <td>push ebp</td>
              </tr>
              <tr>
                <td>8B EC</td>
                <td>mov ebp, esp</td>
              </tr>
              <tr>
                <td>83 EC 08</td>
                <td>sub esp, 8</td>
              </tr>
              <tr>
                <td>33 C5</td>
                <td>xor eax, ebp</td>
              </tr>
              <tr>
                <td>83 7D 0C 01</td>
                <td>cmp dword ptr [ebp+0Ch], 1</td>
              </tr>
            </tbody>
          </table>

          <p>
            These instructions are predominantly arithmetical, and they
            manipulate CPU registers/flags as well as volatile memory, as
            they’re executed.
          </p>

          <h4 id="cpu-registers">CPU registers</h4>

          <p>
            <a href="https://en.wikipedia.org/wiki/Processor_register" rel="noopener noreferrer" target="_blank">A CPU register</a>
            is almost like a temporary integer variable - there’s a small fixed
            number of them, and they exist because they’re quick to access,
            unlike memory-based variables, and they help the CPU keep track of
            its data (results, operands, counts, etc.) during execution.
          </p>

          <p>
            It’s important to note the presence of a special register called the
            <a href="https://en.wikipedia.org/wiki/FLAGS_register" rel="noopener noreferrer" target="_blank"><code>FLAGS</code>
              register</a>
            (<code>EFLAGS</code> on
            32-bit), which houses a bunch of flags (boolean indicators), which
            hold information about the state of the CPU, which include details
            about the last arithmetic operation (zero:
            <code>ZF</code>,
            overflow:
            <code>OF</code>,
            parity:
            <code>PF</code>, sign:
            <code>SF</code>, etc.).
          </p>

          <p>
            <img src="https://i.0x44.cc/b/x32dbg-cpu-registers.png">
            <small>CPU registers visualized while debugging a 32-bit process on
              x64dbg, a debugging tool.</small>
          </p>

          <p>
            Some of these registers can also be spotted on the assembly excerpt
            mentioned <a href="#binary-code">previously</a>, namely:
            <code>EAX</code>,
            <code>ESP</code> (stack
            pointer) and
            <code>EBP</code> (base
            pointer).
          </p>

          <h4 id="memory-access">Memory access</h4>

          <p>
            As the CPU executes stuff, it needs to access and interact with
            memory, that’s when the role of the <em>stack</em> and the
            <em>heap</em> comes.
          </p>

          <p>
            These are (without getting into too much detail) the 2 main ways of
            ‘keeping track of variable data’ during the execution of a program:
          </p>

          <h5 id="-stack">🥞 <em>Stack</em></h5>
          <p>
            The simpler and faster of the two - it’s a linear contiguous LIFO
            (last in = first out) data structure with a push/pop mechanism, it
            serves to remember function-scoped variables, arguments, and keeps
            track of calls (ever heard of a
            <a href="https://en.wikipedia.org/wiki/Stack_trace" rel="noopener noreferrer" target="_blank">stack trace</a>?)
          </p>

          <h5 id="-heap">⛰ <em>Heap</em></h5>
          <p>
            The heap, however, is pretty unordered, and is for more complicated
            data structures, it’s typically used for dynamic allocations, where
            the size of the buffer isn’t initially known, and/or if it’s too
            big, and/or needs to be modified later.
          </p>

          <h3 id="assembly-instructions">Assembly instructions</h3>

          <p>
            As I’ve mentioned earlier, assembly instructions have a varying
            ‘byte-size’, and a varying number of arguments.
          </p>

          <p>
            Arguments can also be either immediate (‘hardcoded’), or they can be
            registers, depending on the instruction:
          </p>

          <div>
              <pre><code>55         push    ebp     ; size: 1 byte,  argument: register
6A 01      push    1       ; size: 2 bytes, argument: immediate
</code></pre>
            </div>

          <p>
            Let’s quickly run through a very small set of some of the common
            ones we’ll get to see - feel free to do your own research for more
            detail:
          </p>

          <h4 id="stack-operations">Stack operations</h4>
          <ul>
            <li>
              <strong>push
                <code>value</code></strong>
              <em>; pushes a value into the stack (decrements
                <code>ESP</code> by
                4, the size of one stack ‘unit’).</em>
            </li>
            <li>
              <strong>pop
                <code>register</code></strong>
              <em>; pops a value to a register (increments
                <code>ESP</code> by
                4).</em>
            </li>
          </ul>

          <h4 id="data-transfer">Data transfer</h4>
          <ul>
            <li>
              <strong>mov
                <code>destination</code>,
                <code>source</code></strong>
              ; <em><del>moves</del> copies a value from/to a register.</em>
            </li>
            <li>
              <strong>mov
                <code>destination</code>, [<code>expression</code>]</strong>
              ;
              <em>copies a value from a memory address resolved from a ‘register
                expression’ (single register or arithmetic expression involving
                one or more registers) into a register.</em>
            </li>
          </ul>

          <h4 id="flow-control">Flow control</h4>
          <ul>
            <li>
              <strong>jmp
                <code>destination</code></strong>
              ;
              <em>jumps into a code location (sets
                <code>EIP</code>
                (instruction pointer)).</em>
            </li>
            <li>
              <strong>jz/je
                <code>destination</code></strong>
              ;
              <em>jumps into a code location if
                <code>ZF</code>
                (the zero flag) is set.</em>
            </li>
            <li>
              <strong>jnz/jne
                <code>destination</code></strong>
              ;
              <em>jumps into a code location if
                <code>ZF</code> is
                not set.</em>
            </li>
          </ul>

          <h4 id="operations">Operations</h4>
          <ul>
            <li>
              <strong>cmp
                <code>operand1</code>,
                <code>operand2</code></strong>
              ;
              <em>compares the 2 operands and sets
                <code>ZF</code> if
                they’re equal.</em>
            </li>
            <li>
              <strong>add
                <code>operand1</code>,
                <code>operand2</code></strong>
              ; <em>operand1 += operand2;</em>
            </li>
            <li>
              <strong>sub
                <code>operand1</code>,
                <code>operand2</code></strong>
              ; <em>operand1 -= operand2;</em>
            </li>
          </ul>

          <h4 id="function-transitions">Function transitions</h4>
          <ul>
            <li>
              <strong>call
                <code>function</code></strong>
              ;
              <em>calls a function (pushes current
                <code>EIP</code>,
                then jumps to the function).</em>
            </li>
            <li>
              <strong>retn</strong> ;
              <em>returns to caller function (pops back the previous
                <code>EIP</code>).</em>
            </li>
          </ul>

          <p>
            <strong><em>Note</em></strong>: You might notice the words ‘equal’ and ‘zero’ being used
            interchangeably in x86 terminology - that’s because comparison
            instructions internally perform a subtraction, which means if the 2
            operands are equal,
            <code>ZF</code> is set.
          </p>

          <h3 id="assembly-patterns">Assembly patterns</h3>

          <p>
            Now that we have a rough idea of the main elements used during the
            execution of a program, let’s get familiarized with the patterns of
            instructions that you can encounter reverse engineering your average
            everyday 32-bit
            <a href="https://en.wikipedia.org/wiki/Portable_Executable" rel="noopener noreferrer" target="_blank">PE</a>
            binary.
          </p>

          <h4 id="function-prologue">Function prologue</h4>

          <p>
            A
            <a href="https://en.wikipedia.org/wiki/Function_prologue" rel="noopener noreferrer" target="_blank">function prologue</a>
            is some initial code embedded in the beginning of most functions, it
            serves to set up a new stack frame for said function.
          </p>

          <p>It typically looks like this (X being a number):</p>

          <div>
              <pre><code>55          push    ebp        ; preserve caller function's base pointer in stack
8B EC       mov     ebp, esp   ; caller function's stack pointer becomes base pointer (new stack frame)
83 EC XX    sub     esp, X     ; adjust the stack pointer by X bytes to reserve space for local variables
</code></pre>
            </div>

          <h4 id="function-epilogue">Function epilogue</h4>

          <p>
            The
            <a href="https://en.wikipedia.org/wiki/Function_epilogue" rel="noopener noreferrer" target="_blank">epilogue</a>
            is simply the opposite of the prologue - it undoes its steps to
            restore the stack frame of the caller function, before it returns to
            it:
          </p>

          <div>
              <pre><code>8B E5    mov    esp, ebp    ; restore caller function's stack pointer (current base pointer) 
5D       pop    ebp         ; restore base pointer from the stack
C3       retn               ; return to caller function
</code></pre>
            </div>

          <p>
            Now at this point, you might be wondering - how do functions talk to
            each other? How exactly do you send/access arguments when calling a
            function, and how do you receive the return value? That’s precisely
            why we have calling conventions.
          </p>

          <h4 id="calling-conventions-__cdecl">Calling conventions: __cdecl</h4>

          <p>
            A
            <a href="https://en.wikipedia.org/wiki/Calling_convention" rel="noopener noreferrer" target="_blank">calling convention</a>
            is basically a protocol used to communicate with functions, there’s
            a few variations of them, but they share the same principle.
          </p>

          <p>
            We will be looking at the
            <a href="https://en.wikipedia.org/wiki/X86_calling_conventions#cdecl" rel="noopener noreferrer" target="_blank">__cdecl (C declaration) convention</a>, which is the standard one when compiling C code.
          </p>

          <p>
            In __cdecl (32-bit), function arguments are passed on the stack
            (pushed in reverse order), while the return value is returned in the
            <code>EAX</code>
            register (assuming it’s not a float).
          </p>

          <p>
            This means that a
            <code>func(1, 2, 3);</code>
            call will generate the following:
          </p>

          <div>
              <pre><code>6A 03             push    3
6A 02             push    2
6A 01             push    1
E8 XX XX XX XX    call    func
</code></pre>
            </div>

          <h4 id="putting-everything-together">Putting everything together</h4>

          <p>
            Assuming
            <code>func()</code>
            simply does an addition on the arguments and returns the result, it
            would probably look like this:
          </p>

          <div>
              <pre><code>int __cdecl func(int, int, int):

           prologue:
55           push    ebp               ; save base pointer
8B EC        mov     ebp, esp          ; new stack frame

           body:
8B 45 08     mov     eax, [ebp+8]      ; load first argument to EAX (return value)
03 45 0C     add     eax, [ebp+0Ch]    ; add 2nd argument
03 45 10     add     eax, [ebp+10h]    ; add 3rd argument

           epilogue:
5D           pop     ebp               ; restore base pointer
C3           retn                      ; return to caller
</code></pre>
            </div>

          <p>
            Now if you’ve been paying attention and you’re still confused, you
            might be asking yourself one of these 2 questions:
          </p>

          <p>
            1) Why do we have to adjust
            <code>EBP</code> by 8
            to get to the first argument?
          </p>

          <ul>
            <li>
              If you
              <a href="#assembly-instructions">check the definition</a> of the
              <code>call</code>
              instruction we mentioned earlier, you’ll realize that, internally,
              it actually pushes
              <code>EIP</code> to
              the stack. And if you also check the definition for
              <code>push</code>,
              you’ll realize that it decrements
              <code>ESP</code>
              (which is copied to
              <code>EBP</code>
              after the prologue) by 4 bytes. In addition, the prologue’s first
              instruction is also a
              <code>push</code>, so
              we end up with 2 decrements of 4, hence the need to add 8.
            </li>
          </ul>

          <p>
            2) What happened to the prologue and epilogue, why are they
            seemingly ‘truncated’?
          </p>

          <ul>
            <li>
              It’s simply because we haven’t had a use for the stack during the
              execution of our function - if you’ve noticed, we haven’t modified
              <code>ESP</code> at
              all, which means we also don’t need to restore it.
            </li>
          </ul>

          <h4 id="if-conditions">If conditions</h4>

          <p>
            To demo the flow control assembly instructions, I’d like to add one
            more example to show how an if condition was compiled to assembly.
          </p>

          <p>Assume we have the following function:</p>

          <div>
              <pre><code><span>void</span> <span>print_equal</span><span>(</span><span>int</span> <span>a</span><span>,</span> <span>int</span> <span>b</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>a</span> <span>==</span> <span>b</span><span>)</span> <span>{</span>
        <span>printf</span><span>(</span><span>"equal"</span><span>);</span>
    <span>}</span>
    <span>else</span> <span>{</span>
        <span>printf</span><span>(</span><span>"nah"</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre>
            </div>

          <p>
            After compiling it, here’s the disassembly that I got with the help
            of
            <a href="https://hex-rays.com/ida-pro/" rel="noopener noreferrer" target="_blank">IDA</a>:
          </p>

          <div>
              <pre><code>void __cdecl print_equal(int, int):

     10000000   55                push   ebp
     10000001   8B EC             mov    ebp, esp
     10000003   8B 45 08          mov    eax, [ebp+8]       ; load 1st argument
     10000006   3B 45 0C          cmp    eax, [ebp+0Ch]     ; compare it with 2nd
  ┌┅ 10000009   75 0F             jnz    short loc_1000001A ; jump if not equal
  ┊  1000000B   68 94 67 00 10    push   offset aEqual  ; "equal"
  ┊  10000010   E8 DB F8 FF FF    call   _printf
  ┊  10000015   83 C4 04          add    esp, 4
┌─┊─ 10000018   EB 0D             jmp    short loc_10000027
│ ┊
│ └ loc_1000001A:
│    1000001A   68 9C 67 00 10    push   offset aNah    ; "nah"
│    1000001F   E8 CC F8 FF FF    call   _printf
│    10000024   83 C4 04          add    esp, 4
│
└── loc_10000027:
     10000027   5D                pop    ebp
     10000028   C3                retn
</code></pre>
            </div>

          <p>
            Give yourself a minute and try to make sense of this disassembly
            output (for simplicity’s sake, I’ve changed the real addresses and
            made the function start from
            <code>10000000</code>
            instead).
          </p>

          <p>
            In case you’re wondering about the
            <code>add esp, 4</code>
            part, it’s simply there to adjust
            <code>ESP</code> back
            to its initial value (same effect as a
            <code>pop</code>,
            except without modifying any register), since we had to
            <code>push</code> the
            printf string argument.
          </p>

          <h3 id="basic-data-structures">Basic data structures</h3>

          <p>
            Now let’s move on and talk about how data is stored (integers and
            strings especially).
          </p>

          <h4 id="endianness">Endianness</h4>

          <p>
            <a href="https://en.wikipedia.org/wiki/Endianness" rel="noopener noreferrer" target="_blank">Endianness</a>
            is the order of the sequence of bytes representing a value in
            computer memory.
          </p>

          <p>There’s 2 types - big-endian and little-endian:</p>

          

          <p>
            For reference, x86 family processors (the ones on pretty much any
            computer you can find) always use little-endian.
          </p>

          <p>
            To give you a live example of this concept, I’ve compiled a Visual
            Studio C++ console app, where I declared an
            <code>int</code>
            variable with the value
            <code>1337</code>
            assigned to it, then I printed the variable’s address using
            <code>printf()</code>,
            on the main function.
          </p>

          <p>
            Then I ran the program attached to the debugger in order to check
            the printed variable’s address on the memory hex view, and here’s
            the result I obtained:
          </p>

          <p>
            <img src="https://i.0x44.cc/b/vs-debug-memory-view.png" alt="">
          </p>

          <p>
            To elaborate more on this -
            <code>int</code>
            variables are 4 bytes long (32 bits) (in case you didn’t know), so
            this means that if the variable starts from the address
            <code>D2FCB8</code> it
            would end right before
            <code>D2FCBC</code>
            (+4).
          </p>

          <p>
            To go from human readable value to memory bytes, follow these steps:
          </p>

          <p>
            decimal:
            <code>1337</code> -&gt;
            hex:
            <code>539</code> -&gt;
            bytes:
            <code>00 00 05 39</code>
            -&gt; little-endian:
            <code>39 05 00 00</code>
          </p>

          <h4 id="signed-integers">Signed integers</h4>

          <p>
            This part is interesting yet relatively simple. What you should know
            here is that integer signing (positive/negative) is typically done
            on computers with the help of a concept called
            <a href="https://en.wikipedia.org/wiki/Signed_number_representations#Two's_complement" rel="noopener noreferrer" target="_blank">two’s complement</a>.
          </p>

          <p>
            The gist of it is that the lowest/first half of an integer is
            reserved for positive numbers, while the highest/last half is for
            negative numbers, here’s what this looks like in hex, for a 32-bit
            signed int (highlighted = hex, in parenthesis = decimal):
          </p>

          <p>
            Positives (1/2):
            <code>00000000</code>
            (0) -&gt;
            <code>7FFFFFFF</code>
            (2,147,483,647 or
            <code>INT_MAX</code>)
          </p>

          <p>
            Negatives (2/2):
            <code>80000000</code>
            (-2,147,483,648 or
            <code>INT_MIN</code>)
            -&gt;
            <code>FFFFFFFF</code>
            (-1)
          </p>

          <p>
            If you’ve noticed, we’re always <em>ascending</em> in value. Whether
            we go up in hex or decimal. And that’s the crucial point of this
            concept - arithmetical operation do not have to do anything special
            to handle signing, they can simply treat all values as
            unsigned/positive, and the result would still be interpreted
            correctly (as long as we don’t go beyond
            <code>INT_MAX</code> or
            <code>INT_MIN</code>),
            and that’s because integers will also <em>‘rollover’</em> on
            overflow/underflow by design, kinda like an analog odometer.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/odometer-rollover.jpg">
          </p>

          <p>
            <strong><em>Protip</em></strong>: The Windows calculator is a very helpful tool - you can set it to
            programmer mode and set the size to DWORD (4 bytes), then enter
            negative decimal values and visualize them in hex and binary, and
            have fun performing operations on them.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/calcexe-int-signing.png" alt="">
          </p>

          <h4 id="strings">Strings</h4>

          <p>
            In C, strings are stored as
            <code>char</code>
            arrays, therefore, there’s nothing special to note here, except for
            something called null termination.
          </p>

          <p>
            If you ever wondered how
            <code>strlen()</code>
            is able to know the size of a string, it’s very simple - strings
            have a character that indicates their end, and that’s the null
            byte/character -
            <code>00</code> or
            <code>'\0'</code>.
          </p>

          <p>
            If you declare a string constant in C code, and hover over it in
            Visual Studio, for instance, it will tell you the size of the
            generated array, and as you can see, for this reason, it’s one
            element more than the ‘visible’ string size.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/vs-null-termination.png" alt="">
          </p>

          <p>
            <strong><em>Note</em></strong>: The endianness concept is not applicable on arrays, only on
            single variables. Therefore, the order of characters in memory would
            be normal here - low to high.
          </p>

          <h3 id="making-sense-of-call-and-jmp-instructions">
            Making sense of
            <code>call</code> and
            <code>jmp</code>
            instructions
          </h3>

          <p>
            Now that you know all of this, you’re likely able to start making
            sense of some machine code, and emulate a CPU with your brain, to
            some extent, so to speak.
          </p>

          <p>
            Let’s take the
            <a href="#if-conditions"><code>print_equal()</code>
              example</a>, but let’s only focus on the
            <code>printf()</code>
            <code>call</code>
            instructions this time.
          </p>

          <div>
              <pre><code>void print_equal(int, int):
...
     10000010   E8 DB F8 FF FF    call   _printf
...
     1000001F   E8 CC F8 FF FF    call   _printf
</code></pre>
            </div>

          <p>
            You might be wondering to yourself - wait a second, if these are the
            same instructions, then why are their bytes different?
          </p>

          <p>
            That’s because,
            <code>call</code> (and
            <code>jmp</code>)
            instructions (usually) take an <em>offset</em> (relative address) as
            an argument, not an absolute address.
          </p>

          <p>
            An offset is basically the difference between the current location,
            and the destination, which also means that it can be either negative
            or positive.
          </p>

          <p>
            As you can see, the
            <a href="https://en.wikipedia.org/wiki/Opcode" rel="noopener noreferrer" target="_blank">opcode</a>
            of a
            <code>call</code>
            instruction that takes a 32-bit offset, is
            <code>E8</code>, and is
            followed by said offset - which makes the full instruction:
            <code>E8 XX XX XX XX</code>.
          </p>

          <p>
            Pull out your calculator,
            <del>why’d you close it so early?!</del> and calculate the
            difference between the offset of both instructions (don’t forget the
            endianness).
          </p>

          <p>
            You’ll notice that (the absolute value of) this difference is the
            same as the one between the instruction addresses (<code>1000001F</code>
            -
            <code>10000010</code> =
            <code>F</code>):
          </p>

          <p>
            <img src="https://i.0x44.cc/b/calcexe-call-inst-diff.png" alt="">
          </p>

          <p>
            Another small detail that we should add, is the fact that the CPU
            only executes an instruction after fully ‘reading’ it, which means
            that by the time the CPU starts ‘executing’,
            <code>EIP</code> (the
            instruction pointer) is already pointing at the
            <em>next</em> instruction to be executed.
          </p>

          <p>
            That’s why these offsets are actually accounting for this behaviour,
            which means that in order to get the <em>real</em> address of the
            target function, we have to also <em>add</em> the size of the
            <code>call</code>
            instruction: 5.
          </p>

          <p>
            Now let’s apply all these steps in order to resolve
            <code>printf()</code>’s
            address from the first instruction on the example:
          </p>

          <div>
              <pre><code>10000010   E8 DB F8 FF FF    call   _printf
</code></pre>
            </div>

          <p>
            1) Extract the offset from the instruction:
            <code>E8 (DB F8 FF FF)</code>
            -&gt;
            <code>FFFFF8DB</code>
            (-1829)
          </p>

          <p>
            2) Add it to the instruction address:
            <code>10000010</code> +
            <code>FFFFF8DB</code> =
            <code>0FFFF8EB</code>
          </p>

          <p>
            3) And finally, add the instruction size:
            <code>0FFFF8EB</code> +
            5 =
            <code>0FFFF8F0</code>
            (<code>&amp;printf</code>)
          </p>

          <p>
            The exact same principle applies to the
            <code>jmp</code>
            instruction:
          </p>

          <div>
              <pre><code>...
┌─── 10000018   EB 0D             jmp    short loc_10000027
...
└── loc_10000027:
     10000027   5D                pop    ebp
...
</code></pre>
            </div>
          <p>
            The only difference in this example is that
            <code>EB XX</code> is a
            short version
            <code>jmp</code>
            instruction - which means it only takes an 8-bit (1 byte) offset.
          </p>

          <p>
            Therefore:
            <code>10000018</code> +
            <code>0D</code> + 2 =
            <code>10000027</code>
          </p>

          <h3 id="conclusion">Conclusion</h3>

          <p>
            That’s it! You should now have enough information (and hopefully,
            motivation) to start your journey reverse engineering executables.
          </p>

          <p>
            Start by writing dummy C code, compiling it, and debugging it while
            single-stepping through the disassembly instructions (Visual Studio
            allows you to do this, by the way).
          </p>

          <p>
            <a href="https://godbolt.org/" rel="noopener noreferrer" target="_blank">Compiler Explorer</a>
            is also an extremely helpful website which compiles C code to
            assembly for you in real time using multiple compilers (select the
            <code>x86 msvc</code>
            compiler for Windows 32-bit).
          </p>

          <p>
            After that, you can try your luck with closed-source native
            binaries, by the help of disassemblers such as
            <a href="https://ghidra-sre.org/" rel="noopener noreferrer" target="_blank">Ghidra</a>
            and
            <a href="https://hex-rays.com/ida-free" rel="noopener noreferrer" target="_blank">IDA</a>, and debuggers such as
            <a href="https://x64dbg.com/" rel="noopener noreferrer" target="_blank">x64dbg</a>.
          </p>

          <p>
            <strong><em>Note</em></strong>: If you’ve noticed inaccurate information, or room for improvement
            regarding this article, and would like to improve it, feel free to
            <a href="https://github.com/thedroidgeek/0x44.cc/edit/master/_posts/2021-07-21-reversing-x86-and-c-code-for-beginners.md" rel="noopener noreferrer" target="_blank">submit a pull request</a>
            on GitHub.
          </p>

          <p>Thanks for reading!</p>

          <p>
            <a href="https://github.com/thedroidgeek/0x44.cc/commits/master/_posts/2021-07-21-reversing-x86-and-c-code-for-beginners.md" rel="noopener noreferrer" target="_blank">(edited)</a>
          </p>

          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing Whistleblower: "If Anything Happens to Me, It's Not Suicide" (134 pts)]]></title>
            <link>https://twitter.com/WallStreetSilv/status/1768517997285482626</link>
            <guid>39715161</guid>
            <pubDate>Fri, 15 Mar 2024 13:20:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/WallStreetSilv/status/1768517997285482626">https://twitter.com/WallStreetSilv/status/1768517997285482626</a>, See on <a href="https://news.ycombinator.com/item?id=39715161">Hacker News</a></p>
Couldn't get https://twitter.com/WallStreetSilv/status/1768517997285482626: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[IAM Is the Worst (214 pts)]]></title>
            <link>https://matduggan.com/iam-is-the-worst/</link>
            <guid>39714155</guid>
            <pubDate>Fri, 15 Mar 2024 10:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/iam-is-the-worst/">https://matduggan.com/iam-is-the-worst/</a>, See on <a href="https://news.ycombinator.com/item?id=39714155">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>Imagine your job was to clean a giant office building. You go from floor to floor, opening doors, collecting trash, getting a vacuum out of the cleaning closet and putting it back. It's a normal job and part of that job is someone gives you a key. The key opens every door everywhere. Everyone understands the key is powerful, but they also understand you need to do your job. </p><p>Then your management hears about someone stealing janitor keys. So they take away your universal key and they say "you need to tell Suzie, our security engineer, which keys you need at which time". But the keys don't just unlock one door, some unlock a lot of doors and some desk drawers, some open the vault (imagine this is the Die Hard building), some don't open any doors but instead turn on the coffee machine. Obviously the keys have titles, but the titles mean nothing. Do you need the "executive_floor/admin" key or the "executive_floor/viewer" key? </p><p>But you are a good employee and understand that security is a part of the job. So you dutifully request the keys you think you need, try to do your job, open a new ticket when the key doesn't open a door you want, try it again, it still doesn't open the door you want so then there's another key. Soon your keyring is massive, just a clanging sound as you walk down the hallway. It mostly works, but a lot of the keys open stuff you don't need, which makes you think maybe this entire thing was pointless. </p><p>The company is growing and we need new janitors, but they don't want to give all the new janitors your key ring. So they roll out a new system which says "now the keys can only open doors that we have written down that this key can open, even if it says "executive_floor/admin". The problem is people move offices all the time, so even if the list of what doors that key opened was true when it was issued, it's not true tomorrow. The Security team and HR share a list, but the list sometimes drifts or maybe someone moves offices without telling the right people. </p><p>Soon nobody is really 100% sure what you can or cannot open, including you. Sure someone can audit it and figure it out, but the risk of removing access means you cannot do your job and the office doesn't get cleaned. So practically speaking the longer someone works as a janitor the more doors they can open until eventually they have the same level of access as your original master key even if that wasn't the intent. </p><p>That's IAM (Identity and access management) in cloud providers today. </p><h3 id="stare-into-madness">Stare Into Madness</h3><figure><img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/PolicyEvaluationHorizontal111621.png" alt="" loading="lazy"><figcaption><span>AWS IAM Approval Flow</span></figcaption></figure><figure><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_copy_3.max-2000x2000.jpg" alt="" loading="lazy"><figcaption><span>GCP IAM Approval Flow</span></figcaption></figure><figure><img src="https://images.fastcompany.com/upload/Simple.jpg" alt="It's Not Natural, It's Just Simple: Food Branding Co-Opts Another Mean" loading="lazy"></figure><p>Honestly I don't even know why I'm complaining. Of course it's entirely reasonable to expect anyone working in a cloud environment to understand the dozen+ ways that they may or may not have access to a particular resource. Maybe they have permissions at a folder level, or an org level, but that permission is gated by specific resources. </p><p>Maybe they don't even have access but the tool they're interacting with the resource with has permission to do it, so they can do it but only as long as they are SSH'd into host01, not if they try to do it through some cloud shell. Possibly they had access to it before, but now they don't since they moved teams. Perhaps the members of this team were previously part of some existing group but now new employees aren't added to that group so some parts of the team can access X but others cannot. Or they actually have the correct permissions to the resource but the resource is located in another account and they don't have the right permission to traverse the networking link between the two VPCs.</p><p>Meanwhile someone is staring at these flowcharts trying to figure out what in hell is even happening here. As someone who has had to do this multiple times in my life, let me tell you the real-world workflow that ends up happening. </p><ul><li>Developer wants to launch a new service using new cloud products. They put in a ticket for me to give them access to the correct "roles" to do this. </li><li>I need to look at two elements of it, both what are the permissions the person needs in order to see if the thing is working and then the permissions the service needs in order to complete the task it is trying to complete. </li><li>So I go through my giant list of roles and try to cobble together something that I think based on the names will do what I want. Do you feel like a <code>roles/datastore.viewer</code> or more of a <code>roles/datastore.keyVisualizerViewer</code>? To run backups is <code>roles/datastore.backupsAdmin</code> sufficient or do I need to add <code>roles/datastore.backupSchedulesAdmin</code> in there as well?</li><li>They try it and it doesn't work. Reopen the ticket with "I still get authorizationerror:foo". I switch that role with a different role, try it again. Run it through the simulator, it seems to work, but they report a new different error because actually in order to use service A you need to also have a role in service B. Go into bathroom, scream into the void and return to your terminal.</li><li>We end up cobbling together a custom role that includes all the permissions that this application needs and the remaining 90% of permissions are something it will never ever use but will just sit there as a possible security hole. </li><li>Because /* permissions are the work of Satan, I need to scope it to specific instances of that resource and just hope nobody ever adds a SQS queue without....checking the permissions I guess. In theory we should catch it in the non-prod environments but there's always the chance that someone messes up something at a higher level of permissions that does something in non-prod and doesn't exist in prod so we'll just kinda cross our fingers there. </li></ul><h3 id="gcp-makes-it-worse">GCP Makes It Worse</h3><p>So that's effectively the AWS story, which is terrible but at least it's possible to cobble together something that works and you can audit. Google looked at this and said "what if we could express how much we hate Infrastructure teams as a service?" Expensive coffee robots were engaged, colorful furniture was sat on and the brightest minds of our generation came up with a system so punishing you'd think you did something to offend them personally. </p><p>Google looked at AWS and said "this is a tire fire" as corporations put non-prod and prod environments in the same accounts and then tried to divide them by conditionals. So they came up with a folder structure:</p><figure><img src="https://infosec.rodeo/assets/img/blog/gcp_resource_hierarchy.png" alt="GCP Resource Hierarchy" loading="lazy"></figure><p>The problem is that this design encourages unsafe practices by promoting "groups should be set at the folder level with one of the default basic roles". It makes sense logically at first that you are a viewer, editor or owner. But as GCP adds more services this model breaks down quickly because each one of these encompasses thousands upon thousands of permissions. So additional IAM predefined roles were layered on. </p><p>People were encouraged to move away from the basic roles and towards the predefined roles. There are ServiceAgent roles that were designated for service accounts, aka the permissions you actual application has and then everything else. Then there are 1687 other roles for you to pick from to assign to your groups of users. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-2.png" alt="" loading="lazy" width="860" height="334" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-2.png 600w, https://matduggan.com/content/images/2024/03/image-2.png 860w" sizes="(min-width: 720px) 720px"></figure><p>The problem is none of this is actually best practice. Even when assigning users "small roles", we're still not following the principal of least privilege. Also the roles don't remain static. As new services come online permissions are added to roles. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-4.png" alt="" loading="lazy" width="2000" height="1254" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-4.png 600w, https://matduggan.com/content/images/size/w1000/2024/03/image-4.png 1000w, https://matduggan.com/content/images/size/w1600/2024/03/image-4.png 1600w, https://matduggan.com/content/images/size/w2400/2024/03/image-4.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>The above is an automated process that pulls down all the roles from the gcloud CLI tool and updates them for latest. It is a constant state of flux with roles with daily changes. It gets even more complicated though. </p><p>You also need to check the launch stage of a role. </p><blockquote>Custom roles include a launch stage as part of the role's metadata. The most common launch stages for custom roles are ALPHA, BETA, and GA. These launch stages are informational; they help you keep track of whether each role is ready for widespread use. Another common launch stage is DISABLED. This launch stage lets you disable a custom role.</blockquote><blockquote>We recommend that you use launch stages to convey the following information about the role:</blockquote><blockquote>EAP or ALPHA: The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.<br>BETA: The role has been tested on a limited basis, or it includes permissions for Google Cloud services or features that are not generally available.<br>GA: The role has been widely tested, and all of its permissions are for Google Cloud services or features that are generally available.<br>DEPRECATED: The role is no longer in use.</blockquote><h3 id="who-cares">Who Cares?</h3><p>Why would anyone care if Google is constantly changing roles? Well it matters because with GCP to make a custom role, you cannot combine predefined roles. Instead you need to go down to the permission level to list out all of the things those roles can do, then feed that list of permissions into the definition of your custom role and push that up to GCP. </p><p>In order to follow best practices this is what you have to do. Otherwise you will always be left with users that have a ton of unused permissions along with the fear of a security breach allowing someone to execute commands in your GCP account through an applications service account that cause way more damage than the actual application justifies. </p><p>So you get to build automated tooling which either queries the predefined roles for change over time and roll those into your custom roles so that you can assign a user or group one specific role that lets them do everything they need. Or you can assign these same folks multiple of the 1600+ predefined roles, accept that they have permissions they don't need and also just internalize that day to day you don't know how much the scope of those permissions have changed. </p><h3 id="the-obvious-solution">The Obvious Solution</h3><p>Why am I ranting about this? Because the solution is so blindly obvious I don't understand why we're not already doing it. It's a solution I've had to build, myself, multiple times and at this point am furious that this keeps being my responsibility as I funnel hundreds of thousands of dollars to cloud providers. </p><p>What is this obvious solution? You, an application developer, need to launch a new service. I give you a service account that lets you do almost everything inside of that account along with a viewer account for your user that lets you go into the web console and see everything. You churn away happily, writing code that uses all those new great services. Meanwhile, we're tracking all the permissions your application and you are using. </p><p>At some time interval, 30 or 90 or whatever days, my tool looks at the permissions your application has used over the last 90 days and says "remove the global permissions and scope it to these". I don't need to ask you what you need, because I can see it. In the same vein I do the same thing with your user or group permissions. You don't need viewer everywhere because I can see what you've looked at. </p><p>Both GCP and AWS support this and have all this functionality baked in. GCP has the <a href="https://cloud.google.com/policy-intelligence/docs/role-recommendations-overview" rel="noreferrer">role recommendations</a> which tracks exactly what I'm talking about and recommends lowering the role. <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html" rel="noreferrer">AWS tracks the exact same information</a> and can be used to do the exact same thing. </p><p><strong>What if the user needs different permissions in a hurry?</strong></p><p>This is not actually that hard to account for and <em>again</em> is something I and countless others have been forced to make over and over. You can issue expiring permissions in both situations where a user can request a role be temporarily granted to them and then it disappears in 4 hours. I've seen every version of these, from Slack bots to websites, but they're all the same thing. If user is in X group they're allowed to request Y temporary permissions. OR if the user is on-call as determined with an API call to the on-call provider they get more powers. Either design works fine. </p><p><strong>That seems like a giant security hole</strong></p><p>Compared to what? Team A guessing what Team B needs even though they don't ever do the work that Team B does? Some security team receiving a request for permissions and trying to figure out if the request "makes sense" or not? At least this approach is based on actual data and not throwing darts at a list of IAM roles and seeing what "feels right". </p><h3 id="conclusion">Conclusion</h3><p>IAM started out as an easy idea that as more and more services were launched, started to become nightmarish to organize. It's too hard to do the right thing now and it's even harder to do the right thing in GCP compared to AWS. The solution is not complicated. We have all the tools, all the data, we understand how they fit together. We just need one of the providers to be brave enough to say "obviously we messed up and this legacy system you all built your access control on is bad and broken". It'll be horrible, we'll all grumble and moan but in the end it'll be a better world for us all. </p><p>Feedback: <a href="https://c.im/@matdevdug">https://c.im/@matdevdug</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Matrix Multiplication with Half the Multiplications (251 pts)]]></title>
            <link>https://github.com/trevorpogue/algebraic-nnhw</link>
            <guid>39714053</guid>
            <pubDate>Fri, 15 Mar 2024 10:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trevorpogue/algebraic-nnhw">https://github.com/trevorpogue/algebraic-nnhw</a>, See on <a href="https://news.ycombinator.com/item?id=39714053">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">This repository contains the source code for ML hardware architectures that require nearly half the number of multiplier units to achieve the same performance, by executing alternative inner-product algorithms that trade nearly half the multiplications for cheap low-bitwidth additions, while still producing identical output as the conventional inner product. This increases the theoretical throughput and compute efficiency limits of ML accelerators. See the following journal publication for the full details:</p>
<p dir="auto">T. E. Pogue and N. Nicolici, "Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators," in IEEE Transactions on Computers, vol. 73, no. 2, pp. 495-509, Feb. 2024, doi: 10.1109/TC.2023.3334140.</p>

<p dir="auto">Article URL: <a href="https://ieeexplore.ieee.org/document/10323219" rel="nofollow">https://ieeexplore.ieee.org/document/10323219</a></p>
<p dir="auto">Open-access version: <a href="https://arxiv.org/abs/2311.12224" rel="nofollow">https://arxiv.org/abs/2311.12224</a></p>
<p dir="auto">Abstract: We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.</p>
<p dir="auto">The following diagram shows an overview of the ML accelerator system implemented in this source code:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE"><img src="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE" width="450"></a></p>
<p dir="auto">The FIP and FFIP systolic array/MXU processing elements (PE)s shown below in (b) and (c) implement the FIP and FFIP inner-product algorithms and each individually provide the same effective computational power as the two baseline PEs shown in (a) combined which implement the baseline inner product as in previous systolic-array ML accelerators:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU"><img src="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU" width="450"></a></p>
<p dir="auto">The following is a diagram of the MXU/systolic array and shows how the PEs are connected:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0"><img src="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0" width="450"></a></p>
<p dir="auto">The source code organization is as follows:</p>
<ul dir="auto">
<li>compiler
<ul dir="auto">
<li>A compiler for parsing Python model descriptions into accelerator instructions that allow it to accelerate the model. This part also includes code for interfacing with a PCIe driver for initiating model execution on the accelerator, reading back results and performance counters, and testing the correctness of the results.</li>
</ul>
</li>
<li>rtl
<ul dir="auto">
<li>Synthesizable SystemVerilog RTL.</li>
</ul>
</li>
<li>sim
<ul dir="auto">
<li>Scripts for setting up simulation environments for testing.</li>
</ul>
</li>
<li>tests
<ul dir="auto">
<li>UVM-based testbench source code for verifying the accelerator in simulation using Cocotb.</li>
</ul>
</li>
<li>utils
<ul dir="auto">
<li>Additional Python packages and scripts used in this project that the author created for general development utilities and aids.</li>
</ul>
</li>
</ul>
<p dir="auto">The files rtl/top/define.svh and rtl/top/pkg.sv contain a number of configurable parameters such as FIP_METHOD in define.svh which defines the systolic array type (baseline, FIP, or FFIP), SZI and SZJ which define the systolic array height/width, and LAYERIO_WIDTH/WEIGHT_WIDTH which define the input bitwidths.</p>
<p dir="auto">The directory rtl/arith includes mxu.sv and mac_array.sv which contain the RTL for the baseline, FIP, and, FFIP systolic array architectures (depending on the value of the parameter FIP_METHOD).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (253 pts)]]></title>
            <link>https://arxiv.org/abs/2403.09629</link>
            <guid>39713634</guid>
            <pubDate>Fri, 15 Mar 2024 09:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.09629">https://arxiv.org/abs/2403.09629</a>, See on <a href="https://news.ycombinator.com/item?id=39713634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.09629">Download PDF</a>
    <a href="https://arxiv.org/html/2403.09629v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Eric Zelikman [<a href="https://arxiv.org/show-email/094380da/2403.09629">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 14 Mar 2024 17:58:16 UTC (510 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>