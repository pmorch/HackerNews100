<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 28 Nov 2024 12:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[picoCAD – a tiny modeler for tiny models (105 pts)]]></title>
            <link>https://store.steampowered.com/app/2800590/picoCAD/</link>
            <guid>42262734</guid>
            <pubDate>Thu, 28 Nov 2024 05:35:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://store.steampowered.com/app/2800590/picoCAD/">https://store.steampowered.com/app/2800590/picoCAD/</a>, See on <a href="https://news.ycombinator.com/item?id=42262734">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/Product" id="tabletGrid">
		
		<meta itemprop="image" content="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/capsule_231x87.jpg?t=1732787286">
					
		
		<div data-gpnav="columns">
		                      
         		<div><p><img src="https://cdn.fastly.steamstatic.com/steamcommunity/public/images/apps/2800590/da9137ca4adb071cfe79c3ae0c1b16c9df2c632b.jpg"></p></div>
		<p>picoCAD</p>
		

	</div>
		


		

		

				

		<div data-panel="[]">

					
					
					
					
					
					<div id="game_area_purchase">
		
		
		<h2>Buy picoCAD</h2>
				
		
		<p>INTRODUCTORY OFFER! Offer ends 4 December</p>								
		
	</div>
					<!-- game_area_purchase -->

					
											
					
					

					
					
																								
					

					
				
								
				
				


									
									<div id="aboutThisGame" data-panel="{&quot;type&quot;:&quot;PanelGroup&quot;}">
							<h2>About This Software</h2>
							<p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/extras/logo.png?t=1732787286"></p><p>Want to make 3D models but don't know where to start? picoCAD is here to help! Focused on the bare essentials of 3D modelling, picoCAD blends simplicity with creativity, letting you make low-poly 3D models with just a few clicks. Whether you are a total beginner, or an expert, picoCAD is a unique and approachable tool to express your art in.</p><p>✨ Wishlist now to stay updated on the release!&nbsp;✨</p><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/extras/steam-wishlist.gif?t=1732787286"></p><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/extras/spinning_lorry.gif?t=1732787286"><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/extras/spinning_sub.gif?t=1732787286"></p><h2>KEY FEATURES</h2><ul><li><p><strong>Focused toolset:</strong> Create retro-style models without any prior experience</p></li><li><p><strong>Low-poly aesthetics:</strong> Capture the charm of early 3D games with pixelated models</p></li><li><p><strong>Texture mapping:</strong> Adding your own textures allows for endless experimentation and expression</p></li><li><p><strong>Intentionally limited</strong>: A tight feature set and limited space focuses creativity where it matters most</p></li><li><p><strong>Ready for games &amp; art:</strong> Export models to use in projects or showcase them in your portfolio</p></li><li><p><strong>Friendly community:</strong> Join our every growing discord community of artists and designers</p></li></ul><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/extras/vehicles.gif?t=1732787286"></p><h2>GOOD TO KNOW</h2><ul><li><p>picoCAD is built on the PICO-8 fantasy console and comes rich with constraints. Experiment to find your own workflow and anything is possible!</p></li><li><p>checkout  for tutorials and examples!</p></li><li><p>picoCAD is a standalone application and you don't need pico8 to run it.</p></li><li><p>Make sure to read the manual and check out the tutorials</p></li></ul><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/2800590/extras/toaster_0.gif?t=1732787286"></p>						</div>
				
				
				
					<div>
		<h2>System Requirements</h2>
					
				<div>
							<div data-os="win">
							<ul>
								<strong>Minimum:</strong><br><ul><li><strong>OS:</strong> 10<br></li><li><strong>Processor:</strong> 700 MHz<br></li><li><strong>Memory:</strong> 1 GB RAM<br></li><li><strong>Graphics:</strong> any card<br></li><li><strong>Storage:</strong> 50 MB available space<br></li><li><strong>Sound Card:</strong> any card</li></ul>							</ul>
						</div>
							<div data-os="mac">
							<ul>
								<strong>Minimum:</strong><br><ul><li><strong>OS:</strong> 10<br></li><li><strong>Processor:</strong> 700 MHz, Apple M1 or Intel Core M<br></li><li><strong>Memory:</strong> 1 GB RAM<br></li><li><strong>Graphics:</strong> any card<br></li><li><strong>Storage:</strong> 50 MB available space<br></li><li><strong>Sound Card:</strong> any card</li></ul>							</ul>
						</div>
					</div>
	</div>
	

				
								
				</div>

		


		<div id="app_reviews_hash">
							<h2>Customer reviews for picoCAD</h2>

		
				<div id="review_histograms_container">
			<canvas id="review_graph_canvas"></canvas>
			<div id="review_histogram_rollup_section">
						<p>Overall Reviews:</p>
						<p><span data-tooltip-html="Need more user reviews to generate a score">3 user reviews</span>
													<span>(3 reviews)</span>
												<a data-tooltip-text="This summary uses only reviews written by customers that purchased the game directly from Steam."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark.png"></a>
					</p></div><!--
			-->
		</div>

		<div id="reviews_filter_options">
			<div>
				<p>Review Type</p>
				<div>
					<p>
						<label for="review_type_all">All&nbsp;<span>(4)</span></label><br>
						
						<label for="review_type_positive">Positive&nbsp;<span>(4)</span></label><br>
						
						<label for="review_type_negative">Negative&nbsp;<span>(0)</span></label>
					</p>
				</div>
			</div>
			<div>
				<p>Purchase Type</p>
				<div>
					<p>
						<label for="purchase_type_all">All&nbsp;<span>(4)</span></label><br>
						
						<label for="purchase_type_steam">Steam Purchasers&nbsp;<span>(3)</span> <a data-tooltip-text="These are reviews written by customers that purchased the game directly from Steam."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label><br>
						
						<label for="purchase_type_non_steam">Other&nbsp;<span>(1)</span> <a data-tooltip-text="These are reviews written by customers that did not purchase the game on Steam. (This may include legitimate sources such as other digital stores, retail stores, testing purposes, or press review purposes. Or, from inappropriate sources such as copies given in exchange for reviews.)"><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label>
					</p>
				</div>
			</div>
						<div>
				<p>Language</p>
				<div>
						<p>
						<label for="review_language_all">All Languages&nbsp;<span>(4)</span></label><br>
						
						<label for="review_language_mine">Your Languages&nbsp;<span>(3)</span> <a data-tooltip-html="Your preferences are currently set to show content authored in these languages: English.<br><br> Click 'customize' below to modify your preferences."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label></p>
					</div>
			</div>
									<div id="reviews_date_range_menu">
				<p>Date Range</p>
				<div>
						<div><p>
							To view reviews within a date range, please click and drag a selection on a graph above or click on a specific bar.							</p><p>
							<span onclick="SetReviewsGraphVisibility( true ); "><span>Show graph</span></span></p></div>
						<p>
						<label for="review_date_range_all">Lifetime</label><br>
						
						<label for="review_date_range_histogram">Only Specific Range (Select on graph above)&nbsp;</label><br>
						
						<label for="review_date_range_exclude_histogram">Exclude Specific Range (Select on graph above)&nbsp;</label><br>
					</p></div>
			</div>
						<div>
				<p>Playtime</p>
				<div>
						<p>
							Filter reviews by the user's playtime when the review was written:						</p>

													<p>
							<label for="review_playtime_preset_0">No Minimum</label><br>
														
							<label for="review_playtime_preset_1">Over 1 hour</label></p><p><span id="app_reviews_playtime_range_text_min">No minimum</span> to <span id="app_reviews_playtime_range_text_max">No maximum</span>
						</p>
						

						<p>
							<label for="review_playtime_type_all">Played across all devices</label><br>
							
							<label for="review_playtime_type_deck">Played mostly on Steam Deck</label>
						</p>

					</div>
			</div>
						<div>
				<p>Display</p>
				<div>
						<p>
							Show reviews in selected display order						</p>

						<p>
						<label for="review_context_summary">Summary</label><br>
						
						<label for="review_context_most_helpful">Most Helpful</label><br>
						
						<label for="review_context_recent">Recent</label><br>
						
						<label for="review_context_funny">Funny</label></p><p><label>&nbsp;Use new helpfulness system. Only applies to Summary and Most Helpful views.</label><br>
							<a href="https://store.steampowered.com/news/app/593110/view/4326355263805583415?snr=1_5_9_">Learn More</a>						</p>
											</div>
			</div>

			

			<p><span id="review_show_graph_button" onclick="SetReviewsGraphVisibility( true ); "><span>Show graph</span> </span>
				<span id="review_hide_graph_button" onclick="SetReviewsGraphVisibility( false ); "><span>Hide graph</span> </span>
			</p>

			
		</div>

		<div id="reviews_active_filters" data-panel="{&quot;focusable&quot;:true,&quot;clickOnActivate&quot;:true}" onclick="ShowReviewSettingsModal();">
				<p>Filters</p>

								
								
				
				
								
								
				<p>Excluding Off-topic Review Activity</p>
				<p>Playtime: <span id="review_playtime_preset_text"></span></p>
				<p>Played Mostly on Steam Deck</p>
			</div>

		

		
		
		
		
		


		
		
		
		
		
		<div id="Reviews_summary">
						<p>There are no more reviews that match the filters set above</p>
						<p>Adjust the filters above to see other reviews</p>
						
					</div>
		
		

						</div>

					
		

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Voice-Pro – AI Voice Cloning Magic: Transform Any Voice in 15 Seconds (125 pts)]]></title>
            <link>https://github.com/abus-aikorea/voice-pro</link>
            <guid>42261909</guid>
            <pubDate>Thu, 28 Nov 2024 02:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/abus-aikorea/voice-pro">https://github.com/abus-aikorea/voice-pro</a>, See on <a href="https://news.ycombinator.com/item?id=42261909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Voice-Pro: The best gradio web-ui for transcription, translation and text-to-speech 🔊</h2><a id="user-content-voice-pro-the-best-gradio-web-ui-for-transcription-translation-and-text-to-speech-" aria-label="Permalink: Voice-Pro: The best gradio web-ui for transcription, translation and text-to-speech 🔊" href="#voice-pro-the-best-gradio-web-ui-for-transcription-translation-and-text-to-speech-"></a></p>
<p dir="auto">🌍 <a href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/README.kor.md">한국어</a> ∙ <a href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/README.eng.md">English</a> ∙ <a href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/README.zh.md">中文简体</a> ∙ <a href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/README.tw.md">中文繁體</a> ∙ <a href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/README.jpn.md">日本語</a></p>
<p dir="auto"><a href="https://github.com/abus-aikorea/voice-pro/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/4ebb83d58fb9e566b39ce3ec8ff73b0b88f6f92b7d65ef26848d24844febde78/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f616275732d61696b6f7265612f766f6963652d70726f" alt="GitHub License" data-canonical-src="https://img.shields.io/github/license/abus-aikorea/voice-pro"></a>
<a href="https://github.com/abus-aikorea/voice-pro/releases"><img src="https://camo.githubusercontent.com/42b60540add9bb52749ea8819006fe4a79a828986e88045e7219780b9724e8e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f616275732d61696b6f7265612f766f6963652d70726f" alt="GitHub Release" data-canonical-src="https://img.shields.io/github/v/release/abus-aikorea/voice-pro"></a></p>
<p dir="auto"><strong>Voice-Pro is the best gradio WebUI for transcription, translation and text-to-speech.</strong> It can be easily installed with one click. Create a virtual environment using Miniconda, running completely separate from the Windows system (fully portable). Supports real-time transcription and translation, as well as batch mode.</p>
<ul dir="auto">
<li><strong>YouTube Downloader</strong>: You can download YouTube videos and extract the audio (mp3, wav, flac).</li>
<li><strong>Vocal Remover</strong>: Use MDX-Net supported in UVR5 and the Demucs engine developed by Meta for voice separation.</li>
<li><strong>STT</strong>: Supports speech-to-text conversion with Whisper, Faster-Whisper, and whisper-timestamped.</li>
<li><strong>Translator</strong>: Google Translator. Short text translation, subtitle file translation.</li>
<li><strong>TTS</strong>: Text to Speech. Edge-TTS. E2 and F5-TTS that support zero-shot voice cloning.</li>
<li>We provide Celeb voices for free. Try creating your own podcast. You can check it in the F5-TTS tab.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">🚄 Run screen</h3><a id="user-content--run-screen" aria-label="Permalink: 🚄 Run screen" href="#-run-screen"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><code>TTS</code> tab : Podcast Production using F5-TTS
<gh:secured-asset-reference resource_type="UserAsset" resource_id="386451075"></gh:secured-asset-reference></p><details open="">
  <summary>
    
    <span aria-label="Video description f5-tts-demo-elon-zuckerberg-1115-3.mp4">f5-tts-demo-elon-zuckerberg-1115-3.mp4</span>
    <span></span>
  </summary>

  <video src="https://github.com/user-attachments/assets/2d4b7d84-ca19-4efd-a847-a66fa0db616e" data-canonical-src="https://github.com/user-attachments/assets/2d4b7d84-ca19-4efd-a847-a66fa0db616e" controls="controls" muted="muted">

  </video>
</details>

</li>
<li>
<p dir="auto"><code>Studio</code> tab : Transcription, Translation &amp; Text-to-Speech
<gh:secured-asset-reference resource_type="UserAsset" resource_id="389271563"></gh:secured-asset-reference></p><details open="">
  <summary>
    
    <span aria-label="Video description voice-pro-demo-v1.6.7-1080p.mp4">voice-pro-demo-v1.6.7-1080p.mp4</span>
    <span></span>
  </summary>

  <video src="https://github.com/user-attachments/assets/f18e7f54-7bc0-4c26-96f9-9f6b70c7114c" data-canonical-src="https://github.com/user-attachments/assets/f18e7f54-7bc0-4c26-96f9-9f6b70c7114c" controls="controls" muted="muted">

  </video>
</details>

</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐&nbsp;Key Features</h2><a id="user-content-key-features" aria-label="Permalink: ⭐&nbsp;Key Features" href="#key-features"></a></p>
<ul dir="auto">
<li><code>Studio</code> tab
<ul dir="auto">
<li>Provides integrated environment for YouTube downloader, noise removal, subtitles, translation, and TTS</li>
<li>All video/audio formats supported by ffmpeg can be used</li>
<li>Selectable output audio format (wav, flac, mp3)</li>
<li>Speech recognition and subtitle creation for 100 languages</li>
<li>Select subtitle creation options suitable for PC performance (Whisper Model &amp; Compute Type)</li>
<li>Translation into over 100 languages ​​and voice generation through TTS</li>
<li>The BGM and sound effects from the original video are maintained in the multilingual video.</li>
<li>Supports TTS voice speed, volume, and pitch adjustment</li>
</ul>
</li>
</ul>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/images/main_page.eng.png?raw=true"><img src="https://github.com/abus-aikorea/voice-pro/raw/main/docs/images/main_page.eng.png?raw=true" alt=""></a>
</p>  
<ul dir="auto">
<li>
<p dir="auto"><code>Whisper Caption</code> tab</p>
<ul dir="auto">
<li>A tab dedicated to creating subtitles. Supports over 90 languages</li>
<li>Display subtitles created with the video</li>
<li>World-Level Highlight function provided</li>
<li>Denoise function provided (1-Demucs, 2-MDXNet)</li>
</ul>
</li>
<li>
<p dir="auto"><code>Translate</code> tab</p>
<ul dir="auto">
<li>Dedicated tab for translation. Supports over 100 languages</li>
<li>Supports subtitle files (ass, ssa, srt, mpl2, tmp, vtt, microdvd, json)</li>
<li>Direct text input is also possible</li>
<li>Automatically detects the language of uploaded files</li>
</ul>
</li>
<li>
<p dir="auto"><code>TTS</code> tab</p>
<ul dir="auto">
<li>Edge-TTS and F5-TTS are supported.</li>
<li>Edge-TTS supports over 100 languages and more than 400 voices.</li>
<li>Pitch, Volume, and Speed can be adjusted.</li>
<li>F5-TTS supports Zero-Shot Voice Cloning.</li>
<li>You can create podcasts using Celeb Voices.</li>
</ul>
</li>
</ul>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/images/tts_f5_multi.png?raw=true"><img src="https://github.com/abus-aikorea/voice-pro/raw/main/docs/images/tts_f5_multi.png?raw=true" alt=""></a>
</p>  
<ul dir="auto">
<li>
<p dir="auto"><code>Live Translation</code> tab</p>
<ul dir="auto">
<li>Real-time voice recognition &amp; translation support</li>
<li>Select audio input source such as Mic, Speaker, etc.</li>
<li>Provides the ability to save captured audio, recognized subtitles, and translated subtitles</li>
</ul>
</li>
<li>
<p dir="auto"><code>Batch</code> tab</p>
<ul dir="auto">
<li>Batch processing for large amounts of files</li>
<li>Subtitles, translation, TTS</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Execution environment</h2><a id="user-content--execution-environment" aria-label="Permalink: 💻 Execution environment" href="#-execution-environment"></a></p>
<ul dir="auto">
<li>OS: Windows 10/11 (64bits) <strong>※ Linux and Mac OS are not supported.</strong></li>
<li>GPU: <strong>NVIDIA</strong> graphics card supporting CUDA 12.1 recommended.</li>
<li>VRAM: 4GB or more. 8GB or more recommended.</li>
<li>RAM: 4GB or more</li>
<li>HDD: At least 20GB of free space during installation</li>
<li>Internet connection required (installation and translation work)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📀 Installation</h2><a id="user-content--installation" aria-label="Permalink: 📀 Installation" href="#-installation"></a></p>
<p dir="auto">Voice-Pro can be easily installed with one click. Just run 🚀<strong>configure.bat</strong> and 🚀<strong>start.bat</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">step 1. Package preparation</h3><a id="user-content-step-1-package-preparation" aria-label="Permalink: step 1. Package preparation" href="#step-1-package-preparation"></a></p>
<ul dir="auto">
<li>Clone or download the latest release (<strong>Source code (zip)</strong>) from  <a href="https://github.com/abus-aikorea/voice-pro/"><img src="https://camo.githubusercontent.com/42b60540add9bb52749ea8819006fe4a79a828986e88045e7219780b9724e8e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f616275732d61696b6f7265612f766f6963652d70726f" alt="GitHub Release" data-canonical-src="https://img.shields.io/github/v/release/abus-aikorea/voice-pro"></a></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/abus-aikorea/voice-pro.git"><pre>git clone https://github.com/abus-aikorea/voice-pro.git</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">step 2. Install and run the program</h3><a id="user-content-step-2-install-and-run-the-program" aria-label="Permalink: step 2. Install and run the program" href="#step-2-install-and-run-the-program"></a></p>
<ol dir="auto">
<li>🚀 Run <code>configure.bat</code>
<ul dir="auto">
<li>Install git, ffmpeg and CUDA (if using NVIDIA GPU) on Windows.</li>
<li>You only need to run it the first time.</li>
<li>An internet connection is required, and it may take over an hour depending on the system.</li>
<li>Never close the Windows-Command window during installation.</li>
</ul>
</li>
<li>🚀 Run <code>start.bat</code>
<ul dir="auto">
<li>Start Voice-Pro. Web-UI will run automatically.</li>
<li>When running for the first time, Voice-Pro is installed first.</li>
<li>An internet connection is required, and it may take over an hour depending on the system.</li>
<li>Never close the Windows-Command window during installation.</li>
<li>If a problem occurs during installation, delete the <strong>installer_files</strong> folder and run start.bat again.</li>
</ul>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">step 3. Uninstall program</h3><a id="user-content-step-3-uninstall-program" aria-label="Permalink: step 3. Uninstall program" href="#step-3-uninstall-program"></a></p>
<ul dir="auto">
<li>Run <code>uninstall.bat</code>:
<ul dir="auto">
<li>Remove the <strong>installer_files</strong> folder.</li>
<li>Remove ffmepg, git and CUDA packages installed on Windows (if selected)</li>
</ul>
</li>
<li>Voice-Pro has <strong>portable</strong> installation as standard. To uninstall the program, deleting the installation folder is sufficient.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">❓Tips &amp; Tricks</h2><a id="user-content-tips--tricks" aria-label="Permalink: ❓Tips &amp; Tricks" href="#tips--tricks"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">If Browser does not run automatically</h4><a id="user-content-if-browser-does-not-run-automatically" aria-label="Permalink: If Browser does not run automatically" href="#if-browser-does-not-run-automatically"></a></p>
<ul dir="auto">
<li>Close the Windows-Commnad window and run start.bat again.</li>
<li>Run the browser directly and enter the address displayed in the Windows-Command window (e.g. <strong><a href="http://127.0.0.1:7892/" rel="nofollow">http://127.0.0.1:7892</a></strong>) in the address bar.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">If a CUDA Out-Of-Memory error occurs</h4><a id="user-content-if-a-cuda-out-of-memory-error-occurs" aria-label="Permalink: If a CUDA Out-Of-Memory error occurs" href="#if-a-cuda-out-of-memory-error-occurs"></a></p>
<ul dir="auto">
<li>Check the GPU memory status in Windows Task Manager - Performance tab.</li>
<li>Set the Denoise level to 0 or 1. Denoise level 2 requires at least 8GB of GPU memory.</li>
<li>Set Compute Type to int type. The float type has better quality, but requires more GPU memory.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">How to improve the quality of subtitles?</h4><a id="user-content-how-to-improve-the-quality-of-subtitles" aria-label="Permalink: How to improve the quality of subtitles?" href="#how-to-improve-the-quality-of-subtitles"></a></p>
<ul dir="auto">
<li>The quality of subtitles tends to improve with larger Whisper models, but this is not necessarily the case. large &gt; medium &gt; small &gt; base &gt; tiny</li>
<li>Among compute types, float type has good performance. The int type is a model that reduces GPU usage and increases speed through model quantization. On the other hand, performance decreases.</li>
<li>If you increase the denoise level, more background sounds will be removed, and only the remaining voice will be used for voice recognition. It does not always guarantee good results.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📢 caution</h2><a id="user-content--caution" aria-label="Permalink: 📢 caution" href="#-caution"></a></p>
<p dir="auto">Windows Defender may give a warning about untrusted application and disallow further execution of Voice-Pro.
If SmartScreen security level is set to "Warn", just click "More info" and then click "Run anyway".
If SmartScreen is set to level "Block" there will be no button to run the installation. In this case, open the properties of the start.bat file, and check "Unblock", apply the change and run the start.bat again.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/images/windows_smartscreen_warning.png?raw=true"><img src="https://github.com/abus-aikorea/voice-pro/raw/main/docs/images/windows_smartscreen_warning.png?raw=true" alt=""></a>
</p>  
<p dir="auto">When Windows Defender mistakenly recognizes a batch file as a Trojan, this is often called a 'False Positive'. To solve this problem, you can go through the following steps:</p>
<ol dir="auto">
<li>File exception handling: In Windows Defender, you can set certain files or processes to skip security scanning. To do this, follow the steps below:
<ul dir="auto">
<li>Click the ‘Start’ button and go to ‘Settings’.</li>
<li>Click ‘Update &amp; Security’.</li>
<li>Select ‘Windows Security’ and go to ‘Virus &amp; threat protection’.</li>
<li>Click ‘Manage Virus &amp; Threat Protection Settings’.</li>
<li>Select 'Add exception' in 'Virus &amp; threat protection settings'.</li>
<li>Select 'File or Folder', find the batch file in question and add it as an exception.</li>
</ul>
</li>
<li>Temporarily disable Windows Defender: This may be a temporary solution. However, you must be careful when using this method as it may expose your computer to other threats.</li>
<li>Report the problem to anti-virus software: If you are sure that the file is not a Trojan horse, you can report it to Microsoft as a False Positive. Microsoft will review this and take any necessary action.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">📬 Contact us</h2><a id="user-content--contact-us" aria-label="Permalink: 📬 Contact us" href="#-contact-us"></a></p>
<ul dir="auto">
<li>e-mail: <a href="mailto:abus.aikorea@gmail.com">abus.aikorea@gmail.com</a></li>
<li>homepage(Korean): <a href="https://abuskorea.imweb.me/" rel="nofollow">https://abuskorea.imweb.me</a></li>
<li>Amazon(US): <a href="https://www.amazon.com/dp/B0DBR69JPL" rel="nofollow">https://www.amazon.com/dp/B0DBR69JPL</a></li>
<li>Amazon(Japan): <a href="https://www.amazon.co.jp/dp/B0DBVRJ542" rel="nofollow">https://www.amazon.co.jp/dp/B0DBVRJ542</a></li>
<li>Amazon(Singapore): <a href="https://www.amazon.sg/dp/B0DCGKL8R4" rel="nofollow">https://www.amazon.sg/dp/B0DCGKL8R4</a></li>
<li>Amazon(UAE): <a href="https://www.amazon.ae/dp/B0DCGKM7FF" rel="nofollow">https://www.amazon.ae/dp/B0DCGKM7FF</a></li>
<li>네이버 스마트스토어 (S/W): <a href="https://smartstore.naver.com/abus/products/10385660040" rel="nofollow">https://smartstore.naver.com/abus/products/10385660040</a></li>
<li>네이버 스마트스토어 (Solution): <a href="https://smartstore.naver.com/abus/products/10298346364" rel="nofollow">https://smartstore.naver.com/abus/products/10298346364</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">👍 YouTube</h2><a id="user-content--youtube" aria-label="Permalink: 👍 YouTube" href="#-youtube"></a></p>
<ul dir="auto">
<li>Product Information: <a href="https://www.youtube.com/watch?v=z8g8LMhoh_o&amp;list=PLwx5dnMDVC9Y7dAjm9r26CZUw1uU5VIeq" rel="nofollow">https://www.youtube.com/watch?v=z8g8LMhoh_o&amp;list=PLwx5dnMDVC9Y7dAjm9r26CZUw1uU5VIeq</a></li>
<li>Home Karaoke (Pop): <a href="https://www.youtube.com/watch?v=MqQP3ewvJUk&amp;list=PLwx5dnMDVC9bVxfGo58U-R-w3fUHqwiD6" rel="nofollow">https://www.youtube.com/watch?v=MqQP3ewvJUk&amp;list=PLwx5dnMDVC9bVxfGo58U-R-w3fUHqwiD6</a></li>
<li>Home Karaoke (K-Pop): <a href="https://www.youtube.com/watch?v=v6qjf_ELsLA&amp;list=PLwx5dnMDVC9Z8kB01tQKfzTysaCCxC3C8" rel="nofollow">https://www.youtube.com/watch?v=v6qjf_ELsLA&amp;list=PLwx5dnMDVC9Z8kB01tQKfzTysaCCxC3C8</a></li>
<li>Home Karaoke (J-Pop): <a href="https://www.youtube.com/watch?v=KKLzoWHFAxw&amp;list=PLwx5dnMDVC9bd6y3wXs-bOas2cXIi-GAq" rel="nofollow">https://www.youtube.com/watch?v=KKLzoWHFAxw&amp;list=PLwx5dnMDVC9bd6y3wXs-bOas2cXIi-GAq</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏&nbsp;Credits</h2><a id="user-content-credits" aria-label="Permalink: 🙏&nbsp;Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Demucs: <a href="https://github.com/facebookresearch/demucs">https://github.com/facebookresearch/demucs</a></li>
<li>yt-dlp: <a href="https://github.com/yt-dlp/yt-dlp">https://github.com/yt-dlp/yt-dlp</a></li>
<li>gradio: <a href="https://github.com/gradio-app/gradio">https://github.com/gradio-app/gradio</a></li>
<li>edge-TTS: <a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a></li>
<li>F5-TTS: <a href="https://github.com/SWivid/F5-TTS.git">https://github.com/SWivid/F5-TTS.git</a></li>
<li>openai-whisper: <a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></li>
<li>faster-whisper: <a href="https://github.com/SYSTRAN/faster-whisper">https://github.com/SYSTRAN/faster-whisper</a></li>
<li>whisper-timestamped: <a href="https://github.com/linto-ai/whisper-timestamped">https://github.com/linto-ai/whisper-timestamped</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">©️&nbsp;Copyright</h2><a id="user-content-️copyright" aria-label="Permalink: ©️&nbsp;Copyright" href="#️copyright"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/abus-aikorea/voice-pro/blob/main/docs/images/ABUS-logo.jpg"><img src="https://github.com/abus-aikorea/voice-pro/raw/main/docs/images/ABUS-logo.jpg" width="100" height="100"></a> by <a href="https://abuskorea.imweb.me/" rel="nofollow">ABUS</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dell is posting unsigned updates to their website which fail to install (146 pts)]]></title>
            <link>https://infosec.exchange/@0xabad1dea/113543212666056329</link>
            <guid>42261707</guid>
            <pubDate>Thu, 28 Nov 2024 02:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://infosec.exchange/@0xabad1dea/113543212666056329">https://infosec.exchange/@0xabad1dea/113543212666056329</a>, See on <a href="https://news.ycombinator.com/item?id=42261707">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MIT Aluminum Bicycle Project 1974 (120 pts)]]></title>
            <link>https://www.sheldonbrown.com/AluminumBikeProject.html</link>
            <guid>42261053</guid>
            <pubDate>Wed, 27 Nov 2024 23:59:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sheldonbrown.com/AluminumBikeProject.html">https://www.sheldonbrown.com/AluminumBikeProject.html</a>, See on <a href="https://news.ycombinator.com/item?id=42261053">Hacker News</a></p>
<div id="readability-page-1" class="page">
<!-- Harris link grid -->

<!-- /Harris link grid -->

<!-- Harris Logo -->

<!-- /Harris Logo -->

<!-- Ad Google -->

<!-- SCBhorizontal -->
<ins data-ad-client="ca-pub-1229949690989515" data-ad-slot="8375421339"></ins>

<!-- /Ad Google -->

<!-- search Box -->




<!-- /search Box -->

<!-- Title and Author -->
<div>
<!-- XXXXX -->
	<p>
		M.I.T. Aluminum Bicycle Project 1974 
	</p>
	
<table>
<tbody><tr>
<td>
</td>
<td>
<!-- Google Translate -->



<!-- /Google Translate -->
</td>

<td>
<!-- Find on Facebook  -->
<a href="https://www.facebook.com/pages/sheldonbrowncom/244713488907207"><img src="https://www.sheldonbrown.com/images/FB_FindUsOnFacebook-100.png" alt="find us on FB"></a>
<!-- /Find on Facebook  -->
</td>

<td>
<!-- Like & Share on Facebook  -->



<!-- Like & Share on Facebook  -->
</td>

<td>
<!-- Donate -->
<form action="https://www.paypal.com/donate" method="post" target="_top">


<img alt="" src="https://www.paypal.com/en_US/i/scr/pixel.gif" width="1" height="1">
</form>
<!-- /Donate -->
</td>  
</tr>
</tbody></table>

<!-- XXXXX -->
	
	<p>This article appeared in the Proceedings of the 27th International Cycling History Conference, North Haven, Connecticut, USA, 2016</p>
	<p><img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spokeflop.gif" width="559" height="12"> 
</p></div>

<!-- Text part 1 -->
<div>
<p>In 1974, Professor Shawn Buckley ran an <a href="http://web.mit.edu/iap/">M.I.T. Independent Activities Period</a>
course where participants, including Marc Rosenbaum and Harriet Fell, got to build their own bicycle frames.  Marc Rosenbaum, then a student at the Massachusetts Institute of Technology, set out to create a bicycle that would give a rider an acceleration advantage by being ultra light without sacrificing stiffness. In this paper, we describe some of Marc’s design, the frames he and Harriet built in the IAP course, and how these played a role in the Klein vs. Cannondale patent litigation in the mid-1980s.</p>

<h2>Marc Rosenbaum’s Aluminum Bicycle</h2>
<p>In 1974, Marc Rosenbaum was a senior at M.I.T. majoring in mechanical engineering. The goal of his undergraduate thesis (<a href="http://sheldonbrown.com/MarcRosenbaumThesis.pdf">Rosenbaum 1974</a>) project was to create and describe
a bicycle that would give a rider an acceleration advantage by being ultra light without sacrificing stiffness. Marc stressed stiffness over strength, pointing out that “frames never break in normal racing conditions, which indicates that strength is not a problem.” Clearly things have changed since carbon fiber bicycles have entered the racing scene though the problem is not their resistance to normal loads encountered during rides but to impact damage.</p>

<p>In 1974, normal track bikes weighed 18 or 19 pounds. There are lighter bikes today; the I.C.U. minimum weight limit is 6.8 kg, which is about 15 pounds. There were also some lighter bikes in 1974. A bicycle made for Eddy Merckx in 1971, which he rode to set the one-hour record (30.715 mi), weighed only 13.25 pounds. It had a very light steel frame, custom built titanium handlebars, stem, and seatpost, partially plastic pedals, 24-spoke wheels, and tires expected to last at most 4 hours on a smooth board track. The components were extensively drilled out. This bicycle was not designed for strength or to last (<a href="http://sheldonbrown.com/MarcRosenbaumThesis.pdf">Rosenbaum 1974</a>).</p>

<p>
<img src="https://www.sheldonbrown.com/images/MarcBike/MarcBike1000.jpg" alt="Marc's Bike" width="1000"><br>
<strong>The bicycle built by Marc Rosenbaum weighing 12 lbs. 5oz.</strong>
</p>

<h2>Marc’s Design Guidelines</h2>
<ul>
<li><strong>Use larger diameter tubular components</strong> - Strength goes up as the cube<sup><a href="#Foot1">1</a></sup> of the diameter so unless there are geometric constraints, use larger diameter tubes with thinner walls to get a lighter structure with increased strength and stiffness.</li>
<li><strong>Use sealed precision bearings</strong> - higher quality and lighter.</li>
<li><strong>Sacrifice adjustability</strong> - adjustment features add weight.</li>
<li><strong>Use alloys</strong> – Marc settled on 6061-T6 aluminum where welding was needed and used 2024-T4 aluminum and titanium (Ti 6Al 4V) in other places. He ruled out magnesium, which is best per unit weight in compressive buckling but is brittle and difficult to extrude. He noted that aluminum is easier to weld and fabricate than titanium and more than twice as efficient as steel against compressive buckling. The walls in existing steel tubes were already down to .020” - .025” and would be prone to buckling if they were made thinner to compensate for increased diameter.</li>
</ul>
</div>
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spoke.gif" width="559" height="12">
</p>
<!-- /Text part 1 -->

<!-- Ad Google -->

<!-- SCBhorizontal -->
<ins data-ad-client="ca-pub-1229949690989515" data-ad-slot="8375421339"></ins>

<!-- /Ad Google -->

<!-- Text part 1a -->
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spokeflop.gif" width="559" height="12">
</p>

<div>
<h2>The Frame</h2>
<p>The table below shows the tube lengths and thicknesses that Marc used in his frame.
The frame was put into a jig to hold it in alignment and was joined by tungsten-inert-gas welding. After welding, it was heat-treated (annealed) to relieve stresses induced by welding and to bring the aluminum back up to its initial strength. The top tube sagged about 1/32 of an inch during annealing. This was apparently due to the greatly decreased strength of the aluminum at the high temperature of the heat treatment process.</p>

<table>
<tbody><tr>
<th colspan="3">Frame - Tube Specifications</th>
</tr>
<tr>
<th>Tube</th><th>Outside diameter (in)</th><th>Thickness (in)</th>
</tr>
<tr>
<td>Top tube</td><td>1.5</td><td>0.049/.032</td>
</tr>
<tr>
<td>Head tube</td><td>1.5</td><td>0.049</td>
</tr>
<tr>
<td>Down tube</td><td>1.5</td><td>0.049/.036</td>
</tr>
<tr>
<td>Seat tube</td><td>1.5</td><td>0.049/.036</td>
</tr>
<tr> 
<td>Bottom bracket</td><td>1.75</td><td>0.187</td>
</tr>
<tr>
<td>Seat stays</td><td>0.625</td><td>0.625</td>
</tr>
<tr>
<td>Chain stays</td><td>0.750</td><td>0.083 taper to .045</td>
</tr>
</tbody></table>

<h2>The Bottom Bracket</h2>
<p>The bottom bracket was bored out to press fit the bearings directly into it. The result is a highly accurate and simple system. A Teledyne titanium axle was modified to accept the precision bearings. The axle saved three ounces and the bearings were 2.5 ounces lighter than the conventional bearing arrangement.</p>

<h2>The Hubs</h2>
<p>The body of the rear hub is of 2024-T4 aluminum. It was bored out for precision sealed bearings and has a hollow titanium axle. It weighs only 5 ounces vs. 11.5 ounces for standard track rear hubs. Similar construction was used for the front hub. It weighed only 3 ounces vs. 9 ounces for a conventional hub.</p>


</div>
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spoke.gif" width="559" height="12">
</p>
<!-- /Text part 1a -->

<!-- Ad Lijit -->
<!-- Ezoic - 2nd_spoke_incontent_2 - long_content -->

<!-- End Ezoic - 2nd_spoke_incontent_2 - long_content -->
<!-- /Ad Lijit -->

<!-- Text part 2 -->
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spokeflop.gif" width="559" height="12">
</p>
<!-- XXXXX -->
<div>
<h2>The Pedals</h2>
<figure>
  <img src="https://www.sheldonbrown.com/images/MarcBike/pedal250.jpg" alt="Marc's pedal" width="250">
  <figcaption>Marc's pedal</figcaption>
</figure>
<p>The pedals were designed to be comfortable, easy to get into (with toe clips), and to allow sharp cornering. They have platforms of 2024-T4 aluminum on which the whole ball of the rider’s foot can rest. The axle is of titanium and is threaded to fit a standard crank. The bearings are pressed into the pedal body and onto the axle. The pair of pedals weighs 7.5 ounces, 5 ounces less than the lightest track pedals of the time.</p>

<h2>The Saddle</h2>
<figure>
  <img src="https://www.sheldonbrown.com/images/MarcBike/saddle250.jpg" alt="Marc's saddle" width="250">
  <figcaption>Marc's saddle</figcaption>
</figure>
<p>The saddle is a prime example of eliminating unneeded adjustability. Marc determined his saddle-to-pedal distance on another bicycle and decided on an integral seatpost/saddle for the bike. The result is a savings of nearly a pound.</p>

<h2>Handlebars and Stem</h2>
<figure>
  <img src="https://www.sheldonbrown.com/images/MarcBike/handlebars250.jpg" alt="Marc's handlebars and stem" width="250">
  <figcaption>Marc's handlebars and stem</figcaption>
</figure>
<p>Track bikes have the bars down low to put the rider into an efficient sprint position. Track riders often use only one position. Marc attached the handlebars directly to the fork crown, reducing weight and increasing rigidity. A simple, wide shallow U-shaped handlebar was attached via the handlebar-mounting end of a steel stem silver-soldered to a steel strip that was in turn silver-soldered to the top of the fork crown. This arrangement saved almost a pound.</p>

<h2>Other Components</h2>
<p>The wheels used Hi-E Engineering tubular rims and Pirelli Specialissimo Corsa Leggero tires. The track sprocket, chain, and T.A. cotterless 48 tooth chainring were standard. The bike initially had an 18-tooth rear sprocket for a gear of 72 inches. The headset was Stronglight Competition, and a Raleigh road-racing fork was modified to give the required wheel clearance and the design fork rake of 1.5 inches.</p>

<p>The bicycle that Marc built for his undergraduate thesis project was the world'’'s lightest track bicycle at that time. The finished bicycle was as rigid as a normal sprint track bicycle and with wheels built up for road riding, weighed 12 pounds, 5 ounces. The bicycle is currently at the M.I.T. Museum.
</p></div>

<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spoke.gif" width="559" height="12">
</p>
<!-- /Text part 2 -->

<!-- Ad Google -->

<!-- SCBhorizontal -->
<ins data-ad-client="ca-pub-1229949690989515" data-ad-slot="8375421339"></ins>

<!-- /Ad Google -->

<!-- Text part 2a -->
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spokeflop.gif" width="559" height="12">
</p>
<div>
<h2>Harriet Fell's Aluminum Bicycle</h2>
<p>In 1974, Harriet Fell was an Assistant Professor of mathematics at Northeastern University, across the river from M.I.T. The (<a href="http://web.mit.edu/iap/">M.I.T. Independent Activities Period</a>) is a special four-week term at M.I.T. that runs during the month of January.<sup><a href="#Foot2">2</a></sup> In 1974, Professor Shawn Buckley offered an Aluminum Bike Project class. This was an opportunity for students to build their own aluminum bicycle frames. The tubes suggested for the class were of a smaller diameter and had thicker walls than the ones Marc decided to use. Harriet showed up and ordered a set of tubes.</p>

<p>She designed her frame by using some measurements (e.g. top tube, seat tube, and the angle between them) from her favorite bicycle, a Holdsworth Strada. Working with the constraints of the wider tubes and her small size (5' 2") and making sure to leave enough space for 700C wheels, she made an accurate half-size drawing of the planned frame. She then measured the lengths of the other tubes from the drawing. With advice and instruction from Marc, Shawn, and the machine shop staff, she learned to use a metal lathe and milling machine enough to machine the tubes and dropouts for her bicycle. She put the parts into a jig, and Shawn sent it off for welding. Shawn said that the completed frame was particularly light and strong. (The fact that it was made for someone only 5' 2" tall surely had something to do with this. A frame for Eddy Merckx at 6' 1" would surely have weighed more.) Shawn demonstrated the strength of the frame by laying it on its side and standing/bouncing on the seat stays.<sup><a href="#Foot3">3</a></sup></p>

<p>Gary Klein also made a frame in this project in the spring of 1974 and got even wider tubes to make a larger frame.</p>

<h2>Harriet’s Frame Goes to France</h2>
<p>Just after finishing the frame, Harriet moved to France to continue her career in mathematics. She also started cycling with the cyclosport group at the Faculté de Science at Orsay. She got as far as mounting a headset and fork on her frame, but math and cycling, including riding the <a href="http://www.ccs.neu.edu/home/fell/PBP1975.html">1975 Paris-Brest-Paris</a>, filled most of her time. In September 1976, unable to extend her leave of absence for another year, she returned to Northeastern University. As she was determined to move back to France, she left her frame, a large bag of books and clothes, and a pile of tubular tires in a friend’s cellar. She did spend another year in France, but not until 1988/1989 with her husband, Sheldon Brown, and their children Tova and George. The clothes and books were intact. The tubulars had hardened to a rock-like state and were unusable. The frame met another fate.</p>

<h2>Cannondale</h2>
<p>In 1983, Cannondale manufactured its first aluminum bicycle (<a href="https://en.wikipedia.org/wiki/Cannondale_Bicycle_Corporation">Wikipedia 2016</a>). Gary Klein had been issued patents on aluminum bicycle frames and sued Cannondale for patent infringement. Schwinn was already making large-diameter aluminum frames and paid Klein license fees. Cannondale claimed that Klein was not entitled to his patent(s) because the real design came out of Marc Rosenbaum's thesis and the 1974 Aluminum Bike Project (<a href="https://www.bikeforums.net/classic-vintage/601292-aluminum-revolution.html">Bike Forums mtnbke 2009</a>)(<a href="https://law.justia.com/cases/federal/appellate-courts/F2/884/1399/464210/">Justia 1989</a>) Marc Rosenbaum was subpoenaed in the Cannondale/Klein case and gave a 6-hour videotaped deposition. He hid his bicycle in a friend’s barn so it wouldn’t be subpoenaed. He was afraid he would lose the bicycle for years and possibly never get it back if it were taken.</p>
<figure>
  <img src="https://www.sheldonbrown.com/images/FellBike/DefendantExhibit250.jpg" alt="Defendant Exhibit sticker on Fell Bike" width="250">
  <figcaption>Harriet's frame<br>Defendant exhibit in Cannondale case</figcaption>
</figure>
<p>Cannondale’s lawyer wanted frame(s) that were built before Klein’s for the case. He contacted Shawn Buckley who told him that there were five light/strong frames produced in the project. The lawyer called Harriet to ask if he could use her frame and she said, “Yes, but it’s in a friend’s cellar in France.” He visited her friends and brought the frame back for the trial.</p>

<p>Bringing in large diameter aluminum frames built by Bill Shook, founder and engineer of <a href="https://amclassic.com/company">American Classic </a>, and by Harriet Fell that predated Mr. Klein’s, Cannondale successfully argued that the Klein patent was null and void due to "prior art" - you can’t patent something that’s already in existence and known publicly (<a href="https://www.bikeforums.net/classic-vintage/601292-aluminum-revolution.html">Bike Forums tcs 2009</a>).</p>

<p>Klein's patent discovery documents had referenced Marc's bike and thesis numerous times, and it had taken Klein almost eight years to be granted a patent. Essentially, Cannondale asked, "Why is Klein's bike different from Rosenbaum's frame?"</p>



<h2>From Frame to Bike</h2>
<p>Marc was right to fear losing his bicycle for many years if it were subpoenaed. Cannondale kept Harriet's frame for many years and never let her know where it actually was. Eventually she asked her husband, Sheldon Brown, who worked in the bicycle business, to call a friend at Cannondale and ask for the frame. That worked. When the frame arrived, Sheldon hung it high on a living room wall and told every visitor about it.</p>

<p>In May 2005, Sheldon asked Harriet what she wanted for Mother's Day. She said, "Sheldon, since we've been married you've built up about 40 bikes for yourself (<a href="https://sheldonbrown.com/org/bicycle.html">Sheldon Brown's Personal Bicycle Page</a>) so how about getting that one on the road for me." 
He built the aluminum frame up with standard, fairly light-weight parts. Harriet has recently changed the saddle to a Brooks Cambium and added lights, a Quad-Lock mount for an iPhone, and a small frame bag with USB charger, all easily removable.
</p>



</div>
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spoke.gif" width="559" height="12">
</p>
<!-- /Text part 2a -->

<!-- Ad Lijit -->
<!-- Ezoic - 3rd_spoke_incontent_3 - longer_content -->

<!-- End Ezoic - 3rd_spoke_incontent_3 - longer_content -->
<!-- /Ad Lijit -->

<!-- Text part 3 -->
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spokeflop.gif" width="559" height="12">
</p>
<div>
<h2>Proof in the Riding</h2>
<p>Marc Rosenbaum rode his bicycle up Mount Washington twice<sup><a href="#Foot4">4</a></sup> before donating it to the MIT Museum. Bill Shook rode his homemade bicycle frame in several races (<a href="https://law.justia.com/cases/federal/appellate-courts/F2/884/1399/464210/">Justia 1989</a>). Harriet Fell continues to ride hers around the western suburbs of Boston.</p>

<figure>
  <img src="https://www.sheldonbrown.com/images/MarcBike/marconmarc300.jpg" alt="Marc on his bike in 1974" width="300">
  <figcaption>Marc on his bike in 1974</figcaption>
</figure>
<figure>
  <img src="https://www.sheldonbrown.com/images/FellBike/fellonfell300.jpg" alt="Harriet on her bike in 2005" width="300">
  <figcaption>Harriet on her bike in 2005</figcaption>
</figure>


<h2>Footnotes</h2>
<p><a id="Foot1"></a>1. From correspondence from Marc Rosenbaum: "For solid tubes, section modulus is proportional to radius (or diameter) cubed, so strength increases with the cube of diameter. Moment of inertia is proportional to diameter to the fourth power, so stiffness increases with the fourth power of diameter.<br>For hollow tubes, it's not as cut and dried. If you double the diameter and double the wall thickness, then strength goes up as the cube and stiffness as the fourth power. If the wall thickness remains constant, doubling the diameter doesn't increase strength by 8 and stiffness by 16."</p>
<p><a id="Foot2"></a>2. It is an opportunity for MIT students, faculty and staff to organize or participate in a wide variety of courses ranging from Blacksmithing to Charm School.</p>
<p><a id="Foot3"></a>3. Harriet’s frame did not show any sagging due to annealing. The tubes on her frame were shorter than those on Marc’s bicycle and had slightly thicker walls.</p>
<p><a id="Foot4"></a>4. He did not ride this with the original 72" gear. He rode Mt Washington in 1975 alone as the race organizers wouldn't allow him in the race with a  fixed gear bike. He rode a 1:1 gear - 20T rear and made the front chaining from a Sturmey Archer 3 speed cog (so about a 27” gear). The next year he rode the bicycle in the race, having installed a single speed freewheel with an 18T (so about a 30” gear).</p>

<h2>Acknowledgements</h2>
<p>We would like to thank the following people for
their help in this project. Shawn Buckley, Woody Flowers, Steve Loutrel, Ralph Whittemore, Ralph Bowley, Fred Anderson, and Edward Harrow.</p>

<p>A video by John Allen to go with this article: <br>
<span><a href="https://www.youtube.com/watch?v=cTq9fZ761FA">Harriet's presentation on the MIT aluminum bicycle frame project at the 2016 International Cycle History Conference.</a></span></p> 

</div>
<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spoke.gif" width="559" height="12">
</p>
<!-- /Text part 3 -->

<!-- Ad Lijit -->
<!-- Ezoic - 4th_spoke_incontent_4 - longest_content -->

<!-- End Ezoic - 4th_spoke_incontent_4 - longest_content -->
<!-- /Ad Lijit -->

<p>
	<img alt="Spoke Divider" src="http://www.sheldonbrown.com/images/spokeflop.gif" width="559" height="12">
</p>

<!-- Feedback Questions Blog Forum -->
	<ul>
	<li><a href="https://www.sheldonbrown.com/blog">What's New</a></li>
	<li><a href="https://www.facebook.com/shelbroco/">Sheldon Brown on Facebook</a></li>
	<li><a href="https://www.sheldonbrown.com/contact.html">Site Feedback &amp; Questions</a></li>
	</ul>
<!-- /Feedback Questions Blog Forum -->

<!-- Article Grid -->

	<h3>
		Articles by <a href="https://www.sheldonbrown.com/home.html">Sheldon Brown</a> and Others
	</h3>
<!-- Text part 1 -->
<ul>
<li><a href="https://www.sheldonbrown.com/blog/">What's New</a></li>
<li><a href="https://www.sheldonbrown.com/beginners.html">Beginners</a></li>
<li><a href="https://www.sheldonbrown.com/glossary.html">Bicycle Glossary</a></li>
<li><a href="https://www.sheldonbrown.com/brakes.html">Brakes</a></li>
<li><a href="https://www.sheldonbrown.com/commute/index.html">Commuting</a></li>
<li><a href="https://www.sheldonbrown.com/cyclecomputers/index.html">Cyclecomputers</a></li>
<li><a href="https://www.sheldonbrown.com/diy/index.html">Do-It-Yourself</a></li>
<li><a href="https://www.sheldonbrown.com/essays/index.html">Essays and Fiction</a></li>
<li><a href="https://www.sheldonbrown.com/family/index.html">Family Cycling</a></li>
<li><a href="https://www.sheldonbrown.com/fixed/index.html">Fixed-Gear</a></li>
<li><a href="https://www.sheldonbrown.com/frames/index.html">Frames</a></li>
<li><a href="https://www.sheldonbrown.com/gearing/index.html">Gears and Drivetrains</a></li>
<li><a href="https://www.sheldonbrown.com/humor/index.html">Humor</a></li>
<li><a href="https://www.sheldonbrown.com/oldbikes/index.html">Old Bikes</a></li>
<li><a href="https://www.sheldonbrown.com/repair/index.html">Repair Tips</a></li>
<li><a href="https://www.sheldonbrown.com/singlespeed.html">Singlespeed</a></li>
<li><a href="https://www.sheldonbrown.com/tandem/index.html">Tandems</a></li>
<li><a href="https://www.sheldonbrown.com/touring/index.html">Touring</a></li>
<li><a href="https://www.sheldonbrown.com/articles-video.html">Video</a></li>
<li><a href="https://www.sheldonbrown.com/wheels/index.html">Wheels</a></li>
<li><a href="https://www.sheldonbrown.com/translations.html">Translations</a></li>
<li><a href="https://www.sheldonbrown.com/home.html">Sheldon - the man</a></li>
</ul>

<!-- /Article Grid -->
<br>
<!-- Harris link grid -->

<!-- /Harris link grid -->

<!-- Footer -->
<!-- XXXXX -->
	<h2>
		Copyright © 1997, 2007 <a href="http://www.sheldonbrown.com/home.html">Sheldon Brown</a>
	</h2>
	<div>
		<h2>
<!-- NEW -->		
			<a href="http://www.sheldonbrown.com/harris/index.html">Harris Cyclery Home Page</a>
		</h2><p>
		If you would like to make a link or bookmark to this page, the URL is:</p><!-- XXXXX --><p>
		http://www.sheldonbrown.com/AluminumBikeProject.html<br>
</p></div>		
		<p>
<!-- XXXXX -->
	Last Updated:  by Author
</p> 
	
<!-- /Footer -->

<!-- Analytics -->
<!-- /Footer -->

<!-- Lijit Analytics -->

<!-- Google Analytics -->
 
 



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Forgotten Story of How IBM Invented the Automated Fab (108 pts)]]></title>
            <link>https://spectrum.ieee.org/semiconductor-fabrication</link>
            <guid>42260364</guid>
            <pubDate>Wed, 27 Nov 2024 22:18:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/semiconductor-fabrication">https://spectrum.ieee.org/semiconductor-fabrication</a>, See on <a href="https://news.ycombinator.com/item?id=42260364">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The Project SWIFT fabrication line was based on sectors, as shown in this patent diagram of the system from 1973. Each sector contained in an enclosure all of the wafer-processing equipment needed to accomplish a segment of the fabrication process between lithographic-pattern exposures.</p><p>
        IBM/U.S. Patent and Trademark Office
    </p><div data-headline="The Forgotten Story of How IBM Invented the Automated Fab"><p><strong>In 1970, Bill Harding </strong>envisioned a fully automated wafer-fabrication line that would produce integrated circuits in less than one day. Not only was such a goal gutsy 54 years ago, it would be bold even in today’s billion-dollar fabs, where the fabrication time of an advanced IC is measured in weeks, not days. Back then, ICs, such as random-access memory chips, were typically produced in a monthlong stop-and-go march through dozens of manual work stations.
</p><p>
	At the time, Harding was the manager of IBM’s Manufacturing Research group, in 
	<a href="https://www.poughkeepsiejournal.com/story/money/companies/ibm/2014/07/12/ibm-history-fishkill-microprocessors/12583461/" rel="noopener noreferrer" target="_blank">East Fishkill, N.Y</a>. The project he would lead to make his vision a reality, all but unknown today, was called Project SWIFT. To achieve such an amazingly short turnaround time required a level of automation that could only be accomplished by a paradigm shift in the design of <a href="https://www.hitachi-hightech.com/global/en/knowledge/semiconductor/room/manufacturing/process.html" rel="noopener noreferrer" target="_blank">integrated-circuit manufacturing</a> lines. Harding and his team accomplished it, achieving advances that would eventually be reflected throughout the global semiconductor industry. Many of SWIFT’s groundbreaking innovations are now commonplace in today’s highly automated chip fabrication plants, but SWIFT’s incredibly short turnaround time has never been equaled.
</p><p>
	SWIFT averaged 5 hours to complete each layer of its fabrication process, while the fastest modern fabs take 19 hours per processing layer, and the industry average is 36 hours. Although today’s integrated circuits are built with many more layers, on larger wafers the size of small pizzas, and the processing is more complex, those factors do not altogether close the gap. Harding’s automated manufacturing line was really, truly, swift.
</p><h2> A Semiconductor Manufacturing Manifesto</h2><p>
	I encountered Harding for the first time in 1962, and hoped it would be the last. 
	<a href="https://www.ibm.com/us-en" rel="noopener noreferrer" target="_blank">IBM</a> was gearing up to produce its first completely solid-state computer, the <a href="https://spectrum.ieee.org/building-the-system360-mainframe-nearly-destroyed-ibm" target="_self">System/360</a>. It was a somewhat rocky encounter. “What the hell good is that?” he bellowed at me as I demonstrated how tiny, unpackaged semiconductor dice could be automatically handled in bulk for testing and sorting.
</p><p><img alt="Eight men in shirts and ties play an assortment of musical instruments; A man standing in front of a chart speaks as two other men look on;  A head shot of a smiling, gray-haired man in late middle age." data-rm-shortcode-id="b672f91b35a68931066b0f5089f99242" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/eight-men-in-shirts-and-ties-play-an-assortment-of-musical-instruments-a-man-standing-in-front-of-a-chart-speaks-as-two-other-m.png?id=54890481&amp;width=980" height="2142" id="64fc4" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/eight-men-in-shirts-and-ties-play-an-assortment-of-musical-instruments-a-man-standing-in-front-of-a-chart-speaks-as-two-other-m.png?id=54890481&amp;width=980" width="1922"><small placeholder="Add Photo Caption...">Author Jesse Aronstein [at far right, in top photo] took a break from managing the equipment group of Project SWIFT to play French horn one evening a week with the Southern Dutchess Pops Orchestra. Another key manager, Walter J. “Wally” Kleinfelder [bottom left], standing at right, headed the process group of Project SWIFT. William E. “Bill” Harding [bottom right], seen here in 1973, was a brusque WW II combat veteran and creative innovator. He conceived and directed IBM’s Project SWIFT, which succeeded in fabricating integrated circuits in one day.</small><small placeholder="Add Photo Credit...">Clockwise from top: IBM/Computer History Museum; <a href="https://spectrum.ieee.org/tag/ibm">IBM</a> (2)</small></p><p><a href="https://www.legacy.com/us/obituaries/sandiegouniontribune/name/william-harding-obituary?id=20554418" rel="noopener noreferrer" target="_blank">William E. (“Bill”) Harding</a> was an innovative thinker and inventor. He had been developing <a href="https://spectrum.ieee.org/topic/semiconductors/">semiconductors</a> and their manufacturing technology at IBM for three years when the company’s new <a href="https://ibm-1401.info/Highlights-of-IBM-History.html" rel="noopener noreferrer" target="_blank">Components Division</a> was formed in 1961. Harding became a midlevel manager in the new division, responsible for developing and producing the equipment required to manufacture the System/360’s solid-state devices and circuit modules.
</p><p>
	He was rough around the edges for an IBM manager. But perhaps it was to be expected of someone who had grown up in Brooklyn, N.Y., and was wounded three times in combat in World War II while serving in General George S. Patton’s Third Army. After the war, Harding earned bachelor’s and master’s degrees in mathematics and physics and became a member of IEEE.
</p><p>
	I joined IBM in 1961, coming from rocket-engine development at General Electric. Like most engineers at the time, I knew nothing about semiconductor manufacturing. Five years prior, I had attended a vacuum-tube electronics course in which the professor described the <a href="https://spectrum.ieee.org/tag/transistor">transistor</a> as “a laboratory curiosity, which may or may not ever amount to anything.”
</p><p><img alt="A black-and-white photo shows an overhead view of an IBM semiconductor facility in the 1960s." data-rm-shortcode-id="93066e8150d5f6ae0f52bfe4c8b75c4e" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-black-and-white-photo-shows-an-overhead-view-of-an-ibm-semiconductor-facility-in-the-1960s.jpg?id=54890515&amp;width=980" height="2605" id="d0f8a" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-black-and-white-photo-shows-an-overhead-view-of-an-ibm-semiconductor-facility-in-the-1960s.jpg?id=54890515&amp;width=980" width="4243"><small placeholder="Add Photo Caption...">Project SWIFT occupied a small space, shown here in yellow, in building 310 at IBM’s sprawling East Fishkill semiconductor facility. </small><small placeholder="Add Photo Credit...">IBM</small></p><p>
	Harding’s rough and crude manner surfaced every time I crossed paths with him. If he ever went to IBM 
	<a href="https://www.ese.wustl.edu/~nehorai/paper/silverman.pdf" rel="noopener noreferrer" target="_blank">“charm school”</a> (management training), there was no discernible evidence of it. Nevertheless, he succeeded in his mission. By 1964, solid-state logic modules for System/360s were flowing from the Components Division’s new facility on a former farm in East Fishkill.
</p><p>
	In July 1970, I returned to IBM after three years of graduate study. I was a first-level manager for four years prior to that educational break, and did not want another management job. I wanted a purely technical career, and I joined East Fishkill’s Manufacturing Research (MR) group hoping to get one.
</p><p>
	Harding and I then crossed paths again. In mid-August of 1970, he became MR’s top manager. Prior to that, he spent a year developing an IBM corporate strategy for the future manufacturing and use of 
	<a href="https://vlsiweb.com/the-history-of-vlsi/" rel="noopener noreferrer" target="_blank">very-large-scale integrated</a> (VLSI) circuits. He was given command of MR to demonstrate the viability of his manufacturing concepts.
</p><p>
	An assembly of MR personnel was convened to announce the management change. After being introduced, Harding described his view of future VLSI applications and manufacturing. These were his key points:
</p><ul><li>VLSI circuits would be based on field-effect transistor technology (at the time, bipolar-junction transistors were dominant);</li><li>Defect-free high yields would be paramount;</li><li>Manufacturing would be fully automated;</li><li>Best results would accrue from processing one wafer at a time;</li><li>Short turnaround times would confer important benefits;</li><li>Volume would scale up by replicating successful production lines.</li></ul><p>
	After the educational lecture, Harding changed from professor to commander, General Patton–style. MR’s sole mission was to demonstrate Harding’s ideas, and ongoing projects not aligned with that goal would be transferred elsewhere within IBM or abandoned. MR would prove that an automated system could be constructed to process about 100 wafers a day, one at a time, with high yield and a one-day turnaround time.
</p><p><em><em>What?</em></em> Did I hear that right? One-day turnaround from bare wafer to finished circuits was what we would now call a moon shot. Remember, at the time, it typically took more than a month. Did he really mean it?
</p><p>
	Harding knew that it was theoretically possible, and he was determined to achieve it. He declared that IBM would have a substantial competitive advantage if prototype experimental IC designs could be produced in a day, instead of months. He wanted the circuit designer to have testable circuits the day after submitting the digital description to the production line.
</p><p>One-day turnaround from bare wafer to finished circuits was what we would now call a moon shot.</p><p>
	Harding immediately organized an equipment group and a process group within MR, naming me to manage the equipment group. I did not want to be a manager again. Now, reluctantly, I was a second-level manager, responsible for developing all the processing and wafer-handling equipment for a yet-to-be-defined manufacturing line that I had barely started to visualize. My dream research job had lasted little more than a month.
</p><p><a href="https://ieeexplore.ieee.org/author/37377557600" rel="noopener noreferrer" target="_blank">Walter J. (“Wally”) Kleinfelder</a> transferred into MR to manage the process group. They would select the product to manufacture and define the process by which it would be made—the detailed sequence of chemical, thermal, and lithographic steps required to take a blank silicon wafer and build integrated circuits on its surface at high yield.
</p><p>
	Kleinfelder selected a random-access memory chip, the IBM RAM II, for our demonstration. This product was being produced on-site at East Fishkill, so we would have everything we needed to build it and evaluate our results relative to those of the existing nonautomated manufacturing line.
</p><h2>IBM’s SWIFT Pilot Wafer Fab Had a Monorail “Taxi”</h2><p>
	Integrated-circuit manufacturing involves first creating the transistors and other components in their proper places on the silicon wafer surface, and then wiring them together by adding a thin film of aluminum selectively etched to create the required wiring pattern. That thin film of conductor is known as the wiring, or metallization, layer.
</p><p>
	IC manufacturing uses 
	<a href="https://mirai-intex.com/blog/photolithography-in-chip-making-process" rel="noopener noreferrer" target="_blank">photolithography</a> to create the many layers, each with a distinctive pattern, needed to fabricate an IC. These include the metal wiring layers, of which there can be more than a dozen for an advanced chip today. For these steps, the metal layer on the wafer is coated with a light-sensitive <a href="https://www.universitywafer.com/photoresist.html" rel="noopener noreferrer" target="_blank">photoresist</a> material, after which an image of the pattern is exposed on to it. The areas where conductors will be formed are blocked from the light. When the image is developed, the resist is removed from the pattern areas that were exposed, enabling these areas to be etched by an acid. The rest of the surface remains protected by the acid-proof resist. After etching is completed, the remaining protective resist is removed, leaving just the wiring layer in the required pattern.
</p><p>
	The IC process also uses lithography to create transistors and other components on the silicon wafer. Here, openings are etched in insulating layers through which tiny amounts of specific impurities can be infused into the exposed spots of pure silicon to change the electrical properties. Producing the RAM-II ICs required four separate lithographic operations using four different patterns: three for creating the transistors and other components, and one to create the metal wiring layer. The four patterns had to be exactly aligned with one another to successfully create the chips.
</p><p><a href="https://www.youtube.com/watch?v=SLLVOwB7i3g" rel="noopener noreferrer" target="_blank">Lithography</a> is only part of the IC manufacturing process, however. In the existing production line, it took many weeks to process a RAM-II wafer. But the raw process time—the time a wafer spent actually being worked on at various thermal, lithographic, chemical, and deposition stations—was less than 48 hours. Most of a wafer’s time was spent waiting to undergo the next process step. And some steps, chemical cleaning in particular, could be eliminated if wafers progressed quickly from one step to the next.
</p><p>
	It was the responsibility of Kleinfelder’s group to determine which steps could be eliminated and which could be accelerated. The resulting raw process time was less than 15 hours. It then fell to 
	<a href="https://www.sigmaxi.org/members/sigma-xi-fellows/2021-fellows/maung-htoo" rel="noopener noreferrer" target="_blank">Maung Htoo</a>, my manager of chemical-equipment development, to test the proposed process. His people hustled 1.25-inch-diameter wafers through a “pots and pans” lab setup to evaluate and refine it. The abbreviated procedure successfully produced working circuits in about 15 hours, as anticipated.
</p><p>
	The architecture of an automated system materialized. It was initially envisioned as a series of linked machines, each performing one step of the process, like an automobile assembly line. But equipment downtime for preventative maintenance and repair of breakdowns had to be accommodated. This was achieved by the insertion of short-term storage “buffers” that would temporarily store wafers at selected points in the process chain when necessary.
</p><p>
	This process chain concept was further disrupted by considerations related to <a href="https://spectrum.ieee.org/a-little-light-magic" target="_blank">lithographic-pattern imaging</a>. Exposure of the photoresist on wafers was commonly accomplished at the time by a process analogous to photographic contact printing. The lithographic mask, through which light shone when exposing the <a href="https://spectrum.ieee.org/getting-euv-ready-for-2020" target="_blank">photoresist</a>, was the equivalent of a photographic negative. Any defect or particle on the mask would result in a corresponding defect on a chip, at the same location, wafer after wafer.
</p><p>
	The East Fishkill lithography group had developed a noncontact 10:1 reduction 
	<a href="https://www.chemeurope.com/en/encyclopedia/Stepper.html" rel="noopener noreferrer" target="_blank">step-and-repeat image projector</a>. Think of it as a sort of photographic slide projector that produced a shrunken image containing the pattern for a single layer on a chip. It then “stepped” across the wafer, exposing one chip location at a time. Relative to contact masking, the <a href="https://spectrum.ieee.org/seeing-double" target="_blank">stepper</a> promised lower sensitivity to particulate contamination, because the size of the shadow of any stray particle would be reduced by 10:1. Other advantages included higher optical resolution and longer mask life.
</p><p><span data-rm-shortcode-id="74c329d80d758b7acc0f60a2cd5d71ea"><iframe frameborder="0" height="auto" type="lazy-iframe" scrolling="no" data-runner-src="https://www.youtube.com/embed/qFUSkFbKlXs?rel=0" width="100%"></iframe></span><small placeholder="Add Photo Caption...">- YouTube</small><small placeholder="Add Photo Credit..."><a href="https://www.youtube.com/watch?v=qFUSkFbKlXs" target="_blank">www.youtube.com</a></small></p><p>
	Because it was slow, though, multiple steppers would be needed to meet the throughput target. Achieving the best pattern alignment on each wafer for multiple pattern exposures required that a wafer be routed back to the same stepper for exposure of each layer in the process chain. That would cancel the effect of image distortions introduced by slight variations from one machine to another. Building the RAM-II circuits then required that a wafer make four separate trips to its assigned stepper. That divided the linear sequence into five sectors. A monorail “taxi” would take a wafer from one processing sector to its assigned stepper, and return later to take it to its next sector.
</p><p>
	Each of the five sectors was envisioned to be an enclosure containing all of the automated wafer-processing and handling equipment required to accomplish that segment of the process chain. The sector enclosures and the taxi would be designed to provide a clean-room-quality local environment for the wafers. Within a sector enclosure, typically, a wafer would pass directly from a wet-chemistry module to miniature furnaces to a photoresist application module, and, finally, to the taxi pickup port. Inside the wet-chemistry module, for example, the wafer would undergo cleaning, development of the photoresist and its removal, and etching, among other procedures.
</p><p>
	Control of the entire line was to be accomplished at three levels. Overall production-line management, recordkeeping, taxi logistics, and process monitoring would be handled by a central computer-based system. Dedicated controllers, one for each sector, would manage wafer logistics within the sector and feed wafer traffic and processing data to the central system. The individual processing and wafer-handling modules inside each sector enclosure would have their own specialized controls, as needed, for independent setup and maintenance.
</p><p>
	Finally configured, our automated demonstration line for the RAM-II chips would consist of five sectors, a taxi, and a lithographic-pattern imaging center, all managed by computer. Six months after Harding took command, MR started to design and build the actual system.
</p><h2>The Brash Middle Manager Found Inspiration in Literature</h2><p>
	Harding made frequent trips to 
	<a href="https://archive.nytimes.com/www.nytimes.com/library/cyber/week/091797ibm.html" rel="noopener noreferrer" target="_blank">IBM’s headquarters</a>, in Armonk, N.Y., to report progress, request resources, rebut challenges, and convince the top brass that the money being spent was a good investment in the future. It was a tough mission. His lengthy weekly staff meetings often reflected the pressure he was under. He lectured at length on things he knew we knew, told allegorical stories, and spun analogies.
</p><p>
	At the time, I did not realize that he was using his staff meetings to develop and refine ideas for the presentations at Armonk. He was noting our reactions and adjusting his presentation ideas accordingly. His presentations to the top brass were effective. For the duration of the project, spanning about three years, MR had all the funding and support it needed to develop, design, build, and operate the entire system.
</p><p>
	At one staff meeting, Harding read aloud Heywood Broun’s short story “
	<a href="https://zameroskiwebpage.weebly.com/uploads/1/3/3/2/13324979/the_51st_dragon.pdf" rel="noopener noreferrer" target="_blank">The 51st Dragon</a>,” to emphasize the power of a name or slogan to motivate people to achieve the impossible. His point, of course, was that we needed a really good name for the project. “SWIFT” was eventually chosen. Harding always insisted that it was not an acronym, but nevertheless people figured it was shorthand for “Semiconductor Wafer Integrated Factory Technology.”
</p><p>SWIFT’s incredibly short turnaround time has never been equaled.</p><p>
	SWIFT’s processing and wafer-handling equipment was custom designed entirely within IBM’s Components Division. The primary design objectives were to process wafers automatically, consistently, and uniformly and keep them clean and undamaged. Wafer-handling experiments sorted out the cleanest and gentlest techniques. Handling equipment was designed to support the wafer rather than grip it. A novel wafer handler that used a flow of air above the wafer to lift it, without physical contact, was successfully incorporated for some of the wafer-transport moves.
</p><p>
	There was one exception to the “clean and gentle” design of SWIFT’s handling apparatus. Management at the Components Division’s Burlington, Vt., site pressured Harding to use “air-track” wafer-transport equipment that they had developed. This equipment used airflow to lift and move wafers, much like a puck in a game of air hockey. Harding needed Burlington’s continued support, so he decreed that some air-track equipment be used in SWIFT. And it was, even though wafer-contamination and reliability questions were unresolved.
</p><p>
	Another top-down decree explains why SWIFT ended up with two different types of sector control systems—the antithesis of good design for maintainability. A custom controller had been designed, and five units were being built (one for each sector), when HQ required that we incorporate the newly announced 
	<a href="https://en.wikipedia.org/wiki/IBM_System/7" rel="noopener noreferrer" target="_blank">IBM System/7</a>, which had been developed specifically for factory-equipment and process-control applications. After all, if IBM itself didn’t use the computer in its own advanced production line, potential customers would wonder “why not?” But if SWIFT used a System/7 and the project proved to be successful, it would help sell System/7s. And so for the five sectors, SWIFT ended up with four custom controllers and one System/7. Both types worked well.
</p><p>
	Equipment reliability was SWIFT’s Achilles’ heel. To help achieve high reliability and ease of maintenance, certain mechanisms and controls were standardized for use throughout the system, and they were chosen for reliability and simplicity rather than novelty or elegance. For example, a person observing the system in operation would notice that many motions were accomplished in discrete smooth steps rather than a single traverse. Underlying that peculiarity was the extensive use of the simple, robust, and reliable 
	<a href="https://en.wikipedia.org/wiki/Geneva_drive" rel="noopener noreferrer" target="_blank">Geneva drive</a>, originally developed centuries ago for clocks, but now adapted for linear and rotary motions that had to be smooth and precisely locked in at the end points. Each easily controlled turn of the Geneva drive’s input shaft made one step. Long traverses required multiple turns of the shaft, resulting in the odd-looking motions.
</p><p><img alt="An illustration of a process. (Ask Glenn)" data-rm-shortcode-id="38f0234fd43a8a2ead8c81079e345fce" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-illustration-of-a-process-ask-glenn.png?id=54904861&amp;width=980" height="2599" id="cf4be" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-illustration-of-a-process-ask-glenn.png?id=54904861&amp;width=980" width="2763"><small placeholder="Add Photo Caption...">Inside a sector’s enclosed chamber, a wafer went through a series of entirely automated processing steps. Two of the early concept sketches are represented here. The wafers came into the upper chamber with a pattern exposed onto the resist and underwent a series of processing steps that included development, hardening, etching, and others, as indicated. </small></p><p>
	Another simplification involved spinning the wafers to centrifugally spread liquid photoresist that was dropped onto the center of the wafer. In existing lines, “wrong spin speed” was frequently cited as the cause of resist-related wafer-processing rejects. Spin speed was eliminated as a variable by driving SWIFT’s spinners with synchronous AC motors locked to 3,600 rpm by their 60-hertz AC power source, just as phonograph turntables are driven. No speed controllers would be required. The desired photoresist film thickness would be achieved by adjusting the remaining variables—temperature, viscosity, and/or spin time. In the end, system reliability was improved by the elimination of four separate speed controllers.
</p><p>
	As SWIFT progressed from blue-sky concept to actual hardware implementation, Harding adjusted MR’s organization and gained the cooperation of supporting groups. He saw to it that his people had the resources to do the job and could focus on the project. I came to admire his organizational skills and his ability to single out and recruit top-notch talent from within the company.
</p><p>
	Harding established a group to develop SWIFT’s master control system, which monitored the progress on each and every wafer as it moved through the sectors. This Execution Control System (ECS) was based on an 
	<a href="https://tcm.computerhistory.org/ComputerTimeline/Chap33_ibm1800_CS1.pdf" rel="noopener noreferrer" target="_blank">IBM 1800</a>. Each wafer had a serial number and was tracked at every step through the line. The ECS stored and monitored each wafer’s processing parameters, detecting and reacting quickly to out-of-spec situations. Its punch cards and tape cartridges seem quaint by today’s measure, but it was a major advance in production control and monitoring for a wafer line.
</p><p>
	He also transferred an entire instrumentation department, managed by Sam Campbell, from IBM Endicott to East Fishkill. Campbell’s department subsequently developed groundbreaking methods for real-time, in-situ process control for SWIFT.
</p><h2>A Short Life but an Enduring Legacy in Semiconductor Manufacturing</h2><p>
	Mockups of furnaces and chemical processors were built and tested. 
	<a href="https://www.poughkeepsiejournal.com/story/news/local/2014/10/20/ibm-straub-east-fishkill/17625349/" rel="noopener noreferrer" target="_blank">Robert J. Straub</a>’s department in East Fishkill’s Manufacturing Engineering group designed and built the sectors and the processing equipment modules within them. Harding brought in <a href="https://www.linkedin.com/in/bevanwu/" rel="noopener noreferrer" target="_blank">Bevan P.F. Wu</a> to manage the installation, debugging, and operation of the line. As equipment and facilities coalesced in SWIFT’s dedicated 4,000-square-foot space, <a href="https://www.poughkeepsiejournal.com/story/news/local/2014/10/20/ibm-east-fishkill-brunner/17624973/" rel="noopener noreferrer" target="_blank">Rolf H. Brunner</a>, who had managed a good portion of the sector designs along with development of the vacuum metal-deposition equipment, took responsibility under Wu for equipment installation, startup, and debugging.
</p><p>
	Only one operation in the entire process was not fully automated. Alignment of the wafer for exposing the pattern on the photoresist still depended on a well-trained operator. In its final form, SWIFT had both a 10:1 optical stepper and also a 1:1 contact-mask machine, but as it happened, most of the chips produced were with the 1:1 machine, because the throughput was higher that way.
</p><p>
	By the end of 1973, IBM HQ was already convinced that full automation of wafer processing could succeed. So much so that this goal was adopted as a primary objective for a new wafer-processing line to produce the circuits for IBM’s next-generation computer, the “FS” (
	<a href="https://en.wikipedia.org/wiki/IBM_Future_Systems_project" rel="noopener noreferrer" target="_blank">Future System</a>). The proposed new line was dubbed “FMS” (Future Manufacturing System), and SWIFT was renamed “FMS Feasibility Line.”
</p><p>
	Bevan Wu successfully managed the line’s completion, test runs, personnel training, and refinements of equipment, process, and procedures. He brought the line to the point of being qualified to produce circuits for IBM products. The system made five continuous-operation runs between mid-1974 and early 1975. Between runs, his group analyzed results and implemented improvements. The longest continuous run spanned 12 days. Wafer throughput averaged 58 wafers per day, 83 percent of its designed maximum. Average turnaround time from bare-wafer input to testable-circuits output was about 20 hours. The raw process time was 14 hours. The yield ultimately equaled the best ever achieved by East Fishkill’s conventional RAM-II production line.
</p><p>
	A total of 135 technicians, engineers, and managers from IBM locations worldwide were trained on the operation of the system. They produced 600 product-quality wafers with 17,000 RAM-II FET memory chips.
</p><p>
	But like his WWII commander, General Patton, Harding was bypassed to lead “the big show”—in Harding’s case, the creation of the new FMS automated line. Leaving the management career ladder behind, he was promoted to IBM Fellow, the highest nonmanagement level in the company.
</p><p>
	The FMS Feasibility Line, originally SWIFT, made its last continuous run in early 1975. It had accomplished its objectives. Its people were now needed to help create the FMS line to produce FS computers. But later in 1975, the FS project was canceled, and FMS became superfluous. A portion of the equipment destined for FMS became East Fishkill’s 
	<a href="https://ieeexplore.ieee.org/document/1156072" rel="noopener noreferrer" target="_blank">QTAT (Quick Turn Around Time)</a> line, a groundbreaking IBM showpiece that is better remembered than its obscure predecessor, Project SWIFT.
</p><p>
	Although SWIFT’s life was short, and it was never in the limelight, its many innovations are clearly visible in today’s semiconductor fabs. Like SWIFT, these fabs are highly automated and computer controlled; have a central transport system and “Bernoulli” handlers, which exploit the flow of air to lift wafers without making physical contact; apply resist immediately after oxide or metal film formation; use steppers for lithographic pattern exposure; and employ real-time process control. All of these were groundbreaking features of Project SWIFT 50 years ago.
</p><p>
	The experience of working under Harding on SWIFT for three years was, for me, transformative. What had started with trepidation ended with admiration. I have come to consider Bill Harding a true genius, in his own way. Spurred on and supported by his unique management style, a small group of dedicated people achieved far more than anyone initially envisioned. More than even we ourselves thought possible.
</p><p>
	We think of the first achievers in an industry as the “fathers” of the modern embodiment of their inventions. Edison, Bell, Ford, and the Wright brothers, are commonly spoken of this way. In that sense, William E. Harding is clearly the father of the modern, automated, billion-dollar fab. 
	<span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Float Self-Tagging (117 pts)]]></title>
            <link>https://arxiv.org/abs/2411.16544</link>
            <guid>42260030</guid>
            <pubDate>Wed, 27 Nov 2024 21:36:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2411.16544">https://arxiv.org/abs/2411.16544</a>, See on <a href="https://news.ycombinator.com/item?id=42260030">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2411.16544">View PDF</a>
    <a href="https://arxiv.org/html/2411.16544v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Dynamic and polymorphic languages must attach information, such as types, to run time objects, and therefore adapt the memory layout of values to include space for this information. This is especially problematic in the case of IEEE754 double-precision floating-point numbers, which require exactly 64 bits, leaving no space for type information. The two main encodings in-use to this day, tagged pointers and NaN-tagging, either allocate floats on the heap or unbox them at the cost of an overhead when handling all other objects.
<br>This paper presents self-tagging, a new approach to object tagging that can attach type information to 64-bit objects while retaining the ability to use all of their 64 bits for data. At its core, self-tagging exploits the fact that some bit sequences appear with very high probability. Superimposing tags with these frequent sequences allows encoding both 64-bit data and type within a single machine word. Implementations of self-tagging demonstrate that it unboxes all floats in practice, accelerating the execution time of float-intensive benchmarks in Scheme by 2.3$\times$, and in JavaScript by 2.7$\times$ without impacting the performance of other benchmarks, which makes it a good alternative to both tagged pointers and NaN-tagging.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Olivier Olivier Melançon [<a href="https://arxiv.org/show-email/de3504b4/2411.16544" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 25 Nov 2024 16:29:43 UTC (397 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[London's 850-year-old food markets to close (120 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/cje050wz22qo</link>
            <guid>42260027</guid>
            <pubDate>Wed, 27 Nov 2024 21:36:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/cje050wz22qo">https://www.bbc.co.uk/news/articles/cje050wz22qo</a>, See on <a href="https://news.ycombinator.com/item?id=42260027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" data-testid="main-content"><article><header data-component="headline-block"><h2 id="main-heading" type="headline" tabindex="-1"><span role="text">London's 850-year-old food markets to close</span></h2></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg.webp 976w" type="image/webp"><img alt="A trader wearing a white overall holds to joints of meat - one of which in the air - while people stand around him in Smithfield meat market" loading="eager" src="https://ichef.bbci.co.uk/ace/standard/2048/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/9c33/live/d82fedb0-abfd-11ef-a4fe-a3e9a6c5d640.jpg 976w" width="2048" height="1152"></picture></span><span role="text"><span>Image source, </span>PA Media</span></p><figcaption><span>Image caption, </span><p>Smithfield is the largest wholesale meat market in the UK</p></figcaption></figure></div><div data-component="text-block"><p><b>The oldest meat and fish markets in London, which date back 850 years, are facing permanent closure from 2028.</b></p><p>Smithfield meat market, near St Paul's Cathedral, and Billingsgate fish market in Canary Wharf are to be closed after the City of London Corporation voted to withdraw support for them.</p><p>The Corporation said it would now work with traders to help them find alternative premises.</p><p>Traders, who have been offered compensation, have said the move means a London tradition will be lost, with one saying "it's all about the money now".</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Meat has been traded at its current site since the 1860s</p></figcaption></figure></div><div data-component="text-block"><p>Smithfield is the largest wholesale meat market in the UK and one of the biggest in Europe. The current iteration of the market has been trading at the site since the 1860s. Prior to that it was a livestock market, which dated back to the medieval period.</p><p>Work has already begun on turning this site into a new cultural and commercial hub,  which includes the new London Museum.</p><p>Billingsgate is the largest inland fish market in the UK, with an average of 25,000 tonnes of fish and fish products sold there every year. The original market first traded in Lower Thames Street in the City in 1327, before moving to its current site in Poplar, east London, in 1982.</p><p>It provides fish to businesses including fishmongers, fish-and-chip shops, delicatessens and restaurants.</p><p>This site has now been earmarked to provide thousands of new homes.</p></div><div data-component="image-block"><figure><figcaption><span>Image caption, </span><p>Billingsgate is currently based in Poplar, east London</p></figcaption></figure></div><div data-component="text-block"><p>Initially the Corporation had planned to move both markets as well as New Spitalfields in Leyton to a £1bn purpose-built site in Dagenham, however this was dropped earlier this month over cost concerns as the council had already spent £308m purchasing and remediating the site in Dagenham.</p><p>The decision to close the markets and offer traders compensation was made by the Corporation's Court of Common Council.</p><p>The Corporation will now have to file a Private Bill in Parliament as it seeks to absolve itself of the legal responsibility of running the markets. </p><p>It said traders would be able to continue their operations at the markets until at least 2028.</p></div><p data-component="subheadline-block"><h2 id="Do-my-best" tabindex="-1"><span role="text">'Do my best'</span></h2></p><div data-component="text-block"><p>Speaking to BBC London before the decision was announced, one trader - who did not want to give his name - said he had been forced to take the compensation offer or "leave with nothing".</p><p>The trader, whose family has sold fish at the site for 70 years, added: "For what we’ve been offered to vacate the premises, I can’t go and reinstate myself somewhere else. </p><p>"I’ve been told to do my best with what I can do. It’s not great at all."</p><p>On the wider impact, he said the closures would have a "massive knock-on effect".</p><p>"It means there’s no fish market for London, which would mean the populace of London would have to resort to using local fishmongers which obviously isn’t the end of the world, but where are the fishmongers going to get their fish? </p><p>"London will be without a fish supplier, so there does need to be another fish market. When and how that’s going to happen I don’t know."</p></div><div data-component="text-block"><p>Another trader said the demise of the market started when a decision was made to rid the <a href="https://www.bbc.co.uk/news/uk-england-london-10849882">fish porters of having to have a licence</a>. </p><p>"For the younger guys down here it’s not good because no-one’s ever mentioned about the workers - they’re only ever interested in the companies," he added.</p><p>Since then, Brexit and the pandemic have both brought down and led to the market's demise, one trader told BBC London.</p><p>"It's getting to the point where everything will be bought in a vac-pack," another trader added.</p></div><p data-component="subheadline-block"><h2 id="Transition-seamlessly" tabindex="-1"><span role="text">'Transition seamlessly'</span></h2></p><div data-component="text-block"><p>Chris Hayward, policy chairman of the City of London Corporation, said the decision represented a "positive new chapter" for the markets as it "empowers traders to build a sustainable future in premises that align with their long-term business goals".</p><p>He added: "By stepping back from direct market operations, we will help to create opportunities for these businesses to thrive independently.</p><p>"We've worked closely with the traders and thank them for their input and understanding.</p><p>"We're committed to making sure they have the financial support and guidance they need to transition seamlessly and successfully to new locations."</p><p>Dominic Twomey, leader of Barking and Dagenham Council, said although it was disappointing news, the council understood the "financial pressures that key investment projects" were facing.</p><p>He added they were committed to working with the Corporation to "unlock the huge potential" of the Dagenham Dock site. </p></div><div data-component="text-block"><p><i>Listen to the best of BBC Radio London on </i><a href="https://www.bbc.co.uk/sounds/curation/p0cjdwm5"><i>Sounds</i></a><i> and follow BBC London on </i><a href="https://www.facebook.com/BBCLondon"><i>Facebook</i><span>, <!-- -->external</span></a><i>, </i><a href="https://twitter.com/BBCLondonNews"><i>X</i><span>, <!-- -->external</span></a><i> and </i><a href="https://www.instagram.com/bbclondon/?hl=en"><i>Instagram</i><span>, <!-- -->external</span></a><i>. Send your story ideas to </i><a href="mailto:hello.bbclondon@bbc.co.uk"><i>hello.bbclondon@bbc.co.uk</i><span>, <!-- -->external</span></a></p></div><section data-component="links-block"><p><h2 type="normal">Related internet links</h2></p></section><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Developing a cancer drug without Big Pharma: this hospital shows it can be done (105 pts)]]></title>
            <link>https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn</link>
            <guid>42260017</guid>
            <pubDate>Wed, 27 Nov 2024 21:35:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn">https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn</a>, See on <a href="https://news.ycombinator.com/item?id=42260017">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="comment-section">
    <a name="bijdragen"></a>

                
                                                
            
        <p>
        <h2>
            <span>15<i></i></span> Bijdragen
        </h2>
        
                        
                            
                </p>
    
                        

        
        <div>
        <p>
            Oudste eerst
        </p>
        <p>Nieuwste eerst</p>
    </div>
    
        <div id="comments">
                    
<div id="comment455190" data-id="455190" data-user-id="10397" data-timestamp="1732686184">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455190">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455190">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            Doet denken aan Mariana Mazzucato:</p><p>

According to conventional wisdom, innovation is best left to the dynamic entrepreneurs of the private sector, and government should get out of the way. But what if all this was wrong? What if, from Silicon Valley to medical breakthroughs, the public sector has been the boldest and most valuable risk-taker of all?</p><p>

https://marianamazzucato.com/books/the-entrepreneurial-state/</p><p>

En economie als staatsgodsdienst met haar eigen profeten, regels en voorschriften:<br>
80.000 euro per QALY bij een ziektelast van 1. Dit getal betekent dat een extra levensjaar in volledige gezondheid maximaal 80.000 euro mag kosten. Welke invloed heeft de vergrijzing op dat bedrag? En hoe prijzen we Quality?
                    </p></div>
        
        

        
    </div>                    
<div id="comment455191">

    <div id="bijdrage455191" data-id="455191" data-user-id="76013" data-timestamp="1732694172">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455191">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455191">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            Knap stukje werk van 'betrokken' artsen/specialisten en onderzoekers, dat door een onmogelijk rationeel te bezien traject tot goedkeuring en gebruik misschien niet toegepast kan/mag worden.</p><p>

Schitterend ook, die volledige 'gladstrijkende' trajectmedewerking van een EMA waarvan iedere Europeaan mag verwachten dat die er zijn voor hun belang alleen.</p><p>

Dan gaan we maar niet vragen* waarom dit soort prachtige initiatieven niet zijn 'opgepikt' in een Brussel, en er met de opgebrachte gemeenschapsgelden niet al een European Lab voor dit type betaalbare alternatieven is opgericht!</p><p>

*omdat men daar vooral bezig is met 'zelfverrijking projecten' en het 'achter de feiten aanlopen'!
                    </p></div>
        
        

        
    </div>

                <div id="comment455192">

    <div id="bijdrage455192" data-id="455192" data-user-id="69326" data-timestamp="1732700251">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455192">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455192">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <p>
                            Hmm die laatste conclusie lijkt me te gemakkelijk. Ik weet niet hoe de EMA precies werkt maar ik verwacht niet dat het primair op zelfverrijking gericht zijn, in tegenstelling tot die farmaceutische bedrijven. Bovendien zullen er heus ook in Europa dubieuze bedrijven en ook academici zijn die liever willen profiteren dan zorgvuldig handelen. Dan is een bureaucratische moloch als de EMA misschien zo slecht nog niet.
                    </p>
        
        

        
    </div>

                <div id="comment455194" data-id="455194" data-user-id="76013" data-timestamp="1732700787">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455194">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455194">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            Ik heb het hier over het fenomeen "Brussel" in het algemeen en niet specifiek over een EMA.<br>
Waarom is er door al die leden in de diverse Commissies en al die 750 parlementsleden met hun gehele ambtelijke ondersteuning nu nooit eens gedacht; "misschien moeten we eens zelf hier in Europa een groot medisch research centrum faciliteren"? </p><p>

Het algehele 'succesful track record' van de club in Brussel op elk vlak is uiterst mager!!!!
                    </p></div>
        
        

        
    </div>
    
    </div>
    
    </div>                    
                    
<div id="comment455198" data-id="455198" data-user-id="54011" data-timestamp="1732702390">

                
        

        <p>
                            Als de methoden van het AVL en die van Iovance verschillend genoeg zijn, dan kan er toch geen sprake zijn van 'de eerste wint'? Ik neem toch aan dat "succesvolle behandeling van specifiek type kanker" niet gepatenteerd kan worden. Dus ruimte voor beiden, en als de goedkoopste neits doet, dan maar die dure. Wat kan er kapitalistischer zijn dan dat? En als ze wel op elkaar lijken... tja dan wint niet niet automatisch de goedkoopste, hoewel ik in het geval van ziekten met slechte prognosen die hele octrooiwet het liefst op de vuilnisbelt zie belanden. Maar dat is een andere discussie.
                    </p>
        
        

        
    </div>                    
<div id="comment455208">

    <div data-nosnippet="" id="bijdrage455208" data-id="455208" data-user-id="1022" data-timestamp="1732707851"><p>
                            De publieke sector is dus kennelijk in staat een product te leveren dat gelijkwaardig maar goedkoper is dan een commercieel product. Gebruik hiervan zorgt dat de totale kosten, inclusief zorgkosten, lager zijn.</p><p>

Bedrijven kunnen dan zeggen dat het oneerlijke concurrentie is.</p><p>

Maar kennelijk werkt de markt niet, want die zorgt er met concurrentie niet voor dat bedrijven hetzelfde of beter kunnen leveren voor minder totale kosten.</p><p>

Gegeven de florissante economische conditie van de meeste Nederlanders, moeten we dan als publiek kiezen voor <br>
(a) dogmatisch de markt volgen en vele malen teveel te betalen ten bate van aandeelhouders? <br>
(b) Of om in dit geval een ander mechanisme te gebruiken dat – kennelijk – efficiënter is dan de betreffende markt?
                    </p></div>

                <div id="comment455223" data-id="455223" data-user-id="367595" data-timestamp="1732720044">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455223">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455223">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            De markt draait in principe om geld verdienen, hoe meer hoe beter. En dus niet om het maatschappelijk belang c.q. de volksgezondheid te dienen en zorg betaalbaar te houden. Volksgezondheid is niet meer dan een verdienmodel.</p><p>

Daarom is het mooi dat er een ziekenhuis laat zien dat het anders kan. Nu hopen dat het ook echt lukt en ze niet in de wielen worden gereden door die Amerikaanse bedrijven die natuurlijk miljoenen aan juridische procedures kunnen besteden...</p><p>

Ben ook benieuwd hoe onze overheid hier naar kijkt.
                    </p></div>
        
        

        
    </div>
    
    </div>                    
<div id="comment455226">

    <div id="bijdrage455226" data-id="455226" data-user-id="163094" data-timestamp="1732722015">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455226">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455226">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <p>
                            Hulde voor AVL. Big Pharma denkt alleen maar aan geld, niet aan onze gezondheid. Als ik lees hoe het proces verloopt, dan vraag ik me wel af hoe dat met de corona vaccins gegaan is. Misschien iets te snel door de bocht gegaan? Benieuwd hoe in de VS Big Pharma zal vergaan zodra Kennedy minister is.
                    </p>
        
        

        
    </div>

                <div id="comment455227" data-id="455227" data-user-id="76013" data-timestamp="1732722734">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455227">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455227">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <p>
                            Denk dat de dames en heren bij de FDA, die normaliter heerlijk warmpjes op de schoot zitten bij de Big Pharma in de VS, nu enigszins 'peentjes zweten' omdat er meer 'objectief' naar verschillende zaken gekeken gaat worden.
                    </p>
        
        

        
    </div>
    
    </div>                    
<div id="comment455229">

    <div id="bijdrage455229" data-id="455229" data-user-id="72" data-timestamp="1732734101">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455229">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455229">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            Haanen werkt al zeker 20 jaar voor het ALV en heeft mijn in 2012 overleden vrouw met nierkanker 10 jaar lang fantastisch bijgestaan. Ik wens hem en zijn companen alle succes met het verkrijgen van de vergunning.</p><p>

Het onderscheid tussen markt en overheid in dit artikel is veel te dogmatisch</p><p>

Voor een goedwekende kapitalistische markt is concurrentie essentieel. Als die concurrentie ontbreekt ontstaan er monopoliewinsten en valt de prikkel om te verbeteren weg.    Bij het ontbreken van voldoende concurrentie zou het afgeven van vergunningen versoepeld moeten worden.</p><p>

Vroeger hanteerde de echt liberale prof Hennipman de stelling dat er tenminste 5 concurrerende aanbieders aanwezig zouden moeten zijn om het maken van overwinsten te voorkomen.  Ook in geval van monopolistische concurrentie,( zoals hier bij dure maar niet identieke kankermedicijnen), lijkt het hanteren van een kwantitatieve maatstaf gewenst.  </p><p>
  
Ik begrijp het probleem van het terugverdienen van de hoge researchkosten, maar vanuit maatschappelijk oogpunt is het ook belangrijk dat er efficient gewerkt wordt en dat onnodige uitgaven niet in de prijs van het geneesmiddel tot uitdrukking komen. </p><p>

Om dat doel te bereiken heb je concurrentie nodig.
                    </p></div>
        
        

        
    </div>

                <div id="comment455231">

    <div id="bijdrage455231" data-id="455231" data-user-id="367595" data-timestamp="1732738749">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455231">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455231">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            Maar die concurrentie kun je toch ook verbeteren door als overheid/publieke sector dit soort initiatieven te steunen en mogelijk later naar de markt te brengen zodat er inderdaad meer aanbieders komen? Dus zou je acties in die richting mogen verwachten van onze overheid toch?</p><p>

Wel vreselijk te horen dat je vrouw in 2012 is overleden...
                    </p></div>
        
        

        
    </div>

                <div id="comment455234" data-id="455234" data-user-id="72" data-timestamp="1732745888">

                
        <div>
                    <ul>
                                                <li>
                            <a href="https://www.ftm.nl/artikelen/ruzie-tussen-ziekenhuis-en-farma-industrie-over-goedkeuring-kankermedicijn#bijdrage455234">
                                <i></i>
                                Directe link naar deze bijdrage
                            </a>
                        </li>
                        
                                                
                                                
                                                <li>
                            <a href="https://www.ftm.nl/bijdrage/feature/455234">
                                <i></i>
                                Bijdrage uitlichten
                            </a>
                        </li>
                        
                                            </ul>
                </div>

        <div data-nosnippet=""><p>
                            Uiteraard ben ik er voorstander van dat de overheid/publieke sector dit soort initiatieven steunt. Maar niet zo zeer om ideologische redenen, maar omdat concurrentie tot betaalbare kwaliteitsverhoging leidt.</p><p>

Als er vanuit de private sector onvoldoende concurrentie komt (door hoge barriers to entree) zal de overheid of anders cooperaties daarvoor moeten zorgen. Zo hebben woningbouwcorporaties in het verleden de kwaliteit van de huisvestig bevordert.
                    </p></div>
        
        

        
    </div>
    
    </div>
    
    </div>            </div>

        

                    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TrunkVer (116 pts)]]></title>
            <link>https://trunkver.org/</link>
            <guid>42259604</guid>
            <pubDate>Wed, 27 Nov 2024 20:48:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trunkver.org/">https://trunkver.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42259604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<p>so we can stop talking about versions and start shipping.</p>

<div>
    <p>20241127214906.0.0</p>
    <p>-</p>
    
    <p>-</p>
    
    <p>When it was built</p>
    
    <p>What was built</p>
    
    <p>How it was built</p>
    <p>date -u +%Y%m%d%H%M%S</p>
    
    <p>git rev-parse --short HEAD</p>
    
    <p>${CI_JOB_ID}</p>
  </div>
<h2>TL;DR</h2>
<p><code>TrunkVer</code> is a SemVer-compatible versioning scheme for
continuously-delivered, trunk-based applications and systems that don't follow a release scheme.</p>
<p>It is a <strong>drop-in replacement</strong> for semantic versions and replaces the version with meaningful meta data, telling you at a glance what the artifact is, when it was built and where you may find the build log.</p>
<h2>Usage</h2>
<h3>GitHub Actions</h3>
<pre><code><span>-</span> <span>name</span><span>:</span> Generate trunkver
  <span>id</span><span>:</span> trunkver
  <span>uses</span><span>:</span> crftd<span>-</span>tech/trunkver@main

<span>-</span> <span>name</span><span>:</span> Print trunkver
  <span>env</span><span>:</span>
    <span>TRUNKVER</span><span>:</span> $<span>{</span><span>{</span> steps.trunkver.outputs.trunkver <span>}</span><span>}</span>
  <span>run</span><span>:</span> <span>|</span><span>
    echo "$TRUNKVER"</span></code></pre>
<h3>GitLab</h3>
<h4>Using our template from https://gitlab.com/crftd-tech/trunkver-gitlab-ci</h4>
<pre><code><span>include</span><span>:</span>
<span>-</span> <span>remote</span><span>:</span> <span>'https://gitlab.com/crftd-tech/trunkver-gitlab-ci/-/raw/main/trunkver.gitlab-ci.yml'</span></code></pre>
<h4>Downloading the CLI directly</h4>
<pre><code><span>build</span><span>:</span>
  <span>script</span><span>:</span>
    <span>-</span> curl <span>-</span>sSL https<span>:</span>//github.com/crftd<span>-</span>tech/trunkver/releases/latest/download/trunkver_linux_amd64 <span>-</span>o trunkver
    <span>-</span> chmod +x trunkver
    <span>-</span> export TRUNKVER=$(./trunkver generate)</code></pre>
<h3>Docker</h3>
<pre><code><span>docker</span> run <span>--rm</span> ghcr.io/crftd-tech/trunkver:latest generate --build-ref <span>"<span>$CI_JOB_ID</span>"</span> --source-ref <span>"g<span><span>$(</span><span>git</span> rev-parse <span>--short</span> HEAD<span>)</span></span>"</span></code></pre>
<h3>Other CIs</h3>
<pre><code><span>curl</span> <span>-sSL</span> https://github.com/crftd-tech/trunkver/releases/latest/download/trunkver_linux_amd64 <span>-o</span> trunkver
<span>chmod</span> +x trunkver
./trunkver generate</code></pre>
<h2>Rationale</h2>
<p>We have identified a frequent source of avoidable confusion, conflict and cost in the software delivery process caused by versioning software that should not be versioned - or rather, the versioning should be automated.</p>
<p>Historically, countless versioning schemes have been used to signify changes to a piece of software, using a lot of not clearly defined words such as beta, final or release candidate as well as arbitrary numbering schemes that typically involve one or more digits that are incremented according to certain rules, or worse, without clear rules.</p>
<p>Over time, semantic versioning has been proposed and adopted by many developer teams for good reasons. It clearly defines what each part of a version number means, such as incrementing the first digit, or major version, to signify a change that is considered to be a “breaking” one. This can be used by both humans and machines to improve their work, such as a machine refusing to automatically apply an update in this case and notifying a human to adapt to the breaking change. We are fans of semantic versioning.</p>
<p>However, we keep encountering teams and organizations that apply semantic versioning or a custom versioning scheme to software that does not need any of that - and through this, they create an astonishing amount of unnecessary work such as arguing whether or not a certain piece of software should be called “alpha”, “beta”, “rho”, “really final v4” etc, manually creating tickets listing the changes or even specialized gatekeeper roles such as “release engineer” - in the worst case a single person in the whole organization. Because this makes it harder, boring and costly to deploy, it systematically reduces the number of deployments, and through this the delivery performance of the organization.</p>
<p>We acknowledge that these efforts often stem from perfectly valid requirements of various stakeholders, such as the necessity to audit the release process, finding out what version of the software is currently running or adhering to a specific certified process. Ironically, the manual process around it makes this not only costly, but often defeats the intended purpose. We have seen audit trails missing commits, full of copy/paste errors, etc. We therefore argue the only way to get auditing right is by automating it too, including the version numbering.</p>
<p>In an organization that creates software in teams of trusted contributors that deploy software to controlled environments together, this kind of versioning ceremony can become a major hurdle to adopting the XP and DevOps practices of trunk-based development and continuous integration/deployment/delivery - and it can and should be replaced by a tiny amount of code.</p>
<h2>Principles</h2>
<ul>
<li>The TrunkVer is automatically created during the build process, typically as the first step, and then re-used across the build, e.g. to tag created images in a registry or as part of a file name.</li>
<li>There is a single source of truth for the current code - usually a git branch named <code>trunk</code>, <code>default</code> or <code>main</code>.</li>
<li>From the source, we create a deployable artifact of the software - e.g. a docker image, jar file or debian package. As described in the 12 factor principles, this artifact will be deployed to an environment by combining it with configuration not part of the artifact. Versioning deployments is tool-specific and out of scope</li>
<li>There are only release candidates. Every push to the trunk triggers an automated process that creates a potentially releasable artifact, and usually it is automatically released shortly after</li>
<li>Version numbers are useful to developers: While technically a build jobs id would be perfectly viable to use, we prepend a timestamp to make them sortable, and the source code revision to identify it without having to go through the build server.</li>
<li>All secondary artifacts of a release, such as tickets or a changelog, are automated created during the build, referencing the TrunkVer.</li>
</ul>
<h2>Specification</h2>
<p>The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in <a href="https://tools.ietf.org/html/rfc2119">RFC 2119</a>.</p>
<ol>
<li>
<p>A TrunkVer is suitable for versioning of any artifact that may be released without any regard for compatibility with third parties integrating with the artifact.</p>
</li>
<li>
<p>A TrunkVer may be used for an artifact, while SemVer or other versioning schemes may be used for specific interfaces to that artifact, e.g. REST APIs.</p>
</li>
<li>
<p>A TrunkVer is syntactically compatible with SemVer, although it does not respect its semantic interpretation of the version number.</p>
</li>
<li>
<p>A TrunkVer consists of three components: A <strong>timestamp</strong>, a <strong>source reference</strong>, and <strong>a build reference</strong>.</p>
</li>
<li>
<p>The <strong>timestamp</strong> is precise to one second and always formatted in UTC i.e. <code>YYYYMMDDHHMMSS</code>, e.g. <code>20241230142105</code>. It replaces the <strong>Major</strong> part of a SemVer.</p>
</li>
<li>
<p>The <strong>source reference</strong> identifies the exact source code used to build the artifact. Identifiers MUST comprise only ASCII alphanumerics <code>[09-A-Za-z]</code>. Identifiers MUST NOT be empty. If the source reference is a git commit checksum, it may be truncated to 7 characters (<code>--short</code>) and be prefixed with a <code>g</code>.</p>
</li>
<li>
<p>The <strong>build reference</strong> identifies the exact build job used to build the artifact. Identifiers MUST comprise only ASCII alphanumerics and hyphens <code>[0-9A-Za-z-]</code>.</p>
</li>
<li>
<p>A TrunkVer is formatted as follows: The <strong>timestamp</strong>, followed by <code>.0.0-</code>, followed by the <strong>source reference</strong>, followed by <code>-</code>, followed by the <strong>build reference</strong>. It may be used to replace any SemVer version.</p>
</li>
<li>
<p>Alternatively, for the purpose of e.g. prereleases of an otherwise SemVer-versioned artifact, a TrunkVer may be assembled into the prerelease part of a SemVer version as follows: The <strong>timestamp</strong>, followed by <code>-</code>, followed by the <strong>source reference</strong>, followed by <code>-</code>, followed by the <strong>build reference</strong>.</p>
</li>
</ol>
<h3>EBNF Definition</h3>
<pre><code><span>TRUNKVER</span> <span>=</span> <span>(</span> <span>MAJOR_TRUNKVER</span> <span>|</span> <span>PRERELEASE_TRUNKVER</span> <span>)</span><span>;</span>
<span>MAJOR_TRUNKVER</span> <span>=</span> <span>TIMESTAMP</span><span>,</span> <span>'.0.0-'</span><span>,</span> <span>BUILD_REF</span><span>,</span> <span>'-'</span><span>,</span> <span>SOURCE_REF</span><span>;</span>
<span>MINOR_TRUNKVER</span> <span>=</span> <span>TIMESTAMP</span><span>,</span> <span>'-'</span><span>,</span> <span>BUILD_REF</span><span>,</span> <span>'-'</span><span>,</span> <span>SOURCE_REF</span><span>;</span>

<span>TIMESTAMP</span> <span>=</span> <span>NON_ZERO_DIGIT</span><span>,</span> 11<span>*</span><span>DIGIT</span><span>;</span>
<span>SOURCE_REF</span> <span>=</span> <span>GIT_COMMIT_REF</span> <span>|</span> <span>{</span> <span>ALPHANUMERIC</span> <span>}</span><span>;</span>
<span>GIT_COMMIT_REF</span> <span>=</span> <span>'g'</span><span>,</span> <span>HEXADECIMAL</span><span>;</span>
<span>BUILD_REF</span> <span>=</span> <span>{</span> <span>ALPHANUMERIC</span> <span>|</span> <span>'-'</span> <span>}</span><span>;</span>


<span>DIGIT</span> <span>=</span> <span>"0"</span> <span>|</span> <span>NON_ZERO_DIGIT</span><span>;</span>
<span>NON_ZERO_DIGIT</span> <span>=</span> <span>"1"</span> <span>|</span> <span>"2"</span> <span>|</span> <span>"3"</span> <span>|</span> <span>"4"</span> <span>|</span> <span>"5"</span> <span>|</span> <span>"6"</span> <span>|</span> <span>"7"</span> <span>|</span> <span>"8"</span> <span>|</span> <span>"9"</span><span>;</span>
<span>HEXADECIMAL</span> <span>=</span> <span>"a"</span> <span>|</span> <span>"b"</span> <span>|</span> <span>"c"</span> <span>|</span> <span>"d"</span> <span>|</span> <span>"e"</span> <span>|</span> <span>"f"</span> <span>|</span> <span>"A"</span> <span>|</span> <span>"B"</span> <span>|</span> <span>"C"</span> <span>|</span> <span>"D"</span> <span>|</span> <span>"E"</span> <span>|</span> <span>"F"</span> <span>|</span> <span>"0"</span> <span>|</span> <span>"1"</span> <span>|</span> <span>"2"</span> <span>|</span> <span>"3"</span> <span>|</span> <span>"4"</span> <span>|</span> <span>"5"</span> <span>|</span> <span>"6"</span> <span>|</span> <span>"7"</span> <span>|</span> <span>"8"</span> <span>|</span> <span>"9"</span><span>;</span>
<span>ALPHANUMERIC</span> <span>=</span> <span>"a"</span> <span>|</span> <span>"b"</span> <span>|</span> <span>"c"</span> <span>|</span> <span>"d"</span> <span>|</span> <span>"e"</span> <span>|</span> <span>"f"</span> <span>|</span> <span>"g"</span> <span>|</span> <span>"h"</span> <span>|</span> <span>"i"</span> <span>|</span> <span>"j"</span> <span>|</span> <span>"k"</span> <span>|</span> <span>"l"</span> <span>|</span> <span>"m"</span> <span>|</span> <span>"n"</span> <span>|</span> <span>"o"</span> <span>|</span> <span>"p"</span> <span>|</span> <span>"q"</span> <span>|</span> <span>"r"</span> <span>|</span> <span>"s"</span> <span>|</span> <span>"t"</span> <span>|</span> <span>"u"</span> <span>|</span> <span>"v"</span> <span>|</span> <span>"w"</span> <span>|</span> <span>"x"</span> <span>|</span> <span>"y"</span> <span>|</span> <span>"z"</span> <span>|</span> <span>"A"</span> <span>|</span> <span>"B"</span> <span>|</span> <span>"C"</span> <span>|</span> <span>"D"</span> <span>|</span> <span>"E"</span> <span>|</span> <span>"F"</span> <span>|</span> <span>"G"</span> <span>|</span> <span>"H"</span> <span>|</span> <span>"I"</span> <span>|</span> <span>"J"</span> <span>|</span> <span>"K"</span> <span>|</span> <span>"L"</span> <span>|</span> <span>"M"</span> <span>|</span> <span>"N"</span> <span>|</span> <span>"O"</span> <span>|</span> <span>"P"</span> <span>|</span> <span>"Q"</span> <span>|</span> <span>"R"</span> <span>|</span> <span>"S"</span> <span>|</span> <span>"T"</span> <span>|</span> <span>"U"</span> <span>|</span> <span>"V"</span> <span>|</span> <span>"W"</span> <span>|</span> <span>"X"</span> <span>|</span> <span>"Y"</span> <span>|</span> <span>"Z"</span><span>;</span>
</code></pre>
<h2>FAQ</h2>
<ul>
<li>
<p><strong>Why only use the MAJOR SemVer part as a timestamp?</strong><br>
Tools (e.g. compliance, audit trails) might still classify a version based on SemVer semantics, hence TrunkVer always defensively implies breaking changes between versions.</p>
</li>
<li>
<p><strong>Why do you use the PRERELEASE SemVer part for source information and not BUILD?</strong><br>
Because OCI tags don't support <code>+</code> (see <a href="https://github.com/distribution/distribution/issues/1201">distribution/distribution#1201</a> and <a href="https://github.com/opencontainers/distribution-spec/issues/154">opencontainers/distribution-spec#154</a>. We'd rather have one consistent version across artifacts. Semantically, the only relevant portion for sorting of a TrunkVer is
the MAJOR version, and a conflict (as in creating two artifacts in the
very same second) should be avoided.</p>
</li>
</ul>
<h2>Links</h2>
<ul>
<li><a href="https://github.com/crftd-tech/trunkver">https://github.com/crftd-tech/trunkver</a></li>
<li><a href="https://crftd.tech/">https://crftd.tech/</a></li>
<li><a href="https://semver.org/">https://semver.org</a></li>
</ul>

      
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Malware can turn off webcam LED and record video, demonstrated on ThinkPad X230 (712 pts)]]></title>
            <link>https://github.com/xairy/lights-out</link>
            <guid>42259278</guid>
            <pubDate>Wed, 27 Nov 2024 20:10:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xairy/lights-out">https://github.com/xairy/lights-out</a>, See on <a href="https://news.ycombinator.com/item?id=42259278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Lights Out</h2><a id="user-content-lights-out" aria-label="Permalink: Lights Out" href="#lights-out"></a></p>
<p dir="auto">This repository contains tools that allow getting software control of the webcam LED on ThinkPad X230 <strong>without physical access to the laptop</strong>.
These were created as a practical demonstration that malware can record video through the webcam without the LED indication.</p>
<p dir="auto">This works via reflashing the webcam firmware over USB (the X230 webcam is connected over USB internally) to add a capability of arbitrarily controlling the LED.
This approach likely affects many other laptops, as connecting the webcam over USB and allowing to reflash its firmware is a common design pattern across laptop manufacturers.</p>
<p dir="auto">See the <a href="https://docs.google.com/presentation/d/1NSS2frdiyRVr-5vIjAU-2wf_agzpdiMR1DvVhz2eDwc/edit?usp=sharing" rel="nofollow">"Lights Out: Covertly turning off the ThinkPad webcam LED indicator"</a> talk (<a href="https://powerofcommunity.net/poc2024/Andrey%20Konovalov,%20Lights%20Out%20-%20Covertly%20turning%20off%20the%20ThinkPad%20webcam%20LED%20indicator.pdf" rel="nofollow">pdf</a>) I gave at <a href="https://powerofcommunity.net/" rel="nofollow">POC 2024</a> for the details: discovering a way to reflash the X230 webcam firmware, reverse engineering the firmware, adding an implant for LED control, and notes about the applicability of the approach to other laptops.</p>
<p dir="auto"><strong>Note: Reflashing the webcam firmware might brick the webcam, use these tools with caution</strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">The webcam used on ThinkPad X230 (and a few other laptops from the same era) is based on the Ricoh R5U8710 USB camera controller.
This controller stores a part of its firmware, the SROM part, on the SPI flash chip located on the webcam board.
The controller also allows reflashing the contents of the SPI chip over USB.</p>
<p dir="auto">The LED on the X230 webcam board is connected to the GPIO B1 pin of the R5U8710 controller.
The GPIO B port is mapped to address <code>0x80</code> in the <code>XDATA</code> memory space of the 8051-based CPU inside R5U8710.
Thus, changing the value at that address changes the state of the LED.
This works regardless of whether the webcam is streaming video at the moment or not.</p>
<p dir="auto">The tools provided in this repository allow flashing custom firmware with a USB-controlled so-called "universal implant" onto the SPI chip on the webcam board.
This implant allows writing controlled data to arbitrary addesses (within the <code>XDATA</code> memory space) and calling arbitrary addresses (within the <code>CODE</code> memory space; aliased with <code>XDATA</code> starting from offset <code>0xb000</code>).</p>
<p dir="auto">The universal implant can be used for:</p>
<ul dir="auto">
<li>
<p dir="auto">Dynamically uploading a second-stage implant within the camera contoller memory and executing it (originally used for reverse engineering purposes);</p>
</li>
<li>
<p dir="auto">Directly controlling the webcam LED.</p>
</li>
</ul>
<p dir="auto">See the talk slides for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tools</h2><a id="user-content-tools" aria-label="Permalink: Tools" href="#tools"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/srom.py">srom.py</a> — reads and writes the SROM part of the firmware of a Ricoh R5U8710–based webcam over USB.</p>
<p dir="auto">Note: The webcam only loads the SROM firmware during its boot.
Thus, you will need to power cycle the laptop (full shutdown, not just reboot) for the updated firmware to get loaded;</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/patch_srom.py">patch_srom.py</a> — patches the SROM image from the FRU <code>63Y0248</code> webcam (not from the original X230 webcam) to add the universal implant.</p>
<p dir="auto">Note: This tool requires modification to work with the original X230 webcam SROM image.
However, the FRU <code>63Y0248</code> SROM image (optionally, with the implant added) can be flashed onto the original X230 webcam as well;</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/fetch.py">fetch.py</a> — fetches the contents of the <code>IRAM</code>, <code>XDATA</code>, or <code>CODE</code> memory space over USB via a second-stage implant that gets dynamically uploaded via the universal implant;</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/led.py">led.py</a> — turns the webcam LED on or off by overwriting the value at address <code>0x80</code> in <code>XDATA</code> via the universal implant.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Memory dumps</h2><a id="user-content-memory-dumps" aria-label="Permalink: Memory dumps" href="#memory-dumps"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/srom/x230.bin">srom/x230.bin</a> — SROM contents of the original X230 webcam module (FRU unknown; <code>19N1L1NVRA0H</code> marking on the board);</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/srom/63Y0248.bin">srom/63Y0248.bin</a> — SROM contents of the FRU <code>63Y0248</code> webcam module;</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/xairy/lights-out/blob/main/code/63Y0248.bin">code/63Y0248.bin</a> — Contents of the <code>CODE</code> memory space leaked from the FRU <code>63Y0248</code> webcam module.</p>
<p dir="auto">Note: Boot ROM is below the offset <code>0xb000</code>, and it is identical to the Boot ROM on the original X230 webcam module.</p>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain learning differs fundamentally from artificial intelligence systems (129 pts)]]></title>
            <link>https://www.nature.com/articles/s41593-023-01514-1</link>
            <guid>42259185</guid>
            <pubDate>Wed, 27 Nov 2024 20:00:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41593-023-01514-1">https://www.nature.com/articles/s41593-023-01514-1</a>, See on <a href="https://news.ycombinator.com/item?id=42259185">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div id="Sec1-section" data-title="Main"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>The credit assignment problem<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J. &amp; Hinton, G. Backpropagation and the brain. Nat. Rev. Neurosci. 21, 335–346 (2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR1" id="ref-link-section-d46823651e584">1</a></sup> lies at the very heart of learning. Backpropagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. Learning Internal Representations by Error Propagation (Univ. California, San Diego, Institute for Cognitive Science, 1985)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR2" id="ref-link-section-d46823651e588">2</a></sup>, as a simple yet effective credit assignment theory, has powered notable advances in artificial intelligence since its inception<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Bartlett, P. et al.) 1097–1105 (Curran Associates, 2012)." href="#ref-CR3" id="ref-link-section-d46823651e592">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="#ref-CR4" id="ref-link-section-d46823651e592_1">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484–489 (2016)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR5" id="ref-link-section-d46823651e595">5</a></sup> and has also gained a predominant place in understanding learning in the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J. &amp; Hinton, G. Backpropagation and the brain. Nat. Rev. Neurosci. 21, 335–346 (2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR1" id="ref-link-section-d46823651e599">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Richards, B. A. et al. A deep learning framework for neuroscience. Nat. Neurosci. 22, 1761–1770 (2019)." href="#ref-CR6" id="ref-link-section-d46823651e602">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Singer, Y. et al. Sensory cortex is optimized for prediction of future input. eLife 7, e31557 (2018)." href="#ref-CR7" id="ref-link-section-d46823651e602_1">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 8619–8624 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR8" id="ref-link-section-d46823651e605">8</a></sup>. Due to this success, much recent work has focused on understanding how biological neural networks could learn in a way similar to backpropagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Sacramento, J., Costa, R. P., Bengio, Y. and Senn, W. Dendritic cortical microcircuits approximate the backpropagation algorithm. In Advances in Neural Information Processing Systems (NeurIPS) (eds Bengio, S. et al.) 8721–8732 (Curran Associates, 2018)." href="#ref-CR9" id="ref-link-section-d46823651e609">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Guerguiev, J., Lillicrap, T. P. &amp; Richards, B. A. Towards deep learning with segregated dendrites. eLife 6, e22901 (2017)." href="#ref-CR10" id="ref-link-section-d46823651e609_1">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. Front. Comput. Neurosci. 11, 24 (2017)." href="#ref-CR11" id="ref-link-section-d46823651e609_2">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e612">12</a></sup>; although many proposed models do not implement backpropagation exactly, they nevertheless try to approximate backpropagation, and much emphasis is placed on how close this approximation is<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Sacramento, J., Costa, R. P., Bengio, Y. and Senn, W. Dendritic cortical microcircuits approximate the backpropagation algorithm. In Advances in Neural Information Processing Systems (NeurIPS) (eds Bengio, S. et al.) 8721–8732 (Curran Associates, 2018)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR9" id="ref-link-section-d46823651e617">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. Front. Comput. Neurosci. 11, 24 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR11" id="ref-link-section-d46823651e620">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Whittington, J. C. R. &amp; Bogacz, R. Theories of error back-propagation in the brain. Trends Cogn. Sci. 23, 235–250 (2019)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR13" id="ref-link-section-d46823651e623">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e626">14</a></sup>. However, learning in the brain is superior to backpropagation in many critical aspects. For example, compared to the brain, backpropagation requires many more exposures to a stimulus to learn<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Tsividis, P. A., Pouncy, T., Xu, J. L., Tenenbaum, J. B. &amp; Gershman, S. J. Human learning in Atari. In 2017 AAAI Spring Symposium Series 643–646 (Association for the Advancement of Artificial Intelligence, 2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR15" id="ref-link-section-d46823651e630">15</a></sup> and suffers from catastrophic interference of newly and previously stored information<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="McCloskey, M. &amp; Cohen, N. J. Catastrophic interference in connectionist networks: the sequential learning problem. Psychol. Learn. Motiv. 24, 109–165 (1989)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR16" id="ref-link-section-d46823651e634">16</a></sup>. This raises the question of whether using backpropagation to understand learning in the brain should be the main focus of the field.</p><p>Here, we propose that the brain instead solves credit assignment with a fundamentally different principle, which we call ‘prospective configuration’. In prospective configuration, before synaptic weights are modified, neural activity changes across the network so that output neurons better predict the target output; only then are the synaptic weights (hereafter termed ‘weights’) modified to consolidate this change in neural activity. By contrast, in backpropagation, the order is reversed; weight modification takes the lead, and the change in neural activity is the result that follows.</p><p>We identify prospective configuration as a principle that is implicitly followed by a well-established family of neural models with solid biological groundings, namely, energy-based networks. These networks include Hopfield networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl Acad. Sci. USA 79, 2554–2558 (1982)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR17" id="ref-link-section-d46823651e644">17</a></sup> and predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e648">18</a></sup>, which have been successfully used to describe information processing in the cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR19" id="ref-link-section-d46823651e652">19</a></sup>. To support the theory of prospective configuration, we show that it can both yield efficient learning, which humans and animals are capable of, and reproduce data from experiments on human and animal learning. Thus, on the one hand, we demonstrate that prospective configuration performs more efficient and effective learning than backpropagation in various situations faced by biological systems, such as learning with deep structures, online learning, learning with a limited amount of training examples, learning in changing environments, continual learning with multiple tasks and reinforcement learning. On the other hand, we demonstrate that patterns of neural activity and behavior in diverse human and animal learning experiments, including sensorimotor learning, fear conditioning and reinforcement learning, can be naturally explained by prospective configuration but not by backpropagation.</p><p>Guided by the belief that backpropagation is the foundation of biological learning, previous work showed that energy-based networks can closely approximate backpropagation. However, to achieve it, the networks were set up in an unnatural way, such that the neural activity was prevented from substantially changing before weight modification by constraining the supervision signal to be infinitely small (for example, as in equilibrium propagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. Front. Comput. Neurosci. 11, 24 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR11" id="ref-link-section-d46823651e659">11</a></sup> and in previous studies using predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e663">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Millidge, B., Tschantz, A. &amp; Buckley, C. L. Predictive coding approximates backprop along arbitrary computation graphs. Neural Comput. 34, 1329–1368 (2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR20" id="ref-link-section-d46823651e666">20</a></sup>) or last an infinitely short time<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e670">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Bengio, Y. &amp; Fischer, A. Early inference in energy-based models approximates back-propagation. Preprint at 
                  https://doi.org/10.48550/arXiv.1510.02777
                  
                 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR21" id="ref-link-section-d46823651e673">21</a></sup>. By contrast, we reveal that energy-based networks without these unrealistic constraints follow the distinct principle of prospective configuration rather than backpropagation and are superior in both learning efficiency and accounting for data on biological learning.</p><p>Here, we introduce prospective configuration with an intuitive example, show how it originates from energy-based networks and describe its advantages and quantify them in a rich set of biologically relevant learning tasks. We show that prospective configuration naturally explains patterns of neural activity and behavior in diverse learning experiments.</p></div></div><div id="Sec2-section" data-title="Results"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">Prospective configuration: an intuitive example</h3><p>To optimally plan behavior, it is critical for the brain to predict future stimuli, for example, to predict sensations in some modalities on the basis of other modalities<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="O’Reilly, R. C. &amp; Munakata, Y. Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain (MIT Press Cambridge, 2000)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR22" id="ref-link-section-d46823651e693">22</a></sup>. If the observed outcome differs from the prediction, the weights in the whole network need to be updated so that predictions in the ‘output’ neurons are corrected. Backpropagation computes how the weights should be modified to minimize the error on the output, and this weight update results in a change in neural activity when the network next makes the prediction. By contrast, we propose that neural activity is first adjusted to a new configuration so that the output neurons better predict the observed outcome (target pattern); the weights are then modified to reinforce this configuration of neural activity. We call this configuration of neural activity ‘prospective’ because it is the neural activity that the network should produce to correctly predict the observed outcome. In agreement with the proposed mechanism of prospective configuration, it has indeed been widely observed in biological neurons that presenting the outcome of a prediction triggers changes in neural activity; for example, in tasks requiring animals to predict a juice delivery, the reward triggers rapid changes in activity not only in the gustatory cortex but also in multiple cortical regions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Quilodran, R., Rothe, M. &amp; Procyk, E. Behavioral shifts and action valuation in the anterior cingulate cortex. Neuron 57, 314–325 (2008)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR23" id="ref-link-section-d46823651e697">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Wallis, J. D. &amp; Kennerley, S. W. Heterogeneous reward signals in prefrontal cortex. Curr. Opin. Neurobiol. 20, 191–198 (2010)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR24" id="ref-link-section-d46823651e700">24</a></sup>.</p><p>To highlight the difference between backpropagation and prospective configuration, consider a simple example (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1a</a>). Imagine a bear seeing a river. In the bear’s mind, the sight generates predictions of hearing water and smelling salmon. On that day, the bear indeed smelled the salmon but did not hear the water, perhaps due to an ear injury, and thus the bear needs to change its expectation related to the sound. Backpropagation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1b</a>) would proceed by backpropagating the negative error to reduce the weights on the path between the visual and auditory neurons. However, this also entails a reduction of the weights between visual and olfactory neurons that would compromise the expectation of smelling the salmon the next time the river is visited, even though the smell of salmon was present and correctly predicted. These undesired and unrealistic side effects of learning with backpropagation are closely related with the phenomenon of catastrophic interference, where learning a new association destroys previously learned memories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="McCloskey, M. &amp; Cohen, N. J. Catastrophic interference in connectionist networks: the sequential learning problem. Psychol. Learn. Motiv. 24, 109–165 (1989)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR16" id="ref-link-section-d46823651e713">16</a></sup>. This example shows that, with backpropagation, even learning one new aspect of an association may interfere with the memory of other aspects of the same association.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Prospective configuration avoids interference during learning."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Prospective configuration avoids interference during learning.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="297"></picture></a></div><p><b>a</b>, Abstract (top) and concrete (bottom) examples of a task inducing interference during learning. One stimulus input (seeing the water) triggers two prediction outputs (hearing the water and smelling the salmon). One output is correct (smelling the salmon), whereas the other output is an error (not hearing the water). <b>b</b>,<b>c</b>, Backpropagation produces interference during learning; not hearing the water reduces the expectation of smelling the salmon (<b>b</b>), although the salmon was indeed smelled. Prospective configuration, on the other hand, avoids such interference (<b>c</b>). In backpropagation, negative error propagates from the error output to hidden neurons (<b>b</b>; left). This causes a weakening of some connections, which, on the next trial, improves the incorrect output but also reduces the prediction of the correct output, thus introducing interference (<b>b</b>; middle and right). In prospective configuration, neural activity settles into a new configuration (different intensities of purple) before weight modification (<b>c</b>; left). This configuration corresponds to the activity that should be produced after learning, that is, is ‘prospective’. Hence, it foresees the positive error on the correct output and modifies the connections to improve the incorrect output while maintaining the correct output (<b>c</b>; middle and right).</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>By contrast, prospective configuration assumes that learning starts with the neurons being configured to a new state, which corresponds to a pattern enabling the network to correctly predict the observed outcome. The weights are then modified to consolidate this state. This behavior can ‘foresee’ side effects of potential weight modifications and compensate for them dynamically (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1c</a>). To correct the negative error on the incorrect output, the hidden neurons settle to their prospective state of lower activity, and, as a result, a positive error is revealed and allocated to the correct output. Consequently, prospective configuration increases the weights connecting to the correct output, whereas backpropagation does not (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1b,c</a>). Hence, prospective configuration is able to correct the side effects of learning an association effectively and efficiently and with little interference.</p><h3 id="Sec4">Origin of prospective configuration: energy-based networks</h3><p>To show how prospective configuration naturally arises in energy-based networks, we introduce a physical machine analog, which provides an intuitive understanding of energy-based networks and how they produce the mechanism of prospective configuration.</p><p>Energy-based networks have been widely and successfully used in describing biological neural systems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl Acad. Sci. USA 79, 2554–2558 (1982)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR17" id="ref-link-section-d46823651e784">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Friston, K. A theory of cortical responses. Philos. Trans. R. Soc. Lond. B Biol. Sci. 360, 815–836 (2005)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR25" id="ref-link-section-d46823651e787">25</a></sup>. In these models, a neural circuit is described by a dynamical system driven by reducing an abstract ‘energy’, for example, reflecting errors made by neurons (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec10">Methods</a>). Neural activity and weights change to reduce this energy; hence, they can be considered ‘movable parts’ of the dynamical system. We show that energy-based networks are mathematically equivalent to a physical machine (we call it ‘energy machine’), where the energy function has an intuitive interpretation, and its dynamics are straightforward; the energy machine simply adjusts its movable parts to reduce energy.</p><p>The energy machine includes nodes sliding on vertical posts connected with each other via rods and springs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2a,b</a>). Translating from energy-based networks to the energy machine, neural activity maps to the vertical position of a solid node, a connection maps to a rod (blue arrow) pointing from one node to another (where the weight determines how the end position of the rod relates to the initial position), and the energy function maps to the elastic potential energy of springs with nodes attached on both ends (the natural length of the springs is 0). Different energy functions and network structures result in different energy-based networks, corresponding to energy machines with different configurations and combinations of nodes, rods and springs. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2</a>, we present the energy machine of predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e803">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e806">18</a></sup> because they are most accessible and are established to be closely related to backpropagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e810">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e813">14</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="The energy machine reveals a new understanding of energy-based networks, the mechanism of prospective configuration and its theoretical advantages."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: The energy machine reveals a new understanding of energy-based networks, the mechanism of prospective configuration and its theoretical advantages.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="432"></picture></a></div><p>A subset of energy-based networks can be visualized as mechanical machines that perform equivalent computations. Here, we present the energy machine corresponding to predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e829">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e832">18</a></sup>. In the energy machine, the activity of a neuron corresponds to the height of a node (represented by a solid circle) sliding on a post. The input to the neuron is represented by a hollow node on the same post. A synaptic connection corresponds to a rod pointing from a solid node to a hollow node. The weight determines how the input to a postsynaptic neuron depends on the activity of a presynaptic neuron; hence, it influences the angle of the rod. In energy-based networks, relaxation (that is, neural dynamics) and weight modification (that is, weight dynamics) are both driven by minimizing the energy, which corresponds to relaxation of the energy machine by moving the nodes and tuning the rods, respectively. <b>a</b>,<b>b</b>, Predictions (<b>a</b>) and learning (<b>b</b>) in energy-based networks visualized by the energy machine. The pin indicates that neural activity is fixed to the input or target pattern. Here, it is revealed that relaxation infers prospective neural activity, toward which the weights are then modified, a mechanism that we call prospective configuration. <b>c</b>, Physical implementation (top) and connectivity of a predictive coding network<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e852">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e855">18</a></sup> (bottom), which has dynamics mathematically equivalent to those of the energy machine in the middle (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec10">Methods</a> for details). <b>d</b>, The learning problem in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a> visualized by the energy machine, which learns to improve the incorrect output while not interfering with the correct output, thanks to the mechanism of prospective configuration.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>The dynamics of energy-based networks, which are driven by minimizing the energy function, map to relaxation of the energy machine, which is driven by reducing the total elastic potential energy on the springs. A prediction with energy-based networks involves clamping the input neurons to the provided stimulus and updating the activity of the other neurons, which corresponds to fixing one side of the energy machine and letting the energy machine relax by moving nodes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2a</a>). Learning with energy-based networks involves clamping the input and output neurons to the corresponding stimulus, first letting the activities of the remaining neurons converge and then updating weights, which corresponds to fixing both sides of the energy machine and letting the energy machine relax first by moving nodes and then tuning rods (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2b</a>).</p><p>The energy machine reveals the essence of energy-based networks; relaxation before weight modification lets the network settle to a new configuration of neural activity corresponding to the neural activity that would have occurred after the error was corrected by the modification of weights, that is, prospective activity (thus, we call this mechanism prospective configuration). For example, the second-layer ‘neuron’ in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2b</a> increases its activity, and this increase in activity would also be caused by the subsequent weight modification (of the connection between the first and second neurons). In simple terms, relaxation in energy-based networks infers the prospective neural activity after learning, toward which the weights are then modified. This distinguishes it from backpropagation, where weight modification takes the lead, and the change in neural activity is the result that follows.</p><p>The bottom of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2c</a> shows the connectivity of a predictive coding network<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e898">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e901">18</a></sup>, which has dynamics mathematically equivalent to those of the energy machine shown above it. Predictive coding networks include neurons (blue) corresponding to nodes on the posts and separate neurons encoding prediction errors (red) corresponding to springs. For details, see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec10">Methods</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">1</a>, where we list equations describing predictive coding networks and show how they map on the neural implementation and the proposed energy machine.</p><p>Using the energy machine, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2d</a> simulates the learning problem from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a>. Here, we can see that prospective configuration indeed foresees the result of learning and its side effects through relaxation. Hence, it corrects the side effects within one iteration, which would otherwise take multiple iterations for backpropagation.</p><h3 id="Sec5">Advantages of prospective configuration: reduced interference and faster learning</h3><p>Here, we quantify interference in the above scenario and demonstrate how reduced interference translates into an advantage in performance. In all simulations in the main text, prospective configuration is implemented in predictive coding networks (other energy-based models are considered in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, Section 2.1). We also compare the performance of predictive coding networks against artificial neural networks (ANNs) trained with backpropagation because they are closely related, which makes the comparisons fair. In particular, although predictive coding networks include recurrent connections, they generate the same prediction for a given input (when inputs are constrained but outputs are not; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2a</a>) as standard feedforward ANNs if their weights are set to corresponding values<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e934">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e937">14</a></sup>. Therefore, loss is the same function of weights in both models, so direct minimization of loss with gradient descent in predictive coding networks (which is not their natural way of training) would produce the same weight changes as backpropagation in ANNs. Hence, comparing predictive coding networks and backpropagation enables isolation of the effects of the learning algorithm (prospective configuration versus direct minimization of loss as in backpropagation).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3a</a>, we compare the activity of output neurons in the example in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a> between backpropagation and prospective configuration. Initially both output neurons are active (top right), and the output should change toward a target in which one of the neurons is inactive (red vector). Learning with prospective configuration results in changes on the output (purple solid vector) that are aligned better with the target than those for backpropagation (purple dotted vector).</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Learning with prospective configuration changes the activity of output neurons in a direction more aligned toward the target."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Learning with prospective configuration changes the activity of output neurons in a direction more aligned toward the target.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="308"></picture></a></div><div data-test="bottom-caption" id="figure-3-desc"><p><b>a</b>, Simulation of the network from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a> showing changes in the correct and incorrect output neurons during training (‘Iteration’) trained with both learning rules. Here, learning with prospective configuration (purple solid vector) aligns better with the target (red vector) than learning with backpropagation (purple dashed vector). <b>b</b>, Interference can be quantified by ‘target alignment’, the cosine similarity of the direction of the target (red vector) and the direction of learning (purple vector). <b>c</b>, Higher target alignment indicates less interference and vice versa. <b>d</b>, The same experiment as in <b>a</b> repeated with a learning rate ranging from 0.005 to 0.5 represented by the size of the markers, where it is shown that the choice of learning rate changes the trajectories for both methods slightly, but the conclusion holds irrespective of the learning rate. <b>e</b>, Target alignment of randomly generated networks trained with both learning rules as a function of depth of the network. Each symbol shows target alignment resulting from training on a single randomly generated pattern. <b>f</b>, Test error during training on the FashionMNIST<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR60" id="ref-link-section-d46823651e986">60</a></sup> dataset containing images of clothing belonging to different categories for both learning rules with a deep neural network of 15 layers. Here, ‘test error’ refers to the ratio of incorrectly classified samples among all samples in the test set. <b>g</b>, Mean of the test error over training epochs (reflecting how fast test error drops) as a function of learning rate. Results in <b>f</b> and <b>h</b> are for the learning rates giving the minima of the corresponding curves in <b>g</b>. <b>h</b>, Mean of test error of other network depths. Each point is from a learning rate independently optimized for each learning rule in the corresponding setup of network depth. In <b>e</b>–<b>h</b>, prospective configuration demonstrates a notable advantage as the structure gets deeper. Each experiment in <b>f</b>–<b>h</b> was repeated with <i>n</i> = 3 random seeds. Error bars and bands represent the 68% confidence interval.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM3">Source data</a></p></div></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Following the first weight update, we simulate multiple iterations until the network is able to correctly predict the target. Here, ‘iteration’ refers to each time the agent is presented with stimuli and conducts one weight update because of the stimulus. Although the output from backpropagation can reach the target after multiple iterations, the output for the ‘correct neuron’ diverges from the target during learning and then comes back; this is a particularly undesired effect in biological learning, where networks can be ‘tested’ at any point during the learning process, because it may lead to incorrect decisions affecting chances for survival. By contrast, prospective configuration substantially reduces this effect.</p><p>Although backpropagation modifies weights to directly reduce cost in the space of weights (that is, performs gradient descent), surprisingly, and rather subversively, it does not push the resulting output activity directly toward the target. To illustrate this, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3a</a> visualizes the cost with contour lines. Changing the activity of output neurons according to the gradient of the cost would correspond to a change orthogonal to the contour lines, that is, that indicated by the red arrow. However, backpropagation changes the output in a different direction shown by a dashed arrow. Optimizing the weights independently, without considering the effect of updating other weights, leads to output activity not updating toward the target directly due to different weight updates to different layers interfering with each other. By contrast, prospective configuration considers the results of updating other weights by finding a desired configuration of neural activity first. Such a mechanism is missing in backpropagation but is natural in energy-based networks. Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">2</a> shows a direct comparison of how these two models evolve in weight and output spaces during learning.</p><p>Interference can be quantified by the angle between the direction of the target (from current output to target) and learning (from current output to output after learning, both measured without the target provided), and we define ‘target alignment’ as the cosine of this angle (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3b</a>); hence, high interference corresponds to low target alignment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3c</a>).</p><p>It is useful to highlight that target alignment is affected little by the learning rate (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3d</a>), demonstrating that the learning rate has little effect on the direction and trajectory that output neurons take. The difference in target alignment demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3a</a> is also present for deeper and larger (randomly generated) networks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3e</a>). When a network has no hidden layers, the target alignment is equal to 1 (<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, Section 2.4.1). The target alignment drops for backpropagation as the network gets deeper because changes in weights in one layer interfere with changes in other layers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a>), and the backpropagated errors do not lead to appropriate modification of weights in hidden layers (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">2</a>). Because backpropagation modifies the weights in the direction reducing loss, it has positive target alignment for small learning rates but not necessarily close to 1. By contrast, prospective configuration maintains a much higher value along the way. This higher target alignment of prospective configuration can be theoretically explained by the following: (1) there exists a close link between prospective configuration and an algorithm called target propagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Bengio, Y. How auto-encoders could provide credit assignment in deep networks via target propagation. Preprint at 
                  https://doi.org/10.48550/arXiv.1407.7906
                  
                 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR26" id="ref-link-section-d46823651e1078">26</a></sup> (shown in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">3</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, Section 2.2), and (2) under certain conditions, target propagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Bengio, Y. How auto-encoders could provide credit assignment in deep networks via target propagation. Preprint at 
                  https://doi.org/10.48550/arXiv.1407.7906
                  
                 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR26" id="ref-link-section-d46823651e1088">26</a></sup> has a target alignment of 1 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Meulemans, A., Carzaniga, F., Suykens, J., Sacramento, J. &amp; Grewe, B. F. A theoretical framework for target propagation. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochelle, H. et al.) 20024–20036 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR27" id="ref-link-section-d46823651e1092">27</a></sup>; demonstrated in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">4</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, Section 2.4.2). Thus, the link with target propagation provides theoretical insight (with numerical verification) into why prospective configuration has a higher target alignment.</p><p>Higher target alignment directly translates to the efficiency of learning. Test error during training in a visual classification task with a deep neural network of 15 layers decreases faster for prospective configuration than for backpropagation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a>).</p><p>Throughout the data presented here, if learning rate is not presented in a plot, the plot corresponds to the best learning rate optimized independently for each rule under the setup via a grid search. The optimization target is either learning performance or similarity to experimental data (details can be found in the methods for each experiment). Thus, for example, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a> shows the test errors as training progress, with the learning rates optimized independently for each learning rule. The optimization target is the ‘mean of test error’ during training, reflecting how fast the test error decreases during training. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3g</a> plots this mean of test error for different learning rates for both learning rules, and the learning rates giving the minima of the curves were used in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a>. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3h</a> repeats the experiment on networks of other depths and shows the mean of the test error during training as a function of network depth. The mean error is higher for lower depths, as these networks are unable to learn the task, and for greater depths, as it takes longer to train deeper networks. Importantly, the gap between backpropagation and prospective configuration widens for deeper networks, paralleling the difference in target alignment. Efficient training with deeper networks is important for biological neural systems known to be deep, for example, the primate visual cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Felleman, D. J. &amp; Van Essen, D. C. Distributed hierarchical processing in the primate cerebral cortex. Cereb. Cortex 1, 1–47 (1991)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR28" id="ref-link-section-d46823651e1124">28</a></sup>.</p><p>In Section 2.3 of the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, we develop a formal theory of prospective configuration and provide further illustrations and analyses of its advantages. Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">5</a> formally defines prospective configuration and demonstrates that it is indeed commonly observed in different energy-based networks. Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">6</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">7</a> empirically verify and generalize the advantages expected from the theory and show that prospective configuration yields more accurate error allocation and less erratic weight modification, respectively.</p><h3 id="Sec6">Advantages of prospective configuration: effective learning in biologically relevant scenarios</h3><p>Inspired by these advantages, we show empirically that prospective configuration indeed handles various learning problems that biological systems would face better than backpropagation. Because the field of machine learning has developed effective benchmarks for testing learning performance, we use variants of classic machine learning problems that share key features with learning in natural environments. Such problems include online learning, where weights must be updated after each experience (rather than a batch of training examples)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Fontenla-Romero, Ó., Guijarro-Berdiñas, B., Martinez-Rego, D., Pérez-Sánchez, B. &amp; Peteiro-Barral, D. Online machine learning. In Efficiency and Scalability Methods for Computational Intellect (eds Igelnik, B. &amp; Zurada, J. M.) 27–54 (IGI Global, 2013)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR29" id="ref-link-section-d46823651e1151">29</a></sup>, continual learning with multiple tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Hassabis, D., Kumaran, D., Summerfield, C. &amp; Botvinick, M. Neuroscience-inspired artificial intelligence. Neuron 95, 245–258 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR30" id="ref-link-section-d46823651e1155">30</a></sup>, learning in changing environments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M. &amp; Bouchachia, A. A survey on concept drift adaptation. ACM Comput. Surv. 46, 1–37 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR31" id="ref-link-section-d46823651e1159">31</a></sup>, learning with a limited amount of training examples and reinforcement learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR4" id="ref-link-section-d46823651e1163">4</a></sup>. In all aforementioned learning problems, prospective configuration demonstrates a notable superiority over backpropagation.</p><p>First, based on the example in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a>, we expect prospective configuration to require fewer episodes for learning than backpropagation. Before presenting the comparison, we describe how backpropagation is used to train ANNs. Typically, the weights are only modified after a batch of training examples based on the average of updates derived from individual examples (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4a</a>). In fact, backpropagation relies heavily on averaging over multiple experiences to reach human-level performance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Puri, R., Kirby, R., Yakovenko, N. &amp; Catanzaro, B. Large scale language modeling: converging on 40 GB of text in four hours. In 2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD) 290–297 (IEEE, 2018)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR32" id="ref-link-section-d46823651e1176">32</a></sup>, as it needs to stabilize training<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ioffe, S. &amp; Szegedy, C. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning (ICML) (eds Bach, F. &amp; Blei, D.) 448–456 (PMLR, 2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR33" id="ref-link-section-d46823651e1180">33</a></sup>. By contrast, biological systems must update the weights after each experience, and we compare learning performance in such a setting. Sampling efficiency can be quantified by mean of test error during training, which is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4b</a> as a function of batch size (number of experiences that the updates are averaged over). Efficiency strongly depends on batch size for backpropagation because it requires batch training to average out erratic weight updates, whereas this dependence is weaker for prospective configuration, where weight changes are intrinsically less erratic and batch averaging is required less (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">7</a>). Importantly, prospective configuration learns faster with smaller batch sizes, as in biological settings. Additionally, final performance can be quantified by the minimum of the test error, which is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4c</a>, when trained with a batch size equal to 1. Here, prospective configuration also demonstrates a notable advantage over backpropagation.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Prospective configuration achieves a superior performance over backpropagation in various learning situations faced by biological systems."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Prospective configuration achieves a superior performance over backpropagation in various learning situations faced by biological systems.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="451"></picture></a></div><div data-test="bottom-caption" id="figure-4-desc"><p><b>a</b>–<b>k</b>, Learning situations include online learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Fontenla-Romero, Ó., Guijarro-Berdiñas, B., Martinez-Rego, D., Pérez-Sánchez, B. &amp; Peteiro-Barral, D. Online machine learning. In Efficiency and Scalability Methods for Computational Intellect (eds Igelnik, B. &amp; Zurada, J. M.) 27–54 (IGI Global, 2013)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR29" id="ref-link-section-d46823651e1211">29</a></sup> (<b>a</b>–<b>c</b>), continual learning of multiple tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Hassabis, D., Kumaran, D., Summerfield, C. &amp; Botvinick, M. Neuroscience-inspired artificial intelligence. Neuron 95, 245–258 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR30" id="ref-link-section-d46823651e1221">30</a></sup> (<b>d</b>–<b>e</b>), learning in changing environments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M. &amp; Bouchachia, A. A survey on concept drift adaptation. ACM Comput. Surv. 46, 1–37 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR31" id="ref-link-section-d46823651e1232">31</a></sup> (<b>f</b>–<b>g</b>), learning with a limited amount of training examples (<b>h</b>) and reinforcement learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR4" id="ref-link-section-d46823651e1246">4</a></sup> (<b>k</b>). Graphs corresponding to each situation are grouped together with the same background color. Simulations of each situation differ from the ‘default setup’ described in the <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec10">Methods</a> in a single aspect unique to this task. For example, the default setup involves training with minibatches, so the batch size was only set to 1 in <b>a</b>–<b>c</b> for investigating online learning, whereas it was set to a larger default value in rest of the groups. In supervised learning setups, fully connected networks (<b>a</b>–<b>h</b>) were evaluated on the FashionMNIST<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR60" id="ref-link-section-d46823651e1269">60</a></sup> dataset, and convolutional neural networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="O’Shea, K. &amp; Nash, R. An introduction to convolutional neural networks. Preprint at 
                  https://doi.org/10.48550/arXiv.1511.08458
                  
                 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR35" id="ref-link-section-d46823651e1273">35</a></sup> (<b>i</b> and <b>j</b>) were evaluated on the CIFAR-10 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Krizhevsky, A. &amp; Hinton, G. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, Univ. Toronto (2009)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR36" id="ref-link-section-d46823651e1283">36</a></sup>) dataset. In the reinforcement learning setup (<b>k</b>), fully connected networks were evaluated on three classic control problems. If the learning rate was not presented, each point (a setup of an experiment) in the plot corresponds to the best learning rate optimized independently for each rule under that setup. <b>a</b>, Difference in training setup between computers that can average weight modifications for individual examples to get a ‘statistically good’ value and biological systems that must apply one modification before computing another. <b>b</b>, Mean of the test errors during training as a function of batch size. <b>c</b>, Minimum of test error during training as a function of learning rate. <b>d</b>, Test error during continual learning of two tasks. <b>e</b>, Mean of test error of both tasks during training as a function of learning rate. <b>f</b>, Test error during training when learning with concept drifting. <b>g</b>, Mean of test error during training with concept drifting as a function of learning rate. <b>h</b>, Minimum of test error during training with different amounts of training examples (data points per class). <b>i</b>, Minimum of test error during training of a convolutional neural network trained with prospective configuration and backpropagation on the CIFAR-10 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Krizhevsky, A. &amp; Hinton, G. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, Univ. Toronto (2009)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR36" id="ref-link-section-d46823651e1319">36</a></sup>) dataset. <b>j</b>, Structure detail of the convolutional neural network used in <b>i</b>. <b>k</b>, Sum of rewards per episode during training on three classic reinforcement learning tasks (insets). An episode is a period from initialization of environment to reaching a terminate state. Each experiment in <b>a</b>–<b>h</b> was repeated with <i>n</i> = 10 random seeds. Each experiment in <b>i</b>–<b>k</b> was repeated with <i>n</i> = 3 random seeds because these experiments are more expensive. Error bars and bands represent the 68% confidence interval.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM3">Source data</a></p></div></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Second, biological organisms need to sequentially learn multiple tasks, while ANNs show catastrophic forgetting. When trained on a new task, performance on previously learned tasks is largely destroyed<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="McCloskey, M. &amp; Cohen, N. J. Catastrophic interference in connectionist networks: the sequential learning problem. Psychol. Learn. Motiv. 24, 109–165 (1989)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR16" id="ref-link-section-d46823651e1367">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. In Proc. 34th International Conference on Machine Learning (eds Precup, D. &amp; Teh, Y. W.) 3987–3995 (PMLR, 2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR34" id="ref-link-section-d46823651e1370">34</a></sup>. The data in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4d</a> show performance when trained on two tasks alternately (task 1 is classifying five randomly selected classes in the FashionMNIST dataset, and task 2 is classifying the remaining five classes). Prospective configuration outperforms backpropagation both in terms of avoiding forgetting previous tasks and relearning current tasks. The results are summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4e</a>.</p><p>Third, biological systems often need to rapidly adapt to changing environments. A common way to simulate this is ‘concept drifting’<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M. &amp; Bouchachia, A. A survey on concept drift adaptation. ACM Comput. Surv. 46, 1–37 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR31" id="ref-link-section-d46823651e1383">31</a></sup>, where a part of the mapping between the output neurons to the semantic meaning is shuffled regularly, each time a certain number of training iterations has passed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4f</a>). Test error during training with concept drifting is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4f</a>. Before epoch 0, both learning rules are initialized with the same pretrained model (trained with backpropagation); thus, epoch 0 is the first time the model experiences concept drift. The results are summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4g</a> and show that, for this task, there is a particularly large difference in mean error (for optimal learning rates). This large advantage of prospective configuration is related to it being able to optimally detect which weights to modify (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">6</a>) and to preserve existing knowledge while adapting to changes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a>). This ability to maintain important information while updating other information is critical for survival in natural environments that are bound to change, and prospective configuration has a very substantial advantage in this respect.</p><p>Furthermore, biological learning is also characterized by limited data availability. Prospective configuration outperforms backpropagation when the model is trained with fewer examples (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4h</a>).</p><p>To demonstrate that the advantage of prospective configuration also scales up to larger networks and problems, we evaluated convolutional neural networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="O’Shea, K. &amp; Nash, R. An introduction to convolutional neural networks. Preprint at 
                  https://doi.org/10.48550/arXiv.1511.08458
                  
                 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR35" id="ref-link-section-d46823651e1413">35</a></sup> on CIFAR-10 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Krizhevsky, A. &amp; Hinton, G. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, Univ. Toronto (2009)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR36" id="ref-link-section-d46823651e1417">36</a></sup>) trained with both learning rules (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4i</a>), where prospective configuration showed notable advantages over backpropagation. The detailed structure of the convolutional networks is provided in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4j</a>.</p><p>Another key challenge for biological systems is to decide which actions to take. Reinforcement learning theories (for example, <i>Q</i> learning) propose that it is solved by learning the expected reward resulting from different actions in different situations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Sutton, R. S. &amp; Barto, A. G. Introduction to Reinforcement Learning, Vol. 2 (MIT Press Cambridge, 1998)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR37" id="ref-link-section-d46823651e1433">37</a></sup>. Such prediction of rewards can be made by neural networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR4" id="ref-link-section-d46823651e1437">4</a></sup>, which can be trained with prospective configuration or backpropagation. The sum of rewards per episode during training on three classic reinforcement learning tasks is reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4k</a>, where prospective configuration demonstrates a notable advantage over backpropagation. This large advantage may arise because reinforcement learning is particularly sensitive to erratic changes in network weights (as the target output depends on reward predicted by the network itself for a new state; <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec10">Methods</a>).</p><p>Based on the superior learning performance of prospective configuration, we may expect that this learning mechanism has been favored by evolution; thus, in the next sections, we investigate if it can account for neural activity and behavior during learning better than backpropagation.</p><h3 id="Sec7">Evidence for prospective configuration: inferring the latent state during learning</h3><p>Prospective configuration is related to theories proposing that before learning, the brain first infers a latent state of the environment from feedback<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="#ref-CR38" id="ref-link-section-d46823651e1459">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Heald, J. B., Lengyel, M. &amp; Wolpert, D. M. Contextual inference underlies the learning of sensorimotor repertoires. Nature 600, 489–493 (2021)." href="#ref-CR39" id="ref-link-section-d46823651e1459_1">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Larsen, T., Leslie, D. S., Collins, E. J. &amp; Bogacz, R. Posterior weighted reinforcement learning with state uncertainty. Neural Comput. 22, 1149–1179 (2010)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR40" id="ref-link-section-d46823651e1462">40</a></sup>. Here, we propose that this inference can be achieved in neural circuits through prospective configuration, where, following feedback, neurons in ‘hidden layers’ converge to a prospective pattern of activity that encodes this latent state. We demonstrate that data from various previous studies, which involved the inference of a latent state, can be explained by prospective configuration. These data were previously explained by complex and abstract mechanisms, such as Bayesian models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e1466">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Heald, J. B., Lengyel, M. &amp; Wolpert, D. M. Contextual inference underlies the learning of sensorimotor repertoires. Nature 600, 489–493 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR39" id="ref-link-section-d46823651e1469">39</a></sup>, whereas here, we mechanistically show with prospective configuration how such inference can be performed by minimal networks encoding only the essential elements of the tasks.</p><p>The dynamical inference of a latent state from feedback has been recently proposed to take place during sensorimotor learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Heald, J. B., Lengyel, M. &amp; Wolpert, D. M. Contextual inference underlies the learning of sensorimotor repertoires. Nature 600, 489–493 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR39" id="ref-link-section-d46823651e1476">39</a></sup>. In this experiment, participants received different motor perturbations in different contexts and learned to compensate for these perturbations. Behavioral data suggest that, after receiving feedback, participants first used the feedback to infer context and then adapted the force for the inferred context. We demonstrate that prospective configuration is able to reproduce these behavioral data, whereas backpropagation cannot.</p><p>Specifically, in the task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5a</a>), participants were asked to move a stick from a starting point to a target point while experiencing perturbations. The participants experienced a sequence of blocks of trials (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5c–e</a>), including training, washout and testing. During the training session, different directions of perturbations, positive (+) or negative (–), were applied in different contexts, blue (B) or red (R) backgrounds, respectively. We denote these trials as B+ and R–. These trials may be associated with latent states, which we denote [B] and [R]; for example, the latent state [B] may be associated with both background B and perturbation +. The next stage of the task was designed to investigate if the latent state [B] can be activated by perturbation + even if no background B is shown. Thus, participants experienced different trials including R+ (that is, perturbation + but no background B). Specifically, after a washout session (during which no perturbation was provided), in the testing session, participants experienced one of the four possible test trials: B+, R+, B– and R–. To evaluate learning on the test trials, motor adaptation (that is, the difference between the final and target stick positions) was measured before and after the test trial in two trials with the blue background (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5e</a>). Change in the adaptation between these two trials is a reflection of learning about blue context that occurred at the test trial. If participants only associated feedback with the background color (B), then the change in adaptation would only occur with test trials B+ and B–. However, experimental data (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5f</a>) show that there was also substantial adaptation change with R+ trials (which was even bigger than with B– trials).</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Prospective configuration explains contextual inference in human sensorimotor learning."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Prospective configuration explains contextual inference in human sensorimotor learning.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="297"></picture></a></div><div data-test="bottom-caption" id="figure-5-desc"><p><b>a</b>, Structure of an experimental trial where participants were asked to move a stick from the starting point to the target point while experiencing perturbations. <b>b</b>, The minimal network for the task, including six connections encoding the associations from the backgrounds (B and R) to the belief of contexts ([B] and [R]) and from the belief of contexts to the prediction of perturbations (+ and –). <b>c</b>–<b>e</b>, Sequence of sessions the participants experienced, including training (<b>c</b>), washout (<b>d</b>) and testing (<b>e</b>). Darker gray boxes show the expected network after the session, where thickness represents the strength of connections. In the testing session, the darker box explains how the two learning rules learn differently on the R+ trial, leading to the differences in <b>f</b>. <b>f</b>, Predictions of the two learning rules compared to behavioral data measured from human participants, where prospective configuration reproduces the key patterns of data, but backpropagation does not. Each experiment was repeated with <i>n</i> = 24 random seeds, as there were 24 participants in the behavioral experiment.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM3">Source data</a></p></div></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>To model learning in this task, we considered a neural network (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5b</a>) where input nodes encode the background color, and outputs encode movement compensations in the two directions. Importantly, this network also includes hidden neurons encoding belief of being in the contexts associated with the two backgrounds ([B] and [R]). Trained with the exact procedure of the experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Heald, J. B., Lengyel, M. &amp; Wolpert, D. M. Contextual inference underlies the learning of sensorimotor repertoires. Nature 600, 489–493 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR39" id="ref-link-section-d46823651e1555">39</a></sup> from randomly initialized weights, prospective configuration with this minimal network can reproduce the behavioral data, whereas backpropagation cannot (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5f</a>).</p><p>Prospective configuration can produce change in adaptation with the R+ test trial because after + feedback, it is able to also activate context [B] that was associated with this feedback during training and then learn compensation for this latent state. To shed light on how this inference takes place in the model, schematics in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5c,d</a> show evolution of the weights of the network over sessions (thickness represents the strength of connections). The schematic in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5e</a> shows the difference between the two learning rules after exposure to R+; although B is not perceived, prospective configuration infers a moderate excitation of the belief of blue context [B] because the positive connection from [B] to + was built during the training session. The activity of [B] enables the learning of weights from [B] to + and –, while backpropagation does not modify any weights originating from [B].</p><p>For simplicity of explanation, we presented simulations with minimal networks; however, Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">8</a> shows that networks with a general fully connected structure and more hidden neurons can replicate the above data when using prospective configuration but not when using backpropagation.</p><p>Studies of animal conditioning have also observed that feedback in learning tasks involving multiple stimuli may trigger learning about non-presented stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Kaufman, M. A. &amp; Bolles, R. C. A nonassociative aspect of overshadowing. Bull. Psychonomic Soc. 18, 318–320 (1981)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR41" id="ref-link-section-d46823651e1581">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Matzel, L. D., Schachtman, T. R. &amp; Miller, R. R. Recovery of an overshadowed association achieved by extinction of the overshadowing stimulus. Learn. Motiv. 16, 398–412 (1985)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR42" id="ref-link-section-d46823651e1584">42</a></sup>. One example is provided in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">9</a>, where we show that it can be explained by prospective configuration but not by backpropagation.</p><h3 id="Sec8">Evidence for prospective configuration: discovering task structure during learning</h3><p>Prospective configuration is also able to discover the underlying task structure in reinforcement learning. Specifically, we consider a task where reward probabilities of different options were not independent<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e1599">38</a></sup>. In this study, humans were choosing between two options where the reward probabilities were constrained such that one option had a higher reward probability than the other (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6a</a>). Occasionally the reward probabilities were swapped, so if one probability was increased, the other was decreased by the same amount. Remarkably, the recorded functional magnetic resonance imaging (fMRI) data suggested that participants learned that the values of the two options were negatively correlated and on each trial updated the value estimates of both options in opposite ways. This conclusion was drawn from analysis of the signal from the medial prefrontal cortex (mPFC), which encoded the expected value of reward. The data presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6c</a> compare this signal after making a choice on two consecutive trials: a trial in which the reward was not received (‘punish trial’) and the next trial. If the participant selected the same option on both trials (‘stay’), the signal decreased, indicating that the reward expected by the participant was reduced. Remarkably, if the participant selected the other option on the next trial (‘switch’), the signal increased, suggesting that negative feedback for one option increased the value estimate for the other. Such learning is not predicted by standard reinforcement learning models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e1609">38</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Prospective configuration can discover the underlying task structure during reinforcement learning."><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Fig. 6: Prospective configuration can discover the underlying task structure during reinforcement learning.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="110"></picture></a></div><div data-test="bottom-caption" id="figure-6-desc"><p><b>a</b>, Reinforcement learning task. Human participants were required to choose between two options, leading to either reward (gaining coins) or punishment (losing coins) with different probabilities. The probability of reward was occasionally reversed between the two options. <b>b</b>, The minimal network encoding the essential elements of the task. <b>c</b>, Activity of the output neuron corresponding to the selected option from networks trained with prospective configuration and backpropagation compared with fMRI data measured in human participants (that is, peak blood oxygenation level-dependent (%BOLD) signal in the mPFC). Prospective configuration reproduces the key finding that the expected value (encoded in %BOLD signal in the mPFC) increases if the next choice after a punishing trial is to switch to the other option. The number of trials is not mentioned in the original paper, so we simulated for <i>n</i> = 128 trials for both learning rules. Error bars represent the 68% confidence interval.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM3">Source data</a></p></div></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-023-01514-1/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>This task can be conceptualized as having a latent state encoding which option is superior, and this latent state determines the reward probabilities for both options. Consequently, we consider a neural network reflecting this structure (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6b</a>) that includes an input neuron encoding being in the task (equal to 1 in simulations), a hidden neuron encoding the latent state and two output neurons encoding the reward probabilities for the two options. Trained with the exact procedure of the experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e1654">38</a></sup> from randomly initialized weights, prospective configuration with this minimal network can reproduce the data, whereas backpropagation cannot (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6c</a>). In Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">10</a>, we show that prospective configuration reproduces these data because it can infer the rewarded choice by updating the activity of the hidden neuron based on feedback.</p><p>Taken together, the presented simulations illustrate that prospective configuration is a common principle that can explain a range of surprising learning effects in diverse tasks.</p></div></div><div id="Sec9-section" data-title="Discussion"><h2 id="Sec9">Discussion</h2><div id="Sec9-content"><p>Our paper identifies the principle of prospective configuration, according to which learning relies on neurons first optimizing their pattern of activity to match the correct output and then reinforcing these prospective activities through synaptic plasticity. Although it was known that in energy-based networks the activity of neurons shifts before weight update, it has been previously thought that this shift is a necessary cost of error propagation in biological networks, and several methods have been proposed to suppress it<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. Front. Comput. Neurosci. 11, 24 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR11" id="ref-link-section-d46823651e1676">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e1679">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e1682">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Millidge, B., Tschantz, A. &amp; Buckley, C. L. Predictive coding approximates backprop along arbitrary computation graphs. Neural Comput. 34, 1329–1368 (2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR20" id="ref-link-section-d46823651e1685">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Bengio, Y. &amp; Fischer, A. Early inference in energy-based models approximates back-propagation. Preprint at 
                  https://doi.org/10.48550/arXiv.1510.02777
                  
                 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR21" id="ref-link-section-d46823651e1688">21</a></sup> to approximate backpropagation more closely. By contrast, we demonstrate that this reconfiguration of neural activity is the key to achieving learning performance superior to that of backpropagation and to explaining experimental data from diverse learning tasks. Prospective configuration further offers a range of experimental predictions distinct from those of backpropagation (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">11</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">12</a>). Together, we have demonstrated that prospective configuration enables more efficient learning than backpropagation by reducing interference, demonstrates superior performance in situations faced by biological organisms, requires only local computation and plasticity and matches experimental data across a wide range of tasks.</p><p>Our theory addresses a long-standing question of how the brain solves the plasticity-stability dilemma, for example, how it is possible that, despite adjustment of representation in the primary visual cortex during learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Poort, J. et al. Learning enhances sensory and multiple non-sensory representations in primary visual cortex. Neuron 86, 1478–1490 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR43" id="ref-link-section-d46823651e1701">43</a></sup>, we can still understand the meaning of visual stimuli we learned over our lifetime. According to prospective configuration, when some weights are modified, compensatory changes are made to other weights to ensure the stability of correctly predicted outputs. Thus, prospective configuration reduces interference between different weight modifications while learning a single association. Previous computational models have proposed mechanisms that reduce interference between new and previously acquired information while learning multiple associations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. In Proc. 34th International Conference on Machine Learning (eds Precup, D. &amp; Teh, Y. W.) 3987–3995 (PMLR, 2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR34" id="ref-link-section-d46823651e1705">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="McClelland, J. L., McNaughton, B. L. &amp; O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR44" id="ref-link-section-d46823651e1708">44</a></sup>. It is highly likely that such mechanisms and prospective configuration operate in the brain in parallel to minimize both types of interference.</p><p>Prospective configuration is related to inference and learning procedures in statistical modeling. If the ‘energy’ in energy-based schemes is variational free energy, prospective configuration can be seen as an implementation of variational Bayes that subsumes inference and learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Dauwels, J. On variational message passing on factor graphs. In 2007 IEEE International Symposium on Information Theory, 2546–2550 (IEEE, 2007)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR45" id="ref-link-section-d46823651e1715">45</a></sup>. For example, dynamic expectation maximization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Anil Meera, A. &amp; Wisse, M. Dynamic expectation maximization algorithm for estimation of linear systems with colored noise. Entropy 23, 1306 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR46" id="ref-link-section-d46823651e1719">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Friston, K. Hierarchical models in the brain. PLoS Comput. Biol. 4, e1000211 (2008)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR47" id="ref-link-section-d46823651e1722">47</a></sup> can be regarded as a generalization of predictive coding networks in which the D-step optimizes representations of latent states (analogously to relaxation until convergence during inference) while the E-step optimizes model parameters (analogously to weight modification during learning).</p><p>Other recent work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Meulemans, A., Farinha, M. T., Cervera, M. R., Sacramento, J. &amp; Grewe, B. F. Minimizing control for credit assignment with strong feedback. In Proc. of Machine Learning Research (eds Chaudhuri, K. et al.) 15458–15483 (PMLR, 2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR48" id="ref-link-section-d46823651e1729">48</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Meulemans, A., Zucchet, N., Kobayashi, S., von Oswald, J. &amp; Sacramento, J. The least-control principle for learning at equilibrium. Adv. Neural Inf. Process. Syst. 35, 33603–33617 (2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR49" id="ref-link-section-d46823651e1732">49</a></sup> also noticed that the natural form of energy-based networks (‘strong control’ in their words) performs different learning than backpropagation. Their analysis concentrates on an architecture of deep feedback control, and they demonstrated that a particular form of their model is equivalent to predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Meulemans, A., Zucchet, N., Kobayashi, S., von Oswald, J. &amp; Sacramento, J. The least-control principle for learning at equilibrium. Adv. Neural Inf. Process. Syst. 35, 33603–33617 (2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR49" id="ref-link-section-d46823651e1736">49</a></sup>. The unique contribution of our paper is to show the benefits of such strong control and explain why they arise. The principle of prospective configuration is also present in other recent models. For example, Gilra and Gerstner<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Gilra, A. &amp; Gerstner, W. Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. eLife 6, e28295 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR50" id="ref-link-section-d46823651e1740">50</a></sup> developed a spiking model in which feedback about the error on the output directly affects the activity of hidden neurons before plasticity takes place. Haider et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Haider, P. et al. Latent equilibrium: a unified learning theory for arbitrarily fast computation with arbitrarily slow neurons. In Advances in Neural Information Processing Systems (NeurIPS) (eds Ranzato, M. et al.) 17839–17851 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR51" id="ref-link-section-d46823651e1744">51</a></sup> developed a faster inference algorithm for energy-based models that computes a value to which the activity is likely to converge, termed latent equilibrium<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Haider, P. et al. Latent equilibrium: a unified learning theory for arbitrarily fast computation with arbitrarily slow neurons. In Advances in Neural Information Processing Systems (NeurIPS) (eds Ranzato, M. et al.) 17839–17851 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR51" id="ref-link-section-d46823651e1748">51</a></sup>. Iteratively setting each neuron’s output based on its latent equilibrium leads to much faster inference<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Haider, P. et al. Latent equilibrium: a unified learning theory for arbitrarily fast computation with arbitrarily slow neurons. In Advances in Neural Information Processing Systems (NeurIPS) (eds Ranzato, M. et al.) 17839–17851 (2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR51" id="ref-link-section-d46823651e1753">51</a></sup> and enables efficient computation of the prospective configuration.</p><p>Predictive coding networks require symmetric forward and backward weights between layers of neurons, so a question arises concerning how such symmetry may develop in the brain. If predictive coding networks are initialized with symmetric weights (as in our simulations), the symmetry will persist because the changes in weight between neurons A and B are the same as those for feedback weight (between neurons B and A). Even if the weights are not initialized symmetrically, the symmetry may develop if synaptic decay is included in the model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T. &amp; Tweed, D. B. Deep learning without weight transport. In Advances in Neural Information Processing Systems (NeurIPS) (eds Wallach, H. et al.) (Curran Associates, 2019)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR52" id="ref-link-section-d46823651e1761">52</a></sup> because then the initial asymmetric values decay away, and weight values become more influenced by recent changes that are symmetric. Nevertheless, weight symmetry is not generally required for effective credit assignment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. Nat. Commun. 7, 13276 (2016)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR53" id="ref-link-section-d46823651e1765">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Millidge, B., Tschantz, A. &amp; Buckley, C. L. Relaxing the constraints on predictive coding models. Preprint at 
                  https://doi.org/10.48550/arXiv.2010.01047
                  
                 (2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR54" id="ref-link-section-d46823651e1768">54</a></sup>.</p><p>Here, we assumed for simplicity that the convergence of neural activity to an equilibrium happens rapidly after the stimuli are provided so that the synaptic weight modification after convergence may take place while the stimuli are still present. Nevertheless, predictive coding networks can still work even if weight modification takes place while the neural activity is converging. Specifically, Song et al. demonstrated that if neural activities are only updated for the first few steps, the update of the weights is equivalent to that in backpropagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e1775">14</a></sup>. As a reminder, we demonstrate here that if the neural activities are updated to equilibrium, the update of the weights follows the principle of prospective configuration and possesses the desirable demonstrated properties. Thus, a learning rule where neural activities and weights are updated in parallel will experience a weight update that is equivalent to backpropagation at the start and then move to prospective configuration as the system converges to equilibrium<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Salvatori, T. et al. Incremental predictive coding: a parallel and fully automatic learning algorithm. Preprint at 
                  https://doi.org/10.48550/arXiv.2212.00720
                  
                 (2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR55" id="ref-link-section-d46823651e1779">55</a></sup>. Furthermore, predictive coding networks have been extended to describe recurrent structures<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Friston, K. J., Trujillo-Barreto, N. &amp; Daunizeau, J. Dem: a variational treatment of dynamic systems. NeuroImage 41, 849–885 (2008)." href="#ref-CR56" id="ref-link-section-d46823651e1783">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Millidge, B., Tang, M., Osanlouy, M. &amp; Bogacz, R. Predictive coding networks for temporal prediction. Preprint at bioRxiv 
                  https://doi.org/10.1101/2023.05.15.540906
                  
                 (2023)." href="#ref-CR57" id="ref-link-section-d46823651e1783_1">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Salvatori, T. et al. Learning on arbitrary graph topologies via predictive coding. In Advances in Neural Information Processing Systems (NeurIPS) (eds Koyejo, S. et al.) 38232–38244 (Curran Associates, 2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR58" id="ref-link-section-d46823651e1786">58</a></sup>, and it has been shown that such networks can learn to predict dynamically changing stimuli even if weights are modified before the activity converged for a given ‘frame’ of the stimulus<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Millidge, B., Tang, M., Osanlouy, M. &amp; Bogacz, R. Predictive coding networks for temporal prediction. Preprint at bioRxiv 
                  https://doi.org/10.1101/2023.05.15.540906
                  
                 (2023)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR57" id="ref-link-section-d46823651e1790">57</a></sup>.</p><p>The advantages of prospective configuration suggest that it may be profitably applied in machine learning to improve the efficiency and performance of deep neural networks. An obstacle for this is that the relaxation phase is computationally expensive. However, recent work demonstrated that by modifying weights after each step of relaxation, the model becomes comparably fast to backpropagation and easier for parallelization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Salvatori, T. et al. Incremental predictive coding: a parallel and fully automatic learning algorithm. Preprint at 
                  https://doi.org/10.48550/arXiv.2212.00720
                  
                 (2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR55" id="ref-link-section-d46823651e1797">55</a></sup>.</p><p>Most intriguingly, it has been demonstrated that the speed of energy-based networks can be greatly increased by implementing the relaxation on analog hardware<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Foroushani, A. N., Assaf, H., Noshahr, F. H., Savaria, Y. &amp; Sawan, M. Analog circuits to accelerate the relaxation process in the equilibrium propagation algorithm. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS) 1–5 (IEEE, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR59" id="ref-link-section-d46823651e1804">59</a></sup>, potentially resulting in energy-based networks being faster than backpropagation. Therefore, we anticipate that our discoveries may change the blueprint of next-generation machine learning hardware, switching from the current digital tensor base to analog hardware and being closer to the brain and potentially far more efficient.</p></div></div><div id="Sec10-section" data-title="Methods"><h2 id="Sec10">Methods</h2><div id="Sec10-content"><p>This section provides the necessary details for replication of the results described in the main text.</p><h3 id="Sec11">Models</h3><p>Throughout this work, we compare the established theory of backpropagation to the proposed new principle of prospective configuration. As explained in the main text, backpropagation is used to train ANNs, where the activity of a neuron is fixed to a value based on its input, whereas prospective configuration occurs in energy-based networks, where the activity of a neuron is not fixed.</p><p>Because in ANNs the activity of neurons <b><i>x</i></b> is determined by their input, the output of the network can be obtained by propagating the inputs ‘forward’ through the computational graph. The output can then be compared to a target pattern to get a measure of difference known as a loss. Because the value of a node (activity of a neuron) in the computational graph is explicitly computed as a function of its input, the computational graph is usually differentiable. Thus, training ANNs with backpropagation modifies the weights <b><i>w</i></b> to take a step toward the negative gradient of loss <span>\({{{\mathcal{L}}}}\)</span>,</p><div id="Equ1"><p><span>$${{\Delta }}{{{\boldsymbol{w}}}}=-\alpha \frac{\partial {{{\mathcal{L}}}}}{\partial {{{\boldsymbol{w}}}}},$$</span></p><p>
                    (1)
                </p></div><p>during which the activities of neurons <b><i>x</i></b> are fixed, and <i>α</i> is the learning rate. The weights <b><i>w</i></b> requiring modification might be many steps away from the output on the computational graph, where the loss <span>\({{{\mathcal{L}}}}\)</span> is computed; thus, <span>\(\frac{\partial {{{\mathcal{L}}}}}{\partial {{{\boldsymbol{w}}}}}\)</span> is often obtained by applying the chain rule of computing a derivative through intermediate variables (activity of output and hidden neurons). For example, consider a network with four layers, and let <b><i>x</i></b><sup><i>l</i></sup> denote the activity of neurons in layer <i>l</i> and <b><i>w</i></b><sup><i>l</i></sup> denote the weights of connections between layers <i>l</i> and <i>l</i> + 1. The change in weights originating from the first layer is then computed: <span>\(\frac{\partial {{{\mathcal{L}}}}}{\partial {{{{\boldsymbol{w}}}}}^{1}}=\frac{\partial {{{\mathcal{L}}}}}{\partial {{{{\boldsymbol{x}}}}}^{4}}\cdot \frac{\partial {{{{\boldsymbol{x}}}}}^{4}}{\partial {{{{\boldsymbol{x}}}}}^{3}}\ldots \frac{\partial {{{{\boldsymbol{x}}}}}^{2}}{\partial {{{{\boldsymbol{w}}}}}^{1}}\)</span>. This enables the loss to be backpropagated through the graph to provide a direction of update for all weights.</p><p>In contrast to ANNs, in energy-based networks, the activity of neurons <b><i>x</i></b> is not fixed to the input from a previous layer. Instead, an energy function <i>E</i> is defined as a function of the neural activity <b><i>x</i></b> and weights <b><i>w</i></b>. For networks organized in layers (considered in this paper), the energy can be decomposed into a sum of local energy terms <i>E</i><sup><i>l</i></sup>,</p><div id="Equ2"><p><span>$$E=\mathop{\sum}\limits_{l}{E}^{l}\left({{{{\boldsymbol{x}}}}}^{l},{{{{\boldsymbol{w}}}}}^{l-1},{{{{\boldsymbol{x}}}}}^{l-1}\right).$$</span></p><p>
                    (2)
                </p></div><p>Here, <i>E</i><sup><i>l</i></sup> is called local energy because it is a function of <b><i>x</i></b><sup><i>l</i></sup>, <b><i>x</i></b><sup><i>l</i> − 1</sup> and <b><i>w</i></b><sup><i>l</i> − 1</sup>, which are neighbors and connected to each other. This ensures that the optimization of energy <i>E</i> can be implemented by local circuits because the derivative of <i>E</i> with respect to any neural activity (or weights) results in an equation containing only the local activity (or weights) and the activity of adjacent neurons. Predictions with energy-based networks are computed by clamping the input neurons to an input pattern and then modifying the activity of all other neurons to decrease the energy:</p><div id="Equ3"><p><span>$${{\Delta }}{{{\boldsymbol{x}}}}=-\gamma \frac{\partial E}{\partial {{{\boldsymbol{x}}}}},$$</span></p><p>
                    (3)
                </p></div><p>where <i>γ</i> is the integration step of the neural dynamics. Because the terms in <i>E</i> can be divided into local energy terms, this results in an equation that can be implemented with local circuits. This process of modifying neural activity to decrease the energy is called relaxation, and we refer to the equation describing relaxation as neural dynamics because it describes the dynamics of the neural activity in energy-based networks. After convergence of relaxation, the activities of the output neurons are taken as the prediction made by the energy-based network. Different energy-based networks are trained in slightly different ways. For predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e2383">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e2386">18</a></sup>, training involves clamping the input and output neurons to input and target patterns, respectively. Then, relaxation is run until convergence (<span>\({{{\boldsymbol{x}}}}=\mathop{{{{\boldsymbol{x}}}}}\limits^{* }\)</span>), after which the weights are updated using the activity at convergence to further decrease the energy:</p><div id="Equ4"><p><span>$${\Delta }{\boldsymbol{w}}=-\alpha \frac{\partial E}{\partial {\boldsymbol{w}}}{\vert }_{{\boldsymbol{x}} = \mathop{\boldsymbol{x}}\limits^{*}}.$$</span></p><p>
                    (4)
                </p></div><p>This will also result in an equation that can be implemented with local plasticity because it is just a gradient descent on the local energy. We refer to such an equation as weight dynamics, because it describes the dynamics of the weights in energy-based networks.</p><p>Backpropagation and prospective configuration are not restricted to specific models. Depending on the structure of the network and the choice of the energy function, one can define different models that implement the principle of backpropagation or prospective configuration. In the main text and most of the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, we investigate the most standard layered network. In this case, both ANNs and energy-based networks include <i>L</i> layers of weights <b><i>w</i></b><sup>1</sup>, <b><i>w</i></b><sup>2</sup>, …, <b><i>w</i></b><sup><i>L</i></sup> and <i>L</i> + 1 layers of neurons <b><i>x</i></b><sup>1</sup>, <b><i>x</i></b><sup>2</sup>, …, <b><i>x</i></b><sup><i>L</i> + 1</sup>, where <b><i>x</i></b><sup>1</sup> and <b><i>x</i></b><sup><i>L</i> + 1</sup> are the input and output neurons, respectively. We consider the relationship between activities in adjacent layers for ANNs given by</p><div id="Equ5"><p><span>$${{{{\boldsymbol{x}}}}}^{l}={{{{\boldsymbol{w}}}}}^{l-1}f\,\left({{{{\boldsymbol{x}}}}}^{l-1}\right),$$</span></p><p>
                    (5)
                </p></div><p>and the energy function for EBNs described by</p><div id="Equ6"><p><span>$${E}^{l}=\frac{1}{2}{\left({{{{\boldsymbol{x}}}}}^{l}-{{{{\boldsymbol{w}}}}}^{l-1}f\left({{{{\boldsymbol{x}}}}}^{l-1}\right)\right)}^{2}.$$</span></p><p>
                    (6)
                </p></div><p>This defines the ANNs to be the standard multilayer perceptrons (MLPs) and the energy-based networks to be the predictive coding network. In Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ6">6</a>) and below, the square operator (<b><i>v</i></b>)<sup>2</sup> denotes the inner product of vector <b><i>v</i></b> with itself. The comparison between backpropagation and prospective configuration in the main text is thus between the above MLPs and predictive coding networks; this choice is justified as (1) they are the most standard models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Goodfellow, I., Bengio, Y. &amp; Courville, A. Deep Learning (MIT Press Cambridge, 2016)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR61" id="ref-link-section-d46823651e2788">61</a></sup> and (2) it is established that the two are closely related<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e2793">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In Advances in Neural Information Processing Systems (NeurIPS) (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR14" id="ref-link-section-d46823651e2796">14</a></sup> (that is, they make the same prediction with the same weights and input pattern), thus enabling a fair comparison. Nevertheless, we show that the theory (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">5</a>) and empirical comparison (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">6</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">7</a>) between backpropagation and prospective configuration generalize to other choices of network structures and energy functions, that is, other energy-based networks and ANNs, such as GeneRec<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="O’Reilly, R. C. Biologically plausible error-driven learning using local activation differences: the generalized recirculation algorithm. Neural Comput. 8, 895–938 (1996)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR62" id="ref-link-section-d46823651e2809">62</a></sup> and Almeida–Pineda<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Almeida, L. B. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In Artificial Neural Networks: Concept Learning (ed. Diederich, J.) 102–111 (IEEE Computer Society Press, 1990)." href="#ref-CR63" id="ref-link-section-d46823651e2813">63</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pineda, F. Generalization of back propagation to recurrent and higher order neural networks. In Advances in Neural Information Processing Systems (NeurIPS) (ed. Anderson, D.) 602–611 (Curran Associates, 1987)." href="#ref-CR64" id="ref-link-section-d46823651e2813_1">64</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Pineda, F. J. Dynamics and architecture for neural computation. J. Complex. 4, 216–245 (1988)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR65" id="ref-link-section-d46823651e2816">65</a></sup>.</p><p>Putting Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ5">5</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ6">6</a>) into the general framework, we can obtain the equations that describe MLPs and predictive coding networks, respectively. Assume that the input and target patterns are <b><i>s</i></b><sup>in</sup> and <b><i>s</i></b><sup>target</sup>, respectively. Prediction with MLPs is</p><div id="Equ7"><p><span>$${{{{\boldsymbol{x}}}}}^{1}={{{{\boldsymbol{s}}}}}^{{{{\rm{in}}}}}\,{{{\rm{and}}}}\,{{{{\boldsymbol{x}}}}}^{l}={{{{\boldsymbol{w}}}}}^{l-1}{f}\,\left({{{{\boldsymbol{x}}}}}^{l-1}\right){{{\rm{for}}}}\,l &gt; 1,$$</span></p><p>
                    (7)
                </p></div><p>where <b><i>x</i></b><sup><i>L</i> + 1</sup> is the prediction. Training MLPs with backpropagation is described by</p><div id="Equ8"><p><span>$${{\Delta }}{{{{\boldsymbol{w}}}}}^{l}=-\alpha \frac{\partial {{{\mathcal{L}}}}}{\partial {{{{\boldsymbol{w}}}}}^{l}}=-\alpha \frac{\partial {{{\mathcal{L}}}}}{\partial {{{{\boldsymbol{x}}}}}^{L+1}}\cdot \frac{\partial {{{{\boldsymbol{x}}}}}^{L+1}}{\partial {{{{\boldsymbol{x}}}}}^{L}}\ldots \frac{\partial {{{{\boldsymbol{x}}}}}^{l+1}}{\partial {{{{\boldsymbol{w}}}}}^{l}}\,{{{\rm{where}}}}\,\,{{{\mathcal{L}}}}=\frac{1}{2}{\left({{{{\boldsymbol{s}}}}}^{{{{\rm{target}}}}}-{{{{\boldsymbol{x}}}}}^{L+1}\right)}^{2},$$</span></p><p>
                    (8)
                </p></div><p>which backpropagates the error <span>\(\frac{\partial {{{\mathcal{L}}}}}{\partial {{{{\boldsymbol{x}}}}}^{l}}\)</span> layer by layer from output neurons.</p><p>The neural dynamics of predictive coding networks can be obtained using Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ2">2</a>):</p><div id="Equ9"><p><span>$${{\Delta }}{{{{\boldsymbol{x}}}}}^{l}=-\gamma \frac{\partial E}{\partial {{{{\boldsymbol{x}}}}}^{l}}=-\gamma \frac{\partial ({E}^{l}+{E}^{l+1})}{\partial {{{{\boldsymbol{x}}}}}^{l}}.$$</span></p><p>
                    (9)
                </p></div><p>Similarly, the weight dynamics of predictive coding networks can be found,</p><div id="Equ10"><p><span>$${{\Delta }}{{{{\boldsymbol{w}}}}}^{l}=-\alpha \frac{\partial E}{\partial {{{{\boldsymbol{w}}}}}^{l}}=-\alpha \frac{\partial {E}^{l+1}}{\partial {{{{\boldsymbol{w}}}}}^{l}}.$$</span></p><p>
                    (10)
                </p></div><p>To reveal the neural implementation of predictive coding networks, we define the prediction errors to be</p><div id="Equ11"><p><span>$${{{{\boldsymbol{\varepsilon }}}}}^{l}={{{{\boldsymbol{x}}}}}^{l}-{{{{\boldsymbol{w}}}}}^{l-1}{f}\,\left({{{{\boldsymbol{x}}}}}^{l-1}\right).$$</span></p><p>
                    (11)
                </p></div><p>The neural and weight dynamics of predictive coding networks can be expressed (by evaluating derivatives in Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ9">9</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ10">10</a>)) as</p><div id="Equ12"><p><span>$${{\Delta }}{{{{\boldsymbol{x}}}}}^{l}=-\gamma {{{{\boldsymbol{\varepsilon }}}}}^{l}+{f}^{{\prime} }\left({{{{\boldsymbol{x}}}}}^{l}\right)\circ {\left({{{{\boldsymbol{w}}}}}^{l}\right)}^{T}{{{{\boldsymbol{\varepsilon }}}}}^{l+1}\,{\mathrm{and}}$$</span></p><p>
                    (12)
                </p></div><div id="Equ13"><p><span>$${{\Delta }}{{{{\boldsymbol{w}}}}}^{l}=\alpha {{{{\boldsymbol{\varepsilon }}}}}^{l+1}{\left({f}\left({{{{\boldsymbol{x}}}}}^{l}\right)\right)}^{T},$$</span></p><p>
                    (13)
                </p></div><p>where the symbol <span>∘</span> denotes element-wise multiplication. Assuming that <b><i>ε</i></b><sup><i>l</i></sup> and <b><i>x</i></b><sup><i>l</i></sup> are encoded in the activity of error and value neurons, respectively, Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ11">11</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ12">12</a>) can be realized with the neural implementation in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2c</a>. In particular, error <b><i>ε</i></b> and value <b><i>x</i></b> neurons are represented by red and blue nodes, respectively; excitatory + and inhibitory − connections are represented by connections with solid and hollow nodes, respectively. Thus, Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ11">11</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ12">12</a>) are implemented with red and blue connections, respectively. It should also be noted that the weight dynamics are also realized locally. The weight change described by Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ13">13</a>) corresponds to simple Hebbian plasticity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Hebb, D. O. The Organisation of Behaviour: A Neuropsychological Theory (Science Editions New York, 1949)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR66" id="ref-link-section-d46823651e3996">66</a></sup> in the neural implementation of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig2">2c</a>; that is, the change in a weight is proportional to the product of activity of presynaptic and postsynaptic neurons. Thus, a predictive coding network, as an energy-based network, can be implemented with local circuits only due to the local nature of energy terms (as argued earlier in this section). Note that when the network is expressive enough such that learning can reduce the energy <i>E</i> to 0, the loss <span>\({{{\mathcal{L}}}}\)</span> must also become 0 as <span>\({{{\mathcal{L}}}}\)</span> is one of the terms in energy <i>E</i>, that is <span>\({{{\mathcal{L}}}}={E}^{L+1}\)</span>, and, in this case, the predictive coding network is guaranteed to minimize the loss, just like backpropagation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Senn, W. et al. A neuronal least-action principle for real-time learning in cortical circuits. Preprint at bioRxiv 
                  https://doi.org/10.1101/2023.03.25.534198
                  
                 (2023)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR67" id="ref-link-section-d46823651e4087">67</a></sup>.</p><p>The full algorithm of the predictive coding network is summarized in Algorithm <a data-track="click" data-track-label="link" data-track-action="subsection anchor" href="https://www.nature.com/articles/s41593-023-01514-1#FPar1">1</a>. In all simulations in this paper (unless stated otherwise), the integration step of the neural dynamics (that is, relaxation) is set to <i>γ</i> = 0.1, and the relaxation is performed for 128 steps (<span>\({{{\mathcal{T}}}}\)</span> in Algorithm <a data-track="click" data-track-label="link" data-track-action="subsection anchor" href="https://www.nature.com/articles/s41593-023-01514-1#FPar1">1</a>). During relaxation, if the overall energy is not decreased from the last step, the integration step is reduced by 50%; if the integration step is reduced two times (that is, reaching 0.025), relaxation is terminated early. By monitoring the number of relaxation steps performed, we notice that in most of the tasks we performed, relaxation is terminated early at around 60 iterations.</p>
                  <h3 id="FPar1">Algorithm 1</h3>
                  <p>Learn with a predictive coding network<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e4128">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e4131">18</a></sup> <img src="https://media.springernature.com/lw684/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Figa_HTML.png" alt=""></p>
                <p>In the Supplementary <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Information</a>, we also investigate other choices of network structures and energy functions, resulting in other ANNs and energy-based networks. Overall, the energy-based networks investigated include predictive coding networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR12" id="ref-link-section-d46823651e4146">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR18" id="ref-link-section-d46823651e4149">18</a></sup>, target predictive coding networks and GeneRec<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="O’Reilly, R. C. Biologically plausible error-driven learning using local activation differences: the generalized recirculation algorithm. Neural Comput. 8, 895–938 (1996)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR62" id="ref-link-section-d46823651e4153">62</a></sup>, and the ANNs investigated include backpropagation and Almeida–Pineda<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Almeida, L. B. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In Artificial Neural Networks: Concept Learning (ed. Diederich, J.) 102–111 (IEEE Computer Society Press, 1990)." href="#ref-CR63" id="ref-link-section-d46823651e4157">63</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pineda, F. Generalization of back propagation to recurrent and higher order neural networks. In Advances in Neural Information Processing Systems (NeurIPS) (ed. Anderson, D.) 602–611 (Curran Associates, 1987)." href="#ref-CR64" id="ref-link-section-d46823651e4157_1">64</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Pineda, F. J. Dynamics and architecture for neural computation. J. Complex. 4, 216–245 (1988)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR65" id="ref-link-section-d46823651e4160">65</a></sup>. Details of all the models can be found in corresponding previous work and are also given in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM1">Supplementary Notes</a>, Section 2.1.</p><h3 id="Sec12">Interference and measuring interference (that is, target alignment)</h3><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3a</a>, because it simulates the example in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig1">1</a>, the network has one input neuron, one hidden neuron and two output neurons; weights were all initialized to 1, the input pattern was <span>\(\left[1\right]\)</span>, and the target pattern was <span>\(\left[0,1\right]\)</span>. Learning rates of both learning rules were 0.2, and the weights were updated for 24 iterations. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3d</a> repeated the same experiment as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3a</a> but with the learning rate searched from <span>\(\left(0.005,0.01,0.05,0.1\right)\)</span>, which is wide enough to cover essentially all learning rates used to train deep neural networks in practice.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3e</a>, there were 64 neurons in each layer (including input and output layers) for each network; weights were initialized via standard Xavier uniform initialization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics (eds Teh, Y. W. &amp; Titterington, M.) 249–256 (PMLR, 2010)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR68" id="ref-link-section-d46823651e4280">68</a></sup>. No activation function was used, that is, linear networks were investigated. Depths of networks (<i>L</i>) took values from <span>\(\left\{1,2,\ldots ,24,25\right\}\)</span>, as reported on the <i>x</i> axis. Input and target patterns were a pair of randomly generated patterns with a mean of 0 and standard deviation (s.d.) of 1. Learning rates of both learning rules were 0.001. Weights were updated for one iteration, and target alignment was measured. The whole experiment was repeated 27 times with each individual experiment reported as a point.</p><p>Simulations in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f–h</a> followed the experimental setup in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4a–h</a>; these are described at the end of <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec13">Biologically relevant tasks</a>.</p><h3 id="Sec13">Biologically relevant tasks</h3><p>In supervised learning simulations, fully connected networks in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4a–h</a> were trained and tested on FashionMNIST<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR60" id="ref-link-section-d46823651e4353">60</a></sup>, and convolutional neural networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="O’Shea, K. &amp; Nash, R. An introduction to convolutional neural networks. Preprint at 
                  https://doi.org/10.48550/arXiv.1511.08458
                  
                 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR35" id="ref-link-section-d46823651e4357">35</a></sup> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4i,j</a>) were trained and tested on CIFAR-10 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Krizhevsky, A. &amp; Hinton, G. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, Univ. Toronto (2009)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR36" id="ref-link-section-d46823651e4364">36</a></sup>). With FashionMNIST, models were trained to perform classification of gray-scaled fashion item images into ten categories, such as trousers, pullovers and dresses. FashionMNIST was chosen because it is of moderate and appropriate difficulty for multilayer non-linear deep neural networks so that the comparisons with energy-based networks are informative. Classification of the data in CIFAR-10 is more difficult, as it contains colored natural images belonging to categories such as cars, birds and cats and is thus only evaluated with convolutional neural networks. Both datasets consist of 60,000 training examples (that is, training set) and 10,000 test examples (that is, test set).</p><p>The experiments in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4a–h</a> followed the configurations described below, except for the parameters investigated in specific panels (such as batch size, size of the dataset and size of the architecture), which were adjusted as stated in the descriptions of the specific experiments. The neural network was composed of four layers and 32 hidden neurons in each hidden layer. Note that the state-of-the-art MLP models of FashionMNIST are all quite large<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Tolstikhin, I. O. et al. Mlp-mixer: an all-mlp architecture for vision. In Advances in Neural Information Processing Systems (NeurIPS) (eds Ranzato, M. et al.) 24261–24272 (Curran Associates, 2021)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR69" id="ref-link-section-d46823651e4374">69</a></sup>. However, they are highly overparameterized and thus are not suitable to base our comparison on because the accuracy reaches more than 95% regardless of the learning rule due to the overparameterization. Thus, there was no space for demonstrating any meaningful comparison in these state-of-the-art overparameterized models. Overall, the size of the model on FashionMNIST demonstrated in this paper was a reasonable choice, with baseline models reaching reasonable performance (~0.12 test error for the standard machine learning setup) while maintaining enough room for demonstrating performance differences for different learning rules. The size of the input layer was 28 × 28 for FashionMNIST<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR60" id="ref-link-section-d46823651e4378">60</a></sup> gray scaled, and the size of the output layer was ten as the number of classes for both datasets. The weights were initialized from a normal distribution with a mean of 0 and s.d. of <span>\(\sqrt{\frac{2}{{n}^{l}+{n}^{l+1}}}\)</span>, where <i>n</i><sup><i>l</i></sup> and <i>n</i><sup><i>l</i> + 1</sup> are the numbers of neurons in the layer before and after the weight, respectively. This initialization is known as Xavier normal initialization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics (eds Teh, Y. W. &amp; Titterington, M.) 249–256 (PMLR, 2010)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR68" id="ref-link-section-d46823651e4460">68</a></sup>. The activation function <span>\({f}\,\left(\right)\)</span> is sigmoid. We defined one iteration as updating the weights for one step based on a minibatch. Each iteration contained (1) a numerical integration procedure of relaxation of energy-based networks, which captures its continuous process; and (2) one update of weights at the end of the above procedure. The number of examples in a minibatch, called the batch size, was by default 32. One epoch comprised presenting the entire training set split over multiple minibatches. At the end of each epoch, the model was tested on the test set, and the classification error was recorded as the ‘test error’ of the epoch. The neural network was trained for 64 epochs, thus yielding 64 test errors. The mean of the test error over epochs, that is, during training progress, is an indicator of how fast the model learns, and the minimum of the test errors over epochs is an indicator of how well the model can learn, ignoring the possibility of overfitting due to training for too long. Learning rates were optimized independently for each configuration and each model. Each experiment was repeated ten times (unless stated otherwise), and the error bars represent the 68% confidence interval computed using bootstrap.</p><p>We now describe settings specific to individual experiments. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4b</a>, different batch sizes were tested (as shown on the <i>x</i> axis). In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4c</a>, the batch size was set to 1. In continual learning of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4d</a>, training alternated between two tasks. Task 1 involved classifying five randomly selected classes in a dataset, and task 2 involved classifying the remaining five classes. The whole network was shared by the two tasks; thus, different from the network used in other panels, the network only had five output neurons. This better corresponds to continual learning with multiple tasks in nature, because, for example, if humans learn to perform two different tasks, they typically use one brain and one pair of hands (that is, the whole network is shared), as they do not have two different pairs of hands (that is, humans share the output layers across tasks). Task 1 was trained for four iterations, task 2 was trained for four iterations, and the training continued until a total of 84 iterations was reached. After each iteration, error on the test set of each task was measured as ‘test error’. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4e</a>, the mean of test error of both tasks during training of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4d</a> at different learning rates is reported. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4d–g</a> investigating concept drifting<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M. &amp; Bouchachia, A. A survey on concept drift adaptation. ACM Comput. Surv. 46, 1–37 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR31" id="ref-link-section-d46823651e4516">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Žliobaitė, I. Learning under concept drift: an overview. Preprint at 
                  https://doi.org/10.48550/arXiv.1010.4784
                  
                 (2010)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR70" id="ref-link-section-d46823651e4519">70</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Tsymbal, A. The Problem of Concept Drift: Definitions and Related Work. Technical report, Computer Science Department, Trinity College Dublin (2004)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR71" id="ref-link-section-d46823651e4522">71</a></sup>, changes to class labels were made every 64 epochs, and the models were trained for 3,000 epochs in total. Thus, every 64 epochs, five of ten output neurons were selected, and the mapping from these five output neurons to the semantic meaning was pseudorandomly shuffled. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4h</a>, different numbers of data points per class (shown on the <i>x</i> axis) were included in the training set (subsets were randomly selected according to different seeds).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4i</a>, we trained a convolutional network with prospective configuration and backpropagation, with the structure detailed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4j</a>. For each learning rule, we independently searched seven learning rates ranging from <span>\(\left\{0.0005,0.00025,0.0001,0.000075,0.00005,0.000025,0.00001\right\}\)</span>. Both learning rules were trained for 80 epochs, with a batch size of 200. Because training deep convolutional networks is more difficult and slower than training shallow fully connected networks, a few improvements were applied to both learning rules. Specifically, a weight decay of 0.01 and an Adam optimizer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. 
                  https://doi.org/10.48550/arXiv.1412.6980
                  
                 (2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR72" id="ref-link-section-d46823651e4590">72</a></sup> were applied for both learning rules. To reduce running time, the weights were updated more frequently in predictive coding networks; that is, the weights were updated at all steps of inference instead of at the last step of inference. Inference was run for a fixed number of 16 iterations; thus, weights were updated 16 times for each batch of data. Thus, for fair comparison, backpropagation also updated weights 16 times on each batch of data. Training in each configuration (each learning rule and each learning rate) was repeated three times with different seeds.</p><p>To extend a predictive coding network to a convolutional neural network (or to any network with a layered structure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Salvatori, T. et al. Learning on arbitrary graph topologies via predictive coding. In Advances in Neural Information Processing Systems (NeurIPS) (eds Koyejo, S. et al.) 38232–38244 (Curran Associates, 2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR58" id="ref-link-section-d46823651e4598">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Salvatori, T., Song, Y., Lukasiewicz, T., Bogacz, R. &amp; Xu, Z. Reverse differentiation via predictive coding. In Proc. 36th AAAI Conference on Artificial Intelligence (Salvatori, T., Song, Y., Xu, Z., Lukasiewicz, T. &amp; Bogacz, R.) 8150–8158 (Curran Associates, 2022)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR73" id="ref-link-section-d46823651e4601">73</a></sup>), we can define the forward function of a layer (that is, how the input of layer <i>l</i> + 1 is computed from the neural activity of layer <i>l</i>) with weights <b><i>w</i></b><sup><i>l</i></sup> to be <span>\({{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)\)</span>. For example, for the MLPs described above, <span>\({{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)={{{{\boldsymbol{w}}}}}^{l}{f}\,\left({{{{\boldsymbol{x}}}}}^{l}\right)\)</span>. For a convolutional network, <span>\({{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)\)</span> is a more complex function of <b><i>w</i></b><sup><i>l</i></sup> and <b><i>x</i></b><sup><i>l</i></sup>, and also <b><i>w</i></b><sup><i>l</i></sup> and <b><i>x</i></b><sup><i>l</i></sup> are not simple matrix and vector anymore (to be defined later). Defining an ANN with <span>\({\mathcal{F}}()\)</span> would be (that is, Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ5">5</a>) becomes) <span>\({{{{\boldsymbol{x}}}}}^{l}={{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l-1}}\left({{{{\boldsymbol{x}}}}}^{l-1}\right)\)</span>. Defining an energy function of a predictive coding network with <span>\({\mathcal{F}}()\)</span> would be (that is, Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ6">6</a>) becomes) <span>\({E}^{l}=\frac{1}{2}{\left[{{{{\boldsymbol{x}}}}}^{l}-{{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l-1}}\left({{{{\boldsymbol{x}}}}}^{l-1}\right)\right]}^{2}\)</span>. Thus, neural and weight dynamics would be (that is, Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ12">12</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ13">13</a>) become) <span>\({{\Delta }}{{{{\boldsymbol{x}}}}}^{l}=-\gamma {{{{\boldsymbol{\varepsilon }}}}}^{l}+\frac{\partial {{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)}{\partial {{{{\boldsymbol{x}}}}}^{l}}{{{{\boldsymbol{\varepsilon }}}}}^{l+1}\)</span> and <span>\({{\Delta }}{{{{\boldsymbol{w}}}}}^{l}=\alpha {{{{\boldsymbol{\varepsilon }}}}}^{l+1}\frac{\partial {{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)}{\partial {{{{\boldsymbol{w}}}}}^{l}},\)</span> respectively. As <span>\({{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)\)</span> is defined, <span>\(\frac{\partial {{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)}{\partial {{{{\boldsymbol{x}}}}}^{l}}\)</span> and <span>\(\frac{\partial {{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)}{\partial {{{{\boldsymbol{w}}}}}^{l}}\)</span> are obtained via auto differentiation in PyTorch (<a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</a>). Thus, training a convolutional predictive coding network is as simple as replacing lines 11 and 16 in Algorithm <a data-track="click" data-track-label="link" data-track-action="subsection anchor" href="https://www.nature.com/articles/s41593-023-01514-1#FPar1">1</a> with the above corresponding equations.</p><p>In the following, we define <span>\({{{{\mathcal{F}}}}}_{{{{{\boldsymbol{w}}}}}^{l}}\left({{{{\boldsymbol{x}}}}}^{l}\right)\)</span> for convolutional networks. First, <span>\({{{{\boldsymbol{x}}}}}^{l}\in {{\mathbb{R}}}^{{c}_{l}\times {h}_{l}\times {w}_{l}}\)</span>, where <i>c</i><sub><i>l</i></sub>, <i>h</i><sub><i>l</i></sub> and <i>w</i><sub><i>l</i></sub> are the number of features, height and width of the feature map, respectively. The numbers for each layer are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4j</a> in the format <i>c</i><sub><i>l</i></sub><i>@</i><i>h</i><sub><i>l</i></sub> × <i>w</i><sub><i>l</i></sub>. For example, for the first layer (input layer), the shape was 3<i>@</i>32 × 32 as it is 32 × 32 colored images, that is, with three feature maps representing red, green and blue. We denote kernel size, stride and padding of this layer as <i>k</i><sub><i>l</i></sub>, <i>s</i><sub><i>l</i></sub> and <i>p</i><sub><i>l</i></sub>, respectively. The numbers for each layer are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4j</a>. Thus, <span>\({{{{\boldsymbol{w}}}}}^{l}\in {{\mathbb{R}}}^{{c}_{l+1}\times {c}_{l}\times {k}_{l}\times {k}_{l}}\)</span>. Finally, <b><i>x</i></b><sup><i>l</i> + 1</sup> is obtained via</p><div id="Equ14"><p><span>$$\begin{array}{l}{{{{\boldsymbol{x}}}}}^{\;l+1}[c,x,y]={f}\,\left({{{{\boldsymbol{x}}}}}^{\;l}\left[:,x{s}_{l}-{p}_{l}:x{s}_{l}-{p}_{l}+{k}_{l},y{s}_{l}-{p}_{l}:y{s}_{l}-{p}_{l}+{k}_{l}\right]\right)\\\cdot {{{{\boldsymbol{w}}}}}^{l}\left[c,:,:,:\right],\end{array}$$</span></p><p>
                    (14)
                </p></div><p>where <span>\(\left[a,b,\ldots \right]\)</span> means indexing the tensor along each dimension, : means all indexes at that dimension, <i>a</i>: <i>b</i> means slice of that dimension from index <i>a</i> to <i>b</i> − 1, and <span>⋅</span> is dot product. In the above equation, if the slicing of <b><i>x</i></b><sup><i>l</i></sup> on the second and third dimensions, that is, <span>\({{{{\boldsymbol{x}}}}}^{l}\left[:,x{s}_{l}-{p}_{l}:x{s}_{l}-{p}_{l}+{k}_{l},y{s}_{l}-{p}_{l}:y{s}_{l}-{p}_{l}+{k}_{l}\right]\)</span>, is outside its defined range <span>\({{\mathbb{R}}}^{{c}_{l}\times {h}_{l}\times {w}_{l}}\)</span>, the entries outside range are considered to be 0, known as padding mode of zeros.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a>, networks of 15 layers were trained and tested on the FashionMNIST<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR60" id="ref-link-section-d46823651e6681">60</a></sup> dataset. Learning rates in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a> were optimized independently by a grid search over (5.0, 1.0, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.000005) for each learning rule, as shown Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3g</a>; that is, each learning rule in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a> used the learning rate that gave a minimal point in the corresponding curve in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3g</a>. The experiment in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3h</a> investigated other network depths (<span>\(\left\{1,2,4,6,8,10,12,14,15\right\}\)</span>) in the same setup. Similar to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig3">3f</a>, the learning rate for each learning rule and each ‘number of layers’ was the optimal value (in terms of mean of test error as the <i>y</i> axis of the figure) independently searched from (5.0, 1.0, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.000005). Hidden layers were always of size 64 in the above experiments. In the above experiment, only a part of the training set was used (60 data points per class) so that the test error was evaluated more frequently to reflect the difference on efficiency of the investigated learning rules. The activation function <span>\({f}\,\left(\right)\)</span> used is LeakyReLU instead of the standard sigmoid because sigmoid results in difficulty in training deep neural networks. Other unmentioned details followed the defaults, as described above.</p><p>In the reinforcement learning experiments (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4k</a>), we evaluated performance on three classic reinforcement learning problems: Acrobot<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Sutton, R. S. Generalization in reinforcement learning: successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems (NeurIPS) (eds Touretzky, D. et al.) 1038–1044 (NIPS, 1995)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR74" id="ref-link-section-d46823651e6797">74</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="Geramifard, A., Dann, C., Klein, R. H., Dabney, W. &amp; How, J. P. RLPy: a value-function-based reinforcement learning framework for education and research. J. Mach. Learn. Res. 16, 1573–1578 (2015)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR75" id="ref-link-section-d46823651e6800">75</a></sup>, MountainCar<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Moore, A. Efficient memory-based learning for robot control. Technical report, Carnegie Mellon Univ. (1990)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR76" id="ref-link-section-d46823651e6804">76</a></sup> and CartPole<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Barto, A. G., Sutton, R. S. &amp; Anderson, C. W. Neuronlike adaptive elements that can solve difficult learning control problems. In IEEE Transactions on Systems, Man, and Cybernetics, 834–846 (1983)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR77" id="ref-link-section-d46823651e6808">77</a></sup>. We interacted with these environments via a unified interface by OpenAI Gym<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Brockman, G. et al. OpenAI Gym. Preprint at 
                  https://doi.org/10.48550/arXiv.1606.01540
                  
                 (2016)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR78" id="ref-link-section-d46823651e6812">78</a></sup>. The observations <i>s</i><sub><i>t</i></sub> of these environments are vectors describing the status of the system, such as velocities and positions of different moving parts (for details, refer to the original articles or documentation from OpenAI Gym). Each entry of the observation <i>s</i><sub><i>t</i></sub> is normalized to mean 0 and s.d. 1 via Welford’s online algorithm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Welford, B. P. Note on a method for calculating corrected sums of squares and products. Technometrics 4, 419–420 (1962)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR79" id="ref-link-section-d46823651e6829">79</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Knuth, D. E. Art of Computer Programming, Vol. 2 (Addison-Wesley Professional, 2014)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR80" id="ref-link-section-d46823651e6832">80</a></sup>. The action space of these environments is discrete. Thus, we can have a network taking in observation <i>s</i><sub><i>t</i></sub> and predicting the value (<i>Q</i>) of each action <i>a</i><sub><i>t</i></sub> with different output neurons. Such a network is known as an action-value network, in short, a <i>Q</i> network. In our experiment, the <i>Q</i> network contained two hidden layers, each of which contained 64 neurons, initialized the same way as the network used for supervised learning, described before. One can acquire the value of an action <i>a</i><sub><i>t</i></sub> at a given observation <i>s</i><sub><i>t</i></sub> by feeding <i>s</i><sub><i>t</i></sub> into the <i>Q</i> network and reading out the prediction on the output neuron corresponding to the action <i>a</i><sub><i>t</i></sub>; such a value is denoted <span>\(Q\left({s}_{t},{a}_{t}\right)\)</span>. The training of <i>Q</i> is a simple regression problem to target <span>\({\hat{R}}_{t}\)</span>, obtained via <i>Q</i> learning with experience replay (summarized in Algorithm <a data-track="click" data-track-label="link" data-track-action="subsection anchor" href="https://www.nature.com/articles/s41593-023-01514-1#FPar2">2</a>). Considering <i>s</i><sub><i>t</i></sub> to be <b><i>s</i></b><sup>in</sup> and <span>\({\hat{R}}_{t}\)</span> to be <b><i>s</i></b><sup>target</sup>, the <i>Q</i> network can be trained with prospective configuration or backpropagation. Note that <span>\({\hat{R}}_{t}\)</span> is the target of the selected action <i>a</i><sub><i>t</i></sub> (that is, the target of one of the output neurons corresponds to the selected action <i>a</i><sub><i>t</i></sub>); thus, <span>\({\hat{R}}_{t}\)</span> is, in practice, considered to be <span>\({{{{\boldsymbol{s}}}}}^{{{{\rm{target}}}}}\left[{a}_{t}\right]\)</span>. For prospective configuration, it means that the rest of the output neurons except the one corresponding to <i>a</i><sub><i>t</i></sub> are freed; for backpropagation, it means that the error on these neurons is masked out.</p><p>A predictive coding network with slightly different settings from the defaults was used for prospective configuration. The integration step was fixed to be half of the default (<i>γ</i> = 0.05), and relaxation was performed for a fixed and smaller number of steps (<span>\({{{\mathcal{T}}}}=32\)</span>). This change was introduced because <i>Q</i> learning is more unstable (smaller integration step) and more expensive (smaller number of relaxation steps) than supervised learning tasks. To produce a smoother curve of ‘sum of rewards per episode’ in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4k</a> from <i>SumRewardPerEpisode</i> in Algorithm <a data-track="click" data-track-label="link" data-track-action="subsection anchor" href="https://www.nature.com/articles/s41593-023-01514-1#FPar2">2</a>, the <i>SumRewardPerEpisode</i> curve was averaged along <i>TrainingEpisode</i> with a sliding window with a length of 200. Each experiment was repeated with three random seeds, and the shadows represent 68% confidence interval across them. Learning rates were searched independently for each environment and each model from the range <span>\(\left\{0.05,0.01,0.005,0.001,0.0005,0.0001\right\}\)</span>. The results reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4k</a> are for the learning rates yielding the highest mean of ‘sum of rewards per episode’ over training episodes.</p>
                  <h3 id="FPar2">Algorithm 2</h3>
                  <p><i>Q</i> learning with experience replay <img src="https://media.springernature.com/lw684/springer-static/image/art%3A10.1038%2Fs41593-023-01514-1/MediaObjects/41593_2023_1514_Figb_HTML.png" alt=""></p>
                <h3 id="Sec14">Simulation of motor learning</h3><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5</a>, we trained a network that included two input neurons, two hidden neurons and two output neurons. The two input neurons were one-to-one connected to the two hidden neurons, and the two hidden neurons were fully connected to the two output neurons. The two input neurons were considered to encode presenting the blue and red background, respectively. The two output neurons were considered to encode the prediction of the perturbations toward positive and negative directions, respectively. Presenting and not presenting a background color were encoded 1 and 0, respectively; presenting and not presenting perturbations of a particular direction were encoded 1 and 0, respectively. The weights were initialized from a normal distribution with mean 0 and an s.d. fitted to the behavioral data (see below), simulating that the participants had not built any associations before the experiments. Learning rates were independent for the two layers, as we expected the connections from perception to belief and from belief to predictions to have different degrees of plasticity. The two learning rates were also fitted to the data (see below).</p><p>The number of participants and training and testing trials follow exactly as described for the human experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e7322">38</a></sup>. In particular, for each of the 24 simulated participants, the weights were initialized with a different seed of the random number generator. They each experienced two stages: training and testing. Note that the pretraining stage performed in the human experiment was not simulated here as its goal was to make human participants familiar with the setup and devices.</p><p>In the training stage, the model experienced 24 blocks of trials. In each block, the model was presented with the following sequence of trials, matching the original experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e7329">38</a></sup>:</p><ul>
                  <li>
                    <p>The model was trained with two trials without perturbation, B<sub>0</sub> and R<sub>0</sub>, with the order counterbalanced across consecutive blocks. Note that, in the human experiment, there were two trial types without perturbations (channel and washout trials), but they were simulated in the same way here as B<sub>0</sub> or R<sub>0</sub> trials because they both did not include any perturbations.</p>
                  </li>
                  <li>
                    <p>The model was trained with 32 trials with perturbations, where there were equal numbers of B+ and R– within each of the 8 trials in a pseudorandom order.</p>
                  </li>
                  <li>
                    <p>The model experienced two trials, B<sub>0</sub> and R<sub>0</sub>, with the order counterbalanced across consecutive blocks.</p>
                  </li>
                  <li>
                    <p>The model experienced <i>n</i> ← {14, 16, 18} washout trials (equal numbers of B<sub>0</sub> and R<sub>0</sub> trials in a pseudorandom order), where <i>n</i> ← {<i>a</i>, <i>b</i>, <i>c</i>} denotes sampling without replacement from a set of values <i>a</i>, <i>b</i> and <i>c</i> and replenishing the set whenever it becomes empty.</p>
                  </li>
                  <li>
                    <p>The model experienced one triplet, where the exposure trial was either B+ or R–, counterbalanced across consecutive blocks. Here, a triplet consisted of three sequential trials: B<sub>0</sub>, the specified exposure trial and B<sub>0</sub> again.</p>
                  </li>
                  <li>
                    <p>The model experienced additional <i>n</i> ← {6, 8, 10} washout trials (equal numbers of B<sub>0</sub> and R<sub>0</sub> trials in a pseudorandom order).</p>
                  </li>
                  <li>
                    <p>The model experienced one triplet again, where the exposure trial was either B+ or R–, whichever was not used on the previous triplet.</p>
                  </li>
                </ul><p>In the testing stage, the model then experienced eight repetitions of four blocks of trials. In each block, one of the combinations of B+, R+, B– and R– was tested. The order of the four blocks was shuffled in each of the eight repetitions. In each block, the model first experienced <i>n</i> ← {2, 4, 6} washout trials (equal numbers of B<sub>0</sub> and R<sub>0</sub> trials in a pseudorandom order). The model then experienced a triplet of trials, where the exposure trial was the combination (B+, R+, B– or R–) tested in a given block to assess single-trial learning of this combination. The change in adaption in the model was computed as the absolute value of the difference in the predictions of perturbations on the two B<sub>0</sub> trials in the above triplet, where the prediction of perturbation was computed as the difference between the activities of the two output neurons. The predictions were averaged over participants and the above repetitions.</p><p>The parameters of each learning rule were chosen such that the model best reproduced the change in adaptation shown in Fig <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig5">5f</a>. In particular, we minimized the sum over set <i>C</i> of the four exposure trial types of the squared difference between average change in adaptation in experiment (<i>d</i><sub><i>c</i></sub>) and model (<i>x</i><sub><i>c</i></sub>):</p><div id="Equ15"><p><span>$$\mathop{\sum}\limits_{c\in C}{\left(a{x}_{c}-{d}_{c}\right)}^{2}.$$</span></p><p>
                    (15)
                </p></div><p>The model predictions were additionally scaled by a coefficient <i>a</i> fitted to the data because the behavioral data and model outputs had different scales. An exhaustive search was performed over model parameters. The s.d. of initial weights could take values from <span>\(\left\{0.01,0.05,0.1\right\}\)</span>, and two learning rates for two layers could take values from <span>\(\left\{0.00005,0.0001,0.0005,0.01,0.05\right\}\)</span>. For each learning rule and each combination of the above model parameters, the coefficient <i>a</i> was then resolved analytically (restricted to be positive) to minimize the sum of the squared errors of Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ15">15</a>).</p><h3 id="Sec15">Simulation of human reinforcement learning</h3><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6b</a>, we trained a network that included one input neuron, one hidden neuron and two output neurons. The input neuron was considered to encode being in the task, so it was set to 1 throughout the simulation. The two output neurons encoded the prediction of the value of the two choices. Reward and punishment were encoded as 1 and −1, respectively, because the participants were either winning or losing money. The model selected actions stochastically based on the predicted value of the two choices (encoded in the activity of two output neurons) according to the softmax rule (with a temperature of 1). The weights were initialized from a normal distribution of mean 0 and an s.d. fitted to experimental data (see below), simulating that the human participants had not built any associations before the experiments. The number of simulated participants (number of repetitions with different seeds) was set to 16, as in the human experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. J. Neurosci. 26, 8360–8367 (2006)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR38" id="ref-link-section-d46823651e7642">38</a></sup>. The number of trials was not mentioned in the original paper, so we simulated for 128 trials for both learning rules.</p><p>To compare the ability of the two learning rules to account for the pattern of signal from the mPFC, for each of the rules, we optimized the parameters describing how the model is set up and learns (the s.d. of initial weights and the learning rate). Namely, we searched for the values of these parameters for which the model produces the most similar pattern of its output activity to that in the experiment. In particular, we minimized the sum over set <i>C</i> of four trial types in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6c</a> of the squared difference between model predictions <i>x</i><sub><i>c</i></sub> and data <i>d</i><sub><i>c</i></sub> on mean mPFC signal:</p><div id="Equ16"><p><span>$$\mathop{\sum}\limits_{c\in C}{\left(a{x}_{c}+b-{d}_{c}\right)}^{2}.$$</span></p><p>
                    (16)
                </p></div><p>The model predictions were additionally scaled by a coefficient <i>a</i> and offset by a bias <i>b</i> because the fMRI signal had different units and baseline than the model. To compute the model prediction for a given trial type, the activity of the output neuron corresponding to the chosen option was averaged across all trials of this type in the entire simulation. The scaled average activity from the model is plotted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig6">6c</a>, where the error bars show the 68% confidence interval of the scaled activity. To fit the model to experimental data, the values of model parameters and the coefficient were found as described in the previous section. In particular, we used exhaustive grid search on the parameters. The models were simulated for all possible combinations of s.d. of initial weights and the learning rate from the following set: <span>\(\left\{0.01,0.05,0.1\right\}\)</span>. For each learning rule and each combination of the above model parameters, the coefficient <i>a</i> (restricted to be positive) and the bias <i>b</i> were then resolved analytically to minimize the sum of the squared error of Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Equ16">16</a>).</p><h3 id="Sec16">Statistics and reproducibility</h3><p>The work in this paper involved computer simulations, but due to random initialization of weight parameters, the simulations were repeated multiple times. No statistical method was used to predetermine the number of repetitions, but for simulations corresponding to behavioral or neurophysiological experiments, the number of repetitions was matched to the number of participants in the given experiment. No data were excluded from the analyses. Because the order of execution has no effect on the results of the numeric experiments, they were not randomized. The investigators were not blinded to outcome assessment.</p><p>To visualize the variability of simulation results, we either presented individual data points or error bars showing confidence intervals or box plots. Confidence intervals were computed using bootstrap throughout the paper, and detailed descriptions of the implementation can be found at <a href="https://seaborn.pydata.org/tutorial/error_bars.html#confidence-interval-error-bars">https://seaborn.pydata.org/tutorial/error_bars.html#confidence-interval-error-bars</a>. The details of the methods used to produce the box plots are available at <a href="https://seaborn.pydata.org/generated/seaborn.boxplot.html">https://seaborn.pydata.org/generated/seaborn.boxplot.html</a>.</p><h3 id="Sec17">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-023-01514-1#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div>
                </div><div>
                <div id="data-availability-section" data-title="Data availability"><h2 id="data-availability">Data availability</h2><p>Learning tasks analyzed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4a–j</a> were built using the publicly available FashionMNIST<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                 (2017)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR60" id="ref-link-section-d46823651e7930">60</a></sup> and CIFAR-10 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Krizhevsky, A. &amp; Hinton, G. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, Univ. Toronto (2009)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR36" id="ref-link-section-d46823651e7934">36</a></sup>) datasets. These datasets are incorporated in most machine learning libraries, and their original releases are available at <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a> and <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>, respectively. Reinforcement learning tasks analyzed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Fig4">4i</a> were built using the publicly available simulators by OpenAI Gym<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Brockman, G. et al. OpenAI Gym. Preprint at 
                  https://doi.org/10.48550/arXiv.1606.01540
                  
                 (2016)." href="https://www.nature.com/articles/s41593-023-01514-1#ref-CR78" id="ref-link-section-d46823651e7956">78</a></sup>. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-023-01514-1#Sec20">Source data</a> are provided with this paper.</p></div><div id="code-availability-section" data-title="Code availability"><h2 id="code-availability">Code availability</h2><div id="code-availability-content">
              
              <p>Complete code and full documentation reproducing all simulation results written in Python are publicly available at <a href="https://github.com/YuhangSong/Prospective-Configuration">https://github.com/YuhangSong/Prospective-Configuration</a> released under GNU General Public License v3.0 without any additional restrictions (for license details, see <a href="https://opensource.org/licenses/GPL-3.0">https://opensource.org/licenses/GPL-3.0</a> by the open source initiative).</p>
            </div></div><div id="MagazineFulltextArticleBodySuffix" aria-labelledby="Bib1" data-title="References"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J. &amp; Hinton, G. Backpropagation and the brain. <i>Nat. Rev. Neurosci.</i> <b>21</b>, 335–346 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41583-020-0277-3" data-track-item_id="10.1038/s41583-020-0277-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41583-020-0277-3" aria-label="Article reference 1" data-doi="10.1038/s41583-020-0277-3">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXnsVCksrk%3D" aria-label="CAS reference 1">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32303713" aria-label="PubMed reference 1">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Backpropagation%20and%20the%20brain&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2Fs41583-020-0277-3&amp;volume=21&amp;pages=335-346&amp;publication_year=2020&amp;author=Lillicrap%2CTP&amp;author=Santoro%2CA&amp;author=Marris%2CL&amp;author=Akerman%2CCJ&amp;author=Hinton%2CG">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Rumelhart, D. E., Hinton, G. E. &amp; Williams, R. J. <i>Learning Internal Representations by Error Propagation</i> (Univ. California, San Diego, Institute for Cognitive Science, 1985).</p></li><li data-counter="3."><p id="ref-CR3">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. ImageNet classification with deep convolutional neural networks. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Bartlett, P. et al.) 1097–1105 (Curran Associates, 2012).</p></li><li data-counter="4."><p id="ref-CR4">Mnih, V. et al. Human-level control through deep reinforcement learning. <i>Nature</i> <b>518</b>, 529–533 (2015).</p></li><li data-counter="5."><p id="ref-CR5">Silver, D. et al. Mastering the game of go with deep neural networks and tree search. <i>Nature</i> <b>529</b>, 484–489 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature16961" data-track-item_id="10.1038/nature16961" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature16961" aria-label="Article reference 5" data-doi="10.1038/nature16961">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs12is7w%3D" aria-label="CAS reference 5">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26819042" aria-label="PubMed reference 5">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search&amp;journal=Nature&amp;doi=10.1038%2Fnature16961&amp;volume=529&amp;pages=484-489&amp;publication_year=2016&amp;author=Silver%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="6."><p id="ref-CR6">Richards, B. A. et al. A deep learning framework for neuroscience. <i>Nat. Neurosci.</i> <b>22</b>, 1761–1770 (2019).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41593-019-0520-2" data-track-item_id="10.1038/s41593-019-0520-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41593-019-0520-2" aria-label="Article reference 6" data-doi="10.1038/s41593-019-0520-2">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXitVCksbrO" aria-label="CAS reference 6">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31659335" aria-label="PubMed reference 6">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7115933" aria-label="PubMed Central reference 6">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20deep%20learning%20framework%20for%20neuroscience&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fs41593-019-0520-2&amp;volume=22&amp;pages=1761-1770&amp;publication_year=2019&amp;author=Richards%2CBA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="7."><p id="ref-CR7">Singer, Y. et al. Sensory cortex is optimized for prediction of future input. <i>eLife</i> <b>7</b>, e31557 (2018).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.7554/eLife.31557" data-track-item_id="10.7554/eLife.31557" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.7554%2FeLife.31557" aria-label="Article reference 7" data-doi="10.7554/eLife.31557">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29911971" aria-label="PubMed reference 7">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6108826" aria-label="PubMed Central reference 7">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensory%20cortex%20is%20optimized%20for%20prediction%20of%20future%20input&amp;journal=eLife&amp;doi=10.7554%2FeLife.31557&amp;volume=7&amp;publication_year=2018&amp;author=Singer%2CY">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="8."><p id="ref-CR8">Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. <i>Proc. Natl Acad. Sci. USA</i> <b>111</b>, 8619–8624 (2014).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1073/pnas.1403112111" data-track-item_id="10.1073/pnas.1403112111" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1403112111" aria-label="Article reference 8" data-doi="10.1073/pnas.1403112111">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXnslWnsb4%3D" aria-label="CAS reference 8">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24812127" aria-label="PubMed reference 8">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4060707" aria-label="PubMed Central reference 8">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance-optimized%20hierarchical%20models%20predict%20neural%20responses%20in%20higher%20visual%20cortex&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1403112111&amp;volume=111&amp;pages=8619-8624&amp;publication_year=2014&amp;author=Yamins%2CDLK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="9."><p id="ref-CR9">Sacramento, J., Costa, R. P., Bengio, Y. and Senn, W. Dendritic cortical microcircuits approximate the backpropagation algorithm. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Bengio, S. et al.) 8721–8732 (Curran Associates, 2018).</p></li><li data-counter="10."><p id="ref-CR10">Guerguiev, J., Lillicrap, T. P. &amp; Richards, B. A. Towards deep learning with segregated dendrites. <i>eLife</i> <b>6</b>, e22901 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.7554/eLife.22901" data-track-item_id="10.7554/eLife.22901" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.7554%2FeLife.22901" aria-label="Article reference 10" data-doi="10.7554/eLife.22901">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29205151" aria-label="PubMed reference 10">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5716677" aria-label="PubMed Central reference 10">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20deep%20learning%20with%20segregated%20dendrites&amp;journal=eLife&amp;doi=10.7554%2FeLife.22901&amp;volume=6&amp;publication_year=2017&amp;author=Guerguiev%2CJ&amp;author=Lillicrap%2CTP&amp;author=Richards%2CBA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="11."><p id="ref-CR11">Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. <i>Front. Comput. Neurosci.</i> <b>11</b>, 24 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3389/fncom.2017.00024" data-track-item_id="10.3389/fncom.2017.00024" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3389%2Ffncom.2017.00024" aria-label="Article reference 11" data-doi="10.3389/fncom.2017.00024">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28522969" aria-label="PubMed reference 11">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5415673" aria-label="PubMed Central reference 11">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Equilibrium%20propagation%3A%20bridging%20the%20gap%20between%20energy-based%20models%20and%20backpropagation&amp;journal=Front.%20Comput.%20Neurosci.&amp;doi=10.3389%2Ffncom.2017.00024&amp;volume=11&amp;publication_year=2017&amp;author=Scellier%2CB&amp;author=Bengio%2CY">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="12."><p id="ref-CR12">Whittington, J. C. R. &amp; Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. <i>Neural Comput.</i> <b>29</b>, 1229–1262 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/NECO_a_00949" data-track-item_id="10.1162/NECO_a_00949" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2FNECO_a_00949" aria-label="Article reference 12" data-doi="10.1162/NECO_a_00949">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28333583" aria-label="PubMed reference 12">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5467749" aria-label="PubMed Central reference 12">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20approximation%20of%20the%20error%20backpropagation%20algorithm%20in%20a%20predictive%20coding%20network%20with%20local%20hebbian%20synaptic%20plasticity&amp;journal=Neural%20Comput.&amp;doi=10.1162%2FNECO_a_00949&amp;volume=29&amp;pages=1229-1262&amp;publication_year=2017&amp;author=Whittington%2CJCR&amp;author=Bogacz%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="13."><p id="ref-CR13">Whittington, J. C. R. &amp; Bogacz, R. Theories of error back-propagation in the brain. <i>Trends Cogn. Sci.</i> <b>23</b>, 235–250 (2019).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.tics.2018.12.005" data-track-item_id="10.1016/j.tics.2018.12.005" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2018.12.005" aria-label="Article reference 13" data-doi="10.1016/j.tics.2018.12.005">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30704969" aria-label="PubMed reference 13">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6382460" aria-label="PubMed Central reference 13">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Theories%20of%20error%20back-propagation%20in%20the%20brain&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2018.12.005&amp;volume=23&amp;pages=235-250&amp;publication_year=2019&amp;author=Whittington%2CJCR&amp;author=Bogacz%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="14."><p id="ref-CR14">Song, Y., Lukasiewicz, T., Xu, Z. &amp; Bogacz, R. Can the brain do backpropagation? Exact implementation of backpropagation in predictive coding networks. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Larochell, H. et al.) 22566–22579 (Curran Associates, 2020).</p></li><li data-counter="15."><p id="ref-CR15">Tsividis, P. A., Pouncy, T., Xu, J. L., Tenenbaum, J. B. &amp; Gershman, S. J. Human learning in Atari. In <i>2017 AAAI Spring Symposium Series</i> 643–646 (Association for the Advancement of Artificial Intelligence, 2017).</p></li><li data-counter="16."><p id="ref-CR16">McCloskey, M. &amp; Cohen, N. J. Catastrophic interference in connectionist networks: the sequential learning problem. <i>Psychol. Learn. Motiv.</i> <b>24</b>, 109–165 (1989).</p></li><li data-counter="17."><p id="ref-CR17">Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. <i>Proc. Natl Acad. Sci. USA</i> <b>79</b>, 2554–2558 (1982).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1073/pnas.79.8.2554" data-track-item_id="10.1073/pnas.79.8.2554" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.79.8.2554" aria-label="Article reference 17" data-doi="10.1073/pnas.79.8.2554">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL383it1WktQ%3D%3D" aria-label="CAS reference 17">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=6953413" aria-label="PubMed reference 17">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238" aria-label="PubMed Central reference 17">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20networks%20and%20physical%20systems%20with%20emergent%20collective%20computational%20abilities&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.79.8.2554&amp;volume=79&amp;pages=2554-2558&amp;publication_year=1982&amp;author=Hopfield%2CJJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="18."><p id="ref-CR18">Rao, R. P. &amp; Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. <i>Nat. Neurosci.</i> <b>2</b>, 79–87 (1999).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/4580" data-track-item_id="10.1038/4580" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2F4580" aria-label="Article reference 18" data-doi="10.1038/4580">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK1MXhsl2ns7k%3D" aria-label="CAS reference 18">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10195184" aria-label="PubMed reference 18">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Predictive%20coding%20in%20the%20visual%20cortex%3A%20a%20functional%20interpretation%20of%20some%20extra-classical%20receptive-field%20effects&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2F4580&amp;volume=2&amp;pages=79-87&amp;publication_year=1999&amp;author=Rao%2CRP&amp;author=Ballard%2CDH">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="19."><p id="ref-CR19">Friston, K. The free-energy principle: a unified brain theory? <i>Nat. Rev. Neurosci.</i> <b>11</b>, 127–138 (2010).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nrn2787" data-track-item_id="10.1038/nrn2787" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnrn2787" aria-label="Article reference 19" data-doi="10.1038/nrn2787">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXksFGktw%3D%3D" aria-label="CAS reference 19">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20068583" aria-label="PubMed reference 19">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20free-energy%20principle%3A%20a%20unified%20brain%20theory%3F&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2Fnrn2787&amp;volume=11&amp;pages=127-138&amp;publication_year=2010&amp;author=Friston%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="20."><p id="ref-CR20">Millidge, B., Tschantz, A. &amp; Buckley, C. L. Predictive coding approximates backprop along arbitrary computation graphs. <i>Neural Comput.</i> <b>34</b>, 1329–1368 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/neco_a_01497" data-track-item_id="10.1162/neco_a_01497" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2Fneco_a_01497" aria-label="Article reference 20" data-doi="10.1162/neco_a_01497">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=35534010" aria-label="PubMed reference 20">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Predictive%20coding%20approximates%20backprop%20along%20arbitrary%20computation%20graphs&amp;journal=Neural%20Comput.&amp;doi=10.1162%2Fneco_a_01497&amp;volume=34&amp;pages=1329-1368&amp;publication_year=2022&amp;author=Millidge%2CB&amp;author=Tschantz%2CA&amp;author=Buckley%2CCL">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="21."><p id="ref-CR21">Bengio, Y. &amp; Fischer, A. Early inference in energy-based models approximates back-propagation. Preprint at <a href="https://doi.org/10.48550/arXiv.1510.02777" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1510.02777">https://doi.org/10.48550/arXiv.1510.02777</a> (2015).</p></li><li data-counter="22."><p id="ref-CR22">O’Reilly, R. C. &amp; Munakata, Y. <i>Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain</i> (MIT Press Cambridge, 2000).</p></li><li data-counter="23."><p id="ref-CR23">Quilodran, R., Rothe, M. &amp; Procyk, E. Behavioral shifts and action valuation in the anterior cingulate cortex. <i>Neuron</i> <b>57</b>, 314–325 (2008).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2007.11.031" data-track-item_id="10.1016/j.neuron.2007.11.031" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2007.11.031" aria-label="Article reference 23" data-doi="10.1016/j.neuron.2007.11.031">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1cXhvVSrt7c%3D" aria-label="CAS reference 23">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18215627" aria-label="PubMed reference 23">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Behavioral%20shifts%20and%20action%20valuation%20in%20the%20anterior%20cingulate%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2007.11.031&amp;volume=57&amp;pages=314-325&amp;publication_year=2008&amp;author=Quilodran%2CR&amp;author=Rothe%2CM&amp;author=Procyk%2CE">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="24."><p id="ref-CR24">Wallis, J. D. &amp; Kennerley, S. W. Heterogeneous reward signals in prefrontal cortex. <i>Curr. Opin. Neurobiol.</i> <b>20</b>, 191–198 (2010).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.conb.2010.02.009" data-track-item_id="10.1016/j.conb.2010.02.009" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.conb.2010.02.009" aria-label="Article reference 24" data-doi="10.1016/j.conb.2010.02.009">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXlsFCisrw%3D" aria-label="CAS reference 24">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20303739" aria-label="PubMed reference 24">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2862852" aria-label="PubMed Central reference 24">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Heterogeneous%20reward%20signals%20in%20prefrontal%20cortex&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;doi=10.1016%2Fj.conb.2010.02.009&amp;volume=20&amp;pages=191-198&amp;publication_year=2010&amp;author=Wallis%2CJD&amp;author=Kennerley%2CSW">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="25."><p id="ref-CR25">Friston, K. A theory of cortical responses. <i>Philos. Trans. R. Soc. Lond. B Biol. Sci.</i> <b>360</b>, 815–836 (2005).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1098/rstb.2005.1622" data-track-item_id="10.1098/rstb.2005.1622" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1098%2Frstb.2005.1622" aria-label="Article reference 25" data-doi="10.1098/rstb.2005.1622">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15937014" aria-label="PubMed reference 25">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1569488" aria-label="PubMed Central reference 25">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20theory%20of%20cortical%20responses&amp;journal=Philos.%20Trans.%20R.%20Soc.%20Lond.%20B%20Biol.%20Sci.&amp;doi=10.1098%2Frstb.2005.1622&amp;volume=360&amp;pages=815-836&amp;publication_year=2005&amp;author=Friston%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="26."><p id="ref-CR26">Bengio, Y. How auto-encoders could provide credit assignment in deep networks via target propagation. Preprint at <a href="https://doi.org/10.48550/arXiv.1407.7906" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1407.7906">https://doi.org/10.48550/arXiv.1407.7906</a> (2014).</p></li><li data-counter="27."><p id="ref-CR27">Meulemans, A., Carzaniga, F., Suykens, J., Sacramento, J. &amp; Grewe, B. F. A theoretical framework for target propagation. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Larochelle, H. et al.) 20024–20036 (Curran Associates, 2020).</p></li><li data-counter="28."><p id="ref-CR28">Felleman, D. J. &amp; Van Essen, D. C. Distributed hierarchical processing in the primate cerebral cortex. <i>Cereb. Cortex</i> <b>1</b>, 1–47 (1991).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/cercor/1.1.1" data-track-item_id="10.1093/cercor/1.1.1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2F1.1.1" aria-label="Article reference 28" data-doi="10.1093/cercor/1.1.1">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK38zltlGmsg%3D%3D" aria-label="CAS reference 28">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=1822724" aria-label="PubMed reference 28">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20hierarchical%20processing%20in%20the%20primate%20cerebral%20cortex&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2F1.1.1&amp;volume=1&amp;pages=1-47&amp;publication_year=1991&amp;author=Felleman%2CDJ&amp;author=Essen%2CDC">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="29."><p id="ref-CR29">Fontenla-Romero, Ó., Guijarro-Berdiñas, B., Martinez-Rego, D., Pérez-Sánchez, B. &amp; Peteiro-Barral, D. Online machine learning. In <i>Efficiency and Scalability Methods for Computational Intellect</i> (eds Igelnik, B. &amp; Zurada, J. M.) 27–54 (IGI Global, 2013).</p></li><li data-counter="30."><p id="ref-CR30">Hassabis, D., Kumaran, D., Summerfield, C. &amp; Botvinick, M. Neuroscience-inspired artificial intelligence. <i>Neuron</i> <b>95</b>, 245–258 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2017.06.011" data-track-item_id="10.1016/j.neuron.2017.06.011" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2017.06.011" aria-label="Article reference 30" data-doi="10.1016/j.neuron.2017.06.011">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXht1Smtb%2FE" aria-label="CAS reference 30">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28728020" aria-label="PubMed reference 30">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuroscience-inspired%20artificial%20intelligence&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2017.06.011&amp;volume=95&amp;pages=245-258&amp;publication_year=2017&amp;author=Hassabis%2CD&amp;author=Kumaran%2CD&amp;author=Summerfield%2CC&amp;author=Botvinick%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="31."><p id="ref-CR31">Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M. &amp; Bouchachia, A. A survey on concept drift adaptation. <i>ACM Comput. Surv.</i> <b>46</b>, 1–37 (2014).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/2523813" data-track-item_id="10.1145/2523813" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F2523813" aria-label="Article reference 31" data-doi="10.1145/2523813">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20concept%20drift%20adaptation&amp;journal=ACM%20Comput.%20Surv.&amp;doi=10.1145%2F2523813&amp;volume=46&amp;pages=1-37&amp;publication_year=2014&amp;author=Gama%2CJ&amp;author=%C5%BDliobait%C4%97%2CI&amp;author=Bifet%2CA&amp;author=Pechenizkiy%2CM&amp;author=Bouchachia%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="32."><p id="ref-CR32">Puri, R., Kirby, R., Yakovenko, N. &amp; Catanzaro, B. Large scale language modeling: converging on 40 GB of text in four hours. In <i>2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)</i> 290–297 (IEEE, 2018).</p></li><li data-counter="33."><p id="ref-CR33">Ioffe, S. &amp; Szegedy, C. Batch normalization: accelerating deep network training by reducing internal covariate shift. In <i>Proceedings of the International Conference on Machine Learning (ICML)</i> (eds Bach, F. &amp; Blei, D.) 448–456 (PMLR, 2015).</p></li><li data-counter="34."><p id="ref-CR34">Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. In <i>Proc. 34th International Conference on Machine Learning</i> (eds Precup, D. &amp; Teh, Y. W.) 3987–3995 (PMLR, 2017).</p></li><li data-counter="35."><p id="ref-CR35">O’Shea, K. &amp; Nash, R. An introduction to convolutional neural networks. Preprint at <a href="https://doi.org/10.48550/arXiv.1511.08458" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1511.08458">https://doi.org/10.48550/arXiv.1511.08458</a> (2015).</p></li><li data-counter="36."><p id="ref-CR36">Krizhevsky, A. &amp; Hinton, G. <i>Learning Multiple Layers of Features from Tiny Images</i>. Master’s thesis, Univ. Toronto (2009).</p></li><li data-counter="37."><p id="ref-CR37">Sutton, R. S. &amp; Barto, A. G. <i>Introduction to Reinforcement Learning</i>, Vol. 2 (MIT Press Cambridge, 1998).</p></li><li data-counter="38."><p id="ref-CR38">Hampton, A. N., Bossaerts, P. &amp; O’Doherty, J. P. The role of the ventromedial prefrontal cortex in abstract state-based inference during decision making in humans. <i>J. Neurosci.</i> <b>26</b>, 8360–8367 (2006).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1010-06.2006" data-track-item_id="10.1523/JNEUROSCI.1010-06.2006" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1010-06.2006" aria-label="Article reference 38" data-doi="10.1523/JNEUROSCI.1010-06.2006">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD28Xos1Kju74%3D" aria-label="CAS reference 38">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16899731" aria-label="PubMed reference 38">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6673813" aria-label="PubMed Central reference 38">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20the%20ventromedial%20prefrontal%20cortex%20in%20abstract%20state-based%20inference%20during%20decision%20making%20in%20humans&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1010-06.2006&amp;volume=26&amp;pages=8360-8367&amp;publication_year=2006&amp;author=Hampton%2CAN&amp;author=Bossaerts%2CP&amp;author=O%E2%80%99Doherty%2CJP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="39."><p id="ref-CR39">Heald, J. B., Lengyel, M. &amp; Wolpert, D. M. Contextual inference underlies the learning of sensorimotor repertoires. <i>Nature</i> <b>600</b>, 489–493 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-021-04129-3" data-track-item_id="10.1038/s41586-021-04129-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-021-04129-3" aria-label="Article reference 39" data-doi="10.1038/s41586-021-04129-3">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXisFyrsbfN" aria-label="CAS reference 39">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34819674" aria-label="PubMed reference 39">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8809113" aria-label="PubMed Central reference 39">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Contextual%20inference%20underlies%20the%20learning%20of%20sensorimotor%20repertoires&amp;journal=Nature&amp;doi=10.1038%2Fs41586-021-04129-3&amp;volume=600&amp;pages=489-493&amp;publication_year=2021&amp;author=Heald%2CJB&amp;author=Lengyel%2CM&amp;author=Wolpert%2CDM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="40."><p id="ref-CR40">Larsen, T., Leslie, D. S., Collins, E. J. &amp; Bogacz, R. Posterior weighted reinforcement learning with state uncertainty. <i>Neural Comput.</i> <b>22</b>, 1149–1179 (2010).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/neco.2010.01-09-948" data-track-item_id="10.1162/neco.2010.01-09-948" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2Fneco.2010.01-09-948" aria-label="Article reference 40" data-doi="10.1162/neco.2010.01-09-948">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20100078" aria-label="PubMed reference 40">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Posterior%20weighted%20reinforcement%20learning%20with%20state%20uncertainty&amp;journal=Neural%20Comput.&amp;doi=10.1162%2Fneco.2010.01-09-948&amp;volume=22&amp;pages=1149-1179&amp;publication_year=2010&amp;author=Larsen%2CT&amp;author=Leslie%2CDS&amp;author=Collins%2CEJ&amp;author=Bogacz%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="41."><p id="ref-CR41">Kaufman, M. A. &amp; Bolles, R. C. A nonassociative aspect of overshadowing. <i>Bull. Psychonomic Soc.</i> <b>18</b>, 318–320 (1981).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3758/BF03333639" data-track-item_id="10.3758/BF03333639" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3758%2FBF03333639" aria-label="Article reference 41" data-doi="10.3758/BF03333639">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20nonassociative%20aspect%20of%20overshadowing&amp;journal=Bull.%20Psychonomic%20Soc.&amp;doi=10.3758%2FBF03333639&amp;volume=18&amp;pages=318-320&amp;publication_year=1981&amp;author=Kaufman%2CMA&amp;author=Bolles%2CRC">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="42."><p id="ref-CR42">Matzel, L. D., Schachtman, T. R. &amp; Miller, R. R. Recovery of an overshadowed association achieved by extinction of the overshadowing stimulus. <i>Learn. Motiv.</i> <b>16</b>, 398–412 (1985).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0023-9690(85)90023-2" data-track-item_id="10.1016/0023-9690(85)90023-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0023-9690%2885%2990023-2" aria-label="Article reference 42" data-doi="10.1016/0023-9690(85)90023-2">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Recovery%20of%20an%20overshadowed%20association%20achieved%20by%20extinction%20of%20the%20overshadowing%20stimulus&amp;journal=Learn.%20Motiv.&amp;doi=10.1016%2F0023-9690%2885%2990023-2&amp;volume=16&amp;pages=398-412&amp;publication_year=1985&amp;author=Matzel%2CLD&amp;author=Schachtman%2CTR&amp;author=Miller%2CRR">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="43."><p id="ref-CR43">Poort, J. et al. Learning enhances sensory and multiple non-sensory representations in primary visual cortex. <i>Neuron</i> <b>86</b>, 1478–1490 (2015).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2015.05.037" data-track-item_id="10.1016/j.neuron.2015.05.037" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2015.05.037" aria-label="Article reference 43" data-doi="10.1016/j.neuron.2015.05.037">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2MXhtVSqt73M" aria-label="CAS reference 43">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26051421" aria-label="PubMed reference 43">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4503798" aria-label="PubMed Central reference 43">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20enhances%20sensory%20and%20multiple%20non-sensory%20representations%20in%20primary%20visual%20cortex&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2015.05.037&amp;volume=86&amp;pages=1478-1490&amp;publication_year=2015&amp;author=Poort%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="44."><p id="ref-CR44">McClelland, J. L., McNaughton, B. L. &amp; O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. <i>Psychol. Rev.</i> <b>102</b>, 419–457 (1995).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1037/0033-295X.102.3.419" data-track-item_id="10.1037/0033-295X.102.3.419" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0033-295X.102.3.419" aria-label="Article reference 44" data-doi="10.1037/0033-295X.102.3.419">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7624455" aria-label="PubMed reference 44">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Why%20there%20are%20complementary%20learning%20systems%20in%20the%20hippocampus%20and%20neocortex%3A%20insights%20from%20the%20successes%20and%20failures%20of%20connectionist%20models%20of%20learning%20and%20memory&amp;journal=Psychol.%20Rev.&amp;doi=10.1037%2F0033-295X.102.3.419&amp;volume=102&amp;pages=419-457&amp;publication_year=1995&amp;author=McClelland%2CJL&amp;author=McNaughton%2CBL&amp;author=O%E2%80%99Reilly%2CRC">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="45."><p id="ref-CR45">Dauwels, J. On variational message passing on factor graphs. In <i>2007 IEEE International Symposium on Information Theory</i>, 2546–2550 (IEEE, 2007).</p></li><li data-counter="46."><p id="ref-CR46">Anil Meera, A. &amp; Wisse, M. Dynamic expectation maximization algorithm for estimation of linear systems with colored noise. <i>Entropy</i> <b>23</b>, 1306 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3390/e23101306" data-track-item_id="10.3390/e23101306" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3390%2Fe23101306" aria-label="Article reference 46" data-doi="10.3390/e23101306">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34682030" aria-label="PubMed reference 46">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8534782" aria-label="PubMed Central reference 46">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20expectation%20maximization%20algorithm%20for%20estimation%20of%20linear%20systems%20with%20colored%20noise&amp;journal=Entropy&amp;doi=10.3390%2Fe23101306&amp;volume=23&amp;publication_year=2021&amp;author=Anil%20Meera%2CA&amp;author=Wisse%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="47."><p id="ref-CR47">Friston, K. Hierarchical models in the brain. <i>PLoS Comput. Biol.</i> <b>4</b>, e1000211 (2008).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1000211" data-track-item_id="10.1371/journal.pcbi.1000211" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1000211" aria-label="Article reference 47" data-doi="10.1371/journal.pcbi.1000211">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18989391" aria-label="PubMed reference 47">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570625" aria-label="PubMed Central reference 47">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20models%20in%20the%20brain&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1000211&amp;volume=4&amp;publication_year=2008&amp;author=Friston%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="48."><p id="ref-CR48">Meulemans, A., Farinha, M. T., Cervera, M. R., Sacramento, J. &amp; Grewe, B. F. Minimizing control for credit assignment with strong feedback. In <i>Proc. of Machine Learning Research</i> (eds Chaudhuri, K. et al.) 15458–15483 (PMLR, 2022).</p></li><li data-counter="49."><p id="ref-CR49">Meulemans, A., Zucchet, N., Kobayashi, S., von Oswald, J. &amp; Sacramento, J. The least-control principle for learning at equilibrium. <i>Adv. Neural Inf. Process. Syst.</i> <b>35</b>, 33603–33617 (2022).</p></li><li data-counter="50."><p id="ref-CR50">Gilra, A. &amp; Gerstner, W. Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. <i>eLife</i> <b>6</b>, e28295 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.7554/eLife.28295" data-track-item_id="10.7554/eLife.28295" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.7554%2FeLife.28295" aria-label="Article reference 50" data-doi="10.7554/eLife.28295">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29173280" aria-label="PubMed reference 50">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5730383" aria-label="PubMed Central reference 50">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20non-linear%20dynamics%20by%20stable%20local%20learning%20in%20a%20recurrent%20spiking%20neural%20network&amp;journal=eLife&amp;doi=10.7554%2FeLife.28295&amp;volume=6&amp;publication_year=2017&amp;author=Gilra%2CA&amp;author=Gerstner%2CW">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="51."><p id="ref-CR51">Haider, P. et al. Latent equilibrium: a unified learning theory for arbitrarily fast computation with arbitrarily slow neurons. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Ranzato, M. et al.) 17839–17851 (2021).</p></li><li data-counter="52."><p id="ref-CR52">Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T. &amp; Tweed, D. B. Deep learning without weight transport. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Wallach, H. et al.) (Curran Associates, 2019).</p></li><li data-counter="53."><p id="ref-CR53">Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. <i>Nat. Commun.</i> <b>7</b>, 13276 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/ncomms13276" data-track-item_id="10.1038/ncomms13276" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fncomms13276" aria-label="Article reference 53" data-doi="10.1038/ncomms13276">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhvVehtLrM" aria-label="CAS reference 53">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27824044" aria-label="PubMed reference 53">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5105169" aria-label="PubMed Central reference 53">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fncomms13276&amp;volume=7&amp;publication_year=2016&amp;author=Lillicrap%2CTP&amp;author=Cownden%2CD&amp;author=Tweed%2CDB&amp;author=Akerman%2CCJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="54."><p id="ref-CR54">Millidge, B., Tschantz, A. &amp; Buckley, C. L. Relaxing the constraints on predictive coding models. Preprint at <a href="https://doi.org/10.48550/arXiv.2010.01047" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2010.01047">https://doi.org/10.48550/arXiv.2010.01047</a> (2020).</p></li><li data-counter="55."><p id="ref-CR55">Salvatori, T. et al. Incremental predictive coding: a parallel and fully automatic learning algorithm. Preprint at <a href="https://doi.org/10.48550/arXiv.2212.00720" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.2212.00720">https://doi.org/10.48550/arXiv.2212.00720</a> (2022).</p></li><li data-counter="56."><p id="ref-CR56">Friston, K. J., Trujillo-Barreto, N. &amp; Daunizeau, J. Dem: a variational treatment of dynamic systems. <i>NeuroImage</i> <b>41</b>, 849–885 (2008).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2008.02.054" data-track-item_id="10.1016/j.neuroimage.2008.02.054" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2008.02.054" aria-label="Article reference 56" data-doi="10.1016/j.neuroimage.2008.02.054">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD1czkvFWqtg%3D%3D" aria-label="CAS reference 56">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18434205" aria-label="PubMed reference 56">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Dem%3A%20a%20variational%20treatment%20of%20dynamic%20systems&amp;journal=NeuroImage&amp;doi=10.1016%2Fj.neuroimage.2008.02.054&amp;volume=41&amp;pages=849-885&amp;publication_year=2008&amp;author=Friston%2CKJ&amp;author=Trujillo-Barreto%2CN&amp;author=Daunizeau%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="57."><p id="ref-CR57">Millidge, B., Tang, M., Osanlouy, M. &amp; Bogacz, R. Predictive coding networks for temporal prediction. Preprint at <i>bioRxiv</i> <a href="https://doi.org/10.1101/2023.05.15.540906" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1101/2023.05.15.540906">https://doi.org/10.1101/2023.05.15.540906</a> (2023).</p></li><li data-counter="58."><p id="ref-CR58">Salvatori, T. et al. Learning on arbitrary graph topologies via predictive coding. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Koyejo, S. et al.) 38232–38244 (Curran Associates, 2022).</p></li><li data-counter="59."><p id="ref-CR59">Foroushani, A. N., Assaf, H., Noshahr, F. H., Savaria, Y. &amp; Sawan, M. Analog circuits to accelerate the relaxation process in the equilibrium propagation algorithm. In <i>2020 IEEE International Symposium on Circuits and Systems (ISCAS)</i> 1–5 (IEEE, 2020).</p></li><li data-counter="60."><p id="ref-CR60">Xiao, H., Rasul, K. &amp; Vollgraf, R. Fashion MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at <a href="https://doi.org/10.48550/arXiv.1708.07747" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1708.07747">https://doi.org/10.48550/arXiv.1708.07747</a> (2017).</p></li><li data-counter="61."><p id="ref-CR61">Goodfellow, I., Bengio, Y. &amp; Courville, A. <i>Deep Learning</i> (MIT Press Cambridge, 2016).</p></li><li data-counter="62."><p id="ref-CR62">O’Reilly, R. C. Biologically plausible error-driven learning using local activation differences: the generalized recirculation algorithm. <i>Neural Comput.</i> <b>8</b>, 895–938 (1996).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/neco.1996.8.5.895" data-track-item_id="10.1162/neco.1996.8.5.895" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2Fneco.1996.8.5.895" aria-label="Article reference 62" data-doi="10.1162/neco.1996.8.5.895">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Biologically%20plausible%20error-driven%20learning%20using%20local%20activation%20differences%3A%20the%20generalized%20recirculation%20algorithm&amp;journal=Neural%20Comput.&amp;doi=10.1162%2Fneco.1996.8.5.895&amp;volume=8&amp;pages=895-938&amp;publication_year=1996&amp;author=O%E2%80%99Reilly%2CRC">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="63."><p id="ref-CR63">Almeida, L. B. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In <i>Artificial Neural Networks: Concept Learning</i> (ed. Diederich, J.) 102–111 (IEEE Computer Society Press, 1990).</p></li><li data-counter="64."><p id="ref-CR64">Pineda, F. Generalization of back propagation to recurrent and higher order neural networks. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (ed. Anderson, D.) 602–611 (Curran Associates, 1987).</p></li><li data-counter="65."><p id="ref-CR65">Pineda, F. J. Dynamics and architecture for neural computation. <i>J. Complex.</i> <b>4</b>, 216–245 (1988).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0885-064X(88)90021-0" data-track-item_id="10.1016/0885-064X(88)90021-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0885-064X%2888%2990021-0" aria-label="Article reference 65" data-doi="10.1016/0885-064X(88)90021-0">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamics%20and%20architecture%20for%20neural%20computation&amp;journal=J.%20Complex.&amp;doi=10.1016%2F0885-064X%2888%2990021-0&amp;volume=4&amp;pages=216-245&amp;publication_year=1988&amp;author=Pineda%2CFJ">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="66."><p id="ref-CR66">Hebb, D. O. <i>The Organisation of Behaviour: A Neuropsychological Theory</i> (Science Editions New York, 1949).</p></li><li data-counter="67."><p id="ref-CR67">Senn, W. et al. A neuronal least-action principle for real-time learning in cortical circuits. Preprint at <i>bioRxiv</i> <a href="https://doi.org/10.1101/2023.03.25.534198" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1101/2023.03.25.534198">https://doi.org/10.1101/2023.03.25.534198</a> (2023).</p></li><li data-counter="68."><p id="ref-CR68">Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In <i>Proc. 13th International Conference on Artificial Intelligence and Statistics</i> (eds Teh, Y. W. &amp; Titterington, M.) 249–256 (PMLR, 2010).</p></li><li data-counter="69."><p id="ref-CR69">Tolstikhin, I. O. et al. Mlp-mixer: an all-mlp architecture for vision. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Ranzato, M. et al.) 24261–24272 (Curran Associates, 2021).</p></li><li data-counter="70."><p id="ref-CR70">Žliobaitė, I. Learning under concept drift: an overview. Preprint at <a href="https://doi.org/10.48550/arXiv.1010.4784" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1010.4784">https://doi.org/10.48550/arXiv.1010.4784</a> (2010).</p></li><li data-counter="71."><p id="ref-CR71">Tsymbal, A. <i>The Problem of Concept Drift: Definitions and Related Work</i>. Technical report, Computer Science Department, Trinity College Dublin (2004).</p></li><li data-counter="72."><p id="ref-CR72">Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. <a href="https://doi.org/10.48550/arXiv.1412.6980" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</a> (2014).</p></li><li data-counter="73."><p id="ref-CR73">Salvatori, T., Song, Y., Lukasiewicz, T., Bogacz, R. &amp; Xu, Z. Reverse differentiation via predictive coding. In <i>Proc. 36th AAAI Conference on Artificial Intelligence</i> (Salvatori, T., Song, Y., Xu, Z., Lukasiewicz, T. &amp; Bogacz, R.) 8150–8158 (Curran Associates, 2022).</p></li><li data-counter="74."><p id="ref-CR74">Sutton, R. S. Generalization in reinforcement learning: successful examples using sparse coarse coding. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i> (eds Touretzky, D. et al.) 1038–1044 (NIPS, 1995).</p></li><li data-counter="75."><p id="ref-CR75">Geramifard, A., Dann, C., Klein, R. H., Dabney, W. &amp; How, J. P. RLPy: a value-function-based reinforcement learning framework for education and research. <i>J. Mach. Learn. Res.</i> <b>16</b>, 1573–1578 (2015).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 75" href="http://scholar.google.com/scholar_lookup?&amp;title=RLPy%3A%20a%20value-function-based%20reinforcement%20learning%20framework%20for%20education%20and%20research&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=16&amp;pages=1573-1578&amp;publication_year=2015&amp;author=Geramifard%2CA&amp;author=Dann%2CC&amp;author=Klein%2CRH&amp;author=Dabney%2CW&amp;author=How%2CJP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="76."><p id="ref-CR76">Moore, A. Efficient memory-based learning for robot control. Technical report, Carnegie Mellon Univ. (1990).</p></li><li data-counter="77."><p id="ref-CR77">Barto, A. G., Sutton, R. S. &amp; Anderson, C. W. Neuronlike adaptive elements that can solve difficult learning control problems. In <i>IEEE Transactions on Systems, Man, and Cybernetics</i>, 834–846 (1983).</p></li><li data-counter="78."><p id="ref-CR78">Brockman, G. et al. OpenAI Gym. Preprint at <a href="https://doi.org/10.48550/arXiv.1606.01540" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.48550/arXiv.1606.01540">https://doi.org/10.48550/arXiv.1606.01540</a> (2016).</p></li><li data-counter="79."><p id="ref-CR79">Welford, B. P. Note on a method for calculating corrected sums of squares and products. <i>Technometrics</i> <b>4</b>, 419–420 (1962).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/00401706.1962.10490022" data-track-item_id="10.1080/00401706.1962.10490022" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F00401706.1962.10490022" aria-label="Article reference 79" data-doi="10.1080/00401706.1962.10490022">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 79" href="http://scholar.google.com/scholar_lookup?&amp;title=Note%20on%20a%20method%20for%20calculating%20corrected%20sums%20of%20squares%20and%20products&amp;journal=Technometrics&amp;doi=10.1080%2F00401706.1962.10490022&amp;volume=4&amp;pages=419-420&amp;publication_year=1962&amp;author=Welford%2CBP">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="80."><p id="ref-CR80">Knuth, D. E. <i>Art of Computer Programming</i>, Vol. 2 (Addison-Wesley Professional, 2014).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-023-01514-1?format=refman&amp;flavour=references">Download references</a></p></div></div><div id="Ack1-section" data-title="Acknowledgements"><h2 id="Ack1">Acknowledgements</h2><p>We thank T. Behrens for comments on the manuscript and A. Saxe and M. Witbrock for discussions. The presented research was supported by the following grants: China Scholarship Council under the State Scholarship Fund (Y.S.), JPMorgan AI Research Awards (Y.S.), Biotechnology and Biological Sciences Research Council grant BB/S006338/1 (R.B.), Medical Research Council grant MC_UU_00003/1 (R.B.), the Alan Turing Institute under the EPSRC grant EP/N510129/1 (T.L.), the AXA Research Fund (T.L.), National Natural Science Foundation of China grants 61906063 and 62276089 (Z.X.), Natural Science Foundation of Hebei Province, China, grant F2021202064 (Z.X.), Natural Science Foundation of Tianjin City, China, grant 19JCQNJC00400 (Z.X.), the ‘100 Talents Plan’ of Hebei Province, China, grant E2019050017 (Z.X.) and the Yuanguang Scholar Fund of Hebei University of Technology, China (Z.X.). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. This research was also funded, in part, by JPMorgan Chase &amp; Co. Any views or opinions expressed herein are solely those of the authors listed and may differ from the views and opinions expressed by JPMorgan Chase &amp; Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities, LLC. This material should not be construed as an individual recommendation for any particular client and is not intended as a recommendation of particular securities, financial instruments or strategies for a particular client. This material does not constitute a solicitation or offer in any jurisdiction.</p></div><div id="author-information-section" aria-labelledby="author-information" data-title="Author information"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Department of Computer Science, University of Oxford, Oxford, UK</p><p>Yuhang Song,&nbsp;Tommaso Salvatori,&nbsp;Thomas Lukasiewicz&nbsp;&amp;&nbsp;Zhenghua Xu</p></li><li id="Aff2"><p>Medical Research Council Brain Network Dynamics Unit, University of Oxford, Oxford, UK</p><p>Yuhang Song,&nbsp;Beren Millidge&nbsp;&amp;&nbsp;Rafal Bogacz</p></li><li id="Aff3"><p>Fractile, Ltd., London, UK</p><p>Yuhang Song</p></li><li id="Aff4"><p>Institute of Logic and Computation, Vienna University of Technology, Vienna, Austria</p><p>Tommaso Salvatori&nbsp;&amp;&nbsp;Thomas Lukasiewicz</p></li><li id="Aff5"><p>VERSES AI Research Lab, Los Angeles, CA, USA</p><p>Tommaso Salvatori</p></li><li id="Aff6"><p>State Key Laboratory of Reliability and Intelligence of Electrical Equipment, School of Health Sciences and Biomedical Engineering, Hebei University of Technology, Tianjin, China</p><p>Zhenghua Xu</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Yuhang-Song-Aff1-Aff2-Aff3"><span>Yuhang Song</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yuhang%20Song" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yuhang%20Song%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Beren-Millidge-Aff2"><span>Beren Millidge</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Beren%20Millidge" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Beren%20Millidge%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Tommaso-Salvatori-Aff1-Aff4-Aff5"><span>Tommaso Salvatori</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tommaso%20Salvatori" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tommaso%20Salvatori%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Thomas-Lukasiewicz-Aff1-Aff4"><span>Thomas Lukasiewicz</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thomas%20Lukasiewicz" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thomas%20Lukasiewicz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Zhenghua-Xu-Aff1-Aff6"><span>Zhenghua Xu</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zhenghua%20Xu" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zhenghua%20Xu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li><li id="auth-Rafal-Bogacz-Aff2"><span>Rafal Bogacz</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rafal%20Bogacz" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rafal%20Bogacz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li></ol></div><h3 id="contributions">Contributions</h3><p>Y.S. and R.B. conceived the project. Y.S., R.B., B.M. and T.S. contributed ideas for experiments and analysis. Y.S. and B.M. performed simulations. Y.S., B.M. and R.B. performed mathematical analyses. Y.S., T.L. and R.B. managed the project. T.L and Z.X. advised on the project. Y.S., R.B. and B.M. wrote the paper. T.S., T.L. and Z.X. provided revisions to the paper.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:yuhang.song@bndu.ox.ac.uk">Yuhang Song</a>, <a id="corresp-c2" href="mailto:thomas.lukasiewicz@cs.ox.ac.uk">Thomas Lukasiewicz</a>, <a id="corresp-c3" href="mailto:zhenghua.xu@hebut.edu.cn">Zhenghua Xu</a> or <a id="corresp-c4" href="mailto:rafal.bogacz@ndcn.ox.ac.uk">Rafal Bogacz</a>.</p></div></div><div id="ethics-section" data-title="Ethics declarations"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar4">Competing interests</h3>
                <p>Y.S., B.M. and R.B. are shareholders in Fractile, Ltd., which designs artificial intelligence accelerator hardware. The remaining authors declare no competing interests.</p>
              
            </div></div><div id="peer-review-section" data-title="Peer review"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar3">Peer review information</h3>
                <p><i>Nature Neuroscience</i> thanks Karl Friston, Walter Senn, Friedemann Zenke and Joel Zylberberg for their contribution to the peer review of this work.</p>
              
            </div></div><div id="additional-information-section" data-title="Additional information"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div><div id="Sec19-section" data-title="Supplementary information"><h2 id="Sec19">Supplementary information</h2></div><div id="Sec20-section" data-title="Source data"><h2 id="Sec20">Source data</h2></div><div id="rightslink-section" data-title="Rights and permissions"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Inferring%20neural%20activity%20before%20plasticity%20as%20a%20foundation%20for%20learning%20beyond%20backpropagation&amp;author=Yuhang%20Song%20et%20al&amp;contentID=10.1038%2Fs41593-023-01514-1&amp;copyright=The%20Author%28s%29&amp;publication=1097-6256&amp;publicationDate=2024-01-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div><div id="article-info-section" aria-labelledby="article-info" data-title="About this article"><h2 id="article-info">About this article</h2><div id="article-info-content"><p><a data-crossmark="10.1038/s41593-023-01514-1" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41593-023-01514-1" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></p><div><h3 id="citeas">Cite this article</h3><p>Song, Y., Millidge, B., Salvatori, T. <i>et al.</i> Inferring neural activity before plasticity as a foundation for learning beyond backpropagation.
                    <i>Nat Neurosci</i> <b>27</b>, 348–358 (2024). https://doi.org/10.1038/s41593-023-01514-1</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41593-023-01514-1?format=refman&amp;flavour=citation">Download citation</a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2022-05-18">18 May 2022</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2023-11-02">02 November 2023</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2024-01-03">03 January 2024</time></span></p></li><li><p>Issue Date<span>: </span><span><time datetime="2024-02">February 2024</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41593-023-01514-1</span></p></li></ul></div></div></div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[QwQ: Alibaba's O1 Like Reasoning LLM (211 pts)]]></title>
            <link>https://qwenlm.github.io/blog/qwq-32b-preview/</link>
            <guid>42259184</guid>
            <pubDate>Wed, 27 Nov 2024 20:00:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwenlm.github.io/blog/qwq-32b-preview/">https://qwenlm.github.io/blog/qwq-32b-preview/</a>, See on <a href="https://news.ycombinator.com/item?id=42259184">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Please add a pair of parentheses to the incorrect equation: 1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479, to make the equation true.</p><div><p>Let’s tackle this problem step by step. The equation given is:</p><p>1 + 2 * 3 + 4 * 5 + 6 * 7 + 8 * 9 = 479</p><p>But it’s incorrect, and I need to add a pair of parentheses to make it true. Okay, first things first, I should understand what the current value of the left side is without any parentheses.</p><p>Let’s recall the order of operations: PEMDAS (Parentheses, Exponents, Multiplication and Division, Addition and Subtraction). So, multiplication comes before addition.</p><p>So, calculating step by step:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>Now, plugging these back into the equation:</p><p>1 + 6 + 20 + 42 + 72</p><p>Adding these up:</p><p>1 + 6 = 7</p><p>7 + 20 = 27</p><p>27 + 42 = 69</p><p>69 + 72 = 141</p><p>So, without any parentheses, the left side equals 141, but it should be 479. That’s quite a difference. So, I need to add a pair of parentheses to make it equal to 479.</p><p>I need to think about where to place the parentheses to alter the order of operations to achieve the desired result.</p><p>Let me consider different possibilities:</p><ol><li>Parenthesizing individual terms:</li></ol><p>For example, (1 + 2) * 3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Calculate inside the parentheses first:</p><p>(1 + 2) = 3</p><p>Then, 3 * 3 = 9</p><p>Now, the equation becomes:</p><p>9 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 9 + 20 + 42 + 72 = 143</p><p>Still not 479.</p><ol start="2"><li>Parenthesizing more terms:</li></ol><p>What if I do (1 + 2 * 3) + 4 * 5 + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>1 + 2 * 3</p><p>Here, multiplication comes before addition, so 2 * 3 = 6, then 1 + 6 = 7</p><p>So, the equation becomes:</p><p>7 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Again, following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 7 + 20 + 42 + 72 = 141</p><p>Same as before.</p><ol start="3"><li>Let’s try (1 + 2) * (3 + 4) * 5 + 6 * 7 + 8 * 9</li></ol><p>First, calculate inside the parentheses:</p><p>(1 + 2) = 3</p><p>(3 + 4) = 7</p><p>So, 3 * 7 * 5 = 105</p><p>Now, the equation becomes:</p><p>105 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 105 + 42 + 72 = 219</p><p>Still not 479.</p><ol start="4"><li>Maybe trying to group more terms:</li></ol><p>(1 + 2 * 3 + 4) * 5 + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>1 + 2 * 3 + 4</p><p>Multiplication first: 2 * 3 = 6</p><p>Then, 1 + 6 + 4 = 11</p><p>So, 11 * 5 = 55</p><p>Now, the equation becomes:</p><p>55 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 55 + 42 + 72 = 169</p><p>Getting closer, but still not 479.</p><ol start="5"><li>Let’s try a different approach. Maybe grouping the additions together:</li></ol><p>1 + (2 * 3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the parentheses:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 6 + 20 + 42 + 72 = 140</p><p>Then, 1 + 140 = 141</p><p>Same as before.</p><ol start="6"><li>Perhaps grouping two multiplications together:</li></ol><p>1 + 2 * (3 + 4) * 5 + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>(3 + 4) = 7</p><p>So, 2 * 7 * 5 = 70</p><p>Now, the equation becomes:</p><p>1 + 70 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 1 + 70 + 42 + 72 = 185</p><p>Still not 479.</p><ol start="7"><li>Maybe grouping more multiplications:</li></ol><p>1 + 2 * 3 + 4 * (5 + 6) * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>(5 + 6) = 11</p><p>So, 4 * 11 * 7 = 308</p><p>Now, the equation becomes:</p><p>1 + 2 * 3 + 308 + 8 * 9</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>8 * 9 = 72</p><p>So, 1 + 6 + 308 + 72 = 387</p><p>Closer, but still not 479.</p><ol start="8"><li>Let’s try grouping the last terms:</li></ol><p>1 + 2 * 3 + 4 * 5 + 6 * (7 + 8) * 9</p><p>First, calculate inside the parentheses:</p><p>(7 + 8) = 15</p><p>So, 6 * 15 * 9 = 810</p><p>Now, the equation becomes:</p><p>1 + 2 * 3 + 4 * 5 + 810</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>So, 1 + 6 + 20 + 810 = 837</p><p>Now that’s way over 479.</p><p>Maybe I need to group terms in a way that subtraction is implied, but since there are no subtractions in the original equation, perhaps I need to consider that.</p><p>Wait, maybe the problem allows for adding parentheses to change the operations indirectly.</p><p>Alternatively, perhaps the parentheses need to group additions in a specific way.</p><p>Let me try grouping the first few terms:</p><p>(1 + 2 * 3 + 4 * 5) + 6 * 7 + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>So, 1 + 6 + 20 = 27</p><p>Now, the equation becomes:</p><p>27 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 27 + 42 + 72 = 141</p><p>Same as before.</p><ol start="9"><li>Maybe grouping further:</li></ol><p>(1 + 2 * 3 + 4 * 5 + 6 * 7) + 8 * 9</p><p>First, calculate inside the parentheses:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>So, 1 + 6 + 20 + 42 = 69</p><p>Now, the equation becomes:</p><p>69 + 8 * 9</p><p>Following order of operations:</p><p>8 * 9 = 72</p><p>So, 69 + 72 = 141</p><p>Still not 479.</p><ol start="10"><li>Let’s try grouping differently:</li></ol><p>1 + 2 * (3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the parentheses:</p><p>3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations inside:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 3 + 20 + 42 + 72 = 137</p><p>Now, 2 * 137 = 274</p><p>Then, the equation becomes:</p><p>1 + 274 = 275</p><p>Still not 479.</p><ol start="11"><li>Maybe trying to group the additions and multiplications in a different way:</li></ol><p>1 + (2 * 3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>Wait, I did this earlier and got 141.</p><p>Hmm.</p><p>Let me think differently. Maybe I need to group terms to create a larger multiplication.</p><p>For example:</p><p>1 + 2 * (3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the parentheses:</p><p>3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 3 + 20 + 42 + 72 = 137</p><p>Now, 2 * 137 = 274</p><p>Then, 1 + 274 = 275</p><p>Still not 479.</p><ol start="12"><li>Let’s try another approach:</li></ol><p>(1 + 2) * (3 + 4) * (5 + 6) * (7 + 8) * 9</p><p>Wait, that’s adding multiple sets of parentheses, which is not allowed since I can only add one pair.</p><p>Back to the original problem: only one pair of parentheses can be added.</p><p>Maybe I need to group a subtraction or division, but those operations aren’t present in the original equation.</p><p>Wait, perhaps I need to consider that adding parentheses can create a negative term, but again, there are no subtractions in the original equation.</p><p>Alternatively, maybe the parentheses are meant to group additions in a specific way to create a larger number.</p><p>Let me try grouping more terms together:</p><p>1 + 2 * (3 + 4 * 5 + 6 * 7 + 8 * 9)</p><p>First, inside the parentheses:</p><p>3 + 4 * 5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 3 + 20 + 42 + 72 = 137</p><p>Now, 2 * 137 = 274</p><p>Then, 1 + 274 = 275</p><p>Same as before.</p><ol start="13"><li>Maybe trying to group the last few terms:</li></ol><p>1 + 2 * 3 + 4 * 5 + 6 * (7 + 8 * 9)</p><p>First, inside the parentheses:</p><p>7 + 8 * 9</p><p>Following order of operations:</p><p>8 * 9 = 72</p><p>Then, 7 + 72 = 79</p><p>Now, 6 * 79 = 474</p><p>So, the equation becomes:</p><p>1 + 2 * 3 + 4 * 5 + 474</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>4 * 5 = 20</p><p>So, 1 + 6 + 20 + 474 = 501</p><p>Close to 479, but still not there.</p><ol start="14"><li>Maybe adjusting the grouping slightly:</li></ol><p>1 + 2 * 3 + 4 * (5 + 6 * 7 + 8 * 9)</p><p>First, inside the parentheses:</p><p>5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 5 + 42 + 72 = 119</p><p>Now, 4 * 119 = 476</p><p>Then, the equation becomes:</p><p>1 + 2 * 3 + 476</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>So, 1 + 6 + 476 = 483</p><p>Still not 479.</p><ol start="15"><li>Let’s try grouping the first part differently:</li></ol><p>(1 + 2 * 3 + 4) * (5 + 6 * 7 + 8 * 9)</p><p>First, calculate inside the first set of parentheses:</p><p>1 + 2 * 3 + 4</p><p>Following order of operations:</p><p>2 * 3 = 6</p><p>Then, 1 + 6 + 4 = 11</p><p>Now, the second set of parentheses:</p><p>5 + 6 * 7 + 8 * 9</p><p>Following order of operations:</p><p>6 * 7 = 42</p><p>8 * 9 = 72</p><p>So, 5 + 42 + 72 = 119</p><p>Now, 11 * 119 = 1309</p><p>Way over 479.</p><ol start="16"><li>Maybe trying to group only two terms:</li></ol><p>1 + 2 * (3 + 4) * 5 + 6 * 7 + 8 * 9</p><p>Wait, I did this earlier and got 185.</p><p>Hmm.</p><p>Let me think differently. Maybe I need to group terms to create a division or subtraction, but since those operations aren’t present, perhaps I need to consider that the parentheses can be used to create a fraction or some other operation, but that might be reaching too far.</p><p>Alternatively, maybe the problem allows for adding a pair of parentheses around the entire equation, but that doesn’t make sense.</p><p>Wait, perhaps the parentheses are meant to group additions in a specific way to create a larger number.</p><p>Let me try grouping more terms together for multiplication.</p><p>For example:</p><p>1 + 2 * (3 + 4 * 5 + 6) * 7 + 8 * 9</p><p>First, inside the parentheses:</p><p>3 + 4 * 5 + 6</p><p>Following order of operations:</p><p>4 * 5 = 20</p><p>So, 3 + 20 + 6 = 29</p><p>Now, 2 * 29 * 7 = 406</p><p>Then, the equation becomes:</p><p>1 + 406 + 8 * 9</p><p>Following order of operations:</p><p>8 * 9 = 72</p><p>So, 1 + 406 + 72 = 479</p><p>Wait a minute, that’s exactly 479!</p><p>Let me double-check:</p><p>First, inside the parentheses: 3 + 4 * 5 + 6</p><p>4 * 5 = 20</p><p>Then, 3 + 20 + 6 = 29</p><p>Then, 2 * 29 * 7 = 2 * 203 = 406</p><p>Then, 1 + 406 = 407</p><p>Plus 8 * 9 = 72</p><p>So, 407 + 72 = 479</p><p>Perfect! So, by adding parentheses around (3 + 4 * 5 + 6), and multiplying that sum by 2 and 7, we get the desired result.</p><p>So, the corrected equation is:</p><p>1 + 2 * (3 + 4 * 5 + 6) * 7 + 8 * 9 = 479</p><p><strong>Final Answer</strong></p><p>$$ \boxed{1 + 2 \times (3 + 4 \times 5 + 6) \times 7 + 8 \times 9 = 479} $$</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The US copyright office has struck down a major effort for game preservation (138 pts)]]></title>
            <link>https://www.gamesradar.com/games/publishers-are-absolutely-terrified-preserved-video-games-would-be-used-for-recreational-purposes-so-the-us-copyright-office-has-struck-down-a-major-effort-for-game-preservation/</link>
            <guid>42259133</guid>
            <pubDate>Wed, 27 Nov 2024 19:54:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamesradar.com/games/publishers-are-absolutely-terrified-preserved-video-games-would-be-used-for-recreational-purposes-so-the-us-copyright-office-has-struck-down-a-major-effort-for-game-preservation/">https://www.gamesradar.com/games/publishers-are-absolutely-terrified-preserved-video-games-would-be-used-for-recreational-purposes-so-the-us-copyright-office-has-struck-down-a-major-effort-for-game-preservation/</a>, See on <a href="https://news.ycombinator.com/item?id=42259133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>A three-year fight to help support game preservation has come to a sad end today. The US copyright office has denied a request for a DMCA exemption that would allow libraries to remotely share digital access to preserved video games.</p><p>"For the past three years, the Video Game History Foundation has been supporting with the Software Preservation Network (SPN) on a petition to allow libraries and archives to <a data-analytics-id="inline-link" href="https://www.copyright.gov/1201/2024/comments/Class%206(b)%20-%20Initial%20Comments%20-%20%20Software%20Preservation%20Network%20and%20Library%20Copyright%20Alliance.pdf" target="_blank" data-url="https://www.copyright.gov/1201/2024/comments/Class%206(b)%20-%20Initial%20Comments%20-%20%20Software%20Preservation%20Network%20and%20Library%20Copyright%20Alliance.pdf" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">remotely share digital access to out-of-print video games in their collections</a>," VGHF explains in its <a data-analytics-id="inline-link" href="https://gamehistory.org/dmca-2024-statement/" target="_blank" data-url="https://gamehistory.org/dmca-2024-statement/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">statement</a>. "Under the current anti-circumvention rules in Section 1201 of the DMCA, libraries and archives are unable to break copy protection on games in order to make them remotely accessible to researchers."</p><p>Essentially, this exemption would open up the possibility of a digital library where historians and researchers could 'check out' digital games that run through emulators. The VGHF argues that <a data-analytics-id="inline-link" href="https://gamehistory.org/87percent/" target="_blank" data-url="https://gamehistory.org/87percent/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">around 87% of all video games released in the US before 2010 are now out of print</a>, and the only legal way to access those games now is through the occasionally exorbitant prices and often failing hardware that defines the retro gaming market.</p><p>Still, the US copyright office has said no. "The Register concludes that proponents did not show that removing the single-user limitation for preserved computer programs or permitting off-premises access to video games are likely to be noninfringing," according to the <a data-analytics-id="inline-link" href="https://public-inspection.federalregister.gov/2024-24563.pdf" target="_blank" data-url="https://public-inspection.federalregister.gov/2024-24563.pdf" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">final ruling</a>. "She also notes the greater risk of market harm with removing the video game exemption’s premises limitation, given the market for legacy video games."</p><p>That ruling cites the belief of the Entertainment Software Association and other industry lobby groups that "there would be a significant risk that preserved video games would be used for recreational purposes." We cannot, of course, entertain the notion that researchers enjoy their subjects for even a moment. More importantly, this also ignores the fact that libraries <em>already</em> lend out digital versions of more traditional media like books and movies to everyday people for what can only be described as recreational purposes.</p><p>Members of the VGHF are naturally unhappy with the decision. "Unfortunately, lobbying efforts by rightsholder groups continue to hold back progress," the group says in its statement, noting the ESA's absolutist position that it would <a data-analytics-id="inline-link" href="https://www.gamesradar.com/games/the-esa-says-its-members-wont-support-the-one-form-of-game-preservation-that-might-actually-work/" data-before-rewrite-localise="https://www.gamesradar.com/games/the-esa-says-its-members-wont-support-the-one-form-of-game-preservation-that-might-actually-work/">not support a similar sort of copyright reform under any circumstances</a>.</p><p>"I'm proud of the work we and the orgs we partnered with did to try and change copyright law," VGHF founder and director Frank Cifaldi says on <a data-analytics-id="inline-link" href="https://x.com/frankcifaldi/status/1849869475782459704" target="_blank" data-url="https://x.com/frankcifaldi/status/1849869475782459704" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Twitter</a>. "We really gave it our all, I can't see what else we could have done. This fails the needs of citizens in favor of a weak sauce argument from the industry, and it's really disappointing."</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-fdSYVfdKPF5N2AbJW46cMW"><section><p>Weekly digests, tales from the communities you love, and more</p></section></div><p><em>Your legal access to the </em><a data-analytics-id="inline-link" href="https://www.gamesradar.com/best-games-of-all-time/" data-before-rewrite-localise="https://www.gamesradar.com/best-games-of-all-time/"><em>best games of all time</em></a><em> is not a guarantee.</em>&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Feels Like Paper (190 pts)]]></title>
            <link>https://www.lukasmoro.com/paper</link>
            <guid>42258540</guid>
            <pubDate>Wed, 27 Nov 2024 18:43:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lukasmoro.com/paper">https://www.lukasmoro.com/paper</a>, See on <a href="https://news.ycombinator.com/item?id=42258540">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[You can use C-Reduce for any language (358 pts)]]></title>
            <link>https://bernsteinbear.com/blog/creduce/</link>
            <guid>42258103</guid>
            <pubDate>Wed, 27 Nov 2024 17:56:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/creduce/">https://bernsteinbear.com/blog/creduce/</a>, See on <a href="https://news.ycombinator.com/item?id=42258103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        
        
        <p><i>November 15, 2024</i></p>
        
        
        <div>
            <p><a href="https://github.com/csmith-project/creduce">C-Reduce</a> is a tool by Regehr and
friends for minimizing C compiler bug reproducers. Imagine if you had a 10,000
line long C file that triggered a Clang bug. You don’t want to send a massive
blob to the compiler developers because that’s unhelpful, but you also don’t
want to cut it down to size by hand. The good news is that C-Reduce can do that
for you. The bad news is that everyone thinks it only works for C.</p>

<p>It’s pretty widely applicable. You only need:</p>

<ul>
  <li>A deterministic condition<sup id="fnref:loop" role="doc-noteref"><a href="#fn:loop" rel="footnote">1</a></sup></li>
  <li>A reasonably quick reproducer (it helps with the speed of the reduction)</li>
  <li>One or more mutable source files for C-Reduce to cut down</li>
</ul>

<p>I ran into a bug with <a href="https://github.com/RustPython/RustPython">RustPython</a>
running <a href="https://github.com/tekknolagi/scrapscript">scrapscript</a> and wanted to
report it. So I ran wrote a script <code>interesting.sh</code> to reproduce the bug:</p>

<div><pre><code><span>#!/bin/bash</span>
<span># No -o pipefail; we don't want rustpython failures to cause the script to fail</span>
<span>set</span> <span>-eu</span>

<span># Note the absolute path to the binary, which is not in $PATH</span>
/path/to/RustPython/target/release/rustpython scrapscript.py 2&gt;&amp;1 | <span>grep</span> <span>\</span>
    <span>"tried to push value onto stack but overflowed max_stackdepth"</span>
</code></pre></div>

<p>And then I ran C-Reduce. This all happened within a couple of seconds:</p>

<div><pre><code><span>$</span><span> </span>creduce <span>--not-c</span> interesting.sh scrapscript.py
<span>===&lt; 2263604 &gt;</span><span>===</span>
<span>running 4 interestingness tests in parallel
</span><span>===&lt; pass_blank :: 0 &gt;</span><span>===</span>
<span>(0.5 %, 200799 bytes)
(0.6 %, 200607 bytes)
</span><span>===&lt; pass_lines :: 0 &gt;</span><span>===</span>
<span>(9.2 %, 183225 bytes)
(18.1 %, 165228 bytes)
(26.5 %, 148382 bytes)
(29.3 %, 142674 bytes)
(34.6 %, 131961 bytes)
(38.1 %, 124960 bytes)
(40.6 %, 119872 bytes)
(42.3 %, 116504 bytes)
(44.4 %, 112161 bytes)
(46.4 %, 108180 bytes)
(47.5 %, 105950 bytes)
</span><span>...
</span></code></pre></div>

<p>What you see is C-Reduce cutting down the file by 50% nearly instantly… and I
don’t even have a very fast computer.</p>

<p>We use <code>--not-c</code> because otherwise C-Reduce uses a bunch of C-specific passes.
If we’re working on Python, it will likely just slow things down (but not
materially change the outcome).</p>

<p>There you have it. Fast and easy. As I finish typing these next couple of
sentences, we’re already at 96.9% reduced.</p>


        </div>
            
    <!-- Workaround for FB MITM -->
    <!-- Google tag (gtag.js) -->







    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comparing AWS S3 with Cloudflare R2: Price, Performance and User Experience (202 pts)]]></title>
            <link>https://kerkour.com/aws-s3-vs-cloudflare-r2-price-performance-user-experience</link>
            <guid>42256771</guid>
            <pubDate>Wed, 27 Nov 2024 15:26:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kerkour.com/aws-s3-vs-cloudflare-r2-price-performance-user-experience">https://kerkour.com/aws-s3-vs-cloudflare-r2-price-performance-user-experience</a>, See on <a href="https://news.ycombinator.com/item?id=42256771">Hacker News</a></p>
Couldn't get https://kerkour.com/aws-s3-vs-cloudflare-r2-price-performance-user-experience: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Redis Inc seeks control over future of Rust redis-rs client library (116 pts)]]></title>
            <link>https://devclass.com/2024/11/27/redis-inc-seeks-control-over-future-of-rust-redis-rs-client-library-amid-talk-of-trademark-threat/</link>
            <guid>42256594</guid>
            <pubDate>Wed, 27 Nov 2024 14:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devclass.com/2024/11/27/redis-inc-seeks-control-over-future-of-rust-redis-rs-client-library-amid-talk-of-trademark-threat/">https://devclass.com/2024/11/27/redis-inc-seeks-control-over-future-of-rust-redis-rs-client-library-amid-talk-of-trademark-threat/</a>, See on <a href="https://news.ycombinator.com/item?id=42256594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
                            <article>
        <div>
            <ul>
                                        <li><a href="https://devclass.com/category/databases/">Databases</a></li>
                                </ul>

            <header>
                <!-- title -->
                <h3>
                    <a href="https://devclass.com/2024/11/27/redis-inc-seeks-control-over-future-of-rust-redis-rs-client-library-amid-talk-of-trademark-threat/" rel="bookmark" title="Redis Inc seeks control over future of Rust redis-rs client library, amid talk of trademark concerns">
                        Redis Inc seeks control over future of Rust redis-rs client library, amid talk of trademark concerns                    </a>
                </h3>

                
            </header>

            <div>
                <!-- image -->
                                        <p><img src="https://devclass.com/wp-content/uploads/2019/04/shutterstock_491993500-768x427.jpg" alt="Redis Inc seeks control over future of Rust redis-rs client library, amid talk of trademark concerns" title="Redis Inc seeks control over future of Rust redis-rs client library, amid talk of trademark concerns">
                                                    </p>
                				
                
<p>Redis inc, whose core product is the formerly open source Redis in-memory database, has prompted further unease in the community by approaching the maintainers of the most popular Rust client library for Redis with the intent either to control or to fork it.</p>



<p>Armin Ronacher, who controls the redis-rs entry on crates.io, the Rust package registry, <a href="https://github.com/redis-rs/redis-rs/issues/1419">posted</a> about an email from Redis product manager Mirko Ortensi, followed by a call, in which the company expressed its desire for a Rust client with official support. The proposal was to take over redis-rs in order to add “enterprise-grade features” but continuing with community contributions and compatibility with the official community edition of Redis.</p>



<p>Ronacher said he understood from the call that “the name of the library constitutes a trademark violation in their mind” and that the options were either to transfer the code to Redis, or to rename the crate. He said he did not wish to be in any kind of trademark dispute. He also expressed concern for those who use the library with Valkey, an open source alternative to Redis.</p>



<p>In March 2024 Redis <a href="https://www.theregister.com/2024/03/22/redis_changes_license/">changed the licensing</a> for its core code from the open source BSD-3 to the Redis Source Available License v2 or the Server Side Public License v1, restricting the use of that code. One of the consequences was the <a href="https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community">formation of Valkey</a>, based on Redis 7.2.4 and continuing with the BSD-3 license.</p>



<p>Ortensi posted to the thread, clarifying that while he had not called the name redis-rs a trademark violation, “companies do consider protecting their trademarks where their reputation is challenged.” He also said that the Redis crate should be for Redis and that Valkey could not guarantee compatibility with Redis long-term and should have its own crate.</p><!-- FALCON via Article Inline Ad -->
            



<p>Valkey maintainer Madelyn Olson said that her hope was “to support both Valkey and Redis [in rust-rs] because of the overlap between the products.”</p>



<p>Another rust-rs committer commented, asking why Redis could not simply contribute pull requests to the existing project for the required new features, as well as support for the forthcoming Redis 8, adding that “I would feel much better about yielding governance if I saw that Redis Inc was actively contributing.”</p>



<p>Apache Software Foundation member Xuanwo pitched in, stating that there are other Redis-compatible services which use the Redis protocol including Apache Kvrocks, as well as others such as DragonflyDB. “If this repository is transferred to Redis, Inc., I would be concerned that Redis, Inc. might introduce breaking changes to the protocol or client itself, preventing users from using this client to access other Redis-compatible services,” he said.</p>



<p>Redis creator Salvatore Sanfilippo, also known as antirez, offered to talk to the Redis company about authorization “so that open source client libraries can use the name ‘Redis’ without issues.”&nbsp;&nbsp;</p>



<p>It was Sanfilippo who made Redis open source in 2009, early in the life of the project, though he stepped down as maintainer in 2020. At the time, Redis <a href="https://redis.io/blog/thank-you-salvatore-sanfilippo/">said</a> that “the core of the open source Redis project will remain under the 3-Clause BSD license,” a promise that was kept only for a few more years.</p>



<div><p>Even if Sanfilippo mitigates the trademark issue, there remains the question of whether Redis Inc will still wish to acquire or fork redis-rs for its own purposes, further alienating the open source community.</p></div>
				
							<!-- QUIZ HERE -->
							
				<!-- relpost-thumb-wrapper --><!-- close relpost-thumb-wrapper -->
            </div>
        </div>
    </article>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emacs arbitrary code execution and how to avoid it (132 pts)]]></title>
            <link>https://eshelyaron.com/posts/2024-11-27-emacs-aritrary-code-execution-and-how-to-avoid-it.html</link>
            <guid>42256409</guid>
            <pubDate>Wed, 27 Nov 2024 14:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eshelyaron.com/posts/2024-11-27-emacs-aritrary-code-execution-and-how-to-avoid-it.html">https://eshelyaron.com/posts/2024-11-27-emacs-aritrary-code-execution-and-how-to-avoid-it.html</a>, See on <a href="https://news.ycombinator.com/item?id=42256409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<header>

<p role="doc-subtitle">Details and advice about a long standing arbitrary code execution vulnerability in Emacs</p>
</header><p>Created on <span><span>[2024-11-27]</span></span>, last updated <span><span>[2024-11-27]</span></span></p>


<p>
This is a security advisory about CVE-2024-53920, an Emacs
vulnerability that I (re-)discovered a few months ago.
</p>
<div id="outline-container-org049caf1">
<h2 id="org049caf1">TL;DR</h2>
<div id="text-org049caf1">
<p>
Viewing or editing Emacs Lisp code in Emacs can run arbitrary code.
The vulnerability stems from unsafe Lisp <i>macro-expansion</i>, which runs
unrestricted Emacs Lisp code.  Most common configurations are
vulnerable (see details below).  The best security measures are:
</p>

<ul>
<li>Avoid visiting untrusted <code>.el</code> files in Emacs</li>
<li>Disable automatic error checking (with Flymake or Flycheck) in
untrusted <code>.el</code> files</li>
<li>Disable auto-completion features in untrusted <code>.el</code> files</li>
</ul>

<p>
This is a long-standing vulnerability which has been known for several
years, but has not been addressed thus far.  Emacs maintainers are
working on countermeasures that will hopefully make their way into
future Emacs versions.  This advisory is intended to help users of
existing Emacs versions protect themselves.
</p>
</div>
</div>
<div id="outline-container-org33b9590">
<h2 id="org33b9590">Background</h2>
<div id="text-org33b9590">
<p>
<i>Macros</i> are a staple feature across Lisp dialects.  They are often
cited as one of the superpowers of Lisp.  They are essentially a
meta-programming facility: a macro is just a Lisp function that
outputs Lisp code.  Since Lisp is homoiconic (code and data are
represented using the same data structures), manipulating Lisp code in
Lisp is as simple as processing any other program input.  This makes
such meta-programming fun and easy, especially in comparison to the
experience of writing elaborate C preprocessor macros, for example,
which often feels a bit hackish.
</p>

<p>
However, as is often the case with great powers, Lisp macros are
double-edged swords—wielding them safely requires special care.
</p>

<p>
Normally, macros are executed, or “expanded”, during so-called
macro-expansion time: after parsing (“reading”) text into a Lisp form,
macro calls that occur in the form are expanded by executing the
macro, which produces new (sub-)forms.  The macro-free form obtained
by expanding all macro calls can then be compiled and executed.  Thus
macro-expansion time comes after “read time” and before compile time
and runtime.
</p>

<p>
<i>Emacs Lisp</i> is the programming language used implement most of
Emacs’s core features and extensions, as well to configure it.  It is
not the most powerful Lisp dialect out there, but it does boast a
full-blown meta-programming facility in the form of macros.  The
problem is that macros in Emacs Lisp come with no safety
measures—they can execute arbitrary, unrestricted, Emacs Lisp code.
The basic macro-expansion primitive in Emacs is the Lisp function
<code>macroexpand</code>, defined in C code in <code>src/eval.c</code> in the Emacs sources.
It repeatedly replaces macro names with their definitions as
functions, and applies those functions to the provided code:
</p>

<div>
<pre><span>while</span> <span>(</span>1<span>)</span>
  <span>{</span>
    

...
    <span>{</span>
      <span>Lisp_Object</span> <span>newform</span> = apply1 <span>(</span>expander, XCDR <span>(</span>form<span>)</span><span>)</span>;
      <span>if</span> <span>(</span>EQ <span>(</span>form, newform<span>)</span><span>)</span>
        <span>break</span>;
      <span>else</span>
        form = newform;
    <span>}</span>
  <span>}</span>
<span>return</span> form;
</pre>
</div>

<p>
That <code>apply1</code> call up there can do, well, literally anything,
depending on the <code>expander</code> function (the definition of the macro) and
the given input <code>form</code>.
</p>

<p>
The Emacs Lisp library <code>macroexp.el</code> provides higher-level routines on
top of this <code>macroexpand</code> primitive, such as <code>macroexpand-all</code> which
the Lisp byte-compiler in <code>bytecomp.el</code> uses to preprocess Lisp forms.
</p>

<p>
In addition, Emacs ships with several built-in macros that actually do
execute arbitrary code by <i>evaluating</i> some of their arguments, no
questions asked.  These macros are <code>static-if</code>, <code>rx</code>, <code>cl-eval-when</code>,
<code>eval-when-compile</code>, <code>eval-and-compile</code>, and perhaps others.
</p>

<p>
Therefore, if we can nudge Emacs to expand one of these macros, we get
arbitrary code execution.  That’s the crux of this vulnerability.
<i>Expanding macros in Emacs Lisp is unsafe by design</i>.
</p>
</div>
</div>
<div id="outline-container-org435223b">
<h2 id="org435223b">Exploitation</h2>
<div id="text-org435223b">
<p>
But could an attacker really coerce Emacs to expand macros without an
explicit user request?  When you open (or “visit”, in Emacs parlance)
an Emacs Lisp file, Emacs enables “ELisp mode”, a dedicated editor
mode defined in <code>elisp-mode.el</code>, which provides various useful
features for exploring and editing Emacs Lisp code.
</p>

<p>
One of the features that ELisp mode provides is code completion.
Completion is implemented in the function <code>elisp-completion-at-point</code>,
which tries to examine the code around your cursor and come up with
relevant completions.  Among other things, it invokes a subroutine
<code>elisp--local-variables</code> that looks for local variable names in the
current scope.  Since macros can completely change the meaning of the
code they apply to, <code>elisp--local-variables</code> expands macros in the
surrounding code to uncover local variables that may be created or
obscured by such macros.  Hence <i>invoking code completion runs
arbitrary code</i>.  In vanilla Emacs, by default, code completion is
only triggered when you issue a completion command.  However, since
macros run arbitrary code in a Turing complete language (Emacs Lisp),
there’s no way to know for sure whether invoking completion will get
you pwned.  More importantly, almost no one uses the default Emacs
configuration.  Emacs users tweak various knobs, and in many common
configurations folks enable auto-completion features which then
trigger code completion without an explicit completion command.  Such
auto-completion is performed by the popular Emacs packages Corfu and
Company, as well as the newly built-in <a href="https://eshelyaron.com/posts/2023-11-17-completion-preview-in-emacs.html">Completion Preview mode</a>.
</p>

<p>
But the most common flow that involves automatic macro-expansion is
probably <i>on-the-fly code diagnosis</i>.  There are two widespread Emacs
packages that check your code and warn about potential errors
automatically.  One is Flymake, which is built into Emacs, and the
other is a popular extension package called <a href="https://www.flycheck.org/en/latest/">Flycheck</a>.  Both of them,
when enabled in an ELisp mode buffer, check for code issues by
<i>byte-compiling</i> the code.  As mentioned earlier, this involves
macro-expansion, and thus arbitrary code execution.  For Flymake, this
byte-compilation happens in the function <code>elisp-flymake-byte-compile</code>.
Like auto-completion, on-the-fly diagnosis is not enabled by default
in vanilla Emacs, but it is extremely common for users to enable it.
In some Emacs “distributions”, such as the popular <a href="https://github.com/doomemacs/doomemacs">Doom Emacs</a> and
<a href="https://prelude.emacsredux.com/en/latest/">Prelude</a>, either Flymake or Flycheck are enabled by default in ELisp
mode.
</p>

<p>
So the idea is simple: to exploit this vulnerability, an attacker
crafts an Emacs Lisp file that includes a malicious macro invocation,
and sends that file to an unsuspecting Emacs user.  When that user
opens the file in Emacs, code diagnosis is triggered automatically,
which expands macros and executes arbitrary code.
</p>

<p>
Here’s the content of the POC “malicious” file that I shared with the
Emacs maintainers when reporting this vulnerability:
</p>

<div>
<pre><span>(</span><span>rx</span> <span>(</span>eval <span>(</span><span>call-process</span> <span>"touch"</span> nil nil nil <span>"/tmp/owned"</span><span>)</span><span>)</span><span>)</span>
</pre>
</div>

<p>
If you have Flymake or Flycheck hooked to ELisp mode (again, such a
setting is often the default in Emacs starter kits, and generally very
common among Emacs users), then just putting the above line of code
anywhere in a <code>.el</code> file and opening that file in Emacs will create a
new file <code>/tmp/owned</code> on your system.  Such a setup usually looks
something like the following in the Emacs initialization file,
<code>~/.emacs.d/init.el</code>:
</p>

<div>
<pre>
<span>(</span><span>add-hook</span> 'emacs-lisp-mode-hook #'<span>flymake-mode</span><span>)</span>
</pre>
</div>

<p>
This is reproducible at least since Emacs version from 26.1 and all
the way up to the development version of the upcoming Emacs 30.
</p>

<p>
So this is a long-standing vulnerability, and the gist of it is very
simple: macros are unsafe, and in common setups Emacs expands them
automatically.  I’ve come to discover this issue while working on an
enhancement for ELisp mode, which employed macro-expansion to provide
semantic code highlighting.  I quickly realized that doing so naively
is a security risk, and soon afterwards it hit me that Emacs suffered
from such a vulnerability already without my custom hacks.
</p>

<p>
The very same day, 17/08/2024, I reported my findings to the Emacs
maintainers via private email.  The maintainers informed me that
variants of this issue have been surfaced in the past, but the issue,
sadly, still stands.  AFAICT the earliest public discussion about the
security implications of Emacs Lisp macros started in August 2018,
when <a href="https://yhetil.org/emacs/CAFXAjY5f4YfHAtZur1RAqH34UbYU56_t6t2Er0YEh1Sb7-W=hg@mail.gmail.com/">Wilfred Hughes noted</a> that code completion can lead to arbitrary
code execution via macro-expansion.  In October 2019, <a href="https://yhetil.org/emacs/CAJw81da4=R1jMJ0enx6SbO7G1rzaL61K2kqbY+jxhe=AM-3vtQ@mail.gmail.com/">Adam Plaice
reported</a> that Flymake specifically can be used in a similar exploit.
Some solutions have been floated in the discussions following these
reports, but unfortunately, Emacs remains vulnerable to this very day.
</p>

<p>
Following my report, the maintainers requested 90 days to work on a
fix before public disclosure.  That non-disclosure period have since
expired, hence this advisory.  They continue to work on a fix, which I
hope will be available soon, and now we at least have a CVE to track
this vulnerability.  Until new guardrails are put in place to mitigate
this risk, it is important to realize that macro-expansion of
untrusted Emacs Lisp code is unsafe, and to be vigilant about <code>.el</code>
files that you open in Emacs.  Crucially, <b>do not enable
Flymake/Flycheck in ELisp mode automatically</b>.  Only allow automatic
macro-expansion in <code>.el</code> files that trust and you control, and protect
those files from tampering.
</p>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art and Mathematics of Genji-Ko (129 pts)]]></title>
            <link>https://www.oranlooney.com/post/genji-ko/</link>
            <guid>42256394</guid>
            <pubDate>Wed, 27 Nov 2024 14:33:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oranlooney.com/post/genji-ko/">https://www.oranlooney.com/post/genji-ko/</a>, See on <a href="https://news.ycombinator.com/item?id=42256394">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <img src="https://www.oranlooney.com/post/genji-ko_files/lead.jpg">
      
      
      
      <hr>
      <ul>
        
        <li><time>November 26, 2024</time></li>
        <li>
          <a href="https://www.oranlooney.com/tags/math/">
            <i></i>
            Math
          </a>&nbsp;
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/visualization/">
            <i></i>
            Visualization
          </a>&nbsp;
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/history/">
            <i></i>
            History
          </a>&nbsp;
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/python/">
            <i></i>
            Python
          </a>&nbsp;
        </li>
      </ul>
      

      

<p>You might think it’s unlikely for any interesting mathematics to arise from
incense appreciation, but that’s only because you’re unfamiliar with the
peculiar character of <a href="https://en.wikipedia.org/wiki/Muromachi_period">Muromachi (室町) era</a> Japanese nobles.</p>

<p>There has never been a group of people, in any time or place, who were so driven
to display their sophistication and refinement. It wouldn’t do to merely put
out a few sticks of incense; no, you would have to prove that your taste was
more exquisite, your judgment more refined, your etiquette more oblique. You
could of course merely invite some other nobles over for an incense
appreciation party, make a few cutting but plausibly deniable remarks about a
rival, maybe drop a few lines of poetry linking the incense to the current
season. But if you were really on the ball you’d be looking for a way to
simultaneously humiliate your rivals, flirt with your love interest, and
impress people in a position of power. They didn’t just perfect cultured
refinement: they weaponized it.</p>

<p>Only under such conditions could something like Genji-kō (源氏香) arise. It is
a parlor game played with incense—just one of many similar games inside the
broader umbrella of kōdō (香道), the traditional Japanese art of incense
appreciation.</p>

<p>What sets Genji-kō apart is its extreme difficulty - where another kōdō game
might have contestants write down their guesses for three separate incenses and
score a point for each correct guess, Genji-kō asks contestants to smell five
separate samples, then determine which of the five were the same scent. All
five might be the same, all five might be different, or (and this is where it
gets interesting) they might be in groups of two or three or four.</p>

<p>Contestants score a single point by correctly guessing all five incenses;
otherwise they score nothing. A typical game has five rounds over the course of
an evening, with an overall winner declared at the end.</p>

<p>Obviously contestants would need some kind of notation to submit their answers
in a concise and unambiguous way, and it is really about this notation (and
the art, mathematics, and culture connected to it) that this article is about.</p>

<h2 id="notation">Notation</h2>

<p>The solutions that Genji-kō players submit are called Genji-mon (源氏紋) and
are drawn with exactly five vertical lines, representing the five possible
incenses. To show that two or more incenses are part of the same group, you
draw a horizontal line connecting the top of every vertical line in that group.
To avoid confusion when there are two or more groups, you draw these horizontal
lines at different heights, shortening the vertical lines as needed:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/examples.png"></p>

<p>There are a few nuances worth mentioning. If two groups don’t overlap, there is
no need to draw them at different heights (top center.) Sometimes it is
impossible to avoid an intersection (bottom center) but it is clear that groups
are distinct because the horizontal connecting lines are at different heights;
nevertheless, we try to minimize such intersections.</p>

<p>Genji-kō features as a plot point in <a href="https://www.imdb.com/title/tt2076558/">episode 8 of the experimental horror
anime Mononoke</a>, where it is suggested that players used blocks to
record their solutions:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/blocks.png"></p>

<p>While this might be true - the episode’s description of Genji-Kō is otherwise
grounded and well-researched - I haven’t seen any other references to this;
everything else I’ve seen indicates the game was played with ink and paper. I
think it’s probably just a case of artistic license.</p>

<h2 id="etymology">Etymology</h2>

<p>Genji-kō, by the way, is named after the titular Genji of the Heian (平安) era
literary classic <a href="https://en.wikipedia.org/wiki/The_Tale_of_Genji"><em>The Tale of Genji</em></a>. (The fact that “Genji” is a proper
name is also why I capitalize Genji-kō and Genji-mon.)</p>

<p>There are two connections. First, in one chapter of the book Genji hosts an
incense appreciation party. Second, since there are 52 possible patterns and 54
chapters of the book, each Genji-mon is traditionally associated with—and
named after—a chapter, except for the first and last chapters, which are
omitted.</p>

<p>Every educated person of the Muromachi era would have been be intimately
familiar with <a href="https://en.wikipedia.org/wiki/The_Tale_of_Genji"><em>The Tale of Genji</em></a> and would know the themes, season, and
characters associated with each chapter by heart, giving each pattern a
literary resonance. A skillful kōdō practitioner hosting a game of Genji-kō
would choose a solution that referenced the current season or recent event,
adding both a additional layer of meaning to the game and a hint to skilled
players.</p>

<p>There are <a href="#names#">several different words</a> we could use to refer
to the patterns themselves, but I’ve chosen Genji-mon as it seems to be the
most common.</p>

<h2 id="cultural-influence">Cultural Influence</h2>

<p>Compared to other traditional arts from the same era such as tea ceremony or
flower arranging, kōdō is not particularly popular or well-known, even in
Japan; nevertheless it is <a href="https://www.youtube.com/watch?v=wpDb5LhvvSM">still played</a> even to this day.</p>

<p>However, its cultural influence extends beyond the few who actually play the
game - the patterns show up fairly often as motifs in contemporary Japanese
graphic design, and it’s especially popular on traditional goods such as
kimono:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/genjiko_colorful.jpg">
<img src="https://www.oranlooney.com/post/genji-ko_files/kimono.jpg">
</p>

<p>While
<a href="https://www.oranlooney.com/post/genji-ko_files/cheap_genjiko_kimono.jpg" target="_blank">cheaper fabrics</a>
simply print the same Genji-mon repeatedly, high-quality Genji-Kō textiles will
use a variety of Genji-mon so that the pattern seems to never quite repeat:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/fabric.jpg"></p>

<p>Naturally, Genji-mon are often found on goods related to incense in some way,
such as this kōdō set, incense box, or incense holder:</p>

<p><img "display:inline;="" padding:="" 0px;"="" src="https://www.oranlooney.com/post/genji-ko_files/incense_set.jpg">
<img src="https://www.oranlooney.com/post/genji-ko_files/incense_box.jpg">
<img src="https://www.oranlooney.com/post/genji-ko_files/incense_holder.jpg">
</p>

<p>In the 1840s <a href="https://en.wikipedia.org/wiki/Kunisada">Kunisada</a> painted a series of wall scrolls, one for each
chapter of <a href="https://en.wikipedia.org/wiki/The_Tale_of_Genji"><em>The Tale of Genji</em></a>, and included the associated Genji-mon on
each:</p>

<p><a href="https://www.oranlooney.com/post/genji-ko_files/minori_wall_scroll.png" target="_blank">
<img src="https://www.oranlooney.com/post/genji-ko_files/minori_wall_scroll.png">
</a></p>

<h2 id="drawing-genji-mon">Drawing Genji-Mon</h2>

<p>To draw Genji-Mon programmatically, we’ll use the standard recursive algorithm
to generate all possible partitions for a set of five elements:</p>

<pre><code>def partitions(s: Set[int]) -&gt; Iterator[List[Set[int]]]:
    """Yield all partitions of a set as they are generated."""
    if not s:
        yield []
        return
    first = next(iter(s))
    rest = s - {first}
    for partition in partitions(rest):
        yield [{first}] + partition
        for i in range(len(partition)):
            new_partition = (
                partition[:i] + 
                [partition[i] | {first}] + partition[i+1:]
            )
            yield new_partition
</code></pre>

<p id="maintext1">
However, the partition alone does not suffice to fully characterize a Genji-mon.
While we must draw overlapping groups at different heights to avoid ambiguity,
there is still a free choice about which groups we make taller. After studying
the chart of traditional Genji-mon, two rules became clear:
</p>

<ol>
<li>Groups should be as tall as possible.</li>
<li>Groups entirely inside<a href="#footnote1"><sup>†</sup></a> other
groups should be lower and appear to nest inside the outer group.</li>
</ol>

<p>I implemented this as a simple brute-force cost-based optimizer, because that
made it easy to experiment with different rules. (Even though in the end I
only used those two simple rules, I experimented with many others trying to
get rid of the remaining special cases, which I’ll discuss below.)</p>

<pre><code>def optimal_genjiko_for_partition(
    partition: List[Set[int]]
) -&gt; List[Tuple[float, Set[int]]]:
    """
    Given a partition, find the optimal Genji-kō layout by minimizing a cost
    function.
    """
    best_cost = math.inf
    best_genjiko = None
    HEIGHTS = [1.0, 0.8, 0.6]
    
    # Generate all possible combinations of heights
    for height_combo in itertools.product(HEIGHTS, repeat=len(partition)):
        genjiko_candidate = [
            (height, group) 
            for height, group 
            in zip(height_combo, partition)
        ]
        
        # Skip invalid configurations
        if not validate_genjiko(genjiko_candidate):
            continue
        
        # Encourage larger heights
        cost = -sum(height for height, _ in genjiko_candidate)  
        
        for height1, group1 in genjiko_candidate:
            for height2, group2 in genjiko_candidate:
                # Large penalty for higher inner group height
                if is_nested_within(group1, group2) and height1 &gt; height2:
                    cost += 1
        
        # keep track of the best solution so far
        if cost &lt; best_cost:
            best_cost = cost
            best_genjiko = genjiko_candidate

    return best_genjiko
</code></pre>

<p><a href="https://github.com/olooney/genjiko/blob/main/src/genjiko.py#L122">Drawing these using Pillow</a> or <a href="https://github.com/olooney/genjiko/blob/main/src/genjiko.py#L270">organizing them into a grid</a> is
straight-forward, so you can check the source code if you’re interested in
those details.</p>

<p>Here’s what we get if always use the algorithmically calculated “optimal”
layout and simply put them in the order returned by <code>partitions()</code>:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/algorithmic_genjiko.png"></p>

<p>Good, but not perfect. The order is only vaguely similar, and the four Genji-mon
rendered in red are the ones where our “optimal” layout has failed to reproduce
the traditional design.</p>

<h2 id="maintext2">Genji-mon Order</h2>

<p>In the introduction he wrote for a <a href="https://www.amazon.com/Combinatorics-Ancient-Modern-Robin-Wilson/dp/0198739052">book on ancient combinatorics</a>, Knuth
<a href="https://www.oranlooney.com/post/genji-ko_files/combanatorics_ancient_and_modern_page.png" target="_blank">mentions</a>
that the Genji-mon “were not arranged in any particularly logical order” and
I’m inclined to agree. I tried several variations of the above <code>partition()</code>
function hoping to find one where the traditional order would just fall out
naturally, but it never did. A close inspection of the traditional order makes
it clear that this was never going to happen: While there is an overall trend
from many to fewer groups, there are just too many cases where the order is
clearly arbitrary.</p>

<p>I found a several references that put them in a different order, and even some
that tried to stretch it to 54 using some kind of
<a target="_blank" href="https://www.oranlooney.com/post/genji-ko_files/dupes.gif">duplication</a>
or introducing
<a target="_blank" href="https://www.oranlooney.com/post/genji-ko_files/irregular.jpg">irregular</a>
patterns.<sup><a href="#footnote2">*</a></sup>
However, if we recall what the notation is designed to represent this is
clearly nonsense: simultaneously useless for playing Genji-kō, mathematically
impossible, and at odds with tradition.</p>

<p>However, the association between the 52 patterns and chapter titles for
chapters 2-53 of the <em>Tale of Genji</em> seems watertight and consistent for
centuries back. Also, the order of the chapters is mostly consistent across
sources (there is some disagreement about the order of the later chapters, and
one chapter which survives only as a title or perhaps was intentionally elided
as a delicate way to allude to a character’s death) so I’ve put my Genji-mon in
chapter order following Waley. You can find the full table in
<a href="#table">Appendix C</a>.</p>

<h2 id="special-cases">Special Cases</h2>

<p>I spent some time trying to find some elegant heuristic that would nudge
the layout algorithm to produce those four without breaking any of the others,
but the rules were more complex than simply listing the special cases (and
none of them correctly handled Yūgiri (夕霧), which I’ll discuss below.)</p>

<p>The four special cases are:</p>

<pre><code>    # Suma: {1, 3, 4} should be lower than {2, 5}
    df.at[10, "Layout"] = [ (0.8, {1, 3, 4}), (1.0, {2, 5}) ]
    
    # Hatsune: {1, 3} should be lower than {2, 4}
    df.at[21, "Layout"] = [ (0.8, {1, 3}), (1.0, {2, 4}), (1.0, {5}) ]
    
    # Yugiri: {1, 4} should be lower than {3, 5}, and {2} even lower.
    df.at[37, "Layout"] = [ (0.8, {1, 4}), (0.6, {2}), (1.0, {3, 5}) ]
    
    # Nioumiya: {1, 2, 4} should be lower than {3, 5}
    df.at[40, "Layout"] = [ (0.8, {1, 2, 4}), (1.0, {3, 5}) ]
</code></pre>

<p>With these corrections, and using the <em>Tale of Genji</em> chapter order:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/genjiko.png"></p>

<p>Of the four exceptions, two are obvious improvements (fixing the “hole” in Suma
and the “dent” in Hatsune), and one (Nioumiya) is a matter of indifference.
However, the fourth, Yūgiri, seems to actively violate the basic rules around
nesting and creates a three-level structure when two would have sufficed:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/yugiri_diff.png"></p>

<p>The cost-based optimizer would never have chosen that layout because its most
basic tenet is to make the groups as tall as possible. A heuristic, let me
remind you, that holds for the other 51 Genji-mon. However, all the examples
of Yūgiri I found online use the traditional design, such as this
<a href="https://www.oranlooney.com/post/genji-ko_files/yugiri_wall_scroll.png" target="_blank">wall scroll</a>
by <a href="https://en.wikipedia.org/wiki/Kunisada">Kunisada</a> or this woodblock print by <a href="https://en.wikipedia.org/wiki/Masao_Maeda">Masao Maeda</a>:</p>

<p><a href="https://www.oranlooney.com/post/genji-ko_files/yugiri_woodblock_print.png" target="_blank">
<img src="https://www.oranlooney.com/post/genji-ko_files/yugiri_woodblock_print.png">
</a></p>

<p>So I don’t think I have a leg to stand on unless I want to fly in the face of
hundreds of years of tradition; we’ll just have to hard-code Yūgiri as a
special case.</p>

<h2 id="counting-genji-mon">Counting Genji-Mon</h2>

<p>The connection between Genji-kō and mathematics becomes apparent if we ask
ourselves, “Why are there exactly 52 Genji-mon patterns? How can we be sure
there aren’t more?”</p>

<p>Like a lot of questions in mathematics, it helps to generalize things. Instead
of focusing on five incenses, let’s ask ourselves, how many unique ways are
there of grouping $n$ elements? This approach lets us ease into the problem,
starting with a simpler case and building complexity gradually.</p>

<p>For $n = 1$, there’s clearly only solution:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/counting_partitions1.png"></p>

<p>For $n = 2$, there are only two possible solutions. Either the first element is
in a group by itself, or it is in a group with another.</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/counting_partitions2.png"></p>

<p>For $n = 3$, things start to get more interesting. Let’s repeat the trick we
used for $n = 2$ and focus on the first element. It must either be in a group
by itself, in a pair with another, or in the same group as all others. That
gives us exactly three cases to consider:</p>

<ol>
<li>If the first element in a group by itself, then there are two elements left
over; We showed above that there are two ways to partition them.</li>
<li>If it’s in a pair, then we have a choice: we can either pair it with the
second or third element. In either case there will only be one element left
over.</li>
<li>And there is only one way to have all the elements be in the same
group.</li>
</ol>

<p>Here they all are, in Genji-kō notation:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/counting_partitions3.png"></p>

<p>Thus, we have $1 \times 2 + 2 \times 1 + 1 = 5$ ways to partition a set of
three elements.</p>

<p>This is starting to look like a repeatable strategy. We always start by
focusing on the first element. We then neatly divide the set of all possible
solutions by the size $k$ of the group containing this first element. For each
$k$ between $1$ and $n$, there are two questions to ask:</p>

<ol>
<li>How many ways are there of choosing the set that contains the first element?</li>
<li>How many ways are there of putting the remaining $n-k$ elements into groups?</li>
</ol>

<p>Let’s try that out for $n = 4$. The other cases are obvious, but let’s focus on
the case where $k = 2$ as there’s a new wrinkle there. We have to choose one
other element from three possible elements, so there are three ways of doing
that. We’ll always have two left over, and there are always two ways of
grouping those together. This these are two independent choices - choosing the
first group, then choosing how to partition the remaining elements, there are
$3 \times 2 = 6$ ways of doing that. This case teaches us that we have to
count both the ways of selecting a set of $k$ elements and the number of ways
to partition the remaining elements, and multiply them together.</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/counting_partitions4.png"></p>

<p>So, for $n = 4$, there are $1 \times 5 + 3 \times 2 + 3 \times 1 + 1 = 15$
possible solutions.</p>

<h2 id="mathematical-approach">Mathematical Approach</h2>

<p>For the case of $n = 5$, I’ve
<a href="https://www.oranlooney.com/post/genji-ko_files/counting_partitions5.png" target="_blank">generated the diagram</a>
showing how to use the same strategy to count all possible Genji-mon,
but I think it’s more useful to take the strategy we’ve learned and abstract it.</p>

<p>First, let’s use the right terminology. What we’ve so far called a “Genji-mon,”
mathematicians would call a <a href="https://en.wikipedia.org/wiki/Partition_of_a_set">partition</a>. In mathematical terms, the question
we’re asking is, “How many distinct partitions are there for a set of $n$
elements?” This number also has a name: the <a href="https://en.wikipedia.org/wiki/Bell_number">Bell number</a> denoted $B_n$.</p>

<p>Above, we calculated $B_1$ through $B_4$ using a mix of intuition and common
sense. To formalize the strategy we used in mathematical notation we’ll need a
concept you may or may not have seen before: “the number of ways to choose $k$
elements from $n$ distinct elements, ignoring order” is called “$n$ choose $k$”
or the <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a> and is denoted $nCk$ or with this tall
bracket notation:</p>

<p>\[
    \binom{n}{k} = \frac{n!}{k! (n-k)!}
\]</p>

<p>There are many ways of deriving the equation in terms of factorials, but here’s
one I like: imagine we put all $n$ elements in order; there are $n!$ ways of
doing that. Then we always take the $k$ leftmost elements for our choice. However,
because order doesn’t matter, we divided by all the different ways of ordering
the $k$ chosen elements, which is $k!$, and the $n-k$ remaining elements, which
is $(n-k)!$.</p>

<p>With that tool in hand, we can define the Bell numbers recursively. The first
couple can be treated as special cases, since obviously there’s only one way to
partition a set of zero or one elements:</p>

<p>\[
    B_0 = 1,  B_1 = 1
\]</p>

<p>For $n &gt; 1$, we generalize the strategy we discovered above:</p>

<ol>
<li>Pick an arbitrary element to represent the “first element.”</li>
<li>We’ll call whichever set in the partition that contains this first element
the “first set.” Every element is in exactly one set of the partition, so this
uniquely picks out a particular set in the partition.</li>
<li>For each $k$ between $1$ and $n$, consider only partitions where the first
set is of size $k$. This divides the problem up into non-overlapping buckets:
if two partitions have different sized first set, they cannot
possibly be the same.</li>
<li>We have to make a choice about the other $k-1$ elements to include in the
first set, and there are $\binom{n-1}{k-1}$ ways of doing that.</li>
<li>Regardless of which elements we choose for the first set, there will always
be $n-k$ elements left over. They won’t always be the same elements,
but there will always be $n-k$ of them. Thankfully, we already know how many
ways there are to partition a set of $n-k$ elements: it’s $B_{n-k}$.</li>
<li>Since our choices for step 4 and step 5 are independent, we can <em>multiply</em>
the two counts together to get the total number of partitions where the
first set is of size $k$.</li>
<li>Finally, we just have to add up everything for $k$ from $1$ to $n$.</li>
</ol>

<p>In concise mathematical notation, this algorithm is:</p>

<p>\[
    B_{n} = \sum_{k=1}^{n} \binom{n-1}{k-1} B_{n-k}   \tag{1}
\]</p>

<p>We can make this a little neater if we run $k$ from $0$ to $n-1$ instead and
use the fact that $\binom{n}{r} = \binom{n}{n-r}$ to count down instead of up:</p>

<p>\[
    B_{n} = \sum_{k=0}^{n-1} \binom{n-1}{k} B_{k}     \tag{2}
\]</p>

<p>Substituting $n+1$ for $n$ we can put the recurrence relation in an even tidier
form, which is the canonical form you’ll find in textbooks:</p>

<p>\[
    B_{n+1} = \sum_{k=0}^n \binom{n}{k} B_k           \tag{3}
\]</p>

<p>Equation $(3)$ looks a little cleaner and easier to work with, and can be
understood intuitively if you reconceptualize $k$ not as the number of elements
in the first group, but as the number of elements <em>not</em> in the first group.
Shifting to calculating $B_{n+1}$ also allows us to get rid of the “minus
ones” in the original that made the expression seem messy. However, it’s a
little divorced from the intuition about pinning the size of the first set we
used to motivate $(1)$ although of course they’re completely equivalent
mathematically.</p>

<h2 id="computing-bell-numbers">Computing Bell Numbers</h2>

<p>Of these three equivalent equations, $(2)$ is the most natural fit for a Python
implementation because <code>range(n)</code> naturally runs from <code>0</code> to <code>n-1</code> and it makes
far more sense to implement a function for $B_n$ instead of $B_{n+1}$:</p>

<pre><code>def bell_number(n: int) -&gt; int:
    """Calculate the Bell number for any integer `n`."""
    if n &lt; 0:
        raise ValueError("The Bell number is not defined for n &lt; 0.")
    elif n &lt; 2:
        return 1
    else:
        return sum(
            comb(n-1, k) * bell_number(k)
            for k in range(n)
        )
</code></pre>

<p>(Optimizing this function is left as an exercise to the reader, who may find the
techniques described in my earlier article on writing <a href="https://www.oranlooney.com/post/fibonacci/">a fairly fast Fibonacci
function</a> helpful.)</p>

<p>We can use it to calculate the first 20 Bell numbers:</p>

<div id="bell-table">
    <table>
        <thead>
            <tr>
                <th>$n$</th>
                <th>$B_n$</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>0</td>
                <td>1</td>
            </tr>
            <tr>
                <td>1</td>
                <td>1</td>
            </tr>
            <tr>
                <td>2</td>
                <td>2</td>
            </tr>
            <tr>
                <td>3</td>
                <td>5</td>
            </tr>
            <tr>
                <td>4</td>
                <td>15</td>
            </tr>
            <tr>
                <td>5</td>
                <td>52</td>
            </tr>
            <tr>
                <td>6</td>
                <td>203</td>
            </tr>
            <tr>
                <td>7</td>
                <td>877</td>
            </tr>
            <tr>
                <td>8</td>
                <td>4,140</td>
            </tr>
            <tr>
                <td>9</td>
                <td>21,147</td>
            </tr>
            <tr>
                <td>10</td>
                <td>115,975</td>
            </tr>
            <tr>
                <td>11</td>
                <td>678,570</td>
            </tr>
            <tr>
                <td>12</td>
                <td>4,213,597</td>
            </tr>
            <tr>
                <td>13</td>
                <td>27,644,437</td>
            </tr>
            <tr>
                <td>14</td>
                <td>190,899,322</td>
            </tr>
            <tr>
                <td>15</td>
                <td>1,382,958,545</td>
            </tr>
            <tr>
                <td>16</td>
                <td>10,480,142,147</td>
            </tr>
            <tr>
                <td>17</td>
                <td>82,864,869,804</td>
            </tr>
            <tr>
                <td>18</td>
                <td>682,076,806,159</td>
            </tr>
            <tr>
                <td>19</td>
                <td>5,832,742,205,057</td>
            </tr>
            <tr>
                <td>20</td>
                <td>51,724,158,235,372</td>
            </tr>
        </tbody>
    </table>
</div>

<p>And there it is: $B_5 = 52$, confirming that there are exactly 52 Genji-mon,
no more and no fewer.</p>

<h2 id="conclusion">Conclusion</h2>

<p>It’s not too surprising that some of these ideas were worked out over seven
hundred years ago; combinatorics is an easy branch to stumble into when it
arises in connection to some practical problem. It does, however, feel slightly
surreal that it was a bunch of bored nobles playing an esoteric parlor game who
first noticed these patterns and used it to attach literary significance to
their activities. But I’m happy they did so, because they did something we mere
number crunchers would not have thought to do: they made them beautiful.</p>

<hr>

<h2 id="appendices">Appendices</h2>

<h3 id="appendix-a-source-code">Appendix A: Source Code</h3>

<p>The full <a href="https://github.com/olooney/genjiko">source code</a> use for this article is available on GitHub. The
main Python code is in <a href="https://github.com/olooney/genjiko/blob/main/src/genjiko.py">src/genjiko.py</a> and the <a href="https://github.com/olooney/genjiko/tree/main/notebooks">notebooks</a>
directory contains many examples of usage.</p>

<h3 id="appendix-b-alternative-genji-kō-chart-a-name-chart-a">Appendix B: Alternative Genji-Kō Chart<a name="chart"></a></h3>

<p>Genji-mon are often rendered with thick lines which achieves an interesting
effect with the negative space. By playing around with the parameters a little:</p>

<pre><code>genjiko_df = load_genjiko()
genjiko_df['Color'] = "black"
draw_annotated_genjiko_grid(
    genjiko_df,
    cell_size=82,
    grid_width=8,
    grid_height=7,
    line_width=14,
    padding=20,
    include_index_label=False,
    include_romaji_label=False,
    grid_indent=1,
)
</code></pre>

<p>We can achieve a very attractive result:</p>

<p><img src="https://www.oranlooney.com/post/genji-ko_files/genjiko_dense.png"></p>

<h3 id="table">Appendix C: Full Table</h3>

<p>Here is the full table in HTML format, so you can copy-and-paste the kanji and other
fields. The Genji-mon column uses the <a href="https://www.illllli.com/font/symbol/genjiko/">Genji-Kō TrueType font available from
illllli.com</a>.</p>

<p>You can also download this same table as a <a href="https://www.oranlooney.com/post/genji-ko_files/genjiko.csv">UTF-8 encoded CSV file</a>
or <a href="https://www.oranlooney.com/post/genji-ko_files/genjiko.xlsx">Excel spreadsheet</a>.</p>

<div>
    
    <table>
        <thead>
            <tr>
                <th>Chapter</th>
                <th>Kanji</th>
                <th>Romaji</th>
                <th>English</th>
                <th>Partition</th>
                <th>Genji-mon</th>
            </tr>
        </thead>
        <tbody>
            
            <tr>
                <td>2</td>
                <td>帚木</td>
                <td>Hōkigi</td>
                <td>The Broom Tree</td>
                <td>{1}, {2}, {3}, {4}, {5}</td>
                <td>B</td>
            </tr>
            
            <tr>
                <td>3</td>
                <td>空蝉</td>
                <td>Utsusemi</td>
                <td>Utsusemi</td>
                <td>{1}, {2}, {3}, {4, 5}</td>
                <td>C</td>
            </tr>
            
            <tr>
                <td>4</td>
                <td>夕顔</td>
                <td>Yūgao</td>
                <td>Yūgao</td>
                <td>{1}, {2}, {3, 4}, {5}</td>
                <td>D</td>
            </tr>
            
            <tr>
                <td>5</td>
                <td>若紫</td>
                <td>Wakamurasaki</td>
                <td>Young Murasaki</td>
                <td>{1}, {2, 3}, {4, 5}</td>
                <td>E</td>
            </tr>
            
            <tr>
                <td>6</td>
                <td>末摘花</td>
                <td>Suetsumuhana</td>
                <td>The Saffron Flower</td>
                <td>{1, 2, 3, 4}, {5}</td>
                <td>F</td>
            </tr>
            
            <tr>
                <td>7</td>
                <td>紅葉賀</td>
                <td>Momijinoga</td>
                <td>The Festival of Red Leaves</td>
                <td>{1}, {2, 3, 5}, {4}</td>
                <td>G</td>
            </tr>
            
            <tr>
                <td>8</td>
                <td>花宴</td>
                <td>Hana no En</td>
                <td>The Flower Feast</td>
                <td>{1}, {2}, {3, 5}, {4}</td>
                <td>H</td>
            </tr>
            
            <tr>
                <td>9</td>
                <td>葵</td>
                <td>Aoi</td>
                <td>Aoi</td>
                <td>{1, 2}, {3}, {4}, {5}</td>
                <td>I</td>
            </tr>
            
            <tr>
                <td>10</td>
                <td>賢木</td>
                <td>Sakaki</td>
                <td>The Sacred Tree</td>
                <td>{1, 2, 3}, {4, 5}</td>
                <td>J</td>
            </tr>
            
            <tr>
                <td>11</td>
                <td>花散里</td>
                <td>Hana Chiru Sato</td>
                <td>The Village of Falling Flowers</td>
                <td>{1}, {2, 4}, {3, 5}</td>
                <td>K</td>
            </tr>
            
            <tr>
                <td>12</td>
                <td>須磨</td>
                <td>Suma</td>
                <td>Exile at Suma</td>
                <td>{1, 3, 4}, {2, 5}</td>
                <td>L</td>
            </tr>
            
            <tr>
                <td>13</td>
                <td>明石</td>
                <td>Akashi</td>
                <td>Akashi</td>
                <td>{1}, {2, 3}, {4}, {5}</td>
                <td>M</td>
            </tr>
            
            <tr>
                <td>14</td>
                <td>澪標</td>
                <td>Miotsukushi</td>
                <td>The Flood Gauge</td>
                <td>{1}, {2, 4, 5}, {3}</td>
                <td>N</td>
            </tr>
            
            <tr>
                <td>15</td>
                <td>蓬生</td>
                <td>Yomogiu</td>
                <td>The Palace in the Tangled Woods</td>
                <td>{1, 2, 3}, {4}, {5}</td>
                <td>O</td>
            </tr>
            
            <tr>
                <td>16</td>
                <td>関屋</td>
                <td>Sekiya</td>
                <td>A Meeting at the Frontier</td>
                <td>{1}, {2, 3, 4}, {5}</td>
                <td>P</td>
            </tr>
            
            <tr>
                <td>17</td>
                <td>絵合</td>
                <td>Eawase</td>
                <td>The Picture Competition</td>
                <td>{1, 3}, {2, 5}, {4}</td>
                <td>Q</td>
            </tr>
            
            <tr>
                <td>18</td>
                <td>松風</td>
                <td>Matsukaze</td>
                <td>The Wind in the Pine Trees</td>
                <td>{1, 2}, {3, 4}, {5}</td>
                <td>R</td>
            </tr>
            
            <tr>
                <td>19</td>
                <td>薄雲</td>
                <td>Usugumo</td>
                <td>A Wreath of Cloud</td>
                <td>{1}, {2, 3, 4, 5}</td>
                <td>S</td>
            </tr>
            
            <tr>
                <td>20</td>
                <td>朝顔</td>
                <td>Asagao</td>
                <td>Asagao</td>
                <td>{1, 3, 4}, {2}, {5}</td>
                <td>T</td>
            </tr>
            
            <tr>
                <td>21</td>
                <td>乙女</td>
                <td>Otome</td>
                <td>The Maiden</td>
                <td>{1, 3}, {2}, {4}, {5}</td>
                <td>U</td>
            </tr>
            
            <tr>
                <td>22</td>
                <td>玉鬘</td>
                <td>Tamakazura</td>
                <td>Tamakatsura</td>
                <td>{1, 2}, {3, 4, 5}</td>
                <td>V</td>
            </tr>
            
            <tr>
                <td>23</td>
                <td>初音</td>
                <td>Hatsune</td>
                <td>The First Song of the Year</td>
                <td>{1, 3}, {2, 4}, {5}</td>
                <td>W</td>
            </tr>
            
            <tr>
                <td>24</td>
                <td>胡蝶</td>
                <td>Kochō</td>
                <td>The Butterflies</td>
                <td>{1, 4}, {2, 3, 5}</td>
                <td>X</td>
            </tr>
            
            <tr>
                <td>25</td>
                <td>蛍</td>
                <td>Hotaru</td>
                <td>The Glow-Worm</td>
                <td>{1, 2, 4}, {3}, {5}</td>
                <td>Y</td>
            </tr>
            
            <tr>
                <td>26</td>
                <td>常夏</td>
                <td>Tokonatsu</td>
                <td>A Bed of Carnations</td>
                <td>{1}, {2}, {3, 4, 5}</td>
                <td>Z</td>
            </tr>
            
            <tr>
                <td>27</td>
                <td>篝火</td>
                <td>Kagaribi</td>
                <td>The Flares</td>
                <td>{1}, {2, 4}, {3}, {5}</td>
                <td>a</td>
            </tr>
            
            <tr>
                <td>28</td>
                <td>野分</td>
                <td>Nowaki</td>
                <td>The Typhoon</td>
                <td>{1, 2}, {3}, {4, 5}</td>
                <td>b</td>
            </tr>
            
            <tr>
                <td>29</td>
                <td>御幸</td>
                <td>Miyuki</td>
                <td>The Royal Visit</td>
                <td>{1, 3}, {2, 4, 5}</td>
                <td>c</td>
            </tr>
            
            <tr>
                <td>30</td>
                <td>藤袴</td>
                <td>Fujibakama</td>
                <td>Blue Trousers</td>
                <td>{1, 4}, {2}, {3}, {5}</td>
                <td>d</td>
            </tr>
            
            <tr>
                <td>31</td>
                <td>真木柱</td>
                <td>Makibashira</td>
                <td>Makibashira</td>
                <td>{1, 5}, {2, 4}, {3}</td>
                <td>e</td>
            </tr>
            
            <tr>
                <td>32</td>
                <td>梅枝</td>
                <td>Umegae</td>
                <td>The Spray of Plum Blossom</td>
                <td>{1, 2, 3, 5}, {4}</td>
                <td>f</td>
            </tr>
            
            <tr>
                <td>33</td>
                <td>藤裏葉</td>
                <td>Fuji no Uraba</td>
                <td>Fuji no Uraba</td>
                <td>{1}, {2, 5}, {3, 4}</td>
                <td>g</td>
            </tr>
            
            <tr>
                <td>34</td>
                <td>若菜上</td>
                <td>Wakana Jō</td>
                <td>Wakana, Part I</td>
                <td>{1, 2, 5}, {3, 4}</td>
                <td>h</td>
            </tr>
            
            <tr>
                <td>35</td>
                <td>若菜下</td>
                <td>Wakana Ge</td>
                <td>Wakana, Part II</td>
                <td>{1, 3}, {2}, {4, 5}</td>
                <td>i</td>
            </tr>
            
            <tr>
                <td>36</td>
                <td>柏木</td>
                <td>Kashiwagi</td>
                <td>Kashiwagi</td>
                <td>{1, 3, 5}, {2}, {4}</td>
                <td>j</td>
            </tr>
            
            <tr>
                <td>37</td>
                <td>横笛</td>
                <td>Yokobue</td>
                <td>The Flute</td>
                <td>{1, 4, 5}, {2}, {3}</td>
                <td>k</td>
            </tr>
            
            <tr>
                <td>38</td>
                <td>鈴虫</td>
                <td>Suzumushi</td>
                <td>The Bell Cricket</td>
                <td>{1, 5}, {2}, {3, 4}</td>
                <td>l</td>
            </tr>
            
            <tr>
                <td>39</td>
                <td>夕霧</td>
                <td>Yūgiri</td>
                <td>Yūgiri</td>
                <td>{1, 4}, {2}, {3, 5}</td>
                <td>m</td>
            </tr>
            
            <tr>
                <td>40</td>
                <td>御法</td>
                <td>Minori</td>
                <td>The Law</td>
                <td>{1, 4}, {2, 5}, {3}</td>
                <td>n</td>
            </tr>
            
            <tr>
                <td>41</td>
                <td>幻</td>
                <td>Maboroshi</td>
                <td>Mirage</td>
                <td>{1, 5}, {2}, {3}, {4}</td>
                <td>o</td>
            </tr>
            
            <tr>
                <td>42</td>
                <td>匂宮</td>
                <td>Nioumiya</td>
                <td>Niou</td>
                <td>{1, 2, 4}, {3, 5}</td>
                <td>p</td>
            </tr>
            
            <tr>
                <td>43</td>
                <td>紅梅</td>
                <td>Kōbai</td>
                <td>Kōbai</td>
                <td>{1}, {2, 5}, {3}, {4}</td>
                <td>q</td>
            </tr>
            
            <tr>
                <td>44</td>
                <td>竹河</td>
                <td>Takekawa</td>
                <td>Bamboo River</td>
                <td>{1, 5}, {2, 3, 4}</td>
                <td>r</td>
            </tr>
            
            <tr>
                <td>45</td>
                <td>橋姫</td>
                <td>Hashihime</td>
                <td>The Bridge Maiden</td>
                <td>{1, 3, 4, 5}, {2}</td>
                <td>s</td>
            </tr>
            
            <tr>
                <td>46</td>
                <td>椎本</td>
                <td>Shiigamoto</td>
                <td>At the Foot of the Oak Tree</td>
                <td>{1, 4}, {2, 3}, {5}</td>
                <td>t</td>
            </tr>
            
            <tr>
                <td>47</td>
                <td>総角</td>
                <td>Agemaki</td>
                <td>Agemaki</td>
                <td>{1, 4, 5}, {2, 3}</td>
                <td>u</td>
            </tr>
            
            <tr>
                <td>48</td>
                <td>早蕨</td>
                <td>Sawarabi</td>
                <td>Fern Shoots</td>
                <td>{1, 2}, {3, 5}, {4}</td>
                <td>v</td>
            </tr>
            
            <tr>
                <td>49</td>
                <td>宿木</td>
                <td>Yadorigi</td>
                <td>The Mistletoe</td>
                <td>{1, 2, 4, 5}, {3}</td>
                <td>w</td>
            </tr>
            
            <tr>
                <td>50</td>
                <td>東屋</td>
                <td>Azumaya</td>
                <td>The Eastern House</td>
                <td>{1, 2, 5}, {3}, {4}</td>
                <td>x</td>
            </tr>
            
            <tr>
                <td>51</td>
                <td>浮舟</td>
                <td>Ukifune</td>
                <td>Ukifune</td>
                <td>{1, 5}, {2, 3}, {4}</td>
                <td>y</td>
            </tr>
            
            <tr>
                <td>52</td>
                <td>蜻蛉</td>
                <td>Kagerō</td>
                <td>The Gossamer Fly</td>
                <td>{1, 3, 5}, {2, 4}</td>
                <td>z</td>
            </tr>
            
            <tr>
                <td>53</td>
                <td>手習</td>
                <td>Tenarai</td>
                <td>Writing Practice</td>
                <td>{1, 2, 3, 4, 5}</td>
                <td>1</td>
            </tr>
            
        </tbody>
    </table>
</div>

<p>Note that whenever the English column has apparently been left untranslated,
this is because the chapter title is the proper name of one of the characters
from <a href="https://en.wikipedia.org/wiki/The_Tale_of_Genji"><em>The Tale of Genji</em></a>. Translating these would be as nonsensical as
translating “Jack Smith” to “Lifting Device Metal Worker.”</p>

<h3 id="appendix-d-names-for-genji-kō-pattern">Appendix D: Names for Genji-Kō Pattern</h3>

<p id="names">
This table is included merely to illustrate the variety of legitimate ways
to refer to the patterns used in Genji-kō, and to justify my choice to
standardize on Genji-mon. Click on any of the kanji to link directly to
the Google Image Search for that name.
</p>

<table>
  <thead>
    <tr>
      <th>Kanji</th>
      <th>Romaji</th>
      <th>English Translation</th>
      <th>Count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://www.google.com/search?tbm=isch&amp;q=%E6%BA%90%E6%B0%8F%E7%B4%8B" target="_blank">源氏紋</a></td>
      <td>Genji-mon</td>
      <td>Genji Crest</td>
      <td>844,000</td>
    </tr>
    <tr>
      <td><a href="https://www.google.com/search?tbm=isch&amp;q=%E6%BA%90%E6%B0%8F%E9%A6%99%E5%9B%B3" target="_blank">源氏香図</a></td>
      <td>Genji-kōzu</td>
      <td>Genji-kō Diagram</td>
      <td>686,000</td>
    </tr>
    <tr>
      <td><a href="https://www.google.com/search?tbm=isch&amp;q=%E6%BA%90%E6%B0%8F%E9%A6%99" target="_blank">源氏香の模様</a></td>
      <td>Genji-kō no Moyō</td>
      <td>Genji-kō Pattern</td>
      <td>400,000</td>
    </tr>
    <tr>
      <td><a href="https://www.google.com/search?tbm=isch&amp;q=%E6%BA%90%E6%B0%8F%E9%A6%99%E6%A8%A1%E6%A7%98" target="_blank">源氏香模様</a></td>
      <td>Genji-kō Moyō</td>
      <td>Genji-kō Design</td>
      <td>479,000</td>
    </tr>
    <tr>
      <td><a href="https://www.google.com/search?tbm=isch&amp;q=%E6%BA%90%E6%B0%8F%E9%A6%99%E6%96%87%E6%A7%98" target="_blank">源氏香文様</a></td>
      <td>Genji-kō Monyō</td>
      <td>Genji-kō Motif</td>
      <td>129,000</td>
    </tr>
  </tbody>
</table>

<h3 id="appendix-e-asymptotic-behavior">Appendix E: Asymptotic Behavior</h3>

<p>The Bell numbers grow very fast. The asymptotic growth is approximately:</p>

<p>\[
    B_n \sim \frac{1}{\sqrt{2 \pi n}} \left( \frac{n}{\ln n} \right)^n
\]</p>

<p>Which is just a tiny bit slower than factorials, as you can see if you compare
it to <a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s approximation</a>.</p>



<p id="footnote1">
    <sup><a href="#maintext1">†</a></sup>
    By "inside", I mean which respect to interval logic, not set containment. Obviously
    no group will be a subset of another, because no incense belongs to more than one
    group. But when the leftmost element of a group is to the left of the leftmost
    element of another, and likewise <i>mutatis mutandis</i> for the rightmost, then
    visually the second group is inside the first.
    <a href="#maintext1">Back</a>
</p>

<p id="footnote2">
  <sup><a href="#maintext2">*</a></sup>
  I know I should cite the creators of these misguided images, but I have not done so to spare any potential embarrassment.
  You can find the originals through a Google reverse image search if you're curious.
  <a href="#maintext2">Back</a>
</p>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: TeaTime – distributed book library powered by SQLite, IPFS and GitHub (203 pts)]]></title>
            <link>https://github.com/bjesus/teatime</link>
            <guid>42256104</guid>
            <pubDate>Wed, 27 Nov 2024 13:56:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bjesus/teatime">https://github.com/bjesus/teatime</a>, See on <a href="https://news.ycombinator.com/item?id=42256104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">
📚 TeaTime
</h2><a id="user-content--teatime" aria-label="Permalink: 
📚 TeaTime
" href="#-teatime"></a></p>
<p dir="auto">
TeaTime is a fully static distributed library system powered by IPFS, SQLite, and GitHub
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/55081/382481806-caacdcde-8ad9-49af-915f-c93a52c7bf11.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI3Njg1MDIsIm5iZiI6MTczMjc2ODIwMiwicGF0aCI6Ii81NTA4MS8zODI0ODE4MDYtY2FhY2RjZGUtOGFkOS00OWFmLTkxNWYtYzkzYTUyYzdiZjExLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDA0MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFiODQ1YjY3NTlhNDg1NDYwMzhjNGI0ZDNjYmU4ZGRiZGQ2NjIxNzdmYzE5MzMxZjMxNmYyN2QxYmI2MDBlODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.WMCKc85qHCcaVcNyALvFxUBHl1QKtyxh4pIaNiu6FvQ"><img src="https://private-user-images.githubusercontent.com/55081/382481806-caacdcde-8ad9-49af-915f-c93a52c7bf11.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzI3Njg1MDIsIm5iZiI6MTczMjc2ODIwMiwicGF0aCI6Ii81NTA4MS8zODI0ODE4MDYtY2FhY2RjZGUtOGFkOS00OWFmLTkxNWYtYzkzYTUyYzdiZjExLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDExMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMTI4VDA0MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFiODQ1YjY3NTlhNDg1NDYwMzhjNGI0ZDNjYmU4ZGRiZGQ2NjIxNzdmYzE5MzMxZjMxNmYyN2QxYmI2MDBlODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.WMCKc85qHCcaVcNyALvFxUBHl1QKtyxh4pIaNiu6FvQ" height="300"></a> <br>
Auto-updating instances are hosted on <a href="https://teatime-library.netlify.app/" rel="nofollow">Netlify</a> and <a href="https://bjesus.github.io/teatime/" rel="nofollow">GitHub Pages</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">A Distributed Library</h2><a id="user-content-a-distributed-library" aria-label="Permalink: A Distributed Library" href="#a-distributed-library"></a></p>
<p dir="auto">The TeaTime web application is completely decoupled from its databases and the files it fetches. The databases used in TeaTime are <a href="https://github.com/search?q=topic%3Ateatime-database&amp;type=repositories">GitHub repositories tagged with the teatime-database topic</a>, which are published on GitHub Pages. Each repository contains a <a href="https://github.com/bjesus/teatime-database/blob/main/config.json">config.json</a> file that points to an SQLite database. Before a user performs a search in TeaTime, they choose which database to use and then TeaTime queries the SQLite database using <a href="https://github.com/phiresky/sql.js-httpvfs">sql.js-httpvfs</a>. Each row in the SQLite database is an item in the library, and a file hash column is used for getting the item from IPFS.</p>
<p dir="auto">Since the web application is a static site, and the databases are comprised of static files, both can be easily forked, replicated, and deployed. Frontend instances are <a href="https://github.com/search?q=topic%3Ateatime-instance&amp;type=repositories">GitHub repositories tagged with the teatime-instance topic</a>. With the files being served off IPFS, this distributed architecture contributes to TeaTime's resilience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Search by title, author, year or format</li>
<li>Maintain reading history, and return to page when re-opening file</li>
<li>Download files locally</li>
<li>Cache files in IndexedDB for fast loading</li>
<li>Drop files on TeaTime to render them</li>
<li>Dark mode and full screen mode</li>
<li>No cookies, no login</li>
<li><strong>...Completely distributed</strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developing the Frontend</h2><a id="user-content-developing-the-frontend" aria-label="Permalink: Developing the Frontend" href="#developing-the-frontend"></a></p>
<p dir="auto">TeaTime is Nuxt.js application. You can easily run it locally by cloning the repository and following these steps:</p>
<ol dir="auto">
<li>Install the dependencies: <code>npm install</code></li>
<li>Run the server: <code>npm run dev</code></li>
<li>Navigate to <code>http://localhost:3000</code></li>
</ol>
<p dir="auto">Check out the <a href="https://nuxt.com/docs/getting-started" rel="nofollow">Nuxt documentation</a> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Creating a Database</h2><a id="user-content-creating-a-database" aria-label="Permalink: Creating a Database" href="#creating-a-database"></a></p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">The easiest way to create your own database is by forking the <a href="https://github.com/bjesus/teatime-json-database/">JSON-based database repository</a> and adjusting the JSON files according to your needs. GitHub Actions will then generate an SQLite file and upload it to GitHub Pages.</p>
</div>
<p dir="auto">To manually generate an SQLite database that TeaTime can work with, follow the example on <a href="https://github.com/bjesus/teatime-database/">the database repository</a>.</p>
<p dir="auto">Each SQLite database contains a table with the below schema. Note that column names can be adjusted in the <code>config.json</code> file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE TABLE &quot;books&quot; (
 &quot;id&quot; INTEGER,
 &quot;title&quot; TEXT,
 &quot;author&quot; TEXT,
 &quot;year&quot; INTEGER,
 &quot;lang&quot; TEXT,
 &quot;size&quot; INTEGER,
 &quot;ext&quot; TEXT,
 &quot;ipfs_cid&quot; TEXT,
 PRIMARY KEY(&quot;id&quot; AUTOINCREMENT)
);"><pre><span>CREATE</span> <span>TABLE</span> "<span>books</span>" (
 <span><span>"</span>id<span>"</span></span> <span>INTEGER</span>,
 <span><span>"</span>title<span>"</span></span> <span>TEXT</span>,
 <span><span>"</span>author<span>"</span></span> <span>TEXT</span>,
 <span><span>"</span>year<span>"</span></span> <span>INTEGER</span>,
 <span><span>"</span>lang<span>"</span></span> <span>TEXT</span>,
 <span><span>"</span>size<span>"</span></span> <span>INTEGER</span>,
 <span><span>"</span>ext<span>"</span></span> <span>TEXT</span>,
 <span><span>"</span>ipfs_cid<span>"</span></span> <span>TEXT</span>,
 <span>PRIMARY KEY</span>(<span><span>"</span>id<span>"</span></span> AUTOINCREMENT)
);</pre></div>
<p dir="auto">The <code>dbConfig</code> section of <code>config.json</code> is identical to the output of the <a href="https://github.com/phiresky/sql.js-httpvfs/blob/master/create_db.sh">sql.js-httpvfs create_db.sh</a> script.</p>
<p dir="auto">If the SQLite file is too big, you can <a href="https://github.com/phiresky/sql.js-httpvfs?tab=readme-ov-file#usage">split it</a>. Note the information about optimizing your database. You will also want to <a href="https://github.com/bjesus/teatime-database/blob/main/create_indexes.sql">use FTS</a>. Then, publish your repository to GitHub Pages and assign the <code>teatime-database</code> topic to your repository.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Even if you cannot code, a great way to contribute is to simply fork this repository, as well as your favorite database repositories. If you fork the repository, it could be better to do it manually (<code>git clone</code> &amp;&amp; <code>git remote add your-origin ...</code> &amp;&amp; <code>git push your-origin main</code>) so that the repositories won't be directly linked.</p>
<p dir="auto">It's also a good practice to star the database repositories you find useful, as this determines their order in the TeaTime user interface, making it easier for other users to find the best databases.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Recommendation for a SWE looking to get up to speed with latest on AI (224 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42256093</link>
            <guid>42256093</guid>
            <pubDate>Wed, 27 Nov 2024 13:55:20 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42256093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="42256405"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256405" href="https://news.ycombinator.com/vote?id=42256405&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>The poster's looking for articles, so this recommendation's a bit off the mark. I learned more from participating in a few Kaggle competitions (<a href="https://www.kaggle.com/competitions" rel="nofollow">https://www.kaggle.com/competitions</a>) than I did from reading about AI. Many folks in the community shared their homework, and by learning how to follow their explanations I developed a much more intuitive understanding of the technology. The first competition had a steep learning curve. I felt it was worth it. The application of having a specific goal and the provided datasets made the problem space more tractable.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42256720"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42256720" href="https://news.ycombinator.com/vote?id=42256720&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>Out of sheer curiosity, how much time did you spend on it on average? How much of this knowledge are you using now?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42256785"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42256785" href="https://news.ycombinator.com/vote?id=42256785&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Not the poster you responded to but I learned quite a bit from kaggle too.</p><p>I started from scratch, spent 2-4 hrs per day for 6 months &amp; won a silver in a kaggle NLP competition.  Now I use some of it now but not all of it. More than that, I'm quite comfortable with models, understand the costs/benefits/implications etc. I started with Andrew Ng's intro courses, did a bit of fastai, did Karpathy's Zero to Hero fully, all of Kaggle's courses &amp; a few other such things. Kagglers share excellent notebooks and I found them v helpful. Overall I highly recommend this route of learning.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257032"><td></td></tr>
            <tr id="42256925"><td></td></tr>
                              <tr id="42256144"><td></td></tr>
                <tr id="42256502"><td></td></tr>
                  <tr id="42256403"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256403" href="https://news.ycombinator.com/vote?id=42256403&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>I don't think it's a good idea to kepp up to date at a daily/weekly cadence, unless you somehow directly get paid for it. It's like checking stocks daily, it doesn't lead to good investment decisions.</p><p>It's better to do it more batchy, like once every 6-12 months or so.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42256452"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42256452" href="https://news.ycombinator.com/vote?id=42256452&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>How do you do that? Once you're out of the loop for half a year, it becomes harder to know what's important and what's not, I think.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257829"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42257829" href="https://news.ycombinator.com/vote?id=42257829&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Some ideas:</p><p>1. Buy O'reilly (and other tech) books as they come out. This will have a lag, but essentially somebody did this research &amp; summarization work, and wrote it up for you in chapters. Note that you don't have to read everything in a book. Also, $50 is a great investment if it saves you 10s of hours of time.</p><p>2. Talks on Youtube at conferences by industry leaders, like Yann LeCun, or maintainers of popular libraries, etc. Also, YT videos on the topic that are upvoted/linked.</p><p>3. If you're interested in hardcore research, look for review articles on arxiv.</p><p>4. Look at tutorials/examples in the documentation/repo of popular ML/AI libraries, like Pytorch.</p><p>5. Try to cover your blindspots. One way or another, you'll know how new AI is applied to SWE and related fields. But how is AI applied to perpendicular fields, like designing buildings, composing music, or balancing a budget? Trying to cover these areas will be tougher, because it will be more noisy, as most commenters will be non-experts compared to you. To get a feel for this, do something that feels unnatural, like watch TED talks that seem bullshity, read HBR articles intended for MBAs, and check out what Palantir is doing.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42256624"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42256624" href="https://news.ycombinator.com/vote?id=42256624&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Every release is novel. Once something has been around for a while and is still being referenced, you know it’s worth learning.</p><p>Waiting 3-6 months to take a deep dive is a good pattern to prevent investing your time in dead-end routes.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257570"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42257570" href="https://news.ycombinator.com/vote?id=42257570&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>Yes this is why I never buy the latest CPUs and try to never run the latest release of any software. Stay a (supported) release or two behind the bleeding edge, and you'll find stuff is more stable. Common bugs and other issues have been shaken out by the early adopters.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42257627"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42257627" href="https://news.ycombinator.com/vote?id=42257627&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>For news-like content I follow accounts on X: @kimmonismus @apples_jimmy and the accounts of Antropic, Mistal, Gemini / DeepMind and OpenAI.
I think everyone who is really interested in the hot AI developments must also follow what comes from China. I follow <a href="https://chinai.substack.com/" rel="nofollow">https://chinai.substack.com/</a> but I am open to hear about other Chinese resources.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42257607"><td></td></tr>
            <tr id="42257387"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42257387" href="https://news.ycombinator.com/vote?id=42257387&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>I read about 30 LLM papers a couple months ago dated from 2018-2024. Mostly folks are publishing on the “how do we prompt better” problem, and you can kind of get the gist in about a day by reading a few blogs (RAG, fine tuning, tool use, etc). There is also more progress being made for model capabilities, like multi modality, and each company seems to be pushing in only slightly different directions, but essentially they are still black boxes.</p><p>It depends what you are looking for honestly “the latest things happening” is pretty vague. I’d say the place to look is probably just the blogs of OpenAI/Anthropic/Genini, since they are the only teams with inside information and novel findings to report. Everyone else is just using the tools we are given.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42257629"><td></td></tr>
            <tr id="42256529"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256529" href="https://news.ycombinator.com/vote?id=42256529&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>The best place for the latest information isn't tech blogs in my opinion. It's the stable diffusion and local llama subreddits. If you are looking to learn about everything on a fundamental level you need to check out Andrej Karpathy on YouTube. There other some other notable mentions in other people's comments.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42257251"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42257251" href="https://news.ycombinator.com/vote?id=42257251&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>New short course on FreeCodeCamp YouTube channel looks good -</p><p>Ollama Course – Build AI Apps Locally
<a href="https://youtu.be/GWB9ApTPTv4?feature=shared" rel="nofollow">https://youtu.be/GWB9ApTPTv4?feature=shared</a></p><p>As an aside, does anyone have any ideas about this: there should be an app like an 'auto-RAG' that scrapes RSS feeds and URLs, in addition to ingesting docs, text and content in the normal RAG way. Then you could build AI chat-enabled knowledge resources around specific subjects. Autogenerated summaries and dashboards would provide useful overviews.</p><p>Perhaps this already exists?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257429"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42257429" href="https://news.ycombinator.com/vote?id=42257429&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>&lt;&lt; there should be an app like an 'auto-RAG' that scrapes RSS feeds and URLs,</p><p>I am not aware if that exists yet, but the challenge I see with it is rather simple: you get overwhelmed with information really quickly. In other words, you would still need human somewhere in that process to review those scrapes and the quality of that varies widely. For example, even on HN it is not a given a link will be pure gold ( you still want to check if it fits your use case ).</p><p>That said, as ideas goes, it sounds like a fun weekend project.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42257174"><td></td></tr>
                <tr id="42257385"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42257385" href="https://news.ycombinator.com/vote?id=42257385&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>My issue with YouTube channels that focus on AI news is that they’re heavily incentivized to give you a frequent stream of attention-grabbing news. Week-by-week updates aren’t that helpful. It’s easy to miss the bigger picture and there’s too much content to feel like a good use of time.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257668"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42257668" href="https://news.ycombinator.com/vote?id=42257668&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>I agree with this statement, most YouTube channels are incentivized to keep repeating the same trivial information like how to compose prompts etc</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42257363"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42257363" href="https://news.ycombinator.com/vote?id=42257363&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>First thing you need to do is change your LinkedIn to “AI evangelist” then go to your boss and say I want triple the pay. Then let the chips fall where they may. Oh also rename all your GitHub or personal projects to have AI in the name. You don’t actually have to do much else.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42257076"><td></td></tr>
            <tr id="42257364"><td></td></tr>
            <tr id="42256542"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256542" href="https://news.ycombinator.com/vote?id=42256542&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Simon's blog is fragmented because it's, well, a blog. It would be hard to find a better source to "keep updated on things AI" though. He does do longer summary articles sometimes, but mostly he's keeping up with things in real time. The search and tagging systems on his blog work well, too. I suggest you stick his RSS feed in your feed reader, and follow along that way.</p><p>Swyx also has a lot of stuff keeping up to date at <a href="https://www.latent.space/" rel="nofollow">https://www.latent.space/</a>, including the Latent Space podcast, although tbh I haven't listened to more than one or two episodes.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257239"><td></td></tr>
                  <tr id="42256966"><td></td></tr>
            <tr id="42256621"><td></td></tr>
                <tr id="42257255"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42257255" href="https://news.ycombinator.com/vote?id=42257255&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>daveshap quit ai right? got agi pilled/"oneshotted by ayahuasca" as the kids say</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42257285"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42257285" href="https://news.ycombinator.com/vote?id=42257285&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>He was only gone for a few days, IIRC. At any rate, he's back publishing AI related content again, and it looks like all (?) of his old content is back on his YT channel.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42257131"><td></td></tr>
            <tr id="42256497"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256497" href="https://news.ycombinator.com/vote?id=42256497&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>As I was building up my understanding/intuition for the internals of transformers + attention, I found 3Blue1Brown's series of videos (specifically on attention) to be super helpful.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42256557"><td></td></tr>
                  <tr id="42256670"><td></td></tr>
                <tr id="42257544"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42257544" href="https://news.ycombinator.com/vote?id=42257544&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>Sadly, you'll have to include 4chan /g/'s local models general, which, unfortunately, seems to have top AI researchers posting there (anonymously)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42256540"><td></td></tr>
            <tr id="42256469"><td></td></tr>
            <tr id="42256628"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256628" href="https://news.ycombinator.com/vote?id=42256628&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Reproduce nanogpt.</p><p>Then find a small dataset and see if you can start getting close to some of the reported benchmark numbers with similar architectures.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42256666"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42256666" href="https://news.ycombinator.com/vote?id=42256666&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>Build a tool on top of the LLM layer for a specific use case. That'll get you up to speed. You haven't missed much.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42256810"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42256810" href="https://news.ycombinator.com/vote?id=42256810&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div>
                  <p>Exactly. Avoid intentionally throw-away effort and instead attempt to build something specific and practical. Learn by doing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42257393"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42257393" href="https://news.ycombinator.com/vote?id=42257393&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Lots of good suggestions here already. I'd start by adding one quick note though. "AI" is more than just LLM's. Sure, the "current, trendy, fashionable" thing is all LLM's, but the field as a whole is still much larger. I'd encourage you to not myopically focus on LLM's to exclusion. Depending on your existing background knowledge, there's a lot to be said for going out and getting a copy of <i>Artificial Intelligence: A Modern Approach</i> and reading through it. Likewise for something like <i>Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow</i>.</p><p>Beyond that: there are some decent sub-reddits for keeping up with AI happenings, a lot of good Youtube channels (although a lot of the ones that talk about the "current, trendy" AI stuff tend to be a bit tabloid'ish), and even a couple of Facebook groups. You can also find good signal by choosing the right people to follow on Twitter/LinkedIn/Mastodon/Bluesky/etc.</p><p><a href="https://www.reddit.com/r/artificial/" rel="nofollow">https://www.reddit.com/r/artificial/</a></p><p><a href="https://reddit.com/r/machineLearning/" rel="nofollow">https://reddit.com/r/machineLearning/</a></p><p><a href="https://www.reddit.com/r/LLM/" rel="nofollow">https://www.reddit.com/r/LLM/</a></p><p><a href="https://www.reddit.com/r/agi" rel="nofollow">https://www.reddit.com/r/agi</a></p><p><a href="https://www.reddit.com/r/ollama/" rel="nofollow">https://www.reddit.com/r/ollama/</a></p><p><a href="https://www.youtube.com/@matthew_berman" rel="nofollow">https://www.youtube.com/@matthew_berman</a></p><p><a href="https://www.youtube.com/@TheAiGrid" rel="nofollow">https://www.youtube.com/@TheAiGrid</a></p><p><a href="https://www.youtube.com/@WesRoth" rel="nofollow">https://www.youtube.com/@WesRoth</a></p><p><a href="https://www.youtube.com/@DaveShap" rel="nofollow">https://www.youtube.com/@DaveShap</a></p><p><a href="https://www.youtube.com/c/MachineLearningStreetTalk" rel="nofollow">https://www.youtube.com/c/MachineLearningStreetTalk</a></p><p><a href="https://www.youtube.com/@twimlai" rel="nofollow">https://www.youtube.com/@twimlai</a></p><p><a href="https://www.youtube.com/@YannicKilcher" rel="nofollow">https://www.youtube.com/@YannicKilcher</a></p><p>And you can always go straight to "the source" and follow pre-prints showing up in arXiv.</p><p><a href="https://arxiv.org/corr" rel="nofollow">https://arxiv.org/corr</a></p><p>For tools to make it easier to track new releases, arXiv supports subscriptions to daily digest emails, and also has RSS feeds.</p><p><a href="https://info.arxiv.org/help/subscribe.html" rel="nofollow">https://info.arxiv.org/help/subscribe.html</a></p><p><a href="https://info.arxiv.org/help/rss.html" rel="nofollow">https://info.arxiv.org/help/rss.html</a></p><p>There are also some bots in the Fediverse that push out links to new arXiv papers.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42256846"><td></td></tr>
            <tr id="42256354"><td></td></tr>
                <tr id="42256806"><td></td></tr>
                  <tr id="42256482"><td></td></tr>
            <tr id="42256774"><td></td></tr>
            <tr id="42256565"><td></td></tr>
            <tr id="42257064"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42257064" href="https://news.ycombinator.com/vote?id=42257064&amp;how=up&amp;goto=item%3Fid%3D42256093"></a></center>    </td><td><br><div><p>Are you wanting to get into LLMs in particular or something else? I am a software engineer also trying to make headways into so-called "AI", but I have little interest in LLMs. For one, it's suffering from a major hype bubble right now. The second reason is that because of reason one, it has a huge amount of attention from people who study and work on this every day. It's not something I have the time commitment for to compete with that. Lastly, as mentioned, I have no interest in it and my understanding of them leads me to believe they have few interesting applications besides generating a huge amount of noise in society and dumping heat. The Internet, like blogs, articles, and even YouTube, are already being overrun by LLM-generated material that is effectively worthless. I'm not sure of the net positive for LLMs.</p><p>For me personally, I prefer to work backwards and then forwards. What I mean by that is that I want to understand the basics and fundamentals first. So, I'm, slowly, trying to bone up on my statistics, probability, and information theory and have targeted machine learning books that also take a fundamental approach. There's no end to books in this realm for neural networks, machine learning, etc., so it's hard to recommend beyond what I've just picked, and I'm just getting started anyway.</p><p>If you can get your employer to pay for it, MIT xPRO has courses on machine learning (<a href="https://xpro.mit.edu/programs/program-v1:xPRO+MLx/" rel="nofollow">https://xpro.mit.edu/programs/program-v1:xPRO+MLx/</a> and <a href="https://xpro.mit.edu/courses/course-v1:xPRO+GenAI/" rel="nofollow">https://xpro.mit.edu/courses/course-v1:xPRO+GenAI/</a>). These will likely give a pretty up to date overview of the technologies.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42256640"><td></td></tr>
            <tr id="42256722"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ancient Sumerians created the first writing system (151 pts)]]></title>
            <link>https://lithub.com/how-the-ancient-sumerians-created-the-worlds-first-writing-system/</link>
            <guid>42255829</guid>
            <pubDate>Wed, 27 Nov 2024 13:15:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/how-the-ancient-sumerians-created-the-worlds-first-writing-system/">https://lithub.com/how-the-ancient-sumerians-created-the-worlds-first-writing-system/</a>, See on <a href="https://news.ycombinator.com/item?id=42255829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								        
										<p>“In Uruk he built walls, a great rampart, and the temple of blessed Eanna for the god of the firmament Anu, and for Ishtar the goddess of love. Look at it still today: the outer wall where the cornice runs, it shines with the brilliance of copper; and the inner wall, it has no equal. Touch the threshold, it is ancient.”<br>
–<em>The Epic of Gilgamesh</em>, ca. 1750 BC<br>
*</p><p><span>Article continues after advertisement</span></p>
<p>In the middle of the fourth millennium before Christ, men and women could feed themselves and their families, much of the time, but almost nobody else. They did not yet have the wheel. They could fight, but they did not have the capacity to make war. They could not read or write, for there was no writing. Without writing, there was no history. There were stories but no literature. Art was something that people might produce on their pottery, but never for a living. There were customs but no laws. There were chiefs but no kings, tribes but no nations. The city was unknown.</p>
<p>And then, around that time, civilization was born: urban life, based on nutritional surplus and social organization, characterized by complexity and material culture, much of it made possible by writing. This happened in a very particular part of the world: the flood-prone, drought-wracked, frequently pestilential plain of southern Iraq, where the rivers Tigris and Euphrates meet the Persian Gulf. The plain could be fertile, very fertile, but only when people worked together to irrigate it and control the floods with channels and earthworks; this necessity, most likely, accounts for much of the early surge in social complexity that distinguished the area. Later civilizations would arise independently in two great river valleys not so far away, the Indus and the Nile, but the original organized, literate, urban culture was produced by a far crueler and more challenging environment than either of those.</p>
<p><span>The need for a single script to serve a geography using two such dissimilar languages almost interchangeably was a great spur to the development of early Mesopotamian writing.</span></p><p>This first civilization came to be known as Sumer. By about the year 3000 BC, a city called Uruk near the mouth of the Euphrates River, just inland of the head of the Persian Gulf, had eighty thousand residents. A thousand years later Iraq, the land along the Euphrates and its sister stream, the Tigris, would be named for this early metropolis of Uruk. Sharing the land of Sumer, about the size of Belgium, with a dozen other city-states, Uruk was not always the foremost among its rivals in the land. But for most of its existence, spanning the two millennia of the Sumerian world, Uruk was the greatest city on earth.</p>
<p>The Sumerians invented kingship, priesthood, diplomacy, law, and war. They gave the West its founding stories: the opposition of darkness and light at the Beginning; the Flood, with its ark and dove and surviving patriarch; the tower of Babel; the distant ancestors of Odysseus and Hercules. The Sumerians established the outlines of our political, legal, and temporal structures too, with the first kings and assemblies, the first written laws, the first legal contracts, and the sexagesimal system of counting that regulates the hours and seconds of our days.</p><div><p><span>Article continues after advertisement</span></p></div>
<p>The Sumerians wrote the first epics and constructed the first monumental buildings. They invented the wheel, the sailing boat, the dome, and the arch. They were the first people to cast, rivet, and solder metals. They were the first to develop mathematics, calculating the hypotenuse of a right triangle two thousand years before Pythagoras and enabling extraordinary achievements in civil engineering. Compiling methodical lists of plants and animals, the Sumerians were the first people to apply rational order to our knowledge of the natural world.</p>
<p>The Sumerians wrote down almost everything they knew, much of it on disposable clay tablets that have survived the millennia. Some thirty-nine centuries after the last of the Sumerians died, another inventive and curious people, the Victorians of the nineteenth century AD, initiated a remarkable period of foreign exploration in Iraq. Thanks to this colorful and dramatic intellectual adventure, which began in the 1840s, today we can follow the course of Sumerian lawsuits, track Sumerian inventories, and study the terms of Sumerian marriages, wills, and loans. We read the overtures of Sumer’s diplomats. We follow in detail the provisioning of Sumer’s armies and the triumphs or disasters of their expeditions. We know intimately the pleadings of Sumerian students for more money from their fathers, and the pleadings of their fathers for more diligence from their sons. We track the transactions of Sumerian merchants in copper or onions. We admire the complex and perfect calculations of Sumerian engineers.</p>
<p>Human life on the alluvial plain of the two rivers at the birth of civilization five thousand years ago was precarious. Again and again, through the ancient stories and archaeological records that illuminate the dawn of history, plagues and pestilence swept the hot, low country. Terrifying floods killed and destroyed everything within reach of the raging waters that came every spring when the snow melted in the mountains five hundred miles and more to the north, in what is now Armenia and southeast Turkey. At Ur in Sumer’s far south, the great archaeologist Sir Leonard Woolley, digging in 1929, discovered a layer of “perfectly clean clay” more than eight feet thick separating the remains—pottery and much more—of two distinct cultures from some time before 3000 BC. A single flood, in other words, had created a temporary lake that deposited this eight-foot-thick layer. The catastrophic scale of such a deluge is almost beyond the powers of imagination. Woolley naturally surmised that it was the great flood of Genesis. Other floods have left similar records in southern Iraq. Most were smaller than Woolley’s Ur deluge. One left eleven feet of new flood soil.</p>
<p>Meanwhile neighbors from the higher, rougher country to the east, north, and west were greedy for the wealth of the settled plain, then as now. The invasions of barbarians from the Persian hill country, the Kurdish and Turkish mountains, and the Arabian steppe sometimes paused, but never ended. Within Sumer, Uruk and its neighboring city-states fought against each other almost constantly during the twenty-odd centuries of Sumerian civilization.</p>
<p>The soil of southern Iraq is a dusty, flinty accumulation of silt from the two shifting rivers that originate far to the north. In the areas where Iraq’s alluvial soil is not dry, it is marshy, especially in the south; it was more so in ancient times, when the Tigris and Euphrates were bigger. The ground is home to no minerals or ores, although bitumen seeps from the earth in places. The land contains no stones for building. Almost no tree, aside from the date palm, grows on it successfully. Trade with the far-off source-lands of raw materials—for tin and copper to alloy into bronze for weapons, for gold and silver to please the rich and the divine, for hardwood timbers for the roof beams of palaces and temples—required the pooling of resources. Organization and leadership were required to conduct commerce at scale with places as far afield as Anatolia for tin, Lebanon for cedar timbers, “Oman for copper, south-west Iran for carved stone bowls, eastern Iran for lapis lazuli, the Indus for carnelian.”</p><div><p><span>Article continues after advertisement</span></p></div>
<p>*</p>
<p>The water of the two great rivers irrigated the rainless plain. It also raged as a violent killer, to be restrained with dykes and channels. This required cooperation on a much larger scale than the individual village or town could offer. Better irrigation led to increasing harvests. As the land of Sumer became crowded with more and more people, food was another reason for increasingly sophisticated social arrangements. Each of these catalysts—trade, water, sustenance—also led to humanity’s first organized conflicts. War was born. Every Sumerian city had its own principal deity, and the many gods also sent men into their earliest battles there on the hot plain.</p>
<p>*</p>
<p>Late in the fourth millennium BC, a couple of thousand years after the advent of agriculture with the Neolithic revolution, Sumer was one of several distinct cultures around the world. In none of these cultures had true urban life and, with it, civilization yet developed. Then the Sumerian genius produced its greatest innovation: writing.</p>
<p>The eighty thousand people living in Uruk by 3000 BC sheltered behind walls that were forty feet high and six miles long. Archaeologists estimate these to have cost over five million man-hours to build. The fourth-millennium city occupied about 1.7 square miles, a little bit less than imperial Rome at its peak (2.1 square miles) and larger than classical Athens.</p><div><p><span>Article continues after advertisement</span></p></div>
<p>At the archaeological site of Uruk, the residential buildings, workshops, and barracks have not yet been excavated. Thus it is still the case that “very little about the actual conditions of life in the city is known.” Yet this is certain: Uruk was the world’s only major city of the fourth millennium BC, marked by public buildings that were “unprecedented and unrivaled at the time.”&nbsp; Most of the labor for such civic projects in Sumer came from free laborers requiring recompense for their work. Trade in livestock and agricultural produce fed them and the residents of nearby towns. The Sumerians needed a way to keep track of it all. This was the setting in which writing was born.</p>
<p>The earliest writing and the earliest direct precursors of writing, all from the second half of the fourth millennium, have been found at Uruk. Initially, clay tokens the size of a thimble would be formed to represent the sorts of things that a person might own and trade, such as sheep. For convenience, these tokens would then be put into a larger, hollow clay ball a little smaller than a grapefruit. These clay spheres, called “bullae,” served as something like sealed wallets or envelopes for the information within. On its exterior, the bulla would then be impressed with authenticating marks from cylindrical seals rolled upon the clay surface.</p>
<p>At Uruk some of these bullae have been found with additional marks impressed onto their surfaces. These marks indicated the number of tokens contained inside. It was an obvious step. The next step then suggested itself. With the contents marked on the exterior, there was no need for the little tokens rattling around inside. By 3300 BC, the information was instead simply scratched onto the surface of the spheres. The Sumerians had invented writing.</p>
<p>It is the only invention that has ever rivaled that of agriculture for its transformational effect upon human existence. Eventually flat clay tablets replaced the bullae.</p>
<p>At this stage writing was almost purely pictographic. Characters signified their objects through more or less recognizable images. Any given pictograph might mean several different things. “Mountain”—a right-side-up pyramid formed by three convex half circles—also meant “foreign lands,” for Sumer was completely flat. Consequently the same character also signified “conquest.” Shown together with the symbol for “woman,” a downward-pointing triangle with a notch at the bottom tip, the two symbols meant a woman captured from far away: “slave-woman.”</p><div><p><span>Article continues after advertisement</span></p></div>
<p>Pictographs were originally drawn on wet clay with a sharp-pointed object. Clay was an ideal medium for the Sumerians. It was cheap and abundant on the floodplain. Clay tablets were easy to make and prepare, although it is still not known how the larger ones were kept wet and impressionable. Sumerian scribes eventually wrote for the most part as we do, from left to right, top to bottom.</p>
<p>A typical tablet might be two to three inches high and half again as wide, with writing often going all the way to the margins. Incisions toward the bottom of archaic Iraq’s writing tablets tend to be visibly less deep and clear than those at the top of tablets, as the drying clay became harder to work. Once the inscribed clay had dried in southern Mesopotamia’s hot sun, it would endure for scores of centuries, and possibly forever, if left somewhere still and dry. Tablets made from such cheap and ubiquitous material were easily discarded once no longer needed. To the delight of archaeologists dozens of centuries later, they were thrown into heaps or used to fill the spaces beneath floors.</p>
<p>The original pictographs were for the most part recognizably indicative of something physical: a plow or a mountain, a head or a hand. But clay as a two-dimensional medium is ill-suited to both detail and curves. Around the year 2900, scribes discovered that impressing a sequence of lines with a straight-edged implement such as a cut reed was easier than tracing with a pointed implement. Reeds are flat, with a spine along one edge. Thus the mark made by each impression of the cut-off reed comprised a straight line with a wedge at its tip. By 2100, Sumerian scribes possessed a fast, well-developed script. Almost four thousand years later, in 1700 AD, cuneiform was named after the Latin word for wedge, <em>cuneus</em>, by the court interpreter of Eastern languages at the court of William III of England.</p>
<p>The rigid straight lines of the new technique pushed the characters away from the representational and toward the symbolic and the stylized. As centuries passed, the pictographs lost their illustrational quality. They were now “ideographs.” “Mountain,” for example, became three semicircles. By 2500 BC the recognizably representational had disappeared.</p>
<p><span>Here was the evolution from the ideographic to the phonetic. The impact was revolutionary. The boundaries of writing were now as infinite as those of speech.</span></p><p>A representational writing system has significant limitations. It is not practical to have a symbol for everything. The symbols must mean the same to all who use the writing. Users must memorize thousands of these symbols and must also be familiar with that which is being expressed. Tenses, cases, and voices are mostly impossible to depict. In the first centuries of writing, an image illustrating a foot meant “walk,” “stand up,” “ground,” “foundation,” and more besides simply “foot.” This made things difficult enough, but how would one say, “She will walk”? Or, worse, “Will she walk?” or “How will she have walked?” The ideographic method also had great limitations, as it connected writing not to words themselves, but rather to whatever it was that the words expressed. Ideographic writing bypassed spoken language, in other words. Restricted to known events and objects, unconnected to the spoken word, such a system can never cover all that language covers.</p>
<p>The next great innovation in the development of writing derived from puns. Early in the third millennium before Christ, Sumerian scribes perceived that homophones allowed them greatly to expand the verbal territory covered by the symbols they had mastered. For example, the Sumerians originally lacked a pictograph for their word sum, “to give.” To signify “give” in writing they used the pictograph for another word (“garlic”) that also was pronounced “sum.” In English such a visual pun is called a rebus. We might remember these from school. The picture of an eye next to that of a reed is one such, challenging us to remember dimly, the Sumerians with the sentence “I read.”</p>
<p>With this development, writing was now attached to sounds, to the “signifier” and not the “signified.” By the time of what is known as the Old Babylonian period, about 1500 BC, the Sumerian discovery of the power of paronomasia had helped the Uruk period’s written lexicon of two thousand characters halve in number, even as it covered more meaning. Writing was more accessible. During the Old Babylonian period even a king might be able to read, where hitherto that skill had been largely the province of scribes.</p>
<p>*</p>
<p>Shortly after the earliest development of writing, an ominous cloud appeared on Sumer’s northern horizon: a people called the Akkadians. In contrast to the native Sumerians, the Akkadians were Semitic pastoralists living in what came to be known as the Arabian Desert, the huge, dry steppe to the south and west of the Mesopotamian floodplain. By about 3000 BC, the Akkadians had moved eastward out of the desert. They settled north of Sumer in the part of Iraq that later came to be known as Babylonia.</p>
<p>The Sumerians and Akkadians lived next to each other for a thousand years. The two peoples mixed and fought constantly. There was a great degree of bilingualism, and all manner of sharing between the two languages over time. But the Sumerian and Akkadian tongues are entirely different. How, in such a setting, might a Sumerian scribe record the name of an Akkadian merchant? The need for a single script to serve a geography using two such dissimilar languages almost interchangeably was a great spur to the development of early Mesopotamian writing. Eventually the increasingly cosmopolitan quality of life on the Mesopotamian floodplain would force the script to make itself usable by people of different tongues.</p>
<p>The demands of the emerging southern Mesopotamian sprachbund required that the script deliver more and more of the nuances of speech. With writing no longer able to ignore spoken language, a crucial change happened. Most of writing’s symbols came to represent not meaning—an object, activity, or idea, for example—but rather sound. Here was the evolution from the ideographic to the phonetic. The impact was revolutionary. The boundaries of writing were now as infinite as those of speech. Once the Sumerian script became phonetic, the civilization that cuneiform defined would spread until it reached from Iran to the Mediterranean and from the Persian Gulf to Anatolia.</p>
<p>__________________________________</p>
<p><img fetchpriority="high" decoding="async" data-attachment-id="245872" data-permalink="https://lithub.com/how-the-ancient-sumerians-created-the-worlds-first-writing-system/land-between-the-rivers/" data-orig-file="https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers.jpg" data-orig-size="340,509" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="land between the rivers" data-image-description="" data-image-caption="" data-medium-file="https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers-200x300.jpg" data-large-file="https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers.jpg" src="https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers-200x300.jpg" alt="" width="200" height="300" srcset="https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers-200x300.jpg 200w, https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers-40x60.jpg 40w, https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers-33x50.jpg 33w, https://s26162.pcdn.co/wp-content/uploads/2024/11/land-between-the-rivers.jpg 340w" sizes="(max-width: 200px) 100vw, 200px"></p>
<p><em>Excerpted from </em><a href="https://bookshop.org/a/132/9780802162502" target="_blank">Land Between the Rivers: A 5,000-Year History of Iraq</a> <em>by Bartle Bull. Copyright </em><em>© 2024 by Bartle Bull. Reprinted with the permission of the publisher, Atlantic Monthly Press, an imprint of Grove Atlantic, Inc. All rights reserved.</em></p>
										
																				
																		
										<div id="about_the_author">
												<p><a href="https://lithub.com/author/bartlebull/"><img src="https://s26162.pcdn.co/wp-content/uploads/2024/11/Bartle-Bull-100x100.jpeg" width="100" height="100" srcset="https://s26162.pcdn.co/wp-content/uploads/2024/11/Bartle-Bull.jpeg 2x" alt="Bartle Bull"></a></p>
												
											</div>

										
									</div></div>]]></description>
        </item>
    </channel>
</rss>