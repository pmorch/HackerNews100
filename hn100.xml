<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 01 Apr 2024 15:00:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is the current (Apr. 2024) gold standard of running an LLM locally? (107 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39893142</link>
            <guid>39893142</guid>
            <pubDate>Mon, 01 Apr 2024 11:52:41 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39893142">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39893714"><td></td></tr>
                <tr id="39893796"><td></td></tr>
                <tr id="39894571"><td></td></tr>
            <tr id="39894653"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894653" href="https://news.ycombinator.com/vote?id=39894653&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>RFC: Is there some straightforward way to use a Pi-hole like setup to 302 redirect `reddit.com` to `old.reddit.com`?<p>I'm sure there are myriad browser extensions that will do it at the DOM level, but that's <i>such</i> a heavy-handed solution, and also lol I'm not putting an extension on the cartesian product of all my browsers on all my machines in the service of dis-enshittifying <i>one</i> once-beloved social network.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39894049"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894049" href="https://news.ycombinator.com/vote?id=39894049&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>&gt; (though that won't work on VPNs now)<p>Did for me just now, although as of a week or two ago reddit has been blocking many of my attempts to access through a VPN old. or not. Usually need to reconnect about 3 or 4 times before a page will load.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39894689"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894689" href="https://news.ycombinator.com/vote?id=39894689&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>I use the official mobile app and don't care about old. It's time to face the music and admit that all this big hysteria about mods and communities and "oh they are screwing is" are just empty whining.<p>No, reddit didn't win. The whiners don't understand what exactly they are using so they whine. Stop using it or stop whining.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39894143"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894143" href="https://news.ycombinator.com/vote?id=39894143&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>That doesn't seem to be the case, at least right now. There's no pinned post as far as I can tell, and searching the sub for `best model` returns mostly questions.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39894330"><td></td></tr>
                  <tr id="39893933"><td></td></tr>
                <tr id="39894425"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894425" href="https://news.ycombinator.com/vote?id=39894425&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>The reason everyone loves oobabooga is that it’s made with a maximalist design paradigm.<p>Everyone in the VC world misunderstands why oobabooga is successful and tries to embrace not maximalism.</p><p>Your example product to benchmark yourself against is blender, if you want to serious compete against oobabooga. You need maximalism
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39894572"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39894572" href="https://news.ycombinator.com/vote?id=39894572&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>I wouldn't say maximalism so much, as Oobabooga was designed to emulate Automatic1111, where it's a good enough tool by itself, but then it has a strong extension ecosystem, and it can be utilized through other tools via web service.  It's not great at any one thing but it's "good enough" pretty much everywhere.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39893328"><td></td></tr>
                <tr id="39893765"><td></td></tr>
            <tr id="39894263"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894263" href="https://news.ycombinator.com/vote?id=39894263&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>This repo doesn't seem to say anything about what it does with anything you pass it. There's no privacy policy or anything? It being open source doesn't necessarily mean it isn't passing all my data somewhere. I didn't see anything about this in the repo that everything definitely stays local.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39894695"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894695" href="https://news.ycombinator.com/vote?id=39894695&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>I would be much more concerned if it had a privacy policy, to the point where just having one means I probably wouldn't use it. That is not common practice for Free Software that runs on your machine. The only network operations that Ollama has is managing LLMs (ie: download Mistral from their server).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39894339"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894339" href="https://news.ycombinator.com/vote?id=39894339&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>I run the docker container locally. As far as I can tell, it doesn't call home or anything (from reading the source and from opensnitch). It is just a cgo wrapped llama.cpp that provides an HTTP API. It CAN fetch models from their library, but you can just as easily load in your own GGUF formatted llama models. They implement a docker-like layers mechanism for model configuration that is pretty useful.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39894349"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39894349" href="https://news.ycombinator.com/vote?id=39894349&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>It is free.. So why not just audit the source personally to put your mind at ease and build locally if that’s a major concern of yours?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39894369"><td></td></tr>
                        <tr id="39893970"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39893970" href="https://news.ycombinator.com/vote?id=39893970&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>How does Ollama distribute these models? The downloads page on the Llama website, <a href="https://llama.meta.com/llama-downloads/" rel="nofollow">https://llama.meta.com/llama-downloads/</a>, has a heading that reads:<p>&gt; Request access to Llama</p><p>Which to me gives the impression that access is gated and by application only. But Ollama downloaded it without so much as a "y".</p><p>Is that just Meta's website UI? Registration isn't actually required?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39894162"><td></td></tr>
                <tr id="39894323"><td></td></tr>
                        <tr id="39893340"><td></td></tr>
                <tr id="39893463"><td></td></tr>
                        <tr id="39893749"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39893749" href="https://news.ycombinator.com/vote?id=39893749&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>If you want a code centric interface it's pretty easy to use llangchain with local models. For example, you specify a model from hugging face and it will download and run it locally.<p><a href="https://python.langchain.com/docs/get_started/introduction" rel="nofollow">https://python.langchain.com/docs/get_started/introduction</a></p><p>I like llangchain but it can get complex for use cases beyond a simple "give the llm a string, get a string back". I've found myself spending more time in llangchain docs than working on my actual idea/problem. However, it's still a very good framework and they've done an amazing job IMO.</p><p>edit: "Are options ‘idiot proof’ yet?" - from my limited experience, Ollama is about as straightforward as it gets.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39893429"><td></td></tr>
                <tr id="39894194"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894194" href="https://news.ycombinator.com/vote?id=39894194&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>If you're broke, get a refurbished/used Nvidia P40. Same amount of vram as a 3090, between 4 and 10 times cheaper depending on how cheap you can find a 3090.<p>Granted it's slower of course, but best bang for your buck on vram, so you can run larger models than on a smaller bit faster card might be able to. (Not an expert.)</p><p>Edit: if using in desktop tower, you'll need to cool it somehow. I'm using a 3D printed fan thingy, but some people have figured out how to use a 1080 ti APU cooler with it too.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                      <tr id="39893974"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39893974" href="https://news.ycombinator.com/vote?id=39893974&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>If you're able to purchase a separate GPU, the most popular option is to get an NVIDIA RTX3090 or RTX4090.<p>Apple Mac M2 or M3's are becoming a viable option because of MLX <a href="https://github.com/ml-explore/mlx">https://github.com/ml-explore/mlx</a> . If you are getting an M series Mac for LLMs, I'd recommend getting something with 24GB or more of RAM.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39894020"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39894020" href="https://news.ycombinator.com/vote?id=39894020&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>Jumping on this as a fellow idiot to ask for suggestions on having a local LLM generate a summary from a transcript with multiple speakers. How important is it that the transcript is well formatted (diarization, etc) first. My attempts have failed miserably thus far.<p>Edit: using a P40, whisper as ASR
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39894360"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894360" href="https://news.ycombinator.com/vote?id=39894360&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>My experience with more traditional (non-Whisper-based) diarization &amp; transcription is that it's heavily dependent on how well the audio is isolated. In a perfect scenario (one speaker per audio channel, well-placed mics) you'll potentially see some value from it. If you potentially have scenarios where the speaker's audio is being mixed with other sounds or music, they'll often be flagged as an additional speaker (so, speaker 1 might also be speaker 7 and 9) - which can make for a less useful summary.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39894240"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39894240" href="https://news.ycombinator.com/vote?id=39894240&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>The current HYPERN // MODERN // AI builds are using flox 1.0.2 to install llama.cpp. The default local model is dolphin-8x7b at ‘Q4_KM’ quantization (it lacks defaults for Linux/NXIDIA, that’s coming soon and it works, you just have to configure it manually and Mac gets more love because that’s what myself and the other main contributors have).<p>flox will also install properly accelerated torch/transformers/sentence-transfomers/diffusers/etc: they were kind enough to give me a preview of their soon-to-be-released SDXL environment suite (please don’t hold them to my “soon”, I just know it looks close to me). So you can do all the modern image stuff pretty much up to whatever is on HuggingFace.</p><p>I don’t have the time I need to be emphasizing this, but the last piece before I’m going to open source this is I’ve got a halfway decent sketch of a binary replacement/conplement for the OpenAI-compatible JSON/HTTP one everyone is using now.</p><p>I have incomplete bindings to whisper.cpp and llama.cpp for those modalities, and when it’s good enough I hope the bud.build people will accept it as a donation to the community managed ConnectRPC project suite.</p><p>We’re really close to a plausible shot at open standards on this before NVIDIA or someone totally locks down the protocol via the RT stuff.</p><p>edit: I almost forgot to mention. We have decent support for multi-vendor, mostly in practice courtesy of the excellent ‘gptel’, though both nvim and VSCode are planned for out-of-the-box support too.</p><p>The gap is opening up a bit again between the best closed and best open models.</p><p>This is speculation but I strongly believe the current opus API-accessible build is more than a point release, it’s a fundamental capability increase (though it has a weird BPE truncation issue that could just be a beta bug, but it could hint at something deeper.</p><p>It can produce verbatim artifacts from 100s of thousands of tokens ago and restart from any branch in the context, takes dramatically longer when it needs to go deep, and claims it’s accessing a sophisticated memory hierarchy. Personally I’ve never been slackjawed with amazement on anything in AI except my first night with SD and this thing.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39894214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39894214" href="https://news.ycombinator.com/vote?id=39894214&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>Newbie questions:<p>What do you do with one of these?</p><p>Does it generate images? Write code? Can you ask it generic questions?</p><p>Do you have to 'train' it?</p><p>Do you need a large amount of storage to hold the data to train the model on?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39894285"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894285" href="https://news.ycombinator.com/vote?id=39894285&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>You can chat with the models directly in the same way you can chat with GPT 3.5.<p>Many of the opensource tools that run these models let you also edit the system prompt, which lets you tweak their personality.</p><p>The more advanced tools let you train them, but most of the time, people are downloading pre-existing models and using them directly.</p><p>If you are training models, it depends what you are doing. Finetuning an existing pre-trained model requires lots of examples but you can often do a lot with, say, 1000 examples in a dataset.</p><p>If you are training a large model completely from scratch, then, yes, you need tons of data and very few people are doing that on their local machines.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39894296"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894296" href="https://news.ycombinator.com/vote?id=39894296&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>+1 on these questions.
Can I run a local llm that will, for example
- visit specified URLs and collect tabular data into csv format?
- ingest a series of documents on a topic and answer questions about it
- ingest all my PDF/MD/Word docs and answer questions about them?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39894421"><td></td></tr>
                        <tr id="39893706"><td></td></tr>
            <tr id="39893785"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39893785" href="https://news.ycombinator.com/vote?id=39893785&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><br><div>
                  <p><span>Gpt4all and Simon Willison's llm python tool are a nice way to get started; even on a modest laptop. A modest 14" mac book pro with 16GB goes a long way with most of the 7B models. Anything larger you need more ram.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39894186"><td></td></tr>
            <tr id="39893321"><td></td></tr>
                <tr id="39894038"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39894038" href="https://news.ycombinator.com/vote?id=39894038&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>I love it. It is easy to install and containerized.<p>Its API is great if you want to integrate it with your code editor or create your own applications.</p><p>I have written a blog [1] on the process of deployment and integration with neovim and vscode.</p><p>I also created an application [2] to chat with LLMs by adding the context of a PDF document.</p><p>Update: I would like to add that because the API is simple and Ollama is now available on Windows I don’t have to share my GPU between multiple VMs to interact with it.</p><p>[1] <a href="https://www.avni.sh/posts/homelab/self-hosting-ollama/" rel="nofollow">https://www.avni.sh/posts/homelab/self-hosting-ollama/</a>
[2] <a href="https://github.com/bovem/chat-with-doc">https://github.com/bovem/chat-with-doc</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39894002"><td></td></tr>
            <tr id="39893813"><td></td></tr>
            <tr id="39893368"><td></td></tr>
                <tr id="39893990"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39893990" href="https://news.ycombinator.com/vote?id=39893990&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>&gt; If you're writing something that will run on someone's local machine I think we're at the point where you can start building with the assumption that they'll have a local, fast, decent LLM.<p>I don't believe that at all. I don't have any kind of local LLM. My mother doesn't, either. Nor does my sister. My girl-friend? Nope.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39893939"><td></td></tr>
            <tr id="39893348"><td></td></tr>
            <tr id="39893333"><td></td></tr>
            <tr id="39894237"><td></td></tr>
            <tr id="39893703"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39893703" href="https://news.ycombinator.com/vote?id=39893703&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>oobabooga + berkley sterling LM<p>Seriously, this is the insane duo that can get you going in moments with chatgpt3.5 quality.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39893286"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39893286" href="https://news.ycombinator.com/vote?id=39893286&amp;how=up&amp;goto=item%3Fid%3D39893142"></a></center>    </td><td><p><span>Interesting question, I'd like to know this also.<p>Guess it's going to be a variant of Llama or Grok.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39893599"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Headline Driven Development (125 pts)]]></title>
            <link>https://www.spakhm.com/headline-development</link>
            <guid>39891948</guid>
            <pubDate>Mon, 01 Apr 2024 08:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.spakhm.com/headline-development">https://www.spakhm.com/headline-development</a>, See on <a href="https://news.ycombinator.com/item?id=39891948">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <ul id="nav">
    <li><a href="https://www.spakhm.com/">home</a></li>
    <li id="subscribe">subscribe → <a href="https://www.spakhm.com/feed.rss">rss</a> | <a href="https://www.spakhm.com/subscribe.html">email</a></li>  
  </ul>
  
    
<p>Here is a simple process for shipping software projects that works. First, decompose the project into a stream<a href="#footnote-1AV3" id="ref-1AV3" role="doc-noteref"><sup>1</sup></a> of headlines. Then pick an aggressive date to ship the first headline and work like hell to meet that date. Have everyone work only on one headline at a time– the upcoming one. Ignore everything else. Don’t work on anything that doesn’t help you ship the headline. Once the headline is shipped, switch to the next headline in the stream and repeat. That’s all, you can fire your agile consultant.</p>
<p>A headline is a very short sentence that contains only the highest order bit, with all the other bits culled. Imagine you bump into someone on the street after not having seen them for a few months and they ask what you’ve been up to. What kinds of responses work well in this situation?<span></span> <span>“</span>I trekked through Southeast Asia”,<span></span> <span>“</span>I switched jobs”,<span></span> <span>“</span>I got married”. Software release headlines work the same way.<span></span> <span>“</span>You can now rent VMs through an <span>API</span><span></span><span>”</span>,<span></span> <span>“</span>we rolled out <span>FSD</span> autopilot”,<span></span> <span>“</span>Treasury is available in India”.</p>
<p>Headline driven development works really well for three reasons.</p>
<p>First, headlines is how humans process change. If you ever found your users confused, your boss frustrated, your investors anxious, your peers indifferent– these problems go away when you organize communication around a stream of headlines. But it doesn’t work as an afterthought. Communicating through headlines but developing in some other way is like leading a double life. It gets too messy. So to communicate with headlines you must develop with headlines too.</p>
<p>Second, it makes it easy to ruthlessly prioritize. If you can credibly ship a headline without something, cut that something. For example, suppose you’re working on your Southeast Asia trek headline and you’re planning to visit six countries. Can you credibly say to your friends<span></span> <span>“</span>I trekked through Southeast Asia” only having visited five countries instead of six? Obviously yes. So one of the countries gets cut. How about four countries? Repeatedly go through this exercise and stop before the credibility of the headline is at risk. You want to do the minimum possible amount of work that still leaves the headline credible.</p>
<p>Third, the deadline effect is real. Most of the work in college happens at midnight before the project is due. The industry isn’t that different. So simulating class assignments turns out to be a very effective way to ship quickly. You need a discrete chunk of work, with an arbitrary deadline<a href="#footnote-2AV3" id="ref-2AV3" role="doc-noteref"><sup>2</sup></a>, and a binary outcome. You get this with headlines– a headline has either shipped by a given timestamp or it hasn’t.</p>
<section id="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="footnote-1AV3"><p>I mean<span></span> <span>“</span>stream” in a programming language sense– an infinite list of elements with ability to pop one at a time.<a href="#ref-1AV3" role="doc-backlink"><span>↩︎</span></a></p></li>
<li id="footnote-2AV3"><p>Advertise arbitrary deadlines to candidates up front and let self-selection do its magic.<a href="#ref-2AV3" role="doc-backlink"><span>↩︎</span></a></p></li>
</ol>
</section>

  <p>
Apr 01, 2024  </p>





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Smoking cannabis is now legal in Germany (128 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/apr/01/germany-legal-cannabis-weed-laws-personal-use</link>
            <guid>39891882</guid>
            <pubDate>Mon, 01 Apr 2024 08:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/apr/01/germany-legal-cannabis-weed-laws-personal-use">https://www.theguardian.com/world/2024/apr/01/germany-legal-cannabis-weed-laws-personal-use</a>, See on <a href="https://news.ycombinator.com/item?id=39891882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Smoking cannabis is now legal for over-18s in Germany, after <a href="https://www.theguardian.com/world/2024/feb/23/germany-on-track-to-partly-legalise-cannabis-for-personal-use-after-heated-debate" data-link-name="in body link">new laws for personal possession</a> came into effect.</p><p>As of 1 April, adults in <a href="https://www.theguardian.com/world/germany" data-link-name="in body link" data-component="auto-linked-tag">Germany</a> are allowed to carry up to 25g of dried cannabis on them and cultivate up to three marijuana plants at home.</p><p>The new laws followed a <a href="https://www.theguardian.com/world/2022/sep/12/germany-coalition-legalise-cannabis-laws-eu-european-court-justice" data-link-name="in body link">heated debate</a> about the pros and cons of allowing easier access to the drug.</p><figure id="30374075-60f2-469a-bdba-1f4c41f19ff5" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;With Germany legalising cannabis, Europe is reaching a tipping point. Britain, take note | Steve Rolles&quot;,&quot;elementId&quot;:&quot;30374075-60f2-469a-bdba-1f4c41f19ff5&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/commentisfree/2024/mar/29/germany-legalising-cannabis-europe-britain-tough-drugs?CMP=share_btn_url&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The government says decriminalising weed will hit the hidden market, and reduce the spread of contaminated cannabis, thereby protecting young people. But there has been criticism about the possible impact on youths.</p><p>“From our point of view, the law as it is written is a disaster,” Katja Seidel, a therapist at a drug addiction centre in Berlin, the Tannenhof Berlin-Brandenburg, told Agence France-Presse (AFP).</p><p>“Access to the product will be easier, its image will change and become more normalised, especially among young people,” Seidel said, adding that she expected to see an increase in cannabis use “at least initially”.</p><p>Cannabis consumption by anyone under 18 will continue to be illegal.</p><figure id="fc6faca1-20b5-4c73-be3b-7ae14c3a4f8c" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="People gathered at the Brandenburg Gate in Berlin shortly after midnight to celebrate the partial legalization of cannabis as the new laws come into effect" src="https://i.guim.co.uk/img/media/b4289bfaa448ee0d151473128b23010ca2452eec/0_0_6720_4480/master/6720.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="296.66666666666663" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>People gathered at the Brandenburg Gate in Berlin shortly after midnight to celebrate the partial legalisation of cannabis as the new laws came into effect.</span> Photograph: dts News Agency Germany/Rex/Shutterstock</figcaption></figure><p>The new legislation also has some safeguards to protect young people, including a ban on smoking cannabis within 100 metres (328ft) of a school, kindergarten, playground or sports centre.</p><p>Germany’s health minister, Karl Lauterbach, has promised a major campaign to educate young people about the health risks and boost prevention programmes.</p><p>However, the planned media campaign hasn’t convinced critics. “It doesn’t resonate with them, it will never work,” said Boris Knoblich, a spokesperson for the Tannenhof Berlin-Brandenburg organisation. “What works is someone who goes in, talks to them over a coffee, without a teacher there,” he said.</p><p>The federal centre for health education, linked to the health ministry, told AFP it will “assume its responsibility by expanding its prevention offers”.</p><p>The southern state of Bavaria meanwhile is testing an online training course for teachers on how to approach the topic in the classroom.</p><p>According to official statistics from 2021, 8.8% of adults in Germany aged 18-64 said they had consumed cannabis at least once in the preceding 12 months.</p><p>Among people aged 12 to 17, that number rose to nearly 10%.</p><p>The government has said previously that many users rely on the drug for medicinal reasons and that the new law will also improve the quality of cannabis consumed by growing numbers of young people.</p><p>Observers from around the world will be closely watching how the law works in practice in Germany.</p><p><em>With Kate Connolly in Berlin and Agence France-Presse</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not so fast, Mr. Fourier (101 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/not-so-fast-mr-fourier</link>
            <guid>39891881</guid>
            <pubDate>Mon, 01 Apr 2024 08:25:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/not-so-fast-mr-fourier">https://lcamtuf.substack.com/p/not-so-fast-mr-fourier</a>, See on <a href="https://news.ycombinator.com/item?id=39891881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The discrete Fourier transform (DFT) is one of the most important algorithms in modern computing: it plays a key role in communications,  image and audio processing, machine learning, data compression, and much more. Curiously, it’s also among the worst explained topics in computer science. For example, the Wikipedia article on the matter assaults the reader with around three dozen cryptic formulas, but offers no accessible explanation how or why the algorithm works.</p><p>My goal today is to change this. Let’s start with the basics: DFT takes a time-domain waveform — for example, an audio track — and turns it into frequency-domain data: a series of sine wave intensities that describe the underlying signal. If summed back together, these sine waves of different frequencies, phases, and magnitudes should faithfully recreate the original waveform.</p><p><span>To illustrate the utility of DFT, let’s have a look at a conventional waveform representation of a </span><a href="https://lcamtuf.coredump.cx/police.mp3" rel="">🔈 police siren</a><span> next to its frequency-domain treatment. On both plots, the horizontal axis represents time. In the bottom image, the vertical axis represents frequency and pixel color represents intensity. The top view tells us very little about the recording; in the bottom plot, the shifting pitch of the siren is easy to see, at its base frequency and </span><a href="https://lcamtuf.substack.com/p/square-waves-or-non-elephant-biology" rel="">a number of harmonics</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg" width="990" height="875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:875,&quot;width&quot;:990,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:675735,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed62bf6d-9b51-442f-8a39-a07648b7d080_990x875.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>Police siren: time-domain versus frequency-domain</em></figcaption></figure></div><p>The frequency-domain representation of signals allows us to perform a variety of interesting tricks. For example, when it comes to music, we can easily isolate specific instruments, remove unwanted AC hum, or pitch-shift the vocals. With the edits done, we can then convert the data back to to the time-domain waveform. Indeed, the infamous Auto-Tune pitch corrector, employed by some pop singers to cover up the lack of skill, is one of many applications of DFT.</p><p>To get it out of the way, let’s start with the official formula — which to a layperson, is rather opaque:</p><div data-component-name="Latex"><p><span>\(F_k = \sum\limits_{n=0}^{N-1} s_n \cdot e^{-i 2 \pi k\frac{n}{N} }\)</span></p></div><p><span>In this equation, </span><em><span>F</span><sub>k</sub></em><sub> </sub><span>is the DFT value (“coefficient”) for a particular frequency bin </span><em>k. </em><span>Bin numbers are integers corresponding to fractions of the input sample rate. Moving on, </span><em><span>s</span><sub>0</sub></em><sub> </sub><span>to</span><em><span> s</span><sub>N-1</sub></em><span> are the time-domain samples that make up the DFT input window; and the rest is… some voodoo?</span></p><p>To make sense of the voodoo, let’s conduct a simple thought experiment. Imagine a mechanical arm rotating at a constant angular speed. The arm is holding a pen that makes contact with a piece of drawing paper. The pen moves up and down the length of the arm in response to an input waveform.</p><p>Let’s assume that the arm is completing one rotation per second, and that the input signal is a sine wave running three times as fast (3 Hz). In this setting, the pen will end up drawing a neatly centered, three-lobed shape with every single turn:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png" width="1200" height="1100" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1100,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:98556,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48d91b3d-1687-4bd7-b111-89bec123c791_1200x1100.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>A polar plot of a 3 Hz signal at an angular speed of 1 turn per second.</em></figcaption></figure></div><p>If you prefer, you can watch an animated version below:</p><div id="vimeo-922688562" data-attrs="{&quot;videoId&quot;:&quot;922688562&quot;,&quot;videoKey&quot;:&quot;76ef391e61&quot;,&quot;belowTheFold&quot;:true}" data-component-name="VimeoToDOM"><p><iframe src="https://player.vimeo.com/video/922688562?autoplay=0&amp;h=76ef391e61" frameborder="0" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" loading="lazy"></iframe></p></div><p>In some circumstances — for example if the signal has a frequency lower than the rotation of the arm — it might take multiple revolutions to produce a closed shape. Still, the result should be similar; for example, here’s a pretty nine-lobed figure drawn for a signal at 0.9 Hz:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png" width="1200" height="1100" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1100,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176793,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a502a4d-5626-4e5a-afd8-594b015d42be_1200x1100.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>A polar plot of 0.9 Hz at 1 turn per second (multiple turns).</em></figcaption></figure></div><p>Once again, you can watch an animation below:</p><div id="vimeo-922694900" data-attrs="{&quot;videoId&quot;:&quot;922694900&quot;,&quot;videoKey&quot;:&quot;6c80bc0023&quot;,&quot;belowTheFold&quot;:true}" data-component-name="VimeoToDOM"><p><iframe src="https://player.vimeo.com/video/922694900?autoplay=0&amp;h=6c80bc0023" frameborder="0" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" loading="lazy"></iframe></p></div><p>This radial symmetry crops up every time except for one special case: when the speed of the rotating arm matches the period of the sine waveform. In this scenario, the peaks of the waveform always end up on one side of the plot, and the valleys stay on the other side:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png" width="1200" height="1100" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1100,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:92576,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c1821e-d032-4210-abed-1882cd4d3a2f_1200x1100.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>1 Hz signal @ 1 turn / sec.</em></figcaption></figure></div><p>The shape may end up being rotated depending on the phase of the input waveform, but one side is always sticking out.</p><p>Wondrously, this also works for composite signals consisting of superimposed sine waves. For example, if we take a 1 + 5 Hz signal, it will be off-center when plotted by an arm rotating at 1 Hz or 5 Hz — but will appear centered at 2 Hz:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg" width="1456" height="607" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:607,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:847382,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14575bc7-96e3-4adf-a31b-3e40d9485c7f_3600x1500.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Deconstructing composite signals.</em></figcaption></figure></div><p><span>In other words, we have a fairly ingenious </span><em>frequency detector:</em><span> we set the speed of the rotating arm, plot the samples — and if the results are not distributed evenly around (0,0), we know that there is a matching sine frequency component present in the input signal. We can even quantify the component’s magnitude by measuring the deviation of the shape from the center of rotation.</span></p><p><span>The discrete Fourier transform boils down to this exact operation. It takes a sequence of </span><em>N</em><span> values in the input window and distributes them radially in two dimensions at a given speed, completing </span><em>k </em><span>rotations. It then sums the resulting coordinates (the ∑ part). The distance from the center of rotation to the computed midpoint is the magnitude of the calculated frequency component; the angle of the vector is the (sometimes disregarded) phase of the waveform. Note that some popular online sources mistakenly state that the </span><em>x</em><span> axis component is the magnitude; this is incorrect.</span></p><p><span>This algorithm can be implemented with polar coordinates — angle and distance — mimicking what we did above. It can be done with Cartesian </span><em>x,y</em><span> coordinate pairs and simple trigonometry. The definition quoted at the beginning of this section does it in the most obtuse way possible — with a wacky device known as </span><a href="https://en.wikipedia.org/wiki/Euler%27s_formula" rel="">Euler’s formula</a><span> — but all the methods are equivalent. The </span><em><span>e</span><sup>-i2</sup></em><sup>π</sup><em><sup>kn/N</sup></em><span> part is simply an apparatus for spinning things around in the complex plane by exploiting this surprising equality:</span></p><div data-component-name="Latex"><p><span>\(e^{it} = cos \ t + i \cdot sin \ t\)</span></p></div><p><span>If you’re unfamiliar with complex numbers, the </span><em>i </em><span>part (</span><em>i = √-1</em><span>) is essentially just a math trick to keep two values at an arm’s length in a single equation. To avoid it, the DFT formula can be rewritten using a pair of separate Cartesian coordinates — (</span><em>x, y</em><span>):</span></p><div data-component-name="Latex"><p><span>\(F_k =\Biggl (
 { \sum\limits_{n=0}^{N-1} s_n \cdot cos ( 2 \pi k \frac{n}{N} )},
\ - { \sum\limits_{n=0}^{N-1} s_n \cdot sin( 2 \pi k \frac{n}{N} )} \Biggl )\)</span></p></div><p><span>This notation should be much easier to parse: the </span><em>2π </em><span>part is equal to 360° expressed in radians; </span><em>k</em><span> is the number of turns we want to use to distribute the samples; and </span><em>n/N</em><span> determines the position of each sample within these </span><em>k </em><span>turns. Meanwhile, </span><em>x = cos(t)</em><span> and </span><em>y = sin(t)</em><span> are just a recipe to draw a circle.</span></p><p>A rudimentary implementation of DFT is straightforward; for example, in C, all you need is the following code:</p><blockquote><pre><code>void dft(complex* out_buf, complex* in_buf, uint32_t len) {

  for (int bin_no = 0; bin_no &lt; len; bin_no++) {
    complex sum = 0;
    for (int s_no = 0; s_no &lt; len; s_no++)
      sum += in_buf[s_no] * exp(-2 * I * M_PI * bin_no * s_no / len);
    out_buf[bin_no] = sum;
  }

}</code></pre></blockquote><p><span>The code accepts and outputs complex numbers; for real number inputs, </span><em>in_buf</em><span> can be changed to </span><em>double*</em><span>. To convert the resulting complex coefficients in </span><em>out_buf</em><span> to magnitude, call </span><em>cabs(). </em><span>To get phase information (in radians), use </span><em>carg().</em></p><p><span>Let’s start with the maximum frequency range. If the input data consists of real numbers, the DFT will produce useful output only up to the so-called Nyquist frequency — that is, one half of the sample rate. This frequency component can be found in the bin at </span><em>k = N/2</em><span>.  The coefficients above that can and sometimes need to be computed — but perhaps a bit surprisingly, they just contain mirrored data from the lower-frequency bins.</span></p><p>To understand why this is happening, consider the following animation of our rotating arm. In the video, the arm is spinning so fast in relation to the input sample rate that successive samples are placed more than 180° apart. In fact, the angular distance is 330°, suggesting that we’re running the DFT at ~92% of the input sample rate:</p><div id="vimeo-923565902" data-attrs="{&quot;videoId&quot;:&quot;923565902&quot;,&quot;videoKey&quot;:&quot;e1f3e178d8&quot;,&quot;belowTheFold&quot;:true}" data-component-name="VimeoToDOM"><p><iframe src="https://player.vimeo.com/video/923565902?autoplay=0&amp;h=e1f3e178d8" frameborder="0" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" loading="lazy"></iframe></p></div><p>After watching the clip, it should become clear that the end result is identical to what would be produced by a much slower arm spinning in the opposite direction.</p><p>The coefficient mirroring behavior is an artifact of the DFT algorithm, but the curse of the Nyquist frequency itself is not. It’s a fundamental bandwidth limitation for signal sampling techniques: above it, there’s simply not enough samples available to faithfully discern the frequency. The Nyquist limit is the reason why music is commonly recorded at ~44 kHz, even though our hearing only goes up to about 20 kHz (and only on a good day).</p><p>The other limitation of DFT has to do with frequency separation. A discrete Fourier transform that could use an infinitely-long sample window would have perfect selectivity — that is, it would be able to precisely discern chosen frequencies while rejecting everything else. In practice, DFT is run on finite sample windows, causing each bin to pick up something extra.</p><p><span>To understand the cause, let’s assume we’re doing DFT with bins spaced three hertz apart — i.e., </span><em><span>F</span><sub>0</sub></em><span> is the DC component, </span><em><span>F</span><sub>1</sub></em><span> is 3 Hz, and </span><em><span>F</span><sub>2</sub></em><span> is 6 Hz. The </span><em><span>F</span><sub>1</sub></em><span> bin is always calculated by completing a single turn (the timing expression is </span><em>k × 2π</em><span> and </span><em>k = 1). </em><span>But with an input signal frequency of 2 Hz, we’d need three turns to form a closed shape. Instead, we run out of time — and out of input samples — by the time we’ve drawn only one off-center lobe:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png" width="1200" height="1200" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1200,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41860,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168cb46f-d1b4-4d94-840d-3af82284702d_1200x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em><span>The limitations of DFT: a nearby frequency creeping into the F</span><sub>1 </sub><span>bin.</span></em></figcaption></figure></div><p><span>In practice, the plot ends up badly off-center only if the two frequencies are quite close to each other. To illustrate, here’s an animation for our 3 Hz </span><em><span>F</span><sub>1 </sub></em><span>bin across a range of input signals. Watch the computed blue dot that tracks the “center of mass” of the shape — and thus mirrors the resulting DFT coefficient:</span></p><div id="vimeo-925318836" data-attrs="{&quot;videoId&quot;:&quot;925318836&quot;,&quot;videoKey&quot;:&quot;f8791612a2&quot;,&quot;belowTheFold&quot;:true}" data-component-name="VimeoToDOM"><p><iframe src="https://player.vimeo.com/video/925318836?autoplay=0&amp;h=f8791612a2" frameborder="0" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" loading="lazy"></iframe></p></div><p>As it turns out, the bias is significant only within roughly +/- 75% of bin step size. At lower frequencies, the partial shape is pretty close to a circle; at higher signal rates, it has enough symmetrical lobes that the missing part doesn’t upset the balance much.</p><p>We can measure this more rigorously. The following plot shows the average frequency response across all possible signal phases for a couple of 1 Hz-spaced bins:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png" width="1456" height="844" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:77719,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2db1b3d3-5d3c-4bd7-99fb-5085532f3a82_2377x1378.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Real-world DFT bin frequency response.</em></figcaption></figure></div><p><span>The overlaps mean that the standard set of DFT bins ends up accounting for the entire spectrum between DC and Nyquist. Nothing stops us from calculating ad-hoc DFT coefficients for any non-integer </span><em>k </em><span>in between, but these “partial-turn” bins don’t extract any new, finer-grained information from the system. If you need better frequency discrimination, you gotta up the window size.</span></p><p><span>This brings us to temporal resolution: the transform needs a certain number of samples to work. Selectivity aside, at the very minimum, you must use a window long enough so that the bottom bin (</span><em><span>F</span><sub>1</sub></em><span>) corresponds to the lowest frequency you wish to discern; for example, if you have a 44 kHz audio stream and want to measure down to 20 Hz, you need a window of 2,200 samples — spanning 50 milliseconds — per each DFT.</span></p><p>Yet, within that chosen time window, the algorithm just outputs frequency intensities; this doesn’t make it easy to pinpoint when a particular tone starts or ends, or whether its intensity is going up or down. Indeed, sharp signal discontinuities within the analysis window produce wide-spectrum “halos” that are hard to interpret as any specific frequency.</p><p>It follows that various workarounds, such as the use of overlapping windows, weighted sampling, or specialized aliasing methods might be necessary to get acceptable temporal resolution in certain applications; audio processing tends to be one of them.</p><p><span>The inverse discrete Fourier transform is conceptually simple. It boils down to generating waveforms for to every previously-calculated frequency bin, adjusting their magnitude and phase in accordance with the DFT-produced </span><em><span>F</span><sub>k</sub></em><span> coefficients, and then summing the results to recreate the contents of the time-domain sample window (</span><em><span>s</span><sub>0</sub></em><span> to </span><em><span>s</span><sub>N-1</sub></em><span>). </span></p><p>Here’s the IDFT formula that does just that:</p><div data-component-name="Latex"><p><span>\(s_n = \frac{1}{N} { \sum\limits_{k=0}^{N-1} F_k \cdot e^{i{{2 \pi n \frac{k}{N} }}} } \)</span></p></div><p>Note the similarity of this equation to the forward DFT formula. There is an important symmetry in respect to inputs and outputs at the core of the algorithm; this is known as orthogonality. In a mildly mind-bending sense, the frequency domain is just the time domain turned “inside out”.</p><p>Here’s a simple implementation of IDFT in C:</p><blockquote><pre><code>void idft(complex* out_buf, complex* in_buf, uint32_t len) {

  for (int s_no = 0; s_no &lt; len; s_no++) {
    complex sum = 0;
    for (int bin_no = 0; bin_no &lt; len; bin_no++)
      sum += in_buf[bin_no] * cexp(2 * I * M_PI * s_no * bin_no / len);
    out_buf[s_no] = sum / len;
  }

}</code></pre></blockquote><p><span>As earlier, if working with real-number signals, </span><em>out_buf</em><span> can be declared as </span><em>double* </em><span>instead.</span></p><p>Remarkably, forward DFT followed by IDFT is lossless - i.e., it gets you the exact same time-domain data that you started with. In practice, some attention should be paid to the precision of floating-point calculations to avoid cumulative errors, but it’s usually not a big deal.</p><p><span>The DFT is perfectly adequate except for one tiny problem: even if we precompute the exponentiation part, the algorithm still involves </span><em>K × N </em><span>complex-number multiplications. Because the number of computed bins (</span><em>K</em><span>) is frequently the same as window size (</span><em>N</em><span>), some computer science buffs with a penchant for vagueness describe the computational complexity of DFT as </span><em>O(n²)</em><span>.</span></p><p><span>This is where fast Fourier transform (FFT) comes to play; FFT is still a discrete Fourier transform and it produces the same results, but the calculation is cleverly optimized for large input windows and for the </span><em>K = N</em><span> use case (the algorithm can’t be used to selectively compute only some frequency bins). </span></p><p><span>The performance gains are due to a technique known as the </span><a href="https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm" rel="">radix-2 Cooley-Tukey algorithm</a><span>. The derivation of this optimization is somewhat involved, but on a high level, it involves iteratively splitting the full DFT calculation into smaller sub-sums, and then exploiting the fact that the </span><em><span>e</span><sup>-i2</sup></em><sup>π</sup><em><sup>kn/N</sup></em><span> expression is periodic — it literally goes in circles! — to factor out many redundant multiplications. For larger windows, the resulting complexity of FFT approaches </span><em>O(n ⋅ log₂ n</em><span>), which is a pretty substantial improvement.</span></p><p><span>Numerous reference FFT implementations exist for </span><a href="https://rosettacode.org/wiki/Fast_Fourier_transform" rel="">almost every language imaginable</a><span>; a particularly clean, minimalistic C implementation for power-of-two window sizes is as follows:</span></p><blockquote><pre><code>void __fft_int(complex* buf, complex* tmp, 
               const uint32_t len, const uint32_t step) {

  if (step &gt;= len) return;
  __fft_int(tmp, buf, len, step * 2);
  __fft_int(tmp + step, buf + step, len, step * 2);

  for (uint32_t pos = 0; pos &lt; len; pos += 2 * step) {
    complex t = cexp(-I * M_PI * pos / len) * tmp[pos + step];
    buf[pos / 2] = tmp[pos] + t;
    buf[(pos + len) / 2] = tmp[pos] - t;
  }

}

void in_place_fft(complex* buf, const uint32_t len) {
  complex tmp[len];
  memcpy(tmp, buf, sizeof(tmp));
  __fft_int(buf, tmp, len, 1);
}</code></pre></blockquote><p><span>Turning this code into inverse FFT (IFFT) boils down to removing the minus sign in the </span><em>cexp(…) </em><span>expression and dividing the final values by </span><em>len. </em></p><p><span>Again, in practical applications, it would be wise to precompute the results of complex exponentiation; the value depends only on the </span><em>pos </em><span>index and the chosen window size.</span></p><p>Discrete cosine transform can be thought of as a partial, real-numbers-only version of DFT. Just like discrete Fourier, it converts time-domain samples into a frequency spectrum. And just like DFT, it gets there by multiplying input data by cosine waveforms and then summing the results to obtain a bunch of binned coefficients.</p><p>That said, this description falls a bit short; the algorithm differs from discrete Fourier in a handful of interesting ways. First, let’s have a look at the usual formula for DCT-II, the most common variation of DCT:</p><div data-component-name="Latex"><p><span>\(F_k = \sum\limits_{n=0}^{N-1} s_n \cdot cos ( \pi k { n + \frac12 \over N} )\)</span></p></div><p><span>If you scroll back to the DFT equation, you might notice that in the formula above, </span><em>k</em><span> is multiplied by π (180°) instead of 2π (360°). In effect, the waveform-generating part uses an interleaved pattern of full-turn and partial-turn bins, running at half the frequency of DFT and putting F</span><sub>N-1 </sub><span>at the Nyquist limit. This change isn’t just about being more efficient: if you try to switch the timing expression back to </span><em>2π</em><span>, the entire algorithm goes sideways and many frequency bins start reading zero. In a moment, we’ll discuss why.</span></p><p><span>The other new element is the </span><em>+½</em><span> offset in the timing expression. Recall that sample numbers run from </span><em>n = 0</em><span> to </span><em>N - 1</em><span>, so dividing </span><em>n </em><span>by </span><em>N </em><span>leaves a gap on the upper end of the range. A +</span><em>½ </em><span>shift centers the waveform; the symmetry is important for making the inverse function work. Discrete Fourier doesn’t use this because its symmetry comes from the more basic relationship between cosine and sine in the complex plane.</span></p><p>The usual inverse DCT-II (IDCT-II) formula is:</p><div data-component-name="Latex"><p><span>\(s_n = \frac{1}{N} \sum\limits_{k=0}^{N-1} F_k \cdot w_k \cdot cos [ \pi (n + \frac12) {k \over N} ]\)</span></p></div><p><span>The </span><em><span>w</span><sub>k</sub></em><span> value is</span><em> 1</em><span> for the DC bin (</span><em>k = 0</em><span>) and 2 for all the subsequent partial-turn bins (</span><em>k &gt; 0</em><span>). You might encounter variants that put </span><em><span>w</span><sub>k</sub></em><span> and the scaling factor (</span><em>1/N</em><span>) in the forward DCT formula instead; heck, some authors split the factors between DCT and IDCT. These changes result in differently-scaled coefficients, but the overall operation is the same.</span></p><p>Now, let’s examine the fundamental mechanism of discrete cosine transform. Imagine we’re working with one second’s worth of data sampled at 2 kHz. With DFT, we get N / 2 = 1,000 usable frequency bins before hitting the Nyquist limit (1 kHz); bin spacing is 1 Hz. With the cosine transform, we’re getting N = 2,000 bins representing DC to Nyquist in 0.5 Hz increments. </p><p><span>Let’s pick the 4 Hz bin (</span><em><span>F</span><sub>8</sub></em><span>). First, consider what happens if we’re analyzing an input signal that’s running at 8 Hz. The image shows the reference 4 Hz waveform for the bin (blue), followed the input frequency (green), and then the computed product of the two (red). The product is centered around the </span><em>x</em><span> axis, so the DCT coefficient is zero:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png" width="1456" height="713" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:713,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:130190,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feed1e143-eb67-4342-9a18-906a2be4bb92_2092x1024.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>DCT: mismatched frequencies.</em></figcaption></figure></div><p><span>But what if the frequencies (and phases) match? In the plot below, we’re still looking at the </span><em><span>F</span><sub>8</sub></em><span> bin, but the input signal is now aligned at 4 Hz. The multiplication is always involving two positive or two negative values, so the product waveform now has a positive sum:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png" width="1456" height="713" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:713,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:126340,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11cb79c8-e3e0-4fda-880f-eb6e9d736b78_2092x1024.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>DCT behavior with matching frequencies and phases.</em></figcaption></figure></div><p>If the phases of the signals are off by half a cycle (180°), the discrete cosine transform still works — except the reading is negative.</p><p><span>Given the behavior observed here, one might wonder why we partook in all that complex-number chicanery that comes with real DFT. The answer becomes obvious if we consider the case of a 90° or 270° phase shift (one-fourth or three-fourths of a wavelength). In this situation, any detectable offset in the multiplied waveform simply disappears, and the </span><em><span>F</span><sub>8</sub></em><span> coefficient is zero despite the presence of a matching signal frequency:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png" width="1456" height="713" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:713,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:132640,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11f006d0-de6c-41dd-978b-e956f2196f65_2092x1024.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>DCT bin #8 undone by a phase shift.</em></figcaption></figure></div><p><span>The original Fourier transform has no problem handling this case; at 90°, the DFT vector simply points up, the magnitude of the signal represented solely by the imaginary part (aka the </span><em>y</em><span> coordinate). In contrast, in discrete cosine transform, the imaginary part is AWOL, and we’re not registering anything in the expected frequency bin.</span></p><p><span>At this point, you might be wondering if the misaligned signals are simply lost. The answer is no — the genius of DCT is that the out-of-phase data ends up in the nearby “partial-turn” bins that are not computed in normal DFT. Sticking to our phase-shifted 4 Hz problem signal featured above, let’s peek into the next bin — </span><em><span>F</span><sub>9</sub></em><span> at 4.5 Hz:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png" width="1456" height="694" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:694,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:104442,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07bdd019-e964-407a-aca8-ee29c380eba9_2042x974.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>DCT partial-turn bins doing their job.</em></figcaption></figure></div><p>If you take a local view, the signal appears 90° out of phase at the beginning; seemingly snaps into phase in the middle; and then goes -90° out of phase at the end. Within this “partial-turn” bin’s measurement window, the average product is positive. If we go to the next “full-turn” bin (5 Hz), the product of the waveforms is once again zero.</p><p><span>In other words, DCT bins encode magnitude </span><strong>and</strong><span> phase information, just not always where we’re expecting it. This explains why the transform needs twice as many bins — and why trying to increase their spacing to match DFT breaks the algorithm.</span></p><p>This messy encoding scheme makes discrete cosine transform a fairly poor tool for most signal analysis tasks. That said, it’s great for fast, space-efficient compression algorithms where the aesthetics of the frequency-domain representation don’t matter much. A classic example is the JPEG image format, which performs DCT on 8×8 pixel tiles and then quantizes (i.e., reduces the precision of) the calculated coefficients to make them easier to compress using conventional lossless compression (Huffman coding). </p><p>Oh — if you wish to experiment with DCT-II, a simple implementation is included below:</p><blockquote><pre><code><code>void dct(double* out_buf, double* in_buf, uint32_t len) {

  for (uint32_t bin_no = 0; bin_no &lt; len; bin_no++) {
    double sum = 0;
    for (uint32_t s_no = 0; s_no &lt; len; s_no++)
      sum += in_buf[s_no] * cos(M_PI * bin_no * (s_no + 0.5) / len);
    out_buf[bin_no] = sum;
  }

}

void idct(double* out_buf, double* in_buf, uint32_t len) {

  for (uint32_t s_no = 0; s_no &lt; len; s_no++) {
    double sum = 0;
    for (uint32_t bin_no = 0; bin_no &lt; len; bin_no++)
      sum += in_buf[bin_no] * cos(M_PI * (s_no + 0.5) * bin_no/ len) *
             (bin_no? 2 : 1);
    out_buf[s_no] = sum / len;
  }

}</code></code></pre></blockquote><p><span>For a nonsensical but amusing demo that lets you watch what happens if you apply JPEG-style lossy compression to text, check out </span><a href="https://lcamtuf.substack.com/p/afternoon-project-jpeg-dct-text-lossifizer" rel="">this post</a><span>.</span></p><p><em><span>If you’re interested in how DFT and DCT relate to the operation of radio receivers, check out </span><a href="https://lcamtuf.substack.com/p/radios-how-do-they-work" rel="">this feature</a><span>. For a thematic catalog of articles about electronics and signal processing, </span><a href="https://lcamtuf.coredump.cx/offsite.shtml" rel="">click here</a><span>.</span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What we know about the xz Utils backdoor that almost infected the world (207 pts)]]></title>
            <link>https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/</link>
            <guid>39891607</guid>
            <pubDate>Mon, 01 Apr 2024 07:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/">https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/</a>, See on <a href="https://news.ycombinator.com/item?id=39891607">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      NIGHTMARE SUPPLY CHAIN ATTACK SCENARIO    —
</h4>
            
            <h2 itemprop="description">Malicious updates made to a ubiquitous tool were a few weeks away from going mainstream.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/04/malware-800x450.jpg" alt="Malware Detected Warning Screen with abstract binary code 3d digital concept">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/04/malware.jpg" data-height="619" data-width="1100">Enlarge</a> <span>/</span> Malware Detected Warning Screen with abstract binary code 3d digital concept</p><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 187:single/related:b2a8acd50dd36b60ff62d51a1b220d0e --><!-- empty -->
<p>On Friday, researchers revealed the <a href="https://arstechnica.com/security/2024/03/backdoor-found-in-widely-used-linux-utility-breaks-encrypted-ssh-connections/">discovery</a> of a backdoor that was intentionally planted in xz Utils, an open-source data compression utility available on almost all installations of Linux and other Unix-like operating systems. The person or people behind this project likely spent years on it. They were likely very close to seeing the backdoor update merged into Debian and Red Hat, the two biggest distributions of Linux when an eagle-eyed software developer spotted something fishy.</p>
<p>"This might be the best executed supply chain attack we've seen described in the open, and it's a nightmare scenario: malicious, competent, authorized upstream in a widely used library," software and cryptography engineer Filippo Valsorda <a href="https://bsky.app/profile/filippo.abyssdomain.expert/post/3kouaom62oi2b">said</a> of the effort, which came frightfully close to succeeding.
</p><p>Researchers have spent the weekend gathering clues. Here's what we know so far.</p>
<p><b>What is xz Utils?</b></p>
<p>xz Utils is nearly ubiquitous in Linux. It provides lossless data compression on virtually all Unix-like operating systems, including Linux. xz Utils provides critical functions for compressing and decompressing data during all kinds of all kinds of operations. xz Utils also supports the legacy .lzma format, making this component even more crucial.</p>
<p><b>What happened?</b></p>
<p>Andres Freund, a developer and engineer working on Microsoft’s PostgreSQL offerings, was recently troubleshooting performance problems a Debian system was experiencing with SSH, the most widely used protocol for remotely logging into devices over the Internet. Specifically, SSH logins were consuming too many CPU cycles and were generating errors with <a href="https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/link">valgrind</a>, a utility for monitoring computer memory.</p>
<p>Through a combination of sheer luck and Freund’s careful eye, he eventually discovered the problems were the result of updates that had been made to xz Utils. On Friday, Freund took to the Open Source Security List to disclose the updates were the result of someone intentionally planting a backdoor in the compression software.</p>
<p><b>What does the backdoor do?</b></p>
<p>Malicious code added to xz Utils versions 5.6.0 and 5.6.1 modified the way the software functions when performing operations related to lzma compression or decompression. When these functions involved SSH, they allowed for malicious code to be executed with root privileges. This code allowed someone in possession of a predetermined encryption key to log into the backdoored system over SSH. From then on, that person would have the same level of control as any authorized administrator.</p>
<p><b>How did this backdoor come to be?</b></p>
<p>It would appear that this backdoor was years in the making. In 2021, someone with the username JiaT75 made their <a href="https://github.com/libarchive/libarchive/pull/1609">first known commit</a> to an open-source project. In retrospect, the <a href="https://github.com/libarchive/libarchive/pull/1609">change</a> to the libarchive project is suspicious, because it replaced the safe_fprint funcion with a variant that’s long been recognized as less secure. No one noticed at the time.</p>                                            
                                                        
<p>The following year, JiaT75 submited a patch over the xz Utils mailing list, and almost immediately, a never-before-seen participant named Jigar Kumar joined the discussion and argued that Lasse Collin, the longtime maintainer of xz Utils, hadn’t been updating the software often or fast enough. Kumar, with the support of Dennis Ens and several other people who had never had a presence on the list, pressured Collin to bring on an additional developer to maintain the project.</p>
<p>In January 2023, JiaT75,made their <a href="https://github.com/tukaani-project/xz/pull/7">first commit</a> to xz Utils. In the months following, JiaT75, who used the name Jia Tan, became increasingly involved in xz Utils affairs. For instance, Tan replaced Collins's contact information with their own on oss-fuzz, a project that scans open-source software for vulnerabilities that can be exploited. Tan also requested that oss-fuzz disable the ifunc function during testing, a change that prevented it from detecting the malicious changes Tan would soon make to xz Utils.</p>
<p>In February of this year, Tan issued commits for versions 5.6.0 and 5.6.1 of xz Utils. The updates implemented the backdoor. In the following weeks, Tan or others appeal to developers of Ubuntu, Red Hat, and Debian to merge the updates into their OSes. Eventually, one of the two updates made its way into the following releases, <a href="https://www.tenable.com/blog/frequently-asked-questions-cve-2024-3094-supply-chain-backdoor-in-xz-utils">according to</a> security firm Tenable:</p>

<p><b>Can you say more about what this backdoor does?</b></p>
<p>In a nutshell, it allows someone with the right private key to hijack sshd, the executable file responsible for making SSH connections, and from there to execute malicious commands. The backdoor is implemented through a five-stage loader that uses a series of simple but clever techniques to hide itself. It also provides the means for new payloads to be delivered without major changes being required.</p>
<p>Multiple people who have reverse engineered the updates have much more to say about the backdoor.</p>
<p>Developer Sam James provided <a href="https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27">this overview</a>:</p>
<blockquote><p>This backdoor has several components. At a high level:</p>
<ul>
<li aria-level="1">The release tarballs upstream publishes don't have the same code that GitHub has. This is common in C projects so that downstream consumers don't need to remember how to run autotools and autoconf. The version of build-to-host.m4 in the release tarballs differs wildly from the upstream on GitHub.</li>
<li aria-level="1">There are crafted test files in the tests/ folder within the git repository too. These files are in the following commits:
<ul>
<li aria-level="2">tests/files/bad-3-corrupt_lzma2.xz (<a href="https://github.com/tukaani-project/xz/commit/cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0">cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0</a>,<a href="https://github.com/tukaani-project/xz/commit/74b138d2a6529f2c07729d7c77b1725a8e8b16f1"> 74b138d2a6529f2c07729d7c77b1725a8e8b16f1</a>)</li>
<li aria-level="2">tests/files/good-large_compressed.lzma (<a href="https://github.com/tukaani-project/xz/commit/cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0">cf44e4b7f5dfdbf8c78aef377c10f71e274f63c0</a>,<a href="https://github.com/tukaani-project/xz/commit/74b138d2a6529f2c07729d7c77b1725a8e8b16f1"> 74b138d2a6529f2c07729d7c77b1725a8e8b16f1</a>)</li>
</ul>
</li>
<li aria-level="1">A script called by build-to-host.m4 that unpacks this malicious test data and uses it to modify the build process.</li>
<li aria-level="1">IFUNC, a mechanism in glibc that allows for indirect function calls, is used to perform runtime hooking/redirection of OpenSSH's authentication routines. IFUNC is a tool that is normally used for legitimate things, but in this case it is exploited for this attack path.</li>
</ul>
<p>Normally upstream publishes release tarballs that are different than the automatically generated ones in GitHub. In these modified tarballs, a malicious version of build-to-host.m4 is included to execute a script during the build process.</p>
<p>This script (at least in versions 5.6.0 and 5.6.1) checks for various conditions like the architecture of the machine. Here is a snippet of the malicious script that gets unpacked by build-to-host.m4 and an explanation of what it does:</p>
<pre>if ! (echo "$build" | grep -Eq "^x86_64" &gt; /dev/null 2&gt;&amp;1) &amp;&amp; (echo "$build" | grep -Eq "linux-gnu$" &gt; /dev/null 2&gt;&amp;1);then</pre>
<ul>
<li aria-level="1">If amd64/x86_64 is the target of the build</li>
<li aria-level="1">And if the target uses the name linux-gnu (mostly checks for the use of glibc)</li>
</ul>
<p>It also checks for the toolchain being used:</p>
<pre>if test "x$GCC" != 'xyes' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
if test "x$CC" != 'xgcc' &gt; /dev/null 2&gt;&amp;1;then
exit 0
fi
LDv=$LD" -v"
if ! $LDv 2&gt;&amp;1 | grep -qs 'GNU ld' &gt; /dev/null 2&gt;&amp;1;then
exit 0</pre>
<p>And if you are trying to build a Debian or Red Hat package:</p>
<pre>if test -f "$srcdir/debian/rules" || test "x$RPM_ARCH" = "xx86_64";then</pre>
<p>This attack thusly seems to be targeted at amd64 systems running glibc using either Debian or Red Hat derived distributions. Other systems may be vulnerable at this time, but we don't know.</p></blockquote>
<p>In an online interview, developer and reverse engineer HD Moore confirmed the Sam James suspicion that the backdoor targeted either Debian or Red Hat distributions.</p>                                            
                                                        
<p>“The attack was sneaky in that it only did the final steps of the backdoor if you were building the library on amd64 (intel x86 64-bit) and were building a Debian or a RPM package (instead of using it for a local installation),” he wrote.</p>
<p>Paraphrasing observations from researchers who collectively spent the weekend analyzing the malicius updates, he continued:</p>
<blockquote><p>When verifying an SSH public key, if the public key matches a certain fingerprint function, the key contents are decrypted using a pre-shared key before the public key is actually verified. The decrypted contents are then passed directly to system.</p>
<p>If the fingerprint doesn't match or the decrypted contents don't match a certain format, it falls back to regular key verification and no-one's the wiser.</p>
<p>The backdoor is super sneaky. It uses a little-known feature of the glibc to hook a function. It only triggers when the backdoored xz library gets loaded by a /usr/bin/sshd process on one of the affected distributions. There may be many other backdoors, but the one everyone is talking about uses the function indirection stuff to add the hook. The payload was encoded into fake xz test files and runs as a shellcode effectively, changing the SSH RSA key verification code so that a magic public key (sent during normal authentication) let the attacker gain access</p>
<p>​​Their grand scheme was:</p>
<p>1) sneakily backdoor the release tarballs, but not the source code</p>
<p>2) use sockpuppet accounts to convince the various Linux distributions to pull the latest version and package it</p>
<p>3) once those distributions shipped it, they could take over any downstream user/company system/etc</p></blockquote>
<p>Additional technical analysis is available from the <a href="https://bsky.app/profile/filippo.abyssdomain.expert/post/3kowkezwz6g2q">above</a> Bluesky thread from Valsorda, <a href="https://doublepulsar.com/inside-the-failed-attempt-to-backdoor-ssh-globally-that-got-caught-by-chance-bbfe628fafdd">researcher Kevin Beaumont</a> and <a href="https://www.openwall.com/lists/oss-security/2024/03/29/4">Freund’s Friday disclosure</a>.</p>
<p><b>What more do we know about Jia Tan?</b></p>
<p>At the moment, extremely little, especially for someone entrusted to steward a piece of software as ubiquitous and as sensitive as xz Utils. This developer persona has touched dozens of other pieces of open-source software in the past few years. At the moment, it’s unknown if there was ever a real-world person behind this username or if Jia Tan is a completely fabricated individual.</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First-in-human implantation of bionic device to halt Crohn's disease (146 pts)]]></title>
            <link>https://florey.edu.au/news/2023/12/first-in-human-implantation-of-bionic-device-to-halt-crohns-disease/</link>
            <guid>39891456</guid>
            <pubDate>Mon, 01 Apr 2024 06:43:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://florey.edu.au/news/2023/12/first-in-human-implantation-of-bionic-device-to-halt-crohns-disease/">https://florey.edu.au/news/2023/12/first-in-human-implantation-of-bionic-device-to-halt-crohns-disease/</a>, See on <a href="https://news.ycombinator.com/item?id=39891456">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="sd3-58741ea9">
                                <p><time dateatime="1702459431">13 Dec 2023</time></p>
<p>In a world first, Austin Health clinicians have implanted an electrical nerve stimulation device into a patient with Crohn’s disease to prevent inflammation recurring after surgery.</p>
<p>The device, developed and manufactured by the Bionics Institute in collaboration with researchers at The Florey and The University of Melbourne, could put Crohn’s sufferers<br>
into long-term remission.</p>
<p>Crohn’s disease is a condition that causes inflammation to the lining of the gut. Most patients with Crohn’s eventually require bowel surgery to remove sections of gut<br>
damaged by inflammation. However, the disease frequently returns, requiring further surgery.</p>
<p>Crohn’s sufferers are also often at the mercy of life-long drug therapies to manage symptoms.</p>
<p>Co-investigator and team leader Professor John Furness, head of The Florey’s Digestive Physiology and Nutrition Laboratories, said this breakthrough therapy could transform the lives of people suffering from Crohn’s disease.</p>

<figure id="attachment_12355" aria-describedby="caption-attachment-12355"><img fetchpriority="high" decoding="async" src="https://florey.edu.au/wp-content/uploads/2023/12/John-Furness-resized-1280x907.jpg" alt="" width="1280" height="907"><figcaption id="caption-attachment-12355">Professor John Furness</figcaption></figure>
<blockquote>
<p>“By stimulating anti-inflammatory nerves and addressing inflammation early, the recurrent inflammation and associated scar tissue development can hopefully be prevented altogether, meaning patients will avoid the cycle of surgery and recovery.”</p>
</blockquote>
<p>Associate Professor Peter De Cruz, Austin Health’s Director of Inflammatory Bowel Disease Service said Crohn’s disease can have significant impacts on the lives of Australians throughout some of the best years of their lives.</p>
<p>“Australia has among the highest incidence of Crohn’s disease in the world, and affects young people in their 20s to 30s at a time when they are trying to study, enter the workforce or commence families.”</p>
<p>Professor James Fallon, CTO and Head of Research at the Bionics Institute leads the team that developed the groundbreaking device, which was this month implanted onto the vagus nerve of Crohn’s patient, Anthony Becker.</p>
<p>Mr Becker has suffered from Crohn’s disease for as long as he can remember, now into his 40s and with a young family, he is determined to tackle the disease head-on to ensure he maintains a quality of life that is free of surgeries.</p>
<p>Anthony was the first patient to receive the stimulation device earlier this month, and despite being on medication for a number of years, he still required surgery.</p>
<p>If this treatment proves successful, he may never need additional surgical treatments to keep his Crohn’s disease at bay.</p>
<p>Professor James Fallon said: “The vagus nerve controls many functions in the body, such as digestion, heart rate and the immune system. The device is made up of tiny electrodes that stimulate the vagus nerve to trigger the body’s natural defences and prevent inflammation from damaging the gut.</p>
<figure id="attachment_12354" aria-describedby="caption-attachment-12354"><img decoding="async" src="https://florey.edu.au/wp-content/uploads/2023/12/Crohns-bionics-1280x907.jpg" alt="" width="1280" height="907"><figcaption id="caption-attachment-12354">A vagus nerve device like this is being trialed as a treatment for Crohn’s disease. Picture: The Bionics Institute</figcaption></figure>
<p>“We have designed the device to be a set-and-forget treatment powered by a small battery under the skin, hopefully allowing patients to continue<br>
their lives without the fear of further surgery.”</p>
<p>“Up to 80 per cent of Crohn’s sufferers will require surgery at some point in their lives. It’s hoped that eventually this new device will work as a set and forget, allowing patients to continue with their lives without the fear of ongoing treatment with medication,” said Associate Professor Peter De Cruz.</p>
<p>If successful, this study will confirm safety and efficacy of the device for Crohn’s sufferers and provide data to understand if remission can be sustained after surgery.</p>
<p><strong>Media contact</strong></p>
<p>Kath Powley, Manager, Media and Communications<br>
<a href="https://florey.edu.au/cdn-cgi/l/email-protection#abc0cadfc3d9d2c585dbc4dcc7ced2ebcdc7c4d9ced285cecfde85cade"><span data-cfemail="c5aea4b1adb7bcabebb5aab2a9a0bc85a3a9aab7a0bceba0a1b0eba4b0">[email&nbsp;protected]</span></a>&nbsp;| +61 456 666 271</p>
                            </div><div>

        

<div id="sd4-58741ea9">
                    
                        
                                                <div>
                            <h2>Join our community and stay in the loop</h2>
<p>Latest breakthroughs, news, events &amp; more.</p>
                        </div>
                        
                                                <div>
                            <form id="newsletter-subscription-form" name="newsletter_subscription" data-type="default" novalidate="">
<div>
                                <p><label for="emailAddress">Email</label></p>
<p>Please enter a correct email address</p>
</div></form>
                        </div>
                                            
                    </div>
        
        


        

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hosting a Public Website on MS-DOS (141 pts)]]></title>
            <link>https://fsturmat.net/blog/04202022/</link>
            <guid>39891415</guid>
            <pubDate>Mon, 01 Apr 2024 06:35:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fsturmat.net/blog/04202022/">https://fsturmat.net/blog/04202022/</a>, See on <a href="https://news.ycombinator.com/item?id=39891415">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Hearts of the Super Nintendo (112 pts)]]></title>
            <link>https://fabiensanglard.net/snes_hearts/index.html</link>
            <guid>39891317</guid>
            <pubDate>Mon, 01 Apr 2024 06:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/snes_hearts/index.html">https://fabiensanglard.net/snes_hearts/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39891317">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    

</center><p>
Apr 1, 2024</p>
<p>The hearts of the Super Nintendo</p><hr>


<p>When I start studying a vintage system, the first thing I like to do is understand how its components work together at the hardware level<a name="back_1" href="#footnote_1"><sup>[1]</sup></a>.
</p>


<p>Every computer has at least one heart which beats the cadence to all the other chips. The <b>C</b>lo<b>CK</b> output pin is connected to a copper line which spreads to most components, into their <code>CLK</code> input pin.</p>

<p>If you are mostly a software person like me, you may have never noticed it but all kinds of processors have a <code>CLK</code> input pin. From CPUs (Motorola 68000<a name="back_2" href="#footnote_2"><sup>[2]</sup></a>, Intel Pentium<a name="back_3" href="#footnote_3"><sup>[3]</sup></a>, MOS 6502<a name="back_4" href="#footnote_4"><sup>[4]</sup></a>), to custom graphic chips (Midway's DMA2<a name="back_5" href="#footnote_5"><sup>[5]</sup></a>, Capcom CPS-A<a name="back_6" href="#footnote_6"><sup>[6]</sup></a>/CPS-B<a name="back_7" href="#footnote_7"><sup>[7]</sup></a>, Sega's Genesis VDP<a name="back_8" href="#footnote_8"><sup>[8]</sup></a>) to audio chips (Yamaha 2151<a name="back_9" href="#footnote_9"><sup>[9]</sup></a>, OKI msm6295<a name="back_10" href="#footnote_10"><sup>[10]</sup></a>), they all have one.</p>


<p>How is the CLK generated?</p><hr><p>The <code>CLK</code> can be generated by two types of components. One is a <a href="https://en.wikipedia.org/wiki/Crystal_oscillator#:~:text=The%20crystal%20oscillator%20circuit%20sustains,and%20size%20of%20the%20crystal">crystal oscillator</a> which usually looks like a flattened capsule. The others are named <a href="https://en.wikipedia.org/wiki/Ceramic_resonator">ceramic resonators</a>. These are <a href="https://fabiensanglard.net/snes_hearts/ceramic_resonator_sideview.webp">vertical</a> capacitor which look less high-tech than the crystals (and they also happen to drift over time).
</p>




<p>Let's open it already!</p><hr><p>With this in mind, let's peek inside a Super Nintendo. Can you find the <code>CLK</code> generators on a SNES motherboard? Clue: There are two.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_hearts/Nintendo_SNS-CPU-GPM-02_SNES_Motherboard.webp" width="3095" height="3447" id="mobo"><span><i><small>
<a href="https://consolemods.org/wiki/SNES:SNES_Model_Differences">Source</a>. Click the image to find the CLK generators.
</small></i></span>



<p>Two hearts</p><hr><p>In the X2 slot, the blue thingy is a <code>24.576</code> MHz ceramic resonator. It is located on the side where the audio chips are so it sets the pace of the <b>A</b>udio <b>P</b>rocessing <b>U</b>nit.</p>

<p>In the X1 slot, the yellow one is labeled <code>D21L3</code>. It is a <code>21.300</code> MHz oscillator. It is located near (and sets the pace of) the CPU and the <b>P</b>icture <b>P</b>rocessing <b>U</b>nit.</p>


<p>Documentation discrepancies</p><hr><p>Looking at the Super Nintendo developer guide<a name="back_11" href="#footnote_11"><sup>[11]</sup></a> reveals it does not match our observations.</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_hearts/arch.webp" width="1588" height="1192">
<p>
The diagram shows not two but three oscillators (there is one feeding the CIC chip, responsible for copy-protection). We are missing one!</p>

<p> Moreover, the frequency for the CPU/PPU is documented as <code>21.47727</code>MHz but we found a <code>21.300</code>Mhz oscillator (this is a PAL motherboard, an NTSC one would have featured a <code>21.500</code>Mhz oscillator).</p>

<p>What is going on here?</p>


<p>CPU/PPU: From <code>21.500</code>MHz to <code>21.47727</code>MHz</p><hr><p>If we look at the motherboard again, we will notice a red component in the lower left, just next to the oscillator. This red thingy is a variable capacitor (some people also call it a trimmer capacitor) which turns the <code>21.500</code> MHz frequency into <code>21.47727</code> MHz.</p>

<p>Why did the designers make it adjustable? The likely answer is that Nintendo feared the oscillator would deteriorate over time, and technicians would be able to tune it. They may not have been wrong since a common issue for the Super Nintendo console is to render in black and white. The solution is often to adjust the capacitor (or replace the oscillator)<a name="back_12" href="#footnote_12"><sup>[12]</sup></a>.
</p>

<p>Dividers</p><hr><p>There are only two "master" clocks in the console but none of the processors use them. What happens is that these masters go into dividers to create new clocks. The Ricoh 5A22 CPU for example, runs at 1/6 of the master clock, which results in <code>3.579545</code>MHz. Luckily, the SNES community (and <a href="https://fabiensanglard.net/snes_hearts/%3Ca%20href=" https:="" problemkaputt.de="" fullsnes.htm#snestimingoscillators"="">nocash</a> in particular) has documented all these dividers<a name="back_13" href="#footnote_13"><sup>[13]</sup></a>.</p>

<pre>NTSC Timings
  NTSC crystal      <span color="red">21.4772700</span>MHz (X1)
  NTSC color clock  3.57954500MHz (<span color="red">21.4772700</span>MHz/6)  (generated by PPU2 chip)
  NTSC master clock 21.4772700MHz (<span color="red">21.4772700</span>MHz/1)  (without multiplier/divider)
  NTSC dot clock    5.36931750MHz (<span color="red">21.4772700</span>MHz/4)  (generated by PPU chip)
  NTSC cpu clock    3.57954500MHz (<span color="red">21.4772700</span>MHz/6)  (without waitstates)
  NTSC cpu clock    2.68465875MHz (<span color="red">21.4772700</span>MHz/8)  (short waitstates)
  NTSC cpu clock    1.78977250MHz (<span color="red">21.4772700</span>MHz/12) (joypad waitstates)
  NTSC frame rate   60.09880627Hz (<span color="red">21.4772700</span>MHz/(262*1364-4/2))
 
APU Timings
  APU oscillator    <span color="blue">24.576</span>MHz (X2)
  DSP sample rate   32000Hz   (<span color="blue">24.576</span>MHz/24/32)
  SPC700 cpu clock  1.024MHz  (<span color="blue">24.576</span>MHz/24)
  SPC700 timer 0+1  8000Hz    (<span color="blue">24.576</span>MHz/24/128)
  SPC700 timer 2    64000Hz   (<span color="blue">24.576</span>MHz/24/16)
  CIC clock         3.072MHz  (<span color="blue">24.576</span>MHz/8)
  Expansion Port    8.192MHz  (<span color="blue">24.576</span>MHz/3)  
</pre>

<p>In total there are fifteen clocks in the Super Nintendo. Hopefully this solves the mystery of the "missing" oscillator from the documentation.</p>

<p>Enhancement chips</p><hr><p>The <code>SYS-CLK</code> (21.47727MHz) line is fed into the cartridge port. This signal is normally not needed by the components in the cartridge. These are made of ROM containing the game instructions and assets which don't need a clock signal. So why route it there?</p>

<p>The answer is that it allows cartridges to embed processors of their own, called enhancement chips<a name="back_14" href="#footnote_14"><sup>[14]</sup></a>. The most famous of these games is StarFox which features a "mario" SuperFX processor. The MARIO version has an internal divider which halves the clock to <code>10.738635</code>MHz. Later, GSU-1, versions ran at the full <code>21.47727</code>MHz clock.</p>



<img loading="lazy" src="https://fabiensanglard.net/snes_hearts/SPAL-FO-1-pcb-front-9322.webp" width="800" height="601"><span><i><small>
Starfox PCB (source <a href="https://snescentral.com/pcb.php?id=0636&amp;num=12&amp;side=front">snescentral</a>).
</small></i></span>

<p>
<b><u>Trivia:</u></b> There is a second CLK line fed into the cartridge. It is <code>CIC-CLK</code> (3.072MHz) which is fed in the CIC chip located in the cartridge<a name="back_15" href="#footnote_15"><sup>[15]</sup></a>.</p> 


<p>However <code>SYS-CLK</code> (21.47727MHz) was not always suitable. Some games, like Megaman 2 and 3 used a CX4 enhancement chip for graphic effects. If we open a MM2 PCB, we find a <code>20</code> MHz oscillator (X1 slot) which feeds the CX4 <code>CLK</code> pin.
</p>

<img loading="lazy" src="https://fabiensanglard.net/snes_hearts/SHVC-2DC0N-01-pcb-front.webp" width="700" height="508"><span><i><small>
Megaman 2 PCB. Notice the 20Mhz oscillator (source <a href="https://snescentral.com/pcbboards.php?chip=SHVC-2DC0N-01">snescentral</a>).
</small></i></span>
<p>References</p><hr><p id="paperbox"><table><tbody><tr><td><a name="footnote_1"></a><a href="#back_1">^</a></td><td> [ 1]</td><td>Of course it is not something you can do with modern system and their microscopic SOCs. But for 90's hardware such as the Super Nintendo, it is no problem..</td></tr><tr><td><a name="footnote_2"></a><a href="#back_2">^</a></td><td> [ 2]</td><td><a href="https://wiki.console5.com/wiki/File:MC68000-MC68HC000-MC68010-68-QP-Pinout.png">68000 pinout</a></td></tr><tr><td><a name="footnote_3"></a><a href="#back_3">^</a></td><td> [ 3]</td><td><a href="https://datasheets.chipdb.org/Intel/x86/Pentium/24199710.PDF">Pentium pinout</a></td></tr><tr><td><a name="footnote_4"></a><a href="#back_4">^</a></td><td> [ 4]</td><td><a href="https://user.xmission.com/~trevin/atari/6502_pinout.html">6502 pinout</a></td></tr><tr><td><a name="footnote_5"></a><a href="#back_5">^</a></td><td> [ 5]</td><td><a href="https://www.arcade-museum.com/manuals-videogames/N/NBA_Jam_Kit_Operations_Manual_1643123101_April_1993.pdf">NBA Jam Kit</a></td></tr><tr><td><a name="footnote_6"></a><a href="#back_6">^</a></td><td> [ 6]</td><td><a href="https://petitl.fr/cps2/DL-0311/">CPS-A schematics</a></td></tr><tr><td><a name="footnote_7"></a><a href="#back_7">^</a></td><td> [ 7]</td><td><a href="https://petitl.fr/cps2/DL-0921/">CPS-B schematics</a></td></tr><tr><td><a name="footnote_8"></a><a href="#back_8">^</a></td><td> [ 8]</td><td><a href="https://md.railgun.works/index.php?title=VDP">Genesis VDP schematics</a></td></tr><tr><td><a name="footnote_9"></a><a href="#back_9">^</a></td><td> [ 9]</td><td><a href="https://fabiensanglard.net/sf2_sound_system/">STREET FIGHTER II, SOUND SYSTEM INTERNALS</a></td></tr><tr><td><a name="footnote_10"></a><a href="#back_10">^</a></td><td> [10]</td><td><a href="https://fabiensanglard.net/sf2_sound_system/MSM6295.pdf">msm6295 datasheet</a></td></tr><tr><td><a name="footnote_11"></a><a href="#back_11">^</a></td><td> [11]</td><td><a href="https://archive.org/details/SNESDevManual/book1/page/n97/mode/2up">SNES Development Manual</a></td></tr><tr><td><a name="footnote_12"></a><a href="#back_12">^</a></td><td> [12]</td><td><a href="https://www.instructables.com/Fixing-a-Super-Nintendo-That-Wont-Output-Color/">Fixing a Super Nintendo That Won’t Output Color</a></td></tr><tr><td><a name="footnote_13"></a><a href="#back_13">^</a></td><td> [13]</td><td><a href="https://problemkaputt.de/fullsnes.htm#snestimingoscillators">SNES Timing Oscillators</a></td></tr><tr><td><a name="footnote_14"></a><a href="#back_14">^</a></td><td> [14]</td><td><a href="https://www.youtube.com/watch?v=Q1UyQDqHBfA">SNES Enhancement Chips (Super FX, DSP1, S-DD1, SA-1 etc.)</a></td></tr><tr><td><a name="footnote_15"></a><a href="#back_15">^</a></td><td> [15]</td><td><a href="https://fabiensanglard.net/10nes">10NES, the Super Nintendo copy protection</a></td></tr></tbody></table></p> <hr>
 <center>*</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The failed attempt to backdoor SSH globally – that got caught by chance (207 pts)]]></title>
            <link>https://doublepulsar.com/inside-the-failed-attempt-to-backdoor-ssh-globally-that-got-caught-by-chance-bbfe628fafdd</link>
            <guid>39890817</guid>
            <pubDate>Mon, 01 Apr 2024 04:06:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doublepulsar.com/inside-the-failed-attempt-to-backdoor-ssh-globally-that-got-caught-by-chance-bbfe628fafdd">https://doublepulsar.com/inside-the-failed-attempt-to-backdoor-ssh-globally-that-got-caught-by-chance-bbfe628fafdd</a>, See on <a href="https://news.ycombinator.com/item?id=39890817">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a href="https://medium.com/@networksecurity?source=post_page-----bbfe628fafdd--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Kevin Beaumont" src="https://miro.medium.com/v2/resize:fill:88:88/1*TPJ3sVZRlcq-rj72g82bAg@2x.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://doublepulsar.com/?source=post_page-----bbfe628fafdd--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="DoublePulsar" src="https://miro.medium.com/v2/resize:fill:48:48/1*euFkwA7zJWm-l7aDoNtJrw.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="e7cc">A few days ago, a toot on Mastodon from Andres, a Postgres developer, caught my attention:</p><figure><figcaption><a href="https://mastodon.social/@AndresFreundTec/112180083704606941" rel="noopener ugc nofollow" target="_blank">https://mastodon.social/@AndresFreundTec/112180083704606941</a></figcaption></figure><p id="a9e6">Wait, <em>what</em>?!</p><p id="28c6">What happened here is now well documented elsewhere, so I shall not recap it much, but essentially somebody appears to have hijacked the open source XZ project by social engineering the volunteer developer into handing over maintainer access after they cited some mental health issues, used the package XZ Utils to piggy back into systemd loading liblzma, which in turn loaded XZ, allowing sshd to be hooked to trojan it on Linux distributions that use systemd.</p><p id="ad82">The trojan allows somebody a private key to hijack sshd to execute commands, amongst other functions. It is highly advanced.</p><figure><figcaption>Photo by <a href="https://unsplash.com/@johnmoeses?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">John Moeses Bauan</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bc92">OpenSSH runs on almost 20 million IPs as of today, and is almost 10 times more prevalent than RDP (Remote Desktop Protocol). Had somebody successfully introduced a widely deployed backdoor, it would have been bad later.</p><p id="f2d8">The backdoor uses a five stage loader to try to hide and includes a function where future updates can be placed in extra files without modifying the original XZ code changes.</p><p id="16d1">These changes were committed to Github back in February, and made their way into test releases of Debian, Fedora and Kali Linux. Nobody noticed the problem. Additionally, a request was opened to make the threat actor a Linux kernel module maintainer for XZ Embedded.</p></div><div><h2 id="6991">Q&amp;A</h2><p id="06e7"><strong>Do I need to panic? </strong>No. As an assurance piece, orgs can check they aren’t using the latest unstable releases of Debian and Fedora — but they very likely aren’t.</p><p id="0020">Andres caught the problem — which has allowed it to be evicted from the Linux distribution ecosystem before any stable releases.. released.</p><p id="dc2b">How did Andres find the problem? Well, let us go to Andres:</p><blockquote><p id="118f">“I was doing some micro-benchmarking at the time, needed to quiesce the system to reduce noise. Saw sshd processes were using a surprising amount of CPU, despite immediately failing because of wrong usernames etc. Profiled sshd, showing lots of cpu time in liblzma, with perf unable to attribute it to a symbol. Got suspicious. Recalled that I had seen an odd valgrind complaint in automated testing of postgres, a few weeks earlier, after package updates.</p><p id="3d72">Really required a lot of coincidences.</p><p id="70c0">One more aspect that I think emphasizes the number of coincidences that had to come together to find this:</p><p id="61a5">I run a number “buildfarm” instances for automatic testing of postgres. Among them with valgrind. For some other test instance I had used -fno-omit-frame-pointer for some reason I do not remember. A year or so ago I moved all the test instances to a common base configuration, instead of duplicate configurations. I chose to make all of them use -fno-omit-frame-pointer.</p><p id="edd9">Afaict valgrind would not have complained about the payload without -fno-omit-frame-pointer. It was because _get_cpuid() expected the stack frame to look a certain way.</p><p id="5750">Additionally, I chose to use debian unstable to find possible portability problems earlier. Without that valgrind would have had nothing to complain.</p><p id="7ad7">Without having seen the odd complaints in valgrind, I don’t think I would have looked deeply enough when seeing the high cpu in sshd below _get_cpuid().”</p></blockquote><p id="858a">A couple of points here. First of all, the world owes Andres unlimited free beer. He just saved everybody’s arse in his spare time. I am not joking.</p><p id="5599">Nobody else had raised concerns, and I don’t believe any existing security tooling or processes would have caught this (I realise there will be a torrent of vendors claiming they detect this… but they will detect this <em>now that somebody told them</em>).</p><p id="4a8e">Because Andres privately researched the issue and got the Linux distributions to take it seriously, he averted this reaching any kind of wide (or even small) deployment in the real world.</p><p id="aaa1">Also, Andres had a unique testing environment and a set of coincidental setup issues which allowed him to discover the issue. I don’t know of anybody else has this setup.</p><p id="a458">When I installed a vulnerable Linux box, I had to double check it was actually vulnerable as I wouldn’t even see a speed issue. For me, it was a completely transparent backdoor — where sshd was running from disk as usual, with the usual file hash and no extra network activity.</p><p id="c395"><strong>How advanced was the threat actor? </strong>The backdoor attempt was a very serious one, with a very high bar of knowledge, research, development and tradecraft to reach this far into the Linux ecosystem. Additionally, changes made by the threat actor on Github span multiple years, and include things like introducing functions incompatible with OSS Fuzzer due to outstanding small issues since 2015, then getting OSS Fuzzer to exclude XZ Utils from scanning last year. The backdoor itself is super well put together, and even includes the ability to remotely deactivate and remove the backdoor via a kill command. Several days in, despite global focus, I haven’t seen anybody who has finished reverse engineering it.</p><p id="22fd"><strong>Who did it?</strong> Your mum. Just kidding, it was GCHQ in Cheltnam. Just kidding, it was Russia. Just kidding, it was China. Just kidding, it was America. Just kidding, it was definitely your mum.</p><p id="f5c3"><strong>We should stop using open source and only buy American vendor products! </strong>Yeah, good luck with that.</p><figure></figure><p id="3aea"><strong>We should start funding every open source project! </strong>Yeah, good luck with that.. I’ll start saving for my trip to Mars.</p><p id="a175"><strong>This is a wake up call around cybersecurity! </strong>People will have forgotten about it in a week. Except for LinkedIn, who will still be talking in an echo chamber about supply chain in four decades.</p><p id="2e14"><strong>So what can we do? </strong>First of all, if you rely on <em>any </em>software, you have a risk of insider threat. Be it open source, closed source, or your own in house developers — if people add backdoors and nobody notices, that’s a problem.</p><p id="6d06">Secondly, <strong><em>a core issue here is systemd in Linux</em></strong>. libsystemd is linked to all systemd servies, which opens a rich attack surface of third party services to backdoor. That is what the threat actor abused here. It mean they didn’t need to backdoor systemd, which has a rich development community paying attention — instead they relied on liblzma loading XZ, which is much further down the chain, where nobody was paying attention.</p><p id="71dd">There’s a plot twist. This issue request in systemd was opened today:</p><p id="1efe">But the fix for this was <em>already in train before the XZ issue was highlighted</em>, and long before the Github issue. The fix stopped the XZ backdoor into SSH, but hadn’t yet rolled out into a release of systemd.</p><p id="2482">I believe there’s a good chance the threat actor realised this, and began rapidly accelerated development and deployment, hence publicly filing bug reports to try to get Ubuntu and such to upgrade XZ, as it was about to spoil several years of work. It also appears this is when they started making mistakes.</p><p id="e9ae"><strong>We’re all doomed!!!1! The supply chain will end the world!11! I really want to panic!!! </strong>No. Panic helps nobody. It’s also important we keep things in perspective, e.g. this one was spotted and averted.</p><p id="b2b5">Let’s just keep doing the good SBOM work at CISA, and stop doing stunts around Huawei and such — Huawei is a speck of dust compared to the issues around tens of thousands of unpaid developers writing the core of the world’s most critical infrastructure nowadays.</p><p id="9ee5">Work around tightening systemd and other core Linux dependencies needs to continue. There’s probably more that needs to be done around EDR detection on Linux, e.g. injection into ssh that nobody detected? Detect it. In truth, Linux EDR products are significantly less robust than Windows EDR products. Ask me how I know.</p><p id="1afa">The reality is that all development has a risk of insider account abuse, and that includes open source.</p><figure></figure><p id="50eb">We should also acknowledge that open source developers are largely unpaid, and face significant amounts of online abuse from users of the software. This is not the first time an open source package has been hijacked after a maintainer was added – it actually happens all the time in Python repositories and such, and has been one of the leading causes of infostealers and coin miners in development pipelines. It is absolutely not a surprise that somebody is targeting open source compression libraries that systemd loads.. and it is also sadly not a surprise that people online bully the creators of these libraries, either.</p><p id="f608">There are no easy fixes.. we should just try to reduce the risk and calmly work some solutions.</p><p id="2267">Other reading:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I upgraded my iBook G4 to have an SSD (112 pts)]]></title>
            <link>https://boredzo.org/blog/archives/2024-03-31/i-upgraded-my-ibook-g4-to-have-an-ssd</link>
            <guid>39890494</guid>
            <pubDate>Mon, 01 Apr 2024 02:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boredzo.org/blog/archives/2024-03-31/i-upgraded-my-ibook-g4-to-have-an-ssd">https://boredzo.org/blog/archives/2024-03-31/i-upgraded-my-ibook-g4-to-have-an-ssd</a>, See on <a href="https://news.ycombinator.com/item?id=39890494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p>I did take some notes which I’ll present below, but this isn’t a full how-to. I used <a href="https://www.ifixit.com/Guide/iBook+G4+12-Inch+800+MHz-1.2+GHz+Hard+Drive+Replacement/166">iFixIt’s guide</a> plus occasional reference to the official Apple Service Source repair guide (those are not strictly public but can be had from your favorite abandonware site).</p>
<p>For this year’s <a href="https://marchintosh.com/">#MARCHintosh</a>, I decided to replace my iBook G4’s 30 GB spinning-rust hard drive with an SSD.</p>
<p><a href="https://marchintosh.com/"><img decoding="async" width="192" height="102" alt="The #Marchintosh logo, depicting a smiling compact Mac icon with a four-leaf clover and a stripe of six-color Apple rainbow." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/MARCHintoshLogo@1x.png, https://boredzo.org/blog/wp-content/uploads/2024/03/MARCHintoshLogo@2x.png 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/MARCHintoshLogo@1x.png"></a></p>
<p>This was my second SSD upgrade, as I’d previously <a href="https://boredzo.org/cubehd">replaced my G4 Cube’s hard drive with an SSD</a>. (The pictures on that page show a hard drive because, before the SSD upgrade, I’d replaced the Cube’s hard drive with <em>another hard drive</em>, and that was what I originally documented on that page. Then, after that, when I decided to upgrade to an SSD, I used my own tutorial. iFixIt didn’t exist yet.)</p>
<p>I rather despise working on laptops, though this wasn’t as bad as I’d worried it would be. (Upgrading the memory in my Mac mini was harder. I pointedly did that as soon as the machine arrived so that it would be done and I’d never need to open the machine back up for the rest of its life.)</p>
<p>The thing that motivated me to go forward with it was that the iBook was absolutely filthy. It had been Mom’s, and she was a smoker in her life; she would routinely be smoking a cigarette and working on the computer, and getting so absorbed in the latter that ash would fall from her cigarette onto and into the computer. So I resolved to clean the disassembled parts as well as upgrade the storage.</p>
<p>For the cleaning, I mostly used paper towels wetted with diluted all-purpose cleaner. A couple small spots of deposited nail polish were resolved with cotton pads soaked with nail polish remover. It worked fine, at least so far—if I’ve started some chemical process of plastic deterioration, I don’t know it yet.</p>
<p><img decoding="async" loading="lazy" width="504" height="378" alt="The iBook in question, closed, and visibly dirty even on the outside." src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7672@2x.jpg" srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7672@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7672@2x.jpg 2x"><br>
Before cleaning.</p>
<p><img decoding="async" loading="lazy" width="504" height="378" alt="The iBook in question, closed, now thoroughly cleaned and spiffy." srcset="https://boredzo.org/blog/archives/2024-03-31/blog/wp-content/uploads/2024/03/IMG_7760@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7760@2x.jpg 2x" src="https://boredzo.org/blog/archives/2024-03-31/blog/wp-content/uploads/2024/03/IMG_7760@1x.jpg"><br>
After cleaning.</p>
<p>Once I decided the project was go, I also added in a memory upgrade, because it was less than $20 and I was already buying stuff from OWC for the operation anyway. The machine had 512 MB of RAM; now it has 1 GB. (Plus the 128 MB on the logic board.)</p>
<p>One key difference from the Cube upgrade: The iBook, being a laptop, doesn’t have as much space for the upgraded drive. The Cube had some wiggle room taken up by brackets; the iBook has basically none. In the Cube, I installed a standard-size SATA SSD plus a SATA-to-PATA adapter; in the iBook, that wouldn’t have fit.</p>
<p>So my first thought was an M.2 SSD, that being the form factor that today’s computers generally use. I ran into a problem: There are like three different signaling protocols that all run over the M.2 form factor, and M.2 correspondingly has three different keying combinations to guard against protocol mismatches (an incompatible SSD won’t physically fit, though an SSD that fits isn’t necessarily compatible). I noped out of trying to sort that out.</p>
<p>What I went with instead was mSATA. This form factor is kind of dying off as M.2 takes over, but Kingston still sells mSATA SSDs directly from their own website, and I found a suitable adapter on Amazon. (I buy from alternatives like Micro Center or direct from manufacturers whenever possible, but it wasn’t in this case. The manufacturer’s website links to their Amazon store.)</p>
<p><img decoding="async" loading="lazy" width="606" height="340" alt="The two-and-a-half-inch spinning-rust hard drive, and the mSATA SSD in its IDE adapter, side by side in my hand." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7735@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7735@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7735@1x.jpg"><br>
The old spinning-rust drive is 30 GB; the new SSD is 256 GB.</p>

<h3>What’d I put in there?</h3>
<p>Here’s the bill of materials I ended up with:</p>
<ul>
<li><a href="https://shop.kingston.com/collections/solid-state-drives/products/kc600-ssd-1?variant=40836176576704">Kingston 256GB mSATA SSD</a> (you could go bigger, but I cheaped out)</li>
<li><a href="https://www.amazon.com/Ableconn-IIDE-MSAT-2-5-Inch-Converter-Aluminum/dp/B017VQT5YW/132-2787373-3000614">Ableconn mSATA to IDE adapter</a></li>
<li><a href="https://eshop.macsales.com/item/OWC/2700DDRS1GBA/">OWC 1 GB PC2700 RAM SO-DIMM</a> (the original Apple DIMM was PC2100, so this was a speed upgrade as well as a size upgrade)</li>
<li><a href="https://eshop.macsales.com/item/OWC/MS8U3SSD120/">OWC external FireWire SSD</a> to reinstall the OS from, since this machine can’t boot from USB and I don’t trust the internal optical drive to not eat the next disc I put in it</li>
</ul>
<p>… and as I write this up now, I notice that OWC has now discontinued that last product. Did I buy the last one or something?</p>
<p>(It feels like OWC is moving away from selling products for older Macs, having now discontinued their last FireWire product AFAIK, and I’m bummed. At a time when the retro Mac community is resurgent, we can use every source for quality gear for our old Macs that we can get.)</p>
<p>For tools, I mostly used <a href="https://www.ifixit.com/products/essential-electronics-toolkit">one of iFixIt’s toolkits</a>, plus <a href="https://www.ifixit.com/products/precision-screw-extractor-set">the screw extractor set they sell</a>. I also found <a href="https://www.artsupplywarehouse.com/products/daylight-halo-table-magnifying-lamp%7CDAYU25200.html">a lighted magnifier</a> extremely helpful.</p>
<p>I also used <a href="https://coriolis-systems.com/">iPartition</a> to set up the external SSD with the Tiger and Leopard boot DVDs and various other things (including a copy I’d already made of the HDD).</p>
<h3>Hiccups</h3>
<h4>Hiccup #1: The case of the borked screw</h4>
<p>One of the three M2 (i.e., 2mm, i.e., <em>really tiny</em>) screws on the right side of the keyboard tray was visibly rusted and seemingly damaged. Trying to unscrew it (including a couple of attempts with the wrong size driver) damaged it further.</p>
<p><img decoding="async" loading="lazy" width="504" height="283" alt="The inside of the keyboard tray. There are two screws in the rear and forward corners, and a third in the center. The forward screw has some visible rust on its head." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7696@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7696@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7696@1x.jpg"><br>
This was after I’d already worked over that third screw a bunch. It was even more rust-covered at first.
</p>
<p>I looked at a bunch of options and asked for ideas on Mastodon. A couple of people recommended precision screw extractors (much smaller and a different type than the kind you drill into a screw). I bought <a href="https://www.ifixit.com/products/precision-screw-extractor-set">the set that iFixIt sells</a>, which it turns out is <a href="https://www.centraltools.com/58-0675-4pc-screw-extractor-reversible-set.html">OEM’d by Moody Tools</a>.</p>
<p>The second extractor bit I tried, one of the smaller ones, worked perfectly, and drove the screw out as easily as the normal drivers did the undamaged screws. Thus I was unblocked and able to continue toward the hard drive.</p>
<p>I did encounter a second rusted screw near the latch mechanism. This I was able to clean with a few things, including a nylon-brush bit on a Dremel, without damaging the head. That screw came out and went back in with no further fuss.</p>
<h4>Hiccup #2: The case of the stuck installer</h4>
<p>After I got everything back together, the Tiger DVD wouldn’t boot. After either the verbose spew or the spinner, the OS switches to the normal GUI with a blue background, and should then show the Installer on that background—but it would get stuck at the blue background, with no window and no cursor.</p>
<p>On a whim, I tried Leopard (not a supported OS on this machine), and was pleasantly surprised to find that the iBook booted that DVD happily. Moreover, that it <em>worked</em>.</p>
<p>Leopard’s installer refused to install, of course, but it let me cancel and access the rest of the utilities—including Terminal, from which I was able to run <code>system_profiler</code>. Here, I figured, I could at least see whether any of my new hardware was showing up.</p>
<p>And the answer was <em>no</em>. Well, that was a let-down! The RAM slot read as empty (it definitely wasn’t), and querying the PATA bus <em>hung</em>.</p>
<p>I decided to try switching back to the original Apple DIMM, since it’s easier to get to the RAM than the hard drive. When I put the original DIMM back in, I noticed that it sat farther into the slot than the upgrade DIMM had been.</p>
<p>“HMMMMMM.”</p>
<p>So I popped it back out and re-upgraded the RAM. Sure enough, with a bit more attention to this detail, the new DIMM went into the slot exactly as far as the Apple DIMM had.</p>
<p>And when I booted the machine up again… everything worked. The new RAM showed up. PATA no longer hung.</p>
<p>“Hmm. PATA doesn’t hang anymore. Was that where Tiger was hanging?”</p>
<p>Yup! Or similar enough, anyway. It fixed the problem; Tiger’s installer was now unblocked.</p>
<h4>Hiccup #3: The case of the 512 MB partition map</h4>
<p>One of my goals for this machine is to try to cajole it into booting Mac OS 9 even though that isn’t supported, so I wanted to make sure to install the disk drivers that Mac OS 9 needs onto the SSD. This is an option in Disk Utility of that era… except it wasn’t showing up, presumably because this is a machine that (officially) can’t boot Mac OS 9 anyway, so why would it need that?</p>
<p>(The original hard drive had Mac OS 9 drivers installed. Maybe Panther was more permissive about that.)</p>
<p>Now, there is a second way to get Mac OS 9 drivers installed: <code>hdiutil create</code> has an option, <code>-layout 'UNIVERSAL HD'</code>, that sets up the image with a partition map containing a full set of Mac OS 9 drivers for every then-imaginable device interface, from SCSI to ATA to FireWire. (There is also <code>'UNIVERSAL CD'</code>, which gives a different set of drivers.)</p>
<p>So my plan was:</p>
<ol>
<li>Create a RAM disk using <code>hdiutil create -nomount ram://262144</code> (that creates a 128 MB RAM disk), since I had nothing writable that I wasn’t about to obliterate (besides thumb drives), and I had plenty of RAM.</li>
<li>On the RAM disk, create a sparse disk image with the “UNIVERSAL HD” format and sized to the number of blocks <code>diskutil info</code> reported for the SSD.</li>
<li>Attach that disk image with <code>-nomount</code> so I can munge its contents directly.</li>
<li>Use <code>pdisk</code> to edit its partition map, specifically to create all the partitions I wanted.</li>
<li>Use <code>newfs_hfs</code> to format each of the partitions as HFS+J (or plain HFS+ in the case of the OS 9 partition).</li>
</ol>
<p>Problem: Whenever I attached the sparse disk image, pdisk showed that the partition map was set for a 512 MB storage device, not a 256 GB storage device. It doesn’t offer any way to change that, even in the advanced options.</p>
<p>Solution: This was apparently a Tiger bug, because when I rebooted back into the Leopard DVD, the same series of steps worked perfectly.</p>
<p>So I had to do my partitioning and formatting in Leopard, then reboot back to the Tiger DVD to install the OS.</p>
<h3>Results</h3>
<p>Well, the first sign of success was that the Tiger installation—from the FireWire SSD to the IDE SSD—only took 15 minutes. If you’ve ever installed old Mac OS X from a CD or DVD to a spinning-rust drive, you’ll remember it definitely takes longer than that.</p>
<p>My informal test of the HDD’s speed before the operation showed about 30 MB/sec write speed (<code>dd if=/dev/zero bs=10485760 of=test</code>, followed by math to convert its output to MB/sec because Tiger’s <code>dd</code> doesn’t have <code>status=progress</code>). After the upgrade, I found out about <a href="http://xbench.com/">Xbench</a>, and ran that, and it showed about 90 MB/sec write speed in most cases, which is basically saturating the UDMA100 interface. So, with the caveat that the methodologies of the two tests aren’t the same, I feel comfortable saying I tripled my throughput.</p>
<p>Tiger certainly feels way faster than ever before on this machine. The progress-bar portion of the boot process is now a tiny fraction of it, as the progress bar goes from 0 to 100% in about a second.</p>
<p>I’m very happy with the upgrade. I plan to see if I can convince the machine to boot from a Mac OS 9 partition (not a supported configuration—the machine came with a special version of Panther) or install Leopard (also not supported; this machine officially only supports Panther and Tiger plus Classic).</p>
<h3>More photos</h3>
<h4>How dirty this thing was</h4>
<p><img decoding="async" loading="lazy" width="504" height="283" alt="Photo of the front edge of the machine. To the left of the latch release button, I haven't cleaned yet, and there's a visible beige patina. To the right of the button, my cleaning has revealed mostly-pristine-looking gray and white plastic." src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7685@1x.jpg"></p>
<p><img decoding="async" loading="lazy" width="504" height="353" alt="The underside of the machine. This surface is even filthier, with several brown streaks of unclear origin." src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7687@1x.jpg"></p>
<p><img decoding="async" loading="lazy" width="504" height="283" alt="The latch release mechanism, rife with dust bunnies. I used lots of canned air evicting them. I think I might have also resorted to a toothpick for some of it." src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7691@1x.jpg">
</p>
<h4>Note #1: Drive bay shock bumpers</h4>
<p><img decoding="async" loading="lazy" width="504" height="283" alt="The IDE adapter (with mSATA SSD installed on the far side of it) sitting half-in the iBook's drive bay. Two black rubber washers are permanently affixed inside the drive bay, near the corners of the drive." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7737@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7737@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7737@1x.jpg"><br>
Note the rubber bumpers inside the drive bay, inside the front of the machine. The heads of the screws on that side of the drive bracket (or the IDE adapter replacing it) rest <em>inside</em> these bumpers. There’s also a bracket with matching bumpers on the other side, which should be transferred from the old drive to the new one.</p>
<h4>Note #2: Reinstalling the feet</h4>
<p>The iBook’s feet have a design that ensures there’s only one way to reinstall them. You’ll need to pay attention to it when putting the feet back in, to ensure you don’t put them in crooked.</p>
<p><img decoding="async" loading="lazy" width="504" height="283" alt="View through the lighted magnifier of one of the recesses where a foot is about to be reinstalled. There's a ridge on the inside of one quadrant of the recess." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7740@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7740@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7740@1x.jpg"><br>
<img decoding="async" loading="lazy" width="504" height="283" alt="The metal ring that surrounds one foot, as seen from the underside that sits in the recess. There are three pillars from the outside-facing ring down to the bottom where the screw-hole is. One of those pillars has a lip carved out of it." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7742@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7742@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7742@1x.jpg"><br>
The lip in this foot ring mates to the ridge in the recess. If the foot ring isn’t properly aligned, it won’t sit in the recess evenly. Don’t force it! Pull it out, look for the keying, and turn it so it’s correctly positioned.</p>
<p><img decoding="async" loading="lazy" width="504" height="283" alt="Photo of the recess with the ring installed—so now we're viewing it from the exterior side rather than the interior side. Inside the ring is a well with three keys. Next to the ring, resting upside down, is the foot that will go into this ring; it has three grooves in its underside to match those keys." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7745@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7745@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7745@1x.jpg"><br>
More keying. As before, pay attention and make sure you’re lined up, and back off and try again rather than trying to force it in.</p>
<h4>Just for fun</h4>
<p><img decoding="async" loading="lazy" width="504" height="353" alt="The old spinning-rust hard drive. There's an air-pressure-equalization vent hole near one edge, and the label has a section that says “DO NOT COVER THIS HOLE”. I'm mischievously covering it with my fingertip." srcset="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7732@1x.jpg, https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7732@2x.jpg 2x" src="https://boredzo.org/blog/wp-content/uploads/2024/03/IMG_7732@1x.jpg"></p>

				

				<p>Categories: <a href="https://boredzo.org/blog/archives/category/uncategorized" rel="category tag">@Uncategorized</a>. |  <a href="#comments">Comments: 5</a> (<a href="https://boredzo.org/blog/archives/2024-03-31/i-upgraded-my-ibook-g4-to-have-an-ssd/feed">feed</a>).</p>
				
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLaMA Now Goes Faster on CPUs (882 pts)]]></title>
            <link>https://justine.lol/matmul/</link>
            <guid>39890262</guid>
            <pubDate>Mon, 01 Apr 2024 02:17:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justine.lol/matmul/">https://justine.lol/matmul/</a>, See on <a href="https://news.ycombinator.com/item?id=39890262">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>
Mar 31<sup>st</sup>, 2024 @ <a href="https://justine.lol/index.html">justine's web page</a>
</p>



<p>
I just wrote 84 new matrix multiplication kernels for
<a href="https://github.com/mozilla-Ocho/llamafile">llamafile</a> which
enable it to read prompts / images faster. Compared to llama.cpp, prompt
eval time with llamafile should go anywhere between 30% and 500% faster
when using F16 and Q8_0 weights on CPU. The improvements are most
dramatic for ARMv8.2+ (e.g. RPI 5), Intel (e.g. Alderlake), and AVX512
(e.g. Zen 4) computers. My kernels go 2x faster than MKL for matrices
that fit in L2 cache, which makes them a work in progress, since the
speedup works best for prompts having fewer than 1,000 tokens.

</p><h3>Background</h3>

<p>
llamafile is a local LLM project I started with Mozilla back in Nov
2023. We're using
<a href="https://justine.lol/cosmo3/">Cosmopolitan Libc</a> to package
<a href="https://github.com/ggerganov/llama.cpp/">llama.cpp</a> as a
single-file cross-platform binary that runs on six OSes for AMD64 and
ARM64, while making gentle modifications. I believe that by improving
the core technology, we can give our users the best possible llama.cpp
experience, while helping both projects reach a broader audience.
Mozilla has been giving me the resources to do this.

</p><h3 id="enterprise">
  <a href="#enterprise">Performance Gains on Enterprise Hardware</a>
</h3>

<p>
When I first got into LLMs, my workstation was an austere Hewlett
Packard running Alpine with a spinning disk, slow RAM, an AVX2
processor, and no GPU. What I liked about llama.cpp is they were the
first LLM project that cared about people like me. So I started
volunteering full time and collaborated with guys like Slaren to
<a href="https://github.com/ggerganov/llama.cpp/pull/613">introduce
mmap()</a> support, which made weights load instantly using half as much
RAM. It was a leap forward for local LLMs at the time, but did little to
improve evaluation speed. Most of the inference code was written by
Georgi Gerganov himself, and it's so good that it'd take me another year
to finally improve upon. Now that I have, let's see how much faster
things go on my old Hewlett Packard.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/hp.jpg"><img src="https://justine.lol/matmul/hp.jpg" alt="[photo of an HP
     ProDesk 600 G5 workstation]" width="510" height="222"></a><p> LLM
     Performance on HP Intel® Core™ i9-9900 ($439) w/ 2200 MT/s RAM c. 2020
</p></th></tr><tr>
<th>prompt<br>tok/sec
</th><th>eval<br>tok/sec
</th><th>model
</th><th>weights<br>data type
</th><th>hardware
</th><th>software
</th></tr><tr><td>   28</td><td>   7</td><td>Mistral 7b</td><td> q4_0</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>   17</td><td>   7</td><td>Mistral 7b</td><td> q4_0</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   12</td><td>   7</td><td>Mistral 7b</td><td> q4_0</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>   32</td><td>   4</td><td>Mistral 7b</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>   22</td><td>   4</td><td>Mistral 7b</td><td> q8_0</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   16</td><td>   4</td><td>Mistral 7b</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>   23</td><td>   2</td><td>Mistral 7b</td><td> f16</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>   15</td><td>   2</td><td>Mistral 7b</td><td> f16</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   14</td><td>   2</td><td>Mistral 7b</td><td> f16</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>  205</td><td>  26</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>  144</td><td>  26</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   91</td><td>  23</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>  171</td><td>  15</td><td>TinyLlama 1.1B</td><td> f16</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>  118</td><td>  15</td><td>TinyLlama 1.1B</td><td> f16</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  101</td><td>  15</td><td>TinyLlama 1.1B</td><td> f16</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
Here we see that, on Skylake, llamafile users can expect to see a 2x
speedup and llama.cpp users can expect 50% better performance. Please
note this only applies to certain weights. So far, I've only written
optimized kernels for the q8_0, f16, q4_1, q4_0, and f32 data types. I
think both q8_0 and f16 are really solid choices. Possibly even f32 if
you've got plenty of RAM. That's because my new kernels change the
rules. They're doing such a good job fixing the memory bandwidth quants
always solved, that quantization could become the bigger bottleck. That
would be great news for the future of local language models, since it
means less need to trade away knowledge for speed.

</p><h3 id="hobbyist">
  <a href="#hobbyist">Performance Gains on Hobbyist Hardware</a>
</h3>

<p>
You don't need a large computer to run a large language model. One of
the best personal computers available in stores today is the Raspberry Pi. They
deliver good performance at a great price and consume very little power.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/pi5.jpg"><img src="https://justine.lol/matmul/pi5-small.jpg" alt="[photo of a raspberry pi 5 computer board]" width="320" height="243"></a><p>
LLM Performance on $100 Raspberry Pi v5 (ARMv8.2) and v4 (ARMv8.0)
</p></th></tr><tr>
<th>prompt<br>tok/sec
</th><th>eval<br>tok/sec
</th><th>model
</th><th>weights<br>data type
</th><th>hardware
</th><th>software
</th></tr><tr><td>   62</td><td>   5</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI5</td><td> llamafile-0.7
</td></tr><tr><td>   28</td><td>   5</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI5</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>    8</td><td>   5</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI5</td><td> llamafile-0.6.2
</td></tr><tr><td>   45</td><td>   9</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI5</td><td> llamafile-0.7
</td></tr><tr><td>   35</td><td>   9</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI5</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   20</td><td>   9</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI5</td><td> llamafile-0.6.2
</td></tr><tr><td>   10</td><td>   3</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI4</td><td> llamafile-0.7
</td></tr><tr><td>   10</td><td>   3</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI4</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>    9</td><td>   3</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI4</td><td> llamafile-0.6.2
</td></tr><tr><td>    3</td><td>   2</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI4</td><td> llamafile-0.7
</td></tr><tr><td>    3</td><td>   2</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI4</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>    4</td><td>   2</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI4</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
Raspberry Pi released their fifth edition a few months ago and it's
outrageously fast compared to their previous model. They also introduced
support for the ARMv8.2 dotprod and fp16 arithmetic ISAs, which are very
useful for LLMs. Those two features alone enabled llama.cpp to achieve a
10x performance boost for f16 weights last year. This week I teased out
another 2x performance boost on top of that, by using a kernel that I
originally intended for AVX512. You wouldn't think a kernel designed for
beefy data center equipment would work out for a teensy tiny little
Raspberry Pi, but it actually fit hand in glove since both CPUs have 32
vector registers.

</p><p>
It's worthwhile to note that the new ARMv8.2 fp16 ISA may introduce more
errors than usual, since it causes llamafile to use fp16 words and we
aren't using techniques like <a href="https://justine.lol/matmul/kahan.gif">Kahan summation</a> for
computing dot products. So Q8_0 weights actually end up having slightly
better perplexity, because it uses the dotprod ISA which lets us updot
signed 8-bit integers into a 32-bit compute type which absorbs errors.
However this doesn't mean the faster fp16 weights can't be useful. Many
developers in this field view the differences as negligible.

</p><p>
For example, let's say you want to setup an email server on your pihole
and have TinyLLaMA filter spam. It's possible to
<a href="https://www.postfix.org/FILTER_README.html">configure Postfix
to filter content using a shell script</a> which lets you run the
llamafile command.

</p><pre>llamafile -m TinyLlama-1.1B-Chat-v1.0.f16.gguf \
          --grammar <span>'root ::= "yes" | "no"'</span> --temp 0 -c 0 \
          --no-display-prompt --log-disable -p <span>"&lt;|user|&gt;
Can you say for certain that the following email is spam?

To: jtunney@gmail.com
From: Federal-Tax-DebtHelp &lt;ConfirmationEmail.inzk@janents.com&gt;
Subject: Reduce your payments to what you can afford

Reduce your payments to what you can afford 
 
 [IMG] 
 [IMG] 
 
 [IMG] 
&lt;/s&gt;
&lt;|assistant|&gt;"</span>
</pre>

<p>
When I run the shell script above on my RPI5, it takes 3 seconds.

</p><pre>jart@pi5:~/scratch$ time ./spam.sh
yes

real    0m3.168s
user    0m10.851s
sys     0m0.541s
</pre>

<p>
There are several important things happening here:

</p><ul>

  <li>
    <code>--temp 0</code> turns off the random number generator (we
    don't want improvisation for a spam filter)

  </li><li>
    Here we see why prompt eval time is king. Token generation speed
    (eval time) doesn't matter for this use case, since we're using
    the <code>--grammar</code> flag to force the LLM to only print a
    single "yes\n" or "no\n" token.

  </li><li>
    I piped the original email through <code>links -codepage utf-8
    -force-html -width 400 -dump /dev/stdin</code> which reduced the
    number of tokens and removed hidden HTML content that was put there
    by the spammer to make the email look like ham to naive Bayesian
    filters.

  </li><li>
    The <code>-c 0</code> flag configures TinyLLaMA to use the maximum
    context size, which is 2048 tokens. That's the largest prompt we can
    give it. To help avoid feeding in too much text, you can pipe the
    email through <code>sed 's/&nbsp;&nbsp;&nbsp;*/ /g' | dd bs=1
    count=7000</code> to remove superfluous spaces and place an upper
    limit on its size.

</li></ul>

<p>
Please note that spam filtering is just the tip of the iceberg. I've
always thought that "generative ai" is a misnomer, because language
models (more commonly known as "The Algorithm") have always been used by
tech companies in the past to extract knowledge, curate information, and
rank content. That requires reading rather than writing. AI has never
traditionally needed to talk that much, because the English language
just isn't that useful at the scale tech companies operate. Even if you
could generate English summaries for exabytes of text a millionth its
size, that would still be more words than any team could hope to read in
a lifetime.

</p><h3 id="gaming">
  <a href="#gaming">Performance Gains on Gaming Hardware</a>
</h3>

<p>
Gamers have the highest quality expectations of any value consumer, so
any hardware built for them is usually pretty good. In the machine
learning industry, we have thrived for years repurposing hardware that
was intended for gamers. If it weren't for their important contribution,
the AI Winter may have needed to last another ten years. So a few months
ago, I asked a gamer to build me a computer that can replace my old
Hewlett Packard.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/meatball-pc.png"><img src="https://justine.lol/matmul/meatball-pc-1200.jpg" alt="[photo of computer in black box with metal panels and rgb fans]" width="600" height="413"></a><p> LLM Performance on Intel® Core™
     i9-14900K ($600) w/ 6400 MT/s RAM
<!-- <th colspan="6"><a href="raptorlake.jpg"><img src="raptorlake.jpg" -->
<!--      alt="[photo of an Intel Core i9 14900K unlocked CPU box]" -->
<!--      width="600" height="450"></a><p> LLM Performance on Intel® Core™ -->
<!--      i9-14900K ($600) w/ 6400 MT/s RAM -->
</p></th></tr><tr>
<th>prompt<br>tok/sec
</th><th>eval<br>tok/sec
</th><th>model
</th><th>weights<br>data type
</th><th>hardware
</th><th>software
</th></tr><tr><td>   63</td><td>  12</td><td>Mistral 7b</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>   40</td><td>   9</td><td>Mistral 7b</td><td> q8_0</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   19</td><td>   7</td><td>Mistral 7b</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr><tr><td>   50</td><td>   7</td><td>Mistral 7b</td><td> f16</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>   13</td><td>   5</td><td>Mistral 7b</td><td> f16</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   10</td><td>   4</td><td>Mistral 7b</td><td> f16</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr><tr><td>  406</td><td>  67</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>  273</td><td>  53</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  114</td><td>  43</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr><tr><td>  407</td><td>  42</td><td>TinyLlama 1.1B</td><td> f16</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>   90</td><td>  31</td><td>TinyLlama 1.1B</td><td> f16</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   68</td><td>  30</td><td>TinyLlama 1.1B</td><td> f16</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
I think Alderlake is a great CPU but it's popularly misunderstood, as
evidenced by how easily I quintupled its float16 performance. Unlike
ARMv8.2, I was able to do that without introducing rounding errors,
since my x86 kernels use a float32 compute type internally. This means I
can have an even smarter spam filter. For example, when I run
my <code>spam.sh</code> shell script, it only takes 420 milliseconds,
which is 7x faster than my Raspberry Pi 5. That's right, when it comes
to small workloads, this chip is able to finish before CUDA even gets
started.

</p><p>
Alderlake owners can also look forward to the fact that llamafile takes
special care to not run on your efficiency cores. This is one of the
things that helps llamafile to go faster than llama.cpp. It also means
you can run LLMs around the clock and there's still plenty of resources
leftover for the other programs on your computer. The reason why that's
important is because llama.cpp dispatches threads in lockstep, which
would have meant that if any <code>1</code> core takes longer than the
others to do its job, then all other <code>n</code> cores would need to
busy loop until it completed.

</p><p>
However the greatest feature of this microprocessor is how quickly it
can build all 2.6 million lines of code in the Cosmopolitan monorepo. My
Hewlett Packard always took 64 seconds, but this gaming computer does it
in 20. It actually took 35 seconds originally; what made it faster is was applying
<a href="https://www.thermal-grizzly.com/en/conductonaut/s-tg-c-001-r">liquid
metal</a> and AI overclocking. Another reason systems code is so fast on
the Alderlake is there was a fire fight between the hackers and
scientists in the creation of this CPU, and the hackers won. I hope
they'll strike out a better compromise on AVX512 in the future, but
overall I'm very happy with this chip, since I believe it represents
significant progress over previous models.

</p><h3 id="apple">
  <a href="#apple">Performance Gains on Apple Hardware</a>
</h3>

<p>
If there's a personal computer with the most class, it would definitely
be the Mac Studio. Gaining the performance advantage here was harder for
me, because it's the hardware platform the llama.cpp developers care
about most, plus I'm working with a handicap due to my choice to use
Stallman's compiler instead of Apple's proprietary tools.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/mac-studio.png"><img src="https://justine.lol/matmul/mac-studio-800x800.jpg" alt="[photo of a silvery Mac Studio computer cube]" width="400" height="400"></a><p> LLM Performance on Mac Studio CPU w/ 24-core
     M2 Ultra ($5000)
</p></th></tr><tr>
<th>prompt<br>tok/sec
</th><th>eval<br>tok/sec
</th><th>model
</th><th>weights<br>data type
</th><th>hardware
</th><th>software
</th></tr><tr><td>   90</td><td>  25</td><td>Mistral 7b</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>   90</td><td>  27</td><td>Mistral 7b</td><td> q8_0</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   37</td><td>  24</td><td>Mistral 7b</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr><tr><td>   79</td><td>  15</td><td>Mistral 7b</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>   57</td><td>  15</td><td>Mistral 7b</td><td> f16</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   21</td><td>  15</td><td>Mistral 7b</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr><tr><td>  457</td><td>  95</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>  564</td><td> 108</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  236</td><td>  95</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr><tr><td>  419</td><td>  66</td><td>TinyLlama 1.1B</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>  400</td><td>  67</td><td>TinyLlama 1.1B</td><td> f16</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  141</td><td>  66</td><td>TinyLlama 1.1B</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
I wouldn't want to pick a fight with an Apple user, because their M2
microprocessor turns llamafile into a firehose of synthetic content. The
trick Apple used to do it is leveraging their vertical integration. If
you buy a Mac Studio and look inside, you'll discover that they put the
RAM DIMMs <em>inside</em> the CPU. It makes latency-bound operations
like token generation go much faster, because the CPU no longer needs to
make all these long distance phone calls. However, in terms of sheer
flops (as measured by prompt tok/sec), we can see that compared to my
much cheaper Intel computer, the M2 Ultra only exposes 30% more compute
via the ARM ISA. You need to go through their proprietary frameworks
like Metal and Accelerate if you want to access anything more. If you
have xcode installed, then llamafile by default will compile a small
stub module which does just that, since despite my values I'm happy to
help you get in front of any closed source library standing between you
and your silicon.

</p><p>
One important thing to know if you're considering buying a Mac Studio is
that, like the Windows Executive, XNU does a really good job keeping
your desktop stable, and that means protecting your system from you. It
takes me 45 seconds on Mac Studio to compile the Cosmo monorepo, due to
all these safety features; but if I fork bombed it, I'd be surprised if
Netflix skipped a single frame. My <code>spam.sh</code> script also goes
430ms, which is slower than Intel. However none of this concerns me,
since I've seen the way Asahi Linux is able to unleash the M2's full
potential.

</p><h3 id="professional">
  <a href="#professional">Performance Gains on Professional Hardware</a>
</h3>

<p>
While llamafile cares deeply about helping the GPU poor, it offers a
first-class experience to the 1% too. The AMD Ryzen Threadripper PRO
7995WX was just launched several months ago and it's the most expensive
CPU money can buy right now. It'll set you back $10,000 but you get 96
cores of AVX512, based on the Zen4 architecture.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/threadripper-pro.jpg"><img src="https://justine.lol/matmul/threadripper-pro-800x800.jpg" alt="[photo of AMD Ryzen 4 ThreadRipper Pro box]" width="400" height="400"></a><p> LLM Performance on AMD Ryzen Threadripper PRO 7995WX w/ 96 cores ($10,000)
</p></th></tr><tr>
<th>prompt<br>tok/sec
</th><th>eval<br>tok/sec
</th><th>model
</th><th>weights<br>data type
</th><th>hardware
</th><th>software
</th></tr><tr><td>  557</td><td>  17</td><td>Mistral 7b</td><td> bf16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  485</td><td>  17</td><td>Mistral 7b</td><td> f16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  197</td><td>  16</td><td>Mistral 7b</td><td> f16</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>   52</td><td>  18</td><td>Mistral 7b</td><td> f16</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td>  480</td><td>  10</td><td>Mistral 7b</td><td> f32</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  221</td><td>  10</td><td>Mistral 7b</td><td> f32</td><td>7995WX</td><td> llama.cpp 2024-03-30
</td></tr><tr><td>   38</td><td>   9</td><td>Mistral 7b</td><td> f32</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td>  382</td><td>  25</td><td>Mistral 7b</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  283</td><td>  24</td><td>Mistral 7b</td><td> q8_0</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>   37</td><td>  25</td><td>Mistral 7b</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td> 1929</td><td>  52</td><td>TinyLlama 1.1B</td><td> bf16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td> 1819</td><td>  52</td><td>TinyLlama 1.1B</td><td> f16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  824</td><td>  51</td><td>TinyLlama 1.1B</td><td> f16</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>  295</td><td>  89</td><td>TinyLlama 1.1B</td><td> f16</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td> 1268</td><td>  60</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td> 1127</td><td>  60</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>  169</td><td>  93</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
Here we see that, despite only being twice the price, the 7995WX x86 ISA
offers 7x more raw compute power than the M2 Ultra ARM ISA, and nearly
the same token generation speed, which is likely thanks to its 384mb L3
cache. When I bought this chip, I had to expand support in llama.cpp for
bfloat16 and AVX512 before I could fully test its capabilities. My work
means you can now run LLaMA 2.8x faster on Zen4 than you could before.

</p><p>
One thing I like about AVX512 is that Google's Gemma model can
<a href="https://github.com/google/gemma.cpp/issues/23">solve math
riddles on AVX512 but not on AVX2</a> because the bigger vectors usually
make it easier to reduce rounding errors. Its <code>VDPBF16PS</code>
instruction helps us updot bf16 similar to VNNI and ARM dotprod. Having
native support for bf16 is nice, since models like Mistral and TinyLLaMA
distribute weights using bfloat16 as their canonical format. If we were
to convert bf16 to fp16, then only 13% of the numbers that are possible
can be accurately represented. In practice, it matters little, since
99.71% of the numbers Mistral 7b uses are among that 13%. However I
believe that llamafile should deliver, to the best of its ability,
whatever number of bits are being claimed. Especially when doing so also
enables us to better exploit the capabilities of our hardware. Adding
bf16 support is my first big step towards improving that.

</p><p>
Please be warned that a lot of people who bought this Threadripper ran
into issues with sketchy RAM. I had to RMA the first DIMMs I bought for
this computer, because most of them died and I was getting 5 eval tokens
per second with Mistral. I've been having better luck with a new full
kit of eight sticks that just arrived today. When I run <code>sysbench
memory run</code> it reports 10,033,424 mops, which is oddly faster than
my Mac Studio where 9,892,584 mops is reported, however my Intel
computer does 14,490,952. I expected my Threadripper's RAM to have that
speed since both set of components advertised 6400 MT/s with the same
timings, but I'm told that I traded this away to have 256GB of ECC. As
for disk speed, <code>dd if=/dev/zero of=/tmp/output bs=128k count=50k;
rm -f /tmp/output</code> reports 1.6 GB/s which is 3.6x slower than my
Mac Studio, and 3x slower than my Intel (which has the same M.2 stick).
I'm told that Intel and Apple are just better at this, but I wish I
understood why.

</p><p>
Last but not least, it runs my <code>spam.sh</code> script in 323ms and
builds the whole Cosmo monorepo in 13 seconds. It's actually capable of
building it faster, since this is the first time I've ever seen my build
config being constrained by an individual artifact blocking the critical
path. I never thought I'd live to see the day. I'm also puzzled that
llamafile v0.6.2 is somehow managing to do 93 tokens per second; that's
40% faster than my M2. It's exciting news, since after reviewing the
breadth of this blog post, I would have wept if there were no more
optimizations possible.

</p><h2 id="source">
  <a href="#source">Source Code</a>
</h2>

<p>
The source code for my matrix multiplication kernels can be found at:

</p><ul>
<li><a href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp">https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp</a>
</li></ul>

<p>
Both Mozilla and myself felt it would be worthwhile to contribute these
improvements to the upstream project. Doing that required adapting the
code to their preferred method of handling portability at compile-time.
We also took the liberty of changing the license from Apache 2.0 to MIT,
since the latter is what the llama.cpp developers prefer. Here are links
to the most recent pull requests I've sent them:

</p><ul>
<li><a href="https://github.com/ggerganov/llama.cpp/pull/6414">https://github.com/ggerganov/llama.cpp/pull/6414</a>
</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/6412">https://github.com/ggerganov/llama.cpp/pull/6412</a>
</li></ul>

<h2 id="details">
  <a href="#details">Technical Details</a>
</h2>

<h3 id="howitworks">
  <a href="#howitworks">How I Improved Prompt Eval Time on CPUs</a>
</h3>

<p>
There are dozens of mathematical operations a transformer model needs to
perform in order to generate text, e.g. rope, transpose, reshape,
softmax, rms_norm, etc. All of the performance improvements I described
above, were achieved by focusing exclusively on a single one, which
is <code>GGML_OP_MUL_MAT</code>, because that's what my Linux Perf
profiler told me llamafile spends 95% of its time doing.

</p><p>
So what is this matrix multiplication thing? We shall start by defining
the most important algorithm in the world using the pythonic dialect of
Python that is most popular with developers today:

</p><pre><span>def</span> <span>matmul</span>(A, B):
  <span>assert</span> <span>len</span>(B) == <span>len</span>(A[0])
  <span>return</span> [[<span>sum</span>(A[i][l] * B[l][j]
               <span>for</span> l <span>in</span> <span>range</span>(<span>len</span>(B)))
           <span>for</span> j <span>in</span> <span>range</span>(<span>len</span>(B[0]))]
          <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(A))]
</pre>

<p>
As we can see, it's just three for loops and a multiply-add. How hard
could it be?

</p><p>
On my workstation (which I call meatball), the code above goes a
screeching 0.042 gigaflops. Most Python programmers are smart enough to
know that they should delegate tasks like these to a library
like <code>np.matmul</code>, which goes 29 gigaflops. NumPy achieves its
speed using FORTRAN which for generations has been favored
by <a href="https://justine.lol/dox/pascal.txt">real programmers</a>
who've led us to believe these libraries are something mysterious
chiseled in stone by the hand of Moses himself; but if we look at the
FORTRAN code NumPy actually uses, then it really isn't all that
complicated and could clearly benefit from some revision.

</p><pre>      <span>SUBROUTINE</span> <span>SGEMM</span>(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC)
<span>*     .. Scalar Arguments ..</span>
      <span>REAL</span> ALPHA,BETA
      <span>INTEGER</span> K,LDA,LDB,LDC,M,N
      <span>CHARACTER</span> TRANSA,TRANSB
<span>*     .. Array Arguments ..</span>
      <span>REAL</span> A(LDA,*),B(LDB,*),C(LDC,*)
      [...]
<span>*
*           Form  C := alpha*A*B + beta*C.
*</span>
              <span>DO</span> <span>90</span> J = 1,N
                  <span>IF</span> (BETA.<span>EQ</span>.ZERO) <span>THEN</span>
                      <span>DO</span> <span>50</span> I = 1,M
                          C(I,J) = ZERO
   <span>50</span>                 <span>CONTINUE</span>
                  <span>ELSE IF</span> (BETA.<span>NE</span>.ONE) <span>THEN</span>
                      <span>DO</span> <span>60</span> I = 1,M
                          C(I,J) = BETA*C(I,J)
   <span>60</span>                 <span>CONTINUE</span>
                  <span>END IF</span>
                  <span>DO</span> <span>80</span> L = 1,K
                      <span>IF</span> (B(L,J).<span>NE</span>.ZERO) <span>THEN</span>
                          TEMP = ALPHA*B(L,J)
                          <span>DO</span> <span>70</span> I = 1,M
                              C(I,J) = C(I,J) + TEMP*A(I,L)
   <span>70</span>                     <span>CONTINUE</span>
                      <span>END IF</span>
   <span>80</span>             <span>CONTINUE</span>
   <span>90</span>         <span>CONTINUE</span>
      [...]
      <span>RETURN</span>
      <span>END</span>
</pre>

<p>
I like to define my subroutines using a modern language like C++, which
goes 47 gigaflops. This means C++ is three orders of a magnitude faster
than Python. That's twenty years of progress per Moore's law.

</p><pre><span>// multiplies matrices on cpu
// with column major ordering
//
//     m×k * k×n → m×n
//     k×m * k×n → m×n if aᵀ
//     m×k * n×k → m×n if bᵀ
//     k×m * n×k → m×n if aᵀ and bᵀ
//</span>
<span>template</span> &lt;<span>typename</span> T,  <span>typename</span> TA,
          <span>typename</span> TB, <span>typename</span> TC&gt;
<span>void</span> <span>GEMM</span>(<span>bool</span> aᵀ, <span>bool</span> bᵀ,
          <span>int</span> m, <span>int</span> n, <span>int</span> k, <span>T</span> α,
          <span>const</span> <span>TA</span> *A, <span>int</span> lda,
          <span>const</span> <span>TB</span> *B, <span>int</span> ldb, <span>T</span> β,
          <span>TC</span> *C, <span>int</span> ldc) {
    <span>assert</span>(m &gt;= 0 &amp;&amp; n &gt;= 0 &amp;&amp; k &gt;= 0);
    <span>assert</span>(lda &gt;= <span>std</span>::max(1, aᵀ ? k : m));
    <span>assert</span>(ldb &gt;= <span>std</span>::max(1, bᵀ ? n : k));
    <span>assert</span>(ldc &gt;= <span>std</span>::max(1, m));
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>T</span> d = 0;
            <span>for</span> (<span>int</span> l = 0; l &lt; k; ++l) {
                <span>T</span> a = A[aᵀ ? lda * i + l : lda * l + i];
                <span>T</span> b = B[bᵀ ? ldb * l + j : ldb * j + l];
                d += a * b;
            }
            <span>if</span> (β) {
                <span>T</span> c = C[ldc * j + i];
                C[ldc * j + i] = α * d + β * c;
            } <span>else</span> {
                C[ldc * j + i] = α * d;
            }
        }
}
</pre>

<p>
In order to do better than 47 gigaflops on CPU, most C++ developers are
smart enough to know they should use a BLAS library. Mightiest of the
open source BLAS is <a href="https://github.com/flame/blis/">BLIS</a>
which is funded by Microsoft, Intel, Texas Instruments, AMD, HPE,
Oracle, Huawei, Facebook, ARM, and the National Science Foundation.

</p><blockquote>
"Any time somebody outside Intel beats MKL by a nontrivial amount, I
report it to the MKL team. It is fantastic for any open-source project
to get within 10% of MKL... [T]his is why Intel funds BLIS development."
(@jeffhammond) <a href="https://github.com/flame/blis/issues/264#issuecomment-428673275">blis/issues/264</a>
</blockquote>

<p>
That's very impressive. Matrix multiplication is the practical
application of hardware that hardware makers care about optimizing most.
Since nobody knows more about Intel hardware than Intel, I imagine it's
not everday that somebody manages to challenge Intel for supremacy on
their own platform. Based on my own evaluation, what BLIS says is true.
However that is only true for single-threaded performance. Their
multithreading mode is still experimental, but if I use
a <code>./configure</code> flag to turn it on, then I'm able to boost
performance to 85 gigaflops.

</p><p>
llama.cpp had the important insight that less is more when it comes to
linear algebra. The alpha and beta parameters are never used, so they're
always set to to 1 and 0. The op graph for LLMs are designed in such a
way that the A matrix is almost always transposed and B is almost never
transposed, which means inner dimension dot product can vectorize over
contiguous memory. The m/k dimensions are usually evenly divisible by
64. While generating tokens, n=1 is usually the case, which makes matmul
a de facto matvec for the performance most people care about. BLAS
libraries usually hurt more than they help for matrix-vector
multiplication, because it's so computationally simple by comparison.
Sort of like the difference between downloading a movie and pinging a
server. Matrix vector multiplication is an operation where latency (not
throughput) is the bottleneck, and the bloat of fancy libraries has a
measurable impact. So llama.cpp does something like this, which goes 233
gigaflops.

</p><pre><span>template</span> &lt;<span>typename</span> T&gt;
<span>void</span> <span>LLMM</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
          <span>const</span> <span>T</span> *A, <span>int</span> lda,
          <span>const</span> <span>T</span> *B, <span>int</span> ldb,
          <span>T</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>T</span> d = 0;
            <span>for</span> (<span>int</span> l = 0; l &lt; k; ++l)
                d += A[lda * i + l] * B[ldb * j + l];
            C[ldc * j + i] = d;
        }
}
</pre>

<p>
This gives us the best possible token generation speeds. However
llama.cpp's Achilles heel on CPU has always been prompt processing
speed, which goes much slower. That's because chewing through prompts
requires bona fide matrix-matrix multiplication. Being able to do this
fast is important if you care about text summarization and LLaVA image
processing. That's the reason why support for countless BLAS libraries
has been added to llama.cpp over the past year. The most formidable of
them is Intel's Math Kernel Library (MKL) which goes 384 gigaflops.

</p><p>
The difference between 233 versus 384 gigaflops may not seem like much,
at least not compared to Python, but it's a tremendous gulf. MKL is also
closed source and proprietary. We aren't even allowed to disassemble it
and try to reverse engineer how it works. Intel has been developing math
kernels for fifty years and they hold the secrets they've acquired very
close to their chest. But even if our desire for performance was so
great that we were willing to overlook the ethics of an open source
project spending the majority of its time inside a proprietary blob, the
simple fact of the matter is that integrating foreign BLAS libraries
into llama.cpp isn't that practical, due to the way its threading model
works. In order to improve prompt processing speed, we must figure out
the trick BLAS libraries use, and implement it in a scrappy
dependency-free way that stays true to llama.cpp's roots.

</p><p>
I believe the trick with CPU math kernels is exploiting instruction
level parallelism with fewer memory references. If you compile the
example above with <code>-O3 -ffast-math -march=native</code> then the
code your compiler generates should look like this:

</p><pre><span>void</span> <span>SLLMM</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
           <span>const</span> <span>float</span> *A, <span>int</span> lda,
           <span>const</span> <span>float</span> *B, <span>int</span> ldb,
           <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>__m256</span> c = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8)
                c = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l),
                                    _mm256_loadu_ps(B + ldb * j + l), c);
            C[ldc * j + i] = hsum(c);
        }
}
</pre>

<p>
So what llama.cpp usually does when it wants to improve things, is it'll
unroll the innermost loop like this:

</p><pre><span>void</span> <span>SLLMM2</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
           <span>const</span> <span>float</span> *A, <span>int</span> lda,
           <span>const</span> <span>float</span> *B, <span>int</span> ldb,
           <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>__m256</span> c0 = _mm256_setzero_ps();
            <span>__m256</span> c1 = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 16) {
                c0 = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l + 0),
                                     _mm256_loadu_ps(B + ldb * j + l + 0), c0);
                c1 = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l + 8),
                                     _mm256_loadu_ps(B + ldb * j + l + 8), c1);
            }
            C[ldc * j + i] = hsum(c0) + hsum(c1);
        }
}
</pre>

<p>
That may slightly improve numerical stability, but it does very little
to enhance performance, since modern CPUs are perfectly capable of
speculatively executing future loop iterations on their own. What we
want to do instead is unroll the <em>outer</em> loop. The advantage of
doing this becomes clear if we consider how it enables us to share
the <code>a0</code> register load across multiple floating point
operations.

</p><pre><span>void</span> <span>SLLMM4</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
            <span>const</span> <span>float</span> *A, <span>int</span> lda,
            <span>const</span> <span>float</span> *B, <span>int</span> ldb,
            <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; j += 4) {
            <span>__m256</span> c0 = _mm256_setzero_ps();
            <span>__m256</span> c1 = _mm256_setzero_ps();
            <span>__m256</span> c2 = _mm256_setzero_ps();
            <span>__m256</span> c3 = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8) {
                <strong><span>__m256</span> a0 = _mm256_loadu_ps(A + lda * (i + 0) + l);</strong>
                <span>__m256</span> k0 = _mm256_loadu_ps(B + ldb * (j + 0) + l);
                <span>__m256</span> k1 = _mm256_loadu_ps(B + ldb * (j + 1) + l);
                <span>__m256</span> k2 = _mm256_loadu_ps(B + ldb * (j + 2) + l);
                <span>__m256</span> k3 = _mm256_loadu_ps(B + ldb * (j + 3) + l);
                c0 = _mm256_fmadd_ps(a0, k0, c0);
                c1 = _mm256_fmadd_ps(a0, k1, c1);
                c2 = _mm256_fmadd_ps(a0, k2, c2);
                c3 = _mm256_fmadd_ps(a0, k3, c3);
            }
            C[ldc * (j + 0) + (i + 0)] = hsum(c0);
            C[ldc * (j + 1) + (i + 0)] = hsum(c1);
            C[ldc * (j + 2) + (i + 0)] = hsum(c2);
            C[ldc * (j + 3) + (i + 0)] = hsum(c3);
        }
}
</pre>

<p>
If we unroll both outer loops, the effect is compounded.

</p><pre><span>void</span> <span>SLLMM3X4</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
              <span>const</span> <span>float</span> *A, <span>int</span> lda,
              <span>const</span> <span>float</span> *B, <span>int</span> ldb,
              <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; i += 3)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; j += 4) {
            <span>__m256</span> c00 = _mm256_setzero_ps();
            <span>__m256</span> c01 = _mm256_setzero_ps();
            <span>__m256</span> c02 = _mm256_setzero_ps();
            <span>__m256</span> c03 = _mm256_setzero_ps();
            <span>__m256</span> c10 = _mm256_setzero_ps();
            <span>__m256</span> c11 = _mm256_setzero_ps();
            <span>__m256</span> c12 = _mm256_setzero_ps();
            <span>__m256</span> c13 = _mm256_setzero_ps();
            <span>__m256</span> c20 = _mm256_setzero_ps();
            <span>__m256</span> c21 = _mm256_setzero_ps();
            <span>__m256</span> c22 = _mm256_setzero_ps();
            <span>__m256</span> c23 = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8) {
                <span>__m256</span> k0 = _mm256_loadu_ps(B + ldb * (j + 0) + l);
                <span>__m256</span> k1 = _mm256_loadu_ps(B + ldb * (j + 1) + l);
                <span>__m256</span> k2 = _mm256_loadu_ps(B + ldb * (j + 2) + l);
                <span>__m256</span> k3 = _mm256_loadu_ps(B + ldb * (j + 3) + l);
                <span>__m256</span> a0 = _mm256_loadu_ps(A + lda * (i + 0) + l);
                c00 = _mm256_fmadd_ps(a0, k0, c00);
                c01 = _mm256_fmadd_ps(a0, k1, c01);
                c02 = _mm256_fmadd_ps(a0, k2, c02);
                c03 = _mm256_fmadd_ps(a0, k3, c03);
                <span>__m256</span> a1 = _mm256_loadu_ps(A + lda * (i + 1) + l);
                c10 = _mm256_fmadd_ps(a1, k0, c10);
                c11 = _mm256_fmadd_ps(a1, k1, c11);
                c12 = _mm256_fmadd_ps(a1, k2, c12);
                c13 = _mm256_fmadd_ps(a1, k3, c13);
                <span>__m256</span> a2 = _mm256_loadu_ps(A + lda * (i + 2) + l);
                c20 = _mm256_fmadd_ps(a2, k0, c20);
                c21 = _mm256_fmadd_ps(a2, k1, c21);
                c22 = _mm256_fmadd_ps(a2, k2, c22);
                c23 = _mm256_fmadd_ps(a2, k3, c23);
            }
            C[ldc * (j + 0) + (i + 0)] = hsum(c00);
            C[ldc * (j + 1) + (i + 0)] = hsum(c01);
            C[ldc * (j + 2) + (i + 0)] = hsum(c02);
            C[ldc * (j + 3) + (i + 0)] = hsum(c03);
            C[ldc * (j + 0) + (i + 1)] = hsum(c10);
            C[ldc * (j + 1) + (i + 1)] = hsum(c11);
            C[ldc * (j + 2) + (i + 1)] = hsum(c12);
            C[ldc * (j + 3) + (i + 1)] = hsum(c13);
            C[ldc * (j + 0) + (i + 2)] = hsum(c20);
            C[ldc * (j + 1) + (i + 2)] = hsum(c21);
            C[ldc * (j + 2) + (i + 2)] = hsum(c22);
            C[ldc * (j + 3) + (i + 2)] = hsum(c23);
        }
}
</pre>

<p>
Vectorized outer product with OpenMP goes 810 gigaflops on my Alderlake
i9-14900K with 6400 MT/s RAM when multiplying a 513×512 with a 512×512
matrix. That is twenty eight years of progress per Moore's law compared
to Python. It's clearly optimal since my CPU is listed as only being
capable of going
<a href="https://nanoreview.net/en/cpu/intel-core-i9-14900k">780
gigaflops</a>. Yes, I overclocked it with liquid metal. On the other
hand, MKL processes this matrix size at 295 gigaflops on my machine.

</p><pre><span>1</span>:        <span>vmovups</span>      (<span>%r10</span>,<span>%r9</span>,4),<span>%ymm0</span>
          <span>vmovups</span>      (<span>%rsi</span>,<span>%r9</span>,4),<span>%ymm4</span>
          <span>vmovups</span>      (<span>%rcx</span>,<span>%r9</span>,4),<span>%ymm2</span>
          <span>vmovups</span>      (<span>%rdx</span>,<span>%r9</span>,4),<span>%ymm1</span>
          <span>vfmadd231ps</span>  (<span>%r11</span>,<span>%r9</span>,4),<span>%ymm0</span>,<span>%ymm6</span>
          <span>vfmadd231ps</span>  <span>%ymm4</span>,<span>%ymm0</span>,<span>%ymm15</span>
          <span>vfmadd231ps</span>  <span>%ymm2</span>,<span>%ymm0</span>,<span>%ymm12</span>
          <span>vfmadd231ps</span>  <span>%ymm1</span>,<span>%ymm0</span>,<span>%ymm9</span>
          <span>vmovups</span>      (<span>%rdi</span>,<span>%r9</span>,4),<span>%ymm0</span>
          <span>vfmadd231ps</span>  (<span>%r11</span>,<span>%r9</span>,4),<span>%ymm0</span>,<span>%ymm5</span>
          <span>vfmadd231ps</span>  <span>%ymm4</span>,<span>%ymm0</span>,<span>%ymm14</span>
          <span>vfmadd231ps</span>  <span>%ymm2</span>,<span>%ymm0</span>,<span>%ymm11</span>
          <span>vfmadd231ps</span>  <span>%ymm1</span>,<span>%ymm0</span>,<span>%ymm8</span>
          <span>vmovups</span>      (<span>%rbx</span>,<span>%r9</span>,4),<span>%ymm0</span>
          <span>vfmadd231ps</span>  (<span>%r11</span>,<span>%r9</span>,4),<span>%ymm0</span>,<span>%ymm3</span>
          <span>add</span>          <span>$8</span>,<span>%r9</span>
          <span>vfmadd231ps</span>  <span>%ymm4</span>,<span>%ymm0</span>,<span>%ymm13</span>
          <span>vfmadd231ps</span>  <span>%ymm2</span>,<span>%ymm0</span>,<span>%ymm10</span>
          <span>vfmadd231ps</span>  <span>%ymm1</span>,<span>%ymm0</span>,<span>%ymm7</span>
          <span>cmp</span>          <span>%r9d</span>,<span>%r14d</span>
          <span>jg</span>          1<span>b</span>
</pre>

<p>
But does the C function above generalize to all matrix sizes? Nope. If I
bump the complexity up from 512 to 1024, then I'm pretty much back at
square one, not doing much better than a naive kernel, and MKL wins once
more. I personally don't view this as too problematic, since llama.cpp
by default processes prompts in modestly sized batches, and a kernel
should only need to be good for its intended size. It's also only a
matter of time until I unriddle the tricks needed for optimal tiling and
cache locality that can make my kernels scale.

</p><h3 id="threads">
  <a href="#threads">How I Got Multiple Threads to Work</a>
</h3>

<p>
Now to incorporate this into llamafile, we can't use OpenMP for the same
reason we can't use BLAS libraries. The kernel must be harmonized with
the way llama.cpp works. Its threading model is very similar to GPUs.
Ops in the model graph are processed one by one. A thread is spawned for
each core. Threads are restrained by a spinlock barrier and then set
loose to compute different parts of an output matrix in parallel as soon
as the next op is ready for execution. The id of each thread is
called <code>ith</code> and the number of threads is
called <code>nth</code>. There are no futexes or semaphores, because
kernel scheduling would greatly reduce tokens/sec. If we were to have
the <code>ith=0</code> thread call a BLAS API that spawned threads of
its own, then they'd be immediately starved of resources by all
the <code>ith&gt;0</code> threads returning to the spinlock barrier. We
can work within this model by defining a new kernel framework.

</p><pre><span>#define</span> <span>BEGIN_KERNEL</span>(RM, RN) \
    <span>int</span> ytiles = (m - m0) / RM; \
    <span>int</span> xtiles = (n - n0) / RN; \
    <span>int</span> tiles = ytiles * xtiles; \
    <span>int</span> duty = (tiles + nth - 1) / nth; \
    <span>if</span> (duty &lt; 1) \
        duty = 1; \
    <span>int</span> start = duty * ith; \
    <span>int</span> end = start + duty; \
    <span>if</span> (end &gt; tiles) \
        end = tiles; \
    <span>for</span> (<span>int</span> job = start; job &lt; end; ++job) { \
        <span>int</span> i = m0 + job / xtiles * RM; \
        <span>int</span> j = n0 + job % xtiles * RN;

<span>#define</span> <span>END_KERNEL</span>() }
</pre>

<p>
Along with a solution for packing tiles.

</p><pre><span>template</span> &lt;<span>typename</span> T&gt; <span>class</span> <span>GEMMER</span> {
  <span>public:</span>
    <span>GEMMER</span>(<span>int</span> k, <span>const</span> <span>T</span> *A, <span>int</span> lda, <span>const</span> <span>T</span> *B, <span>int</span> ldb, <span>float</span> *C, <span>int</span> ldc,
           <span>int</span> ith, <span>int</span> nth)
        : k(k), A(A), lda(lda), B(B), ldb(ldb), C(C), ldc(ldc), ith(ith), nth(nth) {
    }

    <span>void</span> <span>llmm</span>(<span>int</span> m, <span>int</span> n) {
        mnpack(0, m, 0, n);
    }

  <span>private:</span>
    <span>void</span> <span>mnpack</span>(<span>int</span> m0, <span>int</span> m, <span>int</span> n0, <span>int</span> n) {
        <span>if</span> (m - m0 &lt;= 0 || n - n0 &lt;= 0)
            return;
        <span>int</span> mc, nc, mp, np;
        <span>if</span> (m - m0 &gt;= 3 &amp;&amp; n - n0 &gt;= 4) {
            mc = 3;
            nc = 4;
            llmm3x4(m0, m, n0, n);
        } <span>else if</span> (m - m0 &gt;= 4 &amp;&amp; n - n0 &gt;= 1) {
            mc = 4;
            nc = 1;
            llmm4x1(m0, m, n0, n);
        } <span>else if</span> (m - m0 &gt;= 1 &amp;&amp; n - n0 &gt;= 4) {
            mc = 1;
            nc = 4;
            llmm1x4(m0, m, n0, n);
        } <span>else</span> {
            mc = 1;
            nc = 1;
            llmm1x1(m0, m, n0, n);
        }
        mp = m0 + (m - m0) / mc * mc;
        np = n0 + (n - n0) / nc * nc;
        mnpack(mp, m, n0, np);
        mnpack(m0, mp, np, n);
        mnpack(mp, m, np, n);
    }

    <span>// ...</span>

    <span>void</span> <span>llmm1x1</span>(<span>int</span> m0, <span>int</span> m, <span>int</span> n0, <span>int</span> n) {
        BEGIN_KERNEL(1, 1)
        <span>__m256</span> c = _mm256_setzero_ps();
        <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8)
            c = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l),
                                _mm256_loadu_ps(B + ldb * j + l), c);
        C[ldc * j + i] = hsum(c);
        END_KERNEL()
    }

    <span>const</span> <span>int</span> k;
    <span>const</span> <span>T</span> *<span>const</span> A;
    <span>const</span> <span>int</span> lda;
    <span>const</span> <span>T</span> *<span>const</span> B;
    <span>const</span> <span>int</span> ldb;
    <span>float</span> *<span>const</span> C;
    <span>const</span> <span>int</span> ldc;
    <span>const</span> <span>int</span> ith;
    <span>const</span> <span>int</span> nth;
};
</pre>

<p>
We can now export nice friendly C APIs to GGML that go 790 gigaflops
while incurring none of the latency disadvantages associated with
traditional BLAS libraries.

</p><pre><span>void</span> <span>SLLMMT</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
            <span>const</span> <span>float</span> *A, <span>int</span> lda,
            <span>const</span> <span>float</span> *B, <span>int</span> ldb,
            <span>float</span> *C, <span>int</span> ldc,
            <span>int</span> ith, <span>int</span> nth) {
    <span>if</span> (nth) {
        <span>GEMMER</span>&lt;<span>float</span>&gt; tb{k, A, lda, B, ldb, C, ldc, ith, nth};
        tb.llmm(m, n);
    } <span>else if</span> (!HAVE_OPENMP || n * m * k &lt; THRESHOLD) {
        <span>GEMMER</span>&lt;<span>float</span>&gt; tb{k, A, lda, B, ldb, C, ldc, 0, 1};
        tb.llmm(m, n);
    } <span>else</span> {
        nth = sysconf(_SC_NPROCESSORS_ONLN);
<span>#pragma omp parallel for</span>
        <span>for</span> (ith = 0; ith &lt; nth; ++ith) {
            <span>GEMMER</span>&lt;<span>float</span>&gt; tb{k, A, lda, B, ldb, C, ldc, ith, nth};
            tb.llmm(m, n);
        }
    }
}
</pre>

<h2 id="methodology">
  <a href="#methodology">Methodology</a>
</h2>

<p>
You need to run the following command on Linux in order to benchmark
llamafile reliably. It also helps a little bit with timings to run as
root, but that shouldn't be necessary.

</p><pre><span>echo</span> performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
</pre>

<p>
On Apple Silicon, I needed to build llama.cpp using the following
command in order to get it to run in CPU mode.

</p><pre>make -j32 LLAMA_NO_ACCELERATE=1 LLAMA_NO_METAL=1
</pre>

<p>
Some of you might be surprised that I didn't write my kernels in
assembly like BLIS does, especially given my history of projects
like <a href="https://justine.lol/matmul/github.com/jart/blink">blink</a>, <a href="https://justine.lol/sectorlisp2/">SectorLISP</a>
and <a href="https://justine.lol/sectorlambda2/">SectorLAMBDA</a>. The truth is I've been
coding in assembly this whole time. I configured Emacs so I can push a
button, and the disassembly for the C++ code I'm working on will pop up
on the screen in a few milliseconds. I know anyone whose codebase has
slow build times doesn't possess this advantage, which has made me
famous. Once I figure out how to do that for .cu files, I'll be
unstoppable.

</p><h2 id="credits">
  <a href="#credits">Credits</a>
</h2>

<p>
I learned how to write math kernels by renting
<a href="https://vast.ai/">Vast</a> VMs and watching
<a href="https://ahgamut.github.io/">Gautham Venkatasubramanian</a>
and
<a href="https://github.com/mrdomino">mrdomino</a> develop CUDA kernels
in a tmux session. They've been focusing on solving a much more
important challenge for llamafile, which is helping it not have a
mandatory dependency on the cuBLAS: the reigning supreme linear algebra
library of such speed, accuracy, and ferocity that it could only have
been written by the prince of darkness himself. You're encouraged to
follow our ongoing progress on GitHub.

</p><h2 class="page" id="discord"><a href="#discord">Discord</a></h2>

Congratulations on reading this far. I'd like to extend an invitation
for you to join us on the
<a href="https://discord.gg/Va7WsS56x3">Mozilla AI Discord</a>, where
you can ask questions and hang out with me, folks from Mozilla, and
others in the llamafile community.

<h2 class="page" id="funding"><a href="#funding">Funding</a></h2>

<p>
  <a href="https://justine.lol/lemuria.png">
    <picture>
      <source srcset="https://worker.jart.workers.dev/sectorlisp2/lemuria.webp" type="image/webp">
      <img src="https://worker.jart.workers.dev/sectorlisp2/lemuria.png" width="850" height="360" alt="[United States of Lemuria - two dollar bill - all debts public and primate]">
    </picture>
  </a>

</p><p>
My full-time work on open source projects like llamafile is funded
thanks to the generous support of Mozilla,
my <a href="https://github.com/sponsors/jart">GitHub sponsors</a>, and
<a href="https://www.patreon.com/jart">Patreon subscribers</a>. Thank
you everyone, for helping me have the opportunity to serve you these
last four years. Your support made it possible for high-quality math
kernels to be shared with the commons.

</p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[InternLM2 (111 pts)]]></title>
            <link>https://arxiv.org/abs/2403.17297</link>
            <guid>39889404</guid>
            <pubDate>Sun, 31 Mar 2024 23:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.17297">https://arxiv.org/abs/2403.17297</a>, See on <a href="https://news.ycombinator.com/item?id=39889404">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+Z">Zheng Cai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+M">Maosong Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+H">Haojiong Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+K">Kai Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+K">Keyu Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">Xin Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">Xun Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zehui Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zhi Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chu,+P">Pei Chu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+X">Xiaoyi Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Duan,+H">Haodong Duan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+Q">Qi Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fei,+Z">Zhaoye Fei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+Y">Yang Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ge,+J">Jiaye Ge</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+C">Chenya Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+Y">Yuzhe Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gui,+T">Tao Gui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+A">Aijia Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+Q">Qipeng Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+C">Conghui He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+Y">Yingfan Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+T">Ting Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+T">Tao Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiao,+P">Penglong Jiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+Z">Zhenjiang Jin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lei,+Z">Zhikai Lei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jiaxing Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jingwen Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+L">Linyang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S">Shuaibin Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+W">Wei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yining Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H">Hongwei Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">Jiangning Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hong,+J">Jiawei Hong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+K">Kaiwen Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+K">Kuikun Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+X">Xiaoran Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lv,+C">Chengqi Lv</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lv,+H">Haijun Lv</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lv,+K">Kai Lv</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+L">Li Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+R">Runyuan Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+Z">Zerun Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ning,+W">Wenchang Ning</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ouyang,+L">Linke Ouyang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+J">Jiantao Qiu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+Y">Yuan Qu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shang,+F">Fukai Shang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shao,+Y">Yunfan Shao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+D">Demin Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Z">Zifan Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sui,+Z">Zhihao Sui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+P">Peng Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+Y">Yu Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+H">Huanze Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+B">Bin Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+G">Guoteng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">Jiaqi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">Jiayu Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R">Rui Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yudong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Ziyi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+X">Xingjian Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Weng,+Q">Qizhen Weng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+F">Fan Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+Y">Yingtong Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+C">Chao Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+R">Ruiliang Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+H">Hang Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+Y">Yirong Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+X">Xiaogui Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+H">Haochen Ye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ying,+H">Huaiyuan Ying</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J">Jia Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J">Jing Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zang,+Y">Yuhang Zang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C">Chuyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L">Li Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+P">Pan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+P">Peng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+R">Ruijie Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Shuo Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Songyang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W">Wenjian Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W">Wenwei Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xingcheng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xinyue Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+H">Hui Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Q">Qian Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+X">Xiaomeng Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+F">Fengzhe Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Z">Zaida Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhuo,+J">Jingming Zhuo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zou,+Y">Yicheng Zou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+X">Xipeng Qiu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiao,+Y">Yu Qiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+D">Dahua Lin</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2403.17297">View PDF</a>
    <a href="https://arxiv.org/html/2403.17297v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Hang Yan [<a href="https://arxiv.org/show-email/8e76a335/2403.17297">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 26 Mar 2024 00:53:24 UTC (3,291 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XZ Backdoor: Times, damned times, and scams (260 pts)]]></title>
            <link>https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and</link>
            <guid>39889286</guid>
            <pubDate>Sun, 31 Mar 2024 23:35:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and">https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and</a>, See on <a href="https://news.ycombinator.com/item?id=39889286">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>re: </span><a href="https://www.openwall.com/lists/oss-security/2024/03/29/4" rel="nofollow ugc noopener">https://www.openwall.com/lists/oss-security/2024/03/29/4</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp" width="1456" height="779" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:779,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:164998,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7beda52c-beec-4d60-983f-13d7f4061d57_1456x779.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em>Written by Rhea Karty and Simon Henniger</em></p><p>There has been a recent backdoor found in the xz/liblzma tarball. In what is likely one of the largest breaches of trust in the free software ecosystem, this backdoor is likely to have been put in by Jia Tan, a long-time maintainer of xz. Throughout his tenure as a maintainer, Jia remained relatively mysterious — as is not uncommon in the community, little beyond his name (which is likely a lie) is known about him. Generally, anonymity in the free software sphere is a good thing: software is inherently based on accomplishment and merit, and there is no reason to know anything about a person’s identity. However, in this case, where someone built up the trust of a community for years and then abused it, it is interesting to see who they are. Luckily for us, Jia’s activity does provide some metadata which we can potentially use to learn more about him. So here’s an analysis on what we can learn from his work patterns and time zone.</p><p><span>What is to be learned from times? The conditions under which software is created! Think about the wide range of things that time patterns tell us about — there are people who are paid to do code and those who work on it as a hobby. Those in some areas code during different times as others. There are holidays, sleep schedules, and work-life balance— code is not exempt from these. Understanding when someone codes helps us understand </span><em>why</em><span> and </span><em>where</em><span> they are coding for. Figuring these out can give us a much better idea about why and who did this.&nbsp;</span></p><p><span>The following analysis was conducted on JiaT75’s (</span><a href="https://github.com/JiaT75?tab=overview&amp;from=2021-12-01&amp;to=2021-12-31" rel="nofollow ugc noopener">https://github.com/JiaT75?tab=overview&amp;from=2021-12-01&amp;to=2021-12-31</a><span>) commits to the XZ repository and their time stamps.</span></p><p>Let’s first address the elephant in the room: Yes, you can change Git timestamps to whatever you want. It doesn’t even take a degree in cybersec to do — setting the environment variables GIT_AUTHOR_DATE and GIT_COMMITTER_DATE as you commit is enough. If you forget and haven’t pushed yet, you can even change the date later as you amend the commit.</p><p>However, plausibly forging time data is actually hard: You shouldn’t push a commit that you made to look as though it was made in the future, and you also shouldn’t set a date that is so far in the past that the commit looks older than online discussions that it references. This means you will often not be able to change the time without adding latency, i.e., withholding commits and thus delaying project development -- and even then, getting it right every time is hard. After all, everyone who develops software knows how tempting it is to type the magic words “git commit” without having properly checked everything.</p><p>An easier way than actually changing the time would be to change only the time zone! Ideally, you would change it to something that still leads to somewhat plausible hours. For example, someone who works during regular European office hours but claims to be on the East Coast would likely raise some suspicion, as 11 am Central European Time is 5 AM Eastern, and who regularly works in the early morning? For a hacker, it is much more plausible to work in the afternoon and late at night, so someone who keeps regular office hours would want to change into a time zone about five hours ahead or so (so they start working at 2 pm (9 am real-time) and end at 10 pm (5 pm real time)).</p><p>I think that is what Jia Tan did. Based on his name, he wanted people to believe he is Asian — specifically Chinese— and the vast majority of his commits (440) appear to have a UTC+08 time stamp. The +0800 is likely CST, the time zone of China (or Indonesia or Philippines or Western Australia), given almost no one lives in Siberia and the Gobi desert.</p><p>However, I believe that he is actually from somewhere in the UTC+02 (winter)/UTC+03 (DST) timezone, which includes Eastern Europe (EET), but also Israel (IST), and some others. Forging time zones would be easy — no need to do any math or delay any commits. He likely just changed his system time to Chinese time every time he committed.</p><p>We see him usually working 9 am to 6 pm (adjusted to EET). This makes much more sense than someone working at midnight and 1 am on a Tuesday night (non-adjusted, using UTC+08).</p><p>Except sometimes, he forgot to change his time zone. There are 3 commits and 6 commits, respectively, with UTC+02 and UTC+03. The UTC+02 time zones match perfectly with the winter time (February and November), while the UTC+03 matches with summer (Jun, Jul, and early October). This matches perfectly with the daylight savings time switchover that happens in Eastern Europe; we see a switch to +0200 in the winter (past the last weekend of October) and +0300 in the summer (past the last Sunday in March). Incidentally, this seems to be the same time zone as Lasse Collin and Hans Jansen.</p><p>OK, you say. That is one theory, but maybe not a plausible one at this point. What if he just flew from wherever he lived in UTC+08 to EET sometimes? Well, it turns out that when we take a closer look, we see that this is not plausible. Let’s analyze the few times when Jia was recorded in a non-+0800 time zone. Here, we notice that there are some situations where Jia switches between +0800 and +0300/+0200 in a seemingly implausible time. Indicating that perhaps he is not actually in +0800 CST time, as his profile would like us to believe.&nbsp;</p><p>Notably, on 6 Oct 2022, we see two commits, one at 21:53:09 +0300 followed by another at 17:00:38 +0800. If we do the math, there is about an 11-hour difference between the two commits. However, a flight from China to anywhere in Eastern Europe or the Middle East takes at least 10-12 hours raw flight time, and likely much longer, considering non-stop flights between these locations are rare.</p><p>Even more damning, on Jun 27, 2023, we see the following: one commit at  23:38:32 +0800, another at 17:27:09 +0300. This is only a difference in a matter of minutes!</p><p><span>There is also one more vital clue to which country he worked in: Holidays. We notice that Jia’s work schedule and holidays seem to align much better with an Eastern European than a Chinese person. Disclaimer: I am not an expert in Chinese holidays, so this very well could be inaccurate. I am referencing this list of bank holidays: </span><a href="https://www.bankofchina.co.id/en-id/service/information/latest-news/2022/public-holidays-in-china-hk-and-the-us-in-2023.html" rel="nofollow ugc noopener">https://www.bankofchina.co.id/en-id/service/information/latest-news/2022/public-holidays-in-china-hk-and-the-us-in-2023.html</a></p><p>Chinese bank holidays (just looking at 2023):</p><p>- Working on 2023, 29 September: Mid Autumn Festival</p><p>- Working on 2023, 05 April: Tomb Sweeping Day</p><p>- Working on 2023, 26, 22, 23, 24, 26, 27 Jan: Lunar New Year</p><p>Eastern European holidays:</p><p>- Never working on Dec 25: Christmas (for many EET countries)</p><p>- Never working Dec 31 or Jan 1: New Years&nbsp;</p><p>To further investigate, we can try to see if he worked on weekends or weekdays: was this a hobbyist or was he paid to do this? The most common working days for Jia were Tue (86), Wed (85), Thu (89), and Fri (79).</p><p>Again, none of this is rock-solid evidence for anything — but we thought this was an interesting enough observation to post. Based on this, “Jia” most likely worked regular office hours and was based somewhere in UTC+02/03 (e.g. EET).</p><p>Update (Mar 31): Thank you all for your great feedback! We slightly updated the post: We switched around the times from the two commits where time zones change too quickly (our point stands; we just reversed the examples). We also clarified that UTC +02/03 includes a wider geographical range than just Eastern Europe.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adaptive RAG – dynamic retrieval methods adjustment (105 pts)]]></title>
            <link>https://arxiv.org/abs/2403.14403</link>
            <guid>39888943</guid>
            <pubDate>Sun, 31 Mar 2024 22:55:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.14403">https://arxiv.org/abs/2403.14403</a>, See on <a href="https://news.ycombinator.com/item?id=39888943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.14403">View PDF</a>
    <a href="https://arxiv.org/html/2403.14403v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: <a href="https://github.com/starsuzi/Adaptive-RAG" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Soyeong Jeong [<a href="https://arxiv.org/show-email/e2dd56b6/2403.14403">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2403.14403v1">[v1]</a></strong>
        Thu, 21 Mar 2024 13:52:30 UTC (8,186 KB)<br>
    <strong>[v2]</strong>
        Thu, 28 Mar 2024 06:45:11 UTC (8,186 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A deep dive into email deliverability in 2024 (259 pts)]]></title>
            <link>https://www.xomedia.io/blog/a-deep-dive-into-email-deliverability/</link>
            <guid>39888383</guid>
            <pubDate>Sun, 31 Mar 2024 21:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xomedia.io/blog/a-deep-dive-into-email-deliverability/">https://www.xomedia.io/blog/a-deep-dive-into-email-deliverability/</a>, See on <a href="https://news.ycombinator.com/item?id=39888383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          				<div data-scroll="1" data-offset="30">
						<ol><li><a href="#overview">Overview</a></li><li><a href="#whos-affected">Who's Affected?</a></li><li><a href="#timeline">Timeline</a></li><li><a href="#the-guidelines">The Guidelines</a></li><li><a href="#sender-authentication">Sender Authentication</a></li><li><a href="#impact">Impact</a></li><li><a href="#tools">Tools</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#bonus">Bonus</a></li><li><a href="#summary">Summary</a></li><li><a href="#references">References</a></li></ol>					</div>
			






<p>Around 50 years ago, in October 1971, Ray Tomlinson, a graduate of MIT, sent the first email email over a network.. Last year, <a href="https://www.statista.com/statistics/456500/daily-number-of-e-mails-worldwide/" rel="noopener" target="_blank"> ~121 trillion emails were sent</a> between <a href="https://www.statista.com/statistics/255080/number-of-e-mail-users-worldwide/" rel="noopener" target="_blank"> ~4.3 billion people</a>. Email is the the most important written form of communication on this planet and will remain so for the foreseeable future.</p>

<h2>Overview</h2>

<p>On October 3, 2023, <a href="https://blog.google/products/gmail/gmail-security-authentication-spam-protection/" rel="noopener" target="_blank">Google</a> and <a href="https://blog.postmaster.yahooinc.com/post/730172167494483968/more-secure-less-spam" rel="noopener" target="_blank">Yahoo</a> announced upcoming email security standards to prevent spam, phishing and malware attempts. <a href="https://learn.microsoft.com/en-us/microsoft-365/security/office-365-security/email-authentication-dmarc-configure?view=o365-worldwide" rel="noopener" target="_blank">Outlook.com</a> (formerly Hotmail) is also enforcing these policies.</p>

<p>With the big 3 Email Service Providers (ESP) in agreement, expect widespread adoption soon. Today’s threats are more complex than ever and more ESPs will begin tightening the reigns. <b><mark>Failure to comply with these guidelines will result in emails being blocked beginning April 2024</mark></b>.</p>



<p>In this article, we’re going to cover these guidelines and explain what senders must do in order to achieve and maintain compliance.</p>

<p>The biggest change involves implementing email authentication standards like SPF, DKIM, and DMARC. These standards have been around for a while, but are only now being strictly enforced by major email service providers. Like SSL (HTTPS) for the web and MFA (Multi Factor Authentication) to protect your online accounts, organizations will be expected to conform to these guidelines.</p>

<p>This is what non-compliance (for Gmail) looks like in an email server log:</p>

<pre><code>host gmail-smtp-in.l.google.com [108.177.15.26]
SMTP error from remote mail server after pipelined end of data:
550-5.7.26 This mail is unauthenticated, which poses a security risk to the
550-5.7.26 sender and Gmail users, and has been blocked. The sender must
550-5.7.26 authenticate with at least one of SPF or DKIM. For this message,
550-5.7.26 DKIM checks did not pass and SPF check for [MYDOMAIJN.com] did not
550-5.7.26 pass with ip: [xxx.xxx.xxx.xxx]. The sender should visit
550-5.7.26  https://support.google.com/mail/answer/81126#authentication for
550 5.7.26 instructions on setting up authentication
</code></pre>

<p><code>
<a href="https://support.google.com/a/answer/3726730#5726" rel="noopener" target="_blank">550, "5.7.26"</a><a>, "This message does not have authentication information or fails to pass authentication checks (SPF or DKIM). To best protect our users from spam, the message has been blocked. Please visit Prevent mail to Gmail users from being blocked or sent to spam for more information."
</a></code></p><p><a>Here’s the complete listing of </a><a href="https://support.google.com/a/answer/3726730" rel="noopener" target="_blank">Gmail SMTP errors codes</a>.</p>

<p>This is quickly becoming a mandatory standard for senders, so every business will need to become familiar with them – or risk email deliverability to customers.</p>

<h2>Who’s Affected?</h2>

<p>Enforcement primarily pertains to <b>Bulk Senders</b>:</p>

<div><blockquote><p>“A <b>bulk sender</b> is any email sender that sends close to 5,000 messages or more to personal Gmail accounts within a 24-hour period. Messages sent from the same primary domain count toward the 5,000 limit.”</p></blockquote></div>





<p>These guidelines require bulk senders to enable SPF, DMARC and DKIM for their domains.</p>

<p>While these guidelines primarily affect bulk senders, senders with less volume per day can also be affected if they are not adhering to these guidelines. We recommend that all organizations, regardless of daily volume – implement these standards and adhere to guidelines.

</p><p>It’s also very important that both senders and recipients understand these requirements. Implementing them protects partners, customers and anyone receiving email. Improving email security and user experience can indirectly influence how your emails reach user inboxes</p>

<h2>Timeline</h2>

<p><a href="https://support.google.com/a/answer/14229414#zippy=%2Cwhat-is-the-timeline-for-enforcement-of-sender-guidelines" rel="noopener" target="_blank">Google</a><a></a></p><p><a>Starting February 2024, Gmail will require bulk senders to authenticate their emails. Changes will be gradual and progressive, giving businesses time to implement and test these changes.</a></p><a>

<p><span>February 2024:</span> Bulk senders who don’t meet sender requirements will start getting temporary errors (with error codes) on a small percentage of their non-compliant email traffic. These temporary errors are meant to help senders identify email traffic that doesn‘t meet guidelines so that they can resolve issues that result in non-compliance.</p><p><span>April 2024:</span> Google will start rejecting a percentage of non-compliant email traffic, and will gradually increase the rejection rate. For example, if 75% of a sender‘s traffic meets  requirements, Google will  start rejecting a percentage of the remaining 25% of traffic that isn’t compliant.</p></a><p><a><span>June 1, 2024: </span>Bulk senders must implement a clearly visible </a><a href="https://datatracker.ietf.org/doc/html/rfc8058" rel="noopener" target="_blank">one-click unsubscribe</a> in the body of the email message for all commercial and  promotional messages.</p>

<p><a href="https://senders.yahooinc.com/best-practices/" rel="noopener" target="_blank">Yahoo</a></p>

<p><span>Q1 2024:</span> All bulk senders will be required to to authenticate their email, enable easy  one-click unsubscribe (also starting June 2024) and only send emails users want.</p>

<h2>The Guidelines</h2>

<p>Google</p>

<ul>
<li><a href="https://support.google.com/a/answer/81126" rel="noopener" target="_blank">Email sender guidelines</a></li>
<li><a href="https://support.google.com/a/answer/14229414" rel="noopener" target="_blank">Email sender guidelines FAQ</a></li>
</ul> 

<p>Yahoo</p>

<ul><li><a href="https://senders.yahooinc.com/best-practices/" rel="noopener" target="_blank">Sender Requirements &amp; Recommendations</a></li></ul>

<p>As mentioned earlier, <a href="https://learn.microsoft.com/en-us/microsoft-365/security/office-365-security/email-authentication-dmarc-configure?view=o365-worldwide" rel="noopener" target="_blank">Outlook.com</a> (formerly Hotmail) is also enforcing these policies.</p>

<p>Here’s a quick summary of the guidelines:</p>

<p><span>Sender Authentication:</span> Senders should implement email authentication protocols like SPF, DKIM, and DMARC to prevent email spoofing and phishing attempts. We’ll cover this in more detail later in this article.</p>

<p><span>Bulk Senders Requirements:</span> Sending unsolicited bulk emails can lead to deliverability  issues (spam filtering) and reputation damage. Email providers have various algorithms and user reports to identify and filter spam. Google will require bulk senders (those sending 5,000+ emails per day to Gmail) to meet stricter requirements for compliance with spam thresholds.</p>

<p><span>Easy Unsubscribe: </span>Implement easy unsubscribe options (One-click Unsubscribe). Gmail users have tools to report spam, unsubscribe from unwanted emails and control their inbox experience. If it is too difficult to unsubscribe from your emails, customers will be more likely to flag your email as spam. Additional links provided in the ‘References’ section at the end of this article.</p>

<p><span>Engagement:</span> Avoid misleading subject lines, excessive personalization, or promotional content that triggers spam filters. Focus on providing relevant and valuable information when considering email content.</p>

<p>Special  Considerations:</p><ul>
<li>Keep your email spam rate is less than 0.3%.</li>
<li>Don’t impersonate email ‘From:’ headers.</li>
<li>Ensure that sending domains or IPs have valid forward and reverse DNS records, also referred to as PTR records.</li>
<li>Use a TLS connection for transmitting email.</li>
<li>Make sure your forward and reverse DNS records are valid.</li>
<li>Ensure receivers can easily unsubscribe from your marketing messages.</li>
<li>Format messages according to the ‘Internet Message Format standard’ <a href="https://datatracker.ietf.org/doc/html/rfc5322" rel="noopener" target="_blank">RFC3322</a></li>
<li>If you regularly forward email, including using mailing lists or inbound gateways – add <a href="https://support.google.com/a/answer/81126#arc" rel="noopener" target="_blank">ARC</a><a> headers to outgoing messages.</a></li><a>
<li>For direct mail, the domain in the sender’s From: header must be aligned with either the SPF domain or the DKIM domain. This is required to pass DMARC alignment.</li>
<li>Marketing messages and subscribed messages must support one-click unsubscribe, include a clearly visible unsubscribe link in the message body and process recipient unsubscribe requests within 2 days.</li>

</a><li><a>Reference </a><a href="https://support.google.com/a/answer/81126" rel="noopener" target="_blank">this</a> for the full list.</li>
</ul>

<h2>Sender Authentication</h2>

<p>In this section e discuss Email Authentication and how to avoid the spam folder.</p>

<p>There are 3 authentication standards to help protect an organization’s email:</p>

<p><b>SPF</b> (Sender Policy Framework) specifies the servers and domains allowed to send email for your business. This protects against spoofing and helps prevent your emails from being flagged as spam. This is added as a record on  public DNS server that is used to check the source IP of the email and compares it with a DNS TXT record.</p>

<p><code>v=spf1 ip4:173.236.251.117 include:netblocks.dreamhost.com include:relay.mailchannels.net mx ~all</code></p>

<p><b>DKIM</b> (DomainKeys Identified Mail), used to digitally sign every outgoing message sent from your organization. The receiving server uses this to verify that it came from your business. It is a unique key for domain that allows mail servers to verify email authenticity and resist tampering. It is a generated key that is configured on a public DNS server.</p>

<p><code>
v=DKIM1; k=rsa; h=sha256; p=MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmY24P6ntSL6CbVrv++vyTdgVJP4jxAEDoYbpo2vMEpOb2SDnUsiiBnd8rINyMh9BMA5whxKC/w7oqYD9dr5mvfPtkVPBSz9PqFHE2s/QsFnJlBsUJJrLFBlSXw+F95TTZyqNboANJuGCGpbg207KloAd0PZxaHjyBqj9fTfFLUCp/TrEmaMZ1E3LwqXd2jqC2mUCBcIMzIOcT68eU0b5LTlRMPL7k07BOlMGSx8Ez2wYltyDXPQc9IM8rOlMmtO92O/PkyqhyqJF+QxMSAgV6CLhtghmwbRFjvKUbkAtdCYmfRqDiPrrTCZhV7RX6+3gg7F6MPstL+KefKKToFunFQIDAQAB
</code></p>

<p><b>DMARC</b> (Domain-based Message Authentication, Reporting and Conformance) is an email authentication protocol designed to give domain owners the ability to protect against  spoofing, phishing, email scams and other cyber threats. It instructs receiving servers on how to handle outgoing messages from your organization that don’t pass SPF or DKIM.</p>

<p>More specifically, DMARC standardizes how email receivers perform email authentication using the SPF and DKIM authentication mechanisms. The policy is is published in the public Domain Name System (DNS) as text TXT records and used  by the receiving email server to authenticate incoming emails based on the name / value tags that are defined:</p>

<p><code>"v=DMARC1; p=reject; sp=none; fo=1; rua=mailto:<span data-cfemail="fa9e979b8899a5888f9bba9f979b93969e9f9c9f94899fd48a8895959c8a9593948ed4999597">[email&nbsp;protected]</span>; ruf=mailto:<span data-cfemail="7d19101c0f1e220f081b3d18101c141119181b18130e18530d0f12121b0d12141309531e1210">[email&nbsp;protected]</span>"</code></p>



<p>DMARC:</p>

<ol>
<li>
<p><b>Reduces email Spoofing &amp; Phishing</b>: Prevents bad actors from impersonating an organization’s domain by verifying which domain the email originated from.</p>
<p>The domain name in the <code>From:</code> field in the email envelop header is inspected and aligned with other domains authenticated by either SPF or DKIM:</p>
<code>From:	John Doe &lt;<span data-cfemail="442e202b2104213c25293428216a272b29">[email&nbsp;protected]</span>&gt;</code>
</li>
<li>
<p><b>Improves Email Deliverability</b>: Sets policies for how the receiving email server should deal with failures.</p>
<p>When emails fail DMARC authentication, the DMARC policy instructs how the receiving email server should handle emails that fail SPF or DKIM checks. The options are: Accept, Reject or Quarantine the email – ensuring senders that their legitimate emails are reaching recipients inboxes.</p>
</li>
<li>
<p><b>Provides Reporting &amp; Feedback</b>: DMARC provides a reporting mechanism for policy actions performed by the above policy.</p>
<p>These reports provide insights to domain owners about emails sent from their organization (even if they were sent by unauthorized parties). They can be helpful in identifying and addressing email spoofing attempts.</p>
</li>
</ol>

<p>Here’s a simple diagram to help explain the entire email journey:</p>

<p><img decoding="async" src="https://www.xomedia.io/wp-content/uploads/2024/02/diag1-8544.png" alt="Email diagram"></p>

<ul>
<li>Sender composes and sends an email.</li>
<li>Sender’s MTA (Mail Transfer Agent on mail server) adds a DKIM signature to the email header as a special field..</li>
<li>Recipient’s MTA checks SPF and DKIM records.</li>
<li><p>DMARC alignment is verified, and the policy is applied:</p>
   <ul>
      <li>If a message passes authentication by the receiving server – <b>Deliver</b> to user's inbox.</li>
      <li><p>If a message fails authentication by the receiving server:</p>
         <p><b>Quarantine</b> (send them to recipients’ spam folder).</p>
         <p>NOTE: If the receiving mail server has a quarantine configured, messages might be sent to quarantine, rather than directly to the recipient's spam folder.</p>
         <p><b>Reject</b> messages are never deliver to the recipient. The receiving server usually sends a bounce message to the sender</p>
      </li>
   </ul>
</li>
</ul>

<p>Here’s an example email envelop from an organization that passes all of the email security guidelines:</p>

<p><img decoding="async" src="https://www.xomedia.io/wp-content/uploads/2024/02/diag2-8544.png" alt="Email Headers"></p>

<p>Proper configuration of these standards shields against attacks and increases deliverability so that messages land in inboxes, not spam folders.</p>

<h2>Impact</h2>

<p>Google continuously updates its algorithms and user-reported data to improve email filtering and user experience. Google’s <a href="https://workspace.google.com/blog/product-announcements/ridding-gmail-of-100-million-more-spam-messages-with-tensorflow" rel="noopener" target="_blank">AI</a> <a href="https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters" rel="noopener" target="_blank">Spam filtering</a> algorithms block 99.9% of spam, phishing and malware attempts from landing in your inbox. For 1.8 billion accounts, 15 billion unwanted emails are being blocked daily.</p>

<p>The following email statistics reveal the impact these new security guidelines will have on deliverability and engagement (especially for email marketing campaigns and newsletters):</p>

<ul>
<li>In 2025, the number of email users is expected to reach 4.6 billion (<a href="https://techjury.net/blog/email-marketing-stats/" rel="noopener" target="_blank">Techjury</a>)</li>
<li>In 2023, we expect to see an average of over 347 billion emails sent per day (<a href="https://www.oberlo.com/statistics/how-many-emails-are-sent-per-day" rel="noopener" target="_blank">Oberlo</a>)</li>
<li>There are projected to be an estimated 4.37 billion email users in 2023 (<a href="https://www.statista.com/statistics/255080/number-of-e-mail-users-worldwide/" rel="noopener" target="_blank">Statistics</a></li>
<li>Millennials and Gen Xers rely on their email more than any other generation at 98% (<a href="https://www.statista.com/statistics/1332384/us-users-depending-on-emails-by-age/" rel="noopener" target="_blank">Statista</a>)</li>
<li>In 2021, an average of just over 2 hours a day are spent on email (<a href="https://www.statista.com/statistics/1332517/time-spent-checking-emails-us-users-daily/" rel="noopener" target="_blank">Statista</a>)</li>
<li>63% of people who open up an email try and find a discount (<a href="https://www.lxahub.com/stories/email-marketing-stats-and-trends-for-2023" rel="noopener" target="_blank">LXA</a>)</li>
<li>99% of email users check their inbox every day, with some checking 20 times a day (<a href="https://blog.hubspot.com/marketing/email-marketing-stats" rel="noopener" target="_blank">HubSpot</a>)</li>
<li>The image above tells us that email marketing revenue is estimated to reach almost 12.5 billion by the end of 2024 (<a href="https://www.statista.com/statistics/812060/email-marketing-revenue-worldwide/" rel="noopener" target="_blank">Statista</a>)</li>
<li>58% of consumers check their email first thing in the morning (<a href="https://optinmonster.com/is-email-marketing-dead-heres-what-the-statistics-show/" rel="noopener" target="_blank">Optinmonster</a>)</li>
<li>84.3% of consumers say they check their emails at least once a day (<a href="https://www.mailjet.com/resources/research/email-engagement-2021/" rel="noopener" target="_blank">Mailjet</a>)</li>
</ul>

<p>Additional email stats from <a href="https://www.sixthcitymarketing.com/email-marketing-stats/" rel="noopener" target="_blank">Sixth City Marketing</a>

</p><p>A list of the most email providers as of 2024 (in millions):</p>

<div>
<table>
  <thead>
    <tr>
      <th scope="col">Provider</th>
      <th scope="col">Market Share</th>
      <th scope="col"># of Users</th>
      <th scope="col">Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">Gmail</th>
      <td>29.67%</td>
      <td>1800</td>
      <td>Dominant provider, growing market share</td>
    </tr>
    <tr>
      <th scope="row">Apple Mail</th>
      <td>22.82%</td>
      <td>1000</td>
      <td>Apple device users</td>
    </tr>
    <tr>
      <th scope="row">Outlook.com</th>
      <td>4.3%</td>
      <td>500</td>
      <td>formerly Hotmail</td>
    </tr>
    <tr>
      <th scope="row">Yahoo Mail</th>
      <td>2.55%</td>
      <td>150</td>
      <td>Declining market share</td>
    </tr>
    <tr>
      <th scope="row">Yandex.Mail</th>
      <td>1.1%</td>
      <td>65</td>
      <td>Popular in Russia / Eastern Europe</td>
    </tr>
    <tr>
      <th scope="row">QQ Mail</th>
      <td>1%</td>
      <td>60</td>
      <td>Dominant in China</td>
    </tr>
    <tr>
      <th scope="row">GMX</th>
      <td>.7%</td>
      <td>42</td>
      <td>Popular in Germany</td>
    </tr>
    <tr>
      <th scope="row">AOL Mail</th>
      <td>.6%</td>
      <td>36</td>
      <td>Declining market share</td>
    </tr>
    <tr>
      <th scope="row">Zoho Mail</th>
      <td>.3%</td>
      <td>18</td>
      <td>Business-focused provider</td>
    </tr>
    <tr>
      <th scope="row">ProtonMail</th>
      <td>.25</td>
      <td>15</td>
      <td>Privacy-focused provider</td>
    </tr>
  </tbody>
</table>
</div>



<p>Sources: <a href="https://www.oberlo.com/ebooks/get-sales-dropshipping/email-marketing" rel="noopener" target="_blank">Oberlo</a>, 
<a href="https://www.litmus.com/email-client-market-share" rel="noopener" target="_blank">Litmus</a>, 
<a href="https://www.statista.com/statistics/255080/number-of-e-mail-users-worldwide/" rel="noopener" target="_blank">Statista</a>

</p><p>NOTE: Countless smaller providers and self-hosted (businesses) email solutions exist. Accounting for a collective market share is difficult to accurately assess.</p>

<h2>Tools</h2>

<p>For convenience, a curated list of free online resources to help you set up, check and maintain your organization’s email hygiene:</p>

<p><a href="https://www.spf-record.com/generator" rel="noopener" target="_blank">SPF Generator</a></p><p><a href="https://easydmarc.com/tools/dkim-record-generator" rel="noopener" target="_blank">DKIM Generator</a></p><p><a href="https://www.kitterman.com/dmarc/assistant.html" rel="noopener" target="_blank">DMARC Generator</a></p><p><a href="https://tools.wordtothewise.com/spf" rel="noopener" target="_blank">SPF Check</a></p>
<p><a href="https://dkimcore.org/tools/keycheck.html" rel="noopener" target="_blank">DKIM Check</a></p>
<p><a href="https://dmarcian.com/dmarc-inspector/" rel="noopener" target="_blank">DMARC Check</a></p>

<p><a href="https://transparencyreport.google.com/safe-browsing/search" rel="noopener" target="_blank">Google Safe Browsing site status</a>:
A free online <a href="https://transparencyreport.google.com/about" rel="noopener" target="_blank">site transparency service</a> you can use periodically to determine if your domain has been added to their unsafe site list. Also check any other domains that are linked to yours.</p><p>

<a href="https://mxtoolbox.com/SuperTool.aspx" rel="noopener" target="_blank">SuperTool</a>: A free online tool to verify your MX, SPF, DKIM and DMARC records. It also has a ‘Blacklist Check’ tool that allows you to verify if your email IP / Domain has been blacklisted on any of the  DNS-based blacklist (DNSBL) services.</p><p><a href="https://mxtoolbox.com/emailhealth" rel="noopener" target="_blank">Email Health Report</a>: A free, comprehensive online email health check that looks for DNS, domain and server issues. It also searches to see if your domain is listed on any blacklist databases. The results of this report can be used to address email related issues for your domain.</p>

<p><a href="https://dnschecker.org/ip-blacklist-checker.php" rel="noopener" target="_blank">IP and Email Blacklist Check</a>: A free online tool to determine if a domain, IP address, or email address is enlisted in the DNSBL and other blacklist databases for suspicious activity. Blacklisted addresses cannot send any emails. It‘s a stricter form being blocked that requires going through an appeals process to be removed from their list.</p>

<p><a href="https://mxtoolbox.com/deliverability" rel="noopener" target="_blank">Email Deliverability Report</a>: Send a test email using this tool and it will: 1)Analyze the headers and blacklist reputation of your outbound IP address, 2) Verify your SPF record and 3) Email link with a comprehensive deliverability report.</p>

<p><a href="https://mxtoolbox.com/DmarcReportAnalyzer.aspx" rel="noopener" target="_blank">DMARC Report Analyzer</a>: This tool will make DMARC Aggregate XML reports human readable by parsing and aggregating them by IP address into readable reports.</p>

<p><a href="https://mxtoolbox.com/SuperTool.aspx" rel="noopener" target="_blank">DNS Reverse Lookup</a>: Using the MxToolbox SuperTool again, perform an MX lookup from the domain you send emails from to determine your MX IP addresses (usually 2). Use these IPs to perform a DNS ‘Reverse Lookup’ from the MxToolbox SuperTool drop-down menu.</p><p><a href="https://support.google.com/a/answer/9981691" rel="noopener" target="_blank">Google Postmaster Tools</a></p>

<p>This is a free service that provides valuable insights and diagnostics. If you are a bulk sender (5,000 daily emails), it’s worth setting up to get valuable information about:

</p><ul>
<li>When recipients mark your messages as spam.</li>
<li>Why your messages might not be delivered.</li>
<li>If your messages are authenticated.</li>
<li>Your domain or IP reputation and its impact on message delivery rates.</li>
</ul>

<p><a href="https://senders.yahooinc.com/" rel="noopener" target="_blank">Yahoo Sender Hub</a>: Yahoo’s version of Postmaster Tools</p>



<h2>Implementation</h2>

<p>Implementing these guidelines will no doubt pose challenges for smaller organizations with limited resources. As mentioned earlier, many companies faced similar challenges transitioning to HTTPS for the Web, and MFA for online accounts. Email compliance is now rapidly becoming a mandatory standard that every business must become familiar with.</p>

<p>While these efforts may seem daunting, it’s important to understand that adapting to stricter security guidelines brings significant benefits such as stronger email security, customer trust and ensures that your emails are reaching their inboxes. Prioritizing these measures will elevate your brand and pave the way for business success.</p>

<p>To implement email authentication, consult with your service provider’s resources or support. For example, here’s collection of resource links for some well-known service providers:</p>

<div id="accordionExample" data-bs-parent="#accordionExample">
            <p>NOTE: Cloudflare provides a nice tool in their dashboard to help you generate and implement these records.</p>
            <p><a href="https://www.cloudflare.com/learning/dns/dns-records/dns-spf-record/" rel="noopener" target="_blank">SPF</a><br>
            <a href="https://www.cloudflare.com/learning/dns/dns-records/dns-dkim-record/" rel="noopener" target="_blank">DKIM</a><br>
            <a href="https://www.cloudflare.com/learning/dns/dns-records/dns-dmarc-record/" rel="noopener" target="_blank">DMARC</a><br>
         </p></div>



<p>Adoption is an important first step, being vigilant in the face of evolving security threats is equally important. Businesses will also need to be diligent about keeping up with changing standards.</p>

<p>By leveraging automation, organizations can effortlessly keep pace with shifting standards – ensuring continuous compliance:</p>

<ol>
<li>Continuously monitor email provider documentation for policy updates.</li>
<li>Query and perform DNS record verification and validation.</li>
<li>Analyze DMARC reports (both aggregate and forensic) for anomalies based on defined keywords.</li>
<li>Configure an email alert to notify you of any events triggered from the above steps.</li>
</ol>





<h2>Bonus</h2>

<p>In this section we’re going to show you a couple of methods hackers employ to exploit email security loopholes. Failing to secure email systems properly exposes your customers to malicious actors who can hijack email domains for nefarious purposes.</p>

<p>Email Spoofing and Phishing are two tactics commonly used to trick recipients:</p>

<p><b>Spoofing (impersonating)</b>: Involves sending emails that appear to be from a legitimate sender, like a bank, a company or even a friend. The goal is to trick the recipient into clicking on a malicious link, opening an infected attachment or revealing personal information. Spoofing is often used in phishing emails that aim to extract personal data from recipients.

</p><p><b>Phishing (action)</b>: Is a type of cyberattack that uses deceptive emails to trick recipients into revealing sensitive information, clicking on malicious links or downloading infected attachments. The attackers typically impersonate legitimate organizations or individuals to gain the recipient’s trust and make the email appear believable.

</p><p>The following command line mail utilities are commonly used in scripts to change email envelop header information such as “From:”, “To:” and “Subject:”.</p>

<p>A hacker will also go to great lengths setting up a remote web server with a login page that looks like the login page of a banking institution or social media platform.</p>

<p>These commands are automated using scripts (small programs) that are easily adaptable to impersonate an email from your bank with a  link to a fake login page to fool you into entering login credentials. These attacks can be very persistent, crafty and intelligently designed to evade both human and spam detection measures.</p>

<p>The <a href="https://linux.die.net/man/1/mail" rel="noopener" target="_blank">mail / mailx</a> utility:</p>

<pre><code>cat login.php | mail -s "$(echo -e "Test\nContent-Type: text/html")" <span data-cfemail="9ff5f0f7f1b1fbf0fadffae7fef2eff3fab1fcf0f2">[email&nbsp;protected]</span> -- -f <span data-cfemail="592a2c2929362b2d193b383732773a3634">[email&nbsp;protected]</span></code></pre>

<pre><code>echo "email body" | mailx -s "An email subject" <span data-cfemail="bac8dfd9d3cad3dfd4cefadfc2dbd7cad6df94d9d5d7">[email&nbsp;protected]</span> -a "From: <span data-cfemail="3d4e585359584f7d58455c504d5158135e5250">[email&nbsp;protected]</span>"</code></pre>

<p>The <a href="https://linux.die.net/man/1/mutt" rel="noopener" target="_blank">Mutt Mail User Agent</a></p>

<pre><code>mutt -H - "$2" &lt;&lt;EOF
From: $1
To: $2
Subject: $3
Importance: high

$4
EOF
</code></pre>

<h2>Summary</h2>

<p>In this article we covered:</p>

<ul>
<li>The new mandatory guidelines and best practices that email providers are enforcing in 2024 and the impact it will have on businesses.</li>
<li>We explained how to implement these guidelines and how to remain compliant.</li>
<li>We also provided some free online tools to help with implementation and compliance.</li>
</ul>

<p>Bulk senders are required to authenticate emails, be careful not exceed spam rate thresholds and implement one-click unsubscribes for email marketing campaigns and newsletters.</p>

<p>As email service providers enforce stricter guidelines, organizations need to be proactive with compliance. While implementing these guidelines can be complex, the benefits will be well worth it. Not only will it enhance security, it will boost email deliverability and engagement rates. Ultimately this will drive more sales and revenue, helping you stand out in a crowded digital marketplace.</p>

<p>Just as many companies faced similar challenges transitioning to HTTPS and MFA, email compliance is rapidly becoming a standard that every business must become familiar with. These security benefits will elevate your brand and pave the way for business success by building customer trust and ensuring emails reach their inboxes.</p>

<h2>References</h2>



<p>Google</p>
<p><a href="https://support.google.com/a/answer/81126" rel="noopener" target="_blank">Email sender guidelines</a><br>
<a href="https://support.google.com/a/answer/14229414" rel="noopener" target="_blank">Email sender guidelines FAQ</a><br>
<a href="https://blog.google/products/gmail/gmail-security-authentication-spam-protection/" rel="noopener" target="_blank">New Gmail protections for a safer, less spammy inbox</a><br>
<a href="https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters" rel="noopener" target="_blank">Understanding Gmail’s spam filters</a><br>
<a href="https://workspace.google.com/blog/product-announcements/ridding-gmail-of-100-million-more-spam-messages-with-tensorflow" rel="noopener" target="_blank">Spam does not bring us joy—ridding Gmail of 100 million more spam messages with TensorFlow</a></p><p>Yahoo</p>
<p><a href="https://senders.yahooinc.com/best-practices/" rel="noopener" target="_blank">Sender Requirements &amp; Recommendations</a><br>
<a href="https://blog.postmaster.yahooinc.com/post/730172167494483968/more-secure-less-spam" rel="noopener" target="_blank">Postmaster @ Yahoo &amp; AOL</a><br>
<a href="https://blog.postmaster.yahooinc.com/" rel="noopener" target="_blank">Yahoo Mail Blog</a><br>
<a href="https://senders.yahooinc.com/subhub/" rel="noopener" target="_blank">One-Tap Unsubscribe</a><br>
<a href="https://senders.yahooinc.com/contact/" rel="noopener" target="_blank">Email Sender Support</a></p><p>Outlook.com (formerly Hotmail)</p>
<p><a href="https://learn.microsoft.com/en-us/microsoft-365/security/office-365-security/email-authentication-dmarc-configure?view=o365-worldwide" rel="noopener" target="_blank">Sender guidelines</a></p><p>



<a href="http://www.open-spf.org/" rel="noopener" target="_blank">open-spf.org</a><br>
<a href="https://dmarc.org/" rel="noopener" target="_blank">dmarc.org</a><br>
<a href="https://www.dkim.org/" rel="noopener" target="_blank">ww.dkim.org</a></p><p>RFCs</p><p>

<a href="https://datatracker.ietf.org/doc/html/rfc7208" rel="noopener" target="_blank">SPF RFC 7208</a><br>
<a href="https://datatracker.ietf.org/doc/html/rfc6376" rel="noopener" target="_blank">DKIM RFC 6376</a><br>
<a href="https://datatracker.ietf.org/doc/html/rfc7489" rel="noopener" target="_blank">DMARC RFC 7489</a><br>
<a href="https://datatracker.ietf.org/doc/html/rfc5322" rel="noopener" target="_blank">Internet Message Format RFC 5322</a><br>
<a href="https://datatracker.ietf.org/doc/html/rfc5321" rel="noopener" target="_blank">SMTP RFC 5321</a><br>
<a href="https://datatracker.ietf.org/doc/html/rfc2369" rel="noopener" target="_blank">Mailing Lists RFC 2369</a><br>
<a href="https://datatracker.ietf.org/doc/html/rfc8058" rel="noopener" target="_blank">One-Click Unsubscribe RFC 8058</a><br>

</p><p>Important: Sending ‘unsolicited email’, or ‘cold emailing’ are governed by various laws and regulations depending on your target audience and location:</p>

<ul>
<li><a href="https://www.ftc.gov/business-guidance/resources/can-spam-act-compliance-guide-business" rel="noopener" target="_blank">CAN-SPAM Act (USA)</a>: This law governs commercial email marketing in the United States. It outlines requirements for subject lines, sender identification, unsubscribe options, and more. Compliance is mandatory.</li>
<li><a href="https://eur-lex.europa.eu/eli/reg/2016/679/oj" rel="noopener" target="_blank">GDPR (EU)</a>: Applies to processing personal data of individuals within the European Union. Requires explicit consent for marketing emails unless you have a legitimate business relationship with the recipient.</li>
<li><a href="https://laws-lois.justice.gc.ca/eng/acts/E-1.6/" rel="noopener" target="_blank">CASL (Canada)</a>: Canadian Anti-Spam Legislation imposes similar requirements to CAN-SPAM, including opt-in consent and unsubscribe options.</li>
<li>Many other countries have specific laws governing unsolicited email. Always do your homework before sending emails internationally.</li>
</ul>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Overview: What are Cpp2 and cppfront? How do I get and build cppfront? (164 pts)]]></title>
            <link>https://hsutter.github.io/cppfront/</link>
            <guid>39888203</guid>
            <pubDate>Sun, 31 Mar 2024 21:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hsutter.github.io/cppfront/">https://hsutter.github.io/cppfront/</a>, See on <a href="https://news.ycombinator.com/item?id=39888203">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  



<div><p><span>hello.cpp2</span></p><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span>main</span><span>:</span><span> </span><span>()</span><span> </span><span>=</span><span> </span><span>{</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span>    </span><span>std</span><span>::</span><span>cout</span><span> </span><span>&lt;&lt;</span><span> </span><span>"Hello, world!</span><span>\n</span><span>"</span><span>;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span>}</span>
</code></pre></div>
<h2 id="what-is-cpp2"><a id="what-is-cpp2"></a> What is Cpp2?</h2>
<p>"Cpp2," short for "C++ syntax 2," is my (<a href="https://github.com/hsutter">Herb Sutter's</a>) personal project to try to make writing ordinary C++ types/functions/objects be much <strong>simpler and safer</strong>, without breaking backward compatibility.</p>
<p><strong>What it isn't.</strong> Cpp2 is not a successor or alternate language with its own divergent or incompatible ecosystem. For example, it does not have its own nonstandard incompatible modules/concepts/etc. that compete with the Standard C++ features; it does not replace your Standard C++ compiler or other tools; and it does not require any changes to your Standard C++ compiler or standard library or other libraries or tools to keep fully using all of them.</p>
<p><strong>What it is.</strong> Cpp2 aims to be another "skin" for C++ itself, just a simpler and safer way to write ordinary C++ types/functions/objects, and a faster way to experiment with proposals for future new Standard C++ features in a simpler compiler and syntax flavor. It seamlessly uses Standard C++ modules and concepts requirements and other features, and it works with all existing C++20 or higher compilers and libraries and tools right out of the box with no changes required to use them all seamlessly and directly with zero overhead.</p>
<p>Bjarne Stroustrup said it best:</p>
<blockquote>
<p>"Inside C++, there is a much smaller and cleaner language struggling to get out." <br>  — Bjarne Stroustrup, <em>The Design and Evolution of C++</em> (D&amp;E), 1994</p>
<p>"Say 10% of the size of C++ in definition and similar in front-end compiler size. ... most of the simplification would come from generalization." <br>  — Bjarne Stroustrup, <em>ACM History of Programming Languages III</em>, 2007</p>
</blockquote>
<p><strong>My goal is to try to prove that Stroustrup is right:</strong> that it's possible and desirable to have true C++ with all its expressive power and control and with full backward compatibility, but in a flavor that's 10x simpler with fewer quirks and special cases to remember, <sup id="fnref:simpler"><a href="#fn:simpler">1</a></sup> and 50x safer where it's far easier to not write security bugs by accident.</p>
<p>We can't make an improvement that large to C++ via gradual evolution to today's syntax, because some important changes would require changing the meaning of code written in today's syntax. For example, we can never change a language feature default in today's syntax, not even if the default creates a security vulnerability pitfall, because changing a default would break vast swathes of existing code. Having a distinct alternative syntax gives us a "bubble of new code" that doesn't exist today, and have:</p>
<ul>
<li>
<p><strong>Freedom to make any desired improvement, without breaking any of today's code.</strong> Cpp2 is designed to take all the consensus C++ best-practices guidance we already teach, and make them the default when using "syntax 2." Examples: Writing unsafe type casts is just not possible in Cpp2 syntax; and Cpp2 can change language defaults to make them simpler and safer. You can always "break the glass" when needed to violate the guidance, but you have to opt out explicitly to write unsafe code, so if the program has a bug you can grep for those places to look at first. For details, see <a href="https://github.com/hsutter/cppfront/wiki/Design-note%3A-Unsafe-code">Design note: unsafe code</a>.</p>
</li>
<li>
<p><strong>Perfect link compatibility always on, perfect source compatibility always available (but you pay for it only if you use it).</strong> Any type/function/object/namespace written in either syntax is always still just a normal C++ type/function/object/namespace, so any code or library written in either Cpp2 or today's C++ syntax ("Cpp1" for short) can seamlessly call each other, with no wrapping/marshaling/thunking. You can write a "mixed" source file that has both Cpp2 and Cpp1 code and get perfect backward C++ source compatibility (even SFINAE and macros), or you can write a "pure" all-Cpp2 source file and write code in a 10x simpler syntax.</p>
</li>
</ul>
<h2 id="what-is-cppfront"><a id="what-is-cppfront"></a> What is cppfront?</h2>
<p><a href="https://github.com/hsutter/cppfront"><strong>Cppfront</strong></a> is a compiler that compiles Cpp2 syntax to today's Cpp1 syntax. This lets you start trying out Cpp2 syntax in any existing C++ project and build system just by renaming a source file from <code>.cpp</code> to <code>.cpp2</code> and <a href="#adding-cppfront-in-your-ide-build-system">adding a build step</a>, and the result Just Works with every C++20 or higher compiler and all existing C++ tools (debuggers, build systems, sanitizers, etc.).</p>
<p>This deliberately follows Bjarne Stroustrup's wise approach with <a href="https://en.wikipedia.org/wiki/Cfront"><strong>cfront</strong></a>, the original C++ compiler: In the 1980s and 1990s, Stroustrup created cfront to translate C++ to pure C, and similarly ensured that C++ could be interleaved with C in the same source file, and that C++ could always call any C code with no wrapping/marshaling/thunking. By providing a C++ compiler that emitted pure C, Stroustrup ensured full compatibility with the C ecosystems that already existed, and made it easy for people to start trying out C++ code in any existing C project by adding just another build step to translate the C++ to C first, and the result Just Worked with existing C tools.</p>
<h2 id="how-do-i-get-and-build-cppfront"><a id="build-cppfront"></a> How do I get and build cppfront?</h2>
<p>The full source code for cppfront is at the <a href="https://github.com/hsutter/cppfront"><strong>Cppfront GitHub repo</strong></a>.</p>
<p>Cppfront builds with any recent C++ compiler. Go to the <code>/cppfront/source</code> directory, and run one of the following:</p>
<p><img width="120" src="https://user-images.githubusercontent.com/1801526/188906112-ef377a79-b6a9-4a30-b318-10b51d8ea934.png"></p>
<div><p><span>MSVC build instructions (Visual Studio 2019 version 16.11 or higher)</span></p><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>cl<span> </span>cppfront.cpp<span> </span>-std:c++20<span> </span>-EHsc
</code></pre></div>
<div><p><span>GCC build instructions (GCC 10 or higher)</span></p><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>g++<span> </span>cppfront.cpp<span> </span>-std<span>=</span>c++20<span> </span>-o<span> </span>cppfront
</code></pre></div>
<div><p><span>Clang build instructions (Clang 12 or higher)</span></p><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>clang++<span> </span>cppfront.cpp<span> </span>-std<span>=</span>c++20<span> </span>-o<span> </span>cppfront
</code></pre></div>
<p>That's it!</p>
<h3 id="next-hello-world">➤ Next: <a href="https://hsutter.github.io/cppfront/welcome/hello-world/">Hello, world!</a></h3>













                
              </article>
            </div>
        
          
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The xz backdoor thing reminds me of a story (166 pts)]]></title>
            <link>https://rigor-mortis.nmrc.org/@simplenomad/112184869681420177</link>
            <guid>39888059</guid>
            <pubDate>Sun, 31 Mar 2024 21:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rigor-mortis.nmrc.org/@simplenomad/112184869681420177">https://rigor-mortis.nmrc.org/@simplenomad/112184869681420177</a>, See on <a href="https://news.ycombinator.com/item?id=39888059">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Documentation for the AMD 7900XTX (201 pts)]]></title>
            <link>https://github.com/geohot/7900xtx</link>
            <guid>39888020</guid>
            <pubDate>Sun, 31 Mar 2024 20:58:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/geohot/7900xtx">https://github.com/geohot/7900xtx</a>, See on <a href="https://news.ycombinator.com/item?id=39888020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:geohot/7900xtx" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Bbm29U7kkirqD69d4U8VCI98hZrKKVw6plEcSdd4rnDC3WvAU8rf-dmgFf2np9163DOlKcMiX6p0NKkohtP3xA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="geohot/7900xtx" data-current-org="" data-current-owner="geohot" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=9nUgLNL7jE2efv1Z3BfexLLQigduzXwDD8NHGKLH5Edff21GcH7XEpfMDolr9sb2uzZsQOYYUWaBshWeYBdyxXM90zmUV4j%2FPPCyC2UDUvDLxda%2BtZX6LTHzzBy7g09l1CToOTCY%2BjmOOnQjDK3SBU66ZL2xwK5YnVMVs4YBI7sqh6XCWzfLfHWRwZTCEn3yiE%2FH33l2eisDGXvH0HS6dRXyxYKWPOtXFLRD7HzhlGuV3Zo6kJ0LiUSKzDzhLOMPswvtCJ3E7bOU%2Fceg%2F5Csi%2F%2FzoLzWRdbId2%2B4D0g2xK8tRp0evRbHV3qHHDFiIW%2FzLp6PTUtDSQQE838hcbhAr5QOQxPpl8fmGA5wnms5ornCLlCKvkZkb7%2FTI%2BrKPohMbU%2FaGDtvYkOGW52aA6NZm%2FJcc3F2MkkHbsGw%2BHJHUJlhPPEkL%2FhYVyjQcTuB8OaP15Q2TvH5sUUO8tdHWDs0OK5k0opjc5FA5FMfMZflAxMeRHQwRUJl%2FMghEWRD%2FUTVEOjLRwovsLjJOg%3D%3D--%2BIkhp4HySKVuwjVn--U3m%2FmTrBOSTVu%2BKpnsHXCg%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=geohot%2F7900xtx" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/geohot/7900xtx&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="8098647099f4edc1c635658ee6c515f4b3f3caa76190c613f24dd4516f096639" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Upscayl – Free and Open Source AI Image Upscaler (228 pts)]]></title>
            <link>https://github.com/upscayl/upscayl</link>
            <guid>39887931</guid>
            <pubDate>Sun, 31 Mar 2024 20:45:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/upscayl/upscayl">https://github.com/upscayl/upscayl</a>, See on <a href="https://news.ycombinator.com/item?id=39887931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:upscayl/upscayl" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="-ZSlF2dTevsgP4IvVVHlZz9oIBtre5cQUP9i7Qif_x9sW3JYtDiWX6LCSNcz-_Tau5hnscBN-oIveLDPjUgdnw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="upscayl/upscayl" data-current-org="upscayl" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=94YiitpG1rJCfo46mf%2BzBwyqzf8QC6Cs6Yckw0vN7OsCmVsVOP2GRYvTaACz9r92XjfYbKr%2FqMJNr4kc8pIenz4gYPG%2FAS9RyUoxAvVXDzEissZlU5ddF9xFoPW6r9HUU1oW%2F5JTTo4cgBfKfuTe9B5jz15jsgZJO7cGCWQoNrEHyhagiycyqDVJn0w5c0Pv3jImQYcrZhmZ1l6EP7PvhvxlbMZR61Vx1Omi%2B%2FsZ668%2FABvScsBbykYxjxorDuA1aoqDBrRiAaGOM0sYt9yui3yRsut7j%2Bi%2B1Iv9uPgxJFQKoAPK4CD%2BhRU%2Bl8wUXcNiveBPz%2FuCvMdb293xVKv0z3vN2HxIgGn7PaU%2Fg0iizMQLP7IxVzoH8YVV5Jz8d%2BRFsfNEzgMoqwecUHJcn3wWKh1ZKDXxtr2glzzb%2F1yvb9ov08Wy%2FF7VwSkEzv4GDu9xijAu4yP6pC8Ihko4w2OUfxsDzBYNtVvXDg120mxwtZd0RpXhN%2BBrG5PWe%2BKLh%2FDaBZFM%2BHXSJDxiTQ%3D%3D--KOrtsqCHmBQoZo%2B2--t6oFgzZXkddmSCHr2RmjcQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=upscayl%2Fupscayl" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/upscayl/upscayl&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ef262b44652cc593e72a06c18e829ecf8e9650ffab9e7f21267b02266ee17e93" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Libmui is a macOS Classic widget lib for Linux (258 pts)]]></title>
            <link>https://github.com/buserror/libmui</link>
            <guid>39887313</guid>
            <pubDate>Sun, 31 Mar 2024 19:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/buserror/libmui">https://github.com/buserror/libmui</a>, See on <a href="https://news.ycombinator.com/item?id=39887313">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:buserror/libmui" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="TLLd6_0f6aEYzoxh2cvEZ2t-iBGlc_PLwd5SLcq84-tApsw7KrMl5-0mkU8J-08JvW_kf2kZecPhv23a-a3RaA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="buserror/libmui" data-current-org="" data-current-owner="buserror" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=QjoZs2vFSsZaikD9c9ZWVcwPpWU6wR4osuGdKXSGgftoMQZD5V1kjz1%2Fq9b17ftvmlWSHIhWqSR34JEkZuINW8%2BfQvc0H15g%2BLw%2FRbW90O2Hrl9TcYFAOEHdVJoVwwcUU4XQUaUqujNN4QEyKQ%2Fu21sv5OnAv4NqoKyXesEg8IigjP0eAdcm9JERJTe5%2BzYEO%2Fm1vGtubnCo2qmP69MKAJEzUwN%2FBpjn1U7NTKP2sq4jQO3pSETgzXuveY%2BQ%2F15%2B8%2B2p%2FDP6LqTxiQJWvO65Ghl4tXm3EUCWCdk3uYMcOoaoYwP%2BJuIr39mWVODaVWD2Lfl0yqf6cwQa49fRTu%2FxYOYmWz%2BIY%2F8HAqnb58eC4LY2MqtWHfkEmFBkzfXroORl6nITcP2ow7oie1FNM2CVHJ%2FhB5DL54H1n0vcKn%2FDJfd%2FPntf1mGSh38ocYLs1qj9DHWOX3JsZL3pdl5O%2FzKTxSt3e59eaV78RNJaMpnp3d9A3Ll3m%2FsSPQfbjvuozX8ut57xnDPO%2FGgmZA%3D%3D--Rp43ntbpkPmnIKwe--%2Fj%2FbYbTG3qBSeP0eMEgNXg%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=buserror%2Flibmui" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/buserror/libmui&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="3baa4d0f6cc5b38e76c6953d8e5f2cd90b90ff081e5bad3b17d18778fa1555fb" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fired Americans Say Indian Firm Gave Their Jobs to H-1B Visa Holders (108 pts)]]></title>
            <link>https://www.msn.com/en-us/money/careers/fired-americans-say-indian-firm-gave-their-jobs-to-h-1b-visa-holders/ar-BB1kIVHE</link>
            <guid>39887307</guid>
            <pubDate>Sun, 31 Mar 2024 19:33:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-us/money/careers/fired-americans-say-indian-firm-gave-their-jobs-to-h-1b-visa-holders/ar-BB1kIVHE">https://www.msn.com/en-us/money/careers/fired-americans-say-indian-firm-gave-their-jobs-to-h-1b-visa-holders/ar-BB1kIVHE</a>, See on <a href="https://news.ycombinator.com/item?id=39887307">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Dan Lynch Has Died (SRI, Arpanet, Internet) (142 pts)]]></title>
            <link>https://www.nytimes.com/2024/03/31/technology/daniel-c-lynch-dead.html</link>
            <guid>39887275</guid>
            <pubDate>Sun, 31 Mar 2024 19:29:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/03/31/technology/daniel-c-lynch-dead.html">https://www.nytimes.com/2024/03/31/technology/daniel-c-lynch-dead.html</a>, See on <a href="https://news.ycombinator.com/item?id=39887275">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/03/31/technology/daniel-c-lynch-dead.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[A proposal to add signals to JavaScript (260 pts)]]></title>
            <link>https://github.com/proposal-signals/proposal-signals</link>
            <guid>39886328</guid>
            <pubDate>Sun, 31 Mar 2024 17:48:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/proposal-signals/proposal-signals">https://github.com/proposal-signals/proposal-signals</a>, See on <a href="https://news.ycombinator.com/item?id=39886328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:proposal-signals/proposal-signals" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="JPLiRTFCVM-zjKL33EUqoyRpWSBEptNwTuAilEPWK0CB9nGInK-HMdTajuzfLdWT7cXUF6XZ-kR6VyNEdNK9Jg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="proposal-signals/proposal-signals" data-current-org="proposal-signals" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=RkL2ks%2FoSafFxr9%2FLwK9qN5oPSjbNo4nnGCFwzdWzvqmclrIxsjgU3xGhFS1dN2QeEP8aon5OIYhJtd0Fkt5C9m5ylHd5%2FOEtX7vLVrmwzJZwQD8Y211k8ySsudUGiduoEvMLDWEFH1ClTIDzYmWr%2FavglT2pNMKbu3Oo%2Fu8LIvJCHoYueDgKmN81ZwfNMkzUPUMLwvh0hdUwyNUHUlkHaTmugQUAS5ZHTBQ1rawLKocVP26HY8soi6u7KxoyzV9uZyBSd5MH3zAME1Vyd2SNUaeqJoSjw41g%2FcD%2F0k4wvLjF3rvkAn0sqSlcpZmTwoxEqpO%2BSU0%2BvdXMkAPukFffUA85XvdFCAs8inSMO49Tyx5v%2ByqLKbRls%2FVzi%2Bn6giaVIEva3OfQ%2FGCn9KT67TLPciF03Mi76NGk6oQJ0U2lwXUMaL%2FfitHNruDMOREVt4T2psAtm%2BIFmydMsM8Rbm5KD6pE3GBueJQLmtHCf0siurrxBfW8JvRoU5z5KtK0i7k4pfERLwX0ERPNkNz1PIWoEnr6DTkgKHoyJK742fu4GwVKg%3D%3D--OPAKdcNwL3DfgP9c--j3vgSAKre8Xyy18%2FHmCtjg%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=proposal-signals%2Fproposal-signals" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/proposal-signals/proposal-signals&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="a0205c3c13aa1a5b5f2431f205e562d88abe74a1fb7b566d52de898d657ddda1" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A. K. Dewdney has died (165 pts)]]></title>
            <link>https://lfpress.remembering.ca/obituary/alexander-dewdney-1089463499</link>
            <guid>39886272</guid>
            <pubDate>Sun, 31 Mar 2024 17:43:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lfpress.remembering.ca/obituary/alexander-dewdney-1089463499">https://lfpress.remembering.ca/obituary/alexander-dewdney-1089463499</a>, See on <a href="https://news.ycombinator.com/item?id=39886272">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			    	<div role="tabpanel" id="obituary" data-gtm-page-section="obit-details">
						
		       			<p><span>
						<img itemprop="image" src="https://cdn-otf-cas.prfct.cc/dfs1/eyJkIjo3MiwieCI6IjAiLCJ5IjoiNDciLCJjdyI6IjQ5OS45OTk5OTk5OTk5OTk5NCIsImNoIjoiNjQzLjEwMzQ0ODI3NTg2MiIsInJvIjoiMCIsImNyIjoiMSIsInciOjUwNSwiaCI6NzM2LCJ1cmwiOiJodHRwOlwvXC9hZGFzLW9yZWdvbi1jYXMtb2JpdHMuczMuYW1hem9uYXdzLmNvbVwvcGhvdG9zXC9jcmVhdGVfc3RvcnlcLzY1ZjBiNzUyMWQ0ZDdcL2U5YjJlMTcwMTg4YzhlYzcxYzlkZTIyMmI5Y2IuanBnIn0=" alt="Alexander 
Keewatin Dewdney">		
						<p> After a brief illness Alexander Keewatin "Kee" Dewdney paased away surrounded by family at Victoria Hospital in London, Ontario. He was predeceased by his first wife, Patricia Dewdney in 2020, and by his son, Jonathan, in 2022. <br>
Kee was born in London to author Selwyn Dewdney and art therapist Irene Dewdney. Early in life he developed a passionate interest in biology, but it was his fascination with higher mathematics that led to a joint doctorate in mathematics from Michigan and Waterloo Universities. From 1969 until his retirement in 1995 Kee went on to teach in the computer science department at The University of Western Ontario, after which he became Full Professor of Computer Science at The University of Waterloo. His mathematical focus was complexity theory and theory of computation. During these years he also wrote a monthly column for Scientific American. <br>
His lifelong passion for biology eventually led to his becoming steward of the Newport Forest tract on the Thame River, where he compiled an extensive list of species while also researching the mathematics of species abundance. In recognition of this work he was appointed Adjunct Professor of Biology at The University of Western Ontario. <br>
He is the author of more than a dozen nonfiction books, including a speculative bestseller called The Planiverse. Briefly, in the 1960s, he made experimental films, most notably The Malatese Cross Movement. <br>
All who knew Kee were touched by his charisma, his wonderful sense of humour and his steadfast generosity. He will be greatly missed by his wife, Elaine Dewdney, his brothers, Donner, Peter and Christopher, his nieces and nephews and many friends, and by members of London's Muslim community, where he was known as Khalil.</p></span></p>							
		               			
						<p>Published online <span> </span>March 12, 2024
							</p>						
		                		

						
						
						

						<div role="none">
						    <p role="none">Send Flowers: When Is the Ordering Deadline?</p>
						    <div role="none">
						   		<div role="none">
						   			<p><i></i> <span role="none">Next-Day Delivery</span>
						   			</p>
						   			<p role="none">ANY DAY OF THE WEEK</p>
						   			<p role="none">Order any time up till the day before</p>
						   		</div>
						   		<div role="none">
						   			<p><i role="none"></i> <span role="none">Same-Day Delivery</span>
						   			</p>
						   			<div role="none">
							   			<p role="none">
							   				<span>MON-FRI</span>
							   				<span>Order by 2:00PM</span>						   				
							   			</p>
							   			<p role="none">
							   				<span>SAT &amp; SUN</span>				   				
							   				<span>Order by noon</span>
							   			</p>
						   			</div>
						   		</div>
						   	</div>
						   	<div role="none">
						   			<p><i role="none"></i> <span role="none">Morning Delivery</span>
						   			</p>
						   			<div role="none">
							   			<p role="none">
							   				<span>TUES-SAT</span>
							   				<span>Order by 3:00PM<br>The day before</span>
							   			</p>
							   			<p role="none">
							   				<span>SAT &amp; SUN</span>
							   				<span>Order by Saturday</span>
							   			</p>
							   		</div>
						   		</div>		   
					    	<p>Note: These are general guidelines; some florists may not be able to operate within these timelines. During peak periods such as Valentine’s Day, Memorial Day and most holidays, florists are not always able to keep up to demand. Tribute will contact you if there are any issues.</p>
					    </div>
					</div>
					<!-- EVENTS TAB PANEL -->
					<div role="tabpanel" id="events">		   
					    

    <div>
        <p>No Events Scheduled At This Time</p>
    </div>   

					    		
						
					    <div role="none">						
						    <p role="none">Send Flowers: When Is the Ordering Deadline?</p>
						    <div role="none">
						   		<div role="none">
						   			<p><i></i> <span role="none">Next-Day Delivery</span>
						   			</p>
						   			<p role="none">ANY DAY OF THE WEEK</p>
						   			<p role="none">Order any time up till the day before</p>
						   		</div>
						   		<div role="none">
						   			<p><i role="none"></i> <span role="none">Same-Day Delivery</span>
						   			</p>
						   			<div role="none">
							   			<p role="none">
							   				<span>MON-FRI</span>
							   				<span>Order by 2:00PM</span>						   				
							   			</p>
							   			<p role="none">
							   				<span>SAT &amp; SUN</span>				   				
							   				<span>Order by noon</span>
							   			</p>
						   			</div>
						   		</div>
						   	</div>
						   	<div role="none">
						   			<p><i role="none"></i> <span role="none">Morning Delivery</span>
						   			</p>
						   			<div role="none">
							   			<p role="none">
							   				<span>TUES-SAT</span>
							   				<span>Order by 3:00PM<br>The day before</span>
							   			</p>
							   			<p role="none">
							   				<span>SAT &amp; SUN</span>
							   				<span>Order by Saturday</span>
							   			</p>
							   		</div>
						   		</div>		   
					    	<p>Note: These are general guidelines; some florists may not be able to operate within these timelines. During peak periods such as Valentine’s Day, Memorial Day and most holidays, florists are not always able to keep up to demand. Tribute will contact you if there are any issues.</p>							
						</div>
					
					
						
						
					</div>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Is anybody getting value from AI Agents? How so? (162 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39886178</link>
            <guid>39886178</guid>
            <pubDate>Sun, 31 Mar 2024 17:32:16 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39886178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39886327"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886327" href="https://news.ycombinator.com/vote?id=39886327&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I’ve seen a lot of attempts but nothing that worked really well. Using an agent as a glorified search engine can work, but trying to replace actual humans to handle anything but the most standard use cases is still incredibly hard. There’s a lot of overhyped rhetoric at the moment around this tech, and looks like we’re heading into another period of post-hype disillusionment.<p>Legal angles here also also super interesting. There’s a growing body of scenarios where companies are held accountable for the goofs of their AI “assistants.” Thus we’re likely heading for some comical train wrecks as companies that don’t properly vet this stuff set themselves up for some expensive disasters (eg think the AI assistant doing things that will get the company into trouble).</p><p>I’m bullish on the tech, but bearish on the ability of folks to deploy it at scale without making a big expensive mess.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886557"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886557" href="https://news.ycombinator.com/vote?id=39886557&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Extrapolating on your legal comment, copyright is the current issue.<p>Right now the USCO says if you use a large model that used copyrighted material to train it, you cannot copyright the generated art. This applies to not just art but might be for code as well. So I wonder what the legal liabilities are say for a publicly traded company to use Midjourney to generate content that is also copyrighted.</p><p>ex) it is possible to generate movie characters that we instantly recognize if you use non-english words which pretty much nail in the coffin for safe harbour status granted under DMCA
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886647"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886647" href="https://news.ycombinator.com/vote?id=39886647&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; Right now the USCO says if you use a large model that used copyrighted material to train it, you cannot copyright the generated art.<p>Unless the Copyright Office has come out with a newer rule, it says if you use a model that does the usual creative parts of making the output you can’t copyright the output, because it is not a work of human authorship. (And that if a work is mixed AI/human, the AI use must be disclosed in copyright registration to avoid copyright protection being applied to elements that are not human work.) “Large model” and “copyrighted material used to train it” are not the issues.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39887936"><td></td></tr>
                  <tr id="39886596"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886596" href="https://news.ycombinator.com/vote?id=39886596&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>AI agents suck. It's too early.<p>The first dominoes to fall will be art, music, and film. These don't require perfection and can be iterated on by the creator. The tools become a method in the process.</p><p>Everyone in the agent space is bumping around until the next big innovation that actually unlocks the technique sparks a race to productize. Maybe some will get lucky and have a fast pivot. Or maybe they'll run out of funds before the big breakthrough.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39887079"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39887079" href="https://news.ycombinator.com/vote?id=39887079&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Talented artists see their creative output drown under a torrent of commercial and mass-produced art. There are dominoes to fell but we should not be reckless about it.<p>With postmodernism art deconstructed its duty and without duty society does not grant rights. It is a big problem because art is often visionary about the future. We don’t know that we even have a future without vision.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39887627"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39887627" href="https://news.ycombinator.com/vote?id=39887627&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; Talented artists see their creative output drown under a torrent of commercial and mass-produced art.<p>There isn't enough art in the world. I've spent the last two decades making films the photons-on-expensive-glass method, and it's a pain in the ass.</p><p>I can start new software projects of scale easily. I can't do that with art. It's capital intensive, logistics intensive, and requires too many people in low-autonomy roles.</p><p>It really sucks the fun out of storytelling when you're chasing down location rights, showing up at the prop house at 7 AM, arranging for catering, etc.</p><p>&gt; We don’t know that we even have a future without vision.</p><p>As a creator, I've dreamed about a better way before GenAI was even on the radar. I'm not the only one. Existing processes suck and too much of the work isn't creative at all.</p><p>As a consumer, my needs are barely being met at all. I want a show about steampunk vampires in space. I want a biopic film exploring Reimann, but told as a musical. I have creative notes for Benioff and Weiss, and I'd rather put those together for myself and my friends than echo words into the void.</p><p>I want so much more than the canned limited selection I have available on Netflix and Criterion. It barely whets the palate, and my appetite is completely unsated. The closest I've ever gotten is performing in improv theater and exploring the worlds I want with the people I feel comfortable creating around. But that's only part of the experience I want. I want so much more. The art we have today is a pale shadow of the mind's unlimited canvas. A projection onto a caveman's wall.</p><p>You can protect a vision of a priestly class of artists from the printing press all you want. I'm tired of living in the dark ages when we're sitting at the precipice of so much more. Their jobs won't go away any more than wedding and event photographers suffer in the advent of digital film.</p><p>If anything, the artists will be the best primed to take advantage of the new alpha. Free of studio meddling, they can build their own audiences that they own without the chains of brand guidelines. Vivziepop, but a million fold.</p><p>I saw a figure recently that said 80% of consumer film, games, and media originates in the US. Think about the rest of the world. So much culture and perspective that we should all share in remains unseen, and we're left with the lens of US media giants.</p><p>Think, too, of all the dreamers lost in opportunity cost. There are a billion stories that die silently in the minds of dreamers because we couldn't help them. And the world is all the worse for it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39891526"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39891526" href="https://news.ycombinator.com/vote?id=39891526&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I very much agree on but one crucial point. In addition to individualist fulfillment I crave collectivist fulfillment and for that to happen we have to assign duties to art.<p>If a collective shoulders that duty then we do get a printing press priesthood. If individualists shoulder the duty to the collective then we should get something different. Perhaps productions like Helluva Boss do have some kind of post-deconstructionist duty of their own design. If that is the case then society should reciprocate with granting rights different from what the priesthood would accrue.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39886861"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886861" href="https://news.ycombinator.com/vote?id=39886861&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; AI agents suck.<p>The idea does not suck. On paper it's great. Just tell an "agent" what
you want and off it goes to get it. And it's possible. LLMs open the
interface to search planning and selection algorithms that have been
around for 40 years and are mature. You could have this tomorrow.</p><p>The assumption is that people want it.</p><p>Tech business has come a long way exploiting people's vices,
specifically laziness of thought we call "convenience". But at heart,
tech is still seen as a tool, to empower people, to give them
<i>agency</i>.</p><p>Agents subtract <i>agency</i>.</p><p>&gt; It's too early.... the big breakthrough.</p><p>Hoping for progress against human psychology seems a fool's errand.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886939"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39886939" href="https://news.ycombinator.com/vote?id=39886939&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>This is wrong. You can't have this tomorrow. The LLMs make too many errors right now for most use cases. If you think it's possible right now, you haven't tried to build it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886640"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886640" href="https://news.ycombinator.com/vote?id=39886640&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>right now the expected future cashflow from whoever "wins" is infinite justifying astronomical amount of capital expenditure. ex) Microsoft's $100 billion supercomputer<p>Sometimes I get they are already sitting on some ground breaking stuff and slowly releasing it to test our responses and get feedbacks.</p><p>I'm certain they will be reading this thread but if one theme repeats itself is the lack of trust in the output of the agents and the companies creating it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39887568"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39887568" href="https://news.ycombinator.com/vote?id=39887568&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; right now the expected future cashflow from whoever "wins" is infinite justifying astronomical amount of capital expenditure. ex) Microsoft's $100 billion supercomputer<p>Absolutely. These agent startups don't have PMF and haven't solved anything yet. They're playing in the kiddie pool while Microsoft is placing chess pieces with a GDP-level moat (which is frankly terrifying).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="39886481"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886481" href="https://news.ycombinator.com/vote?id=39886481&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I built an AI-agents tech demo[1], and am now pivoting. A few thoughts:<p>* I was able to make a simple AI agent that could control my Spotify account, and make playlists based on its world knowledge (rather than Spotify recommendation algos), which was really cool. I used it pretty frequently to guide Spotify into my music tastes, and would say I got value out of it.</p><p>* GPT-4 worked quite well actually, GPT-3.5 worked maybe 80% of the time. Mixtral did not work at all, aside from needing hacks/workarounds to get function-calling working in the first place.</p><p>* It was very slow and VERY expensive. Needing CoT was a limitation. Could easily rack up $30/day just testing it.</p><p>My overall takeaway: it's too early: too expensive, too slow, too unreliable. Unless you somehow have a breakthrough with a custom model.</p><p>From the marketing side, people just don't "get it." I've since niched down, and it's very, very promising from a business perspective.</p><p>[1] <a href="https://konos.ai/" rel="nofollow">https://konos.ai</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886617"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886617" href="https://news.ycombinator.com/vote?id=39886617&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I think it's destined to fail because it basically moved AI back into the "rules based" realm. Deep learning is a decent cognitive interface - like making a guess at some structure out of non-structure. That's where the magic happens. But when you take that and start using rules to chain it together, you're basically back to the same idea as parsing semi-structured data with regex and/or if statements. You can get it to work a bit but edge cases keep coming along that kill you, and your rules will never keep up. For simple cognitive tasks, deep learning figures out enough of the edge cases to work pretty well, but that's gone once you start making rules for how to combine predictions.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887948"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39887948" href="https://news.ycombinator.com/vote?id=39887948&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I totally agree with this. I have been arguing with folks that current Reactflow based agent workflow tools are destined to fail, and more importantly, missing the point. Stop forcing AI into structured work.<p>I do think AI "agents" (or blocks as I like to think of them) unlock the potential for solving unstructured but well-scoped tasks. But it is a block of unstructured work that is very unique to a problem, and you are very likely to not find another problem where that block fits. So, trying to productize these AI blocks as re-usable agents is not that great of a value prop. And building a node based workflow tool is even less of a value prop.</p><p>However, if you can flip it inside out and build an AI agent that takes a question and outputs a node based workflow. But the blocks in the workflow are structured pre-defined blocks with deterministic inputs and outputs, or a custom AI block that you yourself built, then that is something I can find value in. This is almost like the function calling capabilities of GPT.</p><p>Building these block reminds me of the early days of cloud computing. Back then the patterns for high availability were not well-established and people that were sold on the scalability aspects of cloud computing and got onboard without accounting for service failure/availability scenarios and the ephemeral nature of EC2 instances were left burned, complaining about the unfeasibility of cloud computing.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886568"><td></td></tr>
                <tr id="39887303"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39887303" href="https://news.ycombinator.com/vote?id=39887303&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Chain of thought takes time to generate all the characters. If you do a chain-of-thought for every action and every misstep (and you need to for quality + reliability), it adds up.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887414"><td></td></tr>
                <tr id="39889728"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39889728" href="https://news.ycombinator.com/vote?id=39889728&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>There’s caching but only so much can be cached when small changes in the input can lead to an entirely different space of outputs. Furthermore, even with caching LLM inference can take anywhere from 1-15s using GPT4-Turbo via the API. As was mentioned, the more characters you prefix in the context - the longer this takes. Similarly you have a variable length output from model (up to a fixed context length) and so the time it takes to calculate the “answer” can also take awhile. In particular with CoT you are basically forcing the model to use more characters than it otherwise would (in its answer) by asking it to explain itself in a verbose step by step manner.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39886622"><td></td></tr>
                <tr id="39886652"><td></td></tr>
                <tr id="39890064"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39890064" href="https://news.ycombinator.com/vote?id=39890064&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>we only send 0.5-5% of traffic to gpt4, thanks to smaller faster cheaper models. So not all of our traffic is hit with 50s latencies :-/</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39889325"><td></td></tr>
                              <tr id="39887598"><td></td></tr>
                <tr id="39889354"><td></td></tr>
                        <tr id="39886361"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886361" href="https://news.ycombinator.com/vote?id=39886361&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Apparently Pieter Levels:
"
 Interior AI now has &gt;99% profit margins<p>- GPU bill is $200/month for 21,000 designs per month or about 1¢ per render (no character training like Photo AI helps costs)
- Hosted on a shared VPS with my other sites @ $500/mo, but % wise Interior AI is ~$50 of that</p><p>+= $250/month in costs</p><p>It makes about $45,000 in MRR and so $44,730 is pure profits! It is 100% ran by AI robots, no people</p><p>I lead the robots and do product dev but only when necessary"</p><p><a href="https://twitter.com/levelsio/status/1773443837320380759" rel="nofollow">https://twitter.com/levelsio/status/1773443837320380759</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886701"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886701" href="https://news.ycombinator.com/vote?id=39886701&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>This guy makes money by selling "how to get rich using AI" courses and marketing himself on social media (which he is phenomenal at). I'm not really inclined to believe his sales numbers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39886908"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886908" href="https://news.ycombinator.com/vote?id=39886908&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>He is NOT selling any courses. Can you please point me to any of his courses? He has a book he wrote about making software/projects called Make. This book is several years old and doesn't mention AI.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886531"><td></td></tr>
                <tr id="39886597"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886597" href="https://news.ycombinator.com/vote?id=39886597&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; $45,000 in MRR and so $44,730<p>I’ve found a lot of these numbers from people selling passive income methods are extrapolated.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886638"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886638" href="https://news.ycombinator.com/vote?id=39886638&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Levels has been one of the most open entrepreneurs out there. 
I'd be surprised if he lied on revenue<p>All of that just to sell a book?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886888"><td></td></tr>
                        <tr id="39886448"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886448" href="https://news.ycombinator.com/vote?id=39886448&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>And yet "Unfortunately, we cannot offer refunds as costs incurred for generating AI images are extremely high."</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39886998"><td></td></tr>
            <tr id="39886514"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886514" href="https://news.ycombinator.com/vote?id=39886514&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Yeah, he claims because at any point in time there are people redesigning their interiors. I'd say that at any point in time there are people you can convince to give you their money, and if you don't offer refunds, it's not far from a scam.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39890846"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39890846" href="https://news.ycombinator.com/vote?id=39890846&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>most other services (Stripe, ChatGPT, Google Workspace) also don't seem to offer refunds?<p>And neither do most restaurants either; what's to prevent someone from getting a service and then a full refund?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39886440"><td></td></tr>
                <tr id="39886545"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886545" href="https://news.ycombinator.com/vote?id=39886545&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>His robots are just automated scripts which do the bulk repetitive tasks unlike the agents that comment OP thought.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39888637"><td></td></tr>
                  <tr id="39886425"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886425" href="https://news.ycombinator.com/vote?id=39886425&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I get a lot of value out of Copilot and GPT4 for coding, but that's about it.<p>It's true that have to wrestle a lot with them to get them to do what I want for more complex tasks... so they are great for certain tasks and terrible for others, but when I'm in Xcode, I dearly miss vscode because of Copilot autocomplete, which I guess is an indication that it adds <i>some</i> value</p><p>One unexpected synergy has been how good GPT4 is at explaining why my rust code is so bad, thanks to the very verbose compiler messages and availability of high quality training data (i.e. the great rust code in the wild)—despite GPT4 not always being great at writing <i>new</i> rust code from a blank file.</p><p>Part of me thinks in the future this loop is going to be a bit more automated, with an LLM in the mix... similar to how LSPs are "obvious" and ubiquitous these days</p><p>On an unrelated note, I also wrote a small python script for translating my Xcode project's localizable strings into ~10 different languages with some carefully constructed instructions and error checking (basically some simple JSON validation before OpenAI offered JSON as a response type). I only speak ~2 of the target languages, and only 1 natively, but from a quick review the translations seemed mostly fine. Definitely a solid starting point
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886451"><td></td></tr>
                  <tr id="39886661"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886661" href="https://news.ycombinator.com/vote?id=39886661&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Most of the application right now is for purposes for which quality isn't a high priority.  (Also, plagiarism laundering.)<p><i>Don't</i> put it in charge of paying bills.</p><p><i>Do</i> put it in charge of making SEO content sites, conducting mass scam automated interactions, generating bulk code where company tolerates incompetence, making stock art for blog posts that don't need to look professional, handling customer service for accounts you don't care about, etc.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39890564"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39890564" href="https://news.ycombinator.com/vote?id=39890564&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I've been playing with AI agents for months, and most of them are pretty bad. They often get stuck in loops, which is frustrating. This happens in MultiOn, AutoGPT, and others.<p>I've used Devin a few times (see: <a href="https://x.com/varunshenoy/_/status/1767591341289250961?s=20" rel="nofollow">https://x.com/varunshenoy\_/status/1767591341289250961?s=20</a>), and while it's far from perfect, it's by far the best I've seen. It doesn't get stuck in loops, and it keeps trying new things until it succeeds. Devin feels like a fairly competent high school intern.</p><p>Interestingly, Devin seems better suited as an entry-level analyst than a software engineer. We've been using it internally to scrape and structure real estate listings. Their stack for web RPA and browser automation works _really_ well. And it makes sense why this is important: if you want to have a successful agent, you need to provide it with good tools. Again, it's not flawless, but it gives me hope for the future of AI agents.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39892108"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39892108" href="https://news.ycombinator.com/vote?id=39892108&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>We've built a low-code AI agent platform with primary use case in e-commerce (replacing first-line of humans for basic things like product search, QA, etc). It works fairly well if you assemble the script correctly. And if it fails - it just falls back to humans, so customers don't see much difference in their experience.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886353"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886353" href="https://news.ycombinator.com/vote?id=39886353&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Aren’t agents bottlenecked by the underlying models? I’ve read that the number of “chain of thought” steps needed is proportional to task complexity. And if each step has the same probability p of success, probability of success is p^n, where n is the number of steps needed (potentially high). At a 99% success rate per step and 5 steps that’s a 95% overall success rate. 90% drops down to 60%. Not sure what the real numbers are but this seems like it could be a problem without significantly more intelligent ML models?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39886398"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886398" href="https://news.ycombinator.com/vote?id=39886398&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Error-checking and recovery <i>is</i> a potential solution here. Not a well understood one, and might still need higher intelligence than we've got, but-<p>If your math worked out, then humans couldn't work either.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886482"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886482" href="https://news.ycombinator.com/vote?id=39886482&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>If you have a "super-agent" AI that is capable of recovering a business process from an error state, why not just use that agent in the first place?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39886558"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39886558" href="https://news.ycombinator.com/vote?id=39886558&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>You don't need a super agent, you just need two LLM-based systems with errors that aren't too correlated.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886985"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39886985" href="https://news.ycombinator.com/vote?id=39886985&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Chat gpt has often given me the right answer for code after seeing the error trace resulting from its previous attempt.<p>I also often correct my own mistakes based on clashes with reality - I don't just become more intelligent the second time.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39888060"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39888060" href="https://news.ycombinator.com/vote?id=39888060&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I would argue that you are! You will not try to clash with reality the same way you did before, provided you “remember” and I believe future agents/models will have this kind of contextual memory continuously being getting baked in to improve..just a thought.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39888524"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_39888524" href="https://news.ycombinator.com/vote?id=39888524&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I think you could do this with an open model with overnight tuning on the day's errors. Probably very expensive though. Easier to scoop up all the errors on the internet on the first round of pre-training.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39889421"><td></td></tr>
                                          <tr id="39889866"><td></td></tr>
                  <tr id="39886619"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886619" href="https://news.ycombinator.com/vote?id=39886619&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>50 comments so far with 4 about non-agent codegen and rest confirming OPs observations.<p>I'm seeing also an explosion in the number of comments advertising their AI tool on anything remotely related to AI topics. Makes me think we are headed for a major correction.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886890"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886890" href="https://news.ycombinator.com/vote?id=39886890&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Since your definition seems to be a sharp enough razor to triage 50 comments and I'm not read-in enough to tell from the OP, can you help elucidate? :)<p>Are we talking about work on so-called intelligent agents (something with a primitive OODA loop?), a specific ~pattern of conversational AI chatbot, or something else?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39889790"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39889790" href="https://news.ycombinator.com/vote?id=39889790&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>AutoGPT and the like are essentially what OP was referring to. Basically “Siri, but it works and can actually do most of what you ask because the underlying language model is robust rather than a rules engine” (and friends).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39887530"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39887530" href="https://news.ycombinator.com/vote?id=39887530&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>AI startups are funded by VCs funded by LPs long on Nvidia while simultaneously short on the foudners.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39886510"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886510" href="https://news.ycombinator.com/vote?id=39886510&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>What's an agent based workflow? :)<p>I use LLMs as a glorified search engine. That was better than web search at some point, I'm not sure the publicly available LLMs are that good any more. Gemini seems to be extremely worried to not offend anyone instead of giving me results lately.</p><p>At least it's still useful for 'give me the template code for starting an XXX' ...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886576"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886576" href="https://news.ycombinator.com/vote?id=39886576&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Is it something like this?<p>&gt;You are a &lt;blank assistant&gt;, the user has requested &lt;input&gt;. Here are a list of possible actions and their descriptions. Choose the most appropriate action for the user's request. Parse the following from the request...</p><p>In a loop, and you execute whichever action is selected after?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886706"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886706" href="https://news.ycombinator.com/vote?id=39886706&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I'm working on an agent-based tool for software development. I'm getting quite a lot of value out of it. The intention is to minimize copy-pasting and work on complex, multi-file features that are too large for ChatGPT, Copilot, and other AI development tools I've tried.<p><a href="https://github.com/plandex-ai/plandex">https://github.com/plandex-ai/plandex</a></p><p>It's working quite well though I am still ironing out some kinks (PRs welcome btw).</p><p>I think the key to agents that really work is understanding the limitations of the models and working around them rather than trying to do <i>everything</i> with the LLM.</p><p>In the context of software development, imo we are currently at the stage of developer-AI symbiosis and probably will be for some time. We aren't yet at the stage where it makes sense to try to get an agent to code and debug complex tasks end-to-end. Trying to do this is a recipe for burning lots of tokens and spending more time and than it would take to build the thing yourself. But if you follow the 80/20 rule and get the AI to do the bulk of the work, intervening frequently to keep it on track and then polishing up the final product manually at the end, huge productivity gains are definitely in reach.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886483"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886483" href="https://news.ycombinator.com/vote?id=39886483&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>The term "AI agents" might be a bit overhyped. We're using AI agents for the orchestration of our fully automated web scrapers. But instead of trying to have one large general purpose agent that is hard to control and test, we use many smaller agents that basically just pick the right strategy for a specific sub-task in our workflows. In our case, an agent is a medium-sized LLM prompt that has a) context and b) a set of functions available to call.
For example we use it for:<p>- Navigation: Detect navigation elements and handle actions like pagination or infinite scroll automatically.</p><p>- Network Analysis: Identify desired data within network calls.</p><p>- Data transformation: Clean and map the data into the desired format. Finetuned small and performant LLMs are great at this task with a high reliability.</p><p>The main challenge:</p><p>We quickly realized that doing this for a few data sources with low complexity is one thing, doing it for thousands of websites in a reliable, scalable, and cost-efficient way is a whole different beast.</p><p>The integration of tightly constrained agents with traditional engineering methods effectively solved this issue for us.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886653"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886653" href="https://news.ycombinator.com/vote?id=39886653&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Why using llm to chose a proxy if you can just rotate starting from the cheapest based on not getting 403?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39887725"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887725" href="https://news.ycombinator.com/vote?id=39887725&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>To summarize, agents are (essentially) LLMs in a loop: take actions, think, plan, etc, then repeat.<p>Currently, from what I've seen, current LLMs "diverge" when put into a loop. They seem to reason acceptably in small chunks, but when you string the chunks together, they go off the rails and don't recover.</p><p>Can you slap another layer of LLM on top to explicitly recover? People have tried this, it seems like nobody has figured out the error correction needed to get it to converge well.</p><p>My personal opinion is that this is the measure of whether we have AGI or not. When LLM-in-a-loop converges, self-corrects, etc, then we're there.</p><p>It's likely all current agent code out there is just fine, and when you plug in a smart enough LLM it'll just work.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39888356"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39888356" href="https://news.ycombinator.com/vote?id=39888356&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Lots more good answers in [Ask HN: What have you built with LLMs? | Hacker News](<a href="https://news.ycombinator.com/item?id=39263664">https://news.ycombinator.com/item?id=39263664</a>) too<p>If most people's only experience with AI is the chat.openai.com interface then yeah I can see why it seems like too much hassle to most people. The trick is figure out your long prompts ahead of time, and hardcode each one into a HTTP Request in something else (Tasker, BetterTouchTools, Alfred, Apple Shortcuts, etc). For me, I have dozens of long prompts to do exactly what I want, assigned to wakewords, hotkeys, and trigger words on my mac/watch/phone. Another key thing is I use FAST models, i.e. Groq not GPT-4. Latency makes AI too much hassle. i.e. 1. Instant (&lt;1 second end-to-end) answers in just a few words, to voice questions spoken into my watch any time I hold the side button 2. Summarize long articles and youtube videos before I decide to spend time on them 3. Add quick code snippets in plain english with hotkeys or voice 4. Get the main arguments for and against something just to frame it … stuff like that. If it would make your life easier for an AI to save you 1 second per task, why not.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39889514"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39889514" href="https://news.ycombinator.com/vote?id=39889514&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Interesting. Do you have more technical details on how to build these same tools for myself?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886445"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886445" href="https://news.ycombinator.com/vote?id=39886445&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>The best quote I’ve heard from our clients is “don’t trust AI with anything you wouldn’t trust a high schooler to do.”<p>That line of reasoning has held true across basically every project we’ve touched that tried to incorporate LLMs into a core workflow.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886508"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886508" href="https://news.ycombinator.com/vote?id=39886508&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt;&gt; “don’t trust AI with anything you wouldn’t trust a high schooler to do.”<p>Then they should be great for making fast food, staffing amusement parks, and seasonal farm labor.</p><p>They don't seem to be good for those things either.</p><p>[Edit to add: the value high schoolers bring to jobs through non-cognitive abilities, which AIs lack.]
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886628"><td></td></tr>
                        <tr id="39886852"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886852" href="https://news.ycombinator.com/vote?id=39886852&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Whenever I read about how AI is going to automate art or some other creative job I think about a quote I read (maybe here) that went something like “that which is made without effort is enjoyed without pleasure”.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887938"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39887938" href="https://news.ycombinator.com/vote?id=39887938&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Or likening addon AI capabilities to a supervillain's anti-power: "When everybody is super (artistic, whatever), then no one will be!" - Syndrome, Incredibles.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886676"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886676" href="https://news.ycombinator.com/vote?id=39886676&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Not sure if this would qualify has an "agent", but I developed my own AI personal assistant that runs as a telegram bot. I can use it from everywhere easily, handles my events, reminders, sends me a daily agenda and memorizes useful things for me. I even integrated it with whisper so that I can send a telegram voice message and don't need to write.
From a product/selling perspective, no value at all since I haven't even considered that (I'm building it for myself and my needs). But daily usefulness value? heck yeah!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887184"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39887184" href="https://news.ycombinator.com/vote?id=39887184&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I’ve done something similar with iOS Shortcuts, giving the LLM access to my device calendar and reminders. I’m not sure it fits the definition of “agent” but it’s definitely useful being able to query my calendar using natural language rather than Siri’s rigid commands.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887381"><td></td></tr>
                <tr id="39887961"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39887961" href="https://news.ycombinator.com/vote?id=39887961&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I customized Siri COPILOT which I believe I found on HN. I can’t find a link to it anymore. I customized it with my own prompts and my own “tools” which are just Shortcuts that retrieve data from the native apps.<p>I’m currently reworking the concept with Scriptable as I find it so frustrating to develop anything in Shortcuts.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="39886391"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886391" href="https://news.ycombinator.com/vote?id=39886391&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I don't think ai agents are good enough to replace every job today, but they're starting to nip at the more junior / menial knowledge jobs<p>I've seen a lot of success come from AI sales agents, just doing basic SDR style work</p><p>We're having some success automating manual workflows for companies at Skyvern, but we've only begun to scratch the surface.</p><p>I suspect that this will play out a lot like the iPhone era -- first few years will be a lot of discovery and iteration, then things will kick into superdrive and you'll see major shifts in user behavior
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886941"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886941" href="https://news.ycombinator.com/vote?id=39886941&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; junior / menial knowledge jobs<p>Not junior, unthinking. It's like outsourcing/offshoring. You're getting the equivalent of someone who doesn't care about what they're doing, not somebody inexperienced. I'm not saying it doesn't have a place, just commenting on the framing.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886497"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886497" href="https://news.ycombinator.com/vote?id=39886497&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>&gt; I don't think ai agents are good enough to replace every job today<p>You mean <i>any</i>.</p><p>You can fire a human knowledge worker for not doing their job correctly, but what are you gonna do when you only have LLMs and realize they can't do their job correctly?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886553"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886553" href="https://news.ycombinator.com/vote?id=39886553&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I think there are some they're good enough at today. Auto generating meeting notes + AI context, auto responding / following up to emails, filling out forms (we do that pretty well at Skyvern with high accuracy)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886727"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886727" href="https://news.ycombinator.com/vote?id=39886727&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Fire 8 out of 10 of your knowledge workers and have the other remaining 2 review and fix LLM output.<p>Basically the same what we have now, except the grunt workers will be replaced by the machine.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39886960"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886960" href="https://news.ycombinator.com/vote?id=39886960&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Devin seems like the first able to be commercialized. In my opinion the only way to do it well right now is you need to build your own system, the out of box open source projects are just some foundational work.<p>I actually don’t think we will need agents in the future, I think one model will be able to morph itself or just delegate copies of itself like MoE for actions.</p><p>It just seems extremely unlikely to me foundation models don’t get exponentially smarter over the next few years and can’t do this.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39887125"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887125" href="https://news.ycombinator.com/vote?id=39887125&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>When I hear AI agents, I hear RL (reinforcement learning), not LLMs.  RL may not be having the moment that LLMs are, but the progress in recent years is incredible and they are absolutely solving real world problems.  I was just listening to a podcast about using an RL algorithm to enhance the plasma containment system in a fusion reactor, and the results were incredible.  It quickly learned a policy that was competitive with the existing system that had been hand built over many years at a cost of millions.  It even provided some new insights and surprises.  RL is SOTA in robotics control, and some new algorithms like Dreamer V3 can generalize in realtime without millions of samples.  It's has already grown way beyond solving ATARI games and is in many cases being used to train LLMs and other generative AI.<p>There is a good amount of research going into combining LLMs with RL for decision making, it is a powerful combination.  LLMs help with high level reasoning and goal setting, and of course provide a smooth interface for interacting with humans and with other agents.  LLMs also contain much of the collective knowledge of humanity, which is very useful for training agents to do things.  If you want to train a robot to make a sandwich it's helpful to know things, like what is a sandwich, and that it is necessary to move, get bread, etc.</p><p>These feedback loop LLM agent projects are kind of misguided IMO.  AI agents are real and useful and progressing fast, but we need to combine more tools than just LLM to build effective systems.</p><p>Personally, I am using LLMs quite effectively for ecommerce: classifying messages, drafting responses, handling simple requests like order cancellation.  All kinds of glue stuff that used to be painful is now automated and easy.  I could go on.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39887229"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887229" href="https://news.ycombinator.com/vote?id=39887229&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I made a specific design decision to avoid and minimize agentic behavior in aider. My biggest concerns are that agentic loops are slow and expensive. Even worse, they often "go off the rails" and spend a lot of time and money diligently doing the wrong thing, which you ultimately have to undo/redo.<p>Instead, I've found the "pair programming chat" UX to be the most effective for AI coding. With aider, you collaborate with the AI on coding, asking for a sequence of bite sized changes to your code base. At each step, you can offer direction or corrections if you're unhappy with how the AI interpreted your request. If you need to, you can also just jump in and make certain code changes yourself within your normal IDE/editor. Aider will notice your edits and make sure the AI is aware as your chat continues.</p><p><a href="https://github.com/paul-gauthier/aider">https://github.com/paul-gauthier/aider</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886570"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886570" href="https://news.ycombinator.com/vote?id=39886570&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Agents are largely an attempt take the Human-LLM pair and and use the LLM to replace all the work the human finds trivial but which the LLM is terrible at.<p>Trying to get more inference value per-prompt is a good thing. Starting by trying to get it to do long-chain tasks per-prompt makes no sense.</p><p>I'm a huge fan of LLMs for productivity, but even small tasks often require multiple prompts of build-up or fix-up. We should work toward getting those done in a single prompt more often, then work toward slightly larger tasks etc.</p><p>Plugins and GPTs are both attempts at getting more/better inference per-prompt. There is some progress there, but it's pretty limited. There's also plenty of people building task-specific tools that get better results than someone using the chat interface due a lot of prompt work.</p><p>So there <i>is</i> incremental progress happening, but it's been fairly slow. The fact that it's this much work to get incrementally more inference value per prompt makes it very hard to imagine anyone closing the whole loop immediately with an agent.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886556"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886556" href="https://news.ycombinator.com/vote?id=39886556&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Just like many here have said already, GPT4 is being useful for coding for me. It is an amazing parser, specially, and save me precious time. Of course it's not able to do anything on it's own or without supervision, but is has been better than looking up to examples on Google.<p>I also have been experimenting with it to replace the intention classifier part of Google's dialogflow. We use it at work for our chatbot. Earlier, we used Watson and it was amazing, but became very expensive. Dialogflow is cheap, but it is as innacurate with complex natural language as it is cheap.</p><p>Mixtral (8x7B) has proved extremely accurate in identifying intentions with a consistent JSON output, giving it a short context, so i assume a simple 7B model would do the job. I still don't know if it is financially worth it, but it's something i'm gonna try if i can't fix the dialogflow's intentions. But in no way the model's output would directly interface with a client. That's asking for trouble.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886301"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886301" href="https://news.ycombinator.com/vote?id=39886301&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I’ve found the OpenAI assistants API not really up to snuff in terms of predictable behavior yet.<p>That said, I’m very bullish on agents overall though and expect that once they get their assistants behaving a bit more predictably we will see some cool stuff.</p><p>It’s really quite magical to see one of these think through how to solve a problem, use custom tools that you implement to help solve it, and come to a solution.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886707"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886707" href="https://news.ycombinator.com/vote?id=39886707&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I was hype on them initially, but after a few months of using them, I find that they are only useful for simple questions, and for coding help with syntax for basic things I have forgotten.<p>Anything more complex just turns into an irritating back and forth game that when I finally arrive at the solution, I feel like I wasted my time not getting practical experience, but rather gaming a magic 8 ball into giving me what I wanted.</p><p>It just doesn't feel satisfying to me to use them anymore. I don't deny that they improve my productivity, but its at the cost of enjoying what i do. I was never able to enter that feeling of zen flow, while using LLMs regularly.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39887474"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39887474" href="https://news.ycombinator.com/vote?id=39887474&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>LLM tech is breaking records in popularity because it’s extremely user centric. It reminds us of a friendly teammate that seems seasoned in all areas we talk about with them. It’s a pleasure to talk to them now and then, we enjoy the vast source of information it provides us no matter what the subject is. But then, we ask him to do the work for us…<p>Or imagine having a senior engineer in a subject you are clueless about (e.g. Haskell) and think about how annoying would it be for both of you to communicate out some advanced functionality program.</p><p>Analyzing, researching, learning and making decisions are a crucial part of doing work. LLM apps are useful at another approach for learning and researching, but if you solely rely on them their drawbacks are going to keep you behind.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886466"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886466" href="https://news.ycombinator.com/vote?id=39886466&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Opinions mine based on learning from scratch this space in the last couple months only.<p>I feel like these architectures built on top of last gen LLMs are mostly useless now.</p><p>The current gen jump was significant enough that creating a complex chain of thought with RAG on last gen usually is surpassed by 0 shots on current gen.</p><p>So instead of spending time and money building it it's better to focus on 0-shot and update your models to the latest version.</p><p>Feeding LLM outputs into other LLM inputs IMHO just will increase the bias. Initially I expected to mix and match different models to avoid it but that didn't work as much as I expected.</p><p>It depends a lot on your application honestly.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39886704"><td></td></tr>
                  <tr id="39886338"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886338" href="https://news.ycombinator.com/vote?id=39886338&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>We are doing that internally, so I think it is now more a craft than a "product". For example, we look at lot of specific codebase repositories (e.g. GitHub) and try LLMs over the diff just before and after a security code audit was done.<p>Another one is listening to many social media (e.g. Twitter) posts to sense if there is a business opportunity. SDRs scan the results in an Slack channel manually but based on these signals.</p><p>Finally, this is now a workflow but we did this [1] that is a piece in our work.</p><p>[1] <a href="https://news.ycombinator.com/item?id=39280358">https://news.ycombinator.com/item?id=39280358</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886324"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886324" href="https://news.ycombinator.com/vote?id=39886324&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I haven’t found any useful agent workflows, and I’ve not found a tool that’s more productive for me (doing arch/design/implementation of systems) than just copy and pasting from the Playground.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39887547"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887547" href="https://news.ycombinator.com/vote?id=39887547&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I build a lot of side projects and I have gotten a lot of value from <a href="https://www.goagentic.com/" rel="nofollow">https://www.goagentic.com/</a> to send personalised cold emails at scale. I no longer need to spend time researching the prospects as the tool researches every prospect, crafts a personalised message based on what I am selling and send the emails. So far with a 2-5% positive reply rate.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886933"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886933" href="https://news.ycombinator.com/vote?id=39886933&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Seems like we are still in the sheepdog phase where AI agents are extremely capable but not really autonomous helpers that still need overall coordination and control. Logical extension is layers upon layers so the next stage is a shepherd AI and then a farm AI. Then a meta AI that can discern and separate the layers and implement each and combine. May develop like a film studio with area experts combined for a particular project rather than a static one structure fulfils all approach.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39887052"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887052" href="https://news.ycombinator.com/vote?id=39887052&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Devin is the only one that I've been able to use. Set up some projects for me, added some features. Could improve, obviously, but net positive for me in terms of time</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886934"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886934" href="https://news.ycombinator.com/vote?id=39886934&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>We use it for manual research. Think of the times when you visit a certain prospect's website or company website to detect a certain information or to find a hook to talk to them about.<p>We use agents in workflow to be able to do this in bulk. Problem is it does take a long time but at least it saves time at the end of the day and saves you from manually visiting a list of 100 different domains to see a piece of information
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886428"><td></td></tr>
            <tr id="39886469"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886469" href="https://news.ycombinator.com/vote?id=39886469&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I just (as in, five minutes ago) hooked GPT 4 up to my 3D printer and it's fantastic, I use an ESP32 Box and I can ask it what files I have on my printer, I can ask it to print a file, I even added calendar integration so it can read me my events and add new ones. I love it.<p>All that's left is for someone to bundle it all up into a nice package, and we'll be in the future.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39887374"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39887374" href="https://news.ycombinator.com/vote?id=39887374&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I use a 3D printer all the time, but I almost never print a file more than once. It also takes a couple of seconds to select a file with the built-in interface, and then it usually takes anywhere from 30 minutes to a day or more to print. What's your usage model that putting a GPT4 instance between you and the printer is somehow helping things? This feels like someone saying "Now I can e-mail my kleenex box to see how much kleenex it has."</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887555"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39887555" href="https://news.ycombinator.com/vote?id=39887555&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I had some small prints that took ten minutes each, that I needed a lot of, so I figured it would be nice to talk to my ESP box and have it launch prints.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887999"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39887999" href="https://news.ycombinator.com/vote?id=39887999&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Can you not plate multiple copies of it that print at the same time? Usually you need to remove the finished print, clean the build plate etc. between prints, so longer total print times between 'overhead' periods increases your productivity significantly. It seems like a neat demo to be able to say "Print another one" verbally after spending a few minutes physically doing things to re-prep the machine, but not actually more productive than pressing the 'print again' button that shows up on the menu on mine.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39888053"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39888053" href="https://news.ycombinator.com/vote?id=39888053&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I can, but I didn't know how many I'd need beforehand, and multiple copies increase the chance of failure (if one fails, the whole plate is trash).</span></p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39887493"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887493" href="https://news.ycombinator.com/vote?id=39887493&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I think agents are not a fad, they're here to stay since using an LLM in an agent system is the only way to let it access real world task, which they will end up doing when they're good and reliable enough.<p>That said, I believe the current best models are still not good enough - but let's wait a few months.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886532"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886532" href="https://news.ycombinator.com/vote?id=39886532&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I do. I use assistants as containers for different conversations for my GTM work:
An assistant for marketing and copywriting
An assistant for customer support
An assistant sales conversations.<p>These agents aren't super smart: just few PDFs for context plus a few sentences system prompts.</p><p>I do get what I want in 80% of use cases (not measured, just a feeling).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886547"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886547" href="https://news.ycombinator.com/vote?id=39886547&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Agents and tooling around LLM’s can probably make some small number of applications viable, but for the most part we need better foundation models to deliver on the hype.<p>We’re definitely in the “wait” phase of the wait calculation. Everyone is expecting GPT5/q* to change things but really we won’t know until we see it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886424"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886424" href="https://news.ycombinator.com/vote?id=39886424&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I personally use an AI FAQ bot to automate FAQ questions in some of my Discord servers.
It doesn't always work as well as a human answering but it does help, in most cases.
In other words, AI Agents can be very helpful but can only be trusted/useful to a limited extent compared to humans.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886798"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886798" href="https://news.ycombinator.com/vote?id=39886798&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>NVIDIA and one step down the chain OpenAI are making lots of money, mostly by convincing people that LLMs solve all problems.<p>LLMs are perfect for this, super flashy, with a ton of hype. In reality, LLMs are really bad at most applications, they are a solution in search of a problem.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886479"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886479" href="https://news.ycombinator.com/vote?id=39886479&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Some. We are building some new processes from the ground up and will use Agents as a first draft contributor. This is typically where we find the most slowdown. And we will consistently search for the word "delve" as a misspelling :)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39887131"><td></td></tr>
            <tr id="39886408"><td></td></tr>
                <tr id="39887475"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39887475" href="https://news.ycombinator.com/vote?id=39887475&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>In itself, that is an interesting statement of fact:  Whatever value AI agents are going to deliver, they aren't delivering yet.  The jury is still out.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886302"><td></td></tr>
                <tr id="39886411"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886411" href="https://news.ycombinator.com/vote?id=39886411&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>Not sure if this is sarcasm, but the thread was only posted 20 minutes ago and has 9 replies already. I personally am tired of AI/LLM news but it still seems popular from this thread.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886407"><td></td></tr>
                <tr id="39886719"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39886719" href="https://news.ycombinator.com/vote?id=39886719&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I'm also noting this but could be because its Sunday?<p>or was GP implying lot of people are gettin funded to build agents in a thread where the consensus is they dont work well enough for people to pay for
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39886354"><td></td></tr>
                <tr id="39886386"><td></td></tr>
                <tr id="39886981"><td></td></tr>
                  <tr id="39886444"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886444" href="https://news.ycombinator.com/vote?id=39886444&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>...which is not necessarily evidence that Klarna has obtained value from OpenAI Assistants API. It's quite possible (likely, even) that this has effectively been a complete removal of support.<p>I haven't used Klarna, but a few other products I've used are unsupported now because human support were replaced with AI "support" that is completely useless.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886739"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886739" href="https://news.ycombinator.com/vote?id=39886739&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>One of the hardest lessons I learned in tech, maybe in life, was that
if people don't want it you're done.<p>It doesn't matter that you think it's the coolest and most amazing
technology in history. It may be. So what?</p><p>It doesn't matter that experts from every part of industry are yelling
that "this is the future", that the march of this tech is
"inevitable". They need to believe that, for their own reasons.</p><p>It doesn't matter that academics from Yale, Harvard and MIT are
publishing a dozen new papers on it every week. For the mostpart their
horizon ends at the campus gate.</p><p>It doesn't matter that investors are clamouring to give you money and
inviting you to soirees to woo you because your project has the latest
buzzwords in the name. Investors have to invest in something.</p><p>And it doesn't matter if market research people are telling you that
the latent demand and growth opportunity is huge. People tell them
what they want to hear.</p><p>The real test - and I wish I had known this when I was twenty - is do
ordinary people on the London Omnibus want it? Not my inner ego
projection. Not my wishful thinking. Not what "the numbers" say. Go
and ask them.</p><p>My experience right now - from asking people (for a show I make) is
that people are shit scared of AI and if they don't hold a visceral
distaste for it they've an ambivalence that's about as stable
nitro-glycerine on a hot day. I know that may be a difficult thing to
hear as a business person.</p><p>If you are harbouring in your heart any remnant of the idea that you
can create demand, that they will "see the light" and once they have a
taste will be back for more, or that by will and power they can be
<i>made</i>, regulated and peer pressured into accepting your "vision",
then you'd be wise to gently let go of those thoughts.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886392"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886392" href="https://news.ycombinator.com/vote?id=39886392&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I've been a bit disappointed by the AI. I'll admit going in with low expectations (I know about the whole AI summer/winter cycle) and I was blown away that ChatGPT could play Jeopardy! with just a prompt since I remember being blown away by Watson and AlphaGo. But then I had it help me write a letter, and by the time I got it to do anything useful, I basically had to write an outline for it, and then I realized I had already done the hard part. I asked it to write some boilerplate code for an interface to the Slack API in Python, but it used a deprecated API, and it assumed I had a valid token. Turns out Slack has lots of different kinds of tokens and I was using the wrong one, and the AI couldn't help me figure that out. After that, I remembered the story about pain point for radiologists. They don't need help diagnosing cancer, they need help with their internet connectivity.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886565"><td></td></tr>
            <tr id="39886825"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886825" href="https://news.ycombinator.com/vote?id=39886825&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>When I was technical blogging on how to learn from open-source code [1], I used it quite frequently to get unstuck and/or to figure out how to tease apart a large question into multiple smaller functions.  For example, I had no idea how to break up this long `sed` command [2] into its constituent parts, so I plugged it into ChatGPT and asked it to break down the code for me.  I then Googled the different parts to confirm that ChatGPT wasn't leading me astray.<p>If I had asked StackOverflow the same question, it would have been quickly closed as being not broadly applicable enough (since this `sed` command is quite specific to its use case).  After ChatGPT broke the code apart for me, I was able to ask StackOverflow a series of more discrete, more broadly-applicable questions and get a human answer.</p><p>TL;DR- I quite like ChatGPT as a search engine when "you don't know what you don't know", and getting unblocked means being pointed in the right direction.</p><p>1. <a href="https://www.richie.codes/shell" rel="nofollow">https://www.richie.codes/shell</a></p><p>2. <a href="https://github.com/rbenv/rbenv/blob/e8b7a27ee67a5751b899215b4d35fd86ab552dae/libexec/rbenv-versions#L60">https://github.com/rbenv/rbenv/blob/e8b7a27ee67a5751b899215b...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39887181"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39887181" href="https://news.ycombinator.com/vote?id=39887181&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>This has been my experience as well.  I use Phind as a search engine, it's pretty bad for anything else.  It does excel at obvious JS questions, but you can get those anywhere.  It's great at sussing out a function that you only half-remember.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39886528"><td></td></tr>
            <tr id="39886962"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886962" href="https://news.ycombinator.com/vote?id=39886962&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>the use case i "feel" these are useful are for studying any given topic. having one single page helps avoid many google searches, tangential questions, etc., but i'm always looking out for inaccuracies.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886418"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886418" href="https://news.ycombinator.com/vote?id=39886418&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I've been disappointed by my few experiments with Langchain's agent tooling. Things I have experienced:<p>- The pythonrepl or llm-math agent not being used when it should be and the agent returning a wrong or approximate answer.</p><p>- The wikipedia and webbrowsed agents doing spurious research in an attempt to answer a question I did not ask (hallucinating a question, essentially).</p><p>- Agents getting stuck in a loop of asking the same question over and over until they time out.</p><p>- The model not believing an answer it gets from an agent (eg using a Python function to get today's date and not believing the answer because "The date is in the future").</p><p>When you layer all this on top of the usual challenges of writing prompts (plus, with Python function, writing the docstring so the agent knows when to call it), wrong answers, hallucination, etc, etc, I'm unconvinced. But maybe I'm doing it wrong!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39887278"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39887278" href="https://news.ycombinator.com/vote?id=39887278&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>Full disclaimer up top: I have been working on agents for about a year now building what would eventually become HDR [1][2].<p>The first issue is that agents have extremely high failure rates. Agents really don't have the capacity to learn from either success or failure since their internal state is fixed after training. If you ask an agent to repeatedly do some task it has a chance of failing every single time. We have been able to largely mitigate this by modeling agentic software as a state machine. At every step we have the model choose the inputs to the state machine and then we record them. We then 'compile' the resulting state-transition table down into a program that we can executed deterministically. This isn't totally fool proof since the world state can change between program runs, so we have methods that allow the LLM to make slight modifications to the program as needed. The idea here is that agents should never have to solve the same problem twice. The cool thing about this approach is that smarter models make the entire system work better. If you have a particularly complex task, you can call out to gp4-turbo or claude3-opus to map out the correct action sequence and then fall back to less complex models like mistral 7b.</p><p>The second issue is that almost all software is designed for people, not LLMs. What is intuitive for human users may not be intuitive for non-human users. We're focused on making agents reliably interact with the internet so I'll use web pages as an example. Web pages contain tons of visually encoded information in things like the layout hierarchy, images, etc. But most LLMs rely on purely text inputs. You can try exposing the underling HTML or the DOM to the model, but this doesn't work so well in practice. We get around this by treating LLMs as if they were visually impaired users. We give them a purely text interface by using ARIA trees. This interface is much more compact than either the DOM or HTML so responses come back faster and cost way less.</p><p>The third issue I see with people building agents is they go after the wrong class of problem. I meet a lot of people who want to use agents for big ticket items such as planning an entire trip + doing all the booking. The cost of a trip can run into the thousands of dollars and be a nightmare to undo if something goes wrong. You really don't want to throw agents at this kind of problem, at least not yet, because the downside to failure is so high. Users generally want expensive things to be done well and agents can't do that yet.</p><p>However there are a ton of things I would like someone to do for me that would cost less than five dollars of someones time and the stakes for things going wrong are low. My go to example is making reservations. I really don't want to spend the time sorting through the hundreds of nearby restaurants. I just want to give something the general parameters of what I'm looking for and have reservations show up in my inbox. These are the kinds of tasks that agents are going to accelerate.</p><p>[1] <a href="https://github.com/hdresearch/hdr-browser">https://github.com/hdresearch/hdr-browser</a>
[2] <a href="https://hdr.is/" rel="nofollow">https://hdr.is</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886535" href="https://news.ycombinator.com/vote?id=39886535&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>I use one that reads my RSS feeds writes a radio DJ voice over and uses an elevenlabs API call to generate the voice (their Santa voice from last year works really well). Combines it with one of my Spotify playlists and gives me a 45 minutes radio show for my commute... pretty much changed how I consume news and content like hn.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886881"><td></td></tr>
            <tr id="39886815"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886815" href="https://news.ycombinator.com/vote?id=39886815&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>The better question is: is the "value" worth the enormous environmental cost from the massive amounts of power used in training datasets, storing them, and running the GPUs/TPUs?<p>NVIDIA estimates they'll ship up to 2M H100 GPUs. They have a TDP of about 300-400W each. Assume that because of their high cost, their utilization is very high. Assume another 2/3rds of that is used for cooling, which would be another 200W. Be generous and throw out all the overhead from the host computers, storage, power distribution, UPS systems, and networking.</p><p>2M * 600W = 1.2GW.</p><p>Let's say you only operated them during the daytime and wanted to do so from solar power. You'd need between ten and twenty square miles worth of solar panels to do so.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886620"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886620" href="https://news.ycombinator.com/vote?id=39886620&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>It's a bit like with the chatbots revolution from 8-10 years ago (not LLM, just make a choice and maybe parse a few keywords to navigate a chatbot state machine)<p>Sure, we can do that, but do users want that?</p><p>I don't want to chat, talk or interact with people, I want the most efficient ui possible for the task at hand.
When I do chat with someone is because some businesses are crap at automating and I need a human to fix something.
Even then I don't want a robot that can't do anything.</p><p>The only exception I can think of is tutoring but then I'd really question the validity of the answers. RAG is pretty cool in that regard because it can point at the original paragraph being used to answer the question.</p><p>That might be useful to someone but that's not my favourite way of learning.</p><p>Give me a summary of the content, give me the content, Ctrl+F and I'm good to go.</p><p>For low stakes things like gaming where the agent messing up would just be a fun bug, I think it can be great.</p><p>Looking forward to automatically generated side quests based on actions and npc which get pissed if I put a box on their head and hire mercenaries if I murder their families.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886507"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886507" href="https://news.ycombinator.com/vote?id=39886507&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I've found that while agents cannot replace anyone, they can sure help with the use of various things.<p>First, we know these AIs are trained with data from the general Internet, and that data is vast.</p><p>Second, the general Internet contains owner manuals and support forums for practically every active product there is, globally. These are every possible product too: physical products, virtual products like software or music, and experience products like travel or education. Between the owner’s manuals and the support forums for these products there is extremely deep knowledge about the purpose, use and troubleshooting of these products.</p><p>Third, one cannot just ask an LLM direct deep questions about some random product and expect a deep knowledge answer. One has to first create the context within the LLM that activates the area(s) of deep knowledge you want your answers to arise. This requires the use of long form prompts that create the expert you want, and once that expert is active in the LLM’s context, then you ask it questions and receive the deep knowledge answers desired.</p><p>Fourth, one can create an LLM agent that helps a person create the LLM agent they want, the LLM agent can help generate new agents, and dependency chains between different agents are not difficult at all, including information exchange between groups of agents collaborating on shared replies to requests.</p><p>And last, all that deep information about using pretty much every software there is can be tapped with careful prompting to create the context of an expert user of that software, and experts such as these can become plugins and drivers for that software. It's at our finger tips...!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886612"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886612" href="https://news.ycombinator.com/vote?id=39886612&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>I am!<p>In my experience, you need to keep a human in the loop. This implies that you can't get the technology to scale, but I'm optimistic because LLMs have rapidly gotten better at following directions while I've been using them over the last six months.</p><p>Summarization is probably the clearest strength of LLMs over a human. With ever-growing context windows, summarizing books in one shot becomes feasible. Most books can be summarized in one sentence, though the most useful, information-dense ones cannot.</p><p>I had Gemini 1.5 Pro summarize an old book titled Natural Hormonal Enhancement yesterday. Having just read the book, the result was acceptable.</p><p><a href="https://hawleypeters.com/summary-of-natural-hormonal-enhancement/" rel="nofollow">https://hawleypeters.com/summary-of-natural-hormonal-enhance...</a></p><p>For information-dense books, it seems clear to me that chatting with the book is the way to go. I think there's promise to build a competent agent for this kind of use case. Imagine gathering 15 papers and then chatting about their contents with an agent with queries like:</p><p>What's the consensus? 
Where do these papers diverge in their conclusions? 
Please translate this passage into plain English.</p><p>I haven't done this myself, but I have a hard time imagining such an agent being useless. Perhaps this is a failure of imagination on my part.</p><p>The brightest spot in my experimentation is [Cursor](<a href="https://cursor.sh/" rel="nofollow">https://cursor.sh</a>). It's good for little dev tasks like refactoring a small block of code and chatting about how to use vim. I imagine it'd be able to talk about how to set up various configs, particularly if you @ the documentation, a feature that it supports, including [adding documentation](<a href="https://docs.cursor.sh/features/custom-docs" rel="nofollow">https://docs.cursor.sh/features/custom-docs</a>).</p><p>Edit: I think a lot of disappointment comes from these kinds of tools not being AGI, or a replacement for a human that does some repetitive task. They magnify the power of somebody that's already curious and driven. They still empower lazy, disengaged users, but with goals like doing the bare minimum, and avoiding work altogether, these tools cannot help one accomplish much of use.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39888389"><td></td></tr>
                  <tr id="39886422"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886422" href="https://news.ycombinator.com/vote?id=39886422&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>copilot has been useful to me for the boring parts of writing tests, but it needs a lot of review. other than that, dogshit</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39886351"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886351" href="https://news.ycombinator.com/vote?id=39886351&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>At my work, I have colleagues who speak English as a second language. Many of them are using LLMs to up their document and other writing.<p>It’s actually quite awful. It’s obvious the text is LLM generated because of the verbose, generic writing style. It communicates clearly but without substance. Not gonna lie, I secretly judge these people.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39886521"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39886521" href="https://news.ycombinator.com/vote?id=39886521&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>We built a conversion rate optimizing AI Agent and saw about 45% click through rate lift on our own homepage. In other beta testing companies that used it, we saw a similar average and range has been +15%-175%. Agent (AB3.ai) can be tried here: <a href="https://ab3.ai/" rel="nofollow">https://AB3.ai</a></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39886735"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39886735" href="https://news.ycombinator.com/vote?id=39886735&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><br><div>
                  <p><span>you know these driveby comments writing a bit of contextually relevant bit that always ends with a me-too link are getting tiring. To the point where I see it here, I begin to suspect things aren't going well in your other sales/marketing funnels.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39887803"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39887803" href="https://news.ycombinator.com/vote?id=39887803&amp;how=up&amp;goto=item%3Fid%3D39886178"></a></center>    </td><td><p><span>"Ask HN: Is anybody getting value from AI Agents? How so?"<p>If my comment is not on topic to answer OP's thread title question, then I'm not sure what is. The three clicks it will get buried in a HN thread are not going to do anything for us. But I also see little value in stating "Yes, I've gotten 45% of a lift using an AI agent" and not providing context.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can GPT optimize my taxes? An experiment in letting the LLM be the UX (148 pts)]]></title>
            <link>https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/</link>
            <guid>39885107</guid>
            <pubDate>Sun, 31 Mar 2024 15:26:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/">https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=39885107">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    

    
      

    

    

    
<figure>
    
    
    
    
        <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/gilded-robot-accountant.webp" alt="">
        <figcaption><p>Generated using DALL-E.</p></figcaption>
    
</figure>

<p><strong><code>TL;DR</code></strong> We made a GPT interface to an open-source US tax scenarios library,
and it is at times pretty good, but asks a lot of the user.</p>
<hr>
<p>Apparently this blog will be about working out implications of observations by
Andrej Karpathy. This time we’re thinking about <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g&amp;t=2552s">his
comment</a> that LLMs may come
to be thought of as a kind of higher-order operating system: not omnicapable
themselves, but rather the glue that links together other components, such as
data stores, domain-specific libraries and user interfaces. Cam Pedersen wrote a
nice <a href="https://campedersen.com/llm-os/">post</a> spelling out the ideas more fully.</p>
<p>I thought it would be fun to build some concrete data product to explore this
LLM-as-OS idea, and contrast it with the existing state of the art. The idea: a
ways back I’d written a python library called
<a href="https://github.com/mmacpherson/tenforty"><code>tenforty</code></a>, that in turn uses a
wonderful open source package called <a href="https://opentaxsolver.sourceforge.net/">Open Tax
Solver</a>, to explore some tax scenarios
we were dealing with. I’d never written it up, and thought someday I’d make a
webapp from it to share. This seemed like a nice opportunity to try building
something as a newfangled LLM-OS app: an AI tax advisor.</p>
<p><code>tenforty</code> turns tax form calculations into python function calls, which makes
it easy to evaluate one return, or many hypothetical returns. This latter
<em>what-if</em> aspect was the main itch I was aiming to scratch with the library:
What if we sold this stock over two years rather than one? What would happen to
our tax bracket if we maxed out our 401Ks? To answer questions like this in
TurboTax felt Sisyphean; write down tax amount, back-back-back, tweak input,
forward-forward-forward, write down tax amount…<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> (There’s more about
<code>tenforty</code> <em>per se</em> in an <a href="#appendix-about-tenforty">appendix</a>.)</p>
<p>Before LLMs appeared, to build a webapp for these what-ifs, I might have
researched common tax scenarios beyond our own that <code>tenforty</code> could help with,
and built a kind of calculator app to anticipate those cases, perhaps like
<a href="https://smartasset.com/taxes">SmartAsset’s</a> tax overview page.</p>
<p>In 2024, if we let the LLM be the UX, then building the app becomes less, and
different, work. In some sense the user will just bring their own scenario, and
the app doesn’t need to anticipate it. These were the steps:</p>
<ol>
<li>Build a tiny web service that exposes the main functions from <code>tenforty</code> as
endpoints. We used <a href="https://fastapi.tiangolo.com/">FastAPI</a>.</li>
<li>Configure the custom GPT on OpenAI:
<ul>
<li>Come up with a name: <em>Tax Driver</em><sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></li>
<li>Write the GPT’s prompt. We link a copy of it <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-gpt-prompt.txt">here</a>.</li>
<li>Configure a GPT “Action” that provides the details of the endpoints. Since
FastAPI autogenerates OpenAPI specifications
(<a href="https://tax-server.finedataproducts.com/openapi.json">ref</a>), this was
mainly pasting in that spec.</li>
<li>Write a few example prompts to illustrate the kinds of things it can do.</li>
<li>(Revise the GPT prompt to address unexpected behavior… 🤦🏼‍♂️)</li>
</ul>
</li>
</ol>
<p>If you have a paid ChatGPT+ account you can try out Tax Driver <a href="https://chat.openai.com/g/g-jkF9Et8tT-tax-driver">right
here</a>.  The privacy policy is
linked from the GPT, and also
<a href="https://tax-server.finedataproducts.com/privacy">here</a>; the upshot is that the
backing web service is set up as a pure calculator, and logs only which endpoint
was called, and when it was called by OpenAI’s servers. Note that independently
of ChatGPT+, you can also play with <code>tenforty</code> directly using the included
<a href="https://colab.research.google.com/github/mmacpherson/tenforty/blob/main/notebooks/tenforty_Package_Demo.ipynb">Colab
notebook</a>.</p>





<a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-start.png" data-lightbox="gallery" data-title="A robot tax advisor.">
    <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-start.png" alt="A robot tax advisor.">
</a>



<h2 id="what-did-we-learn">What Did We Learn?</h2>
<p>Tax Driver does a bunch of things well:</p>
<ul>
<li>
<p>From minimal input, as in the example screenshot here, it reliably defers to
the Action to evaluate the scenario. We’ve gotten used to GPT-4’s
natural-language abilities over the past year or so, but they really are
stunning. (<em>Note: images enlarge if clicked/tapped.</em>)</p>

  
  
  
  
  <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-short-term-long-term.png" data-lightbox="gallery" data-title="Tax Driver Example: Short-Term v Long-Term Capital Gains">
      <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-short-term-long-term.png" alt="Tax Driver Example: Short-Term v Long-Term Capital Gains">
  </a>
  

</li>
<li>
<p>It is game to at least try to evaluate essentially any tax scenario you might
find yourself faced with, combining its knowledge base with the specific
calculations enabled by <code>tenforty</code>. I would never have been able to match that
breadth with a set of canned scenario calculators. Here’s another example
that’s gone reasonably well, if one thinks of AMT as “the excess tax over the
regular tax due to the AMT calculation” as it’s sometimes <a href="https://www.investopedia.com/terms/a/alternativeminimumtax.asp">discussed
informally</a>.</p>
</li>
</ul>

 
 
 
 
 <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-iso-gain.png" data-lightbox="gallery" data-title="Tax Driver Example: Incentive Stock Option Gains">
     <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-iso-gain.png" alt="Tax Driver Example: Incentive Stock Option Gains">
 </a>
 

<ul>
<li>It can produce tables/graphics that summarize the results well:</li>
</ul>

 
 
 
 
 <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-area-proportion-plot.png" data-lightbox="gallery" data-title="Tax Driver Example: Incentive Stock Option Gains">
     <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-area-proportion-plot.png" alt="Tax Driver Example: Incentive Stock Option Gains">
 </a>
 

<ul>
<li>
<p>If asked, it will reliably produce apt python code using <code>tenforty</code> that you
can drop into a notebook and take from there.</p>

  
  
  
  
  <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-generate-python.png" data-lightbox="gallery" data-title="Tax Driver Example: Write Python Code">
      <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-generate-python.png" alt="Tax Driver Example: Write Python Code">
  </a>
  

</li>
</ul>
<p>Some of those capabilities are simply from the future, and some might have been
achievable by me or a team, only with much more work. And building this “app”
took a tiny fraction of the time it would have taken me to build its pre-LLM
analog.</p>
<p>But it can also be frustrating:</p>
<ul>
<li>
<p>It can just miss the point of a given scenario request, in that cheerfully
confident way GPT-4 has, like a politician not-answering a question at a town
hall. In this example, we’ve asked for it to look at the tax burden if
everything was exercised in one year vs spread out over two or three years,
and Tax Driver misses the mark widely:</p>

  
  
  
  
  <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-missed-the-point.png" data-lightbox="gallery" data-title="Tax Driver Example: Missed The Point">
      <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-missed-the-point.png" alt="Tax Driver Example: Missed The Point">
  </a>
  

</li>
<li>
<p>It has a set of specific instructions to follow from its prompt, and it often
follows them, but also sometimes defies them, especially those having to do
with figure generation. For example, here’s a case in which we’ve explicitly
asked for a <a href="https://en.wikipedia.org/wiki/Area_chart">shaded area chart</a>, in
addition to the instruction to prefer shaded area charts for plots like this
in its <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-gpt-prompt.txt">prompt</a>, and it claims
that it’s drawn one, but hasn’t:</p>

  
  
  
  
  <a href="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-fed-state-range-ny.png" data-lightbox="gallery" data-title="Tax Driver Example: Tax Table Plot">
      <img src="https://finedataproducts.com/posts/2024-03-10-tax-scenarios-with-ai/tax-driver-eg-fed-state-range-ny.png" alt="Tax Driver Example: Tax Table Plot">
  </a>
  

</li>
<li>
<p>It can be erratic. Run the same prompt twice, and once you will get reasonable
output, and the second time it will fail to successfully run an Action or
process the Action’s results. <em>No example shown.</em></p>
</li>
<li>
<p>It occasionally mixes up the Action calls, i.e. web requests, with python
calls to <code>tenforty</code>. <em>No example shown–the output is generated python code,
when it “should” have called out to the Action and done something with the
response.</em></p>
</li>
</ul>
<p>These limitations are familiar to us by now. This lack of meta-cognition is a
big reason why this first generation of LLM-based products – ChatGPT, Stable
Diffusion, GitHub Copilot – are expressed as copilots, as opposed to say
autonomous assistants that can be relied upon to perform a task correctly. We
encountered the same issue in our <a href="https://finedataproducts.com/posts/2023-12-31-llm-based-metaanalysis/">previous
post</a>
about doing scientific literature meta-analysis with LLMs.</p>
<p>More broadly, as easy as it is to create this app, the universal chat interface
is the proverbial jack of all trades, master of none. It places a lot of the
burden on the user to know what they want, write out their requests carefully,
and exercise care in interpreting the results and iterating upon them. Simon
Willison made a connected point in a recent <a href="https://youtu.be/mOzxhcc1I8A?t=1108">podcast with
Outerbounds</a>: the generic chat interface
leaves much to be desired in terms of discoverability, and runs a high risk of
scaring off non-enthusiast prospective users.</p>
<p>The upshot: Tax Driver is a capable if mercurial assistant. At least playing
with our own actual tax scenarios, with a little patience and some coaching, it
produced solid analyses. I found it particularly helpful to ask it to generate
<code>tenforty</code> code that I could play with myself at the end of one of these chats.
That seems useful, compared to having to do that all oneself, but it requires a
fairly tolerant user, and is less surefooted that one ideally would hope for
from a tax advisor.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thanks kindly to Sarah Laskey for improving an earlier draft. All errors are
mine!</p>

<h2 id="appendix-about-tenforty">Appendix: About <code>tenforty</code></h2>
<p><a href="https://opentaxsolver.sourceforge.net/">Open Tax Solver</a> (OTS), the package
that <code>tenforty</code> relies on, is one of these gems of the internet. The OTS team
has published their software for more than twenty consecutive years now. It
provides implementations of the US 1040 form calculations, and the forms for
many states, including California and New York.</p>
<p>You can prepare your tax return either using OTS’ provided GUI, or via a simple
text file format. In either case, OTS outputs the resulting return to another
text file, and can insert the output into the official tax form PDFs. It’s thus
a DIY, open source TurboTax that improves upon doing the calculations by hand.
Very cool.</p>
<p><code>tenforty</code> works by amalgamating the C language source code from OTS releases
into a single big source file, and then wrapping that source file as a cython
extension. Because OTS is fundamentally designed around reading and writing text
files (that contain tax return information), <code>tenforty</code> actually writes and
reads back text files in temporary directories behind the scenes, although only
python dicts and pandas dataframes are presented to the library user.</p>
<p>OTS’s text-file interface works in terms of tax line numbers, e.g. W-2 income
gets reported on Line 1 of the US 1040. Since only accountants and the very
determined will know the line numbers, OTS annotates them with their natural
names like “W-2 Income” in their GUI. To solve the same problem, I added a
higher-level interface to <code>tenforty</code>, that offers natural names for the familiar
tax-form quantities, e.g. <code>w2_income</code>. All available options are listed out in
the repo’s
<a href="https://github.com/mmacpherson/tenforty/blob/main/README.md">README</a>.</p>
<p>This higher-level interface will be limiting for some, because I only did the
fairly common quantities. For example, if you’ve got a small business, there
will be some missing inputs. There are two ways around this:</p>
<ol>
<li>There’s a lower-level interface in <code>tenforty</code> as it stands, where you can
provide any of the line-level inputs as a python dictionary, the return will
be evaluated, and you’ll get back a dictionary with all the line-level
outputs.</li>
<li><code>tenforty</code> is open-source, and people are encouraged to contribute PRs with
their improvements.</li>
</ol>
<p>The document
<a href="https://github.com/mmacpherson/tenforty/blob/main/DEVELOP.md"><code>DEVELOP.md</code></a>
goes into more detail about how the package works.</p>
<h3 id="limitations">Limitations</h3>
<p>Even though OTS goes back to the Bush II administration, I have only included
from 2017 through 2023 tax years into <code>tenforty</code>, and I’ve only wrapped up
California (where we live), Massachusetts, and New York, although OTS supports
more states. More years/states/etc. can be added by PR.</p>
<h3 id="testing">Testing</h3>
<p>There are some basic unit tests in place, and then some property-based tests
using the <a href="https://hypothesis.readthedocs.io/">hypothesis</a> library, that exercise
<code>tenforty</code>/OTS by generating a wide variety of inputs and checking that certain
properties hold, e.g. “If your W-2 income goes up, and nothing else changes,
your total tax must always be the same or higher.”</p>
<p>I further verified that our own externally-prepared returns from the past few
years give the same answers through <code>tenforty</code>. Not a big $n$, but in principle
a larger test set of reference return calculations might be built up.</p>
<p>This is to say, there’s a good-faith effort to test the package, but the testing
game could certainly be improved. PRs welcome. :)</p>


    
      

    

</article></div>]]></description>
        </item>
    </channel>
</rss>