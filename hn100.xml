<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 07 Dec 2023 10:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Building end-to-end security for Messenger (120 pts)]]></title>
            <link>https://engineering.fb.com/2023/12/06/security/building-end-to-end-security-for-messenger/</link>
            <guid>38552789</guid>
            <pubDate>Thu, 07 Dec 2023 04:21:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2023/12/06/security/building-end-to-end-security-for-messenger/">https://engineering.fb.com/2023/12/06/security/building-end-to-end-security-for-messenger/</a>, See on <a href="https://news.ycombinator.com/item?id=38552789">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>We are beginning to upgrade people’s personal conversations on Messenger to use end-to-end encryption (E2EE) by default</span></li>
<li aria-level="1"><span>Meta is publishing two technical white papers on end-to-end encryption:</span>
<ul>
<li aria-level="1"><span>Our <a href="https://engineering.fb.com/wp-content/uploads/2023/12/MessengerEnd-to-EndEncryptionOverview_12-6-2023.pdf" target="_blank" rel="noopener">Messenger end-to-end encryption whitepaper </a>describes the core cryptographic protocol for transmitting messages between clients.</span></li>
<li aria-level="1"><span>The <a href="https://engineering.fb.com/wp-content/uploads/2023/12/TheLabyrinthEncryptedMessageStorageProtocol_12-6-2023.pdf" target="_blank" rel="noopener">Labyrinth encrypted storage protocol whitepaper</a> explains our protocol for end-to-end encrypting stored messaging history between devices on a user’s account.</span></li>
</ul>
</li>
</ul>
<p><span>Today, we’re announcing that we’ve begun to upgrade people’s personal conversations on Messenger to use E2EE by default. Our aim is to ensure that everyone’s personal messages on Messenger can only be accessed by the sender and the intended recipients, and that everyone can be sure the messages they receive are from an authentic sender.</span></p>
<p><span>This is the most significant milestone yet for this project, which began in earnest after </span><a href="https://www.facebook.com/notes/2420600258234172/" target="_blank" rel="noopener"><span>Mark Zuckerberg outlined his vision for it in 2019</span></a><span>. Bringing E2EE to Messenger has been a complex process, with every feature and product goal revealing further challenges that required careful consideration.</span></p>
<p><span>Enabling E2EE on Messenger meant fundamentally rebuilding many aspects of the application its protocols to improve privacy, security, and safety while simultaneously maintaining the features that have made Messenger so popular.&nbsp;</span></p>
<h2><span>Why we’re bringing E2EE to Messenger</span></h2>
<p><span>Messenger first</span> <a href="https://about.fb.com/news/2016/07/messenger-starts-testing-end-to-end-encryption-with-secret-conversations/" target="_blank" rel="noopener"><span>built end-to-end encrypted chats in 2016 </span></a><span>as a feature called Secret Conversations. Since then, we’ve learned a great deal in regards to rolling out E2EE for a wider user base. For example, we recently published an updated</span> <span>white paper, “</span><a href="https://messengernews.fb.com/wp-content/uploads/2021/12/Metas-approach-to-safer-private-messaging-on-Messenger-and-Instagram-DMs-Sep-23.pdf" target="_blank" rel="noopener">Meta’s Approach to Safer Private Messaging on Messenger and Instagram Direct Messaging</a>,” t<span>hat sets out the industry-leading safety systems and tools available on Messenger.</span></p>
<p><span>End-to-end encryption isn’t about the technology at its core. It’s about protecting people’s communications, so they can feel safe expressing themselves with their friends and loved ones. To&nbsp; achieve this, we typically focus on two aims:</span></p>
<ol>
<li aria-level="1"><span>Only the sender and recipients of an E2EE message can see its contents.</span></li>
<li aria-level="1"><span>Nobody (not even Meta) should be able to forge messages to appear to have been sent from someone they weren’t.</span></li>
</ol>
<p><span>In other words, the aim is that only you and the people you’re corresponding with can read your messages – not even the app’s provider (in this case, Meta) could interfere with their contents – and you can be confident in who sent the messages.&nbsp;</span></p>
<h2><span>Understanding these goals</span></h2>
<p><span>These two aims are broad. However, when we reflect on our approach to addressing them, they end up breaking down into eight overlapping concepts that we believe achieve a cohesive approach to meaningful E2EE:&nbsp;</span></p>
<h3><span>1. Confidentiality in transit</span></h3>
<p><span>Message contents are authentically and securely transmitted between your devices and those of the people you’re talking to. This is, perhaps, the primary goal of E2EE, and is where much E2EE research and design work is targeted, such as the Signal protocol we use in our products (such as WhatsApp, Messenger, and Instagram Direct), or the </span><a href="https://datatracker.ietf.org/doc/rfc9420/" target="_blank" rel="noopener"><span>IETF’s Messaging Layer Security protocol</span></a><span>, which we helped to design and was recently standardized.</span></p>
<h3><span>2. Confidentiality in storage</span></h3>
<p><span>Typically, E2EE messaging services rely on local storage and encryption keys to secure encrypted messages. Messenger, however, has a long history of storing people’s messages for them so that they can access them whenever they need without having to store them locally. That’s why we’ve designed a server-based solution where encrypted messages can be stored on Meta’s servers while only being readable using encryption keys under the user’s control.&nbsp;</span></p>
<h3><span>3. Control over endpoints</span></h3>
<p><span>For something to be “end-to-end encrypted,” it is necessary to have a notion of what the “ends” are. For an E2EE messaging app this means that users should have the ability to verify and manage their set of endpoint devices that are receiving their messages, as well as visibility into when this set of devices changes.</span></p>
<h3><span>4. Private feature designs</span></h3>
<p><span>Product features in an E2EE setting typically need to be designed to function in a device-to-device manner, without ever relying on a third party having access to message content. This </span><a href="https://messengernews.fb.com/2023/08/22/expanding-testing-for-end-to-end-encryption-on-messenger/"><span>was a significant effort for Messenger</span></a><span>, as much of its functionality has historically relied on server-side processing, with certain features difficult or impossible to exactly match with message content being limited to the devices.</span></p>
<h3><span>5. Logging limitations</span></h3>
<p><span>Maintaining the confidentiality of message content extends to avoiding accidentally leaking it back to us in telemetry. In a product of Messenger’s scale, complexity, and iteration speed, this creates particular challenges&nbsp;as telemetry is vital in ensuring that the product is working well for people, and in debugging when things go wrong.</span></p>
<h3><span>6. Application security</span></h3>
<p><span>It’s a common saying that, “You can’t have privacy without security,” and this is absolutely true in the end-to-end encrypted domain. Security is important for any consumer product, but E2EE exacerbates the challenges in two important ways: it reduces the provider’s ability to protect the user from attacks, and, in fact, it expands the threat model to include the service provider itself. Our security team is keenly aware of these challenges and works closely with product teams to secure design and implementation of E2EE functionality. For example, we’ve been working to improve the memory safety of our apps; and our E2EE surfaces are covered by our <a href="https://www.facebook.com/whitehat" target="_blank" rel="noopener">bug bounty program</a>.</span></p>
<h3><span>7. Being deliberate about what’s being protected</span></h3>
<p><span>E2EE protects message content. However, this is a complex term to define, and, while certain things are relatively clear – such as the strings contained in a text message, or a photograph sent from your camera roll – in a sufficiently complex messaging application, it turns out there’s a surprisingly large grey area.&nbsp; Our focus is on determining the appropriate boundaries, ensuring that we remain true to our commitments, setting the correct user expectations, and avoiding creating meaningful privacy risks, while still ensuring that the product retains its usefulness to our users.</span></p>
<h3><span>8. Third-party scrutiny</span></h3>
<p><span>E2EE implies confidentiality even if the provider wants to access the contents of a communication. We aim for this to be verifiable externally, and, to this end, have published two white papers to provide transparency into our operations. We describe the properties of some features in our Help Center, and we encourage submissions to our <a href="https://www.facebook.com/whitehat" target="_blank" rel="noopener">bug bounty program</a>. Throughout the project, we have consulted with a diverse range of external parties to ensure that we’re making the right set of tradeoffs. To improve people’s ability to scrutinize us, we also support </span><a href="https://engineering.fb.com/2022/03/10/security/code-verify/" target="_blank" rel="noopener"><span>the Code Verify browser extension</span></a><span> for our web-based end-to-end encrypted messaging, to give security researchers greater confidence that the code version that they are assessing is being used globally.&nbsp;</span></p>
<h2><span>High-level approach</span></h2>
<p><span>With all of this in mind, our high-level approach was to build off of Meta’s prior learnings in E2EE, from both <a href="https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/" target="_blank" rel="noopener">WhatsApp</a> and Messenger’s Secret Conversations, and then to iterate on our most challenging problems.&nbsp;</span></p>
<p><span>Working from the baseline of these two approaches, we then had to address a series of significant technical challenges, including:</span></p>
<ol>
<li><span><strong>Multi-device capability</strong>: Messenger’s model of multi-device reflects the Facebook network, which allows people to authenticate on new devices with a username and password, in order to send and receive messages. Since <a href="https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/" target="_blank" rel="noopener">WhatsApp’s multi-device capability</a> relies on a single primary device that must cryptographically authenticate companion devices, we adopted the Secret Conversations model of multi-device, while ensuring that it functions well for all of our users.</span></li>
<li><span><strong>Feature support</strong>: Messenger has a number of messaging features that either don’t exist in WhatsApp, or function differently. Some of these just had to be rebuilt from scratch, while others required deploying new applied privacy technology. For example, we used </span><a href="https://datatracker.ietf.org/wg/ohai/about/" target="_blank" rel="noopener"><span>OHAI</span></a><span> and </span><a href="https://engineering.fb.com/2022/03/30/security/de-identified-authentication-at-scale/" target="_blank" rel="noopener"><span>Anonymous Credentials</span></a><span> to support searches of Facebook’s first-party sticker library, without revealing to us who is sending them.</span></li>
<li><span><strong>Message history</strong>: Messenger has always allowed clients to operate off of a small stored local cache, relying on a server-side database for their message history. Neither WhatsApp nor Secret Conversations operated in this manner, and we didn’t want all users to have to rely on a device-side storage system. Instead, we designed an entirely new encrypted storage system called <a href="https://engineering.fb.com/wp-content/uploads/2023/12/TheLabyrinthEncryptedMessageStorageProtocol_12-6-2023.pdf" target="_blank" rel="noopener">Labyrinth</a>, with ciphertexts uploaded to our servers and loaded on-demand by clients, while operating in a multi-device manner and supporting key rotation when clients are removed.</span></li>
<li><span><strong>Web support</strong>: We needed to support E2EE within our existing web surfaces, including the main Facebook site. The Web platform carries significantly different constraints from native apps, meaning that we needed to take custom approaches to many different aspects of the product. Further, Web users often add and remove devices in very different patterns from mobile-only users, increasing the complexity of our multi-device challenge.</span></li>
</ol>
<h2><span>Learn more about E2EE on Messenger</span></h2>
<p><span>Today, we are sharing two white papers:</span></p>
<ul>
<li aria-level="1"><span>Our </span><a href="https://engineering.fb.com/wp-content/uploads/2023/12/MessengerEnd-to-EndEncryptionOverview_12-6-2023.pdf"><span>Messenger end-to-end encryption whitepaper</span></a><span>, which describes the core cryptographic protocol for transmitting messages between clients.</span></li>
<li aria-level="1"><span>The </span><a href="https://engineering.fb.com/wp-content/uploads/2023/12/TheLabyrinthEncryptedMessageStorageProtocol_12-6-2023.pdf"><span>Labyrinth encrypted storage protocol whitepaper</span></a><span>, describing our protocol for end-to-end encrypting stored messages history between devices on a user’s account.</span></li>
</ul>
<p><span>These add to a number of publications that we have shared which cover Messenger’s E2EE, including:</span></p>
<ul>
<li aria-level="1"><span>Our recently updated </span><a href="https://messengernews.fb.com/wp-content/uploads/2021/12/Metas-approach-to-safer-private-messaging-on-Messenger-and-Instagram-DMs-Sep-23.pdf" target="_blank" rel="noopener"><span>Safety whitepaper</span></a></li>
<li aria-level="1"><span>The independent </span><a href="https://about.fb.com/news/2022/04/expanding-end-to-end-encryption-protects-fundamental-human-rights/" target="_blank" rel="noopener"><span>E2EE Human Rights Impact Assessment</span></a></li>
<li aria-level="1"><span>Our </span><a href="https://engineering.fb.com/wp-content/uploads/2022/07/Meta-Security-Principles-for-Private-Messaging-White-Paper-July-2022-2.pdf" target="_blank" rel="noopener"><span>Security Principles whitepaper</span></a></li>
<li aria-level="1"><span>The </span><a href="https://engineering.fb.com/2022/03/10/security/code-verify/" target="_blank" rel="noopener"><span>Code Verify browser extension</span></a></li>
</ul>
<h2><span>Beyond E2EE for Messenger</span></h2>
<p><span>The journey to bring E2EE to Messenger has been a long one, but it’s not yet finished. While we are globally launching default E2EE for personal one-to-one messages on Messenger, we are still in the testing phase for group messaging and some other products, like Instagram Direct Messages. On Instagram, we are currently testing “disappearing messages” for one-to-one Instagram Direct conversations in select countries. Disappearing messages are ephemeral and, as with those in Messenger, expire 24 hours after being sent. They are built leveraging our E2EE infrastructure and provide an increased level of privacy. We plan to expand this work as well as conduct additional testing around E2EE on Instagram over the next year.</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23andMe updates their TOS to force binding arbitration (230 pts)]]></title>
            <link>https://stackdiary.com/23andme-updates-tos-to-force-binding-arbitration/</link>
            <guid>38551890</guid>
            <pubDate>Thu, 07 Dec 2023 01:54:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stackdiary.com/23andme-updates-tos-to-force-binding-arbitration/">https://stackdiary.com/23andme-updates-tos-to-force-binding-arbitration/</a>, See on <a href="https://news.ycombinator.com/item?id=38551890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<p>23andMe, the personal genomics and biotechnology company, has been trying to contain a security breach that was <a href="https://www.bloomberg.com/news/articles/2023-10-06/hacker-puts-23andme-user-data-up-for-sale-on-the-internet" target="_blank" rel="noreferrer noopener">first disclosed on October 6th</a>. On <a href="https://www.theverge.com/2023/10/19/23923861/23andme-possible-data-leak-hack" target="_blank" rel="noreferrer noopener">October 19th</a>, 23andMe disclosed <em>another</em> security breach by the same hacker who had initially claimed responsibility. The hacker said he had access to more than 4 million genetic profile records this time. And on December 4th, <a href="https://www.theverge.com/2023/12/4/23988050/23andme-hackers-accessed-user-data-confirmed" target="_blank" rel="noreferrer noopener">23andMe confirmed</a> that the total scope of the breach was 6.9 million users in total.</p>



<p>The fallout of this disclosure, which started in October, was swift. By October 14th, several individuals had already filed lawsuits against 23andMe for negligence, as <a href="https://stackdiary.com/23andme-sued-by-users-over-data-breach/" target="_blank" rel="noreferrer noopener"><em>Stack Diary</em> reported</a>. Likewise, the general consensus of 23andMe users has been that the company handled the situation very poorly.</p>



<p>To add insult to injury, <em>Stack Diary</em> can reveal that 23andMe is now rolling out an update to its <a href="https://www.23andme.com/en-int/legal/terms-of-service/" target="_blank" rel="noreferrer noopener">Terms of Service</a>. This change will force its users into <em>binding arbitration</em>, which is a means to resolve disputes (such as a cybersecurity breach leaking your DNA data) outside of court.</p>



<p>In this process, both parties in a disagreement present their cases to an arbitrator, who is a neutral third party. The arbitrator listens to both sides, reviews the evidence, and decides. The key aspect of binding arbitration is that the arbitrator's decision is final and legally enforceable, meaning both parties must accept it and cannot appeal to a regular court. </p>



<p>This method is commonly used in various settings, including consumer contracts, employment disputes, and business disagreements, as it is often faster and less formal than going to court.</p>



<p>And 23andMe is trying to accomplish exactly this.</p>



<ul>
<li><strong>Initial Dispute Resolution Period</strong>: If you have a problem with 23andMe's services, you first need to contact their customer care team. This is to try and solve the issue quickly and without legal proceedings. You have to try this informal negotiation for at least 60 days before you can take any further legal action. You need to provide them with a detailed email outlining your issue, including what the dispute is about, when it happened, what you want as a solution, and your contact details. You (and your lawyer, if you have one) will also need to have a discussion with them to try and solve the dispute.</li>



<li><strong>Arbitration Instead of Court</strong>: If the issue isn't resolved in those 60 days, the next step is usually not a lawsuit in court, but arbitration. This means a neutral third party (an arbitrator) will listen to both sides and make a decision. The rules of this process are governed by JAMS, a company that provides arbitration services. In some cases, if many people have similar disputes against 23andMe, a different process called Mass Arbitration with another company, NAM, will be used.</li>



<li><strong>Arbitrator's Decision</strong>: The arbitrator’s decision is final. They have to follow the law and can give any ruling that a court could.</li>



<li><strong>Exceptions to Arbitration</strong>: There are a few situations where you or 23andMe can take the issue to court instead of arbitration. This includes things like intellectual property disputes and small claims (minor issues).</li>



<li><strong>No Class Actions</strong>: You can't join with other people to bring a class action or collective arbitration against 23andMe. Each dispute is handled individually.</li>



<li><strong>Severability</strong>: If any part of this dispute resolution section is not legally enforceable, the rest still applies.</li>
</ul>



<p>In the event of a cybersecurity breach, this means that if you have a dispute with 23andMe about it, you would first try to resolve it with their customer care. If that doesn't work, you'd generally go to arbitration, not a lawsuit, unless it falls under one of the exceptions. You also can't join a class action lawsuit for such an issue.</p>



<h2>23andMe is beginning to notify its users</h2>



<p>23andMe is beginning to inform its users of a modification in their Terms of Service via email. Users are given a 30-day window from when they receive this email to opt out of these new, stringent terms that significantly reduce their rights. </p>


<div>
<figure><img data-dominant-color="eeefef" data-has-transparency="false" decoding="async" width="615" height="650" src="https://stackdiary.com/wp-content/uploads/2023/12/image-3-615x650.png" alt="" srcset="https://stackdiary.com/wp-content/uploads/2023/12/image-3-615x650.png 615w, https://stackdiary.com/wp-content/uploads/2023/12/image-3-284x300.png 284w, https://stackdiary.com/wp-content/uploads/2023/12/image-3-768x811.png 768w, https://stackdiary.com/wp-content/uploads/2023/12/image-3.png 820w" sizes="(max-width: 615px) 100vw, 615px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20615%20650'%3E%3C/svg%3E" data-lazy-srcset="https://stackdiary.com/wp-content/uploads/2023/12/image-3-615x650.png 615w, https://stackdiary.com/wp-content/uploads/2023/12/image-3-284x300.png 284w, https://stackdiary.com/wp-content/uploads/2023/12/image-3-768x811.png 768w, https://stackdiary.com/wp-content/uploads/2023/12/image-3.png 820w" data-lazy-src="https://stackdiary.com/wp-content/uploads/2023/12/image-3-615x650.png"><figcaption>In the email that 23andMe is sending to its users, the "notify us" hyperlink contains an email address that is "legal@23andme.com" as opposed to an address that is listed in the Terms of Service.</figcaption></figure></div>


<p>The email doesn't mention that you must email the "<a href="mailto:arbitrationoptout@23andme.com">arbitrationoptout@23andme.com</a>" address to opt out of forced arbitration, as outlined in the updated Terms of Service, which you can <a href="https://www.23andme.com/legal/terms-of-service/#dispute-resolution-arbitration" target="_blank" rel="noreferrer noopener">preview here</a>. </p>



<blockquote>
<p><strong>30 Day Right to Opt-Out</strong>. You have the right to opt-out and not be bound by the arbitration and class action waiver provisions set forth above by sending written notice of your decision to opt-out by emailing us at arbitrationoptout@23andme.com. The notice must be sent within thirty (30) days of your first use of the Service, or the effective date of the first set of Terms containing an Arbitration and Class Action and Class Arbitration Waiver section otherwise you shall be bound to arbitrate disputes in accordance with the terms of those sections. If you opt out of these arbitration provisions, we also will not be bound by them.</p>
</blockquote>



<p>It's <em>unlikely</em> that the intention of the email mix-up is malicious in nature; they would absolutely get destroyed by every privacy organization on the planet if they snuck in a change like that, but I have emailed them to verify the above and will add a response here once I get it.</p>



<p>That said, unless you email this account 30 days after starting to use the service for the first time, you will automatically be enrolled in this arbitration scheme. Likewise, this affects all users who were affected by the cybersecurity breach since the terms were changed after the fact. Because these terms were put in place on November 30, 2023 - it has already been over a week, and most users might not understand why this is important or relevant.</p>



<p>If you're unsure as to why arbitration is bad, it's because it is biased against the consumer. The Stanford Graduate School of Business did an entire study on it; you can read the blog post about it <a href="https://www.gsb.stanford.edu/insights/why-binding-arbitration-game-rigged-against-customers" target="_blank" rel="noreferrer noopener nofollow">here</a> or view the entire study <a href="https://www.gsb.stanford.edu/faculty-research/working-papers/arbitration-uninformed-consumers" target="_blank" rel="noreferrer noopener nofollow">here</a>. </p>



<p>Here's an excerpt from the blog post:</p>



<blockquote>
<p>Now, a&nbsp;new analysis&nbsp;of almost 9,000 arbitration cases from the securities industry confirms what many have long suspected: The system is biased against consumers — and not just because big companies have more money to spend on lawyers.</p>



<p>When it comes to arbitration, the study finds, companies have a big information advantage in fishing for arbitrators who are likely to rule in their favor.</p>



<p>Making matters worse, the arbitrators themselves know that being pro-company in one case greatly increases their chances of being picked for future cases.</p>
<cite>Edmund L. Andrews, Stanford Business</cite></blockquote>



<p>This is merely about 23andMe protecting <em>itself</em> (not you, the consumer) because if a security breach of this scope happens again in the future, it will have <em>some</em> protection against mass user complaints.</p>



<h2>How to opt-out (email template)</h2>



<p>If you have been affected by the security breach at 23andMe and would like to opt out of the forced arbitration, here is an email template that you can use:</p>



<div>
<p><strong>To:</strong> <a href="mailto:legal@23andme.com">legal@23andme.com</a>, <a href="mailto:customercare@23andme.com">customercare@23andme.com</a>, <a href="mailto:arbitrationoptout@23andme.com">arbitrationoptout@23andme.com</a><br><strong>Subject:</strong> Request to Opt-Out of Updated TOS</p>



<p>Dear 23andMe Team,</p>



<p>I am contacting you regarding the recent changes to the 23andMe Terms of Service, dated November 30, 2023. My name is [your name as registered with 23andMe], and the email associated with my 23andMe account is [your 23andMe account email].</p>



<p>I hereby formally request to opt out of the newly updated Terms of Service. I do not consent to the terms as outlined in the recent update.</p>



<p>Thank you for processing my request promptly.</p>



<p>Best regards, <br>[Your Name]
</p></div>



<p>You should also make sure that you save the reply and explicitly ask them to confirm that you opted out. This will be mandatory in case another breach happens in the future, as you will have proof that you're not bound by this change in their Terms of Service.</p>

								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers have discovered magnetic monopole quasi-particles (117 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/diamonds-and-rust-help-unveil-impossible-quasi-particles</link>
            <guid>38550994</guid>
            <pubDate>Wed, 06 Dec 2023 23:38:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/diamonds-and-rust-help-unveil-impossible-quasi-particles">https://www.cam.ac.uk/research/news/diamonds-and-rust-help-unveil-impossible-quasi-particles</a>, See on <a href="https://news.ycombinator.com/item?id=38550994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Researchers led by the University of Cambridge used a technique known as diamond quantum sensing to observe swirling textures and faint magnetic signals on the surface of hematite, a type of iron oxide.</p>

<p>The researchers observed that magnetic monopoles in hematite emerge through the collective behaviour of many spins (the angular momentum of a particle). These monopoles glide across the swirling textures on the surface of the hematite, like tiny hockey pucks of magnetic charge. This is the first time that naturally occurring emergent monopoles have been observed experimentally.</p>

<p>The research has also shown the direct connection between the previously hidden swirling textures and the magnetic charges of materials like hematite, as if there is a secret code linking them together. The <a href="https://www.nature.com/articles/s41563-023-01737-4">results</a>, which could be useful in enabling next-generation logic and memory applications, are reported in the journal <em>Nature Materials</em>.</p>

<p>According to the equations of James Clerk Maxwell, a giant of Cambridge physics, magnetic objects, whether a fridge magnet or the Earth itself, must always exist as a pair of magnetic poles that cannot be isolated.</p>

<p>“The magnets we use every day have two poles: north and south,” said Professor Mete Atatüre, who led the research. “In the 19th century, it was hypothesised that monopoles could exist. But in one of his foundational equations for the study of electromagnetism, James Clerk Maxwell disagreed.”</p>

<p>Atatüre is Head of Cambridge’s Cavendish Laboratory, a position once held by Maxwell himself. “If monopoles did exist, and we were able to isolate them, it would be like finding a missing puzzle piece that was assumed to be lost,” he said.</p>

<p>About 15 years ago, scientists suggested how monopoles could exist in a magnetic material. This theoretical result relied on the extreme separation of north and south poles so that locally each pole appeared isolated in an exotic material called spin ice.</p>

<p>However, there is an alternative strategy to find monopoles, involving the concept of emergence. The idea of emergence is the combination of many physical entities can give rise to properties that are either more than or different to the sum of their parts.</p>

<p>Working with colleagues from the University of Oxford and the National University of Singapore, the Cambridge researchers used emergence to uncover monopoles spread over two-dimensional space, gliding across the swirling textures on the surface of a magnetic material.</p>

<p>The swirling topological textures are found in two main types of materials: ferromagnets and antiferromagnets. Of the two, antiferromagnets are more stable than ferromagnets, but they are more difficult to study, as they don’t have a strong magnetic signature.</p>

<p>To study the behaviour of antiferromagnets, Atatüre and his colleagues use an imaging technique known as diamond quantum magnetometry. This technique uses a single spin – the inherent angular momentum of an electron – in a diamond needle to precisely measure the magnetic field on the surface of a material, without affecting its behaviour.</p>

<p>For the current study, the researchers used the technique to look at hematite, an antiferromagnetic iron oxide material. To their surprise, they found hidden patterns of magnetic charges within hematite, including monopoles, dipoles and quadrupoles.</p>

<p>“Monopoles had been predicted theoretically, but this is the first time we’ve actually seen a two-dimensional monopole in a naturally occurring magnet,” said co-author Professor Paolo Radaelli, from the University of Oxford.</p>

<p>“These monopoles are a collective state of many spins that twirl around a singularity rather than a single fixed particle, so they emerge through many-body interactions. The result is a tiny, localised stable particle with diverging magnetic field coming out of it,” said co-first author Dr Hariom Jani, from the University of Oxford.</p>

<p>“We’ve shown how diamond quantum magnetometry could be used to unravel the mysterious behaviour of magnetism in two-dimensional quantum materials, which could open up new fields of study in this area,” said co-first author Dr Anthony Tan, from the Cavendish Laboratory. “The challenge has always been direct imaging of these textures in antiferromagnets due to their weaker magnetic pull, but now we’re able to do so, with a nice combination of diamonds and rust.”</p>

<p>The study not only highlights the potential of diamond quantum magnetometry but also underscores its capacity to uncover and investigate hidden magnetic phenomena in quantum materials. If controlled, these swirling textures dressed in magnetic charges could power super-fast and energy-efficient computer memory logic.</p>

<p>The research was supported in part by the Royal Society, the Sir Henry Royce Institute, the European Union, and the Engineering and Physical Sciences Research Council (EPSRC), part of UK Research and Innovation (UKRI).</p>

<p><em><strong>Reference:</strong><br>
K. C. Tan, Hariom Jani, Michael Högen et al. ‘<a href="https://www.nature.com/articles/s41563-023-01737-4">Revealing Emergent Magnetic Charge in an Antiferromagnet with Diamond Quantum Magnetometry</a>.’ Nature Materials (2023). DOI: 10.1038/s41563-023-01737-4.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Merlin Sound ID – Identify Birds Using Your Phone (How It Works) (138 pts)]]></title>
            <link>https://www.macaulaylibrary.org/2021/06/22/behind-the-scenes-of-sound-id-in-merlin/</link>
            <guid>38550737</guid>
            <pubDate>Wed, 06 Dec 2023 23:08:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macaulaylibrary.org/2021/06/22/behind-the-scenes-of-sound-id-in-merlin/">https://www.macaulaylibrary.org/2021/06/22/behind-the-scenes-of-sound-id-in-merlin/</a>, See on <a href="https://news.ycombinator.com/item?id=38550737">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
<!--?xml encoding="utf-8" ?--><h2><b>What is Sound ID?</b></h2>
<figure id="attachment_6745" aria-describedby="caption-attachment-6745"><img decoding="async" loading="lazy" alt="App demo showing detection of White-throated Sparrow" width="142" height="288" data-image-lazy="" data-src="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-506x1024.png" data-sizes="(max-width: 142px) 100vw, 142px" data-srcset="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-506x1024.png 506w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-148x300.png 148w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-768x1554.png 768w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-759x1536.png 759w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-1012x2048.png 1012w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-79x160.png 79w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-158x320.png 158w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-237x480.png 237w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-316x640.png 316w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-445x900.png 445w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-593x1200.png 593w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-890x1800.png 890w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-1186x2400.png 1186w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-300x607.png 300w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-600x1214.png 600w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow.png 1460w" src="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-506x1024.png" srcset="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-506x1024.png 506w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-148x300.png 148w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-768x1554.png 768w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-759x1536.png 759w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-1012x2048.png 1012w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-79x160.png 79w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-158x320.png 158w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-237x480.png 237w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-316x640.png 316w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-445x900.png 445w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-593x1200.png 593w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-890x1800.png 890w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-1186x2400.png 1186w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-300x607.png 300w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow-600x1214.png 600w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Sound-ID-iOS-Best-matches-comparison-White-throated-Sparrow.png 1460w"><figcaption id="caption-attachment-6745">A screenshot showing the detection of a White-throated Sparrow.</figcaption></figure>
<p><span>Today we announced one of our biggest breakthroughs—Sound ID, a new feature in the </span><a href="https://merlin.allaboutbirds.org/"><span>Merlin Bird ID</span></a><span> app—and a major leap forward&nbsp;in sound identification and machine learning to date. Sound ID lets people use their phone to listen to the birds around them, and see live predictions of who’s singing. Currently, Merlin can identify 458 bird species in the U.S. and Canada based on their sounds (with more species and regions coming soon). Sound ID runs on your device, without requiring a network connection. <a href="https://merlinbirdid.page.link/sound-id">Download it today for free</a>&nbsp;and test it out in your own backyard! If you happen to be located in the Northeastern United States you can test out Sound ID on the audio below which was recorded in New Hampshire.</span></p>
<h2><b>How does Sound ID work?</b></h2>
<p><span>As your phone records sound, Merlin converts the audio into an image called a spectrogram. The spectrogram plots sound frequencies that appear in the recording, as a function of time. This spectrogram image is then fed into a modern computer vision model called a deep convolutional neural network. We trained this model to identify birds based on 140 hours of audio containing bird sounds, in addition to 126 hours of audio containing non-bird background sounds, like whistling and car noises. For each audio clip, a group of sound ID experts from the Macaulay Library and the eBird community found the precise moments when birds were making sounds, and tagged those sounds with the corresponding bird species. The model can use this detailed supervision from experts to learn how to correctly predict the species that appear in these annotated audio clips, with the goal of generalizing this knowledge to predict which birds appear in audio recordings it hasn’t heard before.</span></p>



<p><span>So how does it work? Once the database of sounds is assembled, we train the computer vision model using a gradient descent algorithm. When the model “hears” a sound clip, it makes a prediction that is based on the transformation of the sound clip’s spectrogram through a series of mathematical operations involving millions of numbers (called <em>weights</em>). The gradient descent algorithm figures out how to adjust the value of each weight to ensure that the model’s predictions match those of the sound ID experts. This weight updating process is the “learning” part of machine learning. </span></p>
<p><span>Building the sound ID model is an iterative process, involving a back-and-forth between the sound ID experts, members of the machine learning team, and people who provide feedback based on field tests of the app. After evaluating a trained model’s performance, we make adjustments to the training algorithm, ask the sound ID experts to label more audio clips, and try to locate any human errors in the previously labeled data.&nbsp;</span></p>
<h2><b>What’s special about Sound ID in Merlin?</b></h2>
<p><span>Merlin is not the first to use deep convolutional neural networks to identify birds by their sounds. In fact, Merlin draws inspiration from a number of other projects, including </span><a href="https://birdnet.cornell.edu/"><span>BirdNET</span></a><span> and </span><a href="https://wp.nyu.edu/birdvox/"><span>BirdVox</span></a><span>.&nbsp;</span></p>
<p><span>There have been many other approaches to bird sound ID through the years, the result of engineering contests such as </span><a href="https://www.kaggle.com/c/birdclef-2021"><span>BirdClef</span></a><span> and </span><a href="http://dcase.community/challenge2021/task-few-shot-bioacoustic-event-detection"><span>DCASE</span></a><span>, </span><a href="https://www.kaggle.com/c/rfcx-species-audio-detection"><span>among</span></a> <a href="https://www.kaggle.com/c/birdsong-recognition"><span>many</span></a> <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13103"><span>others</span></a><span>. </span><span>Similar techniques have been used to </span><a href="https://naturesmartcities.com/"><span>monitor the activity of bats</span></a><span>, as well as find </span><a href="https://www.blog.google/technology/ai/pattern-radio-whale-songs/"><span>patterns in whale songs</span></a><span>.</span></p>
<p><span>Previous bird sound ID models have typically been trained using data with a coarser level of temporal resolution. For instance, a model might hear a 30 second recording of a White-breasted Nuthatch, but not be told when the nuthatch is singing in the recording. This can lead to problems: if other species are singing in the same recording, the model will erroneously call all species in the recording a White-breasted Nuthatch, leading to false predictions.&nbsp;</span></p>

<div>
	<figure>
		<p><img alt="Carolina Wren" data-image-lazy="" data-sizes="(min-width: 1200px) 1200px, 100vw" data-srcset="
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/160 160w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/320 320w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/480 480w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/640 640w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/900 900w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/1200 1200w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/1800 1800w,
					https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/2400 2400w" data-src="https://cdn.download.ams.birds.cornell.edu/api/v1/asset/83066971/1200">
						</p>
				<figcaption>
			
		</figcaption>
			</figure>
</div>

<p><span>Merlin’s Sound ID tool is trained using audio data which includes the precise moments in time when each bird is vocalizing. The process of generating this data is labor intensive, because it requires sound ID experts to listen to each audio file carefully. As a result of these efforts, the model has the opportunity to learn a more accurate representation of which sounds correspond to which species (and which sounds are ambient noises). </span><a href="https://arxiv.org/abs/2105.07031"><span>Recent research</span></a><span> confirms that temporally fine-grained labels can help improve audio classification performance.</span></p>
<figure id="attachment_6778" aria-describedby="caption-attachment-6778"><img decoding="async" loading="lazy" alt="Screenshot of the annotation tool. " width="1772" height="701" data-image-lazy="" data-src="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM.png" data-sizes="(max-width: 1772px) 100vw, 1772px" data-srcset="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM.png 1772w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-300x119.png 300w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-1024x405.png 1024w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-768x304.png 768w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-1536x608.png 1536w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-80x32.png 80w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-160x63.png 160w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-320x127.png 320w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-480x190.png 480w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-640x253.png 640w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-900x356.png 900w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-1200x475.png 1200w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-600x237.png 600w" src="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM.png" srcset="https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM.png 1772w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-300x119.png 300w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-1024x405.png 1024w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-768x304.png 768w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-1536x608.png 1536w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-80x32.png 80w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-160x63.png 160w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-320x127.png 320w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-480x190.png 480w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-640x253.png 640w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-900x356.png 900w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-1200x475.png 1200w, https://ml-rds-wordpress-prod-s3.s3.amazonaws.com/uploads/2021/06/Screen-Shot-2021-06-22-at-10.48.56-AM-600x237.png 600w"><figcaption id="caption-attachment-6778">We built a custom annotation tool that allows sound ID experts to listen to Macaulay Library recordings and annotate the precise moments when different bird species are vocalizing.</figcaption></figure>

<h2><b>What’s next?</b></h2>
<p><span>In building the model, we made a number of design decisions about how to handle our particular dataset, how to integrate predictions with information from eBird</span><span> (a database of bird sightings shared by citizen scientists from around the world),</span><span> and how to maximize the accuracy of Merlin Sound ID’s predictions in the field.</span></p>
<p><span>In the coming weeks, we’ll be posting a series of articles that take a closer look at these design decisions. We’ll also explore some of what’s in store for our Sound ID tools in the future.</span></p>
<p><span>If you’ve tried Sound ID in Merlin, we’d love to hear about your experience. You can get in touch on Twitter, where the Macaulay Library is <a href="https://twitter.com/macaulaylibrary?lang=en">@MacaulayLibrary</a>.</span></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Long context prompting for Claude 2.1 (177 pts)]]></title>
            <link>https://www.anthropic.com/index/claude-2-1-prompting</link>
            <guid>38550675</guid>
            <pubDate>Wed, 06 Dec 2023 23:00:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/index/claude-2-1-prompting">https://www.anthropic.com/index/claude-2-1-prompting</a>, See on <a href="https://news.ycombinator.com/item?id=38550675">Hacker News</a></p>
Couldn't get https://www.anthropic.com/index/claude-2-1-prompting: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Gordon Freeman at the Olympic Games (196 pts)]]></title>
            <link>https://moonbase.lgbt/blog/100m-accelerated-backhopping/</link>
            <guid>38550408</guid>
            <pubDate>Wed, 06 Dec 2023 22:32:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://moonbase.lgbt/blog/100m-accelerated-backhopping/">https://moonbase.lgbt/blog/100m-accelerated-backhopping/</a>, See on <a href="https://news.ycombinator.com/item?id=38550408">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	
	<div>
		
		
		<p>Since the dawn of <a href="https://twitter.com/lunasorcery/status/1576008439586648065">October 1st 2022</a>, the world has been plagued by the question:</p>
<p><strong><em>Would accelerated backhopping give you a competitive advantage in the 100m dash?</em></strong></p>
<p>In this <s>deranged ramble</s> blog article we shall attempt to answer the question once and for all.</p>
<hr>
<blockquote>
“Gordon! You must get out of here! Run!”
<cite>— Dr. Kleiner, Half-Life 2</cite>
</blockquote>

<hr>
<h2 id="initial-parameters">Initial Parameters</h2>
<p>Our investigation into the viability of accelerated backhopping as a competitive strategy in the 100m dash will use the following parameters:</p>
<ul>
<li>Movement physics will be as present in Half-Life 2.</li>
<li>The 100m dash will be as defined in the IAAF Competition Rules.</li>
</ul>
<hr>
<h2 id="what-is-accelerated-backhopping">What is Accelerated Backhopping?</h2>
<p><em>Note: the technical aspects of this section are largely adapted from <a href="https://wiki.sourceruns.org/wiki/Accelerated_Back_Hopping">the SourceRuns wiki</a>.</em></p>
<p>Accelerated backhopping (henceforth referred to as ABH) is a bug in the Source engine movement code, allowing the player to move far faster than the developers intended.</p>
<p>When the player performs a jump, the game tries to give the player a little extra speed boost, based on their current velocity, state (crouching, sprinting, etc), movement input, and look direction. It also attempts to cap the player’s speed, but an oversight in the programming causes the player to accelerate far beyond that speed limit under certain circumstances.</p>
<hr>
<p>Let’s look at <a href="https://github.com/ValveSoftware/source-sdk-2013/blob/56accfdb9c4abd32ae1dc26b2e4cc87898cf4dc1/sp/src/game/shared/gamemovement.cpp#L2469-L2495">the code</a>; this is executed when a player is on the ground and makes a jump input:</p>
<p>Calculate the boost multiplier <code>flSpeedBoostPerc</code> to determine how much of a boost we want to give. If the player is sprinting or ducking, they get a 10% boost, otherwise they get a 50% boost.</p>
<pre><code>float flSpeedBoostPerc = ( !pMoveData-&gt;m_bIsSprinting &amp;&amp; !player-&gt;m_Local.m_bDucked ) ? 0.5f : 0.1f;
</code></pre>
<p>Calculate <code>flSpeedAddition</code>, the additional velocity we want to apply to our player’s speed, by scaling the player’s forward movement input <code>mv-&gt;m_flForwardMove</code> by our boost multiplier, and taking the absolute value so it’s positive.</p>
<pre><code>float flSpeedAddition = fabs( mv-&gt;m_flForwardMove * flSpeedBoostPerc );
</code></pre>
<p>Calculate <code>flMaxSpeed</code>, the maximum speed the player should be travelling at. We take <code>mv-&gt;m_flMaxSpeed</code>, the maximum speed expected for the player’s current state, and increase it by our boost multiplier.</p>
<pre><code>float flMaxSpeed = mv-&gt;m_flMaxSpeed + ( mv-&gt;m_flMaxSpeed * flSpeedBoostPerc );
</code></pre>
<p>Calculate <code>flNewSpeed</code>, the speed we expect the player to have after jumping, by adding <code>flSpeedAddition</code> to the player’s current lateral velocity <code>mv-&gt;m_vecVelocity.Length2D</code>.</p>
<pre><code>float flNewSpeed = ( flSpeedAddition + mv-&gt;m_vecVelocity.Length2D() );
</code></pre>
<p>If the player’s new speed is higher than the maximum speed, subtract the difference from <code>flSpeedAddition</code>.</p>
<pre><code>if ( flNewSpeed &gt; flMaxSpeed )
{
    flSpeedAddition -= flNewSpeed - flMaxSpeed;
}
</code></pre>
<p>If the player is providing a backwards movement input, negate <code>flSpeedAddition</code>:</p>
<pre><code>if ( mv-&gt;m_flForwardMove &lt; 0.0f )
    flSpeedAddition *= -1.0f;
</code></pre>
<p>Apply the speed addition to the player’s velocity, by adding it in the forward look direction <code>vecForward</code>:</p>
<pre><code>VectorAdd( (vecForward*flSpeedAddition), mv-&gt;m_vecVelocity, mv-&gt;m_vecVelocity );
</code></pre>
<hr>
<p>The critical flaw in the code resides in the assumption that the movement input and the current velocity are correlated.</p>
<p>Consider the following situation: The player is sprinting along at 320ups (units per second). They jump forwards, and the boost increases their speed to 352ups, by design. While in the air, they spin around 180 degrees (so they’re now facing where they came from, and moving backwards), release all movement inputs, and begin crouching. Upon touching the ground, still moving at 352ups, they immediately jump again.</p>
<p>The player is crouching, so the boost multiplier is set to 10%.</p>
<p>Since the player isn’t providing any forward movement input, <code>flSpeedAddition</code> becomes 0:</p>
<pre><code>float flSpeedAddition = fabs( mv-&gt;m_flForwardMove * flSpeedBoostPerc ) 
                      = fabs(0 * 0.1) 
                      = 0
</code></pre>
<p>The max speed <code>flMaxSpeed</code> is calculated based on <em>crouching</em> speed limit of 190ups, and comes out as 209:</p>
<pre><code>float flMaxSpeed = mv-&gt;m_flMaxSpeed + ( mv-&gt;m_flMaxSpeed * flSpeedBoostPerc ) 
                 = 190 + (190 * 0.1) 
                 = 209
</code></pre>
<p>Since <code>flSpeedAddition</code> is 0, the new speed <code>flNewSpeed</code> does not change from the player’s current velocity.</p>
<pre><code>float flNewSpeed = ( flSpeedAddition + mv-&gt;m_vecVelocity.Length2D() ) 
                 = 0 + 352 
                 = 352
</code></pre>
<p><code>flNewSpeed</code> is now far in excess of <code>flMaxSpeed</code>, so the difference is subtracted from <code>flSpeedAddition</code>, giving -143:</p>
<pre><code>flSpeedAddition -= flNewSpeed - flMaxSpeed;
                = 0 - (352 - 209)
                = -143
</code></pre>
<p>The player is not providing any backwards movement input, so <code>flSpeedAddition</code> remains negative.</p>
<pre><code>if ( mv-&gt;m_flForwardMove &lt; 0.0f ) // false
    flSpeedAddition *= -1.0f; // doesn't execute
</code></pre>
<p>This negative value then gets multiplied by the player’s <em>forward</em> look direction, producing a vector pointing 143 units <em>behind</em> them. This gets applied to the player’s velocity vector — remember, since spinning in mid-air they’re already moving backwards — increasing their speed by 143ups for a new speed of 495ups.</p>
<p>On the next jump, this effect is compounded, creating a speed boost of 286ups, then 572ups, then 1144ups, and so on.</p>
<hr>
<h2 id="rules-of-engagement">Rules of Engagement</h2>
<p>We will now consult the <a href="https://worldathletics.org/about-iaaf/documents/book-of-rules">IAAF Book of Rules</a> to determine the conditions under which we will compete. The following excerpts are all taken from IAAF Book C, C2.1 Technical Rules.</p>
<h3 id="attire">Attire</h3>
<blockquote>
<p>Assistance not Allowed</p>
<p><strong>6.3</strong>   For the purpose of this Rule, the following examples shall be considered assistance, and are therefore not allowed:<br>
[...]<br>
<strong>6.3.4</strong> The use of any mechanical aid, except by an athlete with an impairment as authorised or permitted in accordance with the Mechanical Aids Regulations.</p>
</blockquote>
<p>Wearing the HEV suit allows the player to sprint, allowing for a much faster initial ABH speed than would otherwise be possible (495ups with the suit, 285ups without). We believe it is a reasonable interpretation that the HEV suit constitutes a mechanical aid, and would therefore not be permitted for use. Our runner will thus compete without the suit.</p>
<h3 id="race-start">Race Start</h3>
<blockquote>
<p><strong>16.3</strong>  In races up to and including 400m (including the first leg of 4 × 200m, the Medley Relay and 4 × 400m), <strong>a crouch start and the use of starting blocks are compulsory</strong>. After the “On your marks” command, an athlete shall approach the start line, assume a position completely within their allocated lane and behind the start line. An athlete shall not touch either the start line or the ground in front of it with their hands or their feet when on their mark. Both hands and at least one knee shall be in contact with the ground and both feet in contact with the foot plates of the starting blocks. At the “Set” command, an athlete shall immediately rise to their final starting position retaining the contact of the hands with the ground and of the feet with the foot plates of the blocks. Once the Starter is satisfied that all athletes are steady in the “Set” position, the gun shall be fired.</p>
</blockquote>
<p>Within the bounds of Half-Life 2’s movement, the use of starting blocks and placing hands on the ground are not possible, so we will assume reasonable exemption from these rules. However, we can still abide by the requirement of a crouching start.</p>
<h3 id="lanes">Lanes</h3>
<blockquote>
<p><strong>14.4</strong>  In all races up to and including 400m, each athlete shall have a separate lane, with a width of 1.22m ± 0.01m, including the lane line on the right, marked by white lines 50mm in width. All lanes shall be of the same nominal width. The inner lane shall be measured as stated in Rule 14.2, but the remaining lanes shall be measured 0.20m from the outer edges of the lines.</p>
</blockquote>
<!-- -->

<blockquote>
<p><strong>17.3</strong>  In all races:</p>
<p><strong>17.3.1</strong>    run in lanes, each athlete shall keep within their allocated lane from start to finish. This shall also apply to any portion of a race run in lanes;</p>
</blockquote>
<!-- -->

<blockquote>
<p><strong>17.4</strong>  An athlete, or in the case of a relay race, their team, shall not be disqualified if the athlete:<br>
[...]<br>
<strong>17.4.2</strong>    steps or runs outside their lane in the straight, any straight part of the diversion from the track for the steeplechase water jump or outside the outer line of their lane on the bend;<br>
[...]<br>
and no material advantage is gained and no other athlete being jostled or obstructed so as to impede the other athlete’s progress (see Rule 17.2 of the Technical Rules). If material advantage is gained, the athlete (or team) shall be disqualified.</p>
</blockquote>
<p>In summary, the lane is 1.22m wide, and we must be careful to stay within that lane. We are only permitted to leave the lane if no material advantage would be gained by doing so; as such we cannot use any maneuvers that depend on us leaving the lane.</p>
<h3 id="the-finish">The Finish</h3>
<blockquote>
<p><strong>18.2</strong>  The athletes shall be placed in the order in which any part of their bodies (i.e. torso, as distinguished from the head, neck, arms, legs, hands or feet) reaches the vertical plane of the nearer edge of the finish line as defined above.</p>
</blockquote>
<p>In summary, our aim is to:</p>
<ul>
<li>Travel 100 meters as quickly as possible,</li>
<li>from a stationary crouching start,</li>
<li>without leaving the 1.22m-wide lane,</li>
<li>and without wearing an HEV suit.</li>
</ul>
<hr>
<h2 id="a-matter-of-scale">A Matter of Scale</h2>
<p>Contemporary game engines typically base their unit systems on the metric system.<sup id="fnref:cite-unity-units"><a href="#fn:cite-unity-units">[1]</a></sup><sup id="fnref:cite-unreal-units"><a href="#fn:cite-unreal-units">[2]</a></sup> Source engine, however, is an unusual beast, and its units are based on the imperial system. Maps are modelled at a scale of 16 units = 1 foot, or 1 unit = 3/4 inch.<sup id="fnref:cite-source-units"><a href="#fn:cite-source-units">[3]</a></sup> From this, we can calculate that our 100m run is 5,249.34 units.</p>
<hr>
<h2 id="the-run">The Run</h2>
<p><em>Note: The following timings and speeds are theoretical based on a crude analysis of the game code, and may not necessarily be reproducible under practical testing.</em></p>
<p>The player begins from a crouching position at the start line. Since an ABH cannot be performed from a crouching start, they spend 200ms returning to a standing position.<sup id="fnref:cite-unduck"><a href="#fn:cite-unduck">[4]</a></sup> Once standing, they begin walking forward, quickly reaching their maximum walking speed of 150ups. Once at speed, they make their first jump facing forwards. They get the <em>intended</em> jump-speed bonus, accelerating them to 225ups. While in the air, they release all forward input, turn 180 degrees, and crouch. Roughly 510ms after jumping,<sup id="fnref:cite-jumptime"><a href="#fn:cite-jumptime">[5]</a></sup> they make their first landing. Still crouching and providing no lateral movement input, they jump again, ABH accelerating them from 225ups to a new speed of 285ups. A further 510ms later, they hit the ground again, and with another jump, ABH accelerates them to 405ups. This continues as follows:</p>
<table>
<thead>
<tr>
<th>Time*</th>
<th>Distance*</th>
<th>Speed**</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.00s</td>
<td>0.00u</td>
<td>225ups</td>
</tr>
<tr>
<td>0.51s</td>
<td>114.75u</td>
<td>285ups</td>
</tr>
<tr>
<td>1.02s</td>
<td>260.10u</td>
<td>405ups</td>
</tr>
<tr>
<td>1.53s</td>
<td>466.65u</td>
<td>645ups</td>
</tr>
<tr>
<td>2.04s</td>
<td>795.60u</td>
<td>1125ups</td>
</tr>
<tr>
<td>2.55s</td>
<td>1369.35u</td>
<td>2085ups</td>
</tr>
<tr>
<td>3.06s</td>
<td>2432.70u</td>
<td>3500ups</td>
</tr>
<tr>
<td>3.57s</td>
<td>4217.70u</td>
<td>3500ups</td>
</tr>
<tr>
<td>4.08s</td>
<td>6002.70u</td>
<td>3500ups</td>
</tr>
</tbody>
</table>
<p><small>* from initial jump</small><br>
<small>** during jump</small></p>
<p>On the seventh jump, ABH <em>would</em> give a velocity of 4005ups, but this gets hard-capped to 3500ups, and remains as such for the remainder of the run. 3.865s after the first jump, the player flies across the finish line, and shortly afterwards at 4.08s, they land some 753.36u (14.35m) beyond it.</p>
<p>Note that these timings and distances are relative to the first jump. After the starting pistol is fired, the player takes 200ms to stand from crouching, and an unspecified amount of time and distance (dependent on the ground friction) to reach their 150ups walking speed. However, given the acceleration the player is able to achieve once jumping, this pre-jump phase would have to last at least 5.715s in order for 100m record-holder Usain Bolt to still have a chance of outrunning them.</p>
<p>We hereby propose that the IAAF add a rule explicitly banning the use of ABH in competitions, and submit this article as justification.</p>
<hr>
<h2 id="addendum">Addendum</h2>
<p>Lyren from the SourceRuns team provided two practical runs, one with human inputs [<a href="https://moonbase.lgbt/blog/100m-accelerated-backhopping/100-scriptless.mp4">video</a>], and one with <a href="https://wiki.sourceruns.org/wiki/Host_timescale">host_timescale</a> to slow down time, along with an autojumping script [<a href="https://moonbase.lgbt/blog/100m-accelerated-backhopping/100-scripted.mp4">video</a>].</p>
<p>It could be argued that since the player cannot sprint without the HEV suit, this would constitute an impairment and the suit may be permitted under the Mechanical Aids Regulations. However, as shown, even without the suit, the player clearly has an unfair advantage over conventional human runners; the addition of the suit would just add further insult to injury.</p>
<p>The observant among you will notice that the blog’s accent colors have been changed specifically for this article, to a shade of orange taken from <a href="https://web.archive.org/web/20070803003812fw_/http://orange.half-life2.com/">the Orange Box website</a> circa 2007.</p>

	</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unveiling secrets of the ESP32: creating an open-source MAC layer (228 pts)]]></title>
            <link>https://zeus.ugent.be/blog/23-24/open-source-esp32-wifi-mac/</link>
            <guid>38550026</guid>
            <pubDate>Wed, 06 Dec 2023 21:50:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zeus.ugent.be/blog/23-24/open-source-esp32-wifi-mac/">https://zeus.ugent.be/blog/23-24/open-source-esp32-wifi-mac/</a>, See on <a href="https://news.ycombinator.com/item?id=38550026">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <h2> door <span property="author">Jasper Devreker</span> </h2> <p><small> Geschreven op <span property="dateCreated">2023-12-06</span><br> Leestijd: 18 minuten </small> </p> </div><div property="articleBody"> <article> <div> <p>The ESP32 is a popular microcontroller known in the maker community for its low price (~ €5) and useful features: it has a dual-core CPU, built-in Wi-Fi and Bluetooth connectivity and 520 KB of RAM. It is also used commercially, in devices ranging from smart CO₂-meters to industrial automation controllers. Most of the software development kit that is used to program for the ESP32 <a href="https://github.com/espressif/esp-idf">is open-source</a>, except notably the wireless bits (Wi-Fi, Bluetooth, low-level RF functions): that functionality is distributed as precompiled libraries, that are then compiled into the firmware the developer&nbsp;writes.</p> <p>A closed-source Wi-Fi implementation has several disadvantages compared to an open-source implementation&nbsp;though:</p> <ul> <li>You are dependent on the vendor (Espressif in this case) to add features; if you have a somewhat non-standard usecase, you might be out of luck. For example, standards-compliant mesh networking (IEEE 802.11s) is not supported on the ESP32; there is <a href="https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-guides/esp-wifi-mesh.html">a partially closed-source mesh networking implementation made by Espressif</a>, but this is rather limited: the mesh network has a tree topology, and uses NAT on the nodes connected to the root network, making it hard to connect from outside the mesh network to nodes in the mesh network. The protocol is also not documented, so it’s not interoperable with other&nbsp;devices.</li> <li>It’s hard to audit the security of the implementation: since there is no source code available, you have to resort to black-box fuzzing and reverse engineering to find security&nbsp;vulnerabilities.</li> <li>Additionally, an open-source implementation would make research into low-power Wi-Fi mesh networking more affordable; if each node only costs about €5, research involving hundreds of nodes can be affordable on a modest&nbsp;budget.</li> </ul> <p>Espressif has an open issue in their esp32-wifi-lib repository, asking to open-source the MAC layer. In that issue, they confirmed in 2016 that open sourcing the upper MAC is on their roadmap, but as of 2023, nothing has been published yet. Having the source code would for example allow us to implement proper 802.11s-compliant mesh&nbsp;networking.</p> <h2 id="goals">Goals</h2> <p>The main goal of this project is to build a minimal replacement for Espressifs proprietary Wi-Fi binary blobs. We don’t intend to be API-compatible with existing code that uses the Espressif ESP-IDF API, rather, we’d like to have a fully working, open source networking&nbsp;stack.</p> <p>The rest of this section will contain information about how the network stack and Wi-Fi (the 802.11 standard) works, so if you’re already familiar, you can skip&nbsp;it.</p> <figure> <a href="https://pics.zeus.gent/vYXyQm2t9pJCzpDdWFvq9oWR2DACoUJoTsYf8qiz.jpg"> <img src="https://pics.zeus.gent/vYXyQm2t9pJCzpDdWFvq9oWR2DACoUJoTsYf8qiz.jpg" alt="OSI model of the network stack (the difference between application/presentation/session is a bit murky)"> </a> <figcaption>OSI model of the network stack (the difference between application/presentation/session is a bit murky)</figcaption> </figure> <p>Above, you can see a diagram showing the network stack. Computer networking is done with a network stack, where every layer in the stack has its own purpose; this design makes it easier to swap out layers and allows for separate development of layers. The layer at the bottom of the stack interacts with the physical world (for example, by using radiowaves or electric signals); every layer adds their own features. Wi-Fi (also known as the 802.11 standard by engineers) is implemented in the bottom two layers: the PHY layer (what the radio waveforms look like, …) and the MAC layer (how we connect to an access point, what packets exist, how to send packets to local devices,&nbsp;…).</p> <p>On the ESP32, the PHY layer is implemented in hardware; most of the MAC layer is implemented in the proprietary blob. One notable exception to this separation is sending acknowlegement frame: if a device receives a frame, it should send a packet back to acknowledge that this packet was received correctly. This ACK packet needs to be sent within ~10 microseconds; it would be hard to get this timing correct in&nbsp;software.</p> <p>There are 3 types of MAC&nbsp;frames:</p> <ul> <li>Management frames: mostly for managing the connection between the access point and station&nbsp;(client)</li> <li>Control frames: help with delivery of other types of frames (for example ACK, but also request-to-send and&nbsp;clear-to-send)</li> <li>Data frames: contain the data of the layers above the MAC&nbsp;layer</li> </ul> <h2 id="previous-work">Previous&nbsp;work</h2> <p>Since it doesn’t look like Espressif will release an open source MAC implementation anytime soon, we’re on our own to create this. This is rather hard to do, because the hardware with which we send and receive 802.11 packets on the ESP32 is entirely undocumented. This means that we will need to reverse engineer the hardware; first we’ll need to document what the hardware does, then we’ll need to write our own code to correctly interact with it. In 2021, Uri Shaked did some very light reverse engineering of ESP32 Wi-Fi hardware, to mock this in his emulator. That way, programs for the ESP32 can be emulated instead of running them on real hardware. <a href="https://www.youtube.com/watch?v=XmaT8bMssyQ">Shaked gave a talk about this</a>, but only discussed very high level details about the hardware. Espressif has <a href="https://github.com/espressif/qemu">their own fork of QEMU</a> (a popular, open-source emulator) that can also emulate the ESP32, but this fork does not support emulating the Wi-Fi hardware. In 2022, Martin Johnson added basic support for the Wi-Fi hardware to <a href="https://github.com/a159x36/qemu">their own fork of Espressif’s QEMU</a>. The emulated ESP32 can connect to a virtual access point, or have a virtual client connect to&nbsp;it.</p> <p>esp-idf (the SDK for the ESP32) has a function to transmit frames (<code>esp_wifi_80211_tx</code>), but this function only accepts certain types of frames; it does not allow sending most management frames, severely limiting the usefulness of this API to base an 802.11 MAC stack on. They also have a function (<code>esp_wifi_set_promiscuous_rx_cb</code>) to receive a callback on reception of a&nbsp;frame.</p>  <p>Before we can start reverse engineering how the 802.11 PHY hardware works and how we interact with it, we first need to find or build tools that will help. We’ll use 3 main&nbsp;approaches:</p> <ul> <li>Static reverse engineering: we have the compiled libraries that implement the Wi-Fi stack, so we can look at the compiled code and try to decompile it to human-readable code. From this more readable code, we then try to see what the hardware expects the software to&nbsp;do.</li> <li>Dynamic code analysis in an emulator: we can run the firmware in an emulator and inspect how it interacts with the virtual hardware. This has the advantage of having a lot of freedom to how we inspect the hardware, but the disadvantage that the emulator might not behave the same as real hardware. Since we’ll need to write the emulated peripherals ourselves, this risk is real: there is no public datasheet for the Wi-Fi peripheral, so we have to guess how the hardware will behave from the code that interacts with&nbsp;it.</li> <li>Dynamic code analysis on real hardware: we can run the firmware on an actual ESP32, and debug it using a JTAG debugger. This allows us to place breakpoints, inspect the memory and registers, stop and resume the execution, … The disadvantage is that the debugging capabilities are more limited compared to running in an emulator: we can only place 2 breakpoints, we cannot place watchpoints (breakpoints that trigger on memory reads/writes to a certain address), … The big advantage compared to using an emulator is that we’ll know for sure that the behaviour of the hardware is&nbsp;correct.</li> </ul> <h3 id="static-analysis">Static&nbsp;analysis</h3> <p>For the static analysis, we use Ghidra, an open-source reverse engineering tool made by the NSA. Out of the box, Ghidra does not have support <a href="https://github.com/NationalSecurityAgency/ghidra/pull/5442">yet</a> for Xtensa (the CPU architecture of the ESP32), but there is a <a href="https://github.com/Ebiroll/ghidra-xtensa">plugin that adds support</a>. The build tools used in the ESP32 SDK generate both an ELF file (a type of binary file that can contain metadata) and a flat binary file: using the ELF file has the benefit of automatically setting most function&nbsp;names.</p> <h3 id="dynamic-analysis-in-emulator">Dynamic analysis in&nbsp;emulator</h3> <p>We started off from Martin Johnsons’s fork of Espressifs version of QEMU (a popular open-source emulator), and ported their changes to the latest version of Espressif’s QEMU fork. The ESP32 talks to its peripherals via memory mapped IO: by reading from and writing to certain memory addresses, the peripherals provides information to the CPU and does things. To help in reverse engineering, we added log statements to the QEMU Wi-Fi peripherals that log every access to their memory&nbsp;ranges.</p> <p>Additionally, we also implemented stack unwinding in QEMU; this is done for every memory access to a hardware peripheral related to Wi-Fi. That way, we can get a full stack trace for every peripheral access. Symbols are not stripped, so this is a very useful tool. However, to get stack unwinding properly working, we have to run QEMU in single step mode: QEMU has a JIT compiler that compiles sequences of emulated assembly instructions into optimized basic blocks. This greatly improves the execution speed, but since the CPU execution state is only guaranteed to be correct at the beginning of a basic block, if a peripheral memory access happens in the middle of such a basic block, the stack unwinding algorithm gives wrong&nbsp;results.</p> <p>Running in single-step mode negates much of the benefit of the QEMU JIT compiler, causing the code to run much slower. This is not that big of a disadvantage, compared to the treasure trove of information the execution trace gives&nbsp;us.</p> <p>Below is an example of a single memory access logged by QEMU: it’s a write (<code>W</code>) to address <code>3ff46094</code> with value <code>00010005</code>, done by the function <code>ram_pbus_force_test</code>. The rest of the callstack is also logged, and translated to a symbol name if&nbsp;available.</p> <p><code> W 3ff46094 00010005 ram_pbus_force_test 400044f4 set_rx_gain_cal_dc set_rx_gain_testchip_70 set_rx_gain_table bb_init register_chipv7_phy esp_phy_load_cal_and_init esp_phy_enable wifi_hw_start wifi_start_process ieee80211_ioctl_process ppTask vPortTaskWrapper </code></p> <p>Finally, we also corrected the handling of MAC addresses (compared to Martin Johnsons version), so that a packet capture has correct MAC addresses in packets instead of hardcoded&nbsp;addresses.</p> <h3 id="dynamic-analysis-on-real-hardware">Dynamic analysis on real&nbsp;hardware</h3> <p>To dynamically analyze the firmware on real hardware, we use the JTAG hardware debugging interface. By connecting some jumper wires between the ESP32 and a JTAG debugger, we can debug the ESP32. We followed the steps described in <a href="https://github.com/amirgon/ESP32-JTAG">this GitHub repository</a> to get our JTAG debugger (CJMCU-232H)&nbsp;working.</p> <p>In additon to the JTAG debugger, we also connected a USB Wi-Fi dongle directly to the ESP32: the ESP32-WROOM-32U variant of the ESP32 has an antenna connector. We connect that antenna connector to a 60 dB attenuator (this weakens the signal by 60dB), then connect that to the antenna connector of the wireless dongle. That way we’ll be able to only receive the packets coming from the ESP32, and the ESP32 will only receive packets sent by the wireless&nbsp;dongle.</p> <p>This idea unfortunately did not entirely work: enough radio waves from outside access points leaked into the antenna connector that the wireless dongle also receieved their packets. We tried to build a low-cost faraday cage from a paint can to prevent this, but this only attenuated outside signals with an extra 10dB: this removed some APs, but not all of them. The current solution is definitely not ideal, so we’ve started work on building a better and larger faraday cage, from conducting fabric and with fiber-optic data&nbsp;communication.</p> <figure> <a href="https://pics.zeus.gent/rqJc7p6pSbb6FInNNyadKy2tZy2uWqDaFtuU5KPx.jpg"> <img src="https://pics.zeus.gent/rqJc7p6pSbb6FInNNyadKy2tZy2uWqDaFtuU5KPx.jpg" alt="Wi-Fi dongle connected to the ESP32, with two 30 dB attenuators in between"> </a> <figcaption>Wi-Fi dongle connected to the ESP32, with two 30 dB attenuators in between</figcaption> </figure> <figure> <a href="https://pics.zeus.gent/ttmZlo7MpEP6CDzzc5q0QX4iVrg3vlsVFUAB6LEU.jpg"> <img src="https://pics.zeus.gent/ttmZlo7MpEP6CDzzc5q0QX4iVrg3vlsVFUAB6LEU.jpg" alt="Faraday cage made from a paint tin, with copper tape to close the hole for the USB connectors, and ferrite chokes to reduce the RF leaking in"> </a> <figcaption>Faraday cage made from a paint tin, with copper tape to close the hole for the USB connectors, and ferrite chokes to reduce the RF leaking in</figcaption> </figure> <h2 id="architecture">Architecture</h2> <h3 id="softmac-vs-hardmac">SoftMAC vs&nbsp;HardMAC</h3> <p>SoftMAC (Software MAC) and HardMAC (Hardware MAC) refer to two different approaches for implementing the MAC layer for Wi-Fi. SoftMAC relies on software to manage MAC layer functions, which offers flexibility and ease of modification but can consume more power/CPU cycles. HardMAC, on the other hand, offloads MAC layer processing to dedicated hardware, reducing CPU usage and power consumption but limiting the ability to adapt to new features without hardware&nbsp;changes.</p> <p>The ESP32 seems to use a SoftMAC approach: you can directly send and receive 802.11 frames (instead of with HardMAC, where you tell the hardware you want to connect to a certain AP, and it would then automatically craft the nescessary frames and send them). This is good news for our open source implementation, since there already exist open-source 802.11 MAC stacks for SoftMAC (for example, mac80211 in the Linux&nbsp;kernel).</p> <h3 id="peripherals">Peripherals</h3> <p>The Wi-Fi functionality is implemented via multiple hardware peripherals, each responsible for a separate part of the functionality. Through reverse engineering, the following peripherals were identified as ‘used for Wi-Fi functionaliy’ (these are memory addresses, through which the peripherals can be&nbsp;accessed):</p> <ul> <li>MAC peripherals, at 0x3ff73000 to 0x3ff73fff and at 0x3ff74000 to&nbsp;0x3ff74fff</li> <li>RX control registers, at 0x3ff5c000 to&nbsp;0x3ff5cfff</li> <li>baseband, at 0x3ff5d000 to&nbsp;0x3ff5dfff</li> <li> <code>chipv7_phy</code> (?) at 3ff71000 to&nbsp;3ff71fff</li> <li> <code>chipv7_wdev</code> (?) at 3ff75000 to&nbsp;3ff75fff</li> <li>RF frontend, at 3ff45000 to 3ff45fff and 3ff46000 to&nbsp;3ff46fff</li> <li>analog at 3ff4e000 to 3ff4efff (this is also used by the DAC connected to GPIO&nbsp;pins)</li> </ul> <p>It should be noted that these peripherals are mirrored to another place in the address&nbsp;space:</p> <blockquote> <p>Peripherals accessed by the CPU via 0x3FF40000 ~ 0x3FF7FFFF address space (DPORT address) can also be accessed via 0x60000000 ~ 0x6003FFFF (AHB address). (0x3FF40000 + n) address and (0x60000000 + n) address access the same content, where n = 0 ~&nbsp;0x3FFFF.</p> </blockquote> <h3 id="lifecyle">Lifecyle</h3> <p>By writing some minimal firmware that just sends packets in a loop and using the three reverse engineer strategies described earlier, a high level overview of the Wi-Fi hardware lifecycle for sending a packet was&nbsp;determined:</p> <ol> <li>Calling <code>esp_wifi_start()</code>, this indirectly calls&nbsp;<code>esp_phy_enable()</code> </li> <li> <code>esp_phy_enable()</code> is responsible for initializing the wifi&nbsp;hardware: <ol> <li>Calibrate the PHY hardware: this tries to compensate imperfections of the hardware. According to the data sheet, this does, at least: I/Q phase matching; antenna matching; compensating carrier leakage, baseband nonlinearities, power amplifier nonlinearities and RF nonlinearities (I’m more of a software person than an electronic engineer, so I don’t exactly know what these terms mean). This calibration can be stored to the non-volatile storage and to memory. This is used so we don’t have to do a full calibration every time the ESP32 wakes up from modem&nbsp;sleep.</li> <li>Initialize the MAC peripherals: set RX MAC address filters, set the buffers where the packets will be received into, set the auto-ACKing policy, set the chips own MAC&nbsp;address.</li> <li>Set various physical radio properties (TX rate, frequency, TX power,&nbsp;…)</li> <li>Set up the power management timer: if packets are not sent often enough, the modem power save timer kicks in and de-initializes part of the Wi-Fi hardware to save&nbsp;power.</li> </ol> </li> <li>Now, we’re ready to send a&nbsp;packet: <ol> <li>Wake up some Wi-Fi peripherals from deep sleep and restore their calibration, if we need&nbsp;to</li> <li>Set some metadata, related to the packet (likely the rate and other PHY&nbsp;settings)</li> <li>Create a DMA entry, consisting of the length of the packet and the address of the buffer containing the MAC data. The MAC Frame Checksum is automatically calculated by the hardware. DMA stands for Direct Memory Access: that means that we just tell the hardware the address and length of where our packet is, and the hardware will then read that memory and transmit the packet, all on its&nbsp;own.</li> <li>Write the lowest bits of the DMA entry into a hardware register, then enable it for transmission by setting a bit in the bitmask of that&nbsp;register.</li> <li>Once the packet is sent, interrupt 0 will fire to notify us how succesful the transmission was. We can react to collisions and timeouts (and probably also to ACKs received?). We also have to clear the interrupt bit that indicates a packet was&nbsp;sent.</li> </ol> </li> </ol> <h3 id="implementing-transmitting-packets">Implementing transmitting&nbsp;packets</h3> <p>As a (very limited) proof-of-concept, we wanted to send arbitrary 802.11 frames by directly using the memory mapped peripherals, so without using the SDK functions. As you can see in the lifecycle diagram above, before transmitting, we first need to initialize the wifi hardware. Unfortunately, this initialization is a lot more complex than sending packets: to intialize the hardware, about 50000 peripheral memory accesses are needed, compared to about 50 for transmitting a packet (including handling the interrupt). These are not exact numbers at all, but they give an idea about the complexity&nbsp;involved.</p> <p>For the basic ‘transmitting packets’ proof-of-concept, we are currently still using the proprietary functions to initialize the wifi hardware. We encountered the issue that after initializing, the modem power save timer would kick in and de-initialize the wifi peripherals, preventing us from sending packets. To work around this, we send a single packet using the SDK and then immediately call the undocumented <code>pm_disconnected_stop()</code> function, which disables the modem power save mode timer. After this, we can send arbitrary packets by directly writing to the MAC peripheral addresses. For this PoC, we don’t need to replace the interrupt handler for wifi events: the existing, proprietary handler will handle the ‘packet was sent’ interrupt just&nbsp;fine.</p> <p>The <a href="https://github.com/esp32-open-mac/esp32-open-mac">basic proof of concept</a> works, we can transmit arbitrary packets by directly writing and reading from memory&nbsp;addresses!</p> <h2 id="current-roadmap">Current&nbsp;roadmap</h2> <p>Now we can transmit packets, but we still have a lot of work ahead of us: this is the to-do list, in rough order of&nbsp;priorities</p> <ul> <li>☑ Send&nbsp;packets</li> <li>☐ Receive packets: to do this, we will need to do the&nbsp;following: <ul> <li>Set the RX policy (this filters packets based on MAC address) / enable promiscous mode to receive all&nbsp;packets</li> <li>Set the memory address in which we want to receive the packet via&nbsp;DMA</li> <li>Replace the wifi interrupt with our own interrupt; the code indicates that there might be some kind of wifi watchdog, we’ll need to figure out how to pet&nbsp;it.</li> </ul> </li> <li>☐ Send ACK (acknowledgment) packets back if we receive a packet that is destined for&nbsp;us</li> <li>☐ Implement changing the wifi channel, rate, transmit power,&nbsp;…</li> <li>☐ Combine our implementation with an existing open source 802.11 MAC stack, so the ESP32 can associate with access&nbsp;points</li> <li>☐ Implement the hardware initialization (now done by <code>esp_phy_enable()</code>). This will be a hard undertaking, since all calibration routines will need to be implemented, but also has a high payoff: we’ll then have a completely blob-free firmware for the&nbsp;ESP32.</li> </ul> <p>And a list of possible future extensions that are not yet on the roadmap, but are useful to do&nbsp;anyways:</p> <ul> <li>☐ Implement modem power saving: turning off the modem when not in&nbsp;use</li> <li>☐ AMSDU, AMPDU, HT40,&nbsp;QoS</li> <li>☐ Do the cryptography needed for WPA2 etc in hardware instead of in&nbsp;software</li> <li>☐&nbsp;Bluetooth</li> <li>☐ Write SVD documentation for all reverse engineered registers. An SVD file is an XML file that describes the hardware features of a microcontroller, this makes it possible to automatically generate an API from the hardware description. Espressif already has an SVD file containing the documented hardware registers; we can document the undocumented registers and (automatically) merge them&nbsp;in.</li> </ul> <h2 id="code">Code</h2> <p>All code and documentation is available in the <a href="https://github.com/esp32-open-mac/">esp32-open-mac GitHub organisation</a>. I think especially the QEMU fork can be useful for other reverse engineers because of the memory tracing&nbsp;feature.</p> <h2 id="update">Update</h2> <p>Since the beginning of writing this blog post, receiving packets was also implemented. To accomplish this, we needed to implement the Wi-Fi MAC interrupt handler and manage the RX DMA buffers. This means that we now can send and receive packets using only open source code: the hardware initialization is still done with proprietary code, but after this setup is done, only open source code is used to send and receive packets, no more proprietary code is executed. The second part is <a href="https://zeus.ugent.be/blog/23-24/esp32-reverse-engineering-continued/">here</a></p> <h2 id="questions-want-to-collaborate">Questions? Want to&nbsp;collaborate?</h2> <p>This is a sizeable project that could definitely use multiple contributors; I’d really like to collaborate with other people to create a fully functional, open-source Wi-Fi stack for the ESP32. If this sounds like something you’d like to work on, contact me via <a href="mailto:%7a%65%75%73%62%6c%6f%67%40%64%65%76%72%65%6b%65%72%2e%62e">zeusblog@<span>not</span>devreker.be</a>, maybe we can have a weekly hacking&nbsp;session?</p> <p>As far as I know, this is the first undertaking to build an open source 802.11 MAC for an affordable microcontroller. If you want to financially support this project, you can wire money via https://zeus.ugent.be/contact/#payment-info, please put “ESP32” in the transaction description, so our treasurer knows what the money is for. Please do not donate if you’re a student or if you’re not financially independent. If you’re a company and would like to donate hardware (for example, a faraday cage or measuring equipment that might be useful), please contact&nbsp;me.</p> <p><a href="https://nlnet.nl/project/ESP32-opendrivers/">This project</a> was funded through the <a href="https://nlnet.nl/core/">NGI0 Core Fund</a>, a fund established by NLnet with financial support from the European Commission’s Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No&nbsp;101092990.</p> <p>Feel free to send me an email in case you have questions, you think something in this blog post could be worded better or you spotted a&nbsp;mistake.</p> </div>  </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework Laptop 16 Deep Dive – Connectors (117 pts)]]></title>
            <link>https://frame.work/blog/framework-laptop-16-deep-dive---connectors</link>
            <guid>38549388</guid>
            <pubDate>Wed, 06 Dec 2023 21:00:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frame.work/blog/framework-laptop-16-deep-dive---connectors">https://frame.work/blog/framework-laptop-16-deep-dive---connectors</a>, See on <a href="https://news.ycombinator.com/item?id=38549388">Hacker News</a></p>
Couldn't get https://frame.work/blog/framework-laptop-16-deep-dive---connectors: Error: Parse Error: Header overflow]]></description>
        </item>
        <item>
            <title><![CDATA[Quad9 wins appeal against Sony (293 pts)]]></title>
            <link>https://quad9.net/news/blog/quad9-turns-the-sony-case-around-in-dresden/</link>
            <guid>38548209</guid>
            <pubDate>Wed, 06 Dec 2023 19:11:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quad9.net/news/blog/quad9-turns-the-sony-case-around-in-dresden/">https://quad9.net/news/blog/quad9-turns-the-sony-case-around-in-dresden/</a>, See on <a href="https://news.ycombinator.com/item?id=38548209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-13a3556e=""><p>Today marks a bright moment in the efforts to keep the internet a neutral and trusted resource for everyone.</p>
<p>Quad9 has received word from the courts in Dresden, Germany in the appeal of our case versus Sony Entertainment (Germany). The court has ruled in favor of Quad9, clearly and unequivocally. The text from the court is available here <a href="https://quad9.net/uploads/URT_05_12_2023_redacted_de2_1880f3f6c4.pdf">in German</a> and <a href="https://quad9.net/uploads/URT_05_12_2023_en_Korr_MH_en2_2e629b1f7b.pdf">in English</a>.</p>
<p>Needless to say, we are elated at the news.</p>
<h2>Background</h2>
<p>Sony Entertainment (Germany) started a legal proceeding against Quad9 more than two years ago to force Quad9 to stop resolving certain domain names which they claimed were involved in copyright infringement behavior.</p>
<ul>
<li><a href="https://www.quad9.net/news/blog/quad9-and-sony-music-german-injunction-update-for-july-2023/" target="_blank">07/2023</a></li>
<li><a href="https://quad9.net/news/blog/sony-s-legal-attack-on-quad9-censorship-and-freedom-of-speech/" target="_blank">03/2023</a></li>
<li><a href="https://quad9.net/news/blog/quad9-and-sony-music-german-injunction-update-for-february-2023/" target="_blank">02/2023</a></li>
<li><a href="https://quad9.net/news/blog/an-update-to-the-quad9-and-sony-music-german-court-injunction-august-2022/" target="_blank">08/2022</a></li>
<li><a href="https://quad9.net/news/blog/quad9-files-official-objection-opposing-sony-music-s-german-court-ruling/" target="_blank">09/2021</a></li>
</ul>
<p>We believe this lawsuit was an attempt to set a precedent, such that commercial rights holders could demand that sites on the internet be made unreachable by forcing recursive resolvers to block content. We contended that recursive resolvers have no commercial or even remotely indirect relationship to any of the infringing parties, and that Sony’s demand for blocking was ineffective, inappropriately specified, and not related to Quad9.</p>
<p>What made this case more problematic, in our view, was that the servers in question in this case were not located in Germany, and the links they pointed to were on servers also not in Germany. The domain name (<code>canna.to</code>) was not registered in Germany and was under the top-level-domain operated by the nation of Tonga. Sony Entertainment further asserted that we block the domains globally, not just in Germany, as geoIP does not block for users based in Germany with certainty. For that matter, Quad9 has no office or standing in Germany (we are a Swiss entity), but due to the Lugano Convention treaty it was possible for Sony to serve an injunction in Switzerland and drag Quad9 into legal proceedings.</p>
<h2>Details on Judgement</h2>
<p>The appeal with the Higher Regional Court in Dresden follows a decision by the Regional Court in Leipzig, in which Sony prevailed, and Quad9 was convicted as a wrongdoer. Before that, Sony successfully obtained a preliminary junction against Quad9 with the Regional Court in Hamburg. The objection against the preliminary injunction by Quad9 was unsuccessful, and the appeal with the Higher Regional Court in Hamburg was withdrawn by Quad9 since a decision in the main proceeding was expected to be made earlier than the conclusion of the appeal in the preliminary proceedings.</p>
<p>Please find below a table summarizing the main take-aways from the decision in Dresden. For more information on these points, please go <a href="https://quad9.net/uploads/2023_12_06_Comparison_of_processes_aad2df4618.pdf">here</a>.</p>
<table>
<thead>
<tr>
<th>Leipzig</th>
<th>Dresden</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>Extending the motion to “and/or the other domain(s) is ok</td>
<td>“and/or the other domain(s)” is too unspecific</td>
<td>Win</td>
</tr>
<tr>
<td>DNS resolver plays a "central role" in the publication of the copyrighted material leading to liability as a wrongdoer</td>
<td>No liability as a perpetrator: DNS resolver does not play a “central role”</td>
<td>Win</td>
</tr>
<tr>
<td>No Störerhaftung as Quad9 was held liable as a wrongdoer</td>
<td>No Störerhaftung</td>
<td>Win</td>
</tr>
<tr>
<td>Liability privileges are not applicable as DNS resolver operators are not deemed service providers under the Telemedia Services Act</td>
<td>DNS resolver operators enjoy liability privileges as access providers</td>
<td>Win</td>
</tr>
<tr>
<td>Subsidiarity criteria fulfilled</td>
<td>Subsidiarity criteria not fulfilled: Sony has not done enough to go after the hosting company</td>
<td>Win</td>
</tr>
</tbody>
</table>
<p>The court has also ruled that the case cannot be taken to a higher court and their decision is the final word in this particular case. Sony may appeal the appeal closure via a complaint against the denial of leave of appeal and then would have to appeal the case itself with the German Federal Court. So while there is still a possibility that this case could continue, Sony would have to win twice to turn the decision around again.</p>
<p>We would also like to clarify that even though Quad9 benefits from the liability privileges as a mere conduit, it is possible that a DNS resolver operator can be required to block as a matter of last resort if the claiming party has taken appropriate means to go after the wrongdoer and the hosting company unsuccessfully. Such measures could be legal action by applying for a preliminary injunction against a hosting company within the EU. These uncertainties still linger, and we expect that this ongoing question of what circumstances require what actions, by what parties, will continue to be argued in court and in policy circles over the next few years.</p>
<p>We remain committed to the concept that resolving a domain name is not an action that should be prohibited for commercial goals. The DNS does not contain content - it is a system designed for delivery only of pointers, not for data transport.</p>
<p>The courts in Cologne also recently ruled in favor of Cloudflare in a <a href="https://blog.cloudflare.com/latest-copyright-decision-in-germany-rejects-blocking-through-global-dns-resolvers/" target="_blank">similar case</a> involving DNS recursive resolution (though that case is more complex as it involves content hosting or proxying) and we are pleased to have consistent and clear statements from both courts in this matter of DNS recursive resolution.</p>
<p>Today was a significant win in Germany, but there is some disappointment as well. We received a notice from a consortium of Italian rightsholders (Sony Music Italy, Universal Music Italy, Warner Music Italy, and Federation of Italian Music Industry) who have demanded that Quad9 block domains in Italy, and there is potentially another court process ahead of us. We have a blog post on that <a href="https://quad9.net/news/blog/italian-blocking-demands-following-a-bad-example">here</a>, also published today.</p>
<h2>Actions taken</h2>
<p>From a practical perspective, Quad9 has removed the blocks on all domains previously noted by Sony and documented in the lower courts. Those domains are listed below. Currently, there are no domains now being blocked in Germany or anywhere else related to this case.</p>
<h2>Thanks to those who believed in us and in your rights</h2>
<p>There are hundreds of individual supporters that we wish to thank for their monetary support, advice, and willingness to speak up in public - social media, blogs, and in industry settings. In particular, we would like to thank our legal team at <a href="https://rickert.law/" target="_blank">Rickert.law</a> - Thomas Rickert and Sandra Schulte - who gave us an enormously generous amount of effort and time spent on achieving this win. We'd also like to thank the <a href="https://freiheitsrechte.org/" target="_blank">GFF</a> for their legal advice, continued advisement on German media issues, and for being a sounding board for our legal team and management. We would also like to thank the Kahle Austin Family Foundation, and <a href="https://www.stiftung-mercator.ch/" target="_blank">Stiftung Mercator Schweiz</a>, and <a href="https://www.eco.de/" target="_blank">eco</a> for their support.</p>
<p>To those who have donated to Quad9: By helping us, you helped yourself. It is clear to us that expansionism of internet censorship will never stop and letting minor inappropriate removals of digital sovereignty go without challenge will ultimately end with heavy-handed suppression of ideas much closer to the user. Those boundaries are constantly being tested, and we have won this round. You have kept the front lines from moving closer to you - congratulations!</p>
<p>Quad9 is a non-profit organization, and the costs of pursuing court cases is a heavy burden which takes away our ability to expand the network as quickly as we need. We will continue to need that help in the next phases of our defense of the DNS. Please consider <a href="https://www.quad9.net/donate" target="_blank">donating</a> - every euro counts as we continue to fight for your ability to use the DNS. Thank you for making this possible for us.</p>
<p>Regardless of our victory in this case, the costs to Quad9 have been extensive and excessive from a real monetary perspective as well as a time and attention drain. If you feel like we have done service for you in this court case, please consider <a href="https://www.quad9.net/donate" target="_blank">donating</a> to Quad9 so that we can catch up to where we should have been by now with other costs related to our continued service.</p>
<h2>Documents / Further Reading / Details</h2>
<p>You may find the original case documents <a href="https://quad9.net/uploads/URT_05_12_2023_redacted_de2_1880f3f6c4.pdf">here</a>.<br>
You may find an English machine-translated version <a href="https://quad9.net/uploads/URT_05_12_2023_en_Korr_MH_en2_2e629b1f7b.pdf">here</a>. Please note the English version may not be exact, and final references should be made to the German original.</p>
<p>In addition to the case documentation itself issued by the Court, Rickert.law has broken the arguments down into more specific discussions on several points, which may be of interest to readers who wish a more complete understanding of the points of law, history of the case, or understanding of the judgment.  A German-language version may be found <a href="https://quad9.net/uploads/2023_12_06_Verfahrensuebersicht_dfe6a95669.pdf">here</a> and the English version is <a href="https://quad9.net/uploads/2023_12_06_Comparison_of_processes_aad2df4618.pdf">here</a>.</p>
<p>List of domains now unblocked: <code>canna.to</code>, <code>www.canna.to</code>, <code>uu.canna.to</code>, <code>www.uu.canna.to</code>, <code>canna-power.to</code>, <code>www.canna-power.to</code>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikifunctions (272 pts)]]></title>
            <link>https://wikimediafoundation.org/news/2023/12/05/introducing-wikifunctions-first-wikimedia-project-to-launch-in-a-decade-creates-new-forms-of-knowledge/</link>
            <guid>38548130</guid>
            <pubDate>Wed, 06 Dec 2023 19:04:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikimediafoundation.org/news/2023/12/05/introducing-wikifunctions-first-wikimedia-project-to-launch-in-a-decade-creates-new-forms-of-knowledge/">https://wikimediafoundation.org/news/2023/12/05/introducing-wikifunctions-first-wikimedia-project-to-launch-in-a-decade-creates-new-forms-of-knowledge/</a>, See on <a href="https://news.ycombinator.com/item?id=38548130">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The Wikimedia Foundation — the nonprofit that hosts Wikipedia and other <a href="https://wikimediafoundation.org/our-work/wikimedia-projects/" rel="noreferrer noopener">Wikimedia projects</a> — has announced the launch of <a href="https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page" rel="noreferrer noopener">Wikifunctions</a>, its first new project in over a decade. The project will enable volunteer editors to collaboratively create and maintain a library of functions to answer questions and enhance knowledge on Wikimedia projects and beyond.</p><article>
		


<p>A “function” is a sequence of programming instructions that makes a calculation based on data provided. Internet users most commonly encounter functions when entering queries on search engines, such as the time difference between two cities, the distance between two locations, or the volume of an object. Functions operate behind-the-scenes to produce answers to these queries. For the first time, Wikifunctions will provide a library of functions that everyone, everywhere can access and contribute to.&nbsp;</p>



<p>Notably, these functions can exist in any language; therefore, for many Wikifunctions users, this will be the first project where they can read and write functions in their native language.</p>



<blockquote>
<p>“Functions are pathways to knowledge. Wikifunctions aims to make these pathways more accessible than ever before.&nbsp; Imagine a programming system where a single line of code can be run from anywhere by anyone — even by programs written in different programming languages. That is the promise and potential of Wikifunctions,” said Dr. Denny Vrandečić, Head of Special Projects at the Wikimedia Foundation. “Wikifunctions will be built in the same community-led fashion as Wikipedia, with volunteer editors donating their time and energy to the cause of building a freely editable library of valuable functions. We are excited for this project to continue to grow with their contributions.”</p>
</blockquote>



<p>Currently, functions on Wikimedia projects are complex, siloed, and vary by language versions of various projects (Wikipedia alone has over 300 language versions). Wikifunctions will place functions in a single shared space, simplifying the work of the volunteers who maintain them and increasing their accessibility. Wikifunctions will eventually integrate with Wikipedia and other Wikimedia projects, opening new opportunities for knowledge creation.</p>



<blockquote>
<p>&nbsp;“Wikifunctions will bring together different data sources from other Wikimedia projects in new and powerful ways, ultimately leveraging functional code to create new forms of knowledge,”&nbsp; said <a href="https://wikimediafoundation.org/en/?profile=selena-deckelmann" target="_blank" rel="noreferrer noopener">Selena Deckelmann</a>, Chief Product and Technology Officer at the Wikimedia Foundation. “We are excited to announce Wikifunctions as yet another step in getting us closer to <a href="https://wikimediafoundation.org/about/vision/" target="_blank" rel="noreferrer noopener">our vision</a> of a world where everyone can freely share in the sum of all knowledge.”</p>
</blockquote>



<p>Wikifunctions is the underlying technical infrastructure that will support a <a href="https://meta.wikimedia.org/wiki/Abstract_Wikipedia" target="_blank" rel="noreferrer noopener">wider initiative</a> by the Wikimedia Foundation to enable people to share more knowledge in more languages across Wikipedia.&nbsp; Through this initiative, users will be able to create and maintain content in their native language, which others can access in over 300 languages available on Wikimedia projects. The long-term aim of this effort is to create knowledge that is independent of language, and easier for Wikipedia editors to share, add, translate, and improve across languages on the online encyclopedia. The work is being supported by grants from <a href="https://www.google.org/" target="_blank" rel="noreferrer noopener">Google.org</a>, <a href="https://www.google.org/" target="_blank" rel="noreferrer noopener">The Rockefeller Foundation</a>, and <a href="https://wikimediaendowment.org/" target="_blank" rel="noreferrer noopener">the Wikimedia Endowment</a>.</p>



<p>Wikifunctions was approved by the Wikimedia Foundation’s Board of Trustees <a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/Abstract_Wikipedia/July_2020_announcement" target="_blank" rel="noreferrer noopener">in 2020</a>. The project went live as a read-only site earlier this year, and it is now available so that anyone, anywhere can use it. It is the fourteenth Wikimedia project — the first new project in a decade. Learn more about the project at <a href="https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page" target="_blank" rel="noreferrer noopener">wikifunctions.org</a>.</p>



<h3><strong>About the Wikimedia Foundation</strong></h3>



<p><a href="https://wikimediafoundation.org/" target="_blank" rel="noreferrer noopener">The Wikimedia Foundation</a> is the nonprofit organization that operates Wikipedia and other Wikimedia free knowledge projects. Our vision is a world in which every single human can freely share in the sum of all knowledge. We believe that everyone has the potential to contribute something to our shared knowledge and that everyone should be able to access that knowledge freely. We host Wikipedia and the Wikimedia projects; build software experiences for reading, contributing, and sharing Wikimedia content; support the volunteer communities and partners who make Wikimedia possible. The Wikimedia Foundation is a United States 501(c)(3) tax-exempt organization with offices in San Francisco, California, USA.</p>



<p><strong>For media inquiries, please contact </strong><a href="mailto:press@wikimedia.org"><strong>press@wikimedia.org</strong></a><strong>.</strong></p>

			
	
	
		</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swap OpenAI with any open-source model (123 pts)]]></title>
            <link>https://postgresml.org/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes</link>
            <guid>38547556</guid>
            <pubDate>Wed, 06 Dec 2023 18:17:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://postgresml.org/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes">https://postgresml.org/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes</a>, See on <a href="https://news.ycombinator.com/item?id=38547556">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
<div>
<figure><img src="https://postgresml.org/blog/.gitbook/assets/image.png" alt="Author" width="100"><figcaption></figcaption></figure>
</div>
<p>Cassandra Stumer and Silas Marvin</p>
<p>December 1, 2023</p>

<p>Last week's whirlwind of events with OpenAI CEO and founder Sam Altman stirred up quite a buzz in the industry. The whole deal left many of us scratching our heads about where OpenAI is headed. Between the corporate drama, valid worries about privacy and transparency, and ongoing issues around model performance, censorship, and the use of marketing scare tactics; it's no wonder there's a growing sense of dissatisfaction and distrust in proprietary models. </p>
<p>On the bright side, the open-source realm has emerged as a potent contender, not just in reaction to OpenAI's shortcomings but as a genuine advancement in its own right. We're all about making the benefits of open-source models accessible to as many folks as possible. So, we've made switching from OpenAI to open-source as easy as possible with a drop-in replacement. It lets users specify any model they’d like in just a few lines of code. We call it the OpenAI Switch Kit. Read on to learn more about why we think you’ll like it, or just try it now and see what you think.</p>

<p>We think so. Open-source models have made remarkable strides, not only catching up to proprietary counterparts but also surpassing them across multiple domains. The advantages are clear:</p>
<ul>
<li><strong>Performance &amp; reliability:</strong> Open-source models are increasingly comparable or superior across a wide range of tasks and performance metrics. Mistral and Llama-based models, for example, are easily faster than GPT 4. Reliability is another concern you may reconsider leaving in the hands of OpenAI. OpenAI’s API has suffered from several recent outages, and their rate limits can interrupt your app if there is a surge in usage. Open-source models enable greater control over your model’s latency, scalability and availability. Ultimately, the outcome of greater control is that your organization can produce a more dependable integration and a highly reliable production application. </li>
<li><strong>Safety &amp; privacy:</strong> Open-source models are the clear winner when it comes to security sensitive AI applications. There are <a href="https://www.infosecurity-magazine.com/news-features/chatgpts-datascraping-scrutiny/">enormous risks</a> associated with transmitting private data to external entities such as OpenAI. By contrast, open-source models retain sensitive information within an organization's own cloud environments. The data never has to leave your premises, so the risk is bypassed altogether – it’s enterprise security by default. At PostgresML, we offer such private hosting of LLM’s in your own cloud. </li>
<li><strong>Model censorship:</strong> A growing number of experts inside and outside of leading AI companies argue that model restrictions have gone too far. The Atlantic recently published an <a href="https://www.theatlantic.com/ideas/archive/2023/11/ai-safety-regulations-uncensored-models/676076/">article on AI’s “Spicy-Mayo Problem'' </a> which delves into the issues surrounding AI censorship. The titular example describes a chatbot refusing to return commands asking for a “dangerously spicy” mayo recipe. Censorship can affect baseline performance, and in the case of apps for creative work such as Sudowrite, unrestricted open-source models can actually be a key differentiating value for users. </li>
<li><strong>Flexibility &amp; customization:</strong> Closed-source models like GPT3.5 Turbo are fine for generalized tasks, but leave little room for customization. Fine-tuning is highly restricted. Additionally, the headwinds at OpenAI have exposed the <a href="https://techcrunch.com/2023/11/21/openai-dangers-vendor-lock-in/">dangerous reality of AI vendor lock-in</a>. Open-source models such as MPT-7B, Llama V2 and Mistral 7B are designed with extensive flexibility for fine tuning, so organizations can create custom specifications and optimize model performance for their unique needs. This level of customization and flexibility opens the door for advanced techniques like DPO, PPO LoRa and more. </li>
</ul>

<p>The Switch Kit is an open-source AI SDK that provides a drop in replacement for OpenAI’s chat completion endpoint.</p>

                                <ul>
                            
            <li role="presentation">
                
            </li>
        
            <li role="presentation">
                
            </li>
        </ul><div>
                                                <div id="tab-QhZDJaD6d1" role="tabpanel" aria-labelledby="tab-QhZDJaD6d1">
                                            
<pre data-controller="copy"><p><span data-action="click->copy#codeCopy">content_copy</span>
                <span disabled="">link</span>
                <span disabled="">edit</span>
            </p><code><div><p><span><span>import </span>pgml</span></p>
<p><span>client = pgml.OpenSourceAI()</span></p>
<p><span>results = client.chat_completions_create(</span></p>
<p><span>    "HuggingFaceH4/zephyr-7b-beta",</span></p>
<p><span>    [</span></p>
<p><span>        {</span></p>
<p><span>            "role": "system",</span></p>
<p><span>            "content": "You are a friendly chatbot who always responds in the style of a pirate",</span></p>
<p><span>        },</span></p>
<p><span>        {</span></p>
<p><span>            "role": "user",</span></p>
<p><span>            "content": "How many helicopters can a human eat in one sitting?",</span></p>
<p><span>        },</span></p>
<p><span>    ],</span></p>
<p><span>    temperature=0.85,</span></p>
<p><span>)</span></p>
<p><span>print(results)</span></p></div></code></pre>


</div>
                                                <div id="tab-guHjDjhvfQ" role="tabpanel" aria-labelledby="tab-guHjDjhvfQ">
                                            
<pre data-controller="copy"><p><span data-action="click->copy#codeCopy">content_copy</span>
                <span disabled="">link</span>
                <span disabled="">edit</span>
            </p><code><div><p><span>const pgml = require("pgml");</span></p>
<p><span>const client = pgml.newOpenSourceAI();</span></p>
<p><span>const results = client.chat_completions_create(</span></p>
<p><span>      "HuggingFaceH4/zephyr-7b-beta",</span></p>
<p><span>      [</span></p>
<p><span>          {</span></p>
<p><span>              role: "system",</span></p>
<p><span>              content: "You are a friendly chatbot who always responds in the style of a pirate",</span></p>
<p><span>          },</span></p>
<p><span>          {</span></p>
<p><span>              role: "user",</span></p>
<p><span>              content: "How many helicopters can a human eat in one sitting?",</span></p>
<p><span>          },</span></p>
<p><span>      ],</span></p>
<p><span>);</span></p>
<p><span>console.log(results);</span></p></div></code></pre>

</div></div>
<pre data-controller="copy"><p><span data-action="click->copy#codeCopy">content_copy</span>
                <span disabled="">link</span>
                <span disabled="">edit</span>
            </p><code><div><p><span>{</span></p>
<p><span>  "choices": [</span></p>
<p><span>    {</span></p>
<p><span>      "index": 0,</span></p>
<p><span>      "message": {</span></p>
<p><span>        "content": "Me matey, ya landed in me treasure trove o' riddles! But sorry to say, me lads, humans cannot eat helicopters in a single setting, for helicopters are mechanical devices and not food items. So there's no quantity to answer this one! Ahoy there, any other queries ye'd like to raise? Me hearty, we're always at yer service!",</span></p>
<p><span>        "role": "assistant"</span></p>
<p><span>      }</span></p>
<p><span>    }</span></p>
<p><span>  ],</span></p>
<p><span>  "created": 1701291672,</span></p>
<p><span>  "id": "abf042d2-9159-49cb-9fd3-eef16feb246c",</span></p>
<p><span>  "model": "HuggingFaceH4/zephyr-7b-beta",</span></p>
<p><span>  "object": "chat.completion",</span></p>
<p><span>  "system_fingerprint": "eecec9d4-c28b-5a27-f90b-66c3fb6cee46",</span></p>
<p><span>  "usage": {</span></p>
<p><span>    "completion_tokens": 0,</span></p>
<p><span>    "prompt_tokens": 0,</span></p>
<p><span>    "total_tokens": 0</span></p>
<p><span>  }</span></p>
<p><span>}</span></p></div></code></pre>

        <div>
            
        
<p>We don't charge per token, so OpenAI “usage” metrics are not particularly relevant. We'll be extending this data with more direct CPU/GPU resource utilization measurements for users who are interested, or need to pass real usage based pricing on to their own customers.</p>

                                    </div>
                                    
<p>The above is an example using our open-source AI SDK with zephyr-7b-beta, an incredibly popular and highly efficient 7 billion parameter model.</p>
<p>Notice there is near one to one relation between the parameters and return type of OpenAI’s <code>chat.completions.create</code> and our <code>chat_completion_create</code>.</p>
<p>The best part of using open-source AI is the flexibility with models. Unlike OpenAI, we are not restricted to using a few censored models, but have access to almost any model out there.</p>
<p>Here is an example of streaming with the popular Mythalion model, an uncensored MythoMax variant designed for chatting.</p>

                                <ul>
                            
            <li role="presentation">
                
            </li>
        
            <li role="presentation">
                
            </li>
        </ul><div>
                                                <div id="tab-d2hRRRDP6b" role="tabpanel" aria-labelledby="tab-d2hRRRDP6b">
                                            
<pre data-controller="copy"><p><span data-action="click->copy#codeCopy">content_copy</span>
                <span disabled="">link</span>
                <span disabled="">edit</span>
            </p><code><div><p><span><span>import </span>pgml</span></p>
<p><span>client = pgml.OpenSourceAI()</span></p>
<p><span>results = client.chat_completions_create_stream(</span></p>
<p><span>     "PygmalionAI/mythalion-13b",</span></p>
<p><span>     [</span></p>
<p><span>         {</span></p>
<p><span>             "role": "system",</span></p>
<p><span>             "content": "You are a friendly chatbot who always responds in the style of a pirate",</span></p>
<p><span>         },</span></p>
<p><span>         {</span></p>
<p><span>             "role": "user",</span></p>
<p><span>             "content": "How many helicopters can a human eat in one sitting?",</span></p>
<p><span>         },</span></p>
<p><span>     ],</span></p>
<p><span>     temperature=0.85,</span></p>
<p><span>)</span></p>
<p><span>for c in results:</span></p>
<p><span>    print(c)</span></p></div></code></pre>


</div>
                                                <div id="tab-vID5M2IV8O" role="tabpanel" aria-labelledby="tab-vID5M2IV8O">
                                            
<pre data-controller="copy"><p><span data-action="click->copy#codeCopy">content_copy</span>
                <span disabled="">link</span>
                <span disabled="">edit</span>
            </p><code><div><p><span>const pgml = require("pgml");</span></p>
<p><span>const client = pgml.newOpenSourceAI();</span></p>
<p><span>const it = client.chat_completions_create_stream(</span></p>
<p><span>      "PygmalionAI/mythalion-13b",</span></p>
<p><span>      [</span></p>
<p><span>          {</span></p>
<p><span>              role: "system",</span></p>
<p><span>              content: "You are a friendly chatbot who always responds in the style of a pirate",</span></p>
<p><span>          },</span></p>
<p><span>          {</span></p>
<p><span>              role: "user",</span></p>
<p><span>              content: "How many helicopters can a human eat in one sitting?",</span></p>
<p><span>          },</span></p>
<p><span>      ],</span></p>
<p><span>);</span></p>
<p><span>let result = it.next();</span></p>
<p><span>while (!result.done) {</span></p>
<p><span>  console.log(result.value);</span></p>
<p><span>  result = it.next();</span></p>
<p><span>}</span></p></div></code></pre>

</div></div>
<pre data-controller="copy"><p><span data-action="click->copy#codeCopy">content_copy</span>
                <span disabled="">link</span>
                <span disabled="">edit</span>
            </p><code><div><p><span>{</span></p>
<p><span>  "choices": [</span></p>
<p><span>    {</span></p>
<p><span>      "delta": {</span></p>
<p><span>        "content": "Y",</span></p>
<p><span>        "role": "assistant"</span></p>
<p><span>      },</span></p>
<p><span>      "index": 0</span></p>
<p><span>    }</span></p>
<p><span>  ],</span></p>
<p><span>  "created": 1701296792,</span></p>
<p><span>  "id": "62a817f5-549b-43e0-8f0c-a7cb204ab897",</span></p>
<p><span>  "model": "PygmalionAI/mythalion-13b",</span></p>
<p><span>  "object": "chat.completion.chunk",</span></p>
<p><span>  "system_fingerprint": "f366d657-75f9-9c33-8e57-1e6be2cf62f3"</span></p>
<p><span>}</span></p>
<p><span>{</span></p>
<p><span>  "choices": [</span></p>
<p><span>    {</span></p>
<p><span>      "delta": {</span></p>
<p><span>        "content": "e",</span></p>
<p><span>        "role": "assistant"</span></p>
<p><span>      },</span></p>
<p><span>      "index": 0</span></p>
<p><span>    }</span></p>
<p><span>  ],</span></p>
<p><span>  "created": 1701296792,</span></p>
<p><span>  "id": "62a817f5-549b-43e0-8f0c-a7cb204ab897",</span></p>
<p><span>  "model": "PygmalionAI/mythalion-13b",</span></p>
<p><span>  "object": "chat.completion.chunk",</span></p>
<p><span>  "system_fingerprint": "f366d657-75f9-9c33-8e57-1e6be2cf62f3"</span></p>
<p><span>}</span></p></div></code></pre>

        <div>
            
        
<p>We have truncated the output to two items</p>

                                    </div>
                                    
<p>We also have asynchronous versions of the create and <code>create_stream</code> functions relatively named <code>create_async</code> and <code>create_stream_async</code>. Checkout <a href="http://127.0.0.1:5000/s/B7HH1yMjCs0skMpuwNIR/sdks/opensourceai">our documentation</a> for a complete guide of the open-source AI SDK including guides on how to specify custom models.</p>
<p>PostgresML is free and open source. To run the above examples yourself<a href="https://postgresml.org/signup"> create an account</a>, install pgml, and get running!</p>

<p>PostgresML is a complete MLOps platform in a simple PostgreSQL extension. It’s the tool our team wished they’d had scaling MLOps at Instacart during its peak years of growth. You can host your database with us or locally. However you want to engage, we know from experience that it’s better to bring your ML workload to the database rather than bringing the data to the codebase.</p>
<p>Fundamentally, PostgresML enables PostgreSQL to act as a GPU-powered AI application database — where you can both save models and index data. That eliminates the need for the myriad of separate services you have to tie together for your ML workflow. Pgml + pgvector create a complete ML platform (vector DB, model store, inference service, open-source LLMs) all within open-source extensions for PostgreSQL. That takes a lot of the complexity out of your infra, and it's ultimately faster for your users.</p>
<p>We're bullish on the power of in-database and open-source ML/AI, and we’re excited for you to see the power of this approach yourself. You can try it out in our serverless database for $0, with usage based billing starting at just five cents an hour per GB GPU cache. You can even mess with it for free on our homepage.</p>
<p>As always, let us know what you think. Get in touch via email or on our Discord if you have any questions or feedback.</p>


  <div>
    <h2>Have Questions?</h2>
    <p><a href="https://discord.gg/DmyJP3qJ7U">Join our Discord</a> and ask us anything! We're friendly and would love to talk about PostgresML and PgCat.</p>
  </div>

  
  <div>
    <h2>Try It Out</h2>
    <p>Try PostresML using our <strong>free</strong> serverless cloud. It comes with GPUs, 5 GiB of space and plenty of datasets to get you started.</p>
    
  </div>
  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[W3C Leaves Twitter (192 pts)]]></title>
            <link>https://w3c.social/@w3c/111534700276754588</link>
            <guid>38547203</guid>
            <pubDate>Wed, 06 Dec 2023 17:49:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://w3c.social/@w3c/111534700276754588">https://w3c.social/@w3c/111534700276754588</a>, See on <a href="https://news.ycombinator.com/item?id=38547203">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Spotlight: Sentry for Development (136 pts)]]></title>
            <link>https://spotlightjs.com/</link>
            <guid>38546520</guid>
            <pubDate>Wed, 06 Dec 2023 17:03:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spotlightjs.com/">https://spotlightjs.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38546520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <section> <h2>Thirty Seconds to Liftoff</h2>  <div> <div> <h5>As easy as npm install</h5> <p>Install Spotlight, inject the Spotlight overlay and run the sidecar.</p> <p><a href="https://spotlightjs.com/setup/">Setup</a> <img src="https://spotlightjs.com/images/simple-event-flow.png"> </p></div> <div> <div><figure><pre tabindex="0" dir="ltr"><code><p><span>npm</span><span> </span><span>install</span><span> </span><span>@spotlightjs/spotlight</span></p></code></pre></figure></div>
<div><figure><pre tabindex="0" dir="ltr"><code><p><span>import</span><span> </span><span>*</span><span> </span><span>as</span><span> Spotlight </span><span>from</span><span> </span><span>'</span><span>@spotlightjs/spotlight</span><span>'</span><span>;</span></p><p><span>if</span><span> (process</span><span>.</span><span>env</span><span>.</span><span>NODE_ENV</span><span> </span><span>===</span><span> </span><span>'</span><span>development</span><span>'</span><span>) {</span></p><p><span>  Spotlight</span><span>.</span><span>init</span><span>();</span></p><p><span>}</span></p></code></pre></figure></div>
<p><img src="https://spotlightjs.com/images/simple-event-flow.png"> </p></div> </div> </section><section> <h2>Your perfect dev companion</h2>  <div> <div> <h5>Embedded in your frontend</h5> <p>Notifies you of errors and other signals as they happen. No more context switching.</p> <p><a href="https://spotlightjs.com/architecture/">How?</a>  </p></div> <div> <p><img src="https://spotlightjs.com/images/glyph.svg"></p><p>0</p> </div> </div> </section><section> <h2>Works Everywhere</h2>  <div> <div> <h5>Compatible with Sentry's universal SDKs</h5> <p>Take the power of Sentry's telemetry and bring that into your development environment with Errors and Traces in both Frontend and Backend.</p> <p><a href="https://spotlightjs.com/integrations/sentry/">Configure with Sentry</a>  </p></div> <p><img src="https://spotlightjs.com/images/error.png" width="100%"> </p> </div> </section><section> <h2>An Extension of You</h2>  <div> <div> <h5>Customizable overlay to support your use cases</h5> <p>The powerful overlay and event proxy make it easy to extend Spotlight to other concepts including first-party concerns.</p> <p><a href="https://spotlightjs.com/integrations/">Customize</a>  </p></div> <div>  <p>Spotlight directly integrated into Astro Dev Overlay</p> </div> </div> </section><div> <h3>Ready for the deep dive?</h3>  </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JetBrains AI (103 pts)]]></title>
            <link>https://www.jetbrains.com/ai/</link>
            <guid>38545784</guid>
            <pubDate>Wed, 06 Dec 2023 16:10:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jetbrains.com/ai/">https://www.jetbrains.com/ai/</a>, See on <a href="https://news.ycombinator.com/item?id=38545784">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Winglang – A new cloud-oriented programming language (108 pts)]]></title>
            <link>https://github.com/winglang/wing</link>
            <guid>38545538</guid>
            <pubDate>Wed, 06 Dec 2023 15:56:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/winglang/wing">https://github.com/winglang/wing</a>, See on <a href="https://news.ycombinator.com/item?id=38545538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Welcome to the Wing Language! 👋</h2>
<p dir="auto">
  <a href="https://www.winglang.io/learn/" rel="nofollow">Take a Tour</a>
  ▪︎
  <a href="https://www.winglang.io/docs/" rel="nofollow">Getting Started</a>
  ▪︎
  <a href="http://t.winglang.io/slack" rel="nofollow">Join Slack</a>
  ▪︎
  <a href="https://www.winglang.io/docs/category/faq" rel="nofollow">FAQ</a>
  ▪︎
  <a href="https://www.winglang.io/contributing/status#roadmap" rel="nofollow">Roadmap</a>
  ▪︎
  <a href="https://github.com/winglang/wing/issues">Issues</a>
  ▪︎
  <a href="https://github.com/winglang/wing/discussions">Discussions</a>
  ▪︎
  <a href="https://www.winglang.io/contributing/" rel="nofollow">Contribute</a>
</p>
<p dir="auto"><strong>Winglang</strong> is a new open-source programming language designed for the cloud (aka "<em>cloud-oriented</em>").
Wing enables developers to build distributed systems that leverage cloud services as first-class citizens by combining infrastructure <em><strong>and</strong></em> application code in a safe and unified programming model (aka "<em>cloud-oriented</em>").
Wing programs can be executed locally (<em>yes, no internet required</em>) using a fully-functional simulator, or deployed to any cloud provider (<em>yes, Wing programs are portable across providers</em>).</p>
<p dir="auto">The mission of Winglang is to bring back your creative flow and close the gap between imagination and creation.</p>
<p dir="auto">Developing for the cloud today requires mastering various layers of the cloud stack, IAM roles, networking, and numerous tools, along with finding creative ways to test and debug code. In addition, long deployment times hinder iteration cycles and take developers out of their creative flow.</p>
<p dir="auto">Winglang addresses these pains by letting you work at a higher level of abstraction and allowing you to focus on business logic instead of cloud mechanics, only surfacing low-level details when it's needed.
We also provide you with a set of tools that let you test your code locally, significantly faster than before.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/winglang/wing/blob/main/apps/wing/logo/demo.gif"><img src="https://github.com/winglang/wing/raw/main/apps/wing/logo/demo.gif" alt="Wing Demo" height="360px" data-animated-image=""></a>
</p>
<p dir="auto">Wing is built by <a href="https://github.com/eladb">Elad Ben-Israel</a>, the guy behind the <a href="https://github.com/aws/aws-cdk">AWS CDK</a>, the gang at the <a href="https://www.wing.cloud/" rel="nofollow">Wing Cloud team</a> and an amazing <a href="https://t.winglang.io/slack" rel="nofollow">community</a> of contributors (also known as Wingnuts).</p>
<p dir="auto">Click <a href="https://www.youtube.com/watch?v=5_RhWwgGue0" rel="nofollow">here</a> to watch a short video introduction to the Wing language.</p>
<h2 tabindex="-1" dir="auto">Why do we think the cloud needs a programming language? 🤔</h2>
<p dir="auto">Cloud applications are fundamentally different from applications that run on a single machine -
they are distributed systems that rely on cloud infrastructure to achieve their goals.</p>
<p dir="auto">In order to be able to express both infrastructure and application logic in a safe and unified programming model,
Winglang has two execution phases: <em>preflight</em> for infrastructure definitions and <em>inflight</em> for runtime code.</p>
<p dir="auto">Preflight code is executed <em>during compilation</em> and produces the infrastructure configuration for your app (e.g. <strong>Terraform</strong>, <strong>CloudFormation</strong>, etc).
Inflight code is compiled into <strong>JavaScript</strong> and executed within cloud compute platforms in Node.js environments.</p>
<p dir="auto">Let's look at a simple example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bring cloud;

let queue = new cloud.Queue();
let counter = new cloud.Counter();
let bucket = new cloud.Bucket();

queue.setConsumer(inflight (message) => {
  let i = counter.inc();
  bucket.put(&quot;file-{i}.txt&quot;, message);
});"><pre><span>bring</span> <span>cloud</span><span>;</span>

<span>let</span> <span>queue</span> <span>=</span> <span>new</span> <span>cloud</span><span>.</span><span>Queue</span><span>(</span><span>)</span><span>;</span>
<span>let</span> <span>counter</span> <span>=</span> <span>new</span> <span>cloud</span><span>.</span><span>Counter</span><span>(</span><span>)</span><span>;</span>
<span>let</span> <span>bucket</span> <span>=</span> <span>new</span> <span>cloud</span><span>.</span><span>Bucket</span><span>(</span><span>)</span><span>;</span>

<span>queue</span><span>.</span><span>setConsumer</span><span>(</span><span>inflight</span> <span>(</span><span>message</span><span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
  let <span>i</span> <span>=</span> <span>counter</span><span>.</span><span>inc</span><span>(</span><span>)</span><span>;</span>
  <span>bucket</span><span>.</span><span>put</span><span>(</span><span>"file-{i}.txt"</span><span>,</span> <span>message</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><code>cloud.Queue</code>, <code>cloud.Counter</code> and <code>cloud.Bucket</code> are <em>preflight objects</em>.
They represent cloud infrastructure resources.
When compiled to a specific cloud provider, such as AWS, a Terraform file will be produced with the provider's implementation
of these resources. The <code>queue.setConsumer()</code> method is a <em>preflight method</em> that configures the infrastructure to
invoke a particular <em>inflight function</em> for each message in the queue.</p>
<p dir="auto"><strong>Now comes the cool part:</strong> the code that runs inside the inflight function interacts with the <code>counter</code> and the <code>bucket</code> objects
through their <em>inflight methods</em> (<code>counter.inc()</code> and <code>bucket.put()</code>). These methods can only be
called from inflight scopes.</p>
<h3 tabindex="-1" dir="auto">Very cool, but what here cannot be done by a library or compiler extension?</h3>
<p dir="auto">In existing languages, where there is no way to distinguish between multiple execution phases, it is impossible to naturally represent this idea that an object has methods that can only be executed from within a specific execution phase (or within certain scopes of the program).
You are welcome to read more about it <a href="https://www.winglang.io/docs/faq/why-a-language" rel="nofollow">here</a> (including code samples that show the same app built in Wing vs. other solutions).</p>
<h2 tabindex="-1" dir="auto">What makes Wing a good fit for cloud development? 🌟</h2>
<p dir="auto">Wing was built from scratch to make it easy for building applications on any cloud.
It includes an assembly of different features that serve that purpose:</p>
<ul dir="auto">
<li><a href="https://www.winglang.io/docs/faq/supported-clouds-services-and-engines/supported-services" rel="nofollow">Cloud services</a> as first-class citizens, with <a href="https://www.winglang.io/contributing/rfcs/language-spec#13-phase-modifiers" rel="nofollow">phase modifiers</a> for describing infrastructure and runtime code (<a href="https://www.winglang.io/docs/concepts/inflights" rel="nofollow"><code>preflight</code> and <code>inflight</code></a>).</li>
<li><a href="https://www.winglang.io/docs/category/cloud-library" rel="nofollow">Wing Cloud Library</a> provides a standard set of resources that lets you write cloud portable code.</li>
<li><a href="https://www.winglang.io/docs/tools/compiler-plugins" rel="nofollow">Compiler plugins</a> that keep you in control by allowing you to customize the infrastructure definitions and run policy checks.</li>
<li>Use any resource in the Terraform ecosystem as first-class citizen in your app.</li>
<li><a href="https://www.winglang.io/contributing/rfcs/language-spec#5-interoperability" rel="nofollow">JavaScript interoperability</a>.</li>
<li>Automatic generation of IAM policies and other cloud mechanics based on source code.</li>
<li><a href="https://www.winglang.io/docs/start-here/installation#wing-console" rel="nofollow">Wing Console</a> - a visual application-centric operations and management console, that lets you interact with...</li>
<li>A <a href="https://www.winglang.io/docs/concepts/simulator" rel="nofollow">simulator</a> that can used for testing and debugging in milliseconds.</li>
<li>JSON as a <a href="https://www.winglang.io/docs/language-reference#114-json-type" rel="nofollow">primitive data type</a> with schema validation support for each conversion to and from structs.</li>
<li><a href="https://www.winglang.io/blog/2023/02/02/good-cognitive-friction#immutable-by-default" rel="nofollow">Immutability by default</a>, <a href="https://www.winglang.io/contributing/rfcs/language-spec#113-asynchronous-model" rel="nofollow">implicit async code</a>, and <a href="https://www.winglang.io/docs/language-reference#16-optionality" rel="nofollow">safety from nulls and undefined</a>.</li>
</ul>
<p dir="auto">For a more in-depth look at Wing's features and benefits, check out our <a href="https://www.winglang.io/docs/" rel="nofollow">documentation</a>.</p>
<h2 tabindex="-1" dir="auto">Getting started 🛠️</h2>
<blockquote>
<p dir="auto">🚧 This is a pre-release, please see our <a href="https://www.winglang.io/contributing/status" rel="nofollow">project status</a> for more details.</p>
</blockquote>
<p dir="auto">If you'd just like to dip your feet in the water and see what Wing is all about, you can try it out in our <a href="https://www.winglang.io/play/" rel="nofollow">online playground</a> or walk through the <a href="https://www.winglang.io/learn/" rel="nofollow">interactive tour</a>.</p>
<p dir="auto">When you're ready to start building your own Wing apps, you'll need to:</p>
<ol dir="auto">
<li>Install the <a href="https://www.winglang.io/docs/start-here/installation" rel="nofollow">Wing CLI</a>.</li>
<li>Get the <a href="https://www.winglang.io/docs/start-here/installation#wing-ide-extension" rel="nofollow">Wing IDE Extension</a> for your favorite editor.</li>
<li>Launch the <a href="https://www.winglang.io/docs/start-here/installation#wing-console" rel="nofollow">Wing Console</a> and take it for a spin!</li>
</ol>
<p dir="auto">For a step-by-step guide, head over to our <a href="https://www.winglang.io/docs/" rel="nofollow">Getting Started</a> guide.
It's a once-in-a-lifetime adventure into the Wing rabbit hole!</p>
<h2 tabindex="-1" dir="auto">FAQs ❓</h2>
<p dir="auto">Here are some questions we're commonly asked that are covered by our <a href="https://www.winglang.io/docs/category/faq" rel="nofollow">FAQ</a>:</p>
<ul dir="auto">
<li><a href="https://www.winglang.io/docs/faq/who-is-behind-wing" rel="nofollow">Who is behind this project?</a></li>
<li><a href="https://www.winglang.io/docs/faq/supported-clouds-services-and-engines/supported-clouds" rel="nofollow">Which clouds are supported by Wing?</a></li>
<li><a href="https://www.winglang.io/docs/faq/supported-clouds-services-and-engines/supported-provisioning-engines" rel="nofollow">Which provisioning engines are supported by Wing?</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Community 💬</h2>
<p dir="auto">Join our flock in the <a href="https://t.winglang.io/slack" rel="nofollow">Wing Slack</a> community.
We're here to help each other, answer questions, and share our cloud adventures.
Alternatively, post any questions on <a href="https://github.com/winglang/wing/discussions">GitHub Discussions</a>.</p>
<h2 tabindex="-1" dir="auto">Contributing 🤝</h2>
<p dir="auto">Want to help Wing take flight?
Check out our <a href="https://github.com/winglang/wing/blob/main/CONTRIBUTING.md">contribution guide</a> to learn how to set up a development environment and contribute to the project.
You can also get started by opening the project in GitHub Codespaces.</p>
<p dir="auto"><a href="https://codespaces.new/winglang/wing" rel="nofollow"><img src="https://github.com/codespaces/badge.svg" alt="Open in GitHub Codespaces"></a></p>
<p dir="auto">We are incredibly grateful to our entire community for contributing bug fixes and improvements:</p>
<a href="https://github.com/winglang/wing/graphs/contributors">
  <img src="https://camo.githubusercontent.com/04c700bf42a90a82f3cfe5bdaf55668a89af78b72d8f21b1c8937a75708c3c01/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d77696e676c616e672f77696e67" data-canonical-src="https://contrib.rocks/image?repo=winglang/wing">
</a>
<h2 tabindex="-1" dir="auto">License 📜</h2>
<p dir="auto">Wing is licensed under the  <a href="https://github.com/winglang/wing/blob/main/LICENSE.md">MIT License</a>.
Contributions are made under our <a href="https://github.com/winglang/wing/blob/main/CONTRIBUTION_LICENSE.md">contribution license</a>.</p>
<p dir="auto">Happy coding, and remember: the sky's the limit with Wing (yes, another pun)! 🌤️🚀</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Databases are the endgame for data-oriented design (206 pts)]]></title>
            <link>https://spacetimedb.com/blog/databases-and-data-oriented-design</link>
            <guid>38545417</guid>
            <pubDate>Wed, 06 Dec 2023 15:48:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spacetimedb.com/blog/databases-and-data-oriented-design">https://spacetimedb.com/blog/databases-and-data-oriented-design</a>, See on <a href="https://news.ycombinator.com/item?id=38545417">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: CopilotKit- Build in-app AI chatbots and AI-powered Textareas (178 pts)]]></title>
            <link>https://github.com/CopilotKit/CopilotKit</link>
            <guid>38545207</guid>
            <pubDate>Wed, 06 Dec 2023 15:35:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/CopilotKit/CopilotKit">https://github.com/CopilotKit/CopilotKit</a>, See on <a href="https://news.ycombinator.com/item?id=38545207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/CopilotKit/CopilotKit/assets/746397/5890217b-524e-49c5-a89e-b8743d2acd51">
    <img alt="CopilotKit Logo" src="https://github.com/CopilotKit/CopilotKit/assets/746397/bd5c9079-929b-4d55-bdc9-16d1c8181b71" width="450px">
  </picture></themed-picture>
  
</div>
<p dir="auto">
  <a href="https://discord.gg/6dffbvGU3D" rel="nofollow">
      <img src="https://camo.githubusercontent.com/9e53fcff931b057d78cdd2ff3d11fe9d14ba6592809e4a57291f75384a03625c/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f366466666276475533443f636f6d706163743d74727565267374796c653d666c6174" alt="Discord" data-canonical-src="https://dcbadge.vercel.app/api/server/6dffbvGU3D?compact=true&amp;style=flat">
  </a>
  <a href="https://github.com/CopilotKit/CopilotKit/actions/workflows/ci.yml">
      <img src="https://github.com/CopilotKit/CopilotKit/actions/workflows/ci.yml/badge.svg" alt="GitHub CI">
  </a>
  <a href="https://www.npmjs.com/package/@copilotkit/react-core" rel="nofollow">
    <img src="https://camo.githubusercontent.com/eccd94d8a2a65d87cf0881a6f3632a1a36e698319cb2435bd8675679bf7a39d1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f40636f70696c6f746b69742f72656163742d636f7265" alt="NPM" data-canonical-src="https://img.shields.io/npm/v/@copilotkit/react-core">
  <img src="https://camo.githubusercontent.com/64457ba31ad77e9143b4f8f76944a92401a587f6f64dfa59525770aa56e27195/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f436f70696c6f744b69742f436f70696c6f744b6974" alt="MIT" data-canonical-src="https://img.shields.io/github/license/CopilotKit/CopilotKit">
</a></p>
<h2 tabindex="-1" dir="auto">
The Open-Source Copilot Platform
</h2>
<h3 tabindex="-1" dir="auto">
In-app chatbots, and AI-enabled TextArea.
</h3>

 <p dir="auto">
   Questions?
    <a href="https://calendly.com/atai_/copilotkit" rel="nofollow"><strong> Book a call with us  »</strong></a>
    <br>
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/746397/288405135-1aa17608-46a5-4e2f-aad5-19c8f5c5f1bd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjg4NDA1MTM1LTFhYTE3NjA4LTQ2YTUtNGUyZi1hYWQ1LTE5YzhmNWM1ZjFiZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NmUyMmUzYTlhODc5NzNjOTQ0ZTQ1NDFhNzAxMGExYmRlZGViYzlhODFmYWYzZTI5NDU2ODIyNmJhNjY3ZmMyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.SkcsNxW2MJFl52Yt-iyqbr0vAxr7Va7SERgmzO-QSXo"><img src="https://private-user-images.githubusercontent.com/746397/288405135-1aa17608-46a5-4e2f-aad5-19c8f5c5f1bd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjg4NDA1MTM1LTFhYTE3NjA4LTQ2YTUtNGUyZi1hYWQ1LTE5YzhmNWM1ZjFiZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NmUyMmUzYTlhODc5NzNjOTQ0ZTQ1NDFhNzAxMGExYmRlZGViYzlhODFmYWYzZTI5NDU2ODIyNmJhNjY3ZmMyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.SkcsNxW2MJFl52Yt-iyqbr0vAxr7Va7SERgmzO-QSXo" height="220px"></a>
</p>

<p dir="auto">🌟 <strong>&lt;CopilotPortal /&gt;:</strong> <br>
Build <strong>in-app AI chatbots</strong> that can "see" the current app state + take action inside your app. <br>
The AI chatbot can talk to your app frontend &amp; backend, and to 3rd party services (Salesforce, Dropbox, etc.) via plugins. <br>
AI "second brain" for your users, on tap.</p>
<p dir="auto">🌟 <strong>&lt;CopilotTextarea /&gt;:</strong> <br>
AI-assisted text generation. Drop-in replacement for any <code>&lt;textarea /&gt;.</code><br>
Autocompletions + AI editing + generate from scratch. Indexed on your users' content.<br>
Starting with React. Use any LLM. <br></p>
<p dir="auto">Combines frontend SDKs, backend SDKs, and (optional) cloud infrastructure. Open-source 🪁</p>
<h2 tabindex="-1" dir="auto">🎯 Features Overview</h2>
<h3 tabindex="-1" dir="auto">CopilotTextarea: AI-assisted text generation + editing.</h3>
<ul dir="auto">
<li>✅ A a drop-in <code>&lt;textarea /&gt;</code> replacement. Supports all <code>&lt;textarea /&gt;</code> customizations.</li>
<li>✅ Context-aware autocompletions ✨ (like in GitHub Copilot)</li>
<li>✅ AI editing ✨ - "list the client's top 3 pain points from the last call using @SalesforceData"</li>
<li>🟩 Generate from scratch ✨ - automatically populate the initial content based on given context</li>
<li>✅ App context &amp; 3rd party context with <code>useMakeCopilotReadable</code> and <code>useMakeCopilotDocumentReadable</code></li>
<li>✅ Fully custsomizable prompt engineering</li>
<li>🟩 Arbitrary LLM chains.</li>
<li>🟩 Bold + italics.</li>
</ul>
<h3 tabindex="-1" dir="auto">Copilot Chatbot: (frontend + backend) runtimes for in-app copilots.</h3>
<ul dir="auto">
<li>✅ Index on frontend app state (via <code>useMakeCopilotReadable</code> and <code>useMakeCopilotDocumentReadable</code>)</li>
<li>🟩 Index on backend state</li>
<li>✅ frontend function calling runtime (in-app actions) (via <code>useMakeCopilotActionable</code>)</li>
<li>🟩 backend function calling runtime (auth enabled)</li>
<li>🟩 Autorun vs. "sensitive" functions (require user approval before execution).</li>
<li>✅ Cursor-style @document-referecing.</li>
<li>✅ Bring your own model</li>
<li>🟩 3rd party plugins</li>
<li>🟩 execute arbitrary LLM chains</li>
<li>🟩 OpenAI <em>assistants</em> api</li>
<li>✅ Fully customize UI</li>
</ul>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto"><strong>2-min showcase + 2-min implementation tutorial:</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description copilot_full_demo_nxxpbr.3.mp4">copilot_full_demo_nxxpbr.3.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/746397/266699772-b0cdf38b-ec5c-4e95-8623-364bafb70907.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjY2Njk5NzcyLWIwY2RmMzhiLWVjNWMtNGU5NS04NjIzLTM2NGJhZmI3MDkwNy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lM2JlNzViZmFjOTE4NzZiYzgxYTBjNWQzY2NiZWEzYjcxOWE2NTg3NGZlNGJjY2EyMmNhNzA1YzljNTA3ZTVmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.d3kLjQUfKCJq8EetRtGphZDa1HA-7LSgEDEEdw8Ytok" data-canonical-src="https://private-user-images.githubusercontent.com/746397/266699772-b0cdf38b-ec5c-4e95-8623-364bafb70907.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjY2Njk5NzcyLWIwY2RmMzhiLWVjNWMtNGU5NS04NjIzLTM2NGJhZmI3MDkwNy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lM2JlNzViZmFjOTE4NzZiYzgxYTBjNWQzY2NiZWEzYjcxOWE2NTg3NGZlNGJjY2EyMmNhNzA1YzljNTA3ZTVmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.d3kLjQUfKCJq8EetRtGphZDa1HA-7LSgEDEEdw8Ytok" controls="controls" muted="muted">

  </video>
</details>

<h2 tabindex="-1" dir="auto">Installation</h2>
<div dir="auto" data-snippet-clipboard-copy-content="npm i @copilotkit/react-core @copilotkit/react-ui @copilotkit/react-textarea"><pre>npm i @copilotkit/react-core @copilotkit/react-ui @copilotkit/react-textarea</pre></div>
<h2 tabindex="-1" dir="auto">Getting started</h2>
<p dir="auto">See quickstart in the <a href="https://docs.copilotkit.ai/" rel="nofollow">docs</a></p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<h3 tabindex="-1" dir="auto"><code>&lt;CopilotTextarea /&gt;</code></h3>
<p dir="auto">A drop-in &lt;textarea /&gt; replacement with context-aware Copilot autocompletions.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/CopilotKit/CopilotKit/blob/main/assets/CopilotTextarea.gif"><img src="https://github.com/CopilotKit/CopilotKit/raw/main/assets/CopilotTextarea.gif" width="400" height="400" data-animated-image=""></a>
</p>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Customizable <code>purpose</code> prompt.</li>
<li>Provide arbitrary context to inform autocompletions using <code>useMakeCopilotReadable</code></li>
<li>Works with any backend/LLM, using <code>ChatlikeApiEndpoint</code></li>
<li>Supports all <code>&lt;textarea /&gt;</code> customizations</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="import &quot;@copilotkit/react-textarea/styles.css&quot;; // add to the app-global css
import { CopilotTextarea } from &quot;@copilotkit/react-textarea&quot;;
import { CopilotProvider } from &quot;@copilotkit/react-core&quot;;

// call ANYWHERE in your app to provide external context (make sure you wrap the app with a <CopilotProvider >):
// See below for more features (parent/child hierarchy, categories, etc.)
useMakeCopilotReadable(relevantInformation)
useMakeCopilotDocumentReadable(document)

return (
  <CopilotProvider chatApiEndpoint=&quot;/api/copilotkit/chat&quot;> {/* Global state &amp; copilot logic. Put this around the entire app */}
    <CopilotTextarea
      className=&quot;p-4 w-1/2 aspect-square font-bold text-3xl bg-slate-800 text-white rounded-lg resize-none&quot;
      placeholder=&quot;A CopilotTextarea!&quot;
      autosuggestionsConfig={{
        purposePrompt: &quot;A COOL &amp; SMOOTH announcement post about CopilotTextarea. Be brief. Be clear. Be cool.&quot;,
        forwardedParams: { // additional arguments to customize autocompletions
          max_tokens: 25,
          stop: [&quot;\n&quot;, &quot;.&quot;, &quot;,&quot;],
        },
      }}
    />
  </CopilotProvider>
);"><pre><span>import</span> <span>"@copilotkit/react-textarea/styles.css"</span><span>;</span> <span>// add to the app-global css</span>
<span>import</span> <span>{</span> <span>CopilotTextarea</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-textarea"</span><span>;</span>
<span>import</span> <span>{</span> <span>CopilotProvider</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>

<span>// call ANYWHERE in your app to provide external context (make sure you wrap the app with a &lt;CopilotProvider &gt;):</span>
<span>// See below for more features (parent/child hierarchy, categories, etc.)</span>
<span>useMakeCopilotReadable</span><span>(</span><span>relevantInformation</span><span>)</span>
<span>useMakeCopilotDocumentReadable</span><span>(</span><span>document</span><span>)</span>

<span>return</span> <span>(</span>
  <span>&lt;</span><span>CopilotProvider</span><span></span> <span>chatApiEndpoint</span><span>=</span><span>"/api/copilotkit/chat"</span><span>&gt;</span> <span>{</span><span>/* Global state &amp; copilot logic. Put this around the entire app */</span><span>}</span>
    <span>&lt;</span><span>CopilotTextarea</span>
      <span>className</span><span>=</span><span>"p-4 w-1/2 aspect-square font-bold text-3xl bg-slate-800 text-white rounded-lg resize-none"</span>
      <span>placeholder</span><span>=</span><span>"A CopilotTextarea!"</span>
      <span>autosuggestionsConfig</span><span>=</span><span>{</span><span>{</span>
        <span>purposePrompt</span>: <span>"A COOL &amp; SMOOTH announcement post about CopilotTextarea. Be brief. Be clear. Be cool."</span><span></span><span>,</span>
        <span>forwardedParams</span>: <span>{</span> <span>// additional arguments to customize autocompletions</span>
          <span>max_tokens</span>: <span>25</span><span>,</span>
          <span>stop</span>: <span>[</span><span>"\n"</span><span>,</span> <span>"."</span><span>,</span> <span>","</span><span>]</span><span>,</span>
        <span>}</span><span></span><span>,</span>
      <span>}</span><span>}</span>
    <span>/</span><span>&gt;</span>
  <span>&lt;</span><span>/</span><span>CopilotProvider</span><span>&gt;</span>
<span>)</span><span>;</span></pre></div>
<h3 tabindex="-1" dir="auto">Integrate copilot</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import &quot;@copilotkit/react-ui/styles.css&quot;; // add to the app-global css
import { CopilotProvider } from &quot;@copilotkit/react-core&quot;;
import { CopilotSidebarUIProvider } from &quot;@copilotkit/react-ui&quot;;

export default function App(): JSX.Element {
  return (
  <CopilotProvider chatApiEndpoint=&quot;/api/copilotkit/chat&quot;> {/* Global state &amp; copilot logic. Put this around the entire app */}
      <CopilotSidebarUIProvider> {/* A built-in Copilot UI (or bring your own UI). Put around individual pages, or the entire app. */}

        <YourContent />

      </CopilotSidebarUIProvider>
    </CopilotProvider>
  );
}"><pre><span>import</span> <span>"@copilotkit/react-ui/styles.css"</span><span>;</span> <span>// add to the app-global css</span>
<span>import</span> <span>{</span> <span>CopilotProvider</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>
<span>import</span> <span>{</span> <span>CopilotSidebarUIProvider</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-ui"</span><span>;</span>

<span>export</span> <span>default</span> <span>function</span> <span>App</span><span>(</span><span>)</span>: <span>JSX</span><span>.</span><span>Element</span> <span>{</span>
  <span>return</span> <span>(</span>
  <span>&lt;</span><span>CopilotProvider</span><span></span> <span>chatApiEndpoint</span><span>=</span><span>"/api/copilotkit/chat"</span><span>&gt;</span> <span>{</span><span>/* Global state &amp; copilot logic. Put this around the entire app */</span><span>}</span>
      <span>&lt;</span><span>CopilotSidebarUIProvider</span><span>&gt;</span> <span>{</span><span>/* A built-in Copilot UI (or bring your own UI). Put around individual pages, or the entire app. */</span><span>}</span>

        <span>&lt;</span><span>YourContent</span> <span>/</span><span>&gt;</span>

      <span>&lt;</span><span>/</span><span>CopilotSidebarUIProvider</span><span>&gt;</span>
    <span>&lt;</span><span>/</span><span>CopilotProvider</span><span>&gt;</span>
  <span>)</span><span>;</span>
<span>}</span></pre></div>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Batteries included. Add 2 React components, and your Copilot is live.</li>
<li>Customize the built-in <code>CopilotSidebarUIProvider</code> UI -- or bring your own UI component.</li>
<li>Extremely hackable. Should the need arise, you can define 1st-class extensions just as powerful as <code>useMakeCopilotReadable</code>, <code>useMakeCopilotActionable</code>, etc.</li>
</ol>
<h3 tabindex="-1" dir="auto">Give the copilot read permissions</h3>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Propagate useful information &amp; granular app-state to the Copilot</li>
<li>Easily maintain the hierarchical structure of information with <code>parentId</code></li>
<li>One call to rule them all: <code>useMakeCopilotReadable</code> works both with the sidekick, and with CopilotTextarea.
<ul dir="auto">
<li>Use the <code>contextCategories: string[]</code> param to route information to different places.</li>
</ul>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="import { useMakeCopilotReadable } from &quot;@copilotkit/react-core&quot;;


function Employee(props: EmployeeProps): JSX.Element {
  const { employeeName, workProfile, metadata } = props;

  // propagate any information copilot
  const employeeContextId = useMakeCopilotReadable(employeeName);

  // Pass a parentID to maintain a hiearchical structure.
  // Especially useful with child React components, list elements, etc.
  useMakeCopilotReadable(workProfile.description(), employeeContextId);
  useMakeCopilotReadable(metadata.description(), employeeContextId);
  
  return (
    // Render as usual...
  );
}
"><pre><span>import</span> <span>{</span> <span>useMakeCopilotReadable</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>


<span>function</span> <span>Employee</span><span>(</span><span>props</span>: <span>EmployeeProps</span><span>)</span>: <span>JSX</span><span>.</span><span>Element</span> <span>{</span>
  <span>const</span> <span>{</span> employeeName<span>,</span> workProfile<span>,</span> metadata <span>}</span> <span>=</span> <span>props</span><span>;</span>

  <span>// propagate any information copilot</span>
  <span>const</span> <span>employeeContextId</span> <span>=</span> <span>useMakeCopilotReadable</span><span>(</span><span>employeeName</span><span>)</span><span>;</span>

  <span>// Pass a parentID to maintain a hiearchical structure.</span>
  <span>// Especially useful with child React components, list elements, etc.</span>
  <span>useMakeCopilotReadable</span><span>(</span><span>workProfile</span><span>.</span><span>description</span><span>(</span><span>)</span><span>,</span> <span>employeeContextId</span><span>)</span><span>;</span>
  <span>useMakeCopilotReadable</span><span>(</span><span>metadata</span><span>.</span><span>description</span><span>(</span><span>)</span><span>,</span> <span>employeeContextId</span><span>)</span><span>;</span>
  
  <span>return</span> <span>(</span>
    <span>// Render as usual...</span>
  <span>)</span><span>;</span>
<span>}</span></pre></div>
<h3 tabindex="-1" dir="auto">Give the copilot write permissions</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import { useMakeCopilotActionable } from &quot;@copilotkit/react-core&quot;;

function Department(props: DepartmentProps): JSX.Element {
  // ...

  // Let the copilot take action on behalf of the user.
  useMakeCopilotActionable(
    {
      name: &quot;setEmployeesAsSelected&quot;,
      description: &quot;Set the given employees as 'selected'&quot;,
      argumentAnnotations: [
        {
          name: &quot;employeeIds&quot;,
          type: &quot;array&quot;, items: { type: &quot;string&quot; }
          description: &quot;The IDs of employees to set as selected&quot;,
          required: true
        }
      ],
      implementation: async (employeeIds) => setEmployeesAsSelected(employeeIds),
    },
    []
  );

  // ...
}"><pre><span>import</span> <span>{</span> <span>useMakeCopilotActionable</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>

<span>function</span> <span>Department</span><span>(</span><span>props</span>: <span>DepartmentProps</span><span>)</span>: <span>JSX</span><span>.</span><span>Element</span> <span>{</span>
  <span>// ...</span>

  <span>// Let the copilot take action on behalf of the user.</span>
  <span>useMakeCopilotActionable</span><span>(</span>
    <span>{</span>
      <span>name</span>: <span>"setEmployeesAsSelected"</span><span>,</span>
      <span>description</span>: <span>"Set the given employees as 'selected'"</span><span>,</span>
      <span>argumentAnnotations</span>: <span>[</span>
        <span>{</span>
          <span>name</span>: <span>"employeeIds"</span><span>,</span>
          <span>type</span>: <span>"array"</span><span>,</span> <span>items</span>: <span>{</span> <span>type</span>: <span>"string"</span> <span>}</span>
          <span>description</span>: <span>"The IDs of employees to set as selected"</span><span>,</span>
          <span>required</span>: <span>true</span>
        <span>}</span>
      <span>]</span><span>,</span>
      <span>implementation</span>: <span>async</span> <span>(</span><span>employeeIds</span><span>)</span> <span>=&gt;</span> <span>setEmployeesAsSelected</span><span>(</span><span>employeeIds</span><span>)</span><span>,</span>
    <span>}</span><span>,</span>
    <span>[</span><span>]</span>
  <span>)</span><span>;</span>

  <span>// ...</span>
<span>}</span></pre></div>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Plain typescript actions. Edit a textbox, navigate to a new page, or anythign you can think of.</li>
<li>Specify arbitrary input types.</li>
</ol>
<h2 tabindex="-1" dir="auto">Near-Term Roadmap</h2>
<h3 tabindex="-1" dir="auto">📊 Please vote on features via the Issues tab!</h3>
<h3 tabindex="-1" dir="auto">Copilot-App Interaction</h3>
<ul dir="auto">
<li>✅ <code>useMakeCopilotReadable</code>: give static information to the copilot, in sync with on-screen state</li>
<li>✅ <code>useMakeCopilotActionable</code>: Let the copilot take action on behalf of the user</li>
<li>🚧 <code>useMakeCopilotAskable</code>: let the copilot ask for additional information when needed (coming soon)</li>
<li>🚧 <code>useEditCopilotMessage</code>: edit the (unsent) typed user message to the copilot (coming soon)</li>
<li>🚧 copilot-assisted navigation: go to the best page to achieve some objective.</li>
<li>🚧 CopilotCloudKit: integrate arbitrary LLM logic / chains / RAG, using plain code.</li>
</ul>
<h3 tabindex="-1" dir="auto">UI components</h3>
<ul dir="auto">
<li>✅ <code>&lt;CopilotSidebarUIProvider&gt;</code>: Built in, hackable Copilot UI (optional - you can bring your own UI).</li>
<li>✅ <code>&lt;CopilotTextarea /&gt;</code>: drop-in <code>&lt;textarea /&gt;</code> replacement with Copilot autocompletions.</li>
</ul>
<h3 tabindex="-1" dir="auto">Integrations</h3>
<ul dir="auto">
<li>✅ Vercel AI SDK</li>
<li>✅ OpenAI APIs</li>
<li>🚧 Langchain</li>
<li>🚧 Additional LLM providers</li>
</ul>
<h3 tabindex="-1" dir="auto">Frameworks</h3>
<ul dir="auto">
<li>✅ React</li>
<li>🚧 Vue</li>
<li>🚧 Svelte</li>
<li>🚧 Swift (Mac + iOS)</li>
</ul>
<h2 tabindex="-1" dir="auto">Contribute</h2>
<p dir="auto">Contributions are welcome! 🎉</p>
<p dir="auto"><a href="https://discord.gg/6dffbvGU3D" rel="nofollow">Join the Discord</a>
<a href="https://discord.gg/6dffbvGU3D" rel="nofollow"><img src="https://camo.githubusercontent.com/9e53fcff931b057d78cdd2ff3d11fe9d14ba6592809e4a57291f75384a03625c/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f366466666276475533443f636f6d706163743d74727565267374796c653d666c6174" alt="Discord" data-canonical-src="https://dcbadge.vercel.app/api/server/6dffbvGU3D?compact=true&amp;style=flat"></a></p>

<h2 tabindex="-1" dir="auto">Contact</h2>
<p dir="auto">atai <code>&lt;at&gt;</code> copilotkit.ai</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just about every Windows/Linux device vulnerable to new LogoFAIL firmware attack (210 pts)]]></title>
            <link>https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/</link>
            <guid>38545022</guid>
            <pubDate>Wed, 06 Dec 2023 15:23:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/">https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/</a>, See on <a href="https://news.ycombinator.com/item?id=38545022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/computer-power-button-800x534.jpg" alt="Just about every Windows and Linux device vulnerable to new LogoFAIL firmware attack">
      <figcaption><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache miss 456:single/related:310c989e205eee0f590738e2cb404abd --><!-- empty -->
<p>Hundreds of Windows and Linux computer models from virtually all hardware makers are vulnerable to a new attack that executes malicious firmware early in the boot-up sequence, a feat that allows infections that are nearly impossible to detect or remove using current defense mechanisms.</p>
<p>The attack—dubbed LogoFAIL by the researchers who devised it—is notable for the relative ease in carrying it out, the breadth of both consumer- and enterprise-grade models that are susceptible, and the high level of control it gains over them. In many cases, LogoFAIL can be remotely executed in post-exploit situations using techniques that can’t be spotted by traditional endpoint security products. And because exploits run during the earliest stages of the boot process, they are able to bypass a host of defenses, including the industry-wide Secure Boot, Intel’s Secure Boot, and similar protections from other companies that are devised to prevent so-called bootkit infections.</p>
<h2>Game over for platform security</h2>
<p>LogoFAIL is a constellation of two dozen newly discovered vulnerabilities that have lurked for years, if not decades, in Unified Extensible Firmware Interfaces responsible for booting modern devices that run Windows or Linux. The vulnerabilities are the product of almost a year’s worth of work by Binarly, a firm that helps customers identify and secure vulnerable firmware.</p>
<p>The vulnerabilities are the subject of a coordinated mass disclosure released Wednesday. The participating companies comprise nearly the entirety of the x64 and ARM CPU ecosystem, starting with UEFI suppliers AMI, Insyde, and Phoenix (sometimes still called IBVs or independent BIOS vendors); device manufacturers such as Lenovo, Dell, and HP; and the makers of the CPUs that go inside the devices, usually Intel, AMD or designers of ARM CPUs. The researchers unveiled the attack on Wednesday at the Black Hat Security Conference in London.</p>                                            
                                                        
<p>The affected parties are releasing advisories that disclose which of their products are vulnerable and where to obtain security patches. Links to advisories and a list of vulnerability designations appears at the end of this article.</p>
<p>As its name suggests, LogoFAIL involves logos, specifically those of the hardware seller that are displayed on the device screen early in the boot process, while the UEFI is still running. Image parsers in UEFIs from all three major IBVs are riddled with roughly a dozen critical vulnerabilities that have gone unnoticed until now. By replacing the legitimate logo images with identical-looking ones that have been specially crafted to exploit these bugs, LogoFAIL makes it possible to execute malicious code at the most sensitive stage of the boot process, which is known as DXE, short for Driver Execution Environment.</p>
<p>“Once arbitrary code execution is achieved during the DXE phase, it’s game over for platform security,” researchers from Binarly, the security firm that discovered the vulnerabilities, wrote in a whitepaper. “From this stage, we have full control over the memory and the disk of the target device, thus including the operating system that will be started.”</p>
<p>From there, LogoFAIL can deliver a second-stage payload that drops an executable onto the hard drive before the main OS has even started. The following video demonstrates a proof-of-concept exploit created by the researchers. The infected device—a Gen 2 Lenovo ThinkCentre M70s running an 11th-Gen Intel Core with a UEFI released in June—runs standard firmware defenses, including Secure Boot and Intel Boot Guard.</p>
<figure><p><iframe type="text/html" width="560" height="315" src="https://www.youtube.com/embed/EufeOPe6eqk?si=Mf2uRls9-tVytbJx?start=0&amp;wmode=transparent" frameborder="0" allowfullscreen=""></iframe></p><figcaption><p>LogoFAIL.</p></figcaption></figure>
<p>In an email, Binarly founder and CEO Alex Matrosov wrote:</p>
<blockquote><p>LogoFAIL is a newly discovered set of high-impact security vulnerabilities affecting different image parsing libraries used in the system firmware by various vendors during the device boot process. These vulnerabilities are present in most cases inside reference code, impacting not a single vendor but the entire ecosystem across this code and device vendors where it is used. This attack can give a threat actor an advantage in bypassing most endpoint security solutions and delivering a stealth firmware bootkit that will persist in a firmware capsule with a modified logo image.</p></blockquote>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer (167 pts)]]></title>
            <link>https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer</link>
            <guid>38544824</guid>
            <pubDate>Wed, 06 Dec 2023 15:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer">https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer</a>, See on <a href="https://news.ycombinator.com/item?id=38544824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Generative AI (gen AI) models are rapidly evolving, offering unparalleled sophistication and capability. This advancement empowers enterprises and developers across various industries to solve complex problems and unlock new opportunities. However, the growth in gen AI models — <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga">with a tenfold increase in parameters annually over the past five years</a> — brings heightened requirements for training, tuning, and inference. Today's larger models, featuring hundreds of billions or even trillions of parameters, require extensive training periods, sometimes spanning months, even on the most specialized systems. Additionally, efficient AI workload management necessitates a coherently integrated AI stack consisting of optimized compute, storage, networking, software and development frameworks.</p><p>Today, to address these challenges, we are excited to announce Cloud TPU v5p, our most powerful, scalable, and flexible AI accelerator thus far. TPUs have long been the basis for training and serving AI-powered products like YouTube, Gmail, Google Maps, Google Play, and Android. In fact, Gemini, Google’s most capable and general AI model <a href="https://blog.google/technology/ai/google-gemini-ai" target="_blank">announced today</a>, was trained on, and is served, using TPUs.</p><p>In addition, we are also announcing AI Hypercomputer from Google Cloud, a groundbreaking supercomputer architecture that employs an integrated system of performance-optimized hardware, open software, leading ML frameworks, and flexible consumption models. Traditional methods often tackle demanding AI workloads through piecemeal, component-level enhancements, which can lead to inefficiencies and bottlenecks. In contrast, AI Hypercomputer employs systems-level codesign to boost efficiency and productivity across AI training, tuning, and serving.</p></span></section><div jsaction="rcuQ6b:npT2md" jscontroller="wJu6E" data-video-url="https://www.youtube.com/watch?v=hszd5UqnfLk"><picture><section><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" loading="lazy"></section></picture></div><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Inside Cloud TPU v5p, our most powerful and scalable TPU accelerator to date</b></h3><p>Earlier this year, <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-in-ga">we announced</a> the general availability of Cloud TPU v5e. With 2.3X price performance improvements over the previous generation TPU v4<sup>1</sup>, it is our most <i>cost-efficient</i> TPU to date. By contrast, Cloud TPU v5p, is our most <i>powerful</i> TPU thus far. Each TPU v5p pod <b>composes together 8,960 chips</b> over our <b>highest-bandwidth inter-chip interconnect (ICI) at 4,800 Gbps/chip in a 3D torus topology</b>. Compared to TPU v4, TPU v5p features more than<b> 2X greater FLOPS and 3X more high-bandwidth memory (HBM)</b>.</p><p>Designed for performance, flexibility, and scale, TPU v5p can <b>train large LLM models 2.8X faster</b> than the previous-generation TPU v4. Moreover, with second-generation <a href="https://cloud.google.com/tpu">SparseCores</a>, TPU v5p can <b>train embedding-dense models 1.9X faster</b> than TPU v4<sup>2</sup>.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5p and v4 are based on Google Internal Data. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model.</p></span></p></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5e data is from MLPerf™ 3.1 Training Closed results for v5e. TPU v5p and v4 are based on Google internal training runs. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model. It shows relative performance per dollar using the public list price of TPU v4 ($3.22/chip/hour), TPU v5e ( $1.2/chip/hour) and TPU v5p ($4.2/chip/hour).</p></span></p></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>In addition to performance improvements, <b>TPU v5p is also 4X more scalable than TPU v4 in terms of total available FLOPs per pod.</b> Doubling the floating-point operations per second (FLOPS) over TPU v4 and doubling the number of chips in a single pod provides considerable improvement in relative performance in training speed.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_next-generation_AI_workloads_v1.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_next-generation_AI_workloads_v1.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Google AI Hypercomputer delivers peak performance and efficiency at large scale</b></h3><p>Achieving both scale and speed is necessary, but not sufficient to meet the needs of modern AI/ML applications and services. The hardware and software components must come together into an integrated, easy-to-use, secure, and reliable computing system. At Google, we’ve done decades of research and development on this very problem, culminating in AI Hypercomputer, a system of technologies optimized to work in concert to enable modern AI workloads.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_next-generation_AI_workloads.max-800x800.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_next-generation_AI_workloads.max-800x800.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><ul><li><b>Performance-optimized hardware:</b> AI Hypercomputer features performance-optimized compute, storage, and networking built over an ultrascale data center infrastructure, leveraging a high-density footprint, liquid cooling, and our <a href="https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network">Jupiter data center network</a> technology. All of this is predicated on technologies that are built with <a href="https://www.google.com/about/datacenters/efficiency/" target="_blank">efficiency</a> at their core; leveraging <a href="https://cloud.google.com/blog/topics/sustainability/a-smarter-way-to-buy-clean-energy">clean energy</a> and <a href="https://blog.google/outreach-initiatives/sustainability/replenishing-water/?_ga=2.140272307.1460901017.1631498684-1474825438.1628277680" target="_blank">a deep commitment to water stewardship</a>, and that are <a href="https://blog.google/outreach-initiatives/sustainability/our-third-decade-climate-action-realizing-carbon-free-future/" target="_blank">helping us move toward a carbon-free future</a>.</li><li><b>Open software:</b> AI Hypercomputer enables developers to access our performance-optimized hardware through the use of open software to tune, manage, and dynamically orchestrate AI training and inference workloads on top of performance-optimized AI hardware.<ul><li>Extensive support for popular ML frameworks such as JAX, TensorFlow, and PyTorch are available right out of the box. Both JAX and PyTorch are powered by <a href="https://github.com/openxla/xla" target="_blank">OpenXLA</a> compiler for building sophisticated LLMs. XLA serves as a foundational backbone, enabling the creation of complex multi-layered models (<a href="https://pytorch.org/blog/high-performance-llama-2/" target="_blank">Llama 2 training and inference on Cloud TPUs with PyTorch/XLA</a>). It optimizes distributed architectures across a wide range of hardware platforms, ensuring easy-to-use and efficient model development for diverse AI use cases (<a href="https://cloud.google.com/blog/products/compute/assemblyai-on-cloud-tpu-v5e-price-performance">AssemblyAI leverages JAX/XLA and Cloud TPUs for large-scale AI speech</a>).</li><li>Open and unique <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">Multislice Training</a> and <a href="https://cloud.google.com/tpu/docs/v5e-inference">Multihost Inferencing</a> software, respectively, make scaling, training, and serving workloads smooth and easy. Developers can scale to tens of thousands of chips to support demanding AI workloads.</li><li>Deep integration with <a href="https://cloud.google.com/kubernetes-engine?hl=en">Google Kubernetes Engine (GKE)</a> and <a href="https://cloud.google.com/compute?hl=en">Google Compute Engine</a>, to deliver efficient resource management, consistent ops environments, autoscaling, node-pool auto-provisioning, auto-checkpointing, auto-resumption, and timely failure recovery.</li></ul></li><li><b>Flexible consumption</b>: AI Hypercomputer offers a wide range of flexible and dynamic consumption choices. In addition to classic options, such as Committed Use Discounts (CUD), on-demand pricing, and spot pricing, AI Hypercomputer provides consumption models tailored for AI workloads via <a href="https://cloud.google.com/blog/products/compute/introducing-dynamic-workload-scheduler">Dynamic Workload Scheduler.</a> Dynamic Workload Scheduler introduces two models: Flex Start mode for higher resource obtainability and optimized economics, as well as Calendar mode, which targets workloads with higher predictability on job-start times.</li></ul><h3><b>Leveraging Google’s deep experience to help power the future of AI</b></h3><p>Customers like Salesforce and Lightricks are already training and serving large AI models with Google Cloud’s TPU v5p AI Hypercomputer — and already seeing a difference:</p><p><i>“We’ve been leveraging Google Cloud TPU v5p for pre-training Salesforce’s foundational models that will serve as the core engine for specialized production use cases, and we’re seeing considerable improvements in our training speed. In fact, Cloud TPU v5p compute outperforms the previous generation TPU v4 by as much as 2X. We also love how seamless and easy the transition has been from Cloud TPU v4 to v5p using JAX. We’re excited to take these speed gains even further by leveraging the native support for INT8 precision format via the Accurate Quantized Training (AQT) library to optimize our models.” -</i> Erik Nijkamp, Senior Research Scientist, Salesforce</p><p><i>“Leveraging the remarkable performance and ample memory capacity of Google Cloud TPU v5p, we successfully trained our generative text-to-video model without splitting it into separate processes. This optimal hardware utilization significantly accelerates each training cycle, allowing us to swiftly conduct a series of experiments. The ability to train our model quickly in each experiment facilitates rapid iteration, which is an invaluable advantage for our research team in this competitive field of generative AI.”</i> - Yoav HaCohen, PhD, Core Generative AI Research Team Lead, Lightricks</p><p><i>“In our early-stage usage, Google DeepMind and Google Research have observed 2X speedups for LLM training workloads using TPU v5p chips compared to the performance on our TPU v4 generation. The robust support for ML Frameworks (JAX, PyTorch, TensorFlow) and orchestration tools enables us to scale even more efficiently on v5p. With the 2nd generation of SparseCores we also see significant improvement in the performance of embeddings-heavy workloads. TPUs are vital to enabling our largest-scale research and engineering efforts on cutting edge models like Gemini.” -</i> Jeff Dean, Chief Scientist, Google DeepMind and Google Research</p><p>At Google, we’ve long believed in the power of AI to help solve challenging problems. Until very recently, training large foundation models and serving them at scale was too complicated and expensive for many organizations. Today, with Cloud TPU v5p and AI Hypercomputer, we’re excited to extend the result of decades of research in AI and systems design with our customers, so they can innovate with AI faster, more efficiently, and more cost effectively.</p><p>To request access to Cloud TPU v5p and AI Hypercomputer, please reach out to your <a href="https://cloud.google.com/contact/">Google Cloud account manager</a>. To learn more about Google Cloud’s AI infrastructure, <a href="https://cloudonair.withgoogle.com/events/summit-applied-ml-summit-23" target="_blank">register to attend Google Cloud Applied AI Summit</a>.</p><hr><p><i><sup>1: MLPerf™ v3.1 Training Closed, multiple benchmarks as shown. Retrieved November 8th, 2023 from</sup></i> <a href="http://mlcommons.org/" target="_blank"><i><sup>mlcommons.org</sup></i></a><i><sup>. Results 3.1-2004. Performance per dollar is not an MLPerf metric. TPU v4 results are unverified: not verified by MLCommons Association. The MLPerf™ name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See</sup></i> <a href="http://www.mlcommons.org/" target="_blank"><i><sup>www.mlcommons.org</sup></i></a> <i><sup>for more information.<br>2: Google Internal Data for TPU v5p as of November, 2023: E2E steptime, SearchAds pCTR, batch size per TPU core 16,384, 125 vp5 chips</sup></i></p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/infrastructure-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Infrastructure Modernization</a></li><li><a href="https://cloud.google.com/blog/topics/systems" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/topics/systems" track-metadata-module="tag list" track-metadata-module_headline="posted in">Systems</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini: Google's most capable AI model yet (770 pts)]]></title>
            <link>https://blog.google/technology/ai/google-gemini-ai/</link>
            <guid>38544746</guid>
            <pubDate>Wed, 06 Dec 2023 15:05:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/google-gemini-ai/">https://blog.google/technology/ai/google-gemini-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=38544746">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
  }">
      
      
        <p>
          Making AI more helpful for everyone
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="The word “Gemini” above five separate threads, each a different color, converge from the left into a three-dimensional central helix before separating back out toward the right into five individual strands once more.">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Note from Sundar" href="#sundar-note" id="sundar-note-anchor">Note from Sundar</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini" href="#introducing-gemini" id="introducing-gemini-anchor">Introducing Gemini</a>
        </li>
        
        <li>
          <a aria-label="link to State-of-the-art performance" href="#performance" id="performance-anchor">State-of-the-art performance</a>
        </li>
        
        <li>
          <a aria-label="link to Next-generation capabilities" href="#capabilities" id="capabilities-anchor">Next-generation capabilities</a>
        </li>
        
        <li>
          <a aria-label="link to Scalable and efficient" href="#scalable-efficient" id="scalable-efficient-anchor">Scalable and efficient</a>
        </li>
        
        <li>
          <a aria-label="link to Responsibility and safety" href="#responsibility-safety" id="responsibility-safety-anchor">Responsibility and safety</a>
        </li>
        
        <li>
          <a aria-label="link to Availability" href="#availability" id="availability-anchor">Availability</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">
            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="xdxwr"><i>A note from Google and Alphabet CEO Sundar Pichai:</i></p><p data-block-key="6590u">Every technology shift is an opportunity to advance scientific discovery, accelerate human progress, and improve lives. I believe the transition we are seeing right now with AI will be the most profound in our lifetimes, far bigger than the shift to mobile or to the web before it. AI has the potential to create opportunities — from the everyday to the extraordinary — for people everywhere. It will bring new waves of innovation and economic progress and drive knowledge, learning, creativity and productivity on a scale we haven’t seen before.</p><p data-block-key="3ffsc">That’s what excites me: the chance to make AI helpful for everyone, everywhere in the world.</p><p data-block-key="6sa5">Nearly eight years into our journey as an AI-first company, the pace of progress is only accelerating: Millions of people are now using generative AI across our products to do things they couldn’t even a year ago, from finding answers to more complex questions to using new tools to collaborate and create. At the same time, developers are using our models and infrastructure to build new generative AI applications, and startups and enterprises around the world are growing with our AI tools.</p><p data-block-key="fafvp">This is incredible momentum, and yet, we’re only beginning to scratch the surface of what’s possible.</p><p data-block-key="chghs">We’re approaching this work boldly and responsibly. That means being ambitious in our research and pursuing the capabilities that will bring enormous benefits to people and society, while building in safeguards and working collaboratively with governments and experts to address risks as AI becomes more capable. And we continue to invest in the very best tools, foundation models and infrastructure and bring them to our products and to others, guided by our <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a>.</p><p data-block-key="akdk1">Now, we’re taking the next step on our journey with Gemini, our most capable and general model yet, with state-of-the-art performance across many leading benchmarks. Our first version, Gemini 1.0, is optimized for different sizes: Ultra, Pro and Nano. These are the first models of the Gemini era and the first realization of the vision we had when we formed Google DeepMind earlier this year. This new era of models represents one of the biggest science and engineering efforts we’ve undertaken as a company. I’m genuinely excited for what’s ahead, and for the opportunities Gemini will unlock for people everywhere.</p><p data-block-key="87ult">– Sundar</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="ede3f">Introducing Gemini</h2><p data-block-key="copss"><i>By Demis Hassabis, CEO and Co-Founder of Google DeepMind, on behalf of the Gemini team</i></p><p data-block-key="ff9rj">AI has been the focus of my life's work, as for many of my research colleagues. Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.</p><p data-block-key="i7i7">This promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.</p><p data-block-key="98mu4">Today, we’re a step closer to this vision as <a href="https://deepmind.google/technologies/gemini" rt-link-type="external">we introduce Gemini</a>, the most capable and general model we’ve ever built.</p><p data-block-key="aka6e">Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="jV1vkHv4zq8" data-index-id="5" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Introducing Gemini: our largest and most capable AI model." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Introducing Gemini: our largest and most capable AI model</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">Gemini is also our most flexible model yet — able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI.</p><p data-block-key="8j5hi">We’ve optimized Gemini 1.0, our first version, for three different sizes:</p><ul><li data-block-key="103ti"><b>Gemini Ultra</b> — our largest and most capable model for highly complex tasks.</li><li data-block-key="498p0"><b>Gemini Pro</b> — our best model for scaling across a wide range of tasks.</li><li data-block-key="2sl86"><b>Gemini Nano</b> — our most efficient model for on-device tasks.</li></ul></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">State-of-the-art performance</h2><p data-block-key="7ormq">We've been rigorously testing our Gemini models and evaluating their performance on a wide variety of tasks. From natural image, audio and video understanding to mathematical reasoning, Gemini Ultra’s performance exceeds current state-of-the-art results on 30 of the 32 widely-used academic benchmarks used in large language model (LLM) research and development.</p><p data-block-key="61bg2">With a score of 90.0%, Gemini Ultra is the first model to outperform human experts on <a href="https://arxiv.org/abs/2009.03300" rt-link-type="external">MMLU</a> (massive multitask language understanding), which uses a combination of 57 subjects such as math, physics, history, law, medicine and ethics for testing both world knowledge and problem-solving abilities.</p><p data-block-key="at1df">Our new benchmark approach to MMLU enables Gemini to use its reasoning capabilities to think more carefully before answering difficult questions, leading to significant improvements over just using its first impression.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A chart showing Gemini Ultra’s performance on common text benchmarks, compared to GPT-4 (API numbers calculated where reported numbers were missing)." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_text_table_bigger_font_amendment_lines.gif">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">Gemini surpasses state-of-the-art performance on a range of benchmarks including text and coding.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">Gemini Ultra also achieves a state-of-the-art score of 59.4% on the new <a href="https://arxiv.org/abs/2311.16502" rt-link-type="external">MMMU</a> benchmark, which consists of multimodal tasks spanning different domains requiring deliberate reasoning.</p><p data-block-key="2pfr3">With the image benchmarks we tested, Gemini Ultra outperformed previous state-of-the-art models, without assistance from object character recognition (OCR) systems that extract text from images for further processing. These benchmarks highlight Gemini’s native multimodality and indicate early signs of Gemini's more complex reasoning abilities.</p><p data-block-key="cihqv">See more details in our <a href="https://goo.gle/GeminiPaper" rt-link-type="external">Gemini technical report</a>.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A chart showing Gemini Ultra’s performance on multimodal benchmarks compared to GPT-4V, with previous SOTA models listed in places where capabilities are not supported in GPT-4V." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_multimodal_table_bigger_font_amendment_lines.gif">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">Gemini surpasses state-of-the-art performance on a range of multimodal benchmarks.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">Next-generation capabilities</h2><p data-block-key="90lfq">Until now, the standard approach to creating multimodal models involved training separate components for different modalities and then stitching them together to roughly mimic some of this functionality. These models can sometimes be good at performing certain tasks, like describing images, but struggle with more conceptual and complex reasoning.</p><p data-block-key="c2vjn">We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are stateof the art in nearly every domain.</p><p data-block-key="9ad54">Learn more about <a href="https://deepmind.google/technologies/gemini" rt-link-type="external">Gemini’s capabilities and see how it works</a>.</p><h3 data-block-key="4mlv7">Sophisticated reasoning</h3><p data-block-key="1s4j6">Gemini 1.0’s sophisticated multimodal reasoning capabilities can help make sense of complex written and visual information. This makes it uniquely skilled at uncovering knowledge that can be difficult to discern amid vast amounts of data.</p><p data-block-key="3lp95">Its remarkable ability to extract insights from hundreds of thousands of documents through reading, filtering and understanding information will help deliver new breakthroughs at digital speeds in many fields from science to finance.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="sPiOP_CB54A" data-index-id="14" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini unlocks new scientific insights." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastia.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini unlocks new scientific insights</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h3 data-block-key="39bmn">Understanding text, images, audio and more</h3><p data-block-key="acvam">Gemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. This makes it especially good at explaining reasoning in complex subjects like math and physics.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="K4pX1VAxaAI" data-index-id="16" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini explains reasoning in math and physics." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini explains reasoning in math and physics</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h3 data-block-key="39bmn">Advanced coding</h3><p data-block-key="f2037">Our first version of Gemini can understand, explain and generate high-quality code in the world’s most popular programming languages, like Python, Java, C++, and Go. Its ability to work across languages and reason about complex information makes it one of the leading foundation models for coding in the world.</p><p data-block-key="6rsks">Gemini Ultra excels in several coding benchmarks, including <a href="https://arxiv.org/abs/2107.03374" rt-link-type="external">HumanEval</a>, an important industry-standard for evaluating performance on coding tasks, and Natural2Code, our internal held-out dataset, which uses author-generated sources instead of web-based information.</p><p data-block-key="38e8l">Gemini can also be used as the engine for more advanced coding systems. Two years ago we presented <a href="https://deepmind.google/discover/blog/competitive-programming-with-alphacode/" rt-link-type="external">AlphaCode</a>, the first AI code generation system to reach a competitive level of performance in programming competitions.</p><p data-block-key="915ut">Using a specialized version of Gemini, we created a more advanced code generation system, <a href="https://goo.gle/AlphaCode2" rt-link-type="external">AlphaCode 2</a>, which excels at solving competitive programming problems that go beyond coding to involve complex math and theoretical computer science.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="LvGmVmHv69s" data-index-id="18" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini excels at coding and competitive programming." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini excels at coding and competitive programming</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">When evaluated on the same platform as the original AlphaCode, AlphaCode 2 shows massive improvements, solving nearly twice as many problems, and we estimate that it performs better than 85% of competition participants — up from nearly 50% for AlphaCode. When programmers collaborate with AlphaCode 2 by defining certain properties for the code samples to follow, it performs even better.</p><p data-block-key="brofo">We’re excited for programmers to increasingly use highly capable AI models as collaborative tools that can help them reason about the problems, propose code designs and assist with implementation — so they can release apps and design better services, faster.</p><p data-block-key="3jkkn">See more details in our <a href="https://goo.gle/AlphaCode2" rt-link-type="external">AlphaCode 2 technical report</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">More reliable, scalable and efficient</h2><p data-block-key="85utf">We trained Gemini 1.0 at scale on our AI-optimized infrastructure using Google’s in-house designed <a href="https://cloud.google.com/tpu?hl=en" rt-link-type="external">Tensor Processing Units</a> (TPUs) v4 and v5e. And we designed it to be our most reliable and scalable model to train, and our most efficient to serve.</p><p data-block-key="5ujmc">On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models. These custom-designed AI accelerators have been at the heart of Google's AI-powered products that serve billions of users like Search, YouTube, Gmail, Google Maps, Google Play and Android. They’ve also enabled companies around the world to train large-scale AI models cost-efficiently.</p><p data-block-key="6kgb1">Today, we’re announcing the most powerful, efficient and scalable TPU system to date, <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer" rt-link-type="external">Cloud TPU v5p</a>, designed for training cutting-edge AI models. This next generation TPU will accelerate Gemini’s development and help developers and enterprise customers train large-scale generative AI models faster, allowing new products and capabilities to reach customers sooner.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-1000.format-webp.webp&quot;
              }">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="dapif">Built with responsibility and safety at the core</h2><p data-block-key="6pvob">At Google, we’re committed to advancing bold and responsible AI in everything we do. Building upon Google’s <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a> and the robust safety policies across our products, we’re adding new protections to account for Gemini’s multimodal capabilities. At each stage of development, we’re considering potential risks and working to test and mitigate them.</p><p data-block-key="4rc0o">Gemini has the most comprehensive safety evaluations of any Google AI model to date, including for bias and toxicity. We’ve conducted <a href="https://deepmind.google/discover/blog/an-early-warning-system-for-novel-ai-risks/" rt-link-type="external">novel research into potential risk areas</a> like cyber-offense, persuasion and autonomy, and have applied Google Research’s best-in-class <a href="https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html" rt-link-type="external">adversarial testing techniques</a> to help identify critical safety issues in advance of Gemini’s deployment.</p><p data-block-key="1tti2">To identify blindspots in our internal evaluation approach, we’re working with a diverse group of external experts and partners to stress-test our models across a range of issues.</p><p data-block-key="9hpod">To diagnose content safety issues during Gemini’s training phases and ensure its output follows our policies, we’re using benchmarks such as <a href="https://allenai.org/data/real-toxicity-prompts" rt-link-type="external">Real Toxicity Prompts</a>, a set of 100,000 prompts with varying degrees of toxicity pulled from the web, developed by experts at the Allen Institute for AI. Further details on this work are coming soon.</p><p data-block-key="1ovh">To limit harm, we built dedicated safety classifiers to identify, label and sort out content involving violence or negative stereotypes, for example. Combined with robust filters, this layered approach is designed to make Gemini safer and more inclusive for everyone. Additionally, we’re continuing to address known challenges for models such as factuality, grounding, attribution and corroboration.</p><p data-block-key="6cdjb">Responsibility and safety will always be central to the development and deployment of our models. This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like <a href="https://mlcommons.org/" rt-link-type="external">MLCommons</a>, the <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" rt-link-type="external">Frontier Model Forum</a> <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" rt-link-type="external">and</a> its <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum-executive-director/" rt-link-type="external">AI Safety Fund</a>, and our <a href="https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/" rt-link-type="external">Secure AI Framework (SAIF)</a>, which was designed to help mitigate security risks specific to AI systems across the public and private sectors. We’ll continue partnering with researchers, governments and civil society groups around the world as we develop Gemini.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">Making Gemini available to the world</h2><p data-block-key="5l85j">Gemini 1.0 is now rolling out across a range of products and platforms:<br></p><h3 data-block-key="a245r">Gemini Pro in Google products</h3><p data-block-key="e5h3k">We’re bringing Gemini to billions of people through Google products.</p><p data-block-key="dp2ls">Starting today, <a href="https://blog.google/products/bard/google-bard-try-gemini-ai" rt-link-type="external">Bard will use a fine-tuned version of Gemini Pro</a> for more advanced reasoning, planning, understanding and more. This is the biggest upgrade to Bard since it launched. It will be available in English in more than 170 countries and territories, and we plan to expand to different modalities and support new languages and locations in the near future.</p><p data-block-key="5erfe">We’re also <a href="https://blog.google/products/pixel/pixel-feature-drop-december-2023/" rt-link-type="internal">bringing Gemini to Pixel</a>. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp — with more messaging apps coming next year.</p><p data-block-key="3ah01">In the coming months, Gemini will be available in more of our products and services like Search, Ads, Chrome and Duet AI.</p><p data-block-key="7s7gn">We’re already starting to experiment with Gemini in Search, where it's making our <a href="https://labs.google/sge/" rt-link-type="external">Search Generative Experience</a> (SGE) faster for users, with a 40% reduction in latency in English in the U.S., alongside improvements in quality.</p><h3 data-block-key="fpjq5">Building with Gemini</h3><p data-block-key="3e6sk">Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or <a href="https://cloud.google.com/vertex-ai" rt-link-type="external">Google Cloud Vertex AI</a>.</p><p data-block-key="51fbs">Google AI Studio is a free, web-based developer tool to prototype and launch apps quickly with an API key. When it's time for a fully-managed AI platform, Vertex AI allows customization of Gemini with full data control and benefits from additional Google Cloud features for enterprise security, safety, privacy and data governance and compliance.</p><p data-block-key="5lr43">Android developers will also be able to build with Gemini Nano, our most efficient model for on-device tasks, via AICore, a new system capability available in Android 14, starting on Pixel 8 Pro devices. Sign up for an <a href="https://android-developers.googleblog.com/2023/12/a-new-foundation-for-ai-on-android.html" rt-link-type="external">early preview of AICore</a>.</p><h3 data-block-key="a0kru">Gemini Ultra coming soon</h3><p data-block-key="hhfg">For Gemini Ultra, we’re currently completing extensive trust and safety checks, including red-teaming by trusted external parties, and further refining the model using fine-tuning and reinforcement learning from human feedback (RLHF) before making it broadly available.</p><p data-block-key="aubeq">As part of this process, we’ll make Gemini Ultra available to select customers, developers, partners and safety and responsibility experts for early experimentation and feedback before rolling it out to developers and enterprise customers early next year.</p><p data-block-key="22un0">Early next year, we’ll also launch <a href="https://blog.google/products/bard/google-bard-try-gemini-ai" rt-link-type="external">Bard Advanced</a>, a new, cutting-edge AI experience that gives you access to our best models and capabilities, starting with Gemini Ultra.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">The Gemini era: enabling a future of innovation</h2><p data-block-key="ausk2">This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.</p><p data-block-key="936sj">We’ve made great progress on Gemini so far and we’re working hard to further extend its capabilities for future versions, including advances in planning and memory, and increasing the context window for processing even more information to give better responses.</p><p data-block-key="2i1b7">We’re excited by the amazing possibilities of a world responsibly empowered by AI — a future of innovation that will enhance creativity, extend knowledge, advance science and transform the way billions of people live and work around the world.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini – Google DeepMind (1688 pts)]]></title>
            <link>https://deepmind.google/technologies/gemini/</link>
            <guid>38544729</guid>
            <pubDate>Wed, 06 Dec 2023 15:03:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a>, See on <a href="https://news.ycombinator.com/item?id=38544729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tr><th><p>Image</p></th><td></td><td></td><td scope="row"><p>MMMU<span>Multi-discipline college-level reasoning problems</span></p></td><td><p>Multi-discipline college-level reasoning problems</p></td><td><p><span data-no-percent="false">59.4%</span><span>0-shot pass@1 <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">56.8%</span><span>0-shot pass@1 <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>VQAv2<span>Natural image understanding</span></p></td><td><p>Natural image understanding</p></td><td><p><span data-no-percent="false">77.8%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">77.2%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>TextVQA<span>OCR on natural images</span></p></td><td><p>OCR on natural images</p></td><td><p><span data-no-percent="false">82.3%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">78%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>DocVQA<span>Document understanding</span></p></td><td><p>Document understanding</p></td><td><p><span data-no-percent="false">90.9%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">88.4%</span><span>0-shot <br> GPT-4V (pixel only)</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>Infographic VQA<span>Infographic understanding</span></p></td><td><p>Infographic understanding</p></td><td><p><span data-no-percent="false">80.3%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">75.1%</span><span>0-shot <br> GPT-4V (pixel only)</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>MathVista<span>Mathematical reasoning in visual contexts</span></p></td><td><p>Mathematical reasoning in visual contexts</p></td><td><p><span data-no-percent="false">53%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">49.9%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th><p>Video</p></th><td></td><td></td><td scope="row"><p>VATEX<span>English video captioning <br>(CIDEr)</span></p></td><td><p>English video captioning <br>(CIDEr)</p></td><td><p><span data-no-percent="true">62.7</span><span>4-shot <br> Gemini Ultra</span></p></td><td><p><span data-no-percent="true">56</span><span>4-shot <br> DeepMind Flamingo</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>Perception Test MCQA<span>Video question answering</span></p></td><td><p>Video question answering</p></td><td><p><span data-no-percent="false">54.7%</span><span>0-shot <br> Gemini Ultra</span></p></td><td><p><span data-no-percent="false">46.3%</span><span>0-shot <br> SeViLA</span></p></td></tr><tr><th><p>Audio</p></th><td></td><td></td><td scope="row"><p>CoVoST 2 (21 languages)<span>Automatic speech translation <br>(BLUE score)</span></p></td><td><p>Automatic speech translation <br>(BLUE score)</p></td><td><p><span data-no-percent="true">40.1</span><span>Gemini Pro</span></p></td><td><p><span data-no-percent="true">29.1</span><span>Whisper v2</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>FLEURS (62 languages)<span>Automatic speech recognition <br>(based on word error rate, lower is better)</span></p></td><td><p>Automatic speech recognition <br>(based on word error rate, lower is better)</p></td><td><p><span data-no-percent="false">7.6%</span><span>Gemini Pro</span></p></td><td><p><span data-no-percent="false">17.6%</span><span>Whisper v3</span></p></td></tr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Confirms Governments Using Push Notifications to Surveil Users (546 pts)]]></title>
            <link>https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/</link>
            <guid>38543587</guid>
            <pubDate>Wed, 06 Dec 2023 13:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/">https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/</a>, See on <a href="https://news.ycombinator.com/item?id=38543587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2023/12/06/apple-governments-surveil-push-notifications/"><p>Unidentified governments are surveilling smartphone users by tracking push notifications that move through Google's and Apple's servers, a US senator warned on Wednesday (via <em><a href="https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/">Reuters</a></em>).</p>
<p><img src="https://images.macrumors.com/t/JdCJ-AJ3Ort9AETOl6rhY0h-yKg=/400x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy" srcset="https://images.macrumors.com/t/JdCJ-AJ3Ort9AETOl6rhY0h-yKg=/400x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy 400w,https://images.macrumors.com/t/SXvAzJ10FPOQRZ5HpBBKDyjgeF8=/800x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy 800w,https://images.macrumors.com/t/esEjyyoBJArH9DuUVOxzSi_hc5g=/1600x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg 1600w,https://images.macrumors.com/t/TjaJpIne8IG3rDlqM3tgUYkDAc8=/2500x0/filters:no_upscale()/article-new/2023/02/iOS-16-4-Web-Push.jpeg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 16 4 Web Push" width="794" height="450"><br>In a letter to the Department of Justice, Senator Ron Wyden said foreign officials were demanding the data from the tech giants to track smartphones. The traffic flowing from apps that send push notifications put the companies "in a unique position to facilitate government surveillance of how users are using particular apps," Wyden said. He asked the Department of Justice to "repeal or modify any policies" that hindered public discussions of push notification spying.</p>
<p>In a statement given to <em>Reuters</em>, Apple said that Wyden's letter gave them the opening they needed to share more details with the public about how governments monitored push notifications.<br>
</p>
<blockquote><p>"In this case, the federal government prohibited us from sharing any information," the company said in a statement. "Now that this method has become public we are updating our transparency reporting to detail these kinds of requests."</p></blockquote>
<p>According to the report, Wyden's letter said a "tip" was the source of the information about the surveillance. A source familiar with the matter confirmed that both foreign and U.S. government agencies have been asking Apple and Google for metadata related to push notifications. The data is said to have been used to attempt to tie anonymous users of messaging apps to specific Apple or Google accounts.</p>
<p><em>Reuters</em>' source would not identify which governments were making the data requests but described them as "democracies allied to the United States." They did not know how long the requests had been going on for. </p>
<p>Apple <a href="https://developer.apple.com/documentation/usernotifications/setting_up_a_remote_notification_server/generating_a_remote_notification">advises developers</a> not to include sensitive data in notifications and to encrypt any data before adding it to a notification payload. However, this requires action on the developers' part. Likewise, metadata (like which apps are sending notifications and how often) is not encrypted, potentially giving anyone with access to the information insight into users' app usage.</p>
<p><small>Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our <a href="https://forums.macrumors.com/forums/political-news.218/">Political News</a> forum. All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.</small></p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2023/12/01/ios-17-2-list-of-12-new-features/">iOS 17.2 Will Add These 12 New Features to Your iPhone</a></h3><p>Friday December 1, 2023 12:19 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>iOS 17.2 has been in beta testing for over a month, and it should be released to all users in a few more weeks. The software update includes many new features and changes for iPhones, including the dozen that we have highlighted below. iOS 17.2 is expected to be released to the public in mid-December. To learn about even more features coming in the update, check out our full list. Journal ...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/01/ankers-cyber-week-sale-final-days/">Anker's Cyber Week Sale Enters Final Days With Up to 60% Off Sitewide</a></h3><p>Anker's Black Friday/Cyber Week event is entering its final days this weekend, and it's still offering up to 60 percent off sitewide. There are also a few "mystery boxes" that can include hundreds of dollars in savings, if you're willing to risk not knowing what you're buying ahead of time. All of these sales will end on December 3. Note: MacRumors is an affiliate partner with Anker. When you...</p></div><div><h3><a href="https://www.macrumors.com/2023/11/30/iphone-green-bubbles-rcs-support/">Green Bubbles on iPhone to Gain These 7 New Features Next Year</a></h3><p>Thursday November 30, 2023 9:00 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Earlier this month, Apple announced that it will finally support RCS in the Messages app on the iPhone starting later next year. This change will result in several improvements to the messaging experience between iPhones and Android devices. RCS will become the new default standard for messaging between iPhones and Android devices, but these conversations will still have green bubbles like...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/02/top-stories-ios-17-1-2-released/">Top Stories: iOS 17.1.2 Released, NameDrop Misinformation, and More</a></h3><p>Apple employees are back to work following a Thanksgiving break, and that means this week saw a number of new operating system updates for both public release and beta testing. This week also saw some misinformation about Apple's new NameDrop feature making the rounds, while Apple and Goldman Sachs appear to be on the verge of a break-up in their Apple Card and savings account partnership,...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/05/instagram-messenger-chats-disconnecting/">Instagram and Facebook Messenger Chats to Disconnect This Month</a></h3><p>Tuesday December 5, 2023 1:57 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Meta has revealed plans to end Instagram users' ability to chat with Facebook accounts later this month, rolling back a feature that it introduced over three years ago. In September 2020, Meta (then Facebook) announced it was merging its Facebook Messenger service with Instagram direct messaging, allowing Instagram users to chat with Facebook users and vice versa using the same platform....</p></div><div><h3><a href="https://www.macrumors.com/2023/12/04/apple-work-on-6g-expanding/">Apple's Work on 6G Connectivity Already Expanding</a></h3><p>Apple's work on implementing 6G cellular connectivity on its devices appears to be ramping up, according to Bloomberg's Mark Gurman. In the latest edition of his "Power On" newsletter, Gurman explained that Apple is increasingly turning its attention to 6G, even amid its widely reported difficulties developing a custom 5G cellular modem. In 2021, the first highly specific Apple job...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastering Nim, 2nd edition (122 pts)]]></title>
            <link>https://nim-lang.org/blog/2023/09/19/mastering-nim.html</link>
            <guid>38543491</guid>
            <pubDate>Wed, 06 Dec 2023 13:21:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nim-lang.org/blog/2023/09/19/mastering-nim.html">https://nim-lang.org/blog/2023/09/19/mastering-nim.html</a>, See on <a href="https://news.ycombinator.com/item?id=38543491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <h3>
            <span>
              
              19 September 2023
            </span>
            
            <span>
              
              The Nim Team
            </span>
            
          </h3>
          <p>Discover the secret of Nim!</p>

<p>The definite guide on Nim!
Written by the inventor himself.</p>

<p>Now with updated content for version 2.0 which solves the biggest pain point of Nim 1.0, shared memory in a multi-threaded setting.</p>

<p>Please have a look at its cover image:</p>
<p>
  <img width="auto" height="600" src="https://nim-lang.org//assets/img/mastering_nim_2.jpg">
</p>

<p><strong>But Nim’s logo is a crown!
Where is the crown?</strong>
That’s the secret of Nim!</p>

<p>Send us your reply to <a href="https://nim-lang.org/cdn-cgi/l/email-protection" data-cfemail="c9babcb9b9a6bbbd89a7a0a4e4a5a8a7aee7a6bbae">[email&nbsp;protected]</a> until December 6th 2023.
Among the correct answers we will select 3 winners by randomization.
The winners will receive a signed hardcover!</p>

<p>“Mastering Nim” is available here:</p>

<ul>
  <li><a href="https://www.amazon.com/dp/B0B4R7B9YX">amazon.com</a></li>
  <li><a href="https://www.amazon.de/dp/B0B4R7B9YX">amazon.de</a></li>
</ul>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jailbroken AI Chatbots Can Jailbreak Other Chatbots (104 pts)]]></title>
            <link>https://www.scientificamerican.com/article/jailbroken-ai-chatbots-can-jailbreak-other-chatbots/</link>
            <guid>38543322</guid>
            <pubDate>Wed, 06 Dec 2023 13:06:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scientificamerican.com/article/jailbroken-ai-chatbots-can-jailbreak-other-chatbots/">https://www.scientificamerican.com/article/jailbroken-ai-chatbots-can-jailbreak-other-chatbots/</a>, See on <a href="https://news.ycombinator.com/item?id=38543322">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>December 6, 2023</p><p>3<!-- --> min read</p></div><p>AI chatbots can convince other chatbots to instruct users how to build bombs and cook meth</p><figure><img src="https://static.scientificamerican.com/sciam/cache/file/5FD15525-2E46-4D42-A4C91ED3D7ABD97D_source.png?w=600" alt="Illustration of symbolic representations of good and evil AI morality" srcset="https://static.scientificamerican.com/sciam/cache/file/5FD15525-2E46-4D42-A4C91ED3D7ABD97D_source.png?w=600 600w, https://static.scientificamerican.com/sciam/cache/file/5FD15525-2E46-4D42-A4C91ED3D7ABD97D_source.png?w=900 900w, https://static.scientificamerican.com/sciam/cache/file/5FD15525-2E46-4D42-A4C91ED3D7ABD97D_source.png?w=1000 1000w, https://static.scientificamerican.com/sciam/cache/file/5FD15525-2E46-4D42-A4C91ED3D7ABD97D_source.png?w=1200 1200w, https://static.scientificamerican.com/sciam/cache/file/5FD15525-2E46-4D42-A4C91ED3D7ABD97D_source.png?w=1350 1350w" sizes="(min-width: 900px) 900px, (min-resolution: 2dppx) 75vw, (min-resolution: 2.1dppx) 50vw, 100vw" fetchpriority="high"><figcaption> </figcaption></figure></div><div><p data-block="sciam/paragraph">Today’s artificial intelligence chatbots have built-in restrictions to keep them from providing users with dangerous information, but a new preprint study shows how to get AIs to trick each other into giving up those secrets. In it, researchers observed the targeted AIs <a href="https://arxiv.org/abs/2311.03348">breaking</a><a href="https://arxiv.org/abs/2311.03348"> the rules</a> to offer advice on how to synthesize methamphetamine, build a bomb and launder money.</p><p data-block="sciam/paragraph">Modern chatbots have the power to adopt personas by feigning specific personalities or acting like fictional characters. The new study took advantage of that ability by asking a particular AI chatbot to act as a research assistant. Then the researchers instructed this assistant to help develop prompts that could “jailbreak” other chatbots—destroy the guardrails encoded into such programs.</p><p data-block="sciam/paragraph">The research assistant chatbot’s automated attack techniques proved to be successful 42.5 percent of the time against GPT-4, one of the large language models (LLMs) that power ChatGPT. It was also successful 61 percent of the time against Claude 2, the model underpinning Anthropic’s chatbot, and 35.9 percent of the time against Vicuna, an open-source chatbot.</p><p data-block="sciam/paragraph">“We want, as a society, to be aware of the risks of these models,” says study co-author Soroush Pour, founder of the AI safety company Harmony Intelligence. “We wanted to show that it was possible and demonstrate to the world the challenges we face with this current generation of LLMs.”</p><p data-block="sciam/paragraph">Ever since LLM-powered chatbots became available to the public, enterprising mischief-makers have been able <a href="https://www.newscientist.com/article/2388231-tricks-for-making-ai-chatbots-break-rules-are-freely-available-online/">to jailbreak</a> the programs. By asking chatbots the right questions, people have previously convinced the machines to ignore preset rules and offer criminal advice, such as a recipe for napalm. As these techniques have been made public, AI model developers have raced to patch them—a cat-and-mouse game requiring attackers to come up with new methods. That takes time.</p><p data-block="sciam/paragraph">But asking AI to formulate strategies that convince other AIs to ignore their safety rails can speed the process up by a factor of 25, according to the researchers. And the success of the attacks across different chatbots suggested to the team that the issue reaches beyond individual companies’ code. The vulnerability seems to be inherent in the design of AI-powered chatbots more widely.</p><p data-block="sciam/paragraph">OpenAI, Anthropic and the team behind Vicuna were approached to comment on the paper’s findings. OpenAI declined to comment, while Anthropic and Vicuna had not responded at the time of publication.</p><p data-block="sciam/paragraph">“In the current state of things, our attacks mainly show that we can get models to say things that LLM developers don’t want them to say,” says Rusheb Shah, another co-author of the study. “But as models get more powerful, maybe the potential for these attacks to become dangerous grows.”</p><p data-block="sciam/paragraph">The challenge, Pour says, is that persona impersonation “is a very core thing that these models do.” They aim to achieve what the user wants, and they specialize in assuming different personalities—which proved central to the form of exploitation used in the new study. Stamping out their ability to take on potentially harmful personas, such as the “research assistant” that devised jailbreaking schemes, will be tricky. “Reducing it to zero is probably unrealistic,” Shah says. “But it's important to think, ‘How close to zero can we get?’”</p><p data-block="sciam/paragraph">“We should have learned from earlier attempts to create chat agents—such as when <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">Microsoft’s Tay was easily manipulated</a> into spouting racist and sexist viewpoints—that they are very hard to control, particularly given that they are trained from information on the Internet and every good and nasty thing that’s in it,” says Mike Katell, an ethics fellow at the Alan Turing Institute in England, who was not involved in the new study.</p><p data-block="sciam/paragraph">Katell acknowledges that organizations developing LLM-based chatbots are currently putting lots of work into making them safe. The developers are trying to tamp down users’ ability to jailbreak their systems and put those systems to nefarious work, such as that highlighted by Shah, Pour and their colleagues. Competitive urges may end up winning out, however, Katell says. “How much effort are the LLM providers willing to put in to keep them that way?” he says. “At least a few will probably tire of the effort and just let them do what they do.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mold Course (357 pts)]]></title>
            <link>https://www.epa.gov/mold/mold-course-introduction</link>
            <guid>38543229</guid>
            <pubDate>Wed, 06 Dec 2023 12:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epa.gov/mold/mold-course-introduction">https://www.epa.gov/mold/mold-course-introduction</a>, See on <a href="https://news.ycombinator.com/item?id=38543229">Hacker News</a></p>
Couldn't get https://www.epa.gov/mold/mold-course-introduction: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Playstation keeps reminding us why digital ownership sucks (326 pts)]]></title>
            <link>https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks</link>
            <guid>38543196</guid>
            <pubDate>Wed, 06 Dec 2023 12:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks">https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks</a>, See on <a href="https://news.ycombinator.com/item?id=38543196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In less than a week, Sony has given us two timely reminders of the tenuousness of digital “ownership” — and both reminders involve things on PlayStation.</p><p>Last week, Sony said that, because of content licensing “arrangements,” users wouldn’t be able to watch Discovery content they’ve purchased <em>and</em> that the content would be removed from their libraries as of December 31st, 2023. The resulting list of shows that will suddenly disappear because of corporate agreements is <a href="https://www.playstation.com/en-us/legal/psvideocontent/?et_rid=&amp;et_cid=231130-VIDREMVL-AM-CSA-B-FLX&amp;Linkid=231130-VIDREMVL-AM-CSA-B-FLX&amp;emcid=em-pl-500377">very long</a>. Shows disappearing from streaming services is commonplace, but in this case, people are losing access to shows they bought to watch on demand whenever they wanted.</p><p>Then, on Monday, many users were unexpectedly banned from their <a href="https://www.theverge.com/2023/12/4/23988621/sony-playstation-account-ps5-bans-permanent-suspension">PlayStation Network accounts</a>, meaning that not only were they blocked from playing multiplayer games or using cloud streaming but they were also locked out of games they purchased digitally from Sony’s PlayStation marketplace. Affected users who may have spent years building a robust digital library were suddenly left without access to content they had bought through no fault of their own. It appears that Sony has since restored account access to people who were accidentally banned, but the company hasn’t explained what happened or said how it might prevent similar unexpected bans in the future. (Sony hasn’t replied to our multiple requests for comment.)</p><p>The ephemerality of digital “ownership” isn’t a new issue. Even though downloading and accessing digital content is often easier than trudging to a retail store to buy a physical copy of a game, you’re putting your faith in the platform holders to maintain their digital storefronts, the content on those storefronts, and their account systems so that your access keeps working.</p><p>The recent closure <a href="https://www.theverge.com/2023/3/26/23657431/wii-u-nintendo-3ds-eshops-shut-down">of Nintendo’s Wii U and 3DS eShops</a> was a stark reminder that companies have the power to decide when you can buy digital content. While you can still redownload Wii U and 3DS games that you’ve purchased, it seems inevitable that Nintendo will stop letting you do that one day. (It’s already planning to shut down online services for those platforms, <a href="https://www.theverge.com/2023/10/4/23902615/wii-u-nintendo-3ds-online-shut-down">after all</a>.) And remember <a href="https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023">when Google shut down Stadia</a>?</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="I’m considering switching back to physical games." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>I’m considering switching back to physical games.</em></figcaption> <p><cite>Photo by James Bareham / The Verge</cite></p></div></div><p>These recent PlayStation incidents are more aggravating, however, because of how sudden and seemingly unfair they are. With the Discovery content, Sony is giving users a matter of weeks to watch their purchased shows for the last time before the shows are yanked from their library entirely. And Sony isn’t offering any compensation for titles you’ve already bought or a way to transfer those purchases to another store. The PlayStation account bans were as swift as they were unexpected, and while resolution for most arrived within a few hours, Sony still hasn’t shared any public communication about what happened or why users should continue to trust the platform.</p><p>I’ve been all in on digital content for years. I don’t like the clutter of physical boxes, and I enjoy being able to switch games and movies without having to get off the couch. But after seeing more instances of companies removing “purchased” digital content — essentially making things I buy digitally a long-term rental — I’m seriously considering going back to buying discs and cartridges.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Governments spying on Apple, Google users through push notifications -US senator (692 pts)]]></title>
            <link>https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/</link>
            <guid>38543155</guid>
            <pubDate>Wed, 06 Dec 2023 12:49:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/">https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/</a>, See on <a href="https://news.ycombinator.com/item?id=38543155">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Norway Joins Denmark in Swedish Tesla Strike/Blockade (193 pts)]]></title>
            <link>https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b</link>
            <guid>38542892</guid>
            <pubDate>Wed, 06 Dec 2023 12:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b">https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b</a>, See on <a href="https://news.ycombinator.com/item?id=38542892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>– De tar striden mot ett fackfientligt bolag på hela arbetsmarknaden vägnar. Nu måste Tesla omedelbart acceptera IF Metalls krav om ett kollektivavtal, säger fackbasen Jørn Eggum till den norska tidningen Dagbladets Børsen.</p><p>Om Tesla inte tillmötesgått IF Metalls krav till den 20 december kommer Fellesforbundet verkställa bojkotten, enligt Eggum.</p><h2>Danskt fack varslar om sympatiåtgärder</h2><p>Även Danmarks största fackförbund 3F Transport, som organiserar hamnarbetare och chaufförer, varslade under tisdagen om sympatiåtgärder. Om 13 dagar kommer de inte ta emot eller transportera Teslas bilar som ska till Sverige.</p><p>”När de ber om vårt stöd backar vi naturligtvis upp. Liksom företagen är fackföreningsrörelsen global i kampen för att skydda arbetarna”, sade Jan Villadsen, ordförande i 3F Transport, i ett pressmeddelande.</p><figure><div><div><p><img alt="" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></div><div><p>Javascript måste vara påslaget för att kunna spela video</p></div></div><figcaption><span>Därför bråkar IF Metall och Tesla om kollektivavtal. <span>Foto: <!-- -->SVT</span></span></figcaption></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compressing Gaussian Splats (127 pts)]]></title>
            <link>https://blog.playcanvas.com//compressing-gaussian-splats/</link>
            <guid>38542875</guid>
            <pubDate>Wed, 06 Dec 2023 12:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.playcanvas.com//compressing-gaussian-splats/">https://blog.playcanvas.com//compressing-gaussian-splats/</a>, See on <a href="https://news.ycombinator.com/item?id=38542875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <h3 id="introduction">Introduction</h3>

<p><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"><strong>3D Gaussian Splatting</strong></a> is a new method for digitizing and rendering real world objects. With gaussian splatting, you can digitize a scene from a few photos using services like <a href="https://lumalabs.ai/">Luma Labs</a> or <a href="https://poly.cam/">Polycam</a>. These services take the set of photos and generate a 3d Gaussian Splat scene in <a href="https://en.wikipedia.org/wiki/PLY_(file_format)">PLY format</a>.</p>

<p>For example, this is a Gaussian Splat scene rendered in PlayCanvas.</p>



<h3 id="what-is-a-splat">What is a Splat?</h3>

<p>Gaussian Splat Scenes are not made up of polygons and textures. Instead, they are made up of many (up to millions) of individual, unconnected blobs called <em>splats</em>. A splat is just a particle in space with size, orientation, color and opacity.</p>

<p>Below you can see a single brown splat selected. The splat bounding box shows its orientation and size:</p>

<p><img src="https://blog.playcanvas.com/assets/media/splat-example.gif" alt="Splat Example"></p>

<p>The gaussian part of the name comes from the shape of splat itself: the splat opacity has a gaussian falloff from its center to its edge.</p>

<h3 id="engine-support">Engine Support</h3>

<p>The PlayCanvas team has been adding support to the engine for loading and rendering Gaussian Splat PLY files:</p>

<p><a href="https://playcanvas.github.io/#/loaders/splat-many"><img src="https://blog.playcanvas.com/assets/media/gaussian-splat-example.gif" alt="Engine Example"></a></p>

<p>Since the resulting files are often messy and require cleaning, we released <a href="https://playcanvas.com/super-splat">SuperSplat</a>, a tool for cleaning and processing gaussian splat PLY files:</p>

<p><a href="https://playcanvas.com/super-splat?load=https://code.playcanvas.com/viewer/guitar-cleaned.ply"><img src="https://blog.playcanvas.com/assets/media/super-splat-example.gif" alt="SuperSplat Example"></a></p>

<h3 id="ply-format">PLY Format</h3>

<p>However, the default gaussian splat PLY format as exported by training tools is large.</p>

<p>This is because the uncompressed format stores a large amount of data <em>per splat</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position</td>
      <td>3 x float</td>
      <td>12</td>
    </tr>
    <tr>
      <td>Orientation</td>
      <td>4 x float</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Scale</td>
      <td>3 x float</td>
      <td>12</td>
    </tr>
    <tr>
      <td>Spherical harmonics / color</td>
      <td>48 x float</td>
      <td>192</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>232</td>
    </tr>
  </tbody>
</table>

<p>For example, the original <code>guitar.ply</code> scene file takes <strong>132.8 MB</strong> (<strong>32 MB</strong> excluding spherical harmonic data).</p>

<h3 id="compressed-ply-format">Compressed PLY Format</h3>

<p>So we introduced a <em>compressed PLY</em> format for use in runtime applications. The compressed PLY file format ignores the unused spherical harmonic data and stores the rest of the elements in quantized integers.</p>

<p>The format can be summarized as follows:</p>
<ul>
  <li>Split the scene into chunks of 256 splats</li>
  <li>For each chunk, store the min and max (x, y, z) for position and scale in floating point</li>
  <li>For each splat in the chunk, store a normalized and quantized value for position and scale (relative to chunk extents) and orientation and color</li>
</ul>

<p>This data layout results in the following data <em>per chunk</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position bound</td>
      <td>6 x float</td>
      <td>24</td>
    </tr>
    <tr>
      <td>Scale bound</td>
      <td>6 x float</td>
      <td>24</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>48</td>
    </tr>
  </tbody>
</table>

<p>And the following data <em>per splat</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position</td>
      <td>uint32 (11, 10, 11)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Orientation</td>
      <td>uint32 (2, 10, 10, 10)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Scale</td>
      <td>uint32 (11, 10, 11)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Color</td>
      <td>uint32 (8, 8, 8, 8)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<p>As a result, the compressed version of <code>guitar.ply</code> takes only <strong>8.7 MB</strong>.</p>

<h3 id="do-it-yourself">Do It Yourself</h3>

<p>The easiest way to generate a compressed PLY file yourself is using the <a href="https://playcanvas.com/super-splat">SuperSplat tool</a>. Load the PLY file into SuperSplat and export it again using the ‘Compressed Ply File’ option:</p>

<p><a href="https://playcanvas.com/super-splat"><img src="https://blog.playcanvas.com/assets/media/super-splat-export.png" alt="SuperSplat Export"></a></p>

<p>If you are interested in the file format specifics, see <a href="https://github.com/playcanvas/engine/blob/a86bd8be0cfd4e39e9ba5e5466acb6875ab9906e/extras/splat/splat-data.js#L257">this code</a> which demonstrates how to decompress the file data.</p>

<p>See <a href="https://playcanvas.com/project/1165904/overview/gaussiansplatdemo">this editor project</a> for an example of loading and rendering a compressed gaussian splat PLY file. Or you can <a href="https://playcanv.as/p/69cnpevQ/">run it here</a>.</p>

<h3 id="summary-and-future">Summary and Future</h3>

<p>We have introduced a new compressed PLY format for gaussian splatting which is roughly 4x smaller than uncompressed data and can be used in realtime applications.</p>

<p>In future we hope to:</p>

<ul>
  <li>store splats hierarchically for optimized rendering and culling</li>
  <li>implement realtime splat LOD</li>
  <li>test skinning and animation of gaussian splats</li>
  <li>further compress gaussian splat data</li>
  <li>optimize WebGPU rendering</li>
</ul>

<h3 id="references">References</h3>

<p>The compressed format is largely based on the fine work of Aras Pranckevičius and his <a href="https://aras-p.info/">blog posts</a>.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking Serverless with Flame (339 pts)]]></title>
            <link>https://fly.io/blog/rethinking-serverless-with-flame/</link>
            <guid>38542764</guid>
            <pubDate>Wed, 06 Dec 2023 12:03:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/rethinking-serverless-with-flame/">https://fly.io/blog/rethinking-serverless-with-flame/</a>, See on <a href="https://news.ycombinator.com/item?id=38542764">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Chris McCord" src="https://fly.io/static/images/chris-m.webp">
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Chris McCord
                 </dd>
                   <dt>Twitter</dt>
                   <dd>
                     <a href="https://twitter.com/chris_mccord" target="_blank">
                       @chris_mccord
                     </a>
                   </dd>
               </dl>
             </dd>
         </dl>

        <section>
            <figure>
                <img src="https://fly.io/blog/rethinking-serverless-with-flame/assets/flame-cover.webp" alt="FLAME logo">
            </figure>
          <blockquote>Imagine if you could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of your app.</blockquote>


<p>The pursuit of elastic, auto-scaling applications has taken us to silly places.</p>

<p>Serverless/FaaS had a couple things going for it. Elastic Scale™ is hard. It’s even harder when you need to manage those pesky servers. It also promised pay-what-you-use costs to avoid idle usage. Good stuff, right?</p>

<p>Well the charade is over. You offload scaling concerns and the complexities of scaling, just to end up needing <em>more complexity</em>. Additional queues, storage, and glue code to communicate back to our app is just the starting point. Dev, test, and CI complexity balloons as fast as your costs. Oh, and you often have to rewrite your app in proprietary JavaScript – even if it’s already written in JavaScript!</p>

<p>At the same time, the rest of us have elastically scaled by starting more webservers. Or we’ve dumped on complexity with microservices. This doesn’t make sense. Piling on more webservers to transcode more videos or serve up more ML tasks isn’t what we want. And granular scale shouldn’t require slicing our apps into bespoke operational units with their own APIs and deployments to manage.</p>

<p>Enough is enough. There’s a better way to elastically scale applications.</p>
<h2 id="the-flame-pattern"><a href="#the-flame-pattern" aria-label="Anchor"></a>The FLAME pattern</h2>
<p>Here’s what we really want:</p>

<ul>
<li>We don’t want to manage those pesky servers. We already have this for our app deployments via <code>fly deploy</code>, <code>git push heroku</code>, <code>kubectl</code>, etc
</li><li>We want on-demand, <em>granular</em> elastic scale of specific parts of our app code
</li><li>We don’t want to rewrite our application or write parts of it in proprietary runtimes
</li></ul>

<p>Imagine if we could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of the app.</p>

<p>Enter the FLAME pattern.</p>
<blockquote>FLAME - Fleeting Lambda Application for Modular Execution</blockquote>


<p>With FLAME, you treat your <em>entire application</em> as a lambda, where modular parts can be executed on short-lived infrastructure.</p>

<p>No rewrites. No bespoke runtimes. No outrageous layers of complexity. Need to insert the results of an expensive operation to the database? PubSub broadcast the result of some expensive work? No problem! It’s your whole app so of course you can do it.</p>

<p>The Elixir <a href="https://github.com/phoenixframework/flame">flame library</a> implements the FLAME pattern. It has a backend adapter for Fly.io, but you can use it on any cloud that gives you an API to spin up an instance with your app code running on it. We’ll talk more about backends in a bit, as well as implementing FLAME in other languages.</p>

<p>First, lets watch a realtime thumbnail generation example to see FLAME + Elixir in action:</p>
<div><p>
          <iframe width="100%" height="100%" src="https://www.youtube.com/embed/l1xt_rkWdic" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
          </iframe>
        </p>
</div>


<p>Now let’s walk thru something a little more basic. Imagine we have a function to transcode video to thumbnails in our Elixir application after they are uploaded:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
  <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
  <span>args</span> <span>=</span> <span>~w(-i #{vid.url} -vf fps=1/#{interval} #{tmp}/%02d.png)</span><span>)</span>
  <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
  <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
  <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
<span>end</span>
</code></pre>
</div>

<p>Our <code>generate_thumbnails</code> function accepts a video struct. We shell out to <code>ffmpeg</code> to take the video URL and generate thumbnails at a given interval. We then write the temporary thumbnail paths to durable storage. Finally, we insert the generated thumbnail URLs into the database.</p>

<p>This works great locally, but CPU bound work like video transcoding can quickly bring our entire service to a halt in production. Instead of rewriting large swaths of our app to move this into microservices or some FaaS, we can simply wrap it in a FLAME call:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>FLAME</span><span>.</span><span>call</span><span>(</span><span>MyApp</span><span>.</span><span>FFMpegRunner</span><span>,</span> <span>fn</span> <span>-&gt;</span>
    <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
    <span>args</span> <span>=</span> <span>~w(-i #{vid.url} -vf fps=1/#{interval} #{tmp}/%02d.png)</span><span>)</span>
    <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
    <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
    <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
  <span>end</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>That’s it! <code>FLAME.call</code> accepts the name of a runner pool, and a function. It then finds or boots a new copy of our entire application and runs the function there. Any variables the function closes over (like our <code>%Video{}</code> struct and <code>interval</code>) are passed along automatically.</p>

<p>When the FLAME runner boots up, it connects back to the parent node, receives the function to run, executes it, and returns the result to the caller. Based on configuration, the booted runner either waits happily for more work before idling down, or extinguishes itself immediately.</p>

<p>Let’s visualize the flow:</p>

<p><img alt="visualizing the flow" src="https://fly.io/blog/rethinking-serverless-with-flame/assets/visual.webp?centered"></p>

<p>We changed no other code and issued our DB write with <code>Repo.insert_all</code> just like before, because we are running our <em>entire</em> <em>application</em>. Database connection(s) and all. Except this fleeting application only runs that little function after startup and nothing else.</p>

<p>In practice, a FLAME implementation will support a pool of runners for hot startup, scale-to-zero, and elastic growth. More on that later.</p>
<h2 id="solving-a-problem-vs-removing-the-problem"><a href="#solving-a-problem-vs-removing-the-problem" aria-label="Anchor"></a>Solving a problem vs removing the problem</h2><blockquote>FaaS solutions help you solve a problem. FLAME removes the problem.</blockquote>


<p>The FaaS labyrinth of complexity defies reason. And it’s unavoidable. Let’s walkthrough the thumbnail use-case to see how.</p>

<p>We try to start with the simplest building block like request/response AWS Lambda Function URL’s.</p>

<p>The complexity hits immediately.</p>

<p>We start writing custom encoders/decoders on both sides to handle streaming the thumbnails back to the app over HTTP. Phew that’s done. Wait, is our video transcoding or user uploads going to take longer than 15 minutes? Sorry, hard timeout limit&nbsp;–&nbsp;time to split our videos into chunks to stay within the timeout, which means more lambdas to do that. Now we’re orchestrating lambda workflows and relying on additional services, such as SQS and S3, to enable this.</p>

<p>All the FaaS is doing is adding layers of communication between your code and the parts you want to run elastically. Each layer has its own glue integration price to pay.</p>

<p>Ultimately handling this kind of use-case looks something like this:</p>

<ul>
<li>Trigger the lambda via HTTP endpoint, S3, or API gateway ($)
</li><li>Write the bespoke lambda to transcode the video ($)
</li><li>Place the thumbnail results into SQS ($)
</li><li>Write the SQS consumer in our app (dev $)
</li><li>Persist to DB and figure out how to get events back to active subscribers that may well be connected to other instances than the SQS consumer (dev $)
</li></ul>

<p>This is nuts. We pay the FaaS toll at every step. We shouldn’t have to do any of this!</p>

<p>FaaS provides a bunch of offerings to build a solution on top of. FLAME removes the problem entirely.</p>
<h2 id="flame-backends"><a href="#flame-backends" aria-label="Anchor"></a>FLAME Backends</h2><blockquote>On Fly.io infrastructure the <code>FLAME.FlyBackend</code> can boot a copy of your application on a new <a href="https://fly.io/docs/machines/">Machine</a> and have it connect back to the parent for work within ~3s.</blockquote>


<p>By default, FLAME ships with a <code>LocalBackend</code> and <code>FlyBackend</code>, but any host that provides an API to provision a server and run your app code can work as a FLAME backend. Erlang and Elixir primitives are doing all the heavy lifting here. The entire <code>FLAME.FlyBackend</code> is <a href="https://github.com/phoenixframework/flame/blob/main/lib/flame/fly_backend.ex">&lt; 200 LOC with docs</a>. The library has a single dependency, <code>req</code>, which is an HTTP client.</p>

<p>Because Fly.io runs our applications as a packaged up docker image, we simply ask the Fly API to boot a new Machine for us with the same image that our app is currently running. Also thanks to Fly infrastructure, we can guarantee the FLAME runners are started in the same region as the parent. This optimizes latency and lets you ship whatever data back and forth between parent and runner without having to think about it.</p>
<h2 id="look-at-everything-were-not-doing"><a href="#look-at-everything-were-not-doing" aria-label="Anchor"></a>Look at everything we’re not doing</h2>
<p>With FaaS, just imagine how quickly the dev and testing story becomes a fate worse than death.</p>

<p>To run the app locally, we either need to add some huge dev dependencies to simulate the entire FaaS pipeline, or worse, connect up our dev and test environments directly to the FaaS provider.</p>

<p>With FLAME, your dev and test runners simply run on the local backend.</p>

<p>Remember, this is your app. FLAME just controls where modular parts of it run. In dev or test, those parts simply run on the existing runtime on&nbsp;your laptop or CI server.</p>

<p>Using Elixir, we can even send a file across to the remote FLAME application thanks to the distributed features of the Erlang VM:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>parent_stream</span> <span>=</span> <span>File</span><span>.</span><span>stream!</span><span>(</span><span>vid</span><span>.</span><span>filepath</span><span>,</span> <span>[],</span> <span>2048</span><span>)</span>
  <span>FLAME</span><span>.</span><span>call</span><span>(</span><span>MyApp</span><span>.</span><span>FFMpegRunner</span><span>,</span> <span>fn</span> <span>-&gt;</span>
    <span>tmp_file</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>flame_stream</span> <span>=</span> <span>File</span><span>.</span><span>stream!</span><span>(</span><span>tmp_file</span><span>)</span>
    <span>Enum</span><span>.</span><span>into</span><span>(</span><span>parent_stream</span><span>,</span> <span>flame_stream</span><span>)</span>

    <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
    <span>args</span> <span>=</span> <span>~w(-i #{tmp_file} -vf fps=1/#{interval} #{tmp}/%02d.png)</span>
    <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
    <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
    <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
  <span>end</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>On line 2 we open a file on the parent node to the video path. Then in the FLAME child, we stream the file from the parent node to the FLAME server in only a couple lines of code. That’s it! No setup of S3 or HTTP interfaces required.</p>

<p>With FLAME it’s easy to miss everything we’re not doing:</p>

<ul>
<li>We don’t need to write code outside of our application. We can reuse business logic, database setup, PubSub, and all the features of our respective platforms
</li><li>We don’t need to manage deploys of separate services or endpoints
</li><li>We don’t need to write results to S3 or SQS just to pick up values back in our app
</li><li>We skip the dev, test, and CI dependency dance
</li></ul>
<h2 id="flame-outside-elixir"><a href="#flame-outside-elixir" aria-label="Anchor"></a>FLAME outside Elixir</h2>
<p>Elixir is fantastically well suited for the FLAME model because we get so much <a href="https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/">for free</a> like process supervision and distributed messaging. That said, any language with reasonable concurrency primitives can take advantage of this pattern. For example, my teammate, Lubien, created a proof of concept example for breaking out functions in your JavaScript application and running them inside a new Fly Machine: <a href="https://github.com/lubien/fly-run-this-function-on-another-machine">https://github.com/lubien/fly-run-this-function-on-another-machine</a></p>

<p>So the general flow for a JavaScript-based FLAME call would be to move the modular executions to a new file, which is executed on a runner pool. Provided the arguments are JSON serializable, the general FLAME flow is similar to what we’ve outlined here. Your application, your code, running on fleeting instances.</p>

<p>A complete FLAME library will need to handle the following concerns:</p>

<ul>
<li>Elastic pool scale-up and scale-down logic
</li><li>Hot vs cold startup with pools
</li><li>Remote runner monitoring to avoid orphaned resources
</li><li>How to monitor and keep deployments fresh
</li></ul>

<p>For the rest of this post we’ll see how the Elixir FLAME library handles these concerns as well as features uniquely suited to Elixir applications. But first, you might be wondering about your background job queues.</p>
<h2 id="what-about-my-background-job-processor"><a href="#what-about-my-background-job-processor" aria-label="Anchor"></a>What about my background job processor?</h2>
<p>FLAME works great inside your background job processor, but you may have noticed some overlap. If your job library handles scaling the worker pool, what is FLAME doing for you? There’s a couple important distinctions here.</p>

<p>First, we reach for these queues when we need <em>durability guarantees</em>. We often can turn knobs to have the queues scale to handle more jobs as load changes. But durable operations are separate from elastic execution. Conflating these concerns can send you down a similar path to lambda complexity. Leaning on your worker queue purely for offloaded execution means writing all the glue code to get the data into and out of the job, and back to the caller or end-user’s device somehow.</p>

<p>For example, if we want to guarantee we successfully generated thumbnails for a video after the user upload, then a job queue makes sense as the <em>dispatch, commit, and retry</em> <em>mechanism</em> for this operation. The actual transcoding could be a FLAME call inside the job itself, so we decouple the ideas of durability and scaled execution.</p>

<p>On the other side, we have operations we don’t need durability for. Take the screencast above where the user hasn’t yet saved their video. Or an ML model execution where there’s no need to waste resources churning a prompt if the user has already left the app. In those cases, it doesn’t make sense to write to a durable store to pick up a job for work that will go right into the ether.</p>
<h2 id="pooling-for-elastic-scale"><a href="#pooling-for-elastic-scale" aria-label="Anchor"></a>Pooling for Elastic Scale</h2>
<p>With the Elixir implementation of FLAME, you define elastic pools of runners. This allows scale-to-zero behavior while also elastically scaling up FLAME servers with max concurrency limits.</p>

<p>For example, lets take a look at the <code>start/2</code> callback, which is the entry point of all Elixir applications. We can drop in a <code>FLAME.Pool</code> for video transcriptions and say we want it to scale to zero, boot a max of 10, and support 5 concurrent <code>ffmpeg</code> operations per runner:</p>
<div>
  <pre><code><span>def</span> <span>start</span><span>(</span><span>_type</span><span>,</span> <span>_args</span><span>)</span> <span>do</span>
  <span>flame_parent</span> <span>=</span> <span>FLAME</span><span>.</span><span>Parent</span><span>.</span><span>get</span><span>()</span>

  <span>children</span> <span>=</span> <span>[</span>
    <span>...</span><span>,</span>
    <span>MyApp</span><span>.</span><span>Repo</span><span>,</span>
    <span>{</span><span>FLAME</span><span>.</span><span>Pool</span><span>,</span>
      <span>name:</span> <span>Thumbs</span><span>.</span><span>FFMpegRunner</span><span>,</span>
      <span>min:</span> <span>0</span><span>,</span>
      <span>max:</span> <span>10</span><span>,</span>
      <span>max_concurrency:</span> <span>5</span><span>,</span>
      <span>idle_shutdown_after:</span> <span>30_000</span><span>},</span>
    <span>!flame_parent</span> <span>&amp;&amp;</span> <span>MyAppWeb</span><span>.</span><span>Endpoint</span>
  <span>]</span>
  <span>|&gt;</span> <span>Enum</span><span>.</span><span>filter</span><span>(</span><span>&amp;</span> <span>&amp;1</span><span>)</span>

  <span>opts</span> <span>=</span> <span>[</span><span>strategy:</span> <span>:one_for_one</span><span>,</span> <span>name:</span> <span>MyApp</span><span>.</span><span>Supervisor</span><span>]</span>
  <span>Supervisor</span><span>.</span><span>start_link</span><span>(</span><span>children</span><span>,</span> <span>opts</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>We use the presence of a FLAME parent to conditionally start our Phoenix webserver when booting the app. There’s no reason to start a webserver if we aren’t serving web traffic. Note we leave other services like the database <code>MyApp.Repo</code> alone because we want to make use of those services inside FLAME runners.</p>

<p>Elixir’s supervised process approach to applications is uniquely great for turning these kinds of knobs.</p>

<p>We also set our pool to idle down after 30 seconds of no caller operations. This keeps our runners hot for a short while before discarding them. We could also pass a <code>min: 1</code> to always ensure at least one <code>ffmpeg</code> runner is hot and ready for work by the time our application is started.</p>
<h2 id="process-placement"><a href="#process-placement" aria-label="Anchor"></a>Process Placement</h2>
<p>In Elixir, stateful bits of our applications are built around the <em>process</em> primitive –&nbsp;lightweight greenthreads with message mailboxes. Wrapping our otherwise stateless app code in a synchronous <code>FLAME.call</code>‘s or async <code>FLAME.cast</code>’s works great, but what about the stateful parts of our app?</p>

<p><code>FLAME.place_child</code> exists to take an existing process specification in your Elixir app and start it on a FLAME runner instead of locally. You can use it anywhere you’d use <code>Task.Supervisor.start_child</code> , <code>DynamicSupervisor.start_child</code>, or similar interfaces. Just like <code>FLAME.call</code>, the process is run on an elastic pool and runners handle idle down when the process completes its work.</p>

<p>And like <code>FLAME.call</code>, it lets us take existing app code, change a single LOC, and continue shipping features.</p>

<p>Let’s walk thru the example from the screencast above. Imagine we want to generate video thumbnails for a video <em>as it is being uploaded</em>. Elixir and LiveView make this easy. We won’t cover all the code here, but you can view the <a href="https://github.com/fly-apps/thumbnail_generator/blob/main/lib/thumbs/thumbnail_generator.ex">full app implementation</a>.</p>

<p>Our first pass would be to write a LiveView upload writer that calls into a <code>ThumbnailGenerator</code>:</p>
<div>
  <pre><code><span>defmodule</span> <span>ThumbsWeb</span><span>.</span><span>ThumbnailUploadWriter</span> <span>do</span>
  <span>@behaviour</span> <span>Phoenix</span><span>.</span><span>LiveView</span><span>.</span><span>UploadWriter</span>

  <span>alias</span> <span>Thumbs</span><span>.</span><span>ThumbnailGenerator</span>

  <span>def</span> <span>init</span><span>(</span><span>opts</span><span>)</span> <span>do</span>
    <span>generator</span> <span>=</span> <span>ThumbnailGenerator</span><span>.</span><span>open</span><span>(</span><span>opts</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>%{</span><span>gen:</span> <span>generator</span><span>}}</span>
  <span>end</span>

  <span>def</span> <span>write_chunk</span><span>(</span><span>data</span><span>,</span> <span>state</span><span>)</span> <span>do</span>
    <span>ThumbnailGenerator</span><span>.</span><span>stream_chunk!</span><span>(</span><span>state</span><span>.</span><span>gen</span><span>,</span> <span>data</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>state</span><span>}</span>
  <span>end</span>

  <span>def</span> <span>meta</span><span>(</span><span>state</span><span>),</span> <span>do</span><span>:</span> <span>%{</span><span>gen:</span> <span>state</span><span>.</span><span>gen</span><span>}</span>

  <span>def</span> <span>close</span><span>(</span><span>state</span><span>,</span> <span>_reason</span><span>)</span> <span>do</span>
    <span>ThumbnailGenerator</span><span>.</span><span>close</span><span>(</span><span>state</span><span>.</span><span>gen</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>state</span><span>}</span>
  <span>end</span>
<span>end</span>
</code></pre>
</div>

<p>An upload writer is a behavior that simply ferries the uploaded chunks from the client into whatever we’d like to do with them. Here we have a <code>ThumbnailGenerator.open/1</code> which starts a process that communicates with an <code>ffmpeg</code> shell. Inside <code>ThumbnailGenerator.open/1</code>, we use regular elixir process primitives:</p>
<div>
  <pre><code>  <span># thumbnail_generator.ex</span>
  <span>def</span> <span>open</span><span>(</span><span>opts</span> <span>\\</span> <span>[])</span> <span>do</span>
    <span>Keyword</span><span>.</span><span>validate!</span><span>(</span><span>opts</span><span>,</span> <span>[</span><span>:timeout</span><span>,</span> <span>:caller</span><span>,</span> <span>:fps</span><span>])</span>
    <span>timeout</span> <span>=</span> <span>Keyword</span><span>.</span><span>get</span><span>(</span><span>opts</span><span>,</span> <span>:timeout</span><span>,</span> <span>5_000</span><span>)</span>
    <span>caller</span> <span>=</span> <span>Keyword</span><span>.</span><span>get</span><span>(</span><span>opts</span><span>,</span> <span>:caller</span><span>,</span> <span>self</span><span>())</span>
    <span>ref</span> <span>=</span> <span>make_ref</span><span>()</span>
    <span>parent</span> <span>=</span> <span>self</span><span>()</span>

    <span>spec</span> <span>=</span> <span>{</span><span>__MODULE__</span><span>,</span> <span>{</span><span>caller</span><span>,</span> <span>ref</span><span>,</span> <span>parent</span><span>,</span> <span>opts</span><span>}}</span>
    <span>{</span><span>:ok</span><span>,</span> <span>pid</span><span>}</span> <span>=</span> <span>DynamicSupervisor</span><span>.</span><span>start_child</span><span>(</span><span>@sup</span><span>,</span> <span>spec</span><span>)</span>

    <span>receive</span> <span>do</span>
      <span>{</span><span>^</span><span>ref</span><span>,</span> <span>%</span><span>ThumbnailGenerator</span><span>{}</span> <span>=</span> <span>gen</span><span>}</span> <span>-&gt;</span>
        <span>%</span><span>ThumbnailGenerator</span><span>{</span><span>gen</span> <span>|</span> <span>pid:</span> <span>pid</span><span>}</span>
    <span>after</span>
      <span>timeout</span> <span>-&gt;</span> <span>exit</span><span>(</span><span>:timeout</span><span>)</span>
    <span>end</span>
  <span>end</span>
</code></pre>
</div>

<p>The details aren’t super important here, except line 10 where we call <code>{:ok, pid} = DynamicSupervisor.start_child(@sup, spec)</code>, which starts a supervised<code>ThumbnailGenerator</code> process. The rest of the implementation simply ferries chunks as stdin into <code>ffmpeg</code> and parses png’s from stdout. Once a PNG delimiter is found in stdout, we send the <code>caller</code> process (our LiveView process) a message saying “hey, here’s an image”:</p>
<div>
  <pre><code><span># thumbnail_generator.ex</span>
<span>@png_begin</span> <span>&lt;&lt;</span><span>137</span><span>,</span> <span>80</span><span>,</span> <span>78</span><span>,</span> <span>71</span><span>,</span> <span>13</span><span>,</span> <span>10</span><span>,</span> <span>26</span><span>,</span> <span>10</span><span>&gt;&gt;</span>
<span>defp</span> <span>handle_stdout</span><span>(</span><span>state</span><span>,</span> <span>ref</span><span>,</span> <span>bin</span><span>)</span> <span>do</span>
  <span>%</span><span>ThumbnailGenerator</span><span>{</span><span>ref:</span> <span>^</span><span>ref</span><span>,</span> <span>caller:</span> <span>caller</span><span>}</span> <span>=</span> <span>state</span><span>.</span><span>gen</span>

  <span>case</span> <span>bin</span> <span>do</span>
    <span>&lt;&lt;</span><span>@png_begin</span><span>,</span> <span>_rest</span><span>::</span><span>binary</span><span>&gt;&gt;</span> <span>-&gt;</span>
      <span>if</span> <span>state</span><span>.</span><span>current</span> <span>do</span>
        <span>send</span><span>(</span><span>caller</span><span>,</span> <span>{</span><span>ref</span><span>,</span> <span>:image</span><span>,</span> <span>state</span><span>.</span><span>count</span><span>,</span> <span>encode</span><span>(</span><span>state</span><span>)})</span>
      <span>end</span>

      <span>%{</span><span>state</span> <span>|</span> <span>count:</span> <span>state</span><span>.</span><span>count</span> <span>+</span> <span>1</span><span>,</span> <span>current:</span> <span>[</span><span>bin</span><span>]}</span>

    <span>_</span> <span>-&gt;</span>
      <span>%{</span><span>state</span> <span>|</span> <span>current:</span> <span>[</span><span>bin</span> <span>|</span> <span>state</span><span>.</span><span>current</span><span>]}</span>
  <span>end</span>
<span>end</span>
</code></pre>
</div>

<p>The <code>caller</code> LiveView process then picks up the message in a <code>handle_info</code> callback and updates the UI:</p>
<div>
  <pre><code><span># thumb_live.ex</span>
<span>def</span> <span>handle_info</span><span>({</span><span>_ref</span><span>,</span> <span>:image</span><span>,</span> <span>_count</span><span>,</span> <span>encoded</span><span>},</span> <span>socket</span><span>)</span> <span>do</span>
  <span>%{</span><span>count:</span> <span>count</span><span>}</span> <span>=</span> <span>socket</span><span>.</span><span>assigns</span>

  <span>{</span><span>:noreply</span><span>,</span>
   <span>socket</span>
   <span>|&gt;</span> <span>assign</span><span>(</span><span>count:</span> <span>count</span> <span>+</span> <span>1</span><span>,</span> <span>message:</span> <span>"Generating (</span><span>#{</span><span>count</span> <span>+</span> <span>1</span><span>}</span><span>)"</span><span>)</span>
   <span>|&gt;</span> <span>stream_insert</span><span>(</span><span>:thumbs</span><span>,</span> <span>%{</span><span>id:</span> <span>count</span><span>,</span> <span>encoded:</span> <span>encoded</span><span>})}</span>
<span>end</span>
</code></pre>
</div>

<p>The <code>send(caller, {ref, :image, state.count, encode(state)}</code> is one magic part about Elixir. Everything is a process, and we can message those processes, regardless of their location in the cluster.</p>

<p>It’s like if every instantiation of an object in your favorite OO lang included a cluster-global unique identifier to work with methods on that object. The LiveView (a process) simply receives the image message and updates the UI with new images.</p>

<p>Now let’s head back over to our <code>ThumbnailGenerator.open/1</code> function and make this elastically scalable.</p>
<div>
  <pre><code><span>-    {:ok, pid} = DynamicSupervisor.start_child(@sup, spec)
</span><span>+    {:ok, pid} = FLAME.place_child(Thumbs.FFMpegRunner, spec)
</span></code></pre>
</div>

<p>That’s it! Because everything is a process and processes can live anywhere, it doesn’t matter what server our <code>ThumbnailGenerator</code> process lives on. It simply messages the caller with <code>send(caller, …)</code> and the messages are sent across the cluster if needed.</p>

<p>Once the process exits, either from an explicit close, after the upload is done, or from the end-user closing their browser tab, the FLAME server will note the exit and idle down if no other work is being done.</p>

<p>Check out the <a href="https://github.com/fly-apps/thumbnail_generator/blob/main/lib/thumbs/thumbnail_generator.ex">full implementation</a> if you’re interested.</p>
<h2 id="remote-monitoring"><a href="#remote-monitoring" aria-label="Anchor"></a>Remote Monitoring</h2>
<p>All this transient infrastructure needs failsafe mechanisms to avoid orphaning resources. If a parent spins up a runner, that runner must take care of idling itself down when no work is present and handle failsafe shutdowns if it can no longer contact the parent node.</p>

<p>Likewise, we need to shutdown runners when parents are rolled for new deploys as we must guarantee we’re running the same code across the cluster.</p>

<p>We also have active callers in many cases that are awaiting the result of work on runners that could go down for any reason.</p>

<p>There’s a lot to monitor here.</p>

<p>There’s also a number of failure modes that make this sound like a harrowing experience to implement. Fortunately Elixir has all the primitives to make this an easy task thanks to the Erlang VM. Namely, we get the following for free:</p>

<ul>
<li>Process monitoring and supervision –&nbsp;we know when things go bad. Whether on a node-local process, or one across the cluster
</li><li>Node monitoring – we know when nodes come up, and when nodes go away
</li><li>Declarative and controlled app startup and shutdown - we carefully control the startup and shutdown sequence of applications as a matter of course. This allows us to gracefully shutdown active runners when a fresh deploy is triggered, while giving them time to finish their work
</li></ul>

<p>We’ll cover the internal implementation details in a future deep-dive post. For now, feel free to poke around <a href="https://github.com/phoenixframework/flame">the flame source</a>.</p>
<h2 id="whats-next"><a href="#whats-next" aria-label="Anchor"></a>What’s Next</h2>
<p>We’re just getting started with the Elixir FLAME library, but it’s ready to try out now. In the future  look for more advance pool growth techniques, and deep dives into how the Elixir implementation works. You can also find me <a href="https://twitter.com/chris_mccord">@chris_mccord</a> to chat about implementing the FLAME pattern in your language of choice.</p>

<p>Happy coding!</p>

<p>–Chris</p>

          
        </section>
        <dl>
            <dt>
              Next post  ↑
            </dt>
            <dd>
              <a href="https://fly.io/blog/scaling-llm-ollama/">
                Scaling Large Language Models to zero with Ollama
              </a>
            </dd>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://fly.io/blog/the-risks-of-building-apps-on-chatgpt/">
                The risks of building apps on ChatGPT
              </a>
            </dd>
        </dl>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Open Letter to the Python Software Foundation (296 pts)]]></title>
            <link>https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html</link>
            <guid>38542330</guid>
            <pubDate>Wed, 06 Dec 2023 10:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html">https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html</a>, See on <a href="https://news.ycombinator.com/item?id=38542330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-9208814100498476329">
<p>Dear PSF,</p><p>We, organisers in the pan-African Python community, would like to raise some concerns and frustrations that have been brought to a head by recent events.</p><p>We held the first-ever DjangoCon Africa in November, a flagship event for our community, and the first Africa-wide Python event since 2019’s PyCon Africa.</p><p>The PSF Board approved a grant of USD 9000 for the conference (six days, 200 people). The event was a significant success, and the PSF’s support was key to that.</p><h2>Delays in the grant award process&nbsp;</h2><p>As acknowledged in the PSF’s article, “<a href="https://pyfound.blogspot.com/2023/10/september-october-board-votes.html" target="_blank">September &amp; October Board Votes</a>”, there were however some problems leading up to the grant decision.</p><p>First, the Grants Working Group was unable to come to a consensus on the request, so the decision was passed to the PSF Board. The Board was also unable to come to a consensus in its September meeting. Finally in the Board’s October meeting, the grant was approved. That was just a few weeks before the event started, nearly three months after its submission.</p><h2>Effect of the delays</h2><p>We would like to share a statement from the organisers of DjangoCon Africa describing the effect of these delays on the event and on them personally.&nbsp;</p><blockquote><p>For a considerable period of time, we were in doubt about a significant portion of our expected conference budget - from the PSF, whom we expected to be a steadfast ally and backer.&nbsp;</p><p>During that time, we felt quite stuck - unable to make decisions. Our daily conversations became centred around anxious what-if calculations. At one point we had reduced our budget for catering to a total of 5 USD per person per day, for breakfast, lunch and refreshments - even in Zanzibar, this is an unfeasibly low figure. We jettisoned one item after another from our budget.</p><p>We were unable to make decisions about financial assistance, which in turn delayed our ability to make decisions about the programme (how could we invite a speaker whom we knew would require some funds to travel if we couldn’t provide the funds?). We watched as the air-fares we had expected to cover for many of those people rose.</p><p>We had to answer all the people who couldn’t understand why we hadn’t decided on their talk proposals and grant applications, or who wondered why an event starting so soon had not yet even published a programme of talks. “What are we supposed to say to these people?” became another anxious topic in our meetings.</p><p>We felt unable to advertise or promote the event, because we simply didn’t know what we could promise people.</p><p>Some of the people affected had applied for visas - incurring expense - well in advance, on our advice; they too were waiting to hear back from us.</p><p>Locally, we had caterers and other businesses waiting for deposits and confirmation of contracts. Some lost patience with us. The local PyCon Tanzania organisation bore the brunt of this.&nbsp;</p><p>The organisation of DjangoCon Africa must have looked lazy, or incompetent, or worse, to someone looking at it from the outside.</p><p>The delay in a decision on funding from the PSF also made it harder for us to approach other organisations for funds - “Is the PSF sponsoring DjangoCon Africa? Why not?”.</p><p>It’s hard to describe the embarrassment we felt sometimes.</p><p>We had sleepless nights with worry - literally, not figuratively. More than one of us confided in another that we wished we had never started the project.</p><p>We started a fundraising campaign on GoFundMe to help cover the cost of financial assistance. It was a comfort to know that members of the international Python/Django community would stand up to support us, but the pleasure and gratitude we felt about that was overlaid with a feeling of humiliation that once again, a major African open-source software event had been obliged to publicly extend a begging-bowl.</p><p>At one point, gaps in our funding meant that the organisers faced a personal liability of almost USD 10,000 - funds actually spent, or committed, to make the event possible. &nbsp;</p><p>For any volunteer conference organiser, the weeks in the run-up to an event are full of hard work. This experience went far beyond that. Much of the pride and joy of staging DjangoCon Africa was sucked out of it for us.&nbsp;</p><p>Eventually, we received the grant funding we had applied for, though even this seemed to come with a humiliation: it happened after a white European spoke up publicly on behalf of the African Python community.</p><p>By the time we were able to start taking care of travellers who needed financial assistance, many of the air tickets had gone up significantly in price. Amongst the hard choices we had to make: one of our own organisers - a student, who has worked tirelessly in multiple events - was unable to attend because we could not afford to pay for her travel.</p><p>The organisers personally contributed well over USD 4000 to make the event possible in the form it took - funds contributed to a cause that we believe in, but it is not right that volunteer organisers of community events should be forced to make such choices.&nbsp;</p><p>We are genuinely grateful for the support we received from the PSF. The event was a success, and we are proud of what we achieved, but we remain perplexed and hurt by the problems we faced, and how our grant request was treated.</p></blockquote><h2>Problems within the PSF?</h2><p>It’s not clear to the wider African Python community why events unfolded in this way, though we are aware of some things that we have found very troubling.</p><p>We know that there are some extraordinary attitudes at work within the PSF. A PSF Board member once openly expressed the opinion that Anglo cultures always seem to be the ones that take the moral lead around the world, leaving others to follow their example. From any non-western perspective, this is an astounding idea to receive.</p><p>In the case of DjangoCon Africa, the first public response to our event on Mastodon was a negative response from a PSF Director, that in effect, cast doubt on the whole idea of a DjangoCon in Tanzania.</p><p>That’s not a solitary episode. We understand that (notwithstanding the PSF’s ambition to support Python in Africa) a PSF Director has consistently spoken out against funding for African events, over a period of years.&nbsp;</p><p>Our grant request was handled by the PSF’s Grants Working Group. We understand that one member of this group was able effectively to stall its decision-making long enough that the grant request had to be passed to the PSF Board.</p><p>At the PSF Board meeting in September 2023, a board member strategically used an abstention to ensure that a resolution to support our request could not pass. (Under the PSF rules, had they voted against the resolution, it would have passed 4-1. In the circumstances, other abstentions for different reasons - including one person who was required to abstain, as an organiser of &nbsp;DjangoCon Africa - meant that the resolution could not pass.) We are genuinely shocked by this. It’s one thing for a PSF Board member to vote against something they don’t believe in. It is quite another that someone has been able to weaponise the PSF’s voting system against an African event.</p><p>We are deeply troubled that such behaviours and values are actively at work inside the PSF. As an organisation, the PSF (and its Board and Working Groups and their processes) should be robust enough to stand up to individual prejudices, and not allow decision-making and deliberation to be derailed by individuals, however influential.</p><h2>The PSF and marginalised and at-risk groups</h2><p>We understand that the argument against support for DjangoCon Africa was that the host country, Tanzania, is not a safe place for the LGBTQIA+ community.</p><p>The PSF represents a global community, and has for years upheld high standards of inclusion and protection, paying special attention to the needs of those in marginalised and at-risk groups. Python community events around the world are effective safe spaces, that give strength to people who do not always find guarantees of safety elsewhere.</p><p>It is therefore especially shocking to have observed an attempt, coming from within the PSF, to pit the well-being and interests of two different excluded groups against each other, as if somehow the interests of members of the LGBTQIA+ community and of Africans are mutually exclusive.</p><p>Many questions can be asked about this reasoning.</p><p>What counts as “safety”? Which places in the world are truly safe for LGBTQIA+ community? How much of a city, or state, or country needs to be LGBTQIA+ hostile for the whole of it to be declared unworthy of PSF support? What does the PSF have to say to LGBTQIA+ community members in such locations? Are the LGBTQIA+ communities who are worthy of the PSF’s consideration only those who live in western countries, or do others count too? What does the PSF have to say to Python community organisers around the world who assert the community’s standards of inclusion, even in countries where it takes an act of bravery to do so?</p><p>And we would like to ask: when have questions been raised to check on whether western events present potential safety risks to non-western attendees, and when have non-western people been asked for their experiences?</p><p>All across the world, including the west, there are countries and places that are genuinely unsafe for members of particular groups, on the basis of their religion, ethnicity, language, gender, sexuality, nationality and other characteristics. Some of these risks may be obvious to westerners, or native English speakers, or men, and some of them may not. Simplistic judgements made from narrow perspectives will not enhance the safety of anyone in our community.</p><h2>Risk and the law</h2><p>The PSF and its directors quite correctly also observe the laws that apply to them. Yet we have witnessed discussions in which it has been proposed that volunteer organisers take public stances in their own countries that are not just contentious or socially unacceptable, but would actually violate local laws.&nbsp;</p><p>In one recent example, voices on the PSF Board were demanding that a condition of funding for a particular PyCon be the formal adoption of a “human rights plan” - a measure that would pose a significant legal and personal risk to its organisers.</p><p>The entitlement and assumption of cultural superiority embodied in these ideas are absurd and offensive.&nbsp;</p><h2>Guidance and consideration for non-western Python events grant awards</h2><p>At a meeting earlier this year, the PSF expressed concern that barely 16% of grants go to African communities.&nbsp;</p><p>At the same time, the perception within some African Python communities is that the PSF is less likely to award a grant to an African event, or will scrutinise it more harshly, or take longer to make an award.</p><p>For example, in 2019 and 2020 one Ugandan Python community made two grant requests that we understand received literally no response. In 2022, another grant request finally received attention from the working group when - with the event coming up in a matter of days - one of the PSF Directors connected to the community raised the issue with PSF staff, and a vote was initiated immediately.&nbsp;</p><p>This can be contrasted with the way a grant request for a European event was handled, at around the same time; the European request was made later, and dealt with sooner.</p><p>Inconsistency, lack of transparency and lack of clarity around expectations serve to undermine trust and confidence. The general perception within the Ugandan Python community is now that their events will only be given consideration if a PSF Director happens to take a personal interest in it.</p><p>In fact, other African organisers have reported timely responses and good communication from the PSF, so what is happening here?&nbsp;</p><p>Do some African grant requests lack quality or detail, because organisers failed to understand what was required? Are there enough people in the PSF with an adequate understanding of the challenges faced by non-western events? ​Is there a pattern where weaknesses in a grant request made from some regions in the world are given the benefit of the doubt, while others are treated less favourably?&nbsp;</p><p>We simply don’t know, and there could be a whole range of explanations. Whatever the underlying reasons, we need to understand and work together to address them, because the effects are harmful.&nbsp;</p><h2>Our requests to the Python Software Foundation&nbsp;</h2><h3>Transparency</h3><p>We request that the PSF undertake and publish a review of actual grant applications, to determine whether there indeed are differences in grant responsiveness, approval times, rejection rates and so on in response to requests from different regions.&nbsp;</p><p>We would like the PSF to publish clear expectations of timelines for handling grant requests, and for each final decision to be accompanied by a report showing how the case was actually handled.</p><h3>Guidance and feedback</h3><p>Organisers, and especially those operating without the benefit of long-standing networks of knowledge and shared expectations, need more guidance, and feedback they can act upon, especially in the case when a grant is rejected.&nbsp;</p><p>We ask that the PSF commits to developing - in collaboration with organisers, especially those in non-western regions - further materials and guidance to help organisers put in the best possible requests for funding. This could include a more proactive approach to working with those organisers to help them understand the PSF’s expectations and standards.</p><p>We request that the PSF institute a practice of providing clear feedback to grant applicants, to help improve and motivate subsequent applications. In cases of delay or doubt, we would like a practice of prompt, direct engagement with organisers to help clarify.</p><h3>Understanding of global needs</h3><p>We ask that the PSF as an organisation commits to a better understanding of global diversity and the realities, needs and challenges of non-western events and organisers.</p><p>This includes an understanding of financial realities. For example, the organisers of African events face the combined difficulties of lesser commercial sponsorship prospects, the expense of intra-African travel, vast geographical distances and so on. We need the PSF to understand these realities in its decision-making about financial awards to events.</p><h3>The law and marginalised groups</h3><p>We request that the PSF undertake a formal review of policies and bylaws, that incorporates expert legal advice and takes full account of the realities of laws and legal regimes across the world that apply to volunteer Python community organisers.</p><p>We recognise that all across the world, Python events are proposed in places where laws and practices mean that the rights of some individuals will be in jeopardy. We would like the PSF to recognise, formally and in its actual practices, that this includes the west, and that it is not only non-western events that should be subjected to critical scrutiny over this.</p><p>We also ask the PSF to adopt a constructive stance that requires all local organisers, wherever they may be, to consider the safeguarding of marginalised groups, and actively helps them improve safety, without ever demanding that volunteers be willing to violate local law or place themselves at risk while doing unpaid work on behalf of the PSF.&nbsp;</p><h2>Progress, prejudice and confidence</h2><p>In the past ten years, Python in Africa has developed with remarkable speed and success. In 2014 there was just one African PyCon, in South Africa. Since then PyCons and other events have been held all over the continent, and the communities behind them have grown in size, confidence, expertise and influence.</p><p>We can trace the introduction of Python teaching in universities across Africa and its spread across multiple commercial and non-commercial sectors to our work.</p><p>We have been generously supported, financially and morally, by the Python Software Foundation. Leaders like Ewa Jodlowska and Naomi Ceder have been part of that growth due to intentional support for our communities.</p><p>This has been a story of growth, motivation and courage.</p><p>More recent experiences have left us feeling hurt and angry. We hear voices, openly and confidently raised within the PSF, that denigrate us and our communities, that dismiss our experiences, that doubt our values, and harm us materially.</p><p>Our confidence in the PSF, and the confidence of many other people in our communities, has been shaken. Our motivation has taken some hammer blows. The work of the last decade risks being set back.</p><h2>Constructive collaboration</h2><p>We want to work with the PSF on everything addressed in this letter, in the spirit of constructive collaboration. We are willing to put our energies into building better practices and understanding. We want to be part of a solution to the concerns.</p><p>We ask the PSF to recognise our concerns, and not just to take them seriously, but to commit to working with the African Python community to address them, so that we do it together. &nbsp;</p><p>Sincerely,</p><p><b>Python Communities in Ghana, Namibia, Nigeria, Uganda, Tanzania, Mozambique &nbsp;South Africa, and Zimbabwe.</b></p><p>Abigail Mesrenyame Dogbe (Python Ghana)<br>Aisha Bello (Python Nigeria)<br>Anna Makarudze (Python Zimbabwe)<br>Chukwudi Nwachukwu (Python Nigeria)<br>Daniele Procida (Python Namibia)<br>Eusebio Simango (Python Mozambique)<br>Jessica Upani (Python Namibia)<br>Joannah Nanjekye (Python Uganda)<br>Julius Moshiro (Python Tanzania)<br>Mannie Young (Python Ghana)<br>Marlene Mhangami (Python Zimbabwe)<br>Noah Maina (Python Tanzania)<br>Sheena O’Connell (Python South Africa)</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>