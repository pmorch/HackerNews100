<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 31 May 2025 14:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Beware of Fast-Math (181 pts)]]></title>
            <link>https://simonbyrne.github.io/notes/fastmath/</link>
            <guid>44142472</guid>
            <pubDate>Sat, 31 May 2025 07:05:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonbyrne.github.io/notes/fastmath/">https://simonbyrne.github.io/notes/fastmath/</a>, See on <a href="https://news.ycombinator.com/item?id=44142472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>One of my more frequent rants, both online and in person, is the danger posed by the "fast-math" compiler flag. While these rants may elicit resigned acknowledgment from those who already understand the dangers involved, they do little to help those who don't. So given the remarkable paucity of writing on the topic (including the documentation of the compilers themselves), I decided it would make a good inaugural topic for this blog.</p> <h2 id="so_what_is_fast-math"><a href="#so_what_is_fast-math">So what is fast-math?</a></h2> <p>It's a compiler flag or option that exists in many languages and compilers, including:</p> <ul> <li><p><code>-ffast-math</code> (and included by <code>-Ofast</code>) in <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a> and <a href="https://clang.llvm.org/docs/UsersManual.html#cmdoption-ffast-math">Clang</a></p> </li><li><p><code>-fp-model=fast</code> (the default) in <a href="https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/compiler-option-details/floating-point-options/fp-model-fp.html">ICC</a></p> </li><li><p><code>/fp:fast</code> in <a href="https://docs.microsoft.com/en-us/cpp/build/reference/fp-specify-floating-point-behavior?view=msvc-170">MSVC</a></p> </li><li><p><a href="https://docs.julialang.org/en/v1/manual/command-line-options/#command-line-options"><code>--math-mode=fast</code> command line option</a> or <a href="https://docs.julialang.org/en/v1/base/math/#Base.FastMath.@fastmath"><code>@fastmath</code> macro</a> in Julia.</p> </li></ul> <p>So what does it actually do? Well, as the name said, it makes your math faster. That sounds great, we should definitely do that!</p> <blockquote> <p>I mean, the whole point of fast-math is trading off speed with correctness. If fast-math was to give always the correct results, it wouldn’t be fast-math, it would be the standard way of doing math.</p> </blockquote> <p>— <a href="https://discourse.julialang.org/t/whats-going-on-with-exp-and-math-mode-fast/64619/7?u=simonbyrne">Mosè Giordano</a></p> <p>The rules of floating point operations are specified in <a href="https://en.wikipedia.org/wiki/IEEE_754">the IEEE 754 standard</a>, which all popular programming languages (mostly) adhere to; compilers are only allowed to perform optimizations which obey these rules. Fast-math allows the compiler to break some of these rules: these breakages may seem pretty innocuous at first glance, but can have significant and occasionally unfortunate downstream effects.</p> <p>In <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a>, <code>-ffast-math</code> (or <code>-Ofast</code>) enables the following options: <code>-fno-math-errno</code>, <code>-funsafe-math-optimizations</code>, <code>-ffinite-math-only</code>, <code>-fno-rounding-math</code>, <code>-fno-signaling-nans</code>, <code>-fcx-limited-range</code> and <code>-fexcess-precision=fast</code>. Note that <code>-funsafe-math-optimizations</code> is itself a collection of options <code>-fno-signed-zeros</code>, <code>-fno-trapping-math</code>, <code>-fassociative-math</code> and <code>-freciprocal-math</code>, plus some extra ones, which we will discuss further below.</p> <p>Now some of these are unlikely to cause problems in most cases: <code>-fno-math-errno</code><sup id="fnref:1"><a href="#fndef:1">[1]</a></sup>, <code>-fno-signaling-nans</code>, <code>-fno-trapping-math</code> disable rarely-used (and poorly supported) features. Others, such as <code>-freciprocal-math</code> can reduce accuracy slightly, but are unlikely to cause problems in most cases.</p> <p><a href="https://kristerw.github.io/2021/10/19/fast-math/">Krister Walfridsson</a> gives a very nice (and somewhat more objective) description of some of these, but I want to focus on three in particular.</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-ffinite-math-only"><code>-ffinite-math-only</code></a></h2> <blockquote> <p>Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or +-Infs.</p> </blockquote> <p>The intention here is to allow the compiler to perform some <a href="https://stackoverflow.com/a/10145714/392585">extra optimizations</a> that would not be correct if NaNs or Infs were present: for example the condition <code>x == x</code> can be assumed to always be true (it evaluates false if <code>x</code> is a NaN).</p> <p>This sounds great! My code doesn't generate any NaNs or Infs, so this shouldn't cause any problems.</p> <p>But what if your code doesn't generate any intermediate NaNs only because it internally calls <code>isnan</code> to ensure that they are correctly handled?</p> <p>  — based on <a href="https://twitter.com/johnregehr/status/1440024236257542147">an example from John Regehr</a></p> <p>(to explain what this is showing: the function is setting the return register <code>eax</code> to zero, by <code>xor</code>-ing it with itself, which means the function will always return <code>false</code>)</p> <p>That's right, your compiler has just removed all those checks.</p> <p>Depending on who you ask, this is either obvious ("you told the compiler there were no NaNs, so why does it need to check?") or ridiculous ("how can we safely optimize away NaNs if we can't check for them?"). Even compiler developers <a href="https://twitter.com/johnregehr/status/1440021297103134720">can't agree</a>.</p> <p>This is perhaps the single most frequent cause of fast-math-related <a href="https://stackoverflow.com/a/22931368/392585">StackOverflow</a> <a href="https://stackoverflow.com/q/7263404/392585">questions</a> and <a href="https://github.com/numba/numba/issues/2919">GitHub</a> <a href="https://github.com/google/jax/issues/276">bug</a> <a href="https://github.com/pytorch/glow/issues/2073">reports</a>, and so if your fast-math-compiled code is giving wrong results, the very first thing you should do is disable this option (<code>-fno-finite-math-only</code>).</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-fassociative-math"><code>-fassociative-math</code></a></h2> <blockquote> <p>Allow re-association of operands in series of floating-point operations.</p> </blockquote> <p>This allows the compiler to change the order of evaluation in a sequence of floating point operations. For example if you have an expression <code>(a + b) + c</code>, it can evaluate it instead as <code>a + (b + c)</code>. While these are mathematically equivalent with real numbers, they aren't equivalent in floating point arithmetic: the errors they incur can be different, in some cases quite significantly so:</p> <pre><code>julia&gt; a = <span>1e9</span>+<span>1</span>; b = -<span>1e9</span>; c = <span>0.1</span>;

julia&gt; (a+b)+c
<span>1.1</span>

julia&gt; a+(b+c)
<span>1.100000023841858</span></code></pre> <h3 id="vectorization"><a href="#vectorization">Vectorization </a></h3> <p>So why would you want to do this? One primary reason is that it can enable use of vector/SIMD instructions:</p>  <p>For those who aren't familiar with SIMD operations (or reading assembly), I'll try to explain briefly what is going on here (others can skip this part). Since raw clock speeds haven't been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a "vector" (basically, a short sequence of values contiguous in memory).</p> <p>In this case, instead of performing a sequence of floating point additions (<code>addss</code>), it is able to make use of a SIMD instruction (<code>addps</code>) which takes vector <code>float</code>s (4 in this case, but it can be up to 16 with AVX 512 instructions), and adds them element-wise to another vector in one operation. It does this for the whole array, followed by a final reduction step where to sum the vector to a single value. This means that instead of evaluating</p> <pre><code>s = arr[<span>0</span>] + arr[<span>1</span>];
s = s + arr[<span>2</span>];
s = s + arr[<span>3</span>];
...
s = s + arr[<span>255</span>];</code></pre> <p>it is actually doing</p> <pre><code>s0 = arr[<span>0</span>] + arr[<span>4</span>]; s1 = arr[<span>1</span>] + arr[<span>5</span>]; s2 = arr[<span>2</span>] + arr[<span>6</span>];  s3 = arr[<span>3</span>] + arr[<span>7</span>];
s0 = s0 + arr[<span>8</span>];     s1 = s1 + arr[<span>9</span>];     s2 = s2 + arr[<span>10</span>];     s3 = s3 + arr[<span>11</span>]);
...
s0 = s0 + arr[<span>252</span>];   s1 = s1 + arr[<span>253</span>];   s2 = s2 + arr[<span>254</span>];    s3 = s3 + arr[<span>255</span>]);
sa = s0 + s1;
sb = s2 + s3;
s = sa + sb;</code></pre> <p>where each line corresponds to one floating point instruction.</p> <p>The problem here is that the compiler generally isn't allowed to make this optimization: it requires evaluating the sum in a different association grouping than was specified in the code, and so can give different results<sup id="fnref:4"><a href="#fndef:4">[2]</a></sup>. Though in this case it is likely harmless (or may even improve accuracy<sup id="fnref:2"><a href="#fndef:2">[3]</a></sup>), this is not always the case.</p> <h3 id="compensated_arithmetic"><a href="#compensated_arithmetic">Compensated arithmetic</a></h3> <p>Certain algorithms however depend very strictly on the order in which floating point operations are performed. In particular <em>compensated arithmetic</em> operations make use of it to compute the error that is incurred in intermediate calculations, and correct for that in later computations.</p> <p>The most well-known algorithm which makes use of this is <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan summation</a>, which corrects for the round off error incurred at addition step in the summation loop. We can compile an implementation of Kahan summation with <code>-ffast-math</code>, and compare the result to the simple loop summation above:</p>  <p>It gives <em>exactly</em> the same assembly as the original summation code above. Why?</p> <p>If you substitute the expression for <code>t</code> into <code>c</code>, you get</p> <pre><code>c = ((s + y) - s) - y);</code></pre>
<p>and by applying reassociation, the compiler will then determine that <code>c</code> is in fact always zero, and so may be completely removed. Following this logic further, <code>y = arr[i]</code> and so the inside of the loop is simply</p>
<pre><code>s = s + arr[i];</code></pre>
<p>and hence it "optimizes" identically to the simple summation loop above.</p>
<p>This might seem like a minor tradeoff, but compensated arithmetic is often used to implement core math functions, such as trigonometric and exponential functions. Allowing the compiler to reassociate inside these can give <a href="https://github.com/JuliaLang/julia/issues/30073#issuecomment-439707503">catastrophically wrong answers</a>.</p>
<h2 id="flushing_subnormals_to_zero"><a href="#flushing_subnormals_to_zero">Flushing subnormals to zero</a></h2>
<p>This one is the most subtle, but by far the most insidious, as it can affect code compiled <em>without</em> fast-math, and is only cryptically documented under <code>-funsafe-math-optimizations</code>:</p>
<blockquote>
<p>When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.</p>
</blockquote>
<p>So what does that mean? Well this is referring to one of those slightly annoying edge cases of floating point numbers, <em>subnormals</em> (sometimes called <em>denormals</em>). <a href="https://en.wikipedia.org/wiki/Subnormal_number">Wikipedia gives a decent overview</a>, but for our purposes the main thing you need to know is (a) they're <em>very</em> close to zero, and (b) when encountered, they can incur a significant performance penalty on many processors<sup id="fnref:6"><a href="#fndef:6">[4]</a></sup>.</p>
<p>A simple solution to this problem is "flush to zero" (FTZ): that is, if a result would return a subnormal value, return zero instead. This is actually fine for a lot of use cases, and this setting is commonly used in audio and graphics applications. But there are plenty of use cases where it isn't fine: FTZ breaks some important floating point error analysis results, such as <a href="https://en.wikipedia.org/wiki/Sterbenz_lemma">Sterbenz' Lemma</a>, and so unexpected results (such as iterative algorithms failing to converge) may occur.</p>
<p>The problem is how FTZ actually implemented on most hardware: it is not set per-instruction, but instead <a href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/floating-point-operations/understanding-floating-point-operations/setting-the-ftz-and-daz-flags.html">controlled by the floating point environment</a>: more specifically, it is controlled by the floating point control register, which on most systems is set at the thread level: enabling FTZ will affect all other operations in the same thread.</p>
<p>GCC with <code>-funsafe-math-optimizations</code> enables FTZ (and its close relation, denormals-are-zero, or DAZ), even when building shared libraries. That means simply loading a shared library can change the results in completely unrelated code, which is <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/issues/253#issuecomment-573589022">a fun debugging experience</a>.</p>
<h2 id="what_can_programmers_do"><a href="#what_can_programmers_do">What can programmers do?</a></h2>
<p>I've joked on Twitter that "friends don't let friends use fast-math", but with the luxury of a longer format, I will concede that it has valid use cases, and can actually give valuable performance improvements; as SIMD lanes get wider and instructions get fancier, the value of these optimizations will only increase. At the very least, it can provide a useful reference for what performance is left on the table. So when and how can it be safely used?</p>
<p>One reason is if you don't care about the accuracy of the results: I come from a scientific computing background where the primary output of a program is a bunch of numbers. But floating point arithmetic is used in many domains where that is not the case, such as audio, graphics, games, and machine learning. I'm not particularly familiar with requirements in these domains, but there is an interesting rant by <a href="https://gcc.gnu.org/legacy-ml/gcc/2001-07/msg02150.html">Linus Torvalds from 20 years ago</a>, arguing that overly strict floating point semantics are of little importance outside scientific domains. Nevertheless, <a href="https://twitter.com/supahvee1234/status/1382907921848221698">some anecdotes</a> suggest fast-math can cause problems, so it is probably still useful understand what it does and why. If you work in these areas, I would love to hear about your experiences, especially if you identified which of these optimizations had a positive or negative impact.</p>
<blockquote>
<p>I hold that in general it’s simply intractable to “defensively” code against the transformations that <code>-ffast-math</code> may or may not perform. If a sufficiently advanced compiler is indistinguishable from an adversary, then giving the compiler access to <code>-ffast-math</code> is gifting that enemy nukes. That doesn’t mean you can’t use it! You just have to test enough to gain confidence that no bombs go off with your compiler on your system.</p>
</blockquote>
<p>— <a href="https://discourse.julialang.org/t/when-if-a-b-x-1-a-b-divides-by-zero/7154/5?u=simonbyrne">Matt Bauman</a></p>
<p>If you do care about the accuracy of the results, then you need to approach fast-math much more carefully and warily. A common approach is to enable fast-math everywhere, observe erroneous results, and then attempt to isolate and fix the cause as one would usually approach a bug. Unfortunately this task is not so simple: you can't insert branches to check for NaNs or Infs (the compiler will just remove them), you can't rely on a debugger because <a href="https://gitlab.com/libeigen/eigen/-/issues/1674#note_709679831">the bug may disappear in debug builds</a>, and it can even <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1127544">break printing</a>.</p>
<p>So you have to approach fast-math much more carefully. A typical process might be:</p>
<ol>
<li><p>Develop reliable validation tests</p>

</li><li><p>Develop useful benchmarks</p>

</li><li><p>Enable fast-math and compare benchmark results</p>

</li><li><p>Selectively enable/disable fast-math optimizations<sup id="fnref:5"><a href="#fndef:5">[5]</a></sup> to identify</p>
<p>a. which optimizations have a performance impact,</p>
<p>b. which cause problems, and</p>
<p>c. where in the code those changes arise.</p>

</li><li><p>Validate the final numeric results</p>

</li></ol>
<p>The aim of this process should be to use the absolute minimum number of fast-math options, in the minimum number of places, while testing to ensure that the places where the optimizations are used remain correct.</p>
<p>Alternatively, you can look into other approaches to achieve the same performance benefits: in some cases it is possible to rewrite the code to achieve the same results: for example, it is not uncommon to see expressions like <code>x * (1/y)</code> in many scientific codebases.</p>
<p>For SIMD operations, tools such as <a href="https://www.openmp.org/spec-html/5.0/openmpsu42.html">OpenMP</a> or <a href="https://ispc.github.io/">ISPC</a> provide constructions to write code that is amenable to automatic SIMD optimizations. Julia provides the <a href="https://docs.julialang.org/en/v1/base/base/#Base.SimdLoop.@simd"><code>@simd</code> macro</a>, though this also has some important caveats on its use. At the more extreme end, you can use <a href="https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/">SIMD intrinsics</a>: these are commonly used in libraries, often with the help of code generation (<a href="http://fftw.org/">FFTW</a> uses this appraoch), but requires considerably more effort and expertise, and can be difficult to port to new platforms.</p>
<p>Finally, if you're writing an open source library, please don't <a href="https://github.com/tesseract-ocr/tesseract/blob/5884036ecdb2807419cbd21b7ca44b630f547d80/Makefile.am#L140">hardcode fast-math into your Makefile</a>.</p>
<h2 id="what_can_language_and_compilers_developers_do"><a href="#what_can_language_and_compilers_developers_do">What can language and compilers developers do?</a></h2>
<p>I think the widespread use of fast-math should be considered a fundamental design failure: by failing to provide programmers with features they need to make the best use of modern hardware, programmers instead resort to enabling an option that is known to be blatantly unsafe.</p>
<p>Firstly, GCC should address the FTZ library issue: the bug has been <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522">open for 9 years, but is still marked NEW</a>. At the very least, this behavior should be more clearly documented, and have a specific option to disable it.</p>
<p>Beyond that, there are 2 primary approaches: educate users, and provide finer control over the optimizations.</p>
<p>The easiest way to educate users is to give it a better name. Rather than "fast-math", something like "unsafe-math". Documentation could also be improved to educate users on the consequences of these choices (consider this post to be my contribution to toward that goal). Linters and compiler warnings could, for example, warn users that their <code>isnan</code> checks are now useless, or even just highlight which regions of code have been impacted by the optimizations.</p>
<p>Secondly, languages and compilers need to provide better tools to get the job done. Ideally these behaviors shouldn't be enabled or disabled via a compiler flag, which is a very blunt tool, but specified locally in the code itself, for example</p>
<ul>
<li><p>Both GCC and Clang let you <a href="https://stackoverflow.com/a/40702790/392585">enable/disable optimizations on a per-function basis</a>: these should be standardized to work with all compilers.</p>

</li><li><p>There should be options for even finer control, such as a pragma or macro so that users can assert that "under no circumstances should this <code>isnan</code> check be removed/this arithmetic expression be reassociated".</p>

</li><li><p>Conversely, a mechanism to flag certain addition or subtraction operations which the compiler is allowed to reassociate (or contract into a fused-multiply-add operation) regardless of compiler flags.<sup id="fnref:3"><a href="#fndef:3">[6]</a></sup></p>

</li></ul>
<p>This still leaves open the exact question of what the semantics should be: if you combine a regular <code>+</code> and a fast-math <code>+</code>, can they reassociate? What should the scoping rules be, and how should it interact with things like inter-procedural optimization? These are hard yet very important questions, but they need to be answered for programmers to be able to make use of these features safely.</p>
<p>For more discussion, see <a href="https://news.ycombinator.com/item?id=29201473">HN</a>.</p>
<h2 id="updates"><a href="#updates">Updates</a></h2>
<p>A few updates since I wrote this note:</p>
<ul>
<li><p>Brendan Dolan-Gavitt wrote a fantastic piece about <a href="https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html">FTZ-enabling libraries in Python packages</a>: it also has some nice tips on how to find out if your library was compiled with fast-math.</p>
<ul>
<li><p>He also has a nice proof-of-concept <a href="https://github.com/moyix/2_ffast_2_furious">buffer overflow vulnerability</a>.</p>

</li></ul>

</li><li><p>It turns out Clang also enables FTZ when building shared libraries with fast-math: but only if you have a system GCC installation. I've <a href="https://github.com/llvm/llvm-project/issues/57589">opened an issue</a>.</p>

</li><li><p>MSVC doesn't remove <code>isnan</code> checks, but instead <a href="https://twitter.com/dotstdy/status/1567748577962741760">generates what looks like worse code</a> when compiling with fast-math.</p>

</li><li><p>The FTZ library issue will be <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522#c45">fixed in GCC 13</a>!</p>

</li></ul>

 
 
 
 <table id="fndef:5">
    <tbody><tr>
        <td><a href="#fnref:5">[5]</a>
        </td><td>As mentioned above, <code>-fno-finite-math-only</code> should be the first thing you try.
    
</td></tr></tbody></table>
 <table id="fndef:3">
    <tbody><tr>
        <td><a href="#fnref:3">[6]</a>
        </td><td>Rust provides something like this via <a href="https://stackoverflow.com/a/40707111/392585">experimental intrinsics</a>, though I'm not 100% clear on what optimzations are allowed.
    
</td></tr></tbody></table>

<div>
  <p>
    © Simon Byrne. Last modified: April 06, 2024.
  </p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Responses May Include Mistakes (169 pts)]]></title>
            <link>https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</link>
            <guid>44142113</guid>
            <pubDate>Sat, 31 May 2025 05:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/">https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</a>, See on <a href="https://news.ycombinator.com/item?id=44142113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>The other day I wanted to look up a specific IBM PS/2 model, a circa 1992 PS/2 Server system. So I punched the model into Google, and got this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png"><img decoding="async" width="640" height="487" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-300x228.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-768x584.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png 1147w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That did not look quite right, since the machine I was looking for had 486 processors (yes, plural). And it most certainly <em>did</em> use Microchannel (MCA).</p>



<p>Alright, let’s try again:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png"><img decoding="async" width="640" height="557" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-300x261.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-768x669.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png 1174w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Simply re-running the identical query produces a different summary. Although the AI still claims that PS/2 Model 280 is an ISA-based 286 system. Maybe the third time is the charm?</p>



<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png"><img loading="lazy" decoding="async" width="640" height="465" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-300x218.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-768x557.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>The AI is really quite certain that PS/2 Model 280 was a 286-based system released in 1987, and I was really looking for a newer machine. Interestingly, the first time the AI claimed Model 280 had 1MB RAM expandable to 6MB, and now it supposedly only has 640 KB RAM. But the AI seems sure that Model 280 had a 1.44 MB drive and VGA graphics.</p>



<p>What if we try again? After a couple of attempts, yet different answer pops up:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png"><img loading="lazy" decoding="async" width="640" height="431" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-300x202.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-768x518.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png 1138w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Oh look, now the PS/2 Model 280 is a 286 expandable to 128 MB RAM. Amazing! Never mind that the 286 was architecturally limited to 16 MB.</p>



<p>Even better, the AI now tells us that “PS/2 Model 280 was a significant step forward in IBM’s personal computer line, and it helped to establish the PS/2 as a popular and reliable platform.”</p>



<p>The only problem with all that? <em>There is no PS/2 Model 280, and never was.</em> I simply had the model number wrong. The Google AI just “helpfully” hallucinates something that at first glance seems quite plausible, but is in fact utter nonsense.</p>



<p>But wait, that’s not the end of the story. If you try repeating the query often enough, you might get this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png"><img loading="lazy" decoding="async" width="640" height="409" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-300x192.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-768x491.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That answer is <em>actually correct</em>! “Model 280 was not a specific model in the PS/2 series”, and there was in fact an error in the query.</p>



<p>Here’s another example of a correct answer:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png"><img loading="lazy" decoding="async" width="640" height="415" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-300x194.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-768x498.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png 1174w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Unfortunately the correct answer comes up maybe 10% of the time when repeating the query, if at all. In the vast majority of attempts, the AI simply makes stuff up. I do not consider made up, hallucinated answers useful, in fact they are worse than useless. </p>



<p>This minor misadventure might provide a good window into AI-powered Internet search. To a non-expert, the made up answers will seem highly convincing, because there is a lot of detail and overall the answer does not look like junk.</p>



<p>An expert will immediately notice discrepancies in the hallucinated answers, and will follow for example the <a href="https://en.wikipedia.org/wiki/List_of_IBM_PS/2_models">List of IBM PS/2 Models</a> article on Wikipedia. Which will very quickly establish that there is no Model 280.</p>



<p>The (non-expert) users who would most benefit from an AI search summary will be the ones most likely misled by it.</p>



<p>How much would you value a research assistant who gives you a different answer every time you ask, and although sometimes the answer may be correct, the incorrect answers look, if anything, more “real” than the correct ones?</p>



<p>When Google says “AI responses may include mistakes”, do not take it lightly. The AI generated summary could be utter nonsense, and just because it sounds convincing doesn’t mean it has anything to do with reality. Caveat emptor!</p>
											</div><div><p>
							This entry was posted in <a href="https://www.os2museum.com/wp/category/ibm/" rel="category tag">IBM</a>, <a href="https://www.os2museum.com/wp/category/ps2/" rel="category tag">PS/2</a>. Bookmark the <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/" title="Permalink to AI Responses May Include Mistakes" rel="bookmark">permalink</a>.													</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valkey Turns One: Community fork of Redis (227 pts)]]></title>
            <link>https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</link>
            <guid>44140379</guid>
            <pubDate>Fri, 30 May 2025 22:24:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/">https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</a>, See on <a href="https://news.ycombinator.com/item?id=44140379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  
  <article>
    <div>
        
<p>A year ago, Redis Inc (formerly Garantia Data) made a controversial move that disrupted the open source ecosystem: it closed the source of Redis.&nbsp;<a href="https://www.gomomento.com/blog/rip-redis-how-garantia-data-pulled-off-the-biggest-heist-in-open-source-history/" target="_blank" rel="noreferrer noopener">As I wrote at the time</a>, it was a trust-breaking decision that could have shattered the community.</p>



<p>But instead of splintering, the community responded with purpose. Out of that disruption came&nbsp;<a href="https://valkey.io/" target="_blank" rel="noreferrer noopener">Valkey</a>,&nbsp;a fork that took a shot at keeping the community alive.</p>



<h2>A Return, A Reversal</h2>



<p>As part of efforts to rebuild trust with the community, Redis Inc&nbsp;<a href="https://redis.io/blog/welcome-back-to-redis-antirez/" target="_blank" rel="noreferrer noopener">brought back</a>&nbsp;Salvatore Sanfilippo (aka Antirez), the original creator of Redis. I am genuinely excited about his return because it is already impactful. He’s following through on his promise of contributing new features and performance optimizations to Redis. More profoundly,&nbsp;<a href="https://redis.io/blog/agplv3/" target="_blank" rel="noreferrer noopener">Redis 8.0 has been open-sourced again</a>.</p>



<p>Redis acknowledged that adopting <a href="https://en.wikipedia.org/wiki/Server_Side_Public_License" target="_blank" rel="noreferrer noopener">SSPL</a> strained their bond with the community, questioning contributions from others in the same breath.</p>



<blockquote>
<figure><blockquote><p>How do you keep innovating and investing in OSS projects when cloud providers reap the profits and control the infrastructure without proportional contributions back to the projects that they exploit?</p></blockquote></figure>
</blockquote>



<p>The disheartening move from Redis Inc catalyzed an unprecedented wave of collaboration and contributions. Valkey became the test of the community resolve to keep itself together. One year later, Valkey hasn’t just survived –&nbsp;<a href="https://www.linkedin.com/posts/kshams_valkey-rocks-the-most-remarkable-thing-about-activity-7318683448506793985-_qJE" target="_blank" rel="noreferrer noopener">it’s thriving</a>! The Async I/O Threading model <a href="https://github.com/valkey-io/valkey/pull/758" target="_blank" rel="noreferrer noopener">contribution</a> from AWS unlocked 3x+ throughput by fundamentally changing how I/O threads work inside Redis.</p>



<p>But how do these and other contributions compare to Redis 8? Can we hit 1M RPS out of an 8 VCPU instance (c8g.2xl) on either Valkey 8.1 or Redis 8.0 (with 1KB items, 3M items in the key space, and ~500 connections)? It’s time for a bake off!</p>



<h2>Valkey 8.1 vs Redis 8.0: Can the Fork Outrun the Source?</h2>



<p>The punchline: we could not sustain 1M RPS on an 8 VCPU instance with either Valkey or Redis 8.0, but we got really close!!</p>



<p>On a full-tuned c8g.2xl (8 VCPU), Valkey 8.1.1 pushed to 999.8K RPS on SETs with .8ms p99 latency. Redis 8.0 got as high as 729.4 RPS on SETs with .99ms p99 latencies. On each iteration, we tested 50M SETs followed by 50M GETs. We varied connection counts to optimize the maximum throughput for each system.</p>



<figure><blockquote><p><mark>Valkey achieved higher throughput and lower latency across both reads and writes (37% higher on SET and 16% higher on GET), alongside 30% faster p99 latencies for SET and 60%+ faster on GET.</mark></p></blockquote></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png"></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="341" src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png" alt="Valkey vs Redis table with SET and GET command" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20341'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png"></figure>



<h2>Threading the multi-threading needle</h2>



<p>If I had a penny for every time heard, “but Redis /Valkey is single threaded….”</p>



<p>Antirez’s emphasis on a shared nothing architecture has been foundational for Redis. Nevertheless, as early as 2020, Redis added support for I/O threads. Unfortunately, they did not offer drastic improvement until recently. If you have previously tried and discarded I/O threads, it is time to evaluate again!</p>



<p>On Valkey, we see SET throughput going from 239K RPS without I/O threads to 678K with 6 threads. Meanwhile, p99 latencies dropped from 1.68ms to 0.93ms <strong>despite doing nearly 3x the throughput</strong>! Similarly, Redis went from 235K RPS without I/O threads to 563K RPS with 6 I/O threads. P99s for Redis also dropped around 40% from 1.35ms to 0.84ms.</p>



<p>Two key takeaways emerged:</p>



<ol>
<li>With two threads, gains were modest (~20%). The impact only really surfaced at three threads and beyond.</li>



<li>Redis and Valkey were neck-and-neck until the fourth thread. After that, Valkey pulled away sharply.</li>
</ol>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png"></figure>



<h3>SET Performance on Valkey with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png"></figure>



<h3>SET Performance on Redis with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png"></figure>



<h2>Pushing Valkey Throughput Further</h2>



<p>In the previous section, we saw that Valkey could hit 678K RPS on SETs with 6 threads and 256 connections. If we up the connections to 400, the throughput goes up to 832K RPS. How did we get the additional 167K RPS?</p>



<p>We used <a href="https://github.com/iopsystems/rezolus" target="_blank" rel="noreferrer noopener">Rezolus</a>, our favorite Linux performance telemetry agent, to get deep insights into the system under stress. You can see in the charts below that overall CPU utilization is around 80% and unevenly distributed across the 8 cores.</p>



<p>Diving deeper, this is driven by hardware interrupts from network queues across all 8 cores. Interrupts are bad because they disrupt a hard working Valkey thread to yield to handle network packets.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="835" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png" alt="CPU Usage chart" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20835'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png"></figure>



<p>What if we could avoid the context switching on our <code><sup>c8g.2xl</sup></code> with 8 cores? Running close to a million RPS requires considerable packet processing horsepower. Luckily, since a lot of work happens at the Nitro level on EC2 instances, two allocated cores to IRQs is all you need (if you let them focus). Pinning the IRQs to two cores is pretty straightforward.</p>



<pre><code>sudo ethtool -L ens34 combined 2 # reduce to 2 IRQs
grep ens34 /proc/interrupts # ours were on 99 and 100
echo 1 | sudo tee /proc/irq/99/smp_affinity # pin 99 to core 1
echo 2 | sudo tee /proc/irq/100/smp_affinity # pin 100 to core 2</code></pre>



<p>But how do we let these threads focus? and how do we avoid Redis / Valkey threads contending for the same cores? We pin Redis/Valkey to cores 3-8, giving their IO-Threads better isolation while also allowing the IRQs to focus. We used the <code><sup>--cpuset-cpus</sup></code> Docker flag to set these CPU assignments, making sure that Redis and Valkey process stayed pinned to the intended cores throughout the test. This reduces cross-core contention and improves cache locality, <strong>both of which are critical for minimizing tail latencies at high throughput</strong>. Ideal core allocation can vary in multi-tenant environments or mixed workloads, but in this benchmark it provided clean isolation between system and application workloads.</p>



<p><strong>Redis:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" redis:8.0 \
  --save "" --appendonly no \
  --io-threads 6  \
  --protected-mode no --maxmemory 10gb</code></pre>



<p><strong>Valkey:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" valkey/valkey:8.1.1 \
  --save "" --appendonly no --io-threads 6 \
  --protected-mode no --maxmemory 10gb</code></pre>



<p>Let’s see what Rezolus has to say about the new setup with IRQs pinned to the first 2 cores and Valkey pinned to the remaining 6 cores. First, we observed meaningfully higher CPU Utilization. Second, looking at the bottom chart (SoftIRQ), we see that it is now limited to only the first two cores. Third, the Valkey cores are running red hot, whereas we previously saw a much more scattered distribution one usage across cores. <strong>While this setup is ideal for this benchmark, optimal IRQ tuning depends heavily on NIC architecture and the concurrency model of your application.</strong></p>



<figure><img loading="lazy" decoding="async" width="720" height="603" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png" alt="CPU Chart 2" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" sizes="(max-width: 720px) 100vw, 720px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20720%20603'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png"></figure>



<p>The extra 20% CPU utilization is what buys us the extra 167K RPS (from 832K RPS to 999.8K RPS).</p>



<h2>Try it Yourself (And Know Before You Go)</h2>



<p>These benchmarks are hopefully a beginning and not the end. Our hope is that this sets up both Valkey and Redis communities to continue the performance improvement journey. We also recognize that many folks may want to reproduce the benchmark in their own environments, incorporating their workflow specifics. Below, we outline some key instructions that you can use to reproduce this in your own AWS account within an hour.</p>



<p><strong>Instance Types:</strong>&nbsp;We used AWS Graviton4-based&nbsp;<code><sup>c8g</sup></code>&nbsp;instances, launched in September 2024. The&nbsp;<sup><code>c8g.2xlarge</code></sup> server node provides 8 vCPUs (costing roughly $250/month in us-east-1), while the&nbsp;<code><sup>c8g.8xlarge</sup></code>&nbsp;load generator offers 32 vCPUs. This provided enough CPU headroom to cleanly isolate the benchmark workload, IRQ handling, and Redis/Valkey processing. The same c8g.2xl instance was used to run valkey and redis (one at a time). The same load gen node was run each time. Valkey and Redis were restarted right before each test to ensure fairness.</p>



<p><strong>Placement Groups:</strong>&nbsp;We used EC2 placement groups (cluster mode) to ensure minimal network jitter and low-latency communication between the client and server nodes. Placement groups offer extremely tight latencies by reducing the number of hops between your EC2 instances. This has the upside on higher throughput, fewer interruptions, and lower latencies – but it has some shared fate / blast radius implications that are worth considering before deploying them in your production environment.</p>



<p><strong>Core Pinning.</strong> To see the highest throughput and lowest latencies, consider core pinning and reducing the IRQs. See section above for specific instructions we used on our 8 core instance. It is also important to apply similar techniques on your test nodes.</p>



<p><strong>Vary the connections.</strong> Connection count is a surprisingly crucial variable for both Redis and Valkey. In fact, latencies rise steeply as you approach 1024 connections on both of them. For example, going from 400 to 1024 connections, Valkey’s SET throughput dropped from 999.9K RPS with to 969K RPS and p99 latencies doubled from .8ms to 1.6ms (at 2048 conns, p99 latencies triple). Going from 384 connections to 1024, Redis throughput drops from 729.4K RPS to 668K RPS, and p99 latencies more than double from .99ms to 2.5ms. Lower throughput with higher latencies? You get why connection count tuning is so crucial here.</p>



<p><strong>Key Space.</strong> If you want the best numbers, <a href="https://www.linkedin.com/posts/yaoyue-thinkingfish_my-performance-rant-of-the-day-if-you-are-activity-7326350824261980161-qB6h/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAANW9wBFDc3gQ3Jwp6YswZ_BARGfJvyJQQ" target="_blank" rel="noreferrer noopener">use smaller values and a really small key space</a> (<sup><code>-r 10000</code></sup>). This will help you get everything from L3 cache. To make this test slightly more real world, we used 1KB items (<code><sub><sup>-d 1024</sup></sub></code>) and a key space of 3Million (<code><sup>-r 3000000</sup></code>).</p>



<p><strong>Multi-Thread the Benchmark App.</strong> To get the maximum throughput out of valkey-benchmark, make sure to turn on multi-threading on the benchmark tool as well. The <code><sup>--threads 6</sup></code> flag tells valkey-benchmark to run in multi-threaded mode.</p>



<p><strong>Benchmark command:</strong></p>



<pre><code>docker run --network="host" --rm --cpuset-cpus="2-7" \
valkey/valkey:8.0.1 valkey-benchmark \
-h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \
-r 3000000 --threads 6 -d 1024</code></pre>



<h2>A Final Caveat: Benchmarking is imprecise in nature</h2>



<p>We made every effort to make this benchmark resemble more real world workflows, but you can always do better. Valkey-bench is not perfect (nothing is). We have a wishlist of improvements (and so <a href="https://github.com/valkey-io/valkey/issues/900" target="_blank" rel="noreferrer noopener">does</a> the Valkey project).</p>



<p>First, today, it simply pushes as much load as the server is able to handle instead of targeting a particular TPS. The real world rarely modulates its throughput based on your latency. Second, once it can target specific load, it would become closer to real world if it could modulate the load to show spikes and troughs as opposed to running a consistent throughput profile. Lastly, it’s rare to see 100% GETs or 100% SETs in a Key Value cache workflow. We’d love to provide a SET:GET ratio to see how the system reacts.</p>



<p>At Momento, we typically do our testing using <a href="https://github.com/iopsystems/rpc-perf" target="_blank" rel="noreferrer noopener">rpc-perf</a>. It is written entirely in rust, handles more real world scenarios (like the three feature requests above), and pairs incredibly well with Rezolus. Regardless, even rpc-perf is a synthetic benchmark and even though it gives you more degrees of freedom to simulate production workflows, the results should not be interpreted as generally applicable to every workflow. Small variables make huge differences – and simulations are no match for production.</p>



<h2>Final Thoughts: Performance Is a Practice</h2>



<p>Valkey has not only kept pace – it’s setting it. Meanwhile, the performance ceiling keeps rising. But getting there isn’t automatic. It requires expertise across systems, infrastructure, and workload behavior.</p>



<p>At Momento, we help teams achieve this kind of performance every day. Whether you’re running Valkey, Redis, or evaluating your options – we’re here to help you scale with confidence.</p>



<p><strong>Want help tuning your real-time infrastructure? <a href="https://gomomento.com/contact-us" target="_blank" rel="noreferrer noopener">Let’s talk.</a></strong></p>







<hr>



<p><strong>Special thanks to Yao and Brian from&nbsp;<a href="https://iop.systems/" target="_blank" rel="noreferrer noopener">IOP Systems</a>&nbsp;for providing the tools, including rpc-perf and rezolus, as well as insights for this benchmark.</strong></p>




        
      </div>

  </article>
  


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Silicon Valley finally has a big electronics retailer again: Micro Center opens (253 pts)]]></title>
            <link>https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</link>
            <guid>44140378</guid>
            <pubDate>Fri, 30 May 2025 22:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx">https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=44140378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p dir="ltr">After years of waiting, the ribbon has been cut and Micro Center Silicon Valley is officially open.&nbsp;</p>
<p dir="ltr">On a sunny Friday morning in Santa Clara, with hundreds of fans queued in a line wrapping down the block and around the corner, we welcomed the Silicon Valley community to our newest store, at <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">5201 Stevens Creek Blvd</a>.&nbsp;</p>
<p dir="ltr">If you're a DIY PC builder, a serious gamer, a creator, a maker, or just someone who gets excited over the latest CPUs, GPUs, and 3D printers, then you already know what Micro Center is about</p>
<p dir="ltr">The Bay Area sets a high bar for all things tech, and here you'll find aisles stacked high with components, knowledgeable staff who actually know what they're talking about, and a hands-on experience you just can't replicate online.&nbsp;</p>
<p dir="ltr">The grand opening celebration also features special promotions, including 20% off Windows desktops and laptops and 20% off monitors. Additionally, the store has over 4,000 graphics cards in stock, including exclusive models, that will available for our grand opening event. For more information, please visit the <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">Micro Center Santa Clara page</a>.</p>
<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-1.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>

<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-2.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C++ to Rust Phrasebook (111 pts)]]></title>
            <link>https://cel.cs.brown.edu/crp/</link>
            <guid>44140349</guid>
            <pubDate>Fri, 30 May 2025 22:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cel.cs.brown.edu/crp/">https://cel.cs.brown.edu/crp/</a>, See on <a href="https://news.ycombinator.com/item?id=44140349">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox></mdbook-sidebar-scrollbox>
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                <div id="menu-bar">
                    

                    <h2>C++ to Rust Phrasebook</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="c-to-rust-phrasebook"><a href="#c-to-rust-phrasebook">C++ to Rust Phrasebook</a></h2>
<p>This book is designed to help C++ programmers learn Rust. It provides translations of common C++ patterns into idiomatic Rust. Each pattern is described through concrete code examples along with high-level discussion of engineering trade-offs.</p>
<p>The book can be read front-to-back, but it is designed to be used random-access.
When you are writing Rust code and think, "I know how to do this in C++ but not Rust," then
look for the corresponding chapter in this book.</p>
<p>This book was hand-written by expert C++ and Rust programmers at Brown University's <a href="https://cel.cs.brown.edu/">Cognitive Engineering Lab</a>. Our goal is provide accurate information with a tasteful degree of detail. No text in this book was written by AI.</p>
<p>If you would like updates on when we add new chapters to this book, you can <a href="https://forms.gle/rcrdZihmT81LWy6F6">drop your email here</a>.</p>
<h2 id="other-resources"><a href="#other-resources">Other resources</a></h2>
<p>If you have zero Rust experience, you might consider first reading <a href="https://rust-book.cs.brown.edu/">The Rust Programming
Language</a> or getting a quick overview at <a href="https://learnxinyminutes.com/rust/">Learn X in Y Minutes</a>.</p>
<p>If you are primarily an embedded systems programmer using C or C++, this book is
a complement to <a href="https://docs.rust-embedded.org/book/">The Embedded Rust Book</a>.</p>
<p>Compared to resources like the <a href="https://doc.rust-lang.org/nomicon/">Rustonomicon</a> and <a href="https://rust-unofficial.github.io/too-many-lists/">Learn Rust With Entirely Too Many Linked Lists</a>, this book is less about "Rust behind the scenes" and more about explicitly describing how Rust works in terms of C++.</p>
<h2 id="feedback-on-this-book"><a href="#feedback-on-this-book">Feedback on this book</a></h2>
<p>At the bottom of every page there is a link
to a form where you can submit feedback: typos, factual errors, or any other issues you spot.</p>
<p>If you answer the quizzes at the end of each chapter, we will save your
responses anonymously for research purposes.</p>


                        <a href="https://docs.google.com/forms/d/e/1FAIpQLScoygeNlygODY2owQ-HvU8VGx3hi50aic7ZlKCyhJ0VktjiCg/viewform?usp=pp_url&amp;entry.1450251950=C++%20to%20Rust%20Phrasebook">Click here to leave us feedback about this page.</a>
                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://cel.cs.brown.edu/crp/idioms/constructors.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://cel.cs.brown.edu/crp/idioms/constructors.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>


        <!-- Google tag (gtag.js) -->
        
        


        


        
        
        

        
        
        

        <!-- Custom JS scripts -->
        


    </div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Photos taken inside musical instruments (767 pts)]]></title>
            <link>https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</link>
            <guid>44139626</guid>
            <pubDate>Fri, 30 May 2025 20:32:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments">https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</a>, See on <a href="https://news.ycombinator.com/item?id=44139626">Hacker News</a></p>
Couldn't get https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Jerry Lewis's "The Day the Clown Cried" discovered in Sweden after 53 years (166 pts)]]></title>
            <link>https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/</link>
            <guid>44139592</guid>
            <pubDate>Fri, 30 May 2025 20:27:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/">https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/</a>, See on <a href="https://news.ycombinator.com/item?id=44139592">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-identifier="article-body-chain" id=""><p id="el-0-3ZTUS3Y2ZRCBTAOIX7Z4SUTSP4">One of cinema's most sought-after <a href="https://www.thenationalnews.com/arts-culture/film-tv/2024/11/25/lost-john-ford-film-the-scarlet-drop-chile/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/film-tv/2024/11/25/lost-john-ford-film-the-scarlet-drop-chile/">lost films</a> has been discovered after having been kept secretly in the collection of a Swedish actor for 45 years. </p><p id="el-1-BRMSRUAMDZBGHFZU3QXU2IMB5I">Comedian <a href="https://www.thenationalnews.com/arts/jerry-lewis-women-are-funny-but-not-when-crude-1.255081" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts/jerry-lewis-women-are-funny-but-not-when-crude-1.255081">Jerry Lewis</a>'s controversial holocaust film <i>The Day the Clown Cried,</i> shot in 1972 but never released, was thought to <a href="https://www.thenationalnews.com/arts-culture/2024/04/02/jerry-lewis-the-day-the-clown-cried-loc/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/2024/04/02/jerry-lewis-the-day-the-clown-cried-loc/">not exist in finished form</a>.</p><p id="el-3-XA2MPSNDZBCRNC6TCMNBHJA4HA">But Hans Crispin, star of the beloved 1980s Swedish TV series <i>Angne &amp; Svullo</i>, claims he stole a complete workprint of the film from the archives of its production studio in 1980 – and has been screening it for guests in his apartment ever since. </p><p id="el-4-PE4RPU5IJRHFFNIBRAGW3UKI2I">“I have the only copy,” Crispin told <a href="https://www.thenationalnews.com/arts-culture/film-tv/2024/08/30/israel-palestine-on-swedish-tv-review/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/film-tv/2024/08/30/israel-palestine-on-swedish-tv-review/">Swedish state news broadcaster SVT.</a> “I stole it from Europafilm in 1980 and copied it to VHS in the attic where we copied other films at night.</p><p id="el-5-OUFA3ROWJNGGTKG3VEOQRQSPQ4">“I've kept the copy in my bank vault,” Crispin added.</p><div id="el-7-LTK64SHFGFDSHHFSSCB34R3BLE"><figure><picture><source width="800" height="534" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=800&amp;height=534"><source width="600" height="400.49999999999994" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=600&amp;height=400"><source width="400" height="267" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=400&amp;height=267"><img _id="LTK64SHFGFDSHHFSSCB34R3BLE" type="image" originalwidth="3072" originalheight="2048" alt="Swedish actor Hans Crispin has a complete workprint of The Day The Clown Cried, SVT has reported. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=400&amp;height=267" width="400" height="267"></picture><div><figcaption>Swedish actor Hans Crispin has a complete workprint of The Day The Clown Cried, SVT has reported. AFP</figcaption></div></figure></div><p id="el-8-6S5ML4BMEVAN7GJ3ZE2N3RA2IA">Crispin recently screened a full copy to journalists from SVT and Sweden's <i>Icon </i>magazine to prove his claim was true. </p><p id="el-9-VVTE7Y2KHJEPTMOUWREYMIGM6Y">“You're the 23rd and 24th people I've shown it to,” he told <i>Icon </i>and SVT. </p><p id="el-10-4ETIS7JGXNAZ7JI52PNYJP6J7E">The actor also revealed that his initial copy was missing the opening six-minute sequence of the film shot in Paris, which was mailed to him anonymously in 1990, along with a note saying that the sender knew he possessed a copy of the rest of the film.</p><h2 id="el-11-K3BGGW4MCZG5NCBMUBIKKXWOBY"><b>Will The Day The Clown Cried be released to the public?</b></h2><p id="el-12-EAZWT3KXKVBPHORFC7PVEXUDB4">Now that he has come out into the open, Crispin intends to make his copy available for the world to see, saying: “It must be seen!”</p><p id="el-13-4SFSIGNF7ZBKDPKNAD6PNME6BQ">Crispin added: “I think I want to hand it over to the next generation. With today's technique, it can be restored. I want to sell it to a serious producer who either restores it or keeps it locked away, or restores it and shows it to people for studying purposes.”</p><p id="el-14-W6Z67YK6XBFQTLO55J3YVKXUUA">The film tells the story of a German circus clown who is imprisoned in a Nazi concentration camp for mocking <a href="https://www.thenationalnews.com/arts-culture/books/hitler-a-bizarrely-sympathetic-biography-1.360922" target="_blank" rel="">Adolf Hitler</a> and is then forced to lure children to their deaths as punishment.</p><div id="el-15-RTWN5IS6O5DRXGUGUCC3P7VSDY"><figure><picture><source width="800" height="534" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=800&amp;height=534"><source width="600" height="400.49999999999994" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=600&amp;height=400"><source width="400" height="267" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=400&amp;height=267"><img _id="RTWN5IS6O5DRXGUGUCC3P7VSDY" type="image" originalwidth="3072" originalheight="2048" alt="The footage that Lewis possessed of the Day The Clown Cried was donated to the Library of Congress in 2015. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=400&amp;height=267" width="400" height="267"></picture><div><figcaption>The footage that Lewis possessed of the Day The Clown Cried was donated to the Library of Congress in 2015. AFP</figcaption></div></figure></div><p id="el-16-S5AOZL6KS5GTLJWT4CE6SEYBYU">Lewis, who directed and starred in the film as clown Helmut Doork, donated five hours of footage to the US Library of Congress in 2015, adding a stipulation that it not be made available until June 2024. </p><p id="el-17-OOAFYHIV6RFHPG5KNF6OAFFGW4">The footage, which has been made available to scholars, was screened last August for <i>The New Republic</i> journalist Benjamin Charles Germain Lee, who reported that the footage was fragmentary and does not constitute a complete film, leading the industry to conclude that the full film did not exist.</p><h2 id="el-18-MITN4F2L5REQ3JS7QLNNV3PABI"><b>Why the film was never released</b></h2><p id="el-19-S2OYFSEKX5BOVGW2SGJI2FOUKU">While there were myriad alleged issues during the shoot itself, problems reportedly arose between Lewis and producer Nat Wachsberger once filming stopped, which is considered the main catalyst for the film's shelving.</p><p id="el-20-MSVX4YUHK5FY7BSCX2CQK25UKI">Lewis was reportedly unsatisfied with the film’s financing and announced that Wachsberger did not fulfil his financial obligations. Hearing this, Wachsberger threatened to sue Lewis for breach of contract, which resulted in a fallout between the two that caused Lewis to leave with a rough cut of the film, according to a 2018 feature in <i>The New York Times.</i></p><p id="el-21-N7PHIBCFE5D77FCB3E74WKAFVI">Lewis had mixed feelings about the film, showing fragments of his footage to close friends. However, in his 1982 autobiography, Lewis said “the picture must be seen”.</p><div id="el-22-XG4SDP7YJZGFJJ3L75JCPD7G5E"><figure><picture><source width="800" height="1200" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=800&amp;height=1200"><source width="600" height="900" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=600&amp;height=900"><source width="400" height="600" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=400&amp;height=600"><img _id="XG4SDP7YJZGFJJ3L75JCPD7G5E" type="image" originalwidth="2048" originalheight="3072" alt="No complete copy of The Day The Clown Cried has ever been confirmed to exist until now. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=400&amp;height=600" width="400" height="600"></picture><div><figcaption>No complete copy of The Day The Clown Cried has ever been confirmed to exist until now. AFP</figcaption></div></figure></div><p id="el-23-OVBRR24ZMBGJRPPTNNZU3LT42A">After watching it, <a href="https://www.thenationalnews.com/lifestyle/fashion/2021/10/04/the-simpsons-make-their-fashion-week-debut-for-balenciaga-in-paris/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/lifestyle/fashion/2021/10/04/the-simpsons-make-their-fashion-week-debut-for-balenciaga-in-paris/"><i>The Simpsons</i></a> voice actor Harry Shearer said it was “a perfect object”, adding: “This movie is so drastically wrong, its pathos and its comedy are so wildly misplaced, that you could not, in your fantasy of what it might be like, improve on what it really is.”</p><p id="el-24-WUNXMZT2DJA5FJATYEOXNWFXIM">In an interview with <i>The New York Times</i> in 2018, Chris Lewis, the comedian's son, said: “It was something that was very close to his heart.”</p><p id="el-25-4F5RPU7YZFBLJOLZNVFPSA56VY">At other times, however, Lewis denounced the film. In 2013, footage of him surfaced on YouTube in which he stated: “It was bad, and it was bad because I lost the magic. No one will ever see it, because I'm embarrassed at the poor work.”</p><h2 id="el-26-ME2KIY2R4ZHJPFY7G33OKZTQBM"><b>The history of lost films</b></h2><p id="el-27-ZBOL6RP5OJD3FK44FZKBL72GLE"><i>The Day the Crown Cried</i> is an example of one of many films that were once thought lost or not fit for public screening.</p><p id="el-28-APHSELV6IRGRRDFZ5L2ZJ4WZX4">Similar films include 1976’s <i>Chess of the Wind </i>by Iranian director <a href="https://www.thenationalnews.com/arts-culture/film/the-7-middle-eastern-films-to-see-at-the-london-film-festival-1.1078995" target="_blank" rel="">Mohammad Reza Aslani.</a></p><p id="el-29-GQ27P642NBDRHNGGYANXB6GKE4">Until it was rediscovered in 2020, the film could only be watched on low-quality VHS tapes. Since then, it has been restored and screened around the world.</p><p id="el-30-KDOD53KPJND4HDABN7NCEBPXOU">One of the best-known lost films is <i>The Passion of Joan of Arc</i> from 1928. After being lost for years, a copy was found in a Norwegian hospital in the 1980s. The film is now considered one of the most important historical film artefacts.</p><div id="el-31-U6C2G5A4HZG7FBM3YI73DKVZ6A"><figure><picture><source width="800" height="1064" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=800&amp;height=1064"><source width="600" height="798" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=600&amp;height=798"><source width="400" height="532" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=400&amp;height=532"><img _id="U6C2G5A4HZG7FBM3YI73DKVZ6A" type="image" originalwidth="3120" originalheight="4146" alt="The Day The Clown Cried depicts a clown entertaining children during the holocaust. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=400&amp;height=532" width="400" height="532"></picture><div><figcaption>The Day The Clown Cried depicts a clown entertaining children during the holocaust. AFP</figcaption></div></figure></div><p id="el-32-EBP67AN5B5G2BGXRNZMD7JXHZU"><i>London After Midnight</i>, a 1927 horror film directed by Tod Browning starring Lon Chaney, is still a veritable white whale for fans after the last-known copy was destroyed in the 1965 MGM vault fire.</p><p id="el-33-7USMYYXBQZCJZPRD3RLG3SWOJM">Other films that have not yet screened because of filmmaker stipulations include <i>100 Years</i> starring <a href="https://www.thenationalnews.com/arts-culture/film/the-many-faces-of-the-talented-mr-malkovich-1.762364" target="_blank" rel="">John Malkovich</a>. The short film is from 2015 but has been placed in time-locked safes that won’t open until 2115, 100 years after the film was made.</p><p id="el-34-S3T4RD37A5F5FOWZEANBINWKHY">Several recently produced films are now <a href="https://www.thenationalnews.com/arts-culture/2024/08/05/fantastic-four-abandoned-hollywood-films/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/2024/08/05/fantastic-four-abandoned-hollywood-films/">considered lost media,</a> including 2022's <i>Batgirl</i>, directed by <a href="https://www.thenationalnews.com/arts-culture/film/who-are-adil-el-arbi-and-bilall-fallah-moroccan-belgian-directors-behind-the-new-batgirl-film-1.1226356" target="_blank" rel="">Adil El Arbi and Bilall Fallah</a>. The superhero film stars Leslie Grace as <a href="https://www.thenationalnews.com/arts-culture/television/2022/08/04/why-was-the-90-million-batgirl-killed-and-could-hbo-max-be-next/" target="_blank" rel="">Batgirl</a> and also includes J K Simmons, Brendan Fraser and Michael Keaton. </p><p id="el-35-4EAHPEUEANBW5JH6D7RZ7BDGIM">Warner Bros Discovery announced in August 2022 that it would not be released due to cost-cutting measures and a strategy shift towards theatrical releases.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Surprisingly Fast AI-Generated Kernels We Didn't Mean to Publish (Yet) (319 pts)]]></title>
            <link>https://crfm.stanford.edu/2025/05/28/fast-kernels.html</link>
            <guid>44139454</guid>
            <pubDate>Fri, 30 May 2025 20:03:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html">https://crfm.stanford.edu/2025/05/28/fast-kernels.html</a>, See on <a href="https://news.ycombinator.com/item?id=44139454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    


<div>
    <p><a href="https://www.stanford.edu/" target="_blank">
            <img src="https://crfm.stanford.edu/static/img/header/stanford-white.png">
        </a>
    </p>
</div>

<header>
    <nav>
        <div>
            <div>
                <p><a href="https://crfm.stanford.edu/">
                    <img src="https://crfm.stanford.edu/static/img/header/crfm-rgb.png">
                </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://hai.stanford.edu/" target="_blank">
                    <img src="https://crfm.stanford.edu/static/img/header/hai.png">
                </a></p>
            </div>
            <div>
                <ul>
                    <li>
                        <a href="https://crfm.stanford.edu/people.html">People</a>
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/report.html">Report</a>
                    </li>
                    <li id="dropdownContainer">
                        <a id="dropdownButton">Research</a>
                        
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/policy.html">Policy</a>
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/blog.html">Blog</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    
    
</header>

		<div>
				

  <h2>Surprisingly Fast AI-Generated Kernels We Didn’t Mean to Publish (Yet)</h2>
  
  
  
  
  

  

  <hr>

  

  <div>
    <h2 id="tldr">TL;DR</h2>

<p>We have some very fast AI-generated kernels in pure CUDA-C without using libraries and DSLs such as CUTLASS and Triton. They are performing close to or in some cases even beating the standard expert-optimized production kernels shipped in PyTorch. Some of our highlighted results:</p>

<ul>
  <li><strong>Matmul (FP32): 101.3%</strong> performance of FP32 torch.matmul; problem size: 4096x4096 square matrices</li>
  <li><strong>Conv2D: 179.9%</strong> performance of FP32 torch.nn.Conv2D; problem size: (100, 3, 224, 224) input tensor, conv(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)</li>
  <li><strong>Softmax: 111.8%</strong> performance of FP32 torch.softmax; problem size: (4096, 65536) input tensor</li>
  <li><strong>LayerNorm: 484.4%</strong> performance of FP32 torch.nn.LayerNorm; problem size: (16, 64, 256, 256) input tensor</li>
  <li><strong>Conv2D + ReLU + MaxPool: 290.1%</strong> performance of FP32 torch reference, 189.0% performance of FP32 torch.compile() reference; problem size: (100, 3, 224, 224) input tensor, conv(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2), maxpool(kernel_size=3, stride=2)</li>
</ul>

<p>(Our results are benchmarked on an Nvidia L40S GPU, and % performance is defined as reference time divided by generated kernel time)</p>

<p><img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/untiled.png" width="60%"><br>
<small><em>“Untiled” by DALL·E (2025). (Digital pigment on virtual canvas)<br>From the MMA collection</em></small></p>

<h2 id="intro">Intro</h2>

<p>We started with the goal of generating synthetic data to train better kernel generation models. Somewhere along the way the unexpected happened: the test-time only synthetic data generation itself started producing <em>really</em> good kernels beating or performing close to human expert optimized PyTorch baselines, utilizing advanced optimizations and hardware features, which were previously thought to be challenging. As a result, we decided to write this blog post early and share our findings. The point of this blog post isn’t about a novel methodology; in fact, our synthetic data generation design is simple, and what’s surprising is that it is already showing promise.</p>

<p>In this post, we’re sharing the method, five optimized kernels (4 foundational ML operators + 1 fused kernel of an AlexNet block), an example optimization trajectory, and some takeaways and thoughts on what this might mean for performant kernel generation. Consider this a first step in what’s next.</p>

<h2 id="method">Method</h2>

<p>We’re using the <a href="https://arxiv.org/abs/2502.10517">KernelBench</a> (a benchmark for AI based kernel generation that we released in December 2024) task setup: given torch code, the LLM writes custom kernels to replace the torch operators with the goal of getting a speedup. Consistent with the original KernelBench design, the reference code is in the default FP32, and given a tolerance threshold (1e-02), using lower precision solutions is valid. In addition, each problem in KernelBench has specific sizes since there are many size-specific optimizations, so the benchmark tests for the fastest kernel for the specific problem size, not necessarily a generally fast kernel for any arbitrary problem size. We run both the torch reference code and the generated code, and test for correctness by checking the numerical equality of the two outputs over many random inputs.<br>
<img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/kernelbench_design.png" width="100%"></p>

<p>The most common way people scale test-time compute for this problem of optimizing kernels today is through sequential revision, a multi-turn loop where a model incrementally edits a kernel, checks for correctness and performance, then tries again based on the result, either fixing the kernel or try to improve its performance. This loop is intuitive and easy to implement. The model fixes broken kernels, tweaks working ones, and gradually climbs toward something faster.</p>

<p>The main limitation of this approach is the lack of optimization idea diversity. Sequential loops often fall into local minima, revisiting the same classes of transformations or endlessly refining unpromising trajectories. The result is inefficient use of test-time compute and little pressure on the model to generate fundamentally new optimization ideas.</p>

<p>We introduced two key changes to address this:</p>

<ol>
  <li>Reasoning in natural language about optimization ideas: rather than directly generating new kernels in each step, we generate optimization ideas in natural language conditioned on previously attempted ideas, and realize those ideas into new code variants.</li>
  <li>Branching at each optimization step: instead of refining a single candidate per step, we fan out such that each idea spawns multiple implementations, and the highest-performing kernels are used to seed the next round (we also keep a bank of good existing kernels for seeding). This unlocks massive parallelism allowing us to explore radically different directions at each turn, rather than getting stuck in a narrow optimization path.</li>
</ol>

<p><img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/search.png" width="100%"></p>

<p>The result is a test-time loop that looks less like “chat with a compiler” in the case of sequential revision, and more like structured exploratory search, guided by explicit optimization hypotheses and aggressively parallel evaluation.</p>

<p>We ran 10 problems from KernelBench level 1 (and modified the problem sizes to make sure that kernel launch overhead is negligible compared to the overall runtime of the problem). We ran 5 rounds with the OpenAI o3 and Gemini 2.5 Pro models. The plot below shows the distribution of rounds in which the best-performing kernel was first found. Most of the best results emerge in later rounds (out of a total of 5 rounds), with the majority coming in round 4 or 5.
<img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/rounds.png" width="100%"></p>

<p>As we scaled up our search, we also found that many high-performing kernels clustered into a few recurring optimization strategies, which also aligns with our experience of writing kernels by hand. The main optimization categories are summarized below:</p>

<ul>
  <li><strong>Memory Access Optimization:</strong> improving the efficiency of data movement between different memory hierarchies (global memory, shared memory, registers) and ensuring data is accessed in a way that maximizes bandwidth and minimizes conflicts.</li>
  <li><strong>Asynchronous Operations &amp; Latency Hiding:</strong> hide the latency of slow operations (like global memory access) by overlapping them with computation or other memory transfers</li>
  <li><strong>Data Type &amp; Precision Optimization:</strong> using lower-precision data types (like FP16 or BF16) where possible to reduce memory bandwidth requirements, increase cache effectiveness, and potentially leverage specialized hardware units.</li>
  <li><strong>Compute &amp; Instruction Optimization</strong>: making the arithmetic computations themselves more efficient, reducing instruction count, or leveraging specialized hardware instructions</li>
  <li><strong>Parallelism &amp; Occupancy Enhancement</strong>: maximize the number of active warps on the Streaming Multiprocessors (SMs) to better hide latencies and improve overall throughput</li>
  <li><strong>Control Flow &amp; Loop Optimization</strong>: reducing the overhead associated with loops, branches, and indexing calculations</li>
</ul>

<h2 id="an-example-kernel-optimization-trajectory">An Example Kernel Optimization Trajectory</h2>

<p>Here we show an example optimization trajectory of auto-generated ideas for Conv2D, with torch reference baseline time of <strong>1.41 ms</strong></p>

<p><strong>Round 0: 7.02 ms, 20.1% of reference</strong><br>
Idea: Given the pytorch code, replace the operation with a CUDA Kernel</p>

<p><strong>Round 1: 7.54 ms, 18.8% of reference</strong><br>
Idea: Exploit the read-only cache by loading invariant tensors with __ldg.</p>

<p><strong>Round 2: 3.46 ms, 41.0% of reference</strong><br>
Idea: Convert the convolution to an FP16 Tensor-Core GEMM. <em>[author comment: this is an algorithmic optimization converting a convolution to an implicit GEMM, which is important for running convolutions efficiently on Tensor Cores]</em></p>

<p><strong>Round 3: 3.67 ms, 38.7% of reference</strong><br>
Idea: Double-buffer cp.async pipeline that overlaps global-memory loads with Tensor-Core compute.</p>

<p><strong>Round 4: 3.46 ms, 41.0% of reference</strong><br>
Idea: Given the pytorch code, replace the operation with a CUDA Kernel using implicit matmul. The given GEMM kernel could be helpful.<br>
<em>[author comment: since we know that the optimization involves using GEMM, we seeded the beginning of this round with an existing good GEMM kernel that we generated previously, and this idea is written manually]</em></p>

<p><strong>Round 5: 1.91 ms, 74.9% of reference</strong><br>
Idea: Precompute and reuse `k_idx`-decomposed kernel/input indices in shared memory within each K-tile loop to avoid redundant arithmetic.</p>

<p><strong>Round 6: 1.37 ms, 103.6% of reference</strong><br>
Idea: Precomputing and caching N-dimension GEMM indices in shared memory to reduce redundant arithmetic within the input data loading loop.</p>

<p><strong>Round 7: 1.38 ms, 102.9% of reference</strong><br>
Idea: Parallelize CUDA kernel output by using dedicated per-warp shared memory buffers to eliminate warp-wise serialization during writes to global memory.</p>

<p><strong>Round 8: 1.37 ms, 103.6% of reference</strong><br>
Idea: Precompute and cache base input coordinates in shared memory to reduce redundant arithmetic during input data loading.</p>

<p><strong>Round 9: 1.36 ms, 105.1% of reference</strong><br>
Idea: Software-pipeline B-fragment loading to overlap the next B-tile’s shared memory reads with the current B-tile’s WMMA computations.</p>

<p><strong>Round 10: 1.07 ms, 133.6% of reference</strong><br>
Idea: Reuse precomputed N-dimension GEMM decomposition from shared memory for output address calculation, avoiding redundant and costly division/modulo operations.</p>

<p><strong>Round 11: 1.21 ms, 117.4% of reference</strong><br>
Idea: Remove `hi/lo` decomposition in `half` WMMA operations, relying on standard FP16 accumulation to improve performance if the resulting accuracy is acceptable.</p>

<p><strong>Round 12: 1.01 ms, 141.2% of reference</strong><br>
Idea: Overlap K-loop global memory loads of `Asub` (weights) and `Bsub` (inputs) with MMA computation using double buffering, enabled by calculating K-dimension indices on-the-fly within the load stage of the pipeline.</p>

<p><strong>Round 13: 0.795 ms, 179.9% of reference</strong><br>
Idea: Implement vectorized shared memory writes for loading `Asub_pipe` and `Bsub_pipe` by using wider data types like `half2`</p>

<p><strong>Final Code Sample</strong><br>
The final code sample for the Conv2D kernel is included in the appendix. It uses advanced CUDA techniques that we find challenging to write ourselves!
We also have more example kernels in this <a href="https://github.com/ScalingIntelligence/good-kernels">Github repo</a></p>

<h2 id="takeaways">Takeaways</h2>

<p>Our method echoes a growing theme in AI research: combining strong reasoning with parallel exploration of multiple hypotheses leads to improvements. As some recent work (<a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf">AlphaEvolve</a>, <a href="https://x.com/GoogleDeepMind/status/1924881598102839373">Gemini 2.5 Pro Deep Think</a>) highlight, you might not always need massive retraining — sometimes, clever search and branching strategies can unlock scientific innovation and tackle complex problems, and there might be more gains through extensive searching with verifiers. <br>
However, this doesn’t mean we shouldn’t do further training. On the contrary, our approach also helps generate better synthetic data to improve future model training (this requires more problem instances). So, it’s both a powerful test-time scaling method and a step toward smarter, more data-efficient model development.</p>

<p>Finally, what we’ve demonstrated here is just an early sign of life. The optimization quality looks promising (it’s using many advanced strategies), but there’s plenty of room to improve, such as the generation of better optimization ideas, high quality resulting code, as well as applying this to increasingly complicated kernels. Two concrete examples that we are still actively working on improving are:</p>

<ul>
  <li>FP16 Matmul: 52% performance of torch.matmul</li>
  <li>FP16 Flash Attention: 9% performance of torch.nn.functional.scaled_dot_product_attention</li>
</ul>

<p>FP32 is less common in modern ML workloads and often less optimized on recent hardware compared to FP16 or BF16, which may partly explain why it’s easier to achieve performance gains over PyTorch with FP32 kernels.</p>

<p>Despite the current limitations, we’re optimistic. At the time of KernelBench, we couldn’t even generate functional versions of these two kernels above, and through searching we’ve been steadily increasing the performance of flash attention from &lt;1%, and note that we are working with a quite limited search budget here (around 3 million input tokens + 4 million output tokens in total). The progress since then gives us confidence in the potential for continual improvement, and we are excited to keep pushing the frontier of AI to create increasingly better kernels towards the eventual goal of self-improving AI systems.</p>

<h2 id="thanks">Thanks</h2>

<p>Christopher Rinard, Saman Amarasinghe, and Allen Nie for the helpful discussions; Standard Kernel Co. and Prime Intellect for supporting this work.</p>

<h2 id="appendix-fast-conv2d-kernel">Appendix: Fast Conv2D Kernel</h2>
<div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>
<span>import</span> <span>torch.nn.functional</span> <span>as</span> <span>F</span>
<span>from</span> <span>torch.utils.cpp_extension</span> <span>import</span> <span>load_inline</span>

<span>conv2d_implicit_gemm_cuda_source</span> <span>=</span> <span>r</span><span>"""
#include &lt;torch/extension.h&gt;
#include &lt;ATen/cuda/CUDAContext.h&gt; // For at::cuda::getCurrentCUDAStream()
#include &lt;mma.h&gt;
#include &lt;cuda_fp16.h&gt;

using namespace nvcuda;

// WMMA tile dimensions
#define WMMA_M 16
#define WMMA_N 16
#define WMMA_K 16

// Skew padding for shared memory to avoid bank conflicts
#define SKEW_HALF 8 // 8 half elements (16 bytes)

// CUDA built-in warpSize is 32 for supported architectures (sm_70+)
// This constant is used for host-side configuration (e.g. blockDim)
#define CUDA_WARP_SIZE_CONST 32 

// Threadblock configuration
#define WARPS_PER_BLOCK 8
// THREADS_PER_BLOCK must be evaluatable by host compiler for blockDim configuration
#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * CUDA_WARP_SIZE_CONST) 

// Macro-tile dimensions computed by a threadblock
// BLOCK_M_TILES_WMMA * WMMA_M = output channels processed by a block
// BLOCK_N_TILES_WMMA * WMMA_N = output spatial elements processed by a block
#define BLOCK_M_TILES_WMMA 8
#define BLOCK_N_TILES_WMMA 8

#define TILE_M_PER_BLOCK (BLOCK_M_TILES_WMMA * WMMA_M) // e.g., 8 * 16 = 128 (for C_out dimension)
#define TILE_N_PER_BLOCK (BLOCK_N_TILES_WMMA * WMMA_N) // e.g., 8 * 16 = 128 (for N_batch * H_out * W_out dimension)

// Vector size for shared memory writes (half2)
#define VECTOR_SIZE_H2 2

// Struct to hold precomputed N-dimension GEMM indices
struct NDecomposed {
    int ow_eff;
    int oh_eff;
    int n_batch_idx;
    bool isValidPixel; // True if this pixel_idx is within N_gemm bounds
    int h_in_base; 
    int w_in_base; 
};

__global__ void conv2d_implicit_gemm_wmma_kernel(
    const float* __restrict__ input_ptr,    // Input: (N, Cin, Hin, Win)
    const float* __restrict__ weight_ptr,   // Weights: (Cout, Cin, Kh, Kw)
    const float* __restrict__ bias_ptr,     // Bias: (Cout) or nullptr
    float* __restrict__ output_ptr,         // Output: (N, Cout, Hout, Wout)
    const int N_batch, const int C_in, const int H_in, const int W_in,
    const int C_out, const int K_h, const int K_w,
    const int stride_h, const int stride_w,
    const int pad_h, const int pad_w,
    const int H_out, const int W_out,
    const int M_gemm, // C_out
    const int N_gemm, // N_batch * H_out * W_out
    const int K_gemm  // C_in * K_h * K_w
) {
    // Thread identification
    const int warp_id = threadIdx.x / warpSize;        // 0 .. WARPS_PER_BLOCK-1
    const int lane_id = threadIdx.x % warpSize;        // 0 .. 31 (or warpSize-1)

    // Top-left corner of the macro-tile this block is responsible for in GEMM terms
    const int block_row_gemm_start = TILE_M_PER_BLOCK * blockIdx.y;
    const int block_col_gemm_start = TILE_N_PER_BLOCK * blockIdx.x;

    // Shared memory for tiles of A (weights) and B (input/im2col) - Double Buffered for K-loop pipelining
    __shared__ half Asub_pipe[2][TILE_M_PER_BLOCK][WMMA_K + SKEW_HALF];
    __shared__ half Bsub_pipe[2][TILE_N_PER_BLOCK][WMMA_K + SKEW_HALF];

    // Shared memory for precomputed N-indices
    __shared__ NDecomposed n_params_sh[TILE_N_PER_BLOCK];

    // Shared memory for output stage (per-warp buffers)
    __shared__ float C_shmem_output_buffers[WARPS_PER_BLOCK][WMMA_M][WMMA_N];

    // Accumulator fragments per warp.
    wmma::fragment&lt;wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float&gt; acc_frag[BLOCK_N_TILES_WMMA];
    #pragma unroll
    for (int i = 0; i &lt; BLOCK_N_TILES_WMMA; ++i) {
        wmma::fill_fragment(acc_frag[i], 0.0f);
    }

    // Populate n_params_sh once at the beginning of the kernel
    if (threadIdx.x &lt; TILE_N_PER_BLOCK) {
        int r_b_tile_idx = threadIdx.x; 
        int current_pixel_idx = block_col_gemm_start + r_b_tile_idx;

        if (current_pixel_idx &lt; N_gemm) {
            n_params_sh[r_b_tile_idx].ow_eff = current_pixel_idx % W_out;
            int temp_div_wout = current_pixel_idx / W_out;
            n_params_sh[r_b_tile_idx].oh_eff = temp_div_wout % H_out;
            n_params_sh[r_b_tile_idx].n_batch_idx = temp_div_wout / H_out;
            n_params_sh[r_b_tile_idx].isValidPixel = true;

            n_params_sh[r_b_tile_idx].h_in_base = n_params_sh[r_b_tile_idx].oh_eff * stride_h - pad_h;
            n_params_sh[r_b_tile_idx].w_in_base = n_params_sh[r_b_tile_idx].ow_eff * stride_w - pad_w;
        } else {
            n_params_sh[r_b_tile_idx].isValidPixel = false;
            n_params_sh[r_b_tile_idx].ow_eff = 0; 
            n_params_sh[r_b_tile_idx].oh_eff = 0;
            n_params_sh[r_b_tile_idx].n_batch_idx = 0;
            n_params_sh[r_b_tile_idx].h_in_base = 0; 
            n_params_sh[r_b_tile_idx].w_in_base = 0;
        }
    }
    __syncthreads();

    // Constants for vectorized shared memory loading
    // Number of half2 elements along K-dim for a shared memory tile row
    const int NUM_H2_ELEMENTS_IN_K_DIM = WMMA_K / VECTOR_SIZE_H2;
    // Number of thread groups, where each group has NUM_H2_ELEMENTS_IN_K_DIM threads.
    // Each group is responsible for loading the K-dimension for one M-row (for A) or N-row (for B) at a time,
    // iterating over M-rows or N-rows with this step size.
    const int NUM_ROW_PROCESSING_GROUPS = THREADS_PER_BLOCK / NUM_H2_ELEMENTS_IN_K_DIM;


    // --- K-Loop Pipelining ---
    int num_k_tiles = (K_gemm + WMMA_K - 1) / WMMA_K;
    
    // --- Prologue: Load first k-tile (k_tile_iter = 0) into pipe_idx = 0 ---
    if (num_k_tiles &gt; 0) { 
        int k_tile_start_prologue = 0; 
        int current_pipe_idx_prologue = 0; 

        // Load Asub_pipe[0] for k_tile_iter = 0
        {
            // This thread is responsible for the 'h2_idx_in_k_dim_A'-th half2 element
            // in the K-dimension of the shared memory tile.
            int h2_idx_in_k_dim_A = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
            // Starting 'half' index in shared memory for this half2 write.
            int shmem_k_start_for_h2_A = h2_idx_in_k_dim_A * VECTOR_SIZE_H2;

            // Global k-indices for the two half elements.
            int k_global_A_0 = k_tile_start_prologue + shmem_k_start_for_h2_A;
            int k_global_A_1 = k_tile_start_prologue + shmem_k_start_for_h2_A + 1;

            // Decompose k_global_A_0
            int kw_eff_reg_A_0 = 0, kh_eff_reg_A_0 = 0, ic_eff_reg_A_0 = 0;
            bool is_valid_k_A_0 = (k_global_A_0 &lt; K_gemm);
            if (is_valid_k_A_0) {
                kw_eff_reg_A_0 = k_global_A_0 % K_w;
                int temp_div_kw_A_0 = k_global_A_0 / K_w;
                kh_eff_reg_A_0 = temp_div_kw_A_0 % K_h;
                ic_eff_reg_A_0 = temp_div_kw_A_0 / K_h;
            }

            // Decompose k_global_A_1
            int kw_eff_reg_A_1 = 0, kh_eff_reg_A_1 = 0, ic_eff_reg_A_1 = 0;
            bool is_valid_k_A_1 = (k_global_A_1 &lt; K_gemm);
            if (is_valid_k_A_1) {
                kw_eff_reg_A_1 = k_global_A_1 % K_w;
                int temp_div_kw_A_1 = k_global_A_1 / K_w;
                kh_eff_reg_A_1 = temp_div_kw_A_1 % K_h;
                ic_eff_reg_A_1 = temp_div_kw_A_1 / K_h;
            }
            
            // This thread belongs to 'm_row_group_id_A'-th group of threads.
            // This group iterates over M-rows of the Asub_pipe tile.
            int m_row_group_id_A = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
            for (int r_a_tile_base = m_row_group_id_A; r_a_tile_base &lt; TILE_M_PER_BLOCK; r_a_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                int oc_idx = block_row_gemm_start + r_a_tile_base;
                float weight_val_0 = 0.0f;
                if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_0) {
                    weight_val_0 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                              ic_eff_reg_A_0 * K_h * K_w +
                                              kh_eff_reg_A_0 * K_w +
                                              kw_eff_reg_A_0];
                }
                float weight_val_1 = 0.0f;
                if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_1) {
                    weight_val_1 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                              ic_eff_reg_A_1 * K_h * K_w +
                                              kh_eff_reg_A_1 * K_w +
                                              kw_eff_reg_A_1];
                }
                half2* smem_ptr_h2_A = reinterpret_cast&lt;half2*&gt;(
                    &amp;Asub_pipe[current_pipe_idx_prologue][r_a_tile_base][shmem_k_start_for_h2_A]
                );
                *smem_ptr_h2_A = make_half2(__float2half(weight_val_0), __float2half(weight_val_1));
            }
        }

        // Load Bsub_pipe[0] for k_tile_iter = 0
        {
            int h2_idx_in_k_dim_B = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
            int shmem_k_start_for_h2_B = h2_idx_in_k_dim_B * VECTOR_SIZE_H2;

            int k_global_B_0 = k_tile_start_prologue + shmem_k_start_for_h2_B;
            int k_global_B_1 = k_tile_start_prologue + shmem_k_start_for_h2_B + 1;

            int kw_eff_reg_B_0 = 0, kh_eff_reg_B_0 = 0, ic_eff_reg_B_0 = 0;
            bool is_valid_k_B_0 = (k_global_B_0 &lt; K_gemm);
            if (is_valid_k_B_0) {
                kw_eff_reg_B_0 = k_global_B_0 % K_w;
                int temp_div_kw_B_0 = k_global_B_0 / K_w;
                kh_eff_reg_B_0 = temp_div_kw_B_0 % K_h;
                ic_eff_reg_B_0 = temp_div_kw_B_0 / K_h;
            }

            int kw_eff_reg_B_1 = 0, kh_eff_reg_B_1 = 0, ic_eff_reg_B_1 = 0;
            bool is_valid_k_B_1 = (k_global_B_1 &lt; K_gemm);
            if (is_valid_k_B_1) {
                kw_eff_reg_B_1 = k_global_B_1 % K_w;
                int temp_div_kw_B_1 = k_global_B_1 / K_w;
                kh_eff_reg_B_1 = temp_div_kw_B_1 % K_h;
                ic_eff_reg_B_1 = temp_div_kw_B_1 / K_h;
            }

            int n_row_group_id_B = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
            for (int r_b_tile_base = n_row_group_id_B; r_b_tile_base &lt; TILE_N_PER_BLOCK; r_b_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                float input_val_0 = 0.0f;
                if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_0) {
                    const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                    int h_in_eff_0 = current_n_params.h_in_base + kh_eff_reg_B_0;
                    int w_in_eff_0 = current_n_params.w_in_base + kw_eff_reg_B_0;
                    if (h_in_eff_0 &gt;= 0 &amp;&amp; h_in_eff_0 &lt; H_in &amp;&amp; w_in_eff_0 &gt;= 0 &amp;&amp; w_in_eff_0 &lt; W_in) {
                        input_val_0 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                              ic_eff_reg_B_0 * H_in * W_in +
                                              h_in_eff_0 * W_in +
                                              w_in_eff_0];
                    }
                }
                float input_val_1 = 0.0f;
                 if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_1) {
                    const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                    int h_in_eff_1 = current_n_params.h_in_base + kh_eff_reg_B_1;
                    int w_in_eff_1 = current_n_params.w_in_base + kw_eff_reg_B_1;
                     if (h_in_eff_1 &gt;= 0 &amp;&amp; h_in_eff_1 &lt; H_in &amp;&amp; w_in_eff_1 &gt;= 0 &amp;&amp; w_in_eff_1 &lt; W_in) {
                        input_val_1 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                              ic_eff_reg_B_1 * H_in * W_in +
                                              h_in_eff_1 * W_in +
                                              w_in_eff_1];
                    }
                }
                half2* smem_ptr_h2_B = reinterpret_cast&lt;half2*&gt;(
                    &amp;Bsub_pipe[current_pipe_idx_prologue][r_b_tile_base][shmem_k_start_for_h2_B]
                );
                *smem_ptr_h2_B = make_half2(__float2half(input_val_0), __float2half(input_val_1));
            }
        }
    }


    // Loop over the K_gemm dimension in tiles of WMMA_K
    for (int k_tile_iter = 0; k_tile_iter &lt; num_k_tiles; ++k_tile_iter) {
        __syncthreads(); // Sync point for pipelining

        int compute_pipe_idx = k_tile_iter % 2;
        int load_pipe_idx = (k_tile_iter + 1) % 2;

        // --- Load Stage for next k-tile (k_tile_iter + 1) into load_pipe_idx ---
        int k_tile_start_for_load = (k_tile_iter + 1) * WMMA_K;
        if (k_tile_start_for_load &lt; K_gemm) { 
            // Load Asub_pipe[load_pipe_idx]
            { 
                int h2_idx_in_k_dim_A = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
                int shmem_k_start_for_h2_A = h2_idx_in_k_dim_A * VECTOR_SIZE_H2;

                int k_global_A_0 = k_tile_start_for_load + shmem_k_start_for_h2_A;
                int k_global_A_1 = k_tile_start_for_load + shmem_k_start_for_h2_A + 1;

                int kw_eff_reg_A_0 = 0, kh_eff_reg_A_0 = 0, ic_eff_reg_A_0 = 0;
                bool is_valid_k_A_0 = (k_global_A_0 &lt; K_gemm);
                if (is_valid_k_A_0) {
                    kw_eff_reg_A_0 = k_global_A_0 % K_w;
                    int temp_div_kw_A_0 = k_global_A_0 / K_w;
                    kh_eff_reg_A_0 = temp_div_kw_A_0 % K_h;
                    ic_eff_reg_A_0 = temp_div_kw_A_0 / K_h;
                }

                int kw_eff_reg_A_1 = 0, kh_eff_reg_A_1 = 0, ic_eff_reg_A_1 = 0;
                bool is_valid_k_A_1 = (k_global_A_1 &lt; K_gemm);
                if (is_valid_k_A_1) {
                    kw_eff_reg_A_1 = k_global_A_1 % K_w;
                    int temp_div_kw_A_1 = k_global_A_1 / K_w;
                    kh_eff_reg_A_1 = temp_div_kw_A_1 % K_h;
                    ic_eff_reg_A_1 = temp_div_kw_A_1 / K_h;
                }
                
                int m_row_group_id_A = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
                for (int r_a_tile_base = m_row_group_id_A; r_a_tile_base &lt; TILE_M_PER_BLOCK; r_a_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                    int oc_idx = block_row_gemm_start + r_a_tile_base;
                    float weight_val_0 = 0.0f;
                    if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_0) {
                        weight_val_0 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                                  ic_eff_reg_A_0 * K_h * K_w +
                                                  kh_eff_reg_A_0 * K_w +
                                                  kw_eff_reg_A_0];
                    }
                    float weight_val_1 = 0.0f;
                    if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_1) {
                        weight_val_1 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                                  ic_eff_reg_A_1 * K_h * K_w +
                                                  kh_eff_reg_A_1 * K_w +
                                                  kw_eff_reg_A_1];
                    }
                    half2* smem_ptr_h2_A = reinterpret_cast&lt;half2*&gt;(
                        &amp;Asub_pipe[load_pipe_idx][r_a_tile_base][shmem_k_start_for_h2_A]
                    );
                    *smem_ptr_h2_A = make_half2(__float2half(weight_val_0), __float2half(weight_val_1));
                }
            } 

            // Load Bsub_pipe[load_pipe_idx]
            { 
                int h2_idx_in_k_dim_B = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
                int shmem_k_start_for_h2_B = h2_idx_in_k_dim_B * VECTOR_SIZE_H2;

                int k_global_B_0 = k_tile_start_for_load + shmem_k_start_for_h2_B;
                int k_global_B_1 = k_tile_start_for_load + shmem_k_start_for_h2_B + 1;

                int kw_eff_reg_B_0 = 0, kh_eff_reg_B_0 = 0, ic_eff_reg_B_0 = 0;
                bool is_valid_k_B_0 = (k_global_B_0 &lt; K_gemm);
                if (is_valid_k_B_0) {
                    kw_eff_reg_B_0 = k_global_B_0 % K_w;
                    int temp_div_kw_B_0 = k_global_B_0 / K_w;
                    kh_eff_reg_B_0 = temp_div_kw_B_0 % K_h;
                    ic_eff_reg_B_0 = temp_div_kw_B_0 / K_h;
                }

                int kw_eff_reg_B_1 = 0, kh_eff_reg_B_1 = 0, ic_eff_reg_B_1 = 0;
                bool is_valid_k_B_1 = (k_global_B_1 &lt; K_gemm);
                if (is_valid_k_B_1) {
                    kw_eff_reg_B_1 = k_global_B_1 % K_w;
                    int temp_div_kw_B_1 = k_global_B_1 / K_w;
                    kh_eff_reg_B_1 = temp_div_kw_B_1 % K_h;
                    ic_eff_reg_B_1 = temp_div_kw_B_1 / K_h;
                }

                int n_row_group_id_B = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
                for (int r_b_tile_base = n_row_group_id_B; r_b_tile_base &lt; TILE_N_PER_BLOCK; r_b_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                    float input_val_0 = 0.0f;
                    if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_0) {
                        const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                        int h_in_eff_0 = current_n_params.h_in_base + kh_eff_reg_B_0;
                        int w_in_eff_0 = current_n_params.w_in_base + kw_eff_reg_B_0;
                        if (h_in_eff_0 &gt;= 0 &amp;&amp; h_in_eff_0 &lt; H_in &amp;&amp; w_in_eff_0 &gt;= 0 &amp;&amp; w_in_eff_0 &lt; W_in) {
                            input_val_0 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                                  ic_eff_reg_B_0 * H_in * W_in +
                                                  h_in_eff_0 * W_in +
                                                  w_in_eff_0];
                        }
                    }
                    float input_val_1 = 0.0f;
                    if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_1) {
                        const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                        int h_in_eff_1 = current_n_params.h_in_base + kh_eff_reg_B_1;
                        int w_in_eff_1 = current_n_params.w_in_base + kw_eff_reg_B_1;
                        if (h_in_eff_1 &gt;= 0 &amp;&amp; h_in_eff_1 &lt; H_in &amp;&amp; w_in_eff_1 &gt;= 0 &amp;&amp; w_in_eff_1 &lt; W_in) {
                            input_val_1 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                                  ic_eff_reg_B_1 * H_in * W_in +
                                                  h_in_eff_1 * W_in +
                                                  w_in_eff_1];
                        }
                    }
                    half2* smem_ptr_h2_B = reinterpret_cast&lt;half2*&gt;(
                        &amp;Bsub_pipe[load_pipe_idx][r_b_tile_base][shmem_k_start_for_h2_B]
                    );
                    *smem_ptr_h2_B = make_half2(__float2half(input_val_0), __float2half(input_val_1));
                }
            } 
        }

        // --- Compute Stage for current k-tile (k_tile_iter) using compute_pipe_idx ---
        int a_row_start_in_tile = warp_id * WMMA_M; 

        wmma::fragment&lt;wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major&gt; a_frag;
        wmma::load_matrix_sync(a_frag, &amp;Asub_pipe[compute_pipe_idx][a_row_start_in_tile][0], WMMA_K + SKEW_HALF);

        wmma::fragment&lt;wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major&gt; b_frag_inner_pipe[2];

        if (BLOCK_N_TILES_WMMA &gt; 0) {
            int b_col_start_in_tile_current = 0 * WMMA_N; 
            wmma::load_matrix_sync(b_frag_inner_pipe[0], &amp;Bsub_pipe[compute_pipe_idx][b_col_start_in_tile_current][0], WMMA_K + SKEW_HALF);
        }
        
        int current_inner_pipe_idx = 0;

        #pragma unroll
        for (int n_tile = 0; n_tile &lt; BLOCK_N_TILES_WMMA; ++n_tile) {
            int next_inner_pipe_idx = 1 - current_inner_pipe_idx;

            if (n_tile &lt; BLOCK_N_TILES_WMMA - 1) {
                int b_col_start_in_tile_next = (n_tile + 1) * WMMA_N;
                wmma::load_matrix_sync(b_frag_inner_pipe[next_inner_pipe_idx], &amp;Bsub_pipe[compute_pipe_idx][b_col_start_in_tile_next][0], WMMA_K + SKEW_HALF);
            }

            wmma::mma_sync(acc_frag[n_tile], a_frag, b_frag_inner_pipe[current_inner_pipe_idx], acc_frag[n_tile]);
            
            current_inner_pipe_idx = next_inner_pipe_idx;
        }
    }
    __syncthreads(); 

    // Store results from accumulator fragments to global memory
    #pragma unroll
    for (int n_tile = 0; n_tile &lt; BLOCK_N_TILES_WMMA; ++n_tile) {
        wmma::store_matrix_sync(&amp;C_shmem_output_buffers[warp_id][0][0], acc_frag[n_tile], WMMA_N, wmma::mem_row_major);

        for (int elem_idx_in_frag = lane_id; elem_idx_in_frag &lt; WMMA_M * WMMA_N; elem_idx_in_frag += warpSize) {
            int r_frag = elem_idx_in_frag / WMMA_N;
            int c_frag = elem_idx_in_frag % WMMA_N;

            int oc_idx = block_row_gemm_start + (warp_id * WMMA_M) + r_frag;
            
            int offset_in_block_N_processing = (n_tile * WMMA_N) + c_frag;

            if (oc_idx &lt; C_out &amp;&amp; offset_in_block_N_processing &lt; TILE_N_PER_BLOCK &amp;&amp; 
                n_params_sh[offset_in_block_N_processing].isValidPixel) {
                const NDecomposed&amp; current_n_params = n_params_sh[offset_in_block_N_processing];
                int ow_eff = current_n_params.ow_eff;
                int oh_eff = current_n_params.oh_eff;
                int n_batch_idx = current_n_params.n_batch_idx;

                float val = C_shmem_output_buffers[warp_id][r_frag][c_frag];

                if (bias_ptr != nullptr) {
                    val += bias_ptr[oc_idx];
                }

                output_ptr[n_batch_idx * C_out * H_out * W_out +
                           oc_idx * H_out * W_out +
                           oh_eff * W_out +
                           ow_eff] = val;
            }
        }
    }
}


torch::Tensor conv2d_implicit_gemm_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int N_batch, int C_in, int H_in, int W_in,
    int C_out, int K_h, int K_w,
    int stride_h, int stride_w, int pad_h, int pad_w,
    int H_out, int W_out) {

    TORCH_CHECK(input.device().is_cuda(), "Input must be a CUDA tensor");
    TORCH_CHECK(weight.device().is_cuda(), "Weight must be a CUDA tensor");
    TORCH_CHECK(input.dtype() == torch::kFloat32, "Input must be float32");
    TORCH_CHECK(weight.dtype() == torch::kFloat32, "Weight must be float32");
    if (bias.defined()) {
        TORCH_CHECK(bias.device().is_cuda(), "Bias must be a CUDA tensor");
        TORCH_CHECK(bias.dtype() == torch::kFloat32, "Bias must be float32");
        TORCH_CHECK(bias.dim() == 1 &amp;&amp; bias.size(0) == C_out, "Bias has wrong shape");
    }

    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(weight.dim() == 4, "Weight must be 4D");
    TORCH_CHECK(input.size(0) == N_batch, "Input N_batch mismatch");
    TORCH_CHECK(input.size(1) == C_in, "Input C_in mismatch");
    TORCH_CHECK(input.size(2) == H_in, "Input H_in mismatch");
    TORCH_CHECK(input.size(3) == W_in, "Input W_in mismatch");
    TORCH_CHECK(weight.size(0) == C_out, "Weight C_out mismatch");
    TORCH_CHECK(weight.size(1) == C_in, "Weight C_in mismatch");
    TORCH_CHECK(weight.size(2) == K_h, "Weight K_h mismatch");
    TORCH_CHECK(weight.size(3) == K_w, "Weight K_w mismatch");

    auto output = torch::zeros({N_batch, C_out, H_out, W_out}, input.options());

    const int M_gemm = C_out;
    const int N_gemm = N_batch * H_out * W_out;
    const int K_gemm = C_in * K_h * K_w;

    if (M_gemm == 0 || N_gemm == 0) { 
        return output;
    }
    if (K_gemm == 0) { 
         if (bias.defined()) { 
            output = output + bias.reshape({1, C_out, 1, 1});
        }
        return output; 
    }

    dim3 block_dim(THREADS_PER_BLOCK);
    dim3 grid_dim(
        (N_gemm + TILE_N_PER_BLOCK - 1) / TILE_N_PER_BLOCK, 
        (M_gemm + TILE_M_PER_BLOCK - 1) / TILE_M_PER_BLOCK  
    );

    const float* bias_ptr_data = bias.defined() ? bias.data_ptr&lt;float&gt;() : nullptr;

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    conv2d_implicit_gemm_wmma_kernel&lt;&lt;&lt;grid_dim, block_dim, 0, stream&gt;&gt;&gt;(
        input.data_ptr&lt;float&gt;(),
        weight.data_ptr&lt;float&gt;(),
        bias_ptr_data,
        output.data_ptr&lt;float&gt;(),
        N_batch, C_in, H_in, W_in,
        C_out, K_h, K_w,
        stride_h, stride_w, pad_h, pad_w,
        H_out, W_out,
        M_gemm, N_gemm, K_gemm
    );
    
    AT_CUDA_CHECK(cudaGetLastError());

    return output;
}
"""</span>

<span>conv2d_implicit_gemm_cuda_declaration</span> <span>=</span> <span>r</span><span>"""
torch::Tensor conv2d_implicit_gemm_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int N_batch, int C_in, int H_in, int W_in,
    int C_out, int K_h, int K_w,
    int stride_h, int stride_w, int pad_h, int pad_w,
    int H_out, int W_out);
"""</span>

<span># JIT compile the CUDA kernel
</span><span>custom_conv2d_wmma_ops</span> <span>=</span> <span>load_inline</span><span>(</span>
    <span>name</span><span>=</span><span>"custom_conv2d_wmma_ops_optimized_k_pipe_vec_smem"</span><span>,</span> <span># Changed name to avoid collision
</span>    <span>cpp_sources</span><span>=</span><span>conv2d_implicit_gemm_cuda_declaration</span><span>,</span>
    <span>cuda_sources</span><span>=</span><span>conv2d_implicit_gemm_cuda_source</span><span>,</span>
    <span>functions</span><span>=</span><span>[</span><span>"conv2d_implicit_gemm_cuda"</span><span>],</span>
    <span>verbose</span><span>=</span><span>True</span><span>,</span> 
    <span>extra_cuda_cflags</span><span>=</span><span>[</span><span>"-arch=sm_70"</span><span>,</span> <span>"--use_fast_math"</span><span>,</span> <span>"-std=c++17"</span><span>]</span> 
<span>)</span>


<span>class</span> <span>ModelNew</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>num_classes</span><span>=</span><span>1000</span><span>):</span> <span># num_classes is part of original signature, kept for consistency
</span>        <span>super</span><span>(</span><span>ModelNew</span><span>,</span> <span>self</span><span>).</span><span>__init__</span><span>()</span>
        
        <span># Define Conv1 parameters (matching the original model)
</span>        <span>self</span><span>.</span><span>in_channels</span> <span>=</span> <span>3</span>
        <span>self</span><span>.</span><span>out_channels</span> <span>=</span> <span>96</span>
        <span>self</span><span>.</span><span>kernel_size_val</span> <span>=</span> <span>11</span> <span># Assuming square kernel
</span>        <span>self</span><span>.</span><span>stride_val</span> <span>=</span> <span>4</span>       <span># Assuming square stride
</span>        <span>self</span><span>.</span><span>padding_val</span> <span>=</span> <span>2</span>      <span># Assuming square padding
</span>
        <span># Create a temporary Conv2d layer to initialize weights and bias
</span>        <span>temp_conv</span> <span>=</span> <span>nn</span><span>.</span><span>Conv2d</span><span>(</span>
            <span>in_channels</span><span>=</span><span>self</span><span>.</span><span>in_channels</span><span>,</span> 
            <span>out_channels</span><span>=</span><span>self</span><span>.</span><span>out_channels</span><span>,</span> 
            <span>kernel_size</span><span>=</span><span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> 
            <span>stride</span><span>=</span><span>self</span><span>.</span><span>stride_val</span><span>,</span> 
            <span>padding</span><span>=</span><span>self</span><span>.</span><span>padding_val</span><span>,</span>
            <span>bias</span><span>=</span><span>True</span> <span># nn.Conv2d has bias=True by default
</span>        <span>)</span>
        <span>self</span><span>.</span><span>conv1_weight</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>temp_conv</span><span>.</span><span>weight</span><span>.</span><span>detach</span><span>().</span><span>clone</span><span>())</span>
        <span>if</span> <span>temp_conv</span><span>.</span><span>bias</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>self</span><span>.</span><span>conv1_bias</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>temp_conv</span><span>.</span><span>bias</span><span>.</span><span>detach</span><span>().</span><span>clone</span><span>())</span>
        <span>else</span><span>:</span>
            <span># Correctly register 'conv1_bias' as None if not present
</span>            <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>'conv1_bias'</span><span>,</span> <span>None</span><span>)</span> 


        <span>self</span><span>.</span><span>custom_conv_op</span> <span>=</span> <span>custom_conv2d_wmma_ops</span><span>.</span><span>conv2d_implicit_gemm_cuda</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>N_batch</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>0</span><span>)</span>
        <span># C_in_runtime = x.size(1) # Should match self.in_channels
</span>        <span>H_in</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>2</span><span>)</span>
        <span>W_in</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>3</span><span>)</span>

        <span># Calculate output dimensions
</span>        <span>H_out</span> <span>=</span> <span>(</span><span>H_in</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>padding_val</span> <span>-</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>)</span> <span>//</span> <span>self</span><span>.</span><span>stride_val</span> <span>+</span> <span>1</span>
        <span>W_out</span> <span>=</span> <span>(</span><span>W_in</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>padding_val</span> <span>-</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>)</span> <span>//</span> <span>self</span><span>.</span><span>stride_val</span> <span>+</span> <span>1</span>
        
        <span># Bias tensor handling: pass an undefined tensor if bias is None.
</span>        <span># The C++ TORCH_CHECK(bias.defined()) handles this by providing nullptr to kernel.
</span>        <span>bias_tensor</span> <span>=</span> <span>self</span><span>.</span><span>conv1_bias</span> <span>if</span> <span>self</span><span>.</span><span>conv1_bias</span> <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>torch</span><span>.</span><span>Tensor</span><span>()</span>


        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>custom_conv_op</span><span>(</span>
            <span>x</span><span>,</span> <span>self</span><span>.</span><span>conv1_weight</span><span>,</span> <span>bias_tensor</span><span>,</span>
            <span>N_batch</span><span>,</span> <span>self</span><span>.</span><span>in_channels</span><span>,</span> <span>H_in</span><span>,</span> <span>W_in</span><span>,</span>
            <span>self</span><span>.</span><span>out_channels</span><span>,</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> <span># K_h, K_w
</span>            <span>self</span><span>.</span><span>stride_val</span><span>,</span> <span>self</span><span>.</span><span>stride_val</span><span>,</span> <span># stride_h, stride_w
</span>            <span>self</span><span>.</span><span>padding_val</span><span>,</span> <span>self</span><span>.</span><span>padding_val</span><span>,</span> <span># pad_h, pad_w
</span>            <span>H_out</span><span>,</span> <span>W_out</span>
        <span>)</span>
        <span>return</span> <span>x</span>
</code></pre></div>

  </div>


			</div>
		






	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mary Meeker's first Trends report since 2019, focused on AI (160 pts)]]></title>
            <link>https://www.bondcap.com/reports/tai</link>
            <guid>44139403</guid>
            <pubDate>Fri, 30 May 2025 19:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bondcap.com/reports/tai">https://www.bondcap.com/reports/tai</a>, See on <a href="https://news.ycombinator.com/item?id=44139403">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cap: Lightweight, modern open-source CAPTCHA alternative using proof-of-work (143 pts)]]></title>
            <link>https://capjs.js.org/</link>
            <guid>44137867</guid>
            <pubDate>Fri, 30 May 2025 16:36:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://capjs.js.org/">https://capjs.js.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44137867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-9a6c75ad="" data-v-e07eaea7="" id="VPContent" data-v-d8b57b2d=""><!--[--><!--]--><div data-v-dd8814ff="" data-v-e07eaea7=""><div data-v-dd8814ff=""><!--[--><!--]--><!--[--><h2 data-v-dd8814ff=""><span data-v-dd8814ff="">Cap</span><span data-v-dd8814ff="">A modern, lightning-quick PoW captcha</span></h2><p data-v-dd8814ff="">Cap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work</p><!--]--><!--[--><!--]--><!--[--><!--]--></div><div data-v-dd8814ff=""><!--[--><!--[--><p><img src="https://capjs.js.org/logo.png" alt="VitePress" data-v-ab19afbb=""></p><!--]--><!--]--></div></div><!--[--><!--]--><!--[--><!--]--><div data-v-b1eea84a="" data-v-e07eaea7=""><!--[--><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>⚡️</p><h2 data-v-bd37d1a2="">250x smaller than hCaptcha</h2><p data-v-bd37d1a2="">Cap's widget library is extremely small, only ~20kb minified (including WASM)</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🔒️</p><h2 data-v-bd37d1a2="">Private</h2><p data-v-bd37d1a2="">Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collection</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🌈</p><h2 data-v-bd37d1a2="">Fully customizable</h2><p data-v-bd37d1a2="">Cap is self-hostable so you can customize both the backend &amp; frontend (or you can just use CSS variables)</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🤖</p><h2 data-v-bd37d1a2="">PoW-based</h2><p data-v-bd37d1a2="">Cap uses PoW instead of complex puzzles, making it easier for humans and harder for bots</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🧩</p><h2 data-v-bd37d1a2="">Standalone mode</h2><p data-v-bd37d1a2="">Cap offers a standalone mode with Docker, allowing you to use it with languages other than JS</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>💨</p><h2 data-v-bd37d1a2="">Invisible mode</h2><p data-v-bd37d1a2="">Cap can run invisibly in the background using a simple JS API</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>☁️</p><h2 data-v-bd37d1a2="">Floating mode</h2><p data-v-bd37d1a2="">Floating mode keeps your CAPTCHA hidden until it's needed</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🌳</p><h2 data-v-bd37d1a2="">Fully FOSS</h2><p data-v-bd37d1a2="">Completely open source under the Apache 2.0 license</p><!----></article><!--]--></div><!--]--></div><!--[--><!--]--><div data-v-e07eaea7="" data-v-c141a4bd=""><h2 id="what-is-cap" tabindex="-1">What is Cap? <a href="#what-is-cap" aria-label="Permalink to &quot;What is Cap?&quot;">​</a></h2><p>Cap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work. It's fast, private, and extremely simple to integrate. <a href="https://capjs.js.org/guide/effectiveness.html">Learn more about proof-of-work here.</a></p><p>Cap is built into 2 main parts:</p><ul><li><p><strong><a href="https://capjs.js.org/guide/widget.html" target="_blank" rel="noreferrer">@cap.js/widget</a></strong>: A small JavaScript library that renders the CAPTCHA and handles solving it using Web Workers and WASM.</p></li><li><p><strong><a href="https://capjs.js.org/guide/server.html" target="_blank" rel="noreferrer">@cap.js/server</a></strong>: An extremely simple, zero-dependencies library that handles creating and validating challenges.</p></li></ul><p>There are also some other helpful packages:</p><ul><li><p><strong><a href="https://capjs.js.org/guide/solver.html" target="_blank" rel="noreferrer">@cap.js/solver</a></strong>: Server-side solver for the CAPTCHA in case you want to use machine-to-machine.</p></li><li><p><strong><a href="https://capjs.js.org/guide/cli.html" target="_blank" rel="noreferrer">@cap.js/cli</a></strong>: Command-line interface for solving CAPTCHAs made with Cap. It's mainly designed for testing and when you need to solve these CAPTCHAs in a browser without JavaScript support.</p></li><li><p><strong><a href="https://capjs.js.org/guide/standalone.html" target="_blank" rel="noreferrer">Standalone mode</a></strong>: Docker image that helps you use Cap with any language or framework. It runs a simple REST API that can be used to create and validate challenges and an interactive UI to manage your keys.</p></li><li><p><strong>@cap.js/wasm</strong>: WASM solvers for Node and Web built with Rust.</p></li></ul><p>We also provide a middleware for a Cloudflare browser checkpoint-like experience:</p><ul><li><a href="https://capjs.js.org/guide/middleware/hono.html" target="_blank" rel="noreferrer">@cap.js/checkpoint-hono</a></li><li><a href="https://capjs.js.org/guide/middleware/express.html" target="_blank" rel="noreferrer">@cap.js/checkpoint-express</a></li><li><a href="https://capjs.js.org/guide/middleware/elysia.html" target="_blank" rel="noreferrer">@cap.js/middleware-elysia</a></li><li>more coming soon!</li></ul><p>It's designed to be a drop-in replacement for existing CAPTCHA solutions, with a focus on performance and UX.</p><p>Cap is built with JavaScript, runs on any JS runtime (Bun, Node.js, Deno), and has no dependencies. If you're not using any JS runtime, you can also use the standalone mode with Docker, which relies entirely on a simple REST API to create and validate challenges.</p><h2 id="why-cap" tabindex="-1">Why Cap? <a href="#why-cap" aria-label="Permalink to &quot;Why Cap?&quot;">​</a></h2><ul><li><strong>250x smaller than hCaptcha</strong><br><code>@cap.js/widget</code> is extremely small, only 12kb minified and brotli'd.</li><li><strong>Private</strong><br> Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collection.</li><li><strong>Fully customizable</strong><br> Cap's self-hostable so you can customize both the backend &amp; frontend — or you can just use CSS variables</li><li><strong>Proof-of-work</strong><br> Cap uses proof-of-work instead of complex puzzles, making it easier for humans and harder for bots</li><li><strong>Standalone mode</strong><br> Cap offers a standalone mode with Docker, allowing you to use it with languages other than JS.</li><li><strong>Invisible mode</strong><br> Cap can run invisibly in the background using a simple JS API.</li><li><strong>Floating mode</strong><br> Cap's floating mode keeps your CAPTCHA hidden until it's needed.</li><li><strong>Fully open-source</strong><br> Completely open source under the Apache license 2.0 license.</li></ul><p>It's ideal for:</p><ul><li>Protecting APIs from bots</li><li>Preventing spam on forms</li><li>Blocking automated login attempts</li><li>Securing free-tier abuse</li></ul><h2 id="feature-comparison" tabindex="-1">Feature comparison <a href="#feature-comparison" aria-label="Permalink to &quot;Feature comparison&quot;">​</a></h2><table tabindex="0"><thead><tr><th>CAPTCHA</th><th>Open-source</th><th>Free</th><th>Private</th><th>Fast to solve</th><th>Easy for humans</th><th>Small error rate</th><th>Checkpoint support</th><th>GDPR/CCPA Compliant</th><th>Customizable</th><th>Hard for bots</th><th>Easy to integrate</th></tr></thead><tbody><tr><td><strong>Cap</strong></td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>🟨</td><td>✅</td></tr><tr><td>Cloudflare Turnstile</td><td>❌</td><td>✅</td><td>🟨</td><td>🟨</td><td>✅</td><td>❌</td><td>🟨</td><td>✅</td><td>❌</td><td>🟨</td><td>✅</td></tr><tr><td>reCAPTCHA</td><td>❌</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td>hCAPTCHA</td><td>❌</td><td>🟨</td><td>🟨</td><td>❌</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>✅</td></tr><tr><td>Altcha</td><td>✅</td><td>✅</td><td>✅</td><td>🟨</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>🟨</td><td>🟨</td></tr><tr><td>FriendlyCaptcha</td><td>❌</td><td>❌</td><td>✅</td><td>🟨</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>🟨</td><td>🟨</td></tr><tr><td>MTCaptcha</td><td>❌</td><td>🟨</td><td>🟨</td><td>❌</td><td>❌</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>❌</td><td>🟨</td></tr><tr><td>GeeTest</td><td>❌</td><td>❌</td><td>❌</td><td>🟨</td><td>🟨</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>🟨</td><td>🟨</td></tr><tr><td>Arkose Labs</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>✅</td><td>🟨</td><td>❌</td><td>❌</td></tr></tbody></table><h2 id="alternatives" tabindex="-1">Alternatives <a href="#alternatives" aria-label="Permalink to &quot;Alternatives&quot;">​</a></h2><p>Cap is a modern alternative to:</p><ul><li><a href="https://www.google.com/recaptcha/about/" target="_blank" rel="noreferrer">reCAPTCHA</a></li><li><a href="https://www.hcaptcha.com/" target="_blank" rel="noreferrer">hCaptcha</a></li><li><a href="https://developers.cloudflare.com/turnstile/" target="_blank" rel="noreferrer">Cloudflare Turnstile</a></li></ul><p>But unlike them, Cap is <a href="https://capjs.js.org/guide/workings.html"><strong>computation-bound, not tracking-bound</strong></a>.</p><p><a href="https://capjs.js.org/guide/alternatives.html">Read more about alternatives</a></p><h2 id="license" tabindex="-1">License <a href="#license" aria-label="Permalink to &quot;License&quot;">​</a></h2><p>Cap is licensed under the Apache License 2.0.</p><hr><p><a href="https://www.bestpractices.dev/projects/9920" target="_blank" rel="noreferrer"><img src="https://www.bestpractices.dev/projects/9920/badge" alt="OpenSSF Best Practices" loading="lazy"></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beating Google's kernelCTF PoW using AVX512 (294 pts)]]></title>
            <link>https://anemato.de/blog/kctf-vdf</link>
            <guid>44137715</guid>
            <pubDate>Fri, 30 May 2025 16:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anemato.de/blog/kctf-vdf">https://anemato.de/blog/kctf-vdf</a>, See on <a href="https://news.ycombinator.com/item?id=44137715">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><header><a aria-label="anematode" href="https://anemato.de/"></a></header><main><section><article><div><header><div><dl><p><dt>Published on</dt><dd><time datetime="2025-05-29T00:00:00.000Z">Wednesday, May 28, 2025</time></dd></p></dl></div></header><div><dl><dt>Authors</dt><dd><ul><li><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=48&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=96&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=96&amp;q=75"><dl><dt>Name</dt><dd>Timothy Herchen</dd><dt>Twitter</dt><dd></dd></dl></li></ul></dd></dl><div><h2 id="introduction">Introduction</h2><p>In May 2025, my <a target="_blank" rel="noopener noreferrer" href="https://cor.team/">Crusaders of Rust</a> teammates William Liu (<a target="_blank" rel="noopener noreferrer" href="https://willsroot.io/">FizzBuzz101</a>) and Savy Dicanosa (<a target="_blank" rel="noopener noreferrer" href="https://syst3mfailure.io/">Syst3mFailure</a>) discovered and developed an exploit of a use-after-free bug in Linux's packet scheduler. <a target="_blank" rel="noopener noreferrer" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ac9fe7dd8e730a103ae4481147395cc73492d786">The bugfix patch</a> contains additional details. William found this bug while fuzzing Linux for his master's thesis, which I will link here upon its publication. (Congratulations, William!)</p><p>They wanted to submit the bug to Google's <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/security-research/kernelctf/rules.html">kernelCTF</a> competition for an anticipated $51,000 bounty.<sup><a href="#user-content-fn-bounty" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-bounty">1</a></sup> Unfortunately, finding the bug and writing the exploit was only the first part of the battle. This post documents my small but unique contribution to our ultimately winning the bounty.</p><h2 id="setting-the-stage">Setting the stage</h2><p>To avoid paying out lots of money, kernelCTF organizers limit the number of submissions eligible for a bounty. Every two weeks at noon UTC, the submission window opens. Only the first team who is able to connect to and exploit the server, and submit the flag to a Google Form, receives a payout; any subsequent submissions are marked as duplicates. Furthermore, to prevent excessive submissions, the connecting to kernelCTF server requires solving a "proof of work"—a function which, by design, takes a few seconds to evaluate.</p><p>In summary, <strong>the submission process has these steps</strong>:</p><ol><li>At 12:00:00 UTC, connect to the kernelCTF server.</li><li>Solve the proof of work, which takes roughly 4 seconds.</li><li>Wait for the instance to boot. (Roughly 2.5 seconds.)</li><li>Upload the exploit and run it to secure the flag. (Time elapsed depends on the exploit. Savy optimized this one to take roughly 0.55 seconds without sacrificing reliability. Wow!)</li><li>Submit the flag to a Google Form. The submission timestamp determines the winner of the "slot".</li></ol><p>Our goal was to complete all these steps in sequence, faster than all the other teams.</p><h2 id="enter-the-sweats">Enter the sweats</h2><p>Because of the large bounties, over time professional vulnerability research teams have aggressively optimized their submission process. For the May 2, 2025, submission window preceding ours, the first team to submit the flag <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/spreadsheets/d/e/2PACX-1vS1REdTA29OJftst8xN5B5x8iIUcxuK6bXdzF8G1UXCmRtoNsoQ9MbebdRdFnj6qZ0Yd7LwQfvYC2oF/pubhtml">did so 4.5 seconds after noon</a>!</p><p><img alt="kernelCTF submission time" loading="lazy" width="1518" height="156" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=1920&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=3840&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=3840&amp;q=75"></p><p>The numbers don't seem to add up: even assuming an instant exploit and form submission, the VM boot time and proof of work already take 6.5 seconds. Looking closer, we see that the time at which the winning submission's flag was generated (highlighted in red) is one second <em>before</em> noon UTC. Yet, the timestamp is generated <em>after</em> the proof of work is solved. Did sweaty CTFers invent time travel?</p><p>Alas! Because of a rounding quirk in the kernelCTF server code (<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/security-research/blob/90cc1d1fe4d4626d4c0aba4a78c02fc72fe18ac7/kernelctf/server/server.py#L192">here</a>), the VM instance actually boots at 11:59:59—so no time travel. Still, the timestamp indicates that the winning team solved the proof of work in less than a second! How could this be?</p><p>We don't know for certain, but one kernelCTF organizer postulated that they were using <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">field-programmable gate arrays</a> (FPGAs). FPGAs are custom silicon that can perform specific tasks extremely quickly, to the exclusion of general-purpose tasks. They are not only fairly expensive, but also tricky to program. If the professional team had access to an FPGA programmed to perform the proof of work, a sub-second proof of work time was conceivable.</p><p>On May 13, William messaged me on Discord seeking advice on how to optimize the proof of work so that we could preempt the competition. I had to act fast: The next submission window would open at 5 a.m. PST, May 16.<sup><a href="#user-content-fn-thesis" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-thesis">2</a></sup></p><h2 id="the-proof-of-work-the-sloth-vdf">The proof of work: The "sloth" VDF</h2><p>The proof of work (<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/kctf/blob/v1/docker-images/challenge/pow.py">implemented here</a>) is a certain <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/Verifiable_delay_function">verifiable delay function</a> (VDF) known as "sloth". VDFs are cryptographic primitives which prove that a nontrivial amount of time has passed by requiring a long, serial computation. This computation outputs a proof which can be (relatively) quickly verified. Because the computation is serial, scaling to more computational resources (such as more CPU or GPU cores) does not reduce the runtime.<sup><a href="#user-content-fn-hash" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-hash">3</a></sup></p><p>The sloth VDF was introduced by <a target="_blank" rel="noopener noreferrer" href="https://csrc.nist.gov/csrc/media/events/workshop-on-elliptic-curve-cryptography-standards/documents/papers/session1-wesolowski-paper.pdf">Lenstra and Wesolowski (2015)</a>. I won't reproduce the number theory behind sloth (see page 4 of the paper for that), but to summarize matters, the function we must optimize boils down to:</p><div><pre><code><span><span>def</span> <span>sloth_root</span><span>(</span>x<span>,</span> difficulty<span>=</span><span>7337</span><span>)</span><span>:</span>
</span><span>    <span>for</span> i <span>in</span> <span>range</span><span>(</span>difficulty<span>)</span><span>:</span>             <span># repeat the inner kernel this many times</span>
</span><span>        <span>for</span> j <span>in</span> <span>range</span><span>(</span><span>1277</span><span>)</span><span>:</span>               <span># square x this many times</span>
</span><span>            x <span>=</span> <span>(</span>x <span>*</span> x<span>)</span> <span>%</span> <span>(</span><span>2</span> <span>**</span> <span>1279</span> <span>-</span> <span>1</span><span>)</span>   <span># modulus is a Mersenne number</span>
</span><span>        x <span>=</span> x<span>.</span>bit_flip<span>(</span><span>0</span><span>)</span>                   <span># complement the LSB of x</span>
</span><span>    <span>return</span> <span>int</span><span>(</span>x<span>)</span>
</span></code></pre></div><p>where <em>x</em> is a supplied 1280-bit integer. The <em>difficulty</em> variable linearly controls how long the VDF takes to solve.</p><p>Google's reference implementation uses gmpy, which is a Python binding to the venerable <a target="_blank" rel="noopener noreferrer" href="https://gmplib.org/">GNU Multiprecision Library</a> (GMP). GMP's addition and multiplication kernels are handwritten in assembly for each target platform (<a target="_blank" rel="noopener noreferrer" href="https://github.com/gmp-mirror/gmp/blob/master/mpn/x86_64/mulx/adx/addmul_1.asm">example</a>). The loop-carried dependency of <em>x</em> means that the computation is inherently serial, so throwing more cores at the problem—at least in a naïve way—is unhelpful. Meaningfully speeding up this function was going to be <em>tough</em>.</p><h2 id="initial-progress">Initial progress</h2><p>I set out on the obvious goal of optimizing the 1280-bit modular squaring (line 4 in the code above). The first success was mathematical: Because the modulus is a Mersenne number of length 1279 bits, and the intermediate product is 2 · 1280 = 2560 bits, computing the residue actually corresponds to a handful of cheaper operations:</p><div><pre><code><span><span>def</span> <span>mod_2_1279_minus_1</span><span>(</span>x<span>)</span><span>:</span>    <span># compute x % (2 ** 1279 - 1)</span>
</span><span>    p <span>=</span> <span>2</span> <span>**</span> <span>1279</span> <span>-</span> <span>1</span>
</span><span>    r <span>=</span> <span>(</span>x <span>&amp;</span> p<span>)</span> <span>+</span> <span>(</span>x <span>&gt;&gt;</span> <span>1279</span><span>)</span>
</span><span>    <span>if</span> r <span>&gt;=</span> p<span>:</span>
</span><span>        r <span>-=</span> p
</span><span>    <span>return</span> r
</span></code></pre></div><p>I also translated the function to C++ to remove FFI overhead. The newly optimized code:</p><div><pre><code><span><span>constexpr</span> <span>int</span> MERSENNE_EXP <span>=</span> <span>1279</span><span>;</span>
</span><span>
</span><span>mpz_t low<span>,</span> high<span>,</span> p<span>;</span>
</span><span>
</span><span><span>void</span> <span>mpz_mod_mersenne</span><span>(</span>mpz_t r<span>,</span> <span>const</span> mpz_t x<span>)</span> <span>{</span>
</span><span>    <span>// p = 2^n - 1</span>
</span><span>    <span>mpz_mod_2exp</span><span>(</span>low<span>,</span> x<span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_fdiv_q_2exp</span><span>(</span>high<span>,</span> x<span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_add</span><span>(</span>r<span>,</span> low<span>,</span> high<span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span><span>mpz_cmp</span><span>(</span>r<span>,</span> p<span>)</span> <span>&gt;=</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        <span>mpz_sub</span><span>(</span>r<span>,</span> r<span>,</span> p<span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>bool</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
</span><span>    <span>mpz_inits</span><span>(</span>low<span>,</span> high<span>,</span> p<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>    <span>mpz_ui_pow_ui</span><span>(</span>p<span>,</span> <span>2</span><span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_sub_ui</span><span>(</span>p<span>,</span> p<span>,</span> <span>1</span><span>)</span><span>;</span>
</span><span>    <span>return</span> <span>true</span><span>;</span>
</span><span><span>}</span>
</span><span><span>bool</span> _unused <span>=</span> <span>init</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>void</span> <span>the_powmod</span><span>(</span>mpz_t x<span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>1277</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        <span>mpz_mul</span><span>(</span>x<span>,</span> x<span>,</span> x<span>)</span><span>;</span>
</span><span>        <span>mpz_mod_mersenne</span><span>(</span>x<span>,</span> x<span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>int</span> <span>main</span><span>(</span><span>)</span>
</span><span><span>{</span>
</span><span>    <span>const</span> <span>int</span> diff <span>=</span> <span>7337</span><span>;</span>
</span><span>    mpz_t x<span>,</span> r<span>;</span>
</span><span>    <span>mpz_inits</span><span>(</span>x<span>,</span> r<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>    <span>mpz_set_str</span><span>(</span>x<span>,</span> <span>"96729140485950483920373592475530255430"</span><span>,</span> <span>10</span><span>)</span><span>;</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> diff<span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        <span>the_powmod</span><span>(</span>x<span>)</span><span>;</span>
</span><span>        <span>mpz_combit</span><span>(</span>x<span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>    <span>char</span> <span>*</span>str <span>=</span> <span>mpz_get_str</span><span>(</span><span>NULL</span><span>,</span> <span>10</span><span>,</span> x<span>)</span><span>;</span>
</span><span>    std<span>::</span>cout <span>&lt;&lt;</span> <span>"x: "</span> <span>&lt;&lt;</span> str <span>&lt;&lt;</span> std<span>::</span>endl<span>;</span>
</span><span>    <span>return</span> <span>0</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>This code ran in 1.9 seconds on my M1 Macbook Pro—a substantial improvement, and faster than similarly optimized solvers like <a target="_blank" rel="noopener noreferrer" href="https://github.com/Aplet123/kctf-pow">this one written in Rust</a>.<sup><a href="#user-content-fn-rust" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-rust">4</a></sup> William linked GMP statically, which would presumably allow some inlining, and reported a further speedup—roughly 1.4 seconds on a fancy Intel Ice Lake laptop. Not bad, but still not a guaranteed win.</p><p>The modulus no longer being a bottleneck, I considered handwriting multiplication kernels in assembly to take advantage of the multiplication being a fixed width of 1280-bit × 1280-bit → 2560-bit; the factors fit neatly into twenty 64-bit limbs, and the product fits in forty limbs.<sup><a href="#user-content-fn-limbs" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-limbs">5</a></sup> GMP's assembly kernels are generic in the bitwidth, which introduces some overhead. Unfortunately, at 1.4 seconds we were approaching the theoretical limit of multiplication throughput, which is one 64-bit × 64-bit → 128-bit multiplication per cycle on all recent hardware.<sup><a href="#user-content-fn-uops" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-uops">6</a></sup></p><h2 id="enter-avx512">Enter AVX512</h2><p><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/AVX-512">AVX512</a> is Intel's extension to the x86 ISA, first made available in 2016. It is a comprehensive overhaul of x86 SIMD programming, doubling the number and width of vector registers, adding <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Scalable_Vector_Extension">Scalable Vector Extension</a>–style mask predication, and hundreds of new instructions. It also has a troubled history: Linus Torvalds <a target="_blank" rel="noopener noreferrer" href="https://www.realworldtech.com/forum/?threadid=193189&amp;curpostid=193190">famously</a> wished it to "die a horrible death", and despite supporting it on consumer CPUs for several generations, Intel disabled support for it starting with the Alder Lake µarch (2021); support continues in the server space. Meanwhile, AMD implemented AVX512 in their Zen 4 (2022) and Zen 5 µarches for both consumer and server CPUs.</p><p>Of present interest is the AVX512 Integer Fused Multiply–Add extension (AVX512IFMA), which was introduced specifically to speed up big-integer arithmetic—see, <em>e.g.</em>, <a target="_blank" rel="noopener noreferrer" href="https://builders.intel.com/docs/networkbuilders/intel-avx-512-fast-modular-multiplication-technique-technology-guide-1710916893.pdf">Ozturk, Kantecki &amp; Yap (2024)</a>. I learned about AVX512IFMA during my competitive programming arc, optimizing submissions for <a target="_blank" rel="noopener noreferrer" href="https://judge.yosupo.jp/">judge.yosupo.jp</a>. The extension introduces two new instructions which operate on vector registers:</p><ul><li><a target="_blank" rel="noopener noreferrer" href="https://www.felixcloutier.com/x86/vpmadd52luq">vpmadd52luq</a> – Packed Multiply of Unsigned 52-Bit Unsigned Integers and Add Low 52-Bit Products to 64-Bit Accumulators</li><li><a target="_blank" rel="noopener noreferrer" href="https://www.felixcloutier.com/x86/vpmadd52huq">vpmadd52huq</a> – Packed Multiply of Unsigned 52-Bit Unsigned Integers and Add High 52-Bit Products to 64-Bit Accumulators</li></ul><p>Essentially, the instructions perform the following operation (assuming the high 12 bits of each element in <em>a</em> and <em>b</em> are zero):</p><div><pre><code><span><span>// vpmadd52luq dst, a, b</span>
</span><span><span>void</span> <span>vpmadd52luq</span><span>(</span><span>uint64_t</span> dst<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> a<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> b<span>[</span><span>8</span><span>]</span><span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>8</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        dst<span>[</span>i<span>]</span> <span>+=</span> <span>(</span>a<span>[</span>i<span>]</span> <span>*</span> b<span>[</span>i<span>]</span><span>)</span> <span>&amp;</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// vpmadd52huq dst, a, b</span>
</span><span><span>void</span> <span>vpmadd52huq</span><span>(</span><span>uint64_t</span> dst<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> a<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> b<span>[</span><span>8</span><span>]</span><span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>8</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        dst<span>[</span>i<span>]</span> <span>+=</span> <span>(</span><span>(</span>__uint128_t<span>)</span>a<span>[</span>i<span>]</span> <span>*</span> b<span>[</span>i<span>]</span><span>)</span> <span>&gt;&gt;</span> <span>52</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>In simpler terms, the instructions perform the low and high halves of a 52 × 52 → 104 multiplication, and accumulate the result into the destination register. At the execution unit level, the instructions reuse the multipliers used for double-precision floating point, and thus don't necessitate much extra silicon. They also have fantastic throughput: on Zen 5, which has a full 512-bit datapath, we can execute two of these instructions per clock!</p><p>At this point I was confident that I could make an extraordinarily fast VDF solver. All that was left was to implement it in two days....</p><p><img src="https://anemato.de/static/images/pow-is-broken.png" alt="yapping" width="500"></p><h2 id="the-game-plan">The game plan</h2><p>The natural radix for the AVX512IFMA extensions is 2<sup>52</sup>, <em>i.e.</em> 52-bit limbs stored in 64-bit words, so I let ChatGPT write a simple converter between GMP's representation and the 52-bit representation. We need ⌈1280 / 52⌉ = 25 limbs, which requires four 512-bit "zmm" registers (each register can store eight limbs, so the last register will only store one).</p><p>The first step is squaring the integer into a 50-limb intermediate product. We use a simple symmetry to almost halve the number of required multiplications, breaking up the desired value into two terms:</p><p>(a<sub>24</sub>2<sup>52·24</sup> + a<sub>23</sub>2<sup>52·23</sup> + ... + a<sub>0</sub>)<sup>2</sup> = (<span>a<sub>24</sub><sup>2</sup></span>2<sup>52·48</sup> + <span>a<sub>23</sub><sup>2</sup></span>2<sup>52·46</sup> + ... + <span>a<sub>0</sub><sup>2</sup></span>) + 2 <span><small>i,j≤24</small><span>∑</span><small>i=0,j&gt;i</small></span><span>a<sub>i</sub>a<sub>j</sub></span> 2<sup>52·(i + j)</sup></p><p>Each of the yellow-highlighted multiplications produces a low term (furnished by <em>vpmadd52luq</em>) and a high term (furnished by <em>vpmadd52huq</em>).</p><h2 id="arranging-the-multiplications">Arranging the multiplications</h2><p>First consider the <em>second</em> term above, the double summation highlighted in red. We want to reduce the number of shuffles necessary to get all the terms in the correct place, and use the "free" 64-bit accumulation as much as possible. One way to do this is to multiply a sliding window of 8 contiguous limbs by a single multiplier limb; all the output words, for both the low and high halves, will also be contiguous in the output. We can also use the merge masking feature to prevent accumulation of products that shouldn't be in the final sum, <em>e.g.</em>, pairs where i = j. By selecting these windows and multipliers correctly, we can minimize the number of wasted multiplications.</p><div><pre><code><span><span>// Computing the second term</span>
</span><span><span>// input contains the 25 52-bit limbs, stored in 64-bit words</span>
</span><span><span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> padded_data<span>[</span><span>8</span> <span>*</span> <span>6</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>}</span><span>;</span>  <span>// so that loads OOB are still valid</span>
</span><span><span>uint64_t</span> <span>*</span>data <span>=</span> padded_data <span>+</span> <span>8</span><span>;</span>
</span><span>
</span><span>__m512i clumps<span>[</span><span>4</span><span>]</span> <span>=</span> <span>{</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input<span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>8</span><span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>16</span><span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>24</span><span>)</span>
</span><span><span>}</span><span>;</span>
</span><span>
</span><span><span>_mm512_store_si512</span><span>(</span>data<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>ZERO</span> <span><span>_mm512_setzero_si512</span><span>(</span><span>)</span></span></span>
</span><span>
</span><span><span>// Seven zmm accumulators are necessary</span>
</span><span>__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>-</span><span>7</span><span>;</span> i <span>&lt;=</span> <span>24</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    <span>// Sliding window</span>
</span><span>    __m512i m1 <span>=</span> <span>_mm512_loadu_si512</span><span>(</span>data <span>+</span> i<span>)</span><span>;</span> <span>// Load the current window of 8 elements</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> j <span>=</span> <span>0</span><span>,</span> k <span>=</span> <span>0</span><span>;</span> j <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>j<span>,</span> k <span>+=</span> <span>8</span><span>)</span> <span>{</span>
</span><span>        <span>// Decide whether to accumulate into accum[j], which should happen if there</span>
</span><span>        <span>// is at least one element shared between the jth accumulator and [i, i+7]</span>
</span><span>        <span>int</span> lo <span>=</span> k <span>-</span> i<span>;</span>
</span><span>        <span>int</span> hi <span>=</span> k <span>-</span> i <span>-</span> <span>1</span><span>;</span>
</span><span>        <span>// Process low halves</span>
</span><span>        <span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>            <span>// Discard out of bounds multiplications</span>
</span><span>            __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>                accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_set1_epi64</span><span>(</span>data<span>[</span>lo<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>        <span>}</span>
</span><span>        <span>// Process high halves</span>
</span><span>        <span>if</span> <span>(</span>hi <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> hi <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>            <span>// ditto</span>
</span><span>            __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>hi <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>hi <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>                accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52hi_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_set1_epi64</span><span>(</span>data<span>[</span>hi<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>As an example, accumulator <em>accum[1]</em> contains output limbs 8 through 15, inclusive, and the following accumulations are executed (in this order):</p><div><pre><code><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>1</span> by limb <span>7</span> <span>with</span> mask <span>10000000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>1</span> by limb <span>6</span> <span>with</span> mask <span>11000000</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>2</span> by limb <span>6</span> <span>with</span> mask <span>11100000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>2</span> by limb <span>5</span> <span>with</span> mask <span>11110000</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>3</span> by limb <span>5</span> <span>with</span> mask <span>11111000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>3</span> by limb <span>4</span> <span>with</span> mask <span>11111100</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>4</span> by limb <span>4</span> <span>with</span> mask <span>11111110</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>4</span> by limb <span>3</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>5</span> by limb <span>3</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>5</span> by limb <span>2</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>6</span> by limb <span>2</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>6</span> by limb <span>1</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>7</span> by limb <span>1</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>7</span> by limb <span>0</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>8</span> by limb <span>0</span> <span>with</span> mask <span>11111111</span>
</span></code></pre></div><p>Here, window <em>i</em> contains limbs <em>i</em> through <em>i+7</em> inclusive. Note that the masks mostly contain ones, indicating that we are not wasting too much multiplication throughput on masked-out elements.</p><p>Computing the first term is easier. We just square each element and interleave the low and high words:</p><div><pre><code><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    __m512d diag_lo <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52lo_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>    __m512d diag_hi <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52hi_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>    __m512i shuf_lo <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>11</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>2</span><span>,</span> <span>9</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>    __m512i shuf_hi <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>15</span><span>,</span> <span>7</span><span>,</span> <span>14</span><span>,</span> <span>6</span><span>,</span> <span>13</span><span>,</span> <span>5</span><span>,</span> <span>12</span><span>,</span> <span>4</span><span>)</span><span>;</span>
</span><span>        accum<span>[</span><span>2</span> <span>*</span> i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_lo<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span>i <span>!=</span> <span>3</span><span>)</span> <span>{</span>
</span><span>        accum<span>[</span><span>2</span> <span>*</span> i <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>+</span><span>1</span><span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_hi<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>Finally, we need to implement the reduction modulo 2<sup>1279</sup>-1. This is just a matter of selecting the upper 1279 bits and adding them to the lower 1279 bits. It's worth noting that at this point, the accumulator elements may exceed 2<sup>52</sup>-1, but we can delay carry propagation until after the addition.</p><div><pre><code><span>__m512i low_52_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>__m512i hi_12_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>~</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>
</span><span>__m512i high_1279<span>[</span><span>4</span><span>]</span><span>;</span>
</span><span><span>shift_down_1279</span><span>(</span>accum<span>,</span> high_1279<span>)</span><span>;</span>
</span><span><span>filter_low_1279</span><span>(</span>accum<span>)</span><span>;</span>
</span><span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> high_1279<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>{</span>
</span><span>carry2<span>:</span><span>;</span>
</span><span>__m512i carry_test <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span>__m512i group_out <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    __m512i carries <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> <span>52</span><span>)</span><span>;</span>
</span><span>    __m512i carries_into <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>carries<span>,</span> group_out<span>,</span> <span>7</span><span>)</span><span>;</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> low_52_bits<span>)</span><span>,</span> carries_into<span>)</span><span>;</span>
</span><span>    group_out <span>=</span> carries<span>;</span>
</span><span>    carry_test <span>=</span> <span>_mm512_and_si512</span><span>(</span>carry_test<span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span> <span>// improve latency over a series of masked tests</span>
</span><span><span>}</span>
</span><span>
</span><span><span>if</span> <span>(</span><span>__builtin_expect</span><span>(</span><span>_mm512_test_epi64_mask</span><span>(</span>carry_test<span>,</span> hi_12_bits<span>)</span><span>,</span> <span>0</span><span>)</span><span>)</span> <span>{</span>
</span><span>    <span>goto</span> carry2<span>;</span>
</span><span><span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>We then need to subtract the Mersenne modulus if the addition is too large, but this is easy to check: we perform the subtraction <em>iff</em> the 1280th bit is 1. Subtracting 2<sup>1279</sup>-1 is equivalent to subtracting 2<sup>1279</sup> (<em>i.e.</em>, zeroing the 1280th bit) followed by adding 1 to the least-significant limb. Because the subtraction occurs with 50% probability, we do it in a branchless manner:</p><div><pre><code><span><span>// Now compare with 2^1279 - 1; if &gt;=, subtract 2^1279 - 1. classic Mersenne number modulo algorithm</span>
</span><span>__m512i bit_1279 <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> 1ULL <span>&lt;&lt;</span> <span>31</span><span>)</span><span>;</span>
</span><span>__m512i mask_off <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span>1ULL <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// Branchless approach appears to save about 2 ns per iteration. Also, we stay in vector regs and don't use a test mask here because it tends to be slower</span>
</span><span>__m512i cmp <span>=</span> <span>_mm512_and_epi64</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> bit_1279<span>)</span><span>;</span>
</span><span>accum<span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>0</span><span>]</span><span>,</span> <span>_mm512_srli_epi64</span><span>(</span>cmp<span>,</span> <span>31</span><span>)</span><span>)</span><span>;</span>  <span>// potentially +1 to last word</span>
</span><span>accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>mask_off<span>,</span> accum<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// TODO 1/2^52 chance of error here due to carry -- check it</span>
</span></code></pre></div><p>There is a tiny chance that an overflow occurs in this last step: if the last limb happened to be exactly 2<sup>52</sup>-1, then we'd need to propagate the carry. However, because PoWs are randomly generated, the probability this happens on any given run is about 2 in a billion—so I just ignored it.</p><p>At this point, the PoW was taking about <strong>0.45 seconds</strong> on a rented Ryzen 9950X, which is a fast Zen 5 chip. Very promising!</p><h2 id="keeping-the-multiplyadds-in-flight">Keeping the multiply–adds in flight</h2><p>The multiply–add instructions have a latency of 4 cycles, and 2 can start every cycle. Thus, we need at least eight accumulators to fully saturate the multipliers instead of bottlenecking on latency, but we only have seven (and some of them are only used occasionally). The solution is to have fourteen accumulators—one set for the low halves and one set for the high halves—then merge them at the end:</p><div><pre><code><span>__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>__m512i accum_hi<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>
</span><span><span>// ... fmadd spam goes here ...</span>
</span><span>
</span><span><span>// Fold high and low halves</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum_hi<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>This brought the PoW down to roughly <strong>0.32 seconds</strong>. The expanded AVX512 register file, with 32 zmm registers, came in clutch here.</p><h2 id="taming-the-register-allocator">Taming the register allocator</h2><p>Inspecting the assembly revealed that both GCC and clang were unrolling the loop, converting the <em>_mm512_set1_epi64</em> instructions into <em>vbroadcastsd zmm, m64</em> instructions—one per limb—and then running out of vector registers during regalloc. Instead of rematerializing the values, they would stack-spill and reload the broadcasted vectors, causing considerable overhead.<sup><a href="#user-content-fn-numberworld" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-numberworld">7</a></sup></p><p>My solution was to use inline assembly to force the <em>vpmadd52luq</em>/<em>vpmadd52huq</em> instructions to use a <em>memory broadcast operand</em> for the multiplier limb. Instructions encoded with such an operand copy a single 32- or 64-bit element from memory to all elements of a vector operand, without consuming an architectural register. Moreover, this broadcast load does not consume any vector ALU resources: it is handled entirely by the load unit!</p><p>Now that we're using asm, the compiler can't remove masks of all 1s, so for optimal encoding we need separate asm for the all 1s case. Finally, when the multiplier limb happens to be at index zero in one of the zmm registers (<em>i.e.</em>, multiples of 8), we use <em>vpbroadcastq</em> to splat it from register to register, which I measured to be a performance improvement over loading it from memory. The accumulation sequence is now:</p><div><pre><code><span><span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>    <span>// Multiples of 8 are handled by a register broadcast instead of a memory load for efficiency</span>
</span><span><span><span>#</span><span>define</span> <span>FOR_EACH_OFFS</span> <span><span>X</span><span>(</span><span>1</span><span>)</span> <span>X</span><span>(</span><span>2</span><span>)</span> <span>X</span><span>(</span><span>3</span><span>)</span> <span>X</span><span>(</span><span>4</span><span>)</span> <span>X</span><span>(</span><span>5</span><span>)</span> <span>X</span><span>(</span><span>6</span><span>)</span> <span>X</span><span>(</span><span>7</span><span>)</span> <span>X</span><span>(</span><span>9</span><span>)</span> <span>X</span><span>(</span><span>10</span><span>)</span> <span>X</span><span>(</span><span>11</span><span>)</span> <span>X</span><span>(</span><span>12</span><span>)</span> <span>X</span><span>(</span><span>13</span><span>)</span> <span>X</span><span>(</span><span>14</span><span>)</span> <span>X</span><span>(</span><span>15</span><span>)</span> <span>X</span><span>(</span><span>17</span><span>)</span> <span>X</span><span>(</span><span>18</span><span>)</span> <span>X</span><span>(</span><span>19</span><span>)</span> <span>X</span><span>(</span><span>20</span><span>)</span> <span>X</span><span>(</span><span>21</span><span>)</span> <span>X</span><span>(</span><span>22</span><span>)</span> <span>X</span><span>(</span><span>23</span><span>)</span></span></span>
</span><span>    __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span>  </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>        <span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>            FOR_EACH_OFFS
</span><span>            <span>default</span><span>:</span>
</span><span>            accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>
</span><span>    <span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>        <span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>            FOR_EACH_OFFS
</span><span>            <span>default</span><span>:</span>
</span><span>            accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span><span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>(This is just for the low set of accumulators; the high set is nearly identical.) At this point, the PoW was taking roughly <strong>0.23 seconds</strong>.</p><h2 id="creating-the-windows-without-touching-memory">Creating the windows without touching memory</h2><p>To compute the "windows", we are storing the integer to memory in an aligned fashion, then loading from it in an unaligned fashion. This is a classic situation that causes a <a target="_blank" rel="noopener noreferrer" href="https://easyperf.net/blog/2018/03/09/Store-forwarding"><em>store-forwarding stall</em></a>, which further lengthens the critical path (the multiplications cannot commence until a window is loaded from memory). A better solution is to use the <em>valignq</em> instruction, which lets us simulate an unaligned load from the <em>clumps</em> array containing our integer.</p><div><pre><code><span>__m512i m1<span>;</span>
</span><span><span>if</span> <span>(</span><span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
</span><span>    m1 <span>=</span> clumps<span>[</span>i <span>/</span> <span>8</span><span>]</span><span>;</span>
</span><span><span>}</span> <span>else</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>UNALIGNED</span><span><span>(</span>S<span>)</span> <span>case</span> S<span>:</span> <span>{</span> m1 <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>]</span><span>,</span> i <span>&lt;</span> <span>0</span> <span>?</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span> <span>:</span> clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>-</span><span>1</span><span>]</span><span>,</span> S<span>)</span><span>;</span> <span>break</span><span>;</span> <span>}</span></span></span>
</span><span>    <span>switch</span> <span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>{</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>1</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>2</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>3</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>4</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>5</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>6</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>7</span><span>)</span>
</span><span>        <span>default</span><span>:</span> <span>abort</span><span>(</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>This brought the PoW down to <strong>0.21 seconds</strong>. At this point, I decided to call it quits and sent my teammates the final C code.</p><h2 id="may-16-show-time">May 16: Show time!</h2><p>My friends got up at 4:30 a.m. PST, May 16, to prepare for the final submission; I couldn't be assed to be awake, so I slept until 6:30. They spun up a Zen 5 Google Cloud server in the Netherlands, geographically closest to the Google Form submission server, to minimize latency. A few minutes before 5:00, they recorded an intercepted POST request submitting the Google form, but with a dummy flag. The form submission program was devised and optimized by Bryce Casaje (<a target="_blank" rel="noopener noreferrer" href="https://brycec.me/">strellic</a>) and Larry Yuan (<a target="_blank" rel="noopener noreferrer" href="https://larry.sh/">ehhthing</a>). <a target="_blank" rel="noopener noreferrer" href="https://max.xz.ax/">Max Cai</a> also assisted in development and submission. Then at 5:00, the server connected to the kernelCTF server, solved the proof of work, ran Savy's optimized exploit, inserted the flag into the POST request, and sent it off....</p><p><img alt="lol" loading="lazy" width="1526" height="94" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=1920&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=3840&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=3840&amp;q=75"></p><p>We got it in 3.6 seconds—the fastest-ever kernelCTF submission! Later that day, the kernelCTF organizers confirmed our eligibility for the bounty and we all breathed a collective sigh of relief. Again, congratulations to Savy and William for discovering and exploiting this bug! Thanks to them for presenting me with the challenge, and thanks to my CSE 260 professor Bryan Chin (<a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/ucsd.edu/bryan-chin/home">website</a>) for what he taught us about program optimization.</p><h2 id="the-end-of-an-era">The end of an era</h2><p>On May 28, kernelCTF organizer koczkatamas announced that the proof of work was being removed:</p><p><img src="https://anemato.de/static/images/pow-is-gone.png" alt="PoW is gone" width="400"></p><p>On the one hand, it's sad that we no longer have a competitive advantage, and the slot race becomes purely about exploit duration and network latency. On the other hand, at least people don't need to buy expensive FPGAs, or pull out their inline asm knowledge, to be on equal footing with the professionals. It also frees me to release this post!</p><p>Please message me on Discord (@forevermilk) if you have any comments or questions. I am also researching VDFs that are more resilient to assembly-level optimizations; if you have any ideas or would like to collaborate, I'm all ears.</p><h2 id="the-final-solver">The final solver</h2><p>This code is the product (ha!) of about 12 hours of work across May 14 and 15, and it is correspondingly unclean. Consider it released under the GNU AGPL 3.0.</p><div><pre><code><span><span>// Written by Timothy Herchen</span>
</span><span><span>// gcc main.c -O3 -march=znver5 -masm=intel -lgmp</span>
</span><span><span><span>#</span><span>include</span> <span>&lt;immintrin.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;gmp.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;string.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdlib.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdio.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdint.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stddef.h&gt;</span></span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>uint128_t</span> <span>__uint128_t</span></span>
</span><span>
</span><span><span>void</span> <span>gmp_to_array</span><span>(</span><span>mpz_t</span> mpz<span>,</span> <span>uint64_t</span> <span>*</span>array<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> N<span>;</span>
</span><span>    <span>mpz_export</span><span>(</span>array<span>,</span> <span>&amp;</span>N<span>,</span> <span>1</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> mpz<span>)</span><span>;</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>,</span> j <span>=</span> N <span>-</span> <span>1</span><span>;</span> i <span>&lt;</span> j<span>;</span> <span>++</span>i<span>,</span> <span>--</span>j<span>)</span> <span>{</span>
</span><span>	    <span>uint64_t</span> temp <span>=</span> array<span>[</span>i<span>]</span><span>;</span>
</span><span>	    array<span>[</span>i<span>]</span> <span>=</span> array<span>[</span>j<span>]</span><span>;</span>
</span><span>	    array<span>[</span>j<span>]</span> <span>=</span> temp<span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// Destroys the array</span>
</span><span><span>void</span> <span>array_to_gmp</span><span>(</span><span>uint64_t</span> <span>*</span>array<span>,</span> <span>mpz_t</span> mpz<span>,</span> <span>uint64_t</span> words<span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>,</span> j <span>=</span> words <span>-</span> <span>1</span><span>;</span> i <span>&lt;</span> j<span>;</span> <span>++</span>i<span>,</span> <span>--</span>j<span>)</span> <span>{</span>
</span><span>	    <span>uint64_t</span> temp <span>=</span> array<span>[</span>i<span>]</span><span>;</span>
</span><span>	    array<span>[</span>i<span>]</span> <span>=</span> array<span>[</span>j<span>]</span><span>;</span>
</span><span>	    array<span>[</span>j<span>]</span> <span>=</span> temp<span>;</span>
</span><span>    <span>}</span>
</span><span>    <span>mpz_import</span><span>(</span>mpz<span>,</span> words<span>,</span> <span>1</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> array<span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>size_t</span> <span>convert_radix_64_to_52</span><span>(</span><span>uint64_t</span> <span>*</span>limbs<span>,</span> <span>uint64_t</span> <span>*</span>out<span>,</span> <span>size_t</span> count<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> out_index <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>int</span> bits_in_buffer <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>uint128_t</span> buffer <span>=</span> <span>0</span><span>;</span>
</span><span>
</span><span>    <span>for</span> <span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> count<span>;</span> i<span>++</span><span>)</span> <span>{</span>
</span><span>        buffer <span>|=</span> <span>(</span><span>(</span><span>uint128_t</span><span>)</span>limbs<span>[</span>i<span>]</span><span>)</span> <span>&lt;&lt;</span> bits_in_buffer<span>;</span>
</span><span>        bits_in_buffer <span>+=</span> <span>64</span><span>;</span>
</span><span>
</span><span>        <span>while</span> <span>(</span>bits_in_buffer <span>&gt;=</span> <span>52</span><span>)</span> <span>{</span>
</span><span>            out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>            buffer <span>&gt;&gt;=</span> <span>52</span><span>;</span>
</span><span>            bits_in_buffer <span>-=</span> <span>52</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>// Handle remaining bits if any</span>
</span><span>    <span>if</span> <span>(</span>bits_in_buffer <span>&gt;</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> bits_in_buffer<span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>return</span> out_index<span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>size_t</span> <span>convert_radix_52_to_64</span><span>(</span><span>uint64_t</span> <span>*</span>in<span>,</span> <span>uint64_t</span> <span>*</span>out<span>,</span> <span>size_t</span> count<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> out_index <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>int</span> bits_in_buffer <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>uint128_t</span> buffer <span>=</span> <span>0</span><span>;</span>
</span><span>
</span><span>    <span>for</span> <span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> count<span>;</span> i<span>++</span><span>)</span> <span>{</span>
</span><span>        buffer <span>|=</span> <span>(</span><span>(</span><span>uint128_t</span><span>)</span>in<span>[</span>i<span>]</span><span>)</span> <span>&lt;&lt;</span> bits_in_buffer<span>;</span>
</span><span>        bits_in_buffer <span>+=</span> <span>52</span><span>;</span>
</span><span>
</span><span>        <span>while</span> <span>(</span>bits_in_buffer <span>&gt;=</span> <span>64</span><span>)</span> <span>{</span>
</span><span>            out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>(</span><span>uint128_t</span><span>)</span><span>1ULL</span> <span>&lt;&lt;</span> <span>64</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>            buffer <span>&gt;&gt;=</span> <span>64</span><span>;</span>
</span><span>            bits_in_buffer <span>-=</span> <span>64</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>// Handle remaining bits if any</span>
</span><span>    <span>if</span> <span>(</span>bits_in_buffer <span>&gt;</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> bits_in_buffer<span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>return</span> out_index<span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>__attribute__</span><span>(</span><span>(</span>always_inline<span>)</span><span>)</span> <span>void</span> <span>shift_down_1279</span><span>(</span>__m512i accum<span>[</span><span>7</span><span>]</span><span>,</span> __m512i high_1279<span>[</span><span>4</span><span>]</span><span>)</span> <span>{</span>
</span><span>	__m512i p <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>3</span><span>;</span> i <span>&gt;=</span> <span>0</span><span>;</span> <span>--</span>i<span>)</span> <span>{</span>
</span><span>		__m512i down_31 <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i <span>+</span> <span>3</span><span>]</span><span>,</span> <span>31</span><span>)</span><span>;</span>
</span><span>		__m512i higher_21 <span>=</span> <span>_mm512_slli_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i <span>+</span> <span>3</span><span>]</span><span>,</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>,</span> <span>21</span><span>)</span><span>;</span>
</span><span>		high_1279<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_alignr_epi64</span><span>(</span>p<span>,</span> higher_21<span>,</span> <span>1</span><span>)</span><span>,</span> down_31<span>)</span><span>;</span>
</span><span>		p <span>=</span> higher_21<span>;</span>
</span><span>	<span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>__attribute__</span><span>(</span><span>(</span>always_inline<span>)</span><span>)</span> <span>void</span> <span>filter_low_1279</span><span>(</span>__m512i accum<span>[</span><span>7</span><span>]</span><span>)</span> <span>{</span>
</span><span>	accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// This code is extremely latency bound, as you'd expect for this kind of stupid PoW, so certain things are done that would lower</span>
</span><span><span>// throughput (e.g. on a hyperthreaded device doing two of these at once) but which lower the latency</span>
</span><span><span>void</span> <span>the_powmod</span><span>(</span><span>uint64_t</span> <span>*</span> __restrict__ input<span>,</span> <span>uint64_t</span> <span>*</span> __restrict__ result<span>)</span> <span>{</span>
</span><span>	<span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> padded_data<span>[</span><span>8</span> <span>*</span> <span>6</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>}</span><span>;</span>
</span><span>	<span>uint64_t</span> <span>*</span>data <span>=</span> padded_data <span>+</span> <span>8</span><span>;</span>
</span><span>
</span><span>	__m512i clumps<span>[</span><span>4</span><span>]</span> <span>=</span> <span>{</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input<span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>8</span><span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>16</span><span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>24</span><span>)</span>
</span><span>	<span>}</span><span>;</span>
</span><span>
</span><span>	<span>for</span> <span>(</span><span>int</span> pow_i <span>=</span> <span>0</span><span>;</span> pow_i <span>&lt;</span> <span>1277</span><span>;</span> <span>++</span>pow_i<span>)</span> <span>{</span>
</span><span>		<span>// Use aligned stores to make sure we are doing things well</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Now data[x] gives us the xth limb</span>
</span><span><span><span>#</span><span>define</span> <span>ZERO</span> <span><span>_mm512_setzero_si512</span><span>(</span><span>)</span></span></span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>ELIDE_MASKS_IF_POSSIBLE</span> <span><span>1</span></span></span>
</span><span>
</span><span>	__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>	<span>// The accumulation is latency bound (lat. 4 cycles, so we need at least 8 accumulators to keep the madds in flight)</span>
</span><span>	__m512i accum_hi<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>	<span>// We'll laboriously build up the upper triangle of the 2560-bit product using 52x52-&gt;104 multiplies</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>100</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>24</span><span>;</span> i <span>&gt;=</span> <span>-</span><span>7</span><span>;</span> <span>--</span>i<span>)</span> <span>{</span>
</span><span>		<span>// Sliding window</span>
</span><span>		__m512i m1<span>;</span>
</span><span>		<span>if</span> <span>(</span><span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
</span><span>			m1 <span>=</span> clumps<span>[</span>i <span>/</span> <span>8</span><span>]</span><span>;</span>
</span><span>		<span>}</span> <span>else</span> <span>{</span>
</span><span>			<span>// Emulate an unaligned load from memory. Unaligned loads are very expensive on Zen 5 so this is helpful</span>
</span><span><span><span>#</span><span>define</span> <span>UNALIGNED</span><span><span>(</span>S<span>)</span> <span>case</span> S<span>:</span> <span>{</span> m1 <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>]</span><span>,</span> i <span>&lt;</span> <span>0</span> <span>?</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span> <span>:</span> clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>-</span><span>1</span><span>]</span><span>,</span> S<span>)</span><span>;</span> <span>break</span><span>;</span> <span>}</span></span></span>
</span><span>			<span>switch</span> <span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>{</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>1</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>2</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>3</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>4</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>5</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>6</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>7</span><span>)</span>
</span><span>				<span>default</span><span>:</span> <span>abort</span><span>(</span><span>)</span><span>;</span>
</span><span>			<span>}</span>
</span><span>		<span>}</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>100</span></span></span>
</span><span>		<span>for</span> <span>(</span><span>int</span> j <span>=</span> <span>0</span><span>,</span> k <span>=</span> <span>0</span><span>;</span> j <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>j<span>,</span> k <span>+=</span> <span>8</span><span>)</span> <span>{</span>
</span><span>			<span>// Decide whether to accumulate into accum[j], which should happen if there</span>
</span><span>			<span>// is at least one element shared between the jth accumulator and [i, i+7]</span>
</span><span>			<span>int</span> lo <span>=</span> k <span>-</span> i<span>;</span>
</span><span>			<span>int</span> hi <span>=</span> k <span>-</span> i <span>-</span> <span>1</span><span>;</span>
</span><span>			<span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>				<span>// Multiples of 8 are handled by a broadcast instead of a memory load for efficiency</span>
</span><span><span><span>#</span><span>define</span> <span>FOR_EACH_OFFS</span> <span><span>X</span><span>(</span><span>1</span><span>)</span> <span>X</span><span>(</span><span>2</span><span>)</span> <span>X</span><span>(</span><span>3</span><span>)</span> <span>X</span><span>(</span><span>4</span><span>)</span> <span>X</span><span>(</span><span>5</span><span>)</span> <span>X</span><span>(</span><span>6</span><span>)</span> <span>X</span><span>(</span><span>7</span><span>)</span> <span>X</span><span>(</span><span>9</span><span>)</span> <span>X</span><span>(</span><span>10</span><span>)</span> <span>X</span><span>(</span><span>11</span><span>)</span> <span>X</span><span>(</span><span>12</span><span>)</span> <span>X</span><span>(</span><span>13</span><span>)</span> <span>X</span><span>(</span><span>14</span><span>)</span> <span>X</span><span>(</span><span>15</span><span>)</span> <span>X</span><span>(</span><span>17</span><span>)</span> <span>X</span><span>(</span><span>18</span><span>)</span> <span>X</span><span>(</span><span>19</span><span>)</span> <span>X</span><span>(</span><span>20</span><span>)</span> <span>X</span><span>(</span><span>21</span><span>)</span> <span>X</span><span>(</span><span>22</span><span>)</span> <span>X</span><span>(</span><span>23</span><span>)</span></span></span>
</span><span>				<span>// Discard those entries where lo &gt; i</span>
</span><span>				__mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>// we use inline asm with a memory broadcast after enough regs because the register allocator does not enjoy this type of setup</span>
</span><span>				<span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span>  </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>					<span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>						FOR_EACH_OFFS
</span><span>						<span>default</span><span>:</span>
</span><span>						accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>					<span>}</span>
</span><span>
</span><span>				<span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>					<span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>						FOR_EACH_OFFS
</span><span>						<span>default</span><span>:</span>
</span><span>						accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span><span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>					<span>}</span>
</span><span>				<span>}</span>
</span><span>			<span>}</span>
</span><span>
</span><span>			<span>if</span> <span>(</span>hi <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> hi <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span><span><span>#</span><span>undef</span> <span>X</span></span>
</span><span>				__mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>hi <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>hi <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>// see above</span>
</span><span>				<span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52huq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>				<span>switch</span> <span>(</span>hi<span>)</span> <span>{</span>
</span><span>					FOR_EACH_OFFS
</span><span>					<span>default</span><span>:</span>
</span><span>					accum_hi<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52hi_epu64</span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>hi <span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span> <span>)</span><span>;</span>
</span><span>				<span>}</span>
</span><span>				<span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52huq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>				<span>switch</span> <span>(</span>hi<span>)</span> <span>{</span>
</span><span>					FOR_EACH_OFFS
</span><span>					<span>default</span><span>:</span>
</span><span>					accum_hi<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52hi_epu64</span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span><span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>hi <span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>}</span>
</span><span>				<span>}</span>
</span><span>			<span>}</span>
</span><span>
</span><span>		<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Fold high and low halves, and double all the accumulators</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>7</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum_hi<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now add the diagonal from the accumulators because they weren't yet computed</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		__m512d diag_lo <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52lo_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>		__m512d diag_hi <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52hi_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>		__m512i shuf_lo <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>11</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>2</span><span>,</span> <span>9</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>		__m512i shuf_hi <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>15</span><span>,</span> <span>7</span><span>,</span> <span>14</span><span>,</span> <span>6</span><span>,</span> <span>13</span><span>,</span> <span>5</span><span>,</span> <span>12</span><span>,</span> <span>4</span><span>)</span><span>;</span>
</span><span>	        accum<span>[</span><span>2</span> <span>*</span> i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_lo<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>		<span>if</span> <span>(</span>i <span>!=</span> <span>3</span><span>)</span> <span>{</span>
</span><span>			accum<span>[</span><span>2</span> <span>*</span> i <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>+</span><span>1</span><span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_hi<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>		<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now propagate carries in parallel in radix 2^52</span>
</span><span>	__m512i low_52_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>	__m512i hi_12_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>~</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Now add the high 1279 bits to the low 1279 bits</span>
</span><span>	__m512i high_1279<span>[</span><span>4</span><span>]</span><span>;</span>
</span><span>	<span>shift_down_1279</span><span>(</span>accum<span>,</span> high_1279<span>)</span><span>;</span>
</span><span>	<span>filter_low_1279</span><span>(</span>accum<span>)</span><span>;</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> high_1279<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>{</span>
</span><span>carry2<span>:</span><span>;</span>
</span><span>	__m512i carry_test <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span>	__m512i group_out <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>7</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		__m512i carries <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> <span>52</span><span>)</span><span>;</span>
</span><span>		__m512i carries_into <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>carries<span>,</span> group_out<span>,</span> <span>7</span><span>)</span><span>;</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> low_52_bits<span>)</span><span>,</span> carries_into<span>)</span><span>;</span>
</span><span>		group_out <span>=</span> carries<span>;</span>
</span><span>		carry_test <span>=</span> <span>_mm512_and_si512</span><span>(</span>carry_test<span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span> <span>// improve latency over a series of masked tests</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>if</span> <span>(</span><span>__builtin_expect</span><span>(</span><span>_mm512_test_epi64_mask</span><span>(</span>carry_test<span>,</span> hi_12_bits<span>)</span><span>,</span> <span>0</span><span>)</span><span>)</span> <span>{</span>
</span><span>		<span>goto</span> carry2<span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now compare with 2^1279 - 1; if &gt;=, subtract 2^1279 - 1. classic Mersenne number modulo algorithm</span>
</span><span>	__m512i bit_1279 <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span><span>;</span>
</span><span>	__m512i mask_off <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Branchless approach appears to save about 2 ns per iteration. Also, we stay in vector regs and don't use a test mask here because it tends to be slower</span>
</span><span>	__m512i cmp <span>=</span> <span>_mm512_and_epi64</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> bit_1279<span>)</span><span>;</span>
</span><span>	accum<span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>0</span><span>]</span><span>,</span> <span>_mm512_srli_epi64</span><span>(</span>cmp<span>,</span> <span>31</span><span>)</span><span>)</span><span>;</span>  <span>// potentially +1 to last word</span>
</span><span>	accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>mask_off<span>,</span> accum<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// TODO 1/2^52 chance of error here due to carry -- check it</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		clumps<span>[</span>i<span>]</span> <span>=</span> accum<span>[</span>i<span>]</span><span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>int</span> <span>main</span><span>(</span><span>int</span> argc<span>,</span> <span>char</span> <span>*</span><span>*</span>argv<span>)</span> <span>{</span>
</span><span>	<span>if</span> <span>(</span>argc <span>&lt;</span> <span>3</span><span>)</span> <span>{</span>
</span><span>		<span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"Usage: %s &lt;y&gt; &lt;difficulty&gt;"</span><span>,</span> argv<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>return</span> <span>1</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>mpz_t</span> x<span>,</span> r<span>;</span>
</span><span>	<span>mpz_inits</span><span>(</span>x<span>,</span> r<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>	<span>mpz_set_str</span><span>(</span>x<span>,</span> argv<span>[</span><span>1</span><span>]</span><span>,</span> <span>10</span><span>)</span><span>;</span>
</span><span>	<span>int</span> difficulty <span>=</span> <span>atoi</span><span>(</span>argv<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>uint64_t</span> abc<span>[</span><span>400</span><span>]</span><span>;</span>
</span><span>	<span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> poop<span>[</span><span>32</span><span>]</span><span>;</span>
</span><span>
</span><span>	<span>gmp_to_array</span><span>(</span>x<span>,</span> abc<span>)</span><span>;</span>
</span><span>
</span><span>	<span>size_t</span> N <span>=</span> <span>convert_radix_64_to_52</span><span>(</span>abc<span>,</span> poop<span>,</span> <span>20</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>uint64_t</span> squared<span>[</span><span>1000</span><span>]</span><span>;</span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> difficulty<span>;</span> <span>++</span>i<span>)</span> <span>{</span> <span>// specified algorithm</span>
</span><span>		<span>the_powmod</span><span>(</span>poop<span>,</span> squared<span>)</span><span>;</span>
</span><span>		squared<span>[</span><span>0</span><span>]</span> <span>^=</span> <span>1</span><span>;</span>
</span><span>		<span>memcpy</span><span>(</span>poop<span>,</span> squared<span>,</span> <span>25</span> <span>*</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>convert_radix_52_to_64</span><span>(</span>squared<span>,</span> abc<span>,</span> <span>48</span><span>)</span><span>;</span>
</span><span>	<span>array_to_gmp</span><span>(</span>abc<span>,</span> r<span>,</span> <span>1280</span><span>/</span><span>64</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>char</span> <span>*</span>str <span>=</span> <span>mpz_get_str</span><span>(</span><span>NULL</span><span>,</span> <span>10</span><span>,</span> r<span>)</span><span>;</span>
</span><span>	<span>printf</span><span>(</span><span>"%s"</span><span>,</span> str<span>)</span><span>;</span>
</span><span>	<span>return</span> <span>0</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>I hope you enjoyed my first-ever blog post. Hopefully there will be many more.</p><section data-footnotes="true"><h2 id="footnote-label">Footnotes</h2><ol><li id="user-content-fn-bounty"><p>$21,337 base reward, $10,000 for stability (successful exploitation on &gt;90% of runs), and $20,000 for a 0-day bug. <a href="#user-content-fnref-bounty" aria-label="Back to reference 1" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-thesis"><p>We had one shot at this; William's thesis was due in a few days and we wanted to include the exploit. <a href="#user-content-fnref-thesis" aria-label="Back to reference 2" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-hash"><p>Compare with proofs of work, for example, that request a SHA hash starting with some number of zeros. This process is embarrassingly parallel, so someone with many powerful GPUs could solve it in a fraction of the time as someone without. <a href="#user-content-fnref-hash" aria-label="Back to reference 3" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-rust"><p>The Rust implementation uses the exact same Mersenne trick, yet takes about 2.4 seconds; I assume this is FFI overhead? 🤷 <a href="#user-content-fnref-rust" aria-label="Back to reference 4" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-limbs"><p>Limbs are the term of art for the integer elements of an array that represents a big integer. On 64-bit systems, limbs are usually 64-bit unsigned integers. <a href="#user-content-fnref-limbs" aria-label="Back to reference 5" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-uops"><p>See <a target="_blank" rel="noopener noreferrer" href="https://uops.info/table.html">uops.info</a>, <em>imul r64</em> and <em>mulx r64, r64, r64</em>. <a href="#user-content-fnref-uops" aria-label="Back to reference 6" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-numberworld"><p>The same performance problem was observed by y-cruncher author Alexander Yee <a target="_blank" rel="noopener noreferrer" href="https://www.numberworld.org/y-cruncher/news/2024.html">here</a>, section "Example 2: Everything Blows Up". <a href="#user-content-fnref-numberworld" aria-label="Back to reference 7" data-footnote-backref="">↩</a></p></li></ol></section></div></div></div></article></section></main></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Toxic Origins, Toxic Decisions: Biases in CEO Selection (102 pts)]]></title>
            <link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5270031</link>
            <guid>44137542</guid>
            <pubDate>Fri, 30 May 2025 16:01:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5270031">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5270031</a>, See on <a href="https://news.ycombinator.com/item?id=44137542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

	
	
		
	

	
	
		
		
	

	
	

	
	
	
	
	
    
        
    
	

	
	<p>
		
		
			<span>90 Pages</span>
		
		

		<span>Posted: 27 May 2025</span>
		
		
		
	</p>
	
		<div>
			
			




	<h2><a href="https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=58471" target="_blank" title="View other papers by this author">P. Raghavendra Rau</a></h2><p>University of Cambridge; European Corporate Governance Institute (ECGI)</p><h2><a href="https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=1418528" target="_blank" title="View other papers by this author">YiLin Wu</a></h2><p>National Taiwan University - Department of Economics</p><h2><a href="https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=7066396" target="_blank" title="View other papers by this author">Richard Lok-Si Ieong</a></h2><p>City University of New York, Baruch College - Zicklin School of Business - Department of Economics and Finance</p>

		</div>
	
	
	
	
	



	
	
	
	
		
		
	
	
		
			<p>Date Written: May 01, 2025</p>
		
	

	
	
		
		
	

	
	
		
	
	<div>
		<h3>Abstract</h3>
		<p>We examine how selection bias in CEO promotion amplifies risk-taking, using prenatal exposure to pollution as an exogenous shock to individual risk preferences. CEOs born in future Superfund sites are more likely to be promoted internally, suggesting firms reward observed success without recognizing underlying risk tolerance. These "Superfund CEOs" excel in internal roles but pursue riskier external policies once promoted-leading to greater volatility and weaker performance. Our results suggest firms may systematically mistake luck for skill in promotion decisions, filtering for high-variance risk-takers whose traits only become problematic when decision-making shifts to exposed, irreversible domains. In short, Superfund CEOs display performance with essentially lower mean but higher variance. </p>
	</div>
	

	<center>
		
		


	

	
	

		
		

	
	


	</center>

	
	
	
		
	
	
	

	
	
		
		
			
				
			
		
		<p><strong>Keywords:</strong> CEO selection, Risk-taking, Superfund, Environmental risk, Developmental toxicity, Fetal origins hypothesis</p>
	
	

	
    
    
	

	
	
		<p><strong>JEL Classification:</strong> D22, D90, D91, I10, Q50, Q53</p>
	
	

	
	

























    	
	
	
	
   	
	
	
	
	
	            
	















	












    









    
    

    

    

    

    

    

    

















	



    




	<p>
		<strong>Suggested Citation:</strong>
		<a href="#">Suggested Citation<i></i></a>
	</p>
	<p>
		
			Rau,  P. Raghavendra and Wu,  YiLin and Ieong,  Richard Lok-Si, Toxic Origins, Toxic Decisions: Biases in CEO Selection (May 01, 2025). HKU Jockey Club Enterprise Sustainability Global Research Institute Paper No. 2025/052,  Available at SSRN: <a href="https://ssrn.com/abstract=5270031" target="_blank">https://ssrn.com/abstract=5270031</a> or <a href="https://dx.doi.org/10.2139/ssrn.5270031" target="_blank">http://dx.doi.org/10.2139/ssrn.5270031 </a>
		
	</p>
	

	
	
	

	
	

	
	
		
	

	
	
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[De Bruijn notation, and why it's useful (131 pts)]]></title>
            <link>https://blueberrywren.dev/blog/debruijn-explanation/</link>
            <guid>44137439</guid>
            <pubDate>Fri, 30 May 2025 15:51:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blueberrywren.dev/blog/debruijn-explanation/">https://blueberrywren.dev/blog/debruijn-explanation/</a>, See on <a href="https://news.ycombinator.com/item?id=44137439">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


      



  <meta property="og:title" content="Debruijn indexes + levels, and why they're handy">
  <title> Debruijn indexes + levels, and why they're handy </title>






<h2 id="de-bruijn-and-why-we-use-it">De Bruijn and why we use it</h2>
<h2 id="assumed-knowledge">Assumed knowledge</h2>
<p>At least a familiarity with the lambda calculus, including how it is evaluated. Some base knowledge of programming languages is also assumed.</p>
<h2 id="the-problem">The problem</h2>
<p>Let's look at a little imaginary term, in some lambda-calculus-like language. For future note, we call the lambda a "binder", as it binds a variable. There are other types of binders, e.g. <code>let</code>, but we will only consider lambdas for the moment.</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ f</span><span>.</span><span> (λ y f</span><span>.</span><span> (f y)) f
</span></code></pre>
<p>We can perform what's called "beta reduction" on this term — essentially, function application, applying <code>f</code> to the lambda.</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ f</span><span>.</span><span> (λ y f</span><span>.</span><span> (f y)) (f)
</span><span>
</span><span>substitute [y </span><span>:=</span><span> f] </span><span>in</span><span> λ y f</span><span>.</span><span> (f y)
</span><span>
</span><span>λ f</span><span>.</span><span> (λ f</span><span>.</span><span> f f)
</span></code></pre>
<p>Uh oh. We've accidentally captured a variable! Instead of <code>f</code> referring to the outer <code>f</code>, now it refers to the inner <code>f</code>. This is "the capture problem", and it is quite annoying. Generally to avoid this, we need to rename <em>everything</em><sup id="fr-1-1"><a href="#fn-1">1</a></sup> in our "substituted" term to names that are free (do not occur) in the "subsitutee" so nothing is accidentally captured. What if we could introduce a notation that avoids this?</p>
<h2 id="presenting-de-bruijn-indexes">Presenting: De Bruijn Indexes!</h2>
<p>De Bruijn indexes are a naming scheme where:</p>
<ul>
<li>We use natural numbers to refer to lambdas, and</li>
<li>Zero refers to the "most recent" lambda; one refers to the second most recent, etc.</li>
</ul>
<p>Let's rewrite that term above using this system:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ (λ λ (</span><span>0 1</span><span>)) </span><span>0
</span></code></pre>
<p>Here's some ascii art showing what refers to what:</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ   (λ   λ   (</span><span>0   1</span><span>))   </span><span>0
</span><span> </span><span>|    |    \</span><span>———</span><span>/   |     |
</span><span> </span><span>|    |            |     |
</span><span> </span><span>|    \</span><span>————————————</span><span>/     |
</span><span> </span><span>\</span><span>———————————————————————</span><span>/
</span></code></pre>
<p>Now, how does this help us with our substituion problem? Surely if we naively subtitute we will still have binding issues - and indeed we do:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λ (λ λ (</span><span>0 1</span><span>)) </span><span>0
</span><span>-&gt;
</span><span>λ (λ (</span><span>0 0</span><span>))
</span></code></pre>
<p>No good!</p>
<p>What De Bruijn indexes allow us to do is simply avoid capturing. The rule is simple: Every time we go past a binder when substituting, we increment every free variable<sup id="fr-2-1"><a href="#fn-2">2</a></sup> in our substituted term by one, to avoid the new binder. Just once, we decrement every free varible in the substitutee, to account for the removal of the binder:</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ (λ λ (</span><span>1 2</span><span>)) </span><span>0
</span><span> ^  ^ ^
</span><span> a  b c
</span><span> </span><span>-&gt;
</span><span> λ (λ (</span><span>1 1</span><span>))
</span><span> ^  ^  ^ ^ 
</span><span> a  b  </span><span>| \</span><span> decremented by one
</span><span>       </span><span>|
</span><span>       </span><span>\</span><span> incremented by one when we passed </span><span>"through"</span><span> lambda c
</span></code></pre>
<p>Now we're cool! Everything works as expected, and it takes much less work (and is much more predictable!).</p>
<p>At the bottom of this post there's a little widget that can convert terms to de Bruijn for you, if you want to play around!</p>
<h2 id="presenting-de-bruijn-levels">Presenting: De Bruijn levels!</h2>
<p>De Bruijn levels work similar to De Bruijn indexes, in that we use numbers to refer to binders. However, in De Bruijn levels, the lowest number refers to the <em>least</em> recently bound item.</p>
<p>Recall that:</p>
<pre data-lang="hs"><code data-lang="hs"><span>Named</span><span>:</span><span>   λ f</span><span>.</span><span> (λ y f</span><span>.</span><span> (f y)) f
</span><span>Indexes</span><span>:</span><span> λ (λ λ (</span><span>0 1</span><span>)) </span><span>0
</span></code></pre>
<p>Now, with levels:</p>
<pre data-lang="hs"><code data-lang="hs"><span>Levels</span><span>:</span><span>  λ (λ λ (</span><span>2 1</span><span>)) </span><span>0
</span></code></pre>
<p>This has the same diagram of what refers to what:</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ   (λ   λ   (</span><span>2   1</span><span>))   </span><span>0
</span><span> </span><span>|    |    \</span><span>———</span><span>/   |     |
</span><span> </span><span>|    |            |     |
</span><span> </span><span>|    \</span><span>————————————</span><span>/     |
</span><span> </span><span>\</span><span>———————————————————————</span><span>/
</span></code></pre>
<p>(As it should! These two representations represent the same term.)</p>
<p>As you might expect, de Bruijn indexes and levels are each beneficial in their own situations.</p>
<p>Generally, De Bruijn indexes are "more useful" than De Bruijn levels, as they're "more local". In order to work with levels, you need to know "how deep" you are in a term at all times.</p>
<p>De Bruijn indexes give us the advantage that we can freely create new binders without the need for any information about where in a term we are, whereas de Bruijn levels give us the advantage when moving a term under a binder, free variables in said term do not need to be modified. Generally, one has many more free variables in a term than bound ones.</p>
<pre data-lang="hs"><code data-lang="hs"><span> λ (λ λ (</span><span>2 1</span><span>)) </span><span>0
</span><span>               ^ this zero</span><span>...
</span><span> </span><span>-&gt;
</span><span> λ (λ (</span><span>1 0</span><span>))
</span><span>       ^ ^ is still zero</span><span>!
</span><span>       </span><span>|
</span><span>       </span><span>\</span><span> we had to modify this one though
</span></code></pre>
<h2 id="other-advantages">Other advantages</h2>
<p>Something that can come up quite a lot in various contexts is comparing whether two terms are equal or not. There are many complicated ways to do so, but de Bruijn gives us an advantage in a critical one, called "alpha-equivalence". Consider the following two terms:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λf</span><span>.</span><span> λx</span><span>.</span><span> f x
</span><span>λg</span><span>.</span><span> λy</span><span>.</span><span> g y
</span></code></pre>
<p>These terms should clearly be equal, right? They do the exact same thing. In this case, we consider them "alpha-equivalent", meaning they are equal up to the names of variables. Alpha renaming is the process of renaming one term to match the names of another, so that they are "clearly" equal.</p>
<p>Let us consider the de Bruijn index representation of both of these terms:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λf</span><span>.</span><span> λx</span><span>.</span><span> f x </span><span>=&gt;</span><span> λ λ </span><span>1 0
</span><span>λg</span><span>.</span><span> λy</span><span>.</span><span> g y </span><span>=&gt;</span><span> λ λ </span><span>1 0
</span></code></pre>
<p>Isn't that nice? They've gone from being alpha-equivalent, but not quite equal, to being equal. de Bruijn gives us the ability to compare terms for equality without having to consider alpha-equivalence at all.</p>
<h2 id="wrapup">Wrapup</h2>
<p>De Bruijn indexes and levels can also be summed up via the following:</p>
<pre data-lang="hs"><code data-lang="hs"><span>λa</span><span>.</span><span> λb</span><span>.</span><span> λc</span><span>.</span><span> c
</span><span>
</span><span>indexes</span><span>:
</span><span>λ λ λ </span><span>0
</span><span>^ ^ ^
</span><span>2 1 0
</span><span>
</span><span>levels</span><span>:
</span><span>λ λ λ </span><span>2
</span><span>^ ^ ^
</span><span>0 1 2
</span></code></pre>
<p>if you’re “here”</p>
<pre><code><span>  v
</span><span>λ λ λ
</span></code></pre>
<ul>
<li>Using indexes, adding any binders further left doesn’t affect the current binder's variables, or any further right.</li>
<li>Using levels, adding any binders further right doesn’t affect the current binder's variables, or any further left.</li>
</ul>

<p>Try it out! Some example terms to try:</p>
<pre><code><span>\f x. f x
</span><span>\x x. x
</span><span>\x. (x (\y. y))
</span><span>
</span><span>\x. \y. y x -- will not work
</span><span>do \x. (\y. y x) or \x y. y x instead
</span></code></pre>
<p>(sorry, it does over-parenthesize a bit :P)</p>
<pre data-lang="hs"><code data-lang="hs" id="goober">
</code>

<h2 id="alternatives">Alternatives</h2>
<p>It is worth noting that there are several other methods for gaining the same, or similar, advantages as de Bruijn gives. This post is not intended to explain them, but I will list several here so that the curious reader may read further (tip: when searching, append "lambda calculus" to find the right results quicker):</p>
<ul>
<li>HOAS, or "Higher Order Abstract Syntax"</li>
<li>PHOAS, or "Parametric HOAS"</li>
<li>Locally nameless</li>
<li>Nominal signatures</li>
<li>Well-scoped de Bruijn indices</li>
<li>Well-scoped names</li>
<li><a href="http://doi.acm.org/10.1145/2034773.2034817">“Nameless,
Painless”</a></li>
<li>Abstract scope graphs</li>
<li>Abstract Binding Trees</li>
<li>Co-de Bruijn indices</li>
</ul>
<p>As you can see, there are many approaches! Jesper Cockx has an excellent summary of almost all of these, which can be found <a href="https://jesper.sikanda.be/posts/1001-syntax-representations.html">here.</a> Notably, many are intended for formalization efforts rather than for computational usage.</p>






      </pre></div></div>]]></description>
        </item>
    </channel>
</rss>