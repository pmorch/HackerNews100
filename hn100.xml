<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 28 Jan 2025 12:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[DeepSeek-R1 with Dynamic 1.58-bit Quantization (281 pts)]]></title>
            <link>https://unsloth.ai/blog/deepseekr1-dynamic</link>
            <guid>42850222</guid>
            <pubDate>Tue, 28 Jan 2025 08:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unsloth.ai/blog/deepseekr1-dynamic">https://unsloth.ai/blog/deepseekr1-dynamic</a>, See on <a href="https://news.ycombinator.com/item?id=42850222">Hacker News</a></p>
Couldn't get https://unsloth.ai/blog/deepseekr1-dynamic: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Open-R1: an open reproduction of DeepSeek-R1 (275 pts)]]></title>
            <link>https://huggingface.co/blog/open-r1</link>
            <guid>42849536</guid>
            <pubDate>Tue, 28 Jan 2025 06:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/open-r1">https://huggingface.co/blog/open-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42849536">Hacker News</a></p>
Couldn't get https://huggingface.co/blog/open-r1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Learning in Production (CMU Course) (312 pts)]]></title>
            <link>https://mlip-cmu.github.io/s2025/</link>
            <guid>42847834</guid>
            <pubDate>Tue, 28 Jan 2025 01:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mlip-cmu.github.io/s2025/">https://mlip-cmu.github.io/s2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42847834">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">

<h2>Machine Learning in Production (17-445/17-645/17-745) / AI Engineering (11-695)</h2>
<h3>Spring 2025</h3>
<p><em>CMU course that covers how to build, deploy, assure, and maintain software products with machine-learned models. Includes the entire lifecycle from a prototype ML model to an entire system deployed in production. Covers also <strong>responsible AI</strong> (including safety, security, fairness, explainability) and <strong>MLOps</strong>. For earlier offerings see websites for&nbsp;<a href="https://ckaestne.github.io/seai/F2019">Fall 2019</a>,&nbsp;<a href="https://ckaestne.github.io/seai/S2020">Summer 2020</a>, <a href="https://ckaestne.github.io/seai/F2020/">Fall 2020</a>, <a href="https://ckaestne.github.io/seai/S2021/">Spring 2021</a>&nbsp;&nbsp;<a href="https://ckaestne.github.io/seai/S2022/">Spring 2022</a>, <a href="https://ckaestne.github.io/seai/F2022/">Fall 2022</a>,&nbsp;<a href="https://github.com/mlip-cmu/s2023">Spring 2023</a>, <a href="https://github.com/mlip-cmu/s2024">Spring 2024</a>, and <a href="https://github.com/mlip-cmu/f2024">Fall 2024</a>. This Spring 2025 offering is designed for students with some data science experience (e.g., has taken a machine learning course, has used sklearn) and basic programming skills (e.g., basic Python programming with libraries, can navigate a Unix shell), but will not expect a software engineering background (i.e., experience with testing, requirements, architecture, process, or teams is not required). Going forward we expect to offer this course at least every spring semester and possibly some fall semesters (not summer semesters).</em></p>
<p><strong>Waitlist: we often cannot accommodate all interested students in Spring semesters, though we expect there to be waitlist movement.  We encourage students who are able to move to alternative labs with space, to do so.  For students enrolled in 17-XXX numbers, contact Jenni Cooper (<a href="mailto:cooperj@andrew.cmu.edu">cooperj@andrew.cmu.edu</a>) for assistance; for students enrolled in 11-XXX numbers, contact Amber Vivis (<a href="mailto:albrown@andrew.cmu.edu">albrown@andrew.cmu.edu</a>) and Karen Kirk (<a href="mailto:karensuk@andrew.cmu.edu">karensuk@andrew.cmu.edu</a>).  Note that the instructors cannot help with waitlist/registration movement, please contact the course admins instead!</strong>  </p>
<hr>
<p>For researchers, educators, or others interested in this topic, we share all course material, including slides and assignments, under a creative commons license on GitHub (<a href="https://github.com/mlip-cmu">https://github.com/mlip-cmu</a>) and have also published a <a href="https://mlip-cmu.github.io/book/">textbook</a> with chapters corresponding to almost every lecture. A while ago we also wrote an article describing the rationale and the initial design of this course: <a href="https://arxiv.org/abs/2001.06691">Teaching Software Engineering for AI-Enabled Systems</a>. Video recordings of the Summer 2020 offering are online on the <a href="https://ckaestne.github.io/seai/S2020/#course-content">course page</a>, though they are a bit outdated by now. We would be happy to see this course or a similar version taught at other universities. See also an <a href="https://github.com/ckaestne/seaibib">annotated bibliography</a> on research in this field.</p>
<h2>Course Description</h2>
<p>This is a course for those who want to build <strong>software products</strong> with <strong>machine learning</strong>, not just models and demos. We assume that you can train a model or build prompts to make predictions, but what does it take to turn the model into a product and actually deploy it, have confidence in its quality, and successfully operate and maintain it at scale? </p>
<p>The course is designed to establish a working relationship between <strong>software engineers</strong> and <strong>data scientists</strong>: both contribute to building ML-enabled systems but have different expertise and focuses. To work together they need a mutual understanding of their roles, tasks, concerns, and goals and build a working relationship. This course is aimed at <strong>software engineers</strong> who want to build robust and responsible products meeting the specific challenges of working with ML components and at <strong>data scientists</strong> who want to understand the requirements of the model for production use and want to facilitate getting a prototype model into production; it facilitates communication and collaboration between both roles. The course is a good fit for student looking at a career as an <strong>ML engineer</strong>. <em>The course focuses on all the steps needed to turn a model into a production system in a responsible and reliable manner.</em></p>
<p><img src="https://mlip-cmu.github.io/s2025/overview.svg" alt="Course overview"></p>
<p>It covers topics such as:</p>
<ul>
<li><strong>How to design for wrong predictions the model may make?</strong> How to assure <em>safety</em> and <em>security</em> despite possible mistakes? How to design the <em>user interface</em> and the entire system to operate in the real world?</li>
<li><strong>How to reliably deploy and update models in production?</strong> How can we <em>test</em> the entire machine learning pipeline? How can <em>MLOps</em> tools help to automate and scale the deployment process? How can we <em>experiment in production</em> (A/B testing, canary releases)? How do we detect <em>data quality</em> issues, <em>concept drift</em>, and <em>feedback loops</em> in production?</li>
<li><strong>How to scale production ML systems?</strong> How do we design a system to process huge amounts of training data, telemetry data, and user requests? Should we use stream processing, batch processing, lambda architecture, or data lakes?</li>
<li><strong>How to test and debug production ML systems?</strong> How can we <em>evaluate</em> the quality of a model’s predictions in production? How can we <em>test</em> the entire ML-enabled system, not just the model? What lessons can we learn from <em>software testing</em>, <em>automated test case generation</em>, <em>simulation</em>, and <em>continuous integration</em> for testing for production machine learning?</li>
<li><strong>Which qualities matter beyond a model’s prediction accuracy?</strong> How can we identify and measure important quality requirements, including <em>learning and inference latency, operating cost, scalability, explainablity, fairness, privacy, robustness</em>, and <em>safety</em>? Does the application need to be able to <em>operate offline</em> and how often do we need to update the models? How do we identify what’s important in a ML-enabled product in a production setting for a business? How do we resolve <em>conflicts</em> and <em>tradeoffs</em>?</li>
<li><strong>How to work effectively in interdisciplinary teams?</strong> How can we bring data scientists, software engineers, UI designers, managers, domain experts, big data specialists, operators, legal council, and other roles together and develop a <em>shared understanding</em> and <em>team culture</em>?</li>
</ul>
<p><strong>Examples and case studies</strong> of ML-driven products we discuss include automated audio transcription; distributed detection of missing children on webcams and instant translation in augmented reality; cancer detection, fall detection, COVID diagnosis, and other smart medical and health services; automated slide layout in Powerpoint; semi-automated college admissions; inventory management; smart playlists and movie recommendations; ad fraud detection; delivery robots and smart driving features; and many others.</p>
<p>An extended group project focuses on building, deploying, evaluating, and maintaining a robust and scalable <em>movie recommendation service</em> under somewhat realistic “production” conditions with 1 million users.</p>
<h3>Learning Outcomes</h3>
<p>After taking this course, among others, students should be able to</p>
<ul>
<li>analyze tradeoffs for designing production systems with ML-components, analyzing various qualities beyond accuracy such as operation cost, latency, updateability, and explainability</li>
<li>plan for mistakes in ML components and implement production-quality systems that are robust to those mistakes</li>
<li>design fault-tolerant and scalable data infrastructure for learning models, serving models, versioning, and experimentation</li>
<li>ensure quality of the entire machine learning pipeline with test automation and other quality assurance techniques, including automated checks for data quality, data drift, feedback loops, and model quality</li>
<li>build systems that can be tested and monitored in production and build robust deployment pipelines</li>
<li>consider system-level requirements such as safety, security, privacy, fairness, and usability when building complex ML-enabled products</li>
<li>communicate effectively in interdisciplinary teams</li>
</ul>
<p>In addition, students will gain familiarity with production-quality infrastructure tools, including stream processing with Apache Kafka, test automation with Jenkins, monitoring with Prometheus and Grafana, and deployment with Docker and various MLOps tools.</p>
<h2>Logistics and People</h2>
<p>17-445/17-645/17-745, 12 Units</p>
<p>The course is the same under all course numbers, except for the PhD-level 17-745 number, which replaces two homework assignments with a mandatory <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/research_project.md">research project</a>.</p>
<p>Open to all undergraduate and graduate students meeting the prerequisites.</p>
<h3>Spring 2025</h3>
<p>Lectures Monday/Wednesday 2:00-3:20pm, in person, PH 100</p>
<p>Labs Friday 9:30-10:50am in PH 226C (A) and SH 236 (B) and 11-12:20pm in PH A22 (C) and PH 226A (D) and 2-3:20 in PH 226C (E) and TEP 1308 (F). There is also a remote only lab (G), Friday 11:00-12:20 pm. </p>
<p>Instructors: <a href="https://www.cs.cmu.edu/~clegoues">Claire Le Goues</a> and <a href="https://austinhenley.com/">Austin Henley</a></p>
<p>TAs: </p>
<h3>Coordination</h3>
<p>We are happy to answer questions by email and over Slack, meet in person, and will jump on a quick Zoom call if you ask us. We also always arrive 5 to 10 min early to class and stay longer for discussions and questions. If you have questions about assignments and logistics, we prefer that you ask them publicly on Slack.</p>
<h2>Course content</h2>
<p>The general course content has been fairly stable over the last few years, though specific topics and tools are constantly updated with new research and tooling. Our list of learning goals under <a href="https://github.com/mlip-cmu/s2025/blob/main/learning_goals.md">Learning Goals</a> describes what we aim to cover. Below is a table of a preliminary schedule. This is subject to change and will be updated as the semester progresses, especially to help focus on requested topics or support learning.</p>
<p>[Schedule]</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Topic</th>
<th><a href="https://mlip-cmu.github.io/book/">Book Chapter</a></th>
<th>Reading</th>
<th>Assignment due</th>
</tr>
</thead>
<tbody><tr>
<td>Mon, Jan 13</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.html">Introduction and Motivation </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/01_introduction/intro.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/01/">1</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 15</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.html">From Models to AI-Enabled Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/02_systems/systems.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/02/">2</a>,<a href="https://mlip-cmu.github.io/book/04/">4</a>,<a href="https://mlip-cmu.github.io/book/05/">5</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 4, 5, 7, 8</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 17</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab01.md">Calling, securing, and creating APIs: Flask</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 20</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> MLK Day, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 22</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.html">Gathering Requirements </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/03_requirements/requirements.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/06/">6</a></td>
<td><a href="https://scholar.google.com/scholar?cluster=1090758480873197042">The World and the Machine</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 24</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab02.md">Stream processing: Apache Kafka</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 27</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.html">Planning for Mistakes</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/04_mistakes/mistakes.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/07/">7</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 6, 8, 24</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">I1: ML Product</a></td>
</tr>
<tr>
<td>Wed, Jan 29</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.html">Model Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/05_modelaccuracy/modelquality1.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 19</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 31</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab03.md">Collaboration with git</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 3</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.html">Fostering Interdisciplinary (Student) Teams</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/06_teamwork/teams.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="https://third-bit.com/2018/05/11/meetings/">Meetings</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I2_requirements.md">I2: Requirements</a></td>
</tr>
<tr>
<td>Wed, Feb 5</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.html">Behavioral Model Testing</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/07_modeltesting/modelquality2.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://aclanthology.org/2020.acl-main.442.pdf">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 7</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab04.md">Model testing</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.html">Toward Architecture and Design </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/08_architecture/tradeoffs.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/08/">8</a>,<a href="https://mlip-cmu.github.io/book/09/">9</a>,<a href="https://mlip-cmu.github.io/book/11/">11</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems, Ch. 18</a> and <a href="https://hackernoon.com/choosing-the-right-machine-learning-algorithm-68126944ce1f">Choosing the Right Machine Learning Algorithm</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.html">Deploying a Model</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/09_deploying_a_model/deployment.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/10/">10</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 13 and <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/1feg4j8/alma991019735160604436">Machine Learning Design Patterns</a>, Pat. 16</td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab05.md">Containers: Docker</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.html">Testing and Experimenting in Production</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/10_qainproduction/qainproduction.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Chs. 14 and 15</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M1: Modeling and First Deployment</a></td>
</tr>
<tr>
<td>Wed, Feb 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.html">Data Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/11_dataquality/dataquality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/16/">16</a></td>
<td><a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445518">Data Cascades in High-Stakes AI</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab06.md">Continuous Integration: Jenkins</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.html">Automating and Testing ML Pipelines</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/12_pipelinequality/pipelinequality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/11/">11</a>,<a href="https://mlip-cmu.github.io/book/18/">18</a>,<a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46555.pdf">The ML Test Score</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 26</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 1</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 28</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> No lab (happy spring break)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 3</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 5</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 7</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.html">Scaling the System</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/13_dataatscale/dataatscale.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/12/">12</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019577936304436">Big Data: Principles and best practices of scalable realtime data systems</a>, Ch. 1</td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.html">Planning for Operations</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/14_operations/operations.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/13/">13</a></td>
<td><a href="https://arxiv.org/abs/2209.09125">Operationalizing machine learning: An interview study</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab07.md">Monitoring: Prometheus, Grafana</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.html">Versioning, Provenance, and Reproducability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/15_provenance/provenance.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/24/">24</a></td>
<td><a href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden Technical Debt in Machine Learning Systems</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M2: Infrastructure Quality</a></td>
</tr>
<tr>
<td>Wed, Mar 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.html">Process &amp; Technical Debt</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/16_process/process.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/20/">20</a></td>
<td><a href="https://arxiv.org/pdf/2110.10234.pdf">Collaboration Challenges in Building ML-Enabled Systems</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab08.md">Pipeline automation: MLFlow</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.html">Intro to Ethics + Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/17_intro_ethics_fairness/intro-ethics-fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/23/">23</a>,<a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://datasociety.net/wp-content/uploads/2018/04/Data_Society_Algorithmic_Accountability_Primer_FINAL-4.pdf">Algorithmic Accountability: A Primer</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I3_mlops_tools.md">I3: MLOps Tools</a></td>
</tr>
<tr>
<td>Wed, Mar 26</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.html">Measuring Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/18_fairness_measures/model_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://dl.acm.org/doi/pdf/10.1145/3178876.3186138">Human Perceptions of Fairness in Algorithmic Decision Making</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 28</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab09.md">Container orchestration: Kubernetis</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 31</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.html">Building Fairer Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/19_system_fairness/system_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="http://users.umiacs.umd.edu/~hal/docs/daume19fairness.pdf">Improving Fairness in Machine Learning Systems</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 2</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.html">AVAILABLE</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/20_explainability/explainability.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/25/">25</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 4</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Carnival, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 7</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.html">Explainability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/21_transparency/transparency.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/29/">29</a></td>
<td><a href="https://dataskeptic.com/blog/episodes/2020/black-boxes-are-not-required">Interpretability Podcast</a> or <a href="https://arxiv.org/abs/1811.10154">equivalent artice</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M3: Monitoring and CD</a></td>
</tr>
<tr>
<td>Wed, Apr 9</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.html">Transparency &amp; Accountability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/22_security/security.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/28/">28</a></td>
<td><a href="https://pair.withgoogle.com/chapter/explainability-trust/">Google chapter on Explainability and Trust</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 11</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab10.md">Model Explainability Tools</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 14</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.html">Security and Privacy</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/23_safety/safety.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/27/">27</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 25, and <a href="https://canvas.cmu.edu/courses/45008/files/12156923/download?wrap=1">The Top 10 Risks of Machine Learning Security</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I4_explainability.md">I4: Explainability</a></td>
</tr>
<tr>
<td>Wed, Apr 16</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.html">Safety</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/24_summary/all.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="http://ceur-ws.org/Vol-2560/paper40.pdf">Practical Solutions for Machine Learning Safety in Autonomous Vehicles</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 18</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab11.md">LLM Jailbreaking</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 21</td>
<td>Explainability Discussion / Summary / Review</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 23</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 2</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 25</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> No lab</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M4: Fairness, Security and Feedback Loops</a></td>
</tr>
<tr>
<td>TBD</td>
<td>Final Project Presentations (5:30-8:30pm in GHC 4401)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">Final report</a></td>
</tr>
</tbody></table>
<h2>Course Syllabus and Policies</h2>
<p>The course uses Canvas and Gradescope for homework submission, grading, discussion, questions, announcements, and supplementary documents; slides will be posted here; Slack is used for communication around homework and projects; Github is used to coordinate group work. All public course material (assignments, slides, syllabus) can be found in the course’s <a href="https://github.com/mlip-cmu/s2025">GitHub repository</a>; announcements and all <em>private</em> material (e.g., grades, passwords) will be shared through Canvas.</p>
<p><strong>Prerequisites:</strong> The course does not have formal prerequisites, but we describe background knowledge that will help you be successful in the course. In a nutshell, we expect basic exposure to machine learning and basic programming skills, but do not require software engineering experience. </p>
<p><em>Machine learning (some experience recommended):</em> We suggest that you have basic familiarity with the process of extracting features, building and evaluating models, and a basic understanding of how and when different kinds of learning techniques work. Familiarity with Python and Jupyter notebooks is helpful. Courses such as 10-301, 10-315, and 05-434 will prepare you well, but project experience or self-learning from books or online courses will likely be sufficient for our purposes. For example, if you have no prior experience, we recommend the book <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019665684604436">Hands-On Machine Learning</a> to get practical experience in building and evaluating models prior to taking this course. We have set up a <em><a href="https://forms.gle/JcS61Uao7wHSFQen8">prerequisite knowledge check</a></em> as a Google Form, where we ask 10 questions on machine learning, which help you assess your background – this is set up as an anonymous and ungraded quiz, where you can compare your knowledge against what we believe is useful for you to be successful in this course (click on <em>“view score”</em> after submitting your answer). After submitting your answers, the system will give specific pointers to readings and exercises that may help you fill gaps in background knowledge. </p>
<p><em>Programming (basic proficiency required):</em> The course has a substantial programming component, especially in the first assignment and the team project, so basic programming skills will be needed. If you take the course without programming experience, you will significantly struggle and it may cause conflicts within the group project. We expect that you meet the following criteria: (1) basic fluency in a programming language like Python, (2) ability to install and learn libraries in that language, (3) ability to ssh into a Unix machine and perform basic command line operations, and (4) ability to install and learn new tools like Docker. We do not prescribe a programming language, but almost all student teams decide to work primarily in Python. We will provide some introductions and examples for essential tools like Git, Docker, Grafana, and Jenkins in labs, but we expect that you will be able to pick up new tools and libraries on your own. For example, we expect that you will be able, on your own, to learn basic use of a library like <a href="https://flask.palletsprojects.com/en/2.1.x/">Flask</a> to write a web service. Throughout the semester, expect to read lots of documentation and tutorials to learn various libraries and tools on your own. If you are worried whether your technical background is sufficient, we recommend that you look at (or even try) <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">homework I1</a> before the semester.</p>
<p><em>Software engineering (no experience required):</em> Many students will have some software engineering experience beyond basic programming skills from software engineering courses, from internships, or from working in industry, for example experience with requirements engineering, software design, software testing, distributed systems, continuous deployment, or managing teams. No such experience is expected as a prerequisite; we will cover these topics in the course.</p>
<p>Email the instructors if you would like to further talk to us about prerequisites.</p>
<p><strong>In-person teaching and lecture recordings:</strong> The course will be taught in person.  We consider in-class participation an important part of the learning experience. We <em>do</em> make <em>best effort</em> lecture recordings, which will be available in Canvas.  We do <em>not</em> provide a synchronous remote option, and we do not record labs.  You are welcome to use recordings to make up missed lectures and review material. However, absent extenuating circumstances (see below), viewing the recording will not make up for missed in-class activities.  </p>
<p>We regularly use Slack for in-class activities. Please make sure that you have access to Slack on a laptop, tablet, or mobile phone during class.</p>
<p>If you cannot attend class due to a medical issue, family emergency, interview, or other unforeseeable reason, please contact us about possible accommodations. We try to be as flexible as we can, but will handle these cases individually.</p>
<p><strong>Exams:</strong> The course has two midterms and a final project presentation, but no final exam. We typically use the registrar-assigned final exam timeslot (to be announced about halfway through the semester <a href="https://www.cmu.edu/hub/docs/final-exams.pdf">here</a>) for the final project presentation. The midterms are during the normal class period as per schedule. The second midterm is not comprehensive, and only covers material after the first midterm. Examples of past midterms can be found in the <a href="https://github.com/mlip-cmu/s2025/tree/main/exams">course repository</a>.</p>
<p><strong>Grading:</strong> Evaluation will be based on the following distribution: 35% individual assignments, 30% group project, 15% midterms, 5% participation, 10% labs, 5% reading quizzes. No final exam.</p>
<p>We strive to provide clear specifications and clear point breakdowns for all homework to set clear expectations and take the guessing out of homework. We often give you choices to self-direct your learning, deciding what to work on and how to address a problem (e.g., we never prescribe a programming language and often give choices to answer a subset of possible questions). Clear specifications and point breakdowns allow you to intentionally decide to skip parts of assignments with clear upfront consequences. All parts will be graded pass/fail, no partial credit. For opportunities to redo work, see <em>resubmissions</em> below. For grading participation and quizzes see below. Some assignments have a small amount of bonus points. </p>
<p>Since we give flexibility to resubmit assignments, we set grade boundaries fairly high. We expect the following grade boundaries:</p>
<table>
<thead>
<tr>
<th>Grade</th>
<th>Cutoff</th>
</tr>
</thead>
<tbody><tr>
<td>A+</td>
<td>&gt;99%</td>
</tr>
<tr>
<td>A</td>
<td>&gt;96%</td>
</tr>
<tr>
<td>A-</td>
<td>&gt;94%</td>
</tr>
<tr>
<td>B+</td>
<td>&gt;91%</td>
</tr>
<tr>
<td>B</td>
<td>&gt;86%</td>
</tr>
<tr>
<td>B-</td>
<td>&gt;82%</td>
</tr>
<tr>
<td>C</td>
<td>&gt;75%</td>
</tr>
<tr>
<td>D</td>
<td>&gt;60%</td>
</tr>
</tbody></table>
<p><strong>Participation:</strong> Design and engineering content requires active engagement with the material and discussions of judgment decisions on specific scenarios and cases. We strongly believe in in-class discussions and in-class exercises and want all students to participate, e.g., answering or asking questions in class, sharing own experiences, presenting results, or participating in in-class votes and surveys. We will give many opportunities for participation in every lecture and lab. We note student engagement with in-class activities to include as a component in grading.  We will provide feedback at mid-semester so that you can check in on how you’re doing. Again, please talk to us if you need accommodations.</p>
<p>We assign participation grades as follows:</p>
<ul>
<li>100%: Participates actively at least once in most lectures (4 lectures waived, no questions asked)</li>
<li>90%: Participates actively at least once in two thirds of the lectures</li>
<li>75%: Participates actively at least once in over half of the lectures</li>
<li>50%: Participates actively at least once in one quarter of the lectures</li>
<li>20%: Participates actively at least once in at least 3 lectures.</li>
<li>0%: Participation in less than 3 lectures.</li>
</ul>
<p><strong>Labs:</strong> Labs typically introduce tools and have a task with one or more clear deliverables. Lab assignments are designed to take about 1h of work and can be completed before or during the lab session. Each deliverable is graded pass/fail at any time during that week's lab session by showing your work to the TA. Typically showing your work involves showing source code, demoing executions, and (verbally) answering a few questions. The TA may ask a few questions about your implementation to probe that you understand your work.</p>
<p>We intend labs to be very low stakes – this is your first practical engagement with the material and mistakes are a normal part of the learning process. Deliverables are graded pass/fail on whether they meet the stated expectations for the deliverables. If your solution does not meet the expectations you can continue working on it during the lab session until it does. Outside of explicit accommodations (e.g., medical issues) or using tokens (see below), we do not accept lab solutions after the end of the lab session.</p>
<p>We encourage collaboration on labs: You can work together with other students both before the lab session and during the lab session. While we do not recommend it, you may look at other students’ solutions and reference solutions and even copy them. However, you will have to present and explain your solution to the TA on your own.</p>
<p><strong>Textbook, reading assignments, and reading quizzes:</strong> We will be using Goeff Hulten's <em>"Building Intelligent Systems: A Guide to Machine Learning Engineering"</em> (ISBN: 1484234316) throughout much of the course. The library provides an <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">electronic copy</a>. In addition, we will provide various additional readings, including blog posts and academic papers, throughout the semester.</p>
<p>We also wrote our own textbook "<a href="https://mlip-cmu.github.io/book/">Machine Learning in Production</a>" that aligns closely with the lecture content. The book will be published by MIT Press and is additionally available under a creative commons license online. We will not assign chapters from our own textbook, but we always point to the corresponding chapter for each lecture, which we suggest as supplementary reading.</p>
<p>We will assign readings for most classes and post a corresponding quiz on Canvas that is due before class. Each quiz contains an open-ended question that relates to the reading. Reading quizzes are intended to be low-stakes assessments and are graded pass/fail for a good-faith effort to engage with the question. </p>
<p><strong>Teamwork:</strong> Teamwork is an essential part of this course. The course contains a multi-milestone group project to be done in teams of 3-5 students. Teams will be assigned by the instructor. A TA will serve as a mentor for each team. We will help teams throughout the semester and cover some specific content on teamwork as part of the course. Peer rating will be performed for team assignments with regard to <em>team citizenship</em> (i.e., being active and cooperative members), following a procedure adapted from <a href="https://www.cs.tufts.edu/~nr/cs257/archive/teaching/barbara-oakley/JSCL-collaboration.pdf">this article</a>, which we will further explain in an early lecture. Use <a href="https://mlip-cmu.github.io/s2025/assignments/peergrading.html">this form</a> to preview the expected adjustments for peer ratings. The team's mentor will also debrief with the team after every milestone and discuss possible strategies to improve teamwork. </p>
<p><strong>Late work policy and resubmissions:</strong> We understand that students will always have competing deadlines, unusual events, interviews for job searches, and other activities that compete with coursework. We therefore build flexibility and a safety net directly into the rubric. If you need additional accommodations, please contact us.</p>
<p>In addition, we expect that the past/fail grading scheme without partial credit, may lead to harsh point deductions for missing small parts of the requirements, so we provide a mechanism to resubmit work with a short reflection to regain lost points.</p>
<p>Every student receives <em>8 individual tokens</em> that they can spend throughout the semester in the following ways:</p>
<ul>
<li>For each token, a student can submit a homework assignment 1 day late (with 2 tokens a student can submit two homeworks one day late each or a single homework up to two days late).</li>
<li>For <em>three</em> tokens, a student can improve or redo an individual homework assignment and resubmit together with a short reflection. The earlier submission is discarded and the regraded assignment counts toward the final grade. Resubmissions can be made at any time in the semester up to the final project presentation (see schedule). – Note that this technically allows a student to blow the original deadline (no submission necessary, receiving 0 points initially) and then resubmit the homework arbitrarily late for three tokens.</li>
<li>For one token, a student can submit a reading quiz late (any time before the final presentation) or resubmit a graded reading quiz.</li>
<li>For one token, a student can complete a lab late or redo a lab (any time before the final presentation) by showing the work to a TA during office hours.</li>
<li>Remaining individual tokens at the end of the semester are counted as one participation day each.</li>
</ul>
<p>If a student runs out of tokens, late individual assignments receive a penalty of 15% per started day. Late team formation survey and teamwork peer assessment surveys do not receive any points.</p>
<p>Every team independently receives <em>8 team tokens</em> that they can spend for extensions of any milestone deadline (1 token per day per milestone, except final presentation deadline) or to resubmit any milestone with a reflection (3 tokens each, resubmitted any time before the final presentation). If a team runs out of tokens, late submissions in group assignments receive a penalty of 15% per started day.</p>
<p>Individual tokens and team tokens are entirely separate; it is not possible to use individual tokens for teamwork or vice versa. The team should make collective decisions about how to use team tokens.</p>
<p>In general, late submissions and resubmissions can be done at any point in the semester before the final presentations. Late submissions that are 1-3 days late can be made directly to Gradescope; for everything else see instructions and forms on Canvas.</p>
<p>Exceptions to this policy will be made at the discretion of the instructor in important circumstances, almost always involving a family or medical emergency and an email from your advisor — you can ask your academic advisor or the Dean of Student Affairs requesting the exception on your behalf. Where issues affect teamwork, please communicate proactively with your team.</p>
<p><strong>Communication:</strong> We make important announcements on Slack; we recommend to enable Slack notifications. We answer email and monitor Slack, which may all be used for clarifying homework assignments and other interactions. We strongly recommend to ask questions publicly on Slack if others might have similar questions. Email or slack us if you would like to make an appointment.</p>
<p><strong>Auditing:</strong> Due to the high demand for this course, we do <em>not</em> allow auditing. If you like to self-study, all course materials are online. We welcome interested students and visitors to sit in for lectures as long as the room capacity allows it. </p>
<p><strong>Time management:</strong> This is a 12-unit course, and it is our intention to manage it so that you spend close to 12 hours a week on the course, on average. In general, 3 hours/week will be spent in class, about 1 hour for the labs, 1-2 hours on readings and reading quizzes, and 6-7 hours on assignments. Notice that much homework is done in groups, so please account for the overhead and decreased time flexibility that comes with groupwork. Please give the course staff feedback if the time the course is taking for you differs significantly from our intention.</p>
<p><strong>Writing:</strong> Describing tradeoffs among decisions and communication with stakeholders from other backgrounds are key aspects of this class. Many homework assignments have a component that requires discussing issues in written form or reflecting about experiences. To practice writing skills, the Global Communications Center (GCC) offers one-on-one help for students, along with workshops. The instructors are also happy to provide additional guidance if requested.</p>
<p><strong>Use of content generation AI tools and external sources:</strong> Given the nature of this course, we are open to using AI tools for completing work. We place no restrictions on the use of content generation tools, such as ChatGPT, Bard, Co-Pilot, or Stable Diffusion. You may also reuse code from external sources, such as StackOverflow or tutorials. In any case, you will be solely responsible for the correctness of the solution. Note that content generation tools often create plausible-looking but incorrect answers, which will not receive credit. You are also responsible for complying with any applicable licenses. If you use content generation tools, we encourage you to share your experience with the course staff or the entire class.</p>
<p><strong>Academic honesty and collaboration:</strong> The usual policies apply, especially the <a href="https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html">University Policy on Academic Integrity</a>. Many parts of the work will be done in groups. We expect that group members collaborate with one another, but that groups work independently from other groups, not exchanging results with other groups. Within groups, we expect that you are honest about your contribution to the group's work. This implies not taking credit for others' work and not covering for team members that have not contributed to the team. This also applies to in-class discussions, where indicating working with others who did not participate in the discussion is considered an academic honesty violation. Otherwise, our expectations regarding academic honestly and collaboration for group and pair work are the same as for individual work, substituting elevated to the level of "group."</p>
<p>Beyond that, the key guiding principle of academic honesty in this course is: <em>"You may not copy any part of a solution to a problem that was written by another student (in this or prior iterations of the class), or was developed together with another student, or was delegated to another person. You may not look at another student's solution, even if you have completed your own, nor may you knowingly give your solution to another student or leave your solution where another student can see it.</em>" Note that this implies that you cannot publicly post your solutions on GitHub (e.g., as part of a portfolio during job applications). While the use of AI content generation tools is okay (see above) using the work from other students is not. Discussing challenges and solution strategies with others at a high level is okay, sharing code or text is not.</p>
<p>You may collaborate with other students on labs, but not on reading quizzes, homeworks, and exams.</p>
<p>We also expect and respect honesty when communicating with the course staff.</p>
<p>Any violation of this policy is cheating. The minimum penalty for cheating will be a zero grade for the whole assignment. Cheating incidents will also be reported through University channels, with possible additional disciplinary action (see the University Policy on Academic Integrity). There is no statute of limitations for violations of the collaboration policy; penalties may be assessed (and referred to the university disciplinary board) after you have completed the course, and some requirements of the collaboration policy (such as restrictions on you posting your solutions) extend beyond your completion of the course.</p>
<p>If you have any question about how this policy applies in a particular situation, ask the instructors for clarification.</p>
<p><strong>Research in this Course:</strong> We are conducting academic research in this course. This research will involve analyzing student work of assignment. You will not be asked to do anything above and beyond the normal learning activities and assignments that are part of this course. You are free not to participate in this research, and your participation will have no influence on your grade for this course or your academic career at CMU. If you do not wish to participate, please send an email to Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>). Participants will not receive any compensation or extra credit. The data collected as part of this research will not include student grades. All analyses of data from participants’ coursework will be conducted after the course is over and final grades are submitted -- instructors will not know who chooses not to participate before final grades are submitted. All data will be analyzed in de-identified form and presented in the aggregate, without any personal identifiers. If you have questions pertaining to your rights as a research participant, or to report concerns to this study, please contact Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>) or the Office of Research Integrity and Compliance at Carnegie Mellon University (<a href="mailto:irb-review@andrew.cmu.edu">irb-review@andrew.cmu.edu</a>; phone: 412-268-4721).</p>
<p><strong>Accommodations for students with disabilities:</strong> If you have a disability with an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to contact them at <a href="mailto:access@andrew.cmu.edu">access@andrew.cmu.edu</a>.</p>
<p><strong>Respect for diversity:</strong> It is our intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be viewed as a resource, strength and benefit. It is my intent to present materials and activities that are respectful of diversity: gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Especially in lectures on fairness we will also cover diversity discussions, typically through a lens of the contemporary discourse in the US. Your suggestions are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups. </p>
<p><strong>A note on self care.</strong> Please take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. You are not alone. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is often helpful.
If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website at <a href="http://www.cmu.edu/counseling/">http://www.cmu.edu/counseling/</a>. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.</p>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why OpenAI's $157B valuation misreads AI's future (Oct 2024) (130 pts)]]></title>
            <link>https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/</link>
            <guid>42847825</guid>
            <pubDate>Tue, 28 Jan 2025 01:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/">https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/</a>, See on <a href="https://news.ycombinator.com/item?id=42847825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img fetchpriority="high" decoding="async" width="1024" height="576" src="https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1024x576.jpg" alt="" srcset="https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1024x576.jpg 1024w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-300x169.jpg 300w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-768x432.jpg 768w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1536x864.jpg 1536w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-375x211.jpg 375w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-323x182.jpg 323w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1.jpg 1921w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<p><em>I break down the logic behind the company’s towering price tag and why I think the most valuable AI companies have yet to be built.</em></p>



<p>When <a target="_blank" rel="noreferrer noopener" href="https://finance.yahoo.com/news/openai-closed-funding-round-raising-161842157.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAKFtOVtzprQmt1Og2sCCvZcWat-nslChIXPcvptCstCn0VSkVmkJhLNkMG81WlL0UJ5usiDd_A3A1TlLylFLfmyMmR3XTInJnRHlMkYh5_xXipGOGrWDZK_V5B63fSmIlcw4wbPuxnewKZc8DCEjwjVFHwA-Pj7dQWC7d39RGOKF">OpenAI raised $6.6B</a> earlier this month—the second-largest private funding round in history, topped only by its own $10B raise from Microsoft last January—it wasn’t just setting records. It was making an argument about where AI will create value and who stands to capture it.</p>



<p>Understanding this argument, both its logic and its limitations, sheds light on where I believe the most promising opportunities in AI are likely to emerge.</p>



<h2><strong>The bull case</strong></h2>



<p>OpenAI’s growth has been nothing short of meteoric. Monthly revenue reached <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">$300M</a> in August 2023, a 1,700% increase from January. 10M users pay $20/month for ChatGPT, and the company projects $11.6B in revenue next year.&nbsp;</p>



<p>This growth, which outpaces even the early days of Google and Facebook, underpins the bull case by <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ePfNAKopT20">investors like Brad Gerstner</a>. In his view, calling OpenAI a “model company” is like calling Google a “search algorithm company”—it fundamentally misunderstands the scale of the opportunity. ChatGPT isn’t just another tech product; it’s a fundamental advance in how humans interact with computers, “a hundred times better than the card catalog known as 10 blue links that Google built 25 years ago.”&nbsp;</p>



<p>Google did indeed transcend its origins in search, but only by first mastering its core business. Today, OpenAI is attempting to be a research lab, a developer platform, an enterprise solution, and a consumer product company all at once. Its investors are betting that AI is so transformative that the usual rules of focus and specialization don’t apply. The first company to achieve AGI will win everything, so the optimal strategy is to pursue every advantage simultaneously.</p>



<h2><strong>The bear case</strong></h2>



<h3><strong>💸 Bad economics</strong></h3>



<p>This narrative collides with a stubborn reality: the economics of AI don’t work like traditional software. OpenAI is currently valued at <a target="_blank" rel="noreferrer noopener" href="https://www.linkedin.com/posts/jonmcneill1_157-billion-thats-the-latest-valuation-activity-7252026319608131585-Z-Us?utm_source=share&amp;utm_medium=member_desktop">13.5x forward revenue</a>—similar to what Facebook commanded at its IPO. But while Facebook’s costs decreased as it scaled, OpenAI’s costs are growing in lockstep with its revenue, and sometimes faster.</p>



<p>In traditional software, increasing scale leads to improving economics. A typical software company might spend heavily on development upfront, but each additional user costs almost nothing to serve. Fixed costs are spread across a growing revenue base, creating the enviable margins that make today’s tech giants among the most profitable businesses in history.&nbsp;</p>



<p>Generative AI plays by different rules. Each query to a model costs money in compute resources, while each new model requires massive investments in training. OpenAI expects to lose $5B this year on $3.7B in revenue. Their projected losses through 2028 <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=e43qsi">amount to $44B</a>, excluding stock compensation. Computing costs alone will reach $6B this year.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.newcomer.co/p/the-bear-case-for-openai-at-157-billion">Google’s 2004 IPO</a> followed years of profitable operation, with $106M in profit on $962M in revenue. Facebook went public after achieving $1B in profit on $3.7B in revenue. Both companies demonstrated that growth improved their profit margins. OpenAI, by contrast, plans to 100x revenue to $100B by 2029 while piling up progressively larger losses. This requires maintaining <a target="_blank" rel="noreferrer noopener" href="https://www.marketwatch.com/story/openai-making-money-from-the-ai-revolution-is-far-from-certain-32249d5a">93% annual growth</a> for five years—a rate achieved by only a handful of companies in history, all of which saw their economics improve with scale.</p>



<p>The infrastructure needs to sustain AI’s progress are staggering. Microsoft, OpenAI’s primary partner, plans to spend $80-110B on AI infrastructure next year alone. According to semiconductor analyst <a target="_blank" rel="noreferrer noopener" href="https://www.dwarkeshpatel.com/p/dylan-jon?open=false#%C2%A7are-we-financing-an-ai-bubble">Dylan Patel</a>, Microsoft is building computing clusters with 100,000 GPUs and aims to construct a single facility consuming one gigawatt of power by 2026. By 2028, their computing requirements could reach multiple gigawatts—equivalent to an <a target="_blank" rel="noreferrer noopener" href="https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions">entire country’s</a> electricity demand.&nbsp;</p>



<p>To put these numbers in context: At the height of the dot-com bubble, the entire internet economy generated <a target="_blank" rel="noreferrer noopener" href="https://www.marketwatch.com/story/openai-making-money-from-the-ai-revolution-is-far-from-certain-32249d5a">$1.5T in revenue</a> (adjusted to 2024 dollars). Today, generative AI companies produce less than $10B in revenue while planning infrastructure investments that <a target="_blank" rel="noreferrer noopener" href="https://www.goldmansachs.com/insights/articles/will-the-1-trillion-of-generative-ai-investment-pay-off">could exceed $1T</a>.</p>



<h3><strong>🚫 No technical moat</strong></h3>



<p>OpenAI’s challenges extend beyond economics. Its leadership team has seen <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/behind-openais-staff-churn-turf-wars-burnout-compensation-demands?rc=e43qsi">near total turnover</a> over the past year. Eight of its eleven co-founders, including CTO Mira Murati and chief scientist Ilya Sutskever (who are launching competing ventures), have left. CEO Sam Altman has responded by recruiting experienced executives, like Sarah Friar as CFO and Kevin Weil as head of product. But when nearly all the talent that built your breakthrough technology goes elsewhere, it’s worth asking why.</p>



<p>What’s more, OpenAI’s massive infrastructure investments might not be building a moat—they might just be the cost of staying in the race. As board chair <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=vRhPc0zt2IE">Bret Taylor</a> admits, we’re watching “the fastest technology commoditization cycle we’ve ever seen.” GPT-4’s pricing per token has <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">plummeted 98%</a> since last year’s dev day. The gap between their SOTA models and open-source alternatives is narrowing with a speed that should make any investor nervous.</p>



<h3><strong>🌍 Distribution and openness matter</strong></h3>



<p>This brings me to what might be the most important dynamics in AI today: distribution and openness. In tech, the winners aren’t always those with the most advanced technology—they’re often those who build the most compelling ecosystems.</p>



<p>In a <a target="_blank" rel="noreferrer noopener" href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">recent memo</a>, Zuckerberg draws a parallel to the early days of high-performance computing, when major tech companies poured resources into proprietary Unix systems. At the time, few imagined that open-source alternatives could win. Yet Linux ultimately prevailed—not because it was better from the start, but because it allowed developers to modify the code freely, run it more securely and affordably, and build a broader ecosystem that enabled more capabilities than any closed system.</p>



<p>Meta is betting AI will follow a similar path. While Llama 2 could only match older, closed models, Llama 3 has reached parity with the frontier. Their <a target="_blank" rel="noreferrer noopener" href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1 405B</a> model can reportedly run at roughly half the cost of GPT-4, while giving enterprises something potentially more valuable than raw performance: complete control over their data and freedom from vendor lock-in.&nbsp;</p>



<p>Meanwhile, Meta’s consumer distribution is unmatched. LLaMA 3 currently powers AI features across Facebook, Instagram, and WhatsApp, reaching 1.1B users in 14 countries—and they’re only a third through their rollout. While ChatGPT has reached <a target="_blank" rel="noreferrer noopener" href="https://www.pewresearch.org/short-reads/2024/03/26/americans-use-of-chatgpt-is-ticking-up-but-few-trust-its-election-information/">23% of adults in the U.S.</a>, Meta is deploying its AI to billions of users globally. LLaMA 3 might not match GPT-4 in every research benchmark, but it doesn’t need to. Meta has optimized for what most users want: quick, reliable responses on mobile devices, especially in emerging markets where simpler queries dominate.</p>



<p>This creates brutal economics for OpenAI. While they reportedly plan to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">raise ChatGPT’s subscription price</a> to $44/month over the next five years, Meta can give away their AI for free. As <a target="_blank" rel="noreferrer noopener" href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">Zuckerberg notes</a>: “Selling access to AI models isn’t our business model. Openly releasing Llama doesn’t undercut our revenue or ability to invest in research like it does for closed providers.”</p>



<p>Other factors in Meta’s favor: it’s unencumbered by the legacy issues slowing down competitors like Google, whose search-based advertising business faces an existential threat from generative AI. It also has a founder-CEO willing to prioritize long-term market dominance over short-term profits.</p>



<h3><strong>⚓ The Microsoft dilemma</strong></h3>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html">OpenAI’s relationship with Microsoft</a> introduces another layer of complexity. What started as a lifeline—providing essential capital and computing resources—now risks becoming a constraint. Microsoft receives 75% of OpenAI’s profits until its $13B investment is recouped, followed by 49% until it hits $92B.</p>



<p>Their partnership shows growing signs of strain. Microsoft’s $650M acquisition of Inflection AI’s team looks less like opportunistic talent acquisition and more like a hedge against overreliance on OpenAI. Meanwhile, OpenAI’s $10B computing contract with Oracle (while structured through Microsoft to maintain exclusivity agreements) suggests a push for independence from Microsoft’s infrastructure.</p>



<p>Adding to the tensions is <a target="_blank" rel="noreferrer noopener" href="https://www.wsj.com/tech/ai/the-14-billion-question-dividing-openai-and-microsoft-71cf7d37">OpenAI’s pending change</a> from a nonprofit to a for-profit entity—one of the most controversial corporate restructurings in recent history. Both companies have engaged investment banks to negotiate Microsoft’s future equity position. OpenAI needs to complete this conversion within two years or risk investors from the latest round demanding their money back. Any major changes will also need approval from the FTC, which has been less than friendly of late to big tech.</p>



<h2><strong>Where value in AI will accrue</strong></h2>



<p>So where will the most promising opportunities in AI for investors and startups lie? Analyzing the AI ecosystem layer by layer reveals where the most durable value is likely to emerge.</p>



<h3><strong>🏗️ Physical and cloud infrastructure</strong></h3>



<p>At the bottom of the AI stack is hardware: vast arrays of GPUs, specialized processors, networking equipment, and massive data centers. Hyperscalers are investing billions here—not for immediate returns, but for strategic control. Each is pursuing vertical integration, developing proprietary silicon to reduce reliance on suppliers like NVIDIA, which is also <a target="_blank" rel="noreferrer noopener" href="https://venturebeat.com/ai/nvidia-just-dropped-a-bombshell-its-new-ai-model-is-open-massive-and-ready-to-rival-gpt-4/">moving up the stack</a> to compete in models. Several AI chip startups are also competing at this layer.</p>



<h3><strong>🧬 Foundation models</strong></h3>



<p>This layer seems exciting until you look at the economics. Building and improving foundation models requires massive ongoing investment, yet their value erodes faster than perhaps any other technology to date. Hyperscalers can absorb these costs by using models to drive demand for their cloud services, but independent startups face a steeper climb. The future of general-purpose models increasingly resembles a race to the bottom.&nbsp;</p>



<p>Still, there may be a handful of opportunities for model startups to carve out niches, whether by leveraging proprietary data or developing new architectures like state-space models. We have a few stealth investments in this category.</p>



<h3><strong>🛠️ Software infrastructure and developer tools</strong></h3>



<p>The next layer up includes companies that help developers build AI-powered software. Startups like Anyscale, Arize, Skyflow, and Turing are players here.</p>



<p>The cloud era saw <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">about 15 companies</a> scale to $1B+ revenue at this layer. I expect a similar pattern to play out in AI, with a few dozen startups building valuable franchises to serve AI developers. Our current investments align with this view, and we’re particularly excited about tooling to develop agentic systems.&nbsp;</p>



<p>At the same time, this category is challenging due to the rapid pace of innovation, competition from open source, and bundling by both hyperscalers and proprietary model providers. As one example, vector databases like Pinecone were highly sought after just a year ago, but their appeal has since waned.</p>



<h3><strong>🤖 AI Applications</strong></h3>



<p>The top of the stack is where I see the most promise. AI is not just adding features to existing software; it’s transforming entire service industries into software products. This shift expands the addressable market from the $350B software sector to the multi-trillion dollar services industry.</p>



<p>While OpenAI has focused on building general-purpose models, a new wave of specialized startups is addressing specific industry needs with precision. The early dismissal of these companies as “GPT wrappers”—basic interfaces layered over foundation models—now feels outdated. We’re seeing the rise of <a target="_blank" rel="noreferrer noopener" href="https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/">compound AI systems</a> that combine multiple AI models with retrieval mechanisms, external tools, and diverse data sources, orchestrated by advanced control logic.</p>



<p>History suggests this layer will produce the most winners. The cloud era created <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">over 20 application companies with $1B+ revenue</a>. In AI, we believe this number could exceed 100. These new companies will redefine how industries operate and potentially replace legacy systems of record like Salesforce and SAP.</p>



<h2>Where I land</h2>



<p>New technologies, no matter how revolutionary, don’t automatically translate into sustainable businesses. OpenAI’s $157B valuation suggests we might be forgetting this lesson.</p>



<p>This isn’t to diminish what OpenAI has achieved. They’ve shown us that AI can do things many thought impossible just a few years ago. They’ve forced enterprises to rethink how they operate and changed how humans interact with computers. But starting a revolution isn’t the same as profiting from it. Today’s headline-grabbing AI companies are creating tremendous value, but that doesn’t guarantee they’ll be the ones to capture it in the long run.</p>



<div><p>I’d argue that the most valuable companies of the AI era don’t exist yet. They’ll be the startups that harness AI’s potential to solve specific, costly problems across our economy—from engineering and finance to healthcare, logistics, legal, marketing, sales, and more.</p><p><em>For more, I also publish to <a href="https://ashugarg.substack.com/">Substack</a>, along with more frequent updates on <a href="https://www.linkedin.com/in/ashugargvc">LinkedIn</a> and <a href="https://x.com/ashugarg?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">X</a> where I share perspectives on enterprise software, AI, and the technical founder’s journey.</em></p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I trusted an LLM, now I'm on day 4 of an afternoon project (244 pts)]]></title>
            <link>https://nemo.foo/blog/day-4-of-an-afternoon-project</link>
            <guid>42845933</guid>
            <pubDate>Mon, 27 Jan 2025 21:37:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nemo.foo/blog/day-4-of-an-afternoon-project">https://nemo.foo/blog/day-4-of-an-afternoon-project</a>, See on <a href="https://news.ycombinator.com/item?id=42845933">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>TLDR - AI isn’t a co-pilot; it’s a junior dev faking competence. Trust it at your own risk.</strong></p>
<p>I’m 4 days into an afternoon project. I was so sure I’d crush this one. I had a good plan and the stoke was high. Let me introduce <em>Deskthang</em>. It’s a thang for your desk. When I work, I want to put my phone in the other room, and only get the important notifications (thangs) in a different way. If my deployment pipeline fails, I want a globe on my desk to turn red and show me a gitlab logo. I do not want to check my phone or email or anywhere a distraction might find me.</p>
<blockquote>
<p>Quick backstory: I work full time++ doing boring enterprise software dev and rarely get to flex my engineering skills. While my title says engineer, I’d disagree.</p>
</blockquote>
<h2>The Problem I’m Trying to Solve</h2>
<p>I always try to align multiple interests for a side project. I wanted to pull my electronics hardware box out of storage, I wanted to solve the notifications and focus issue for myself, and I wanted to see how scared I should be about AI taking my job. As they say, I was trying to get a few birds stoned at once.</p>
<p><img src="https://nemo.foo/blog-content/deskthang/two_birds_stoned.gif" alt=""></p>
<h3>1. I miss working with hardware.</h3>
<p>During COVID lock-downs I landed an R&amp;D contract for a IoT Prototype. That R&amp;D job was the most fulfilling work of my career. I worked with a small, scrappy team with some of my best friends. I was 3D printing models, soldering components, writing embedded C, and field-testing with mechanical engineers… Real engineers. We worked hard, often late into the night, and the collaboration felt more like playing StarCraft with the boiz than a 9-to-5. I’ve missed that deeply ever since. Recently, I’ve been inspired recently by <a href="https://x.com/_MaxBlade">@_MaxBlade and DeskHub</a> and wanted to brush the dust off my electronics skills.</p>
<h3>2. I hate the UX of MFA (Multi Factor Authentication).</h3>
<p>I use GitLab heavily for CI/CD with my personal Kubernetes projects. Knowing the status of my pipelines is crucial… broken builds could disrupt all 7 of my users! Logging into GitLab feels like getting stabbed in the spleen. Every time I log in (multiple times a day), I face captchas, authenticator apps, or waiting for email codes, followed by yet another captcha. I’ve tried pipeline notifications through Slack, Discord, and Telegram, but those apps are like productivity black holes. I don’t want my phone near me while working, or to open chat apps that derail my focus. Removing these distractions keeps me locked in.</p>
<h3>3. I want to see how good these AI tools are.</h3>
<p>I want to figure out if AI is going to take my job. I’m skeptical it can replace what I do, but I like testing my assumptions. Sometimes AI surprises me; other times, it’s just a rabbit hole of wasted hours when I avoid doing real thinking.</p>
<p>Recently, I used Claude Sonnet 3.5 to brute-force hundreds of React compile errors while upgrading a project from React 15 to 18. I threw <code>package.json</code> updates, deleted <code>node_modules</code>, and burned through a small fortune in AI tokens. To my surprise, we had a passing build by the end of the day. Work has been encouraging us to adopt an AI-first workflow and giving us unlimited tokens. It’s a wild experiment.</p>
<p>This happened on a Friday. I wiped the sweat off my brow after a hard day’s prompting, and headed home early to start on my side project…</p>
<h2>The Plan</h2>
<p>Unlike me, my wife likes to leave the house and do things. I’ve spent a few years turning my garage into my favorite place to be. My wife and I have a deal where 1 day a month, She takes the kiddo and I am absolved of all responsibilities. I get a full day to lock in and build projects. From her perspective, I order doordash and turn into a degen who is unfit to father. From my perspective, I get to enjoy my favorite place and just tinker or play games or do whatever. These are the days I get to play mad scientist and feel most like myself. I look forward to it every month. My plan was to learn zig, brush off my hardware skills, build this project, write a blog post and make a video about it. Totally achievable.</p>
<p>I wanted to wire up a Raspberry Pi Pico, a small 240x240 LCD display and some RGB LEDs. I was going to learn Zig and use it to send image data over USB to the pico which will put an image on the screen and change the LED color. I would set up webhooks from GitLab to call an API in my Kube cluster and setup my host Zig app to poll that same API for changes and send updates to the Pico. I really wanted to transmit the data over USB because I’ve never done that before. I’ve already used Bluetooth, LTE and Wifi and just wanted to do something new.</p>
<p>The wiring is simple. Common patterns I was familiar with like <a href="https://en.wikipedia.org/wiki/Serial_Peripheral_Interface">SPI (Serial Peripheral Interface)</a> for the display + some RGB leds. The <a href="https://itsfoss.com/what-is-tty-in-linux/?utm_source=chatgpt.com">TTY (TeleTYpewriter)</a> serial data port on Linux <code>/dev/ttyACM0</code> for USB communication with the Pico felt familiar because of how I had setup debug logging in the past. It looked like I had enough example repos collected that I could stitch a solution together. I did a little research each day and felt a little more sure each time. I’ve been using ChatGPT and Claude more and more to do initial research. I was at an AI hype peak and was bold enough to trust it…</p>
<p><img src="https://nemo.foo/blog-content/deskthang/excalidraw.png" alt=""></p>
<p>Since I do full stack web stuff on the daily, the api, webhooks and postgres are out of scope for the degen day. I was scoping the day’s work to Zig -&gt; Pico image transfer.</p>
<h3>1. Setup Pico</h3>
<ul>
<li>Organize the workspace</li>
<li>Find a micro usb cable that supports data and not just charging… really why are they all power only?!</li>
<li>Wire up a breadboard with Pico &amp; display and LED</li>
<li>Setup the C SDK for the raspi pico and a repo <a href="https://gitlab.com/nemofoo/deskthang">gitlab</a>, <a href="https://github.com/nemofoo/deskthang">gihub-mirror</a></li>
<li>Push a build and see logs on <code>cat /dev/ttyACM0</code></li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/first_assembly.jpg" alt=""></p>
<h3>2. Setup Pico Display</h3>
<ul>
<li>Put something on the screen during the boot loop.</li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/first_test_pattern.jpg" alt=""></p>
<p><a href="https://youtube.com/shorts/H6b64PJI40o">youtube link (12 sec)</a></p>
<h3>3. Setup Host Zig Project</h3>
<ul>
<li>Setup a host directory in repo</li>
<li>Init zig project</li>
<li>Send a message and see something on the screen</li>
</ul>
<p><a href="https://youtu.be/Y0wkzbwGWJc">youtube link (9 sec)</a></p>
<h3>4. Image Transfer</h3>
<ul>
<li>Yeet the raw rgb image data over USB</li>
<li>It’s bidirectional safe right</li>
<li>USB CDC is bidirectional safe
<ul>
<li>TTY interface is built on top of USB CDC
<ul>
<li>TTY is bidirectional safe because CDC is (no it’s not… thanks gpt)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://nemo.foo/blog-content/deskthang/gptlies.png" alt=""></p>
<blockquote>
<p>In the above image you can see the outright lie that broke me… USB CDC has separate TX (Transmit) and RX (Receive) buffers so it’s bidirectional safe. The same is not true for TTY which is bidirectional but less safe with a single buffer for TX and RX data.</p>
</blockquote>
<h2>Timeline of Actuality (AI Woes)</h2>
<p>After a dozen duds, I found a data capable usb micro cable and everything went smoothly until the image transfer. I used Claude, Cline, and ChatGPT to AI-max my way to a buggy but working implementation. I sent commands from my terminal with Zig over USB to the Pico which read them and changed the screen. This only took a few hours and I was excited that the AI assisted dream was real. It’s not complex but I think it was faster than I could have done alone. I have no experience with zig besides hearing ThePrimeagen yap about it. I haven’t even read the docs.</p>
<p>Multiple times, I found myself stopping Cline from starting completely new implementations of already solved issues. I didn’t catch everything though. When Cline blew through my API limits, I added Claude to my harem and ran both in parallel when possible.</p>
<blockquote>
<p>As I look through the code now, I realize that I’m lousy at multitasking and was gaslighting myself.</p>
</blockquote>
<h3>The Image Transfer Disaster</h3>
<p>This is where my hubris came into play. In my mind, I pictured sending all the image data in one go, like an S3 upload. I imagined clean, raw data streaming over <code>/dev/ttyACM0</code>. It wasn’t clean. It wasn’t raw. It was chaos.</p>
<p>I expected to see:</p>
<pre><code>pico - heartbeat
zig - start image transfer
zig - [240x240 COLOR PIXELS]
zig - end image transfer
pico - heartbeat
</code></pre>
<p>What I actually saw looked like this, but worse:</p>
<pre><code>pico - heartbeat
zig - start ima%
pico - heage transfert
pico - heartbeat
zig - [240x240 COL
pico - heartbea
OR PIXELtS]
pico - heartbeat
zig - end image transfer
pico - heartbeat
</code></pre>
<p><a href="https://youtu.be/jnmqlsdD6oU">It’s just like that interrupting cow knock knock joke.</a> Completely unfunny and day ruining.</p>
<p><strong>Key Problems:</strong></p>
<p><strong>1. Buffer Conflicts:</strong> <code>/dev/ttyACM0</code> was the battlefield. The same buffer was used for both logging and image transfer. If a log slipped in during the data stream… well good luck figuring out what the hell just happened.</p>
<p><strong>2. Noise:</strong> Some weird corruption was happening. Maybe I wasn’t clearing buffers properly. Maybe the gods of USB communication just hate me.</p>
<p>The bottom line? Neither the Pico nor my laptop could trust the data. Each system needed to learn to yield, and I needed to build the round-a-bout to force them to be polite and wait their turn.</p>
<h3>Packets, Protocols &amp; State Machines, Oh my…</h3>
<p>I needed to get serious. So, naturally, I let Claude write some docs:</p>
<ul>
<li>Detailed a packet shape.</li>
<li>Documented a checksum verification plan.</li>
<li>Described data format for transfer.</li>
<li>Denoted how to chunk and rebuild the image.</li>
<li>Depicted the state machine transitions.</li>
<li>Demonstrated command system.</li>
<li>Designed a logging system that doesn’t break incoming commands.</li>
</ul>
<p>After delving down dem docs, I let AI run with the actual implementations. At this point, my “degen hat” came off, and I resumed my dad duties while letting Cursor and Cline play StarCraft with my codebase. This is the dream use case for AI, right? Just let it rip and come back to a perfectly functioning system. Let’s see just how close we get to the sun.</p>
<p>Reality Check: AI tools are like interns who know how to Google really fast but don’t understand context. Cursor started changing core implementations for unrelated edits. Cline would randomly rewrite half the system without asking. By the time I noticed, my codebase looked like the aftermath of a spaghetti fight at a junior developer convention. Most of the codebase was actually unreachable.</p>
<h2>What did I learn?</h2>
<p>Like Icarus, my codebase is irrecoverable. A tangled heap of wing fragments and melted wax, dripping with half-baked ideas and unsupervised AI chaos. My grand vision of outsourcing grunt work to AI had sent me soaring, but the sun of reality burned away any hope of landing gracefully. Here’s what I’m taking away from this flaming descent.</p>
<h3>1. AI is a tool, not a co-pilot</h3>
<p>AI is great for generating ideas or drafting code, but it doesn’t understand. It’s like giving a junior developer a chainsaw instead of a scalpel—it might finish the job, but you’ll spend twice as long cleaning up the mess. I learned that I need to stay firmly in the driver’s seat when tackling new tech.</p>
<h3>2. Friction forces focus</h3>
<p>Having AI directly in my editor felt like playing with infinite cheat codes. It was too easy to let it run wild and harder to maintain control. Moving forward, I’m introducing deliberate friction. I will be using AI only in web interfaces or as a brainstorming tool. If I have to paste its suggestions into my code manually, I’ll be more mindful of the process and less likely to reach for it.</p>
<h3>3. Mistakes teach better than shortcuts</h3>
<p>When I make mistakes, I learn. Debugging my own failures has always been one of the best ways to understand a new language or concept. Relying on AI to “fix” things for me short-circuited that learning process. As a result, I’m left with no deeper understanding of Zig than when I started.</p>
<h3>4. Patience beats hubris</h3>
<p>Building something new, with unfamiliar tools takes time. The idea that I could fully implement my vision in a single “degen day” was overly optimistic, bordering on foolish. Sometimes, you have to respect the complexity of what you’re trying to achieve.</p>
<h2>Moving Forward</h2>
<p>Deskthang has grown from a casual afternoon project into a saga of overconfidence, AI misadventures, and lessons learned the hard way. For now, I’m shelving the AI driven shortcuts and committing to a rewrite on my next no-responsibilities day.</p>
<p>I’ve picked up a pen and started writing docs by hand like it’s the stone age. I plan to work through some Advent of Code problems in Zig to actually learn the language before taking another crack at this project.</p>
<p>Want to see if Deskthang ever works, or just enjoy the chaos as I fail forward? Subscribe below for a monthly email drop of my latest misadventures and skill issues. Together, we’ll learn how to coexist with AI, relearn the lessons I forget, and hopefully build something worthwhile in the process.</p>
<p>LFG 🚀</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia sheds almost $600B in market cap, biggest one-day loss in US history (241 pts)]]></title>
            <link>https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html</link>
            <guid>42845681</guid>
            <pubDate>Mon, 27 Jan 2025 21:13:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html">https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SpecialReportArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="SpecialReportArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-108082997" data-test="InlineImage"><p>Nvidia CEO Jensen Huang holds a Blackwell GeForce RTX 50 Series GPU (L) and a RTX 5000 laptop as he delivers a keynote address at the Consumer Electronics Show (CES) in Las Vegas, Nevada on January 6, 2025.&nbsp;</p><p>Patrick T. Fallon | Afp | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> lost close to $600 billion in market cap on Monday, the biggest drop for any company on a single day in U.S. history.</p><p>The chipmaker's stock price plummeted 17% to close at $118.58. It was Nvidia's worst day on the market since March 16, 2020, which was early in the Covid pandemic. After <a href="https://www.cnbc.com/2025/01/21/nvidia-passes-apple-again-to-become-worlds-most-valuable-company-.html">Nvidia surpassed</a> <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/AAPL/">Apple</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> last week to become the most valuable publicly traded company, the stock's drop Monday led a 3.1% slide in the tech-heavy Nasdaq.</p><p>The sell-off was <a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html">sparked</a> by concerns that Chinese <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a> lab DeepSeek is presenting increased competition in the global AI battle. In late December, <a href="https://www.cnbc.com/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">DeepSeek unveiled</a> a free, open-source large language model that <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf" target="_blank">it&nbsp;said</a>&nbsp;took only two months and less than $6 million to build, using reduced-capability chips from&nbsp;<a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a>&nbsp;called H800s.&nbsp;</p><p>Nvidia's graphics processing units, or GPUs, dominate the market for AI data center chips in the U.S., with tech giants such as <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-9"><a href="https://www.cnbc.com/quotes/GOOGL/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-10"><a href="https://www.cnbc.com/quotes/META/">Meta</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-11"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> spending billions of dollars on the processors to train and run their AI models. </p><p>Analysts at Cantor wrote in a report Monday that the release of DeepSeek's latest technology has caused "great angst as to the impact for compute demand, and therefore, fears of peak spending on GPUs."</p></div><div id="SpecialReportArticle-RelatedContent-1"><h2>Read more DeepSeek coverage</h2><div><ul><li><a href="https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html">China's DeepSeek AI dethrones ChatGPT on App Store</a></li><li><a href="https://www.cnbc.com/2025/01/27/deepseek-hit-with-large-scale-cyberattack-says-its-limiting-registrations.html">DeepSeek hit with large-scale cyberattack, says it's limiting registrations</a></li><li><a href="https://www.cnbc.com/2025/01/24/how-chinas-new-ai-model-deepseek-is-threatening-us-dominance.html">How China's new AI model DeepSeek is threatening U.S. dominance</a></li><li><a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html">Nvidia hits new low for session on threat from DeepSeek AI model</a></li><li><a href="https://www.cnbc.com/2025/01/27/how-the-buzz-around-chinese-ai-model-deepseek-sparked-a-massive-nasdaq-sell-off.html">Buzz around Chinese AI model DeepSeek sparks massive Nasdaq sell-off</a></li><li><a href="https://www.cnbc.com/2025/01/27/the-key-chart-levels-to-watch-on-nvidia-tech-stocks-on-deepseek-fear.html">Pro: The key chart levels to watch on Nvidia and other tech stocks amid DeepSeek rout</a></li></ul></div></div><div><p>The analysts said they "think this view is farthest from the truth" and that advancements in AI will most likely lead to "the AI industry wanting more compute, not less." They recommend buying Nvidia shares.</p><p>But after Nvidia's huge run-up — the stock soared 239% in <a href="https://www.cnbc.com/2023/05/30/nvidia-on-track-to-hit-1-trillion-market-cap-when-market-opens.html">2023</a> and 171% in <a href="https://www.cnbc.com/2024/12/25/ai-crypto-top-tech-stocks-applovin-microstrategy-palantir-nvidia.html">2024</a> — the market is on edge about any possible pullback in spending. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-14"><a href="https://www.cnbc.com/quotes/AVGO/">Broadcom</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, the other big U.S. chipmaker to see giant valuation gains from AI, fell 17% on Monday, pulling its market cap down by $200 billion.</p><p>Data center companies reliant on Nvidia's GPUs for their hardware sales saw big sell-offs as well. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-15"><a href="https://www.cnbc.com/quotes/DELL/">Dell</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-16"><a href="https://www.cnbc.com/quotes/HPE/">Hewlett Packard Enterprise</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-17"><a href="https://www.cnbc.com/quotes/SMCI/">Super Micro Computer</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> dropped at least 5.8%. <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-18"><a href="https://www.cnbc.com/quotes/ORCL/">Oracle</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, a part of President<a href="https://www.cnbc.com/donald-trump/"> Donald Trump's</a> latest AI initiative, fell 14%.</p><p>For Nvidia, the loss was more than <a href="https://www.cnbc.com/2024/09/04/asian-chip-stocks-fall-after-nvidia-sell-off-on-wall-street-overnight.html">double the $279 billion drop</a> the company saw in September, which was the biggest one-day market value loss in history at the time, unseating Meta's <a href="https://www.cnbc.com/2022/02/03/facebooks-232billion-drop-in-value-sets-all-time-record.html">$232 billion loss</a> in 2022. Before that, the steepest drop was $182 billion by Apple in 2020.</p><p>Nvidia's decline is more than double the market cap of <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-22"><a href="https://www.cnbc.com/quotes/KO/">Coca-Cola</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-23"><a href="https://www.cnbc.com/quotes/CVX/">Chevron</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and exceeds the market value of both Oracle and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-24"><a href="https://www.cnbc.com/quotes/NFLX/">Netflix</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>.</p><p>CEO Jensen Huang's net worth also took a massive hit, declining roughly $21 billion, according to <a href="https://www.forbes.com/real-time-billionaires/#458f33ab3d78" target="_blank">Forbes' real-time billionaires list</a>. The move demoted Huang to 17th on the richest-person list.</p><p>The sudden excitement around DeepSeek over the weekend pushed its app <a href="https://www.cnbc.com/2025/01/27/chinas-deepseek-ai-tops-chatgpt-app-store-what-you-should-know.html">past OpenAI's ChatGPT</a> as the most-downloaded free app in the U.S. on Apple's app store. The model's development comes despite a slew of recent curbs on U.S. chip exports to China.</p><p>Venture capitalist David Sacks, who was <a href="https://www.cnbc.com/2024/12/05/trump-david-sacks-billionaire-ai-crypto.html">tapped</a> by Trump to be the White House's AI and crypto czar, <a href="https://x.com/DavidSacks/status/1883935713877782884" target="_blank">wrote on X</a> that DeepSeek's model "shows that the AI race will be very competitive" and that Trump was right to rescind President <a href="https://www.cnbc.com/video/2019/04/25/joe-biden-enters-2020-presidential-race.html">Joe Biden</a>'s executive order last week on AI safety.</p><p>"I'm confident in the U.S. but we can't be complacent," Sacks wrote.</p><p>Nvidia is now the third most-valuable public company, behind Apple and <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-30"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/01/27/pro-watch-cnbcas-full-interview-with-bernsteins-stacy-rasgon-trivariateas-adam-parker-and-payne-capitalas-courtney-garcia.html">CNBC's full interview with Bernstein's Stacy Rasgon</a></p></div><div id="Placeholder-ArticleBody-Video-108093000" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000364746" aria-labelledby="Placeholder-ArticleBody-Video-108093000"><p><img src="https://image.cnbcfm.com/api/v1/image/108093001-17380094731738009466-38180086461-1080pnbcnews.jpg?v=1738009472&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Watch CNBC’s full interview with Bernstein's Stacy Rasgon, Trivariate’s Adam Parker and Payne Capital’s Courtney Garcia"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated DeepSeek-R1 (375 pts)]]></title>
            <link>https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</link>
            <guid>42845488</guid>
            <pubDate>Mon, 27 Jan 2025 20:51:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1">https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42845488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><span>[Draft post, updates to come, please let me know if you have any suggestions or feedback here or on </span><a href="https://bsky.app/profile/jayalammar.bsky.social" rel="">Bluesky</a><span> or </span><a href="https://x.com/JayAlammar" rel="">X/Twitter</a><span>]</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png" width="1130" height="408" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/98138856-a4de-45e3-ad08-1434378127c2_1130x408.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:408,&quot;width&quot;:1130,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98138856-a4de-45e3-ad08-1434378127c2_1130x408.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>DeepSeek-R1 is the latest resounding beat in the steady drumroll of AI progress. For the ML R&amp;D community, it is a major release for reasons including: </p><ol><li><p>It is an open weights model with smaller, distilled versions and </p></li><li><p>It shares and reflects upon a training method to reproduce a reasoning model like OpenAI O1. </p></li></ol><p>In this post, we’ll see how it was built.</p><p>Contents:</p><ul><li><p>Recap: How LLMs are trained</p></li><li><p>DeepSeek-R1 Training Recipe</p></li><li><p>1- Long chains of reasoning SFT Data</p></li><li><p>2- An interim high-quality reasoning LLM (but worse at non-reasoning tasks).</p></li><li><p>3- Creating reasoning models with large-scale reinforcement learning (RL) </p><ul><li><p>3.1- Large-Scale Reasoning-Oriented Reinforcement Learning (R1-Zero)</p></li><li><p>3.2- Creating SFT reasoning data with the interim reasoning model</p></li><li><p>3.3- General RL training phase </p></li></ul></li><li><p>Architecture</p></li></ul><p><span>Most of the foundational knowledge you need to understand how such a model works is available in our book, </span><a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models" rel="">Hands-On Large Language Models</a><span>.</span></p><p>Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif" width="613" height="152" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5280089e-8989-45d7-8194-93396b25557d_613x152.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:152,&quot;width&quot;:613,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3441758,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5280089e-8989-45d7-8194-93396b25557d_613x152.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The following figure, from Chapter 12 of our book shows the general recipe of creating a high-quality LLM over three steps:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png" width="1456" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa354473-6ae0-4ae7-a20c-e858c804d6c4_1600x477.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>1) The language modeling step where we train the model to predict the next word using a massive amount of web data. This step results in a base model.</p><p>2) a supervised fine-tuning step that makes the model more useful in following instructions and answering questions. This step results in an instruction tuned model or a supervised fine -tuning / SFT model.</p><p>3) and finally a preference tuning step which further polishes its behaviors and aligns to human preferences, resulting in the final preference-tuned LLM which you interact with on playgrounds and apps.</p><p><span>DeepSeek-R1 follows this general recipe. The details of that first step come from a </span><a href="https://arxiv.org/pdf/2412.19437v1" rel="">previous paper for the DeepSeek-V3 model</a><span>. R1 uses the </span><em>base</em><span> model (not the final DeepSeek-v3 model) from that previous paper, and still goes through an SFT and preference tuning steps, but the details of how it does them are what's different.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png" width="854" height="234" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:234,&quot;width&quot;:854,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:30102,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc66dff5b-8332-4696-b484-b2ddb029b78c_854x234.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There are three special things to highlight in the R1 creation process.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png" width="854" height="434" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/26136780-897d-4f64-b1e5-45936b6078dd_854x434.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:854,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:43757,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26136780-897d-4f64-b1e5-45936b6078dd_854x434.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is a large number of long chain-of-thought reasoning examples (600,000 of them). These are very hard to come by and very expensive to label with humans at this scale. Which is why the process to create them is the second special thing to highlight</p><p><span>This data is created by a precursor to R1, an unnamed sibling which specializes in reasoning. This sibling is inspired by a third model called </span><em>R1-Zero </em><span>(that we’ll discuss shortly). It is significant not because it’s a great LLM to use, but because creating it required so little labeled data alongside large-scale reinforcement learning resulting in a model that excels at solving reasoning problems. </span></p><p>The outputs of this unnamed specialist reasoning model can then be used to train a more general model that can also do other, non-reasoning tasks, to the level users expect from an LLM.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png" width="924" height="427" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:427,&quot;width&quot;:924,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:50317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4caea6a5-52a1-4651-8c71-4586c0637f3e_924x427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This happens in two steps:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176092,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Here, RL is used to create the interim reasoning model. The model is then used to  generate the SFT reasoning examples. But what makes creating this model possible is an earlier experiment creating an earlier model called </span><em>DeepSeek-R1-Zero</em><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png" width="1456" height="483" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:103072,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69b9f117-caa3-42fd-a949-dc6433990d26_1526x506.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>R1-Zero is special because it is able to excel at reasoning tasks without having a labeled SFT training set. Its training goes directly from a pre-trained base model through a RL training process (no SFT step). It does this so well that it’s competitive with o1.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png" width="1456" height="383" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:383,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106577,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b5c964f-b654-49b2-ab5a-5618b256ef99_1588x418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This is significant because data has always been the fuel for ML model capability. How can this model depart from that history? This points to two things:</p><p>1- Modern base models have crossed a certain threshold of quality and capability (this base model was trained on 14.8 trillion high-quality tokens).</p><p>2- Reasoning problems, in contrast to general chat or writing requests, can be automatically verified or labeled. Let’s show this with an example. This can be a prompt/question that is a part of this RL training step:</p><blockquote><p>Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.</p></blockquote><p>A question like this lends itself to many ways of automatic verification. Say we present this this to the model being trained, and it generates a completion:</p><ul><li><p>A software linter can check if the completion is proper python code or not</p></li><li><p>We can execute the python code to see if it even runs</p></li><li><p>Other modern coding LLMs can create unit tests to verify the desired behavior (without being reasoning experts themselves). </p></li><li><p>We can go even one step further and measure execution time and make the training process prefer more performant solutions over other solutions — even if they’re correct python programs that solve the issue.</p></li></ul><p>We can present a question like this to the model in a training step, and generate multiple possible solutions.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png" width="798" height="444" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:444,&quot;width&quot;:798,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:60456,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8edd9db2-a071-4bba-9d14-bbdb076d6355_798x444.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We can automatically check (with no human intervention) and see that the first completion is not even code. The second one is indeed python code but does not solve the problem. The third is a possible solution, but fails the unit tests, and the forth is a correct solution.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png" width="972" height="517" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f9645a0-b1fb-4753-942c-583504297c25_972x517.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:517,&quot;width&quot;:972,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74268,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f9645a0-b1fb-4753-942c-583504297c25_972x517.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These are all signals that can be directly use to improve the model. This is of course done over many examples (in mini-batches) and over successive training steps.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png" width="955" height="543" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:543,&quot;width&quot;:955,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:92262,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b595e04-bd57-4f78-8c9b-ab37797e9b66_955x543.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>These reward signals and model updates are how the model continues improving on tasks over the RL training process as seen in Figure 2 from the paper.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png" width="1456" height="833" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:833,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:211203,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe48af6fa-8956-44b0-84cf-915e607f3b5e_1546x884.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Corresponding with the improvement of this capability is the length of the generated response, where the model generates more thinking tokens to process the problem.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png" width="1456" height="875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:875,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:250596,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcd2b7d78-62ac-408c-8bd7-e14053bb8a46_1518x912.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This process is useful, but the R1-Zero model, despite scoring high on these reasoning problems, confronts other issues that make it less usable than desired. </p><blockquote><p>Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing.</p></blockquote><p>R1 is meant to be a more usable model. So instead of relying completely on the RL process, it is used in two places as we’ve mentioned earlier in this section:</p><p>1- creating an interim reasoning model to generate SFT data points</p><p>2- Training the R1 model to improve on reasoning and non-reasoning problems (using other types of verifiers)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45ca8c84-6eb6-4879-ab53-035174b17ce1_1620x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To make the interim reasoning model more useful, it goes through an supervised fine-tuning (SFT) training step on a few thousand examples of reasoning problems (some of which are generated and filtered from R1-Zero). The paper refers to this as cold start data”</p><blockquote><p><strong>2.3.1. Cold Start</strong><br><span>Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators.</span></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png" width="1456" height="468" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:468,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:160514,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a89a9a0-c08f-430d-b135-7f012c2810ba_1824x586.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But wait, if we have this data, then why are we relying on the RL process? It’s because of the scale of the data. This dataset might be 5,000 examples (which is possible to source), but to train R1, 600,000 examples were needed. This interim model bridges that gap and allows to synthetically generate that extremely valuable data.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png" width="1456" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:516,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:244148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F857e61c8-03e7-4bc7-bcbe-ca182f60a70e_3300x1170.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If you’re new to the concept of Supervised Fine-Tuning (SFT), that is the process that presents the model with training examples in the form of prompt and correct completion. This figure from chapter 12 shows a couple of SFT training examples:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png" width="1456" height="851" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:851,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:575809,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b630dbc-aaa4-4c27-804b-542055b0f298_2264x1324.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This enables R1 to excel at reasoning as well as other non-reasoning tasks. The process is similar to the the RL process we’ve seen before. But since it extends to non-reasoning applications, it utilizes a helpfulnes and a safety reward model (not unlike the Llama models) for prompts that belong to these applications.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png" width="902" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:902,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72511,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e5f9acf-b4ca-4ec4-9731-4845c8fc5515_902x394.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Just like previous models from the dawn of </span><a href="https://jalammar.github.io/illustrated-gpt2/" rel="">GPT2</a><span> and </span><a href="https://jalammar.github.io/how-gpt3-works-visualizations-animations/" rel="">GPT 3</a><span>, DeepSeek-R1 is a stack of </span><a href="https://jalammar.github.io/illustrated-transformer/" rel="">Transformer</a><span> decoder blocks. It’s made up 61 of them. The first three are dense, but the rest are mixture-of-experts layers (See my co-author Maarten’s incredible intro guide here: </span><a href="https://substack.com/home/post/p-148217245" rel="">A Visual Guide to Mixture of Experts (MoE)</a><span>).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png" width="538" height="413" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:413,&quot;width&quot;:538,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39245,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F199f326e-9a8d-4a95-8574-4778d5b7657b_538x413.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In terms of model dimension size and other hyperparameters, they look like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png" width="916" height="481" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:481,&quot;width&quot;:916,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:63869,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ee664ae-a544-4e19-a145-0ae87acc43fa_916x481.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>More details about the model architecture are presented in their two earlier papers:</p><ul><li><p><a href="https://arxiv.org/pdf/2412.19437v1" rel="">DeepSeek-V3 Technical Report</a></p></li><li><p><a href="https://arxiv.org/pdf/2401.06066" rel="">DeepSeekMoE: Towards Ultimate Expert Specialization in</a></p><p><a href="https://arxiv.org/pdf/2401.06066" rel="">Mixture-of-Experts Language Models</a></p></li></ul><p>With this, you should now have the main intuitions to wrap your head around the DeepSeek-R1 model. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" width="1456" height="634" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:634,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:341594,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If you felt needed a little more foundational information to understand this post, I’d suggest you pick up a copy of </span><a href="https://www.llm-book.com/" rel="">Hands-On Large Language Models</a><span> or read it online on </span><a href="https://learning.oreilly.com/library/view/hands-on-large-language/9781098150952/" rel="">O’Reilly</a><span> and check it out on </span><a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models" rel="">Github</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png" width="158" height="208.49484536082474" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:582,&quot;resizeWidth&quot;:158,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Book Cover&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Book Cover" title="Book Cover" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7beb5f-e943-4d2d-8b4c-eb1e80231670_582x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Other suggested resources are:</p><ul><li><p><a href="https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1" rel="">DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs</a><span> by </span></p><span> </span></li><li><p><a href="https://substack.com/home/post/p-148217245" rel="">A Visual Guide to Mixture of Experts (MoE)</a><span> by </span></p><span> </span></li><li><p><span>Sasha Rush’s YouTube video </span><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw" rel="">Speculations on Test-Time Scaling (o1)</a><span> </span></p></li><li><p><span>Yannis Kilcher’s </span><a href="https://www.youtube.com/watch?v=bAWV_yrqx4w" rel="">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Paper Explained)</a></p></li><li><p><a href="https://github.com/huggingface/open-r1" rel="">Open R1</a><span> is the HuggingFace project to openly reproduce DeepSeek-R1</span></p></li><li><p><a href="https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" rel="">Putting RL back in RLHF</a></p></li></ul></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go 1.24's go tool is one of the best additions to the ecosystem in years (234 pts)]]></title>
            <link>https://www.jvt.me/posts/2025/01/27/go-tools-124/</link>
            <guid>42845323</guid>
            <pubDate>Mon, 27 Jan 2025 20:33:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jvt.me/posts/2025/01/27/go-tools-124/">https://www.jvt.me/posts/2025/01/27/go-tools-124/</a>, See on <a href="https://news.ycombinator.com/item?id=42845323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For those that aren't aware, one of the big changes in February's upcoming Go 1.24 release is the new <a href="https://tip.golang.org/doc/go1.24#tools"><code>go tool</code></a> command, and <code>tool</code> directive in the <code>go.mod</code> to manage any tools your project uses. I'm <em>incredibly excited</em> about this, and in my opinion, this is one of the best changes we've had in recent years in the ecosystem as a whole.</p><p>I've been meaning to write this post since the first release candidate for Go 1.24 landed, but after reading Howard John's <a href="https://blog.howardjohn.info/posts/go-tools-command/">Exploring the new "go tool" support in Go 1.24</a> this morning, I thought I should write my thoughts up.</p><h2 id="what-is-it">What is it?</h2><p>Within your Go codebases, there's often some additional tools that you need to have installed to be able to build/test/deploy the project.</p><p>Sometimes this will a dependency that's needed for <code>go generate</code>ing, or it may be that you want to pipe your <code>go test</code> output into a JUnit-compatible format, so your CI platform can provide more useful metadata.</p><p>For each of these, you have two choices:</p><ul><li>require that the user knows how to install them, i.e. by knowing to run <code>make deps</code> or <code>just setup</code> before building anything on the project (which will then i.e. <code>go install</code> the commands)</li><li>use the <a href="https://www.jvt.me/posts/2022/06/15/go-tools-dependency-management/"><code>tools.go</code> pattern</a> to make it so you can <em>just</em> run <code>go generate</code>, and that'll call the right dependency via <code>go run</code></li></ul><p>My preference is <a href="https://www.jvt.me/posts/2022/06/15/go-tools-dependency-management/"><code>tools.go</code> pattern</a>, but there are two key problems with this approach.</p><p>Firstly, there's a performance hit of using a <code>tools.go</code>. It's something that is <em>slightly</em> noticeable, moreso if your project relies upon a lot of <code>go run</code> i.e. with lots of <code>go generate</code>s, because prior to Go 1.24, the <code>go run</code> invocations were not cached.</p><p>Secondly, it also leads to dependency tree bloat, because you have to record your dependency on i.e. <code>github.com/sqlc-dev/sqlc/cmd/sqlc</code> which then gets recorded in your <code>go.mod</code>, and then anyone using <em>your module</em> will then see that as an indirect (transitive) dependency.</p><p>This was something we <a href="https://www.jvt.me/posts/2023/10/23/oapi-codegen-v2-decrease/">worked on for <code>oapi-codegen</code>'s v2 release</a> to further reduce unnecessary dependencies, and make things a bit cleaner for our consumers. This is somewhat mitigated by Go's <a href="https://go.dev/ref/mod#graph-pruning">module graph pruning</a> which won't download dependencies that aren't used, but consumers may still see the dependencies coming in as an indirect dependency, which may not be ideal (especially as it can then bloat their indirect dependencies, which then gets passed on to their consumers and so on .</p><p>Dependency tree bloat can also be further mitigated by splitting your <a href="https://www.jvt.me/posts/2024/09/30/go-tools-module/"><code>tools.go</code> into a separate module</a>, which makes it more awkward to invoke dependencies but makes sure that none of your consumers will be seeing any tool-related dependencies.</p><p>For those who know me as co-maintainer of <a href="https://github.com/oapi-codegen/oapi-codegen">oapi-codegen</a>, you'll know that the <code>tools.go</code> pattern is our <a href="https://github.com/oapi-codegen/oapi-codegen#install">explicit recommendation</a> and we believe is better than installing it as a binary, so it's probably unsurprising that I'm very excited about this as an option to manage dependencies.</p><h2 id="how-does-it-work">How does it work?</h2><p>I've started playing around with this <a href="https://gitlab.com/tanna.dev/dependency-management-data/-/commits/spike/go-tools-124">on a branch</a> of the <a href="https://dmd.tanna.dev/">dependency-management-data</a> project, where I've got a mix of different tools that need to be installed and used.</p><p>Let's take a worked example of how we'd move over calls to <code>oapi-codegen</code> to the new <code>go tool</code> pattern.</p><h3 id="existing-state">Existing state</h3><p>For instance let's say that we have the following <code>tools.go</code> in its own module:</p><pre tabindex="0"><code data-lang="gomod"># tools/go.mod
module dmd.tanna.dev/tools

go 1.22.0

require (
	github.com/99designs/gqlgen v0.17.49
	github.com/oapi-codegen/oapi-codegen/v2 v2.4.1
	github.com/sqlc-dev/sqlc v1.26.0
)
</code></pre><p>We can then see that we invoke this via <code>go run</code>:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>// internal/ecosystems/generate.go
</span></span></span><span><span><span>//go:generate go run -modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span></code></pre></div><h3 id="migrating">Migrating</h3><p>To start migrating over to <code>go tool</code>, we need to make sure that we've first pulled in the new version of Go in our top-level Go module:</p><div><pre tabindex="0"><code data-lang="diff"><span><span> module dmd.tanna.dev
</span></span><span><span>
</span></span><span><span><span>-go 1.22.7
</span></span></span><span><span><span></span><span>+go 1.24
</span></span></span><span><span><span></span>
</span></span><span><span><span>-toolchain go1.23.2
</span></span></span><span><span><span></span><span>+toolchain go1.24rc2
</span></span></span></code></pre></div><p>Next, we need to pull in a <code>tool</code> dependency on <code>oapi-codegen</code>'s CLI tool - notice that you need <strong>the full path</strong> to the command that's being invoked:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># NOTE the full import path</span>
</span></span><span><span>% go get -tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@v2.4.1
</span></span></code></pre></div><p>We could also do this by hand, but doing it via <code>go get</code> simplifies this a little.</p><p>From here, we'll notice that our <code>go.mod</code> has a few other changes:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>@@ -57,12 +57,16 @@ require (
</span></span></span><span><span><span></span>        github.com/cenkalti/backoff/v4 v4.3.0 // indirect
</span></span><span><span>        github.com/cespare/xxhash/v2 v2.3.0 // indirect
</span></span><span><span>        github.com/charmbracelet/lipgloss v0.10.0 // indirect
</span></span><span><span><span>+       github.com/dprotaso/go-yit v0.0.0-20220510233725-9ba8df137936 // indirect
</span></span></span><span><span><span></span>        github.com/dustin/go-humanize v1.0.1 // indirect
</span></span><span><span>        github.com/felixge/httpsnoop v1.0.4 // indirect
</span></span><span><span><span>+       github.com/getkin/kin-openapi v0.127.0 // indirect
</span></span></span><span><span><span></span>        github.com/go-ini/ini v1.67.0 // indirect
</span></span><span><span>        github.com/go-logfmt/logfmt v0.6.0 // indirect
</span></span><span><span>        github.com/go-logr/logr v1.4.2 // indirect
</span></span><span><span>        github.com/go-logr/stdr v1.2.2 // indirect
</span></span><span><span><span>+       github.com/go-openapi/jsonpointer v0.21.0 // indirect
</span></span></span><span><span><span>+       github.com/go-openapi/swag v0.23.0 // indirect
</span></span></span><span><span><span></span>        github.com/gobwas/glob v0.2.3 // indirect
</span></span><span><span>        github.com/google/go-querystring v1.1.0 // indirect
</span></span><span><span>        github.com/gorilla/mux v1.8.1 // indirect
</span></span><span><span><span>@@ -72,16 +76,22 @@ require (
</span></span></span><span><span><span></span>        github.com/hashicorp/go-retryablehttp v0.7.5 // indirect
</span></span><span><span>        github.com/hashicorp/golang-lru/v2 v2.0.7 // indirect
</span></span><span><span>        github.com/inconshreveable/mousetrap v1.1.0 // indirect
</span></span><span><span><span>+       github.com/invopop/yaml v0.3.1 // indirect
</span></span></span><span><span><span>+       github.com/josharian/intern v1.0.0 // indirect
</span></span></span><span><span><span></span>        github.com/klauspost/compress v1.17.11 // indirect
</span></span><span><span>        github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
</span></span><span><span><span>+       github.com/mailru/easyjson v0.7.7 // indirect
</span></span></span><span><span><span></span>        github.com/mattn/go-isatty v0.0.20 // indirect
</span></span><span><span>        github.com/mattn/go-runewidth v0.0.15 // indirect
</span></span><span><span>        github.com/mitchellh/mapstructure v1.5.0 // indirect
</span></span><span><span><span>+       github.com/mohae/deepcopy v0.0.0-20170929034955-c48cc78d4826 // indirect
</span></span></span><span><span><span></span>        github.com/muesli/reflow v0.3.0 // indirect
</span></span><span><span>        github.com/muesli/termenv v0.15.2 // indirect
</span></span><span><span>        github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
</span></span><span><span>        github.com/ncruces/go-strftime v0.1.9 // indirect
</span></span><span><span><span>+       github.com/oapi-codegen/oapi-codegen/v2 v2.4.1 // indirect
</span></span></span><span><span><span></span>        github.com/olekukonko/tablewriter v0.0.5 // indirect
</span></span><span><span><span>+       github.com/perimeterx/marshmallow v1.1.5 // indirect
</span></span></span><span><span><span></span>        github.com/prometheus/client_golang v1.20.5 // indirect
</span></span><span><span>        github.com/prometheus/client_model v0.6.1 // indirect
</span></span><span><span>        github.com/prometheus/common v0.60.1 // indirect
</span></span><span><span><span>@@ -91,8 +101,10 @@ require (
</span></span></span><span><span><span></span>        github.com/rivo/uniseg v0.4.7 // indirect
</span></span><span><span>        github.com/sirupsen/logrus v1.9.4-0.20230606125235-dd1b4c2e81af // indirect
</span></span><span><span>        github.com/sosodev/duration v1.3.1 // indirect
</span></span><span><span><span>+       github.com/speakeasy-api/openapi-overlay v0.9.0 // indirect
</span></span></span><span><span><span></span>        github.com/spf13/pflag v1.0.5 // indirect
</span></span><span><span>        github.com/tchap/go-patricia/v2 v2.3.1 // indirect
</span></span><span><span><span>+       github.com/vmware-labs/yaml-jsonpath v0.3.2 // indirect
</span></span></span><span><span><span></span>        github.com/xeipuuv/gojsonpointer v0.0.0-20190905194746-02993c407bfb // indirect
</span></span><span><span>        github.com/xeipuuv/gojsonreference v0.0.0-20180127040603-bd5ef7bd5415 // indirect
</span></span><span><span>        github.com/yashtewari/glob-intersection v0.2.0 // indirect
</span></span><span><span><span>@@ -110,11 +122,13 @@ require (
</span></span></span><span><span><span></span>        go.opentelemetry.io/otel/metric v1.32.0 // indirect
</span></span><span><span>        go.opentelemetry.io/proto/otlp v1.3.1 // indirect
</span></span><span><span>        golang.org/x/exp v0.0.0-20231108232855-2478ac86f678 // indirect
</span></span><span><span><span>+       golang.org/x/mod v0.18.0 // indirect
</span></span></span><span><span><span></span>        golang.org/x/net v0.30.0 // indirect
</span></span><span><span>        golang.org/x/oauth2 v0.23.0 // indirect
</span></span><span><span>        golang.org/x/sys v0.27.0 // indirect
</span></span><span><span>        golang.org/x/term v0.25.0 // indirect
</span></span><span><span>        golang.org/x/time v0.5.0 // indirect
</span></span><span><span><span>+       golang.org/x/tools v0.22.0 // indirect
</span></span></span><span><span><span></span>        google.golang.org/genproto/googleapis/api v0.0.0-20241104194629-dd2ea8efbc28 // indirect
</span></span><span><span>        google.golang.org/genproto/googleapis/rpc v0.0.0-20241104194629-dd2ea8efbc28 // indirect
</span></span><span><span>        google.golang.org/grpc v1.68.0 // indirect
</span></span><span><span><span>@@ -128,3 +142,5 @@ require (
</span></span></span><span><span><span></span>        modernc.org/token v1.1.0 // indirect
</span></span><span><span>        sigs.k8s.io/yaml v1.4.0 // indirect
</span></span><span><span> )
</span></span><span><span><span>+
</span></span></span><span><span><span>+tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen
</span></span></span></code></pre></div><p>From here, we can see:</p><ul><li>there is a <code>tool</code> directive for <code>github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen</code></li><li>the containing Go module for the CLI, <code>github.com/oapi-codegen/oapi-codegen/v2</code>, is now an <code>indirect</code> dependency</li><li>any other required dependencies of <code>github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen</code> are now <code>indirect</code> dependencies</li></ul><p>Now we've done this, we could run:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>% go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --help
</span></span><span><span>Usage of /home/jamie/.cache/go-build/0e/0e04736601c8bbef785d372de02859bf8f39405aae9ccbf371477b0f2d8df755-d/oapi-codegen:
</span></span><span><span><span># ...</span>
</span></span></code></pre></div><p>With this tool set up, we can now modify i.e. <code>internal/ecosystems/generate.go</code> like so to use the new <code>go tool</code>:</p><div><pre tabindex="0"><code data-lang="diff"><span><span> package ecosystems
</span></span><span><span>
</span></span><span><span><span>-//go:generate go run -modfile=../../tools/go.mod github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span><span><span><span></span><span>+//go:generate go tool github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen --config=config.yaml openapi.yaml
</span></span></span></code></pre></div><p>Then running <code>go generate ./internal/ecosystems</code> works as it did before 🚀</p><h2 id="performance-implications">Performance implications</h2><p>A less scientific view than Howard John's article above, but we can see a slight improvement in performance:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># first time using `go tool`, from a fresh cache directory</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  55.05s user 4.57s system 531% cpu 11.220 total
</span></span><span><span><span># a subsequent call</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  0.59s user 0.18s system 424% cpu 0.181 total
</span></span><span><span><span># another just to see</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  0.57s user 0.25s system 404% cpu 0.202 total
</span></span></code></pre></div><p>Compare this to the previous implementation:</p><div><pre tabindex="0"><code data-lang="sh"><span><span><span># first time using `go run`, from a fresh cache directory</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  50.29s user 3.67s system 536% cpu 10.063 total
</span></span><span><span><span># a subsequent call</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  1.04s user 0.21s system 185% cpu 0.677 total
</span></span><span><span><span># another just to see</span>
</span></span><span><span>% <span>time</span> go generate ./internal/ecosystems
</span></span><span><span>go generate ./internal/ecosystems  1.02s user 0.26s system 191% cpu 0.669 total
</span></span></code></pre></div><p>Notice that the first call is similar in speed, but the use of <code>go tool</code>'s subsequent calls are still faster.</p><p>I'm a big fan of the fact that as of Go 1.24+ the <code>go run</code>s will be cached, so even if you don't move over to <code>go tool</code>, you'll get a performance boost!</p><h2 id="concerns">Concerns</h2><p>Now, there are still a few things I've noticed while doing the migration that aren't necessarily what I expected.</p><h3 id="gomod-implications"><code>go.mod</code> implications</h3><p>Something interesting is that the usage of the <code>tool</code> dependencies being treated as an <code>indirect</code> dependency is that they're present in the dependency tree, and treated like any other <code>indirect</code> dependency.</p><p>I'd also have preferred that we had just used <code>// tool</code> instead of <code>// indirect</code>, but I can see why this is likely the choice that's made - so they're treated like any other dependency - but making them less clear as only being required for tools could lead to issues with clashing dependencies, or where you upgrade an <code>indirect</code> dependency and then that breaks other things.</p><p>This means that tools such as Renovate need to be a little more involved in how to do the updates, but <a href="https://github.com/renovatebot/renovate/discussions/33867">that's all in hand</a>.</p><h2 id="gqlgen-fails-to-run-with-go-124rc2"><code>gqlgen</code> fails to run with Go 1.24rc2</h2><p>Something I've noticed while playing around with this is that <a href="https://github.com/99designs/gqlgen/issues/3505"><code>gqlgen</code> struggles to run with Go 1.24rc2</a>, which <a href="https://github.com/golang/go/issues/71448">feels like an upstream Go issue</a>, but it looks like that may be related to the use of <code>/x/tools</code> 🤔</p><p>It may be interesting to find out what else gets affected by this - please give the RC a test!</p><h2 id="closing">Closing</h2><p>Overall, I'm feeling very positive about it, and improving the way that dependencies get installed <em>if they should be built from source</em>, but there are dependencies such as <code>golangci-lint</code> which <a href="https://golangci-lint.run/welcome/install/#install-from-sources">don't recommend building from source</a> and instead using their pre-built binaries, which is fair, and is unlikely to change here.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're bringing Pebble back (2081 pts)]]></title>
            <link>https://repebble.com/</link>
            <guid>42845091</guid>
            <pubDate>Mon, 27 Jan 2025 20:11:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://repebble.com/">https://repebble.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42845091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="text-container">
            
            
            
            <div>
                <div>
                    <h2>We're making new Pebble watches</h2>
                    <p>I've tried pretty much every other smartwatch on Earth, yet I still wear my Pebble every day—nothing else matches its features and long battery life. I really, <em>really</em>, <strong><em>really</em></strong> hoped someone else would create a proper replacement, but no one has stepped up, and my stash of old Pebbles is dwindling!</p>
                    <p>It's time to take matters into my own hands. A small team and I are working on a new Pebble-like smartwatch that runs open source PebbleOS, has the same beloved features (plus some fun new stuff), and stays true to the core Pebble vision. If enough people are interested, we'll build it. <a href="https://repebble.com/signup.html">Sign up</a> to get one!</p>
                    <a href="https://ericmigi.com/blog/why-were-bringing-pebble-back" target="_blank">
                        <h3>Read the full blog post</h3>
                        <p>Why We're Bringing Pebble Back</p>
                    </a>
                </div>

                <div>
                    <h2>PebbleOS is now open source</h2>
                    <p>Google (which purchased Fitbit, which had bought Pebble) still owns PebbleOS. Over the last year, a team inside Google (including some amazing ex-Pebblers turned Googlers) has been working on open sourcing the OS! The source code for PebbleOS is now available at <a href="https://github.com/google/pebble" target="_blank">github.com/google/pebble</a>. Read more on their <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html" target="_blank">blog</a>.</p>
                    <p>Thank you so much, Google! I can't stress how thankful I am to the individuals who did the heavy lifting. This was also made possible by the <a href="https://rebble.io/" target="_blank">Rebble</a> team and community, who have supported Pebble since it shut down. Check out the vibrant <a href="https://reddit.com/r/pebble" target="_blank">r/Pebble</a> and <a href="https://discordapp.com/invite/aRUAYFN" target="_blank">Discord</a>.</p>
                </div>
                
                <div>
                    <h2>What happens now?</h2>
                    <p>The source code that powers each Pebble smartwatch is now freely available to download, modify and improve on <a href="https://github.com/google/pebble" target="_blank">Github</a>. Want a reminder of how awesome Pebble OS is? Dive back into the <a href="https://ericmigi.com/blog/pebbleos-is-awesome" target="_blank">beautiful, retro, pixelated world</a> of Pebble.</p>
                    <p>Anyone can use PebbleOS in any way they want. You can get it working on existing Pebble watches, emulate it, run it on other embedded devices, or create new hardware specifically for it. <a href="https://github.com/pebble-dev/pebbleos" target="_blank">Learn more</a> about PebbleOS.</p>
                    <p>We're setting out to bring Pebble back, we'd love for you to join the fun!</p>
                </div>
            </div>

            <h2 id="do-you-want-one">Do you want a new Pebble?</h2>
            
            <div>
                <p>Eric Migicovsky<br>Founder of Pebble</p>
                <div>
                    <h3>Wait, what is Pebble again?</h3>
                    <p>Pebble is an e-paper smartwatch with simple functionality, long battery life, and fun, quirky design. It first launched on <a href="https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android" target="_blank">Kickstarter</a> in 2012 and sold over 2 million watches before the company's IP was sold to Fitbit in 2016.</p>
                </div>
                <p id="smallText">© Copyright <span id="year">----</span> Core Devices LLC. All Rights Reserved.<br><a href="https://repebble.com/privacy.html">Privacy</a> · <a href="https://repebble.com/terms.html">Terms</a> · <a href="https://twitter.com/pebble" target="_blank">Twitter</a></p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google open-sources the Pebble OS (1128 pts)]]></title>
            <link>https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html</link>
            <guid>42845070</guid>
            <pubDate>Mon, 27 Jan 2025 20:09:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html">https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5561126018697023258" itemprop="articleBody">
<meta name="twitter:image" content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png">
<p>

<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png" imageanchor="1"><img data-original-height="800" data-original-width="100%" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHCX-mb_DqHgkNn1By45jRl-t4yGY82D79aFivyvhLIjiW9oglYr2fu7qOXFTEPj4sg-18anq6Aydli437ogx_AfTNI4V8Kq9Wjm1pPpOpqsSG1aiTwNLURTHgzFTeND8VuCxmndTLxT48Hr5RQgWvilKyeI9ORfoRNE40ZyqV49xuxTNarCAIoErsYbw/s1600/Pebble-Smartwatch%20%281%29.png"></a></p><p>We are excited to announce that the source code that powered Pebble smartwatches is now <a href="https://github.com/google/pebble" target="_blank">available for download</a>.</p>

<p>This is part of an effort from Google to help and support the <a href="https://rebble.io/" target="_blank">volunteers</a> who have come together to maintain functionality for Pebble watches after the original company ceased operations in 2016.</p><br>

<h3><b>A quick look back</b></h3>

<p>Pebble was initially launched through a very successful <a href="https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android" target="_blank">Kickstarter project</a>. Pebble’s first Kickstarter was the single most funded at the time, and its successor Kickstarter for the <a href="https://www.kickstarter.com/projects/getpebble/pebble-time-awesome-smartwatch-no-compromises" target="_blank">Pebble Time</a> repeated that feat – and remains the second most funded today! Over the course of four years, Pebble sold over two million smartwatches, cultivating a thriving community of thousands of developers who created over ten thousand Pebble apps and watchfaces.</p>

<p>In 2016, Fitbit acquired Pebble, including Pebble’s intellectual property. Later on, Fitbit itself was acquired by Google, taking the Pebble OS with it.</p>

<p>Despite the Pebble hardware and software support being discontinued eight years ago, Pebble still has thousands of dedicated fans.</p><br>

<h3><b>What is being released</b></h3>

<p>We are releasing most of the source code for the Pebble operating system. This repository contains the entire OS, which provides all the standard smartwatch functionality – notifications, media controls, fitness tracking, and support for custom apps and watchfaces – on tiny ARM Cortex-M microcontrollers. Built with <a href="https://www.freertos.org/" target="_blank">FreeRTOS</a>, it contains multiple modules for memory management, graphics, and timekeeping, as well as an extensive framework to load and run custom applications written in C, as well as in Javascript via the <a href="https://jerryscript.net/" target="_blank">Jerryscript</a> Javascript engine. The Pebble architecture allowed for a lightweight system delivering a rich user experience as well as a very long battery life.</p>

<p>It's important to note that some proprietary code was removed from this codebase, particularly for chipset support and the Bluetooth stack. This means the code being released contains all the build system files (using the <a href="https://waf.io/" target="_blank">waf</a> build system), but it will not compile or link as released.</p><br>

<h3><b>The path forward</b></h3>

<p>From here, we are hoping this release will assist the dedicated community and volunteers from the <a href="https://rebble.io/" target="_blank">Rebble project</a> to carry forward the support for Pebble watches that users still love. For someone to build a new firmware update, there is a non-trivial amount of work to do in finding replacements for the pieces that were stripped out of this code, as well as updating this source code that has not been maintained for a few years.</p>

<p><i>By Matthieu Jeanson, Katharine Berry, and Liam McLoughlin</i></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google has open-sourced the Pebble smartwatch operating system (369 pts)]]></title>
            <link>https://rebble.io/2025/01/27/the-future-of-rebble.html</link>
            <guid>42845017</guid>
            <pubDate>Mon, 27 Jan 2025 20:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rebble.io/2025/01/27/the-future-of-rebble.html">https://rebble.io/2025/01/27/the-future-of-rebble.html</a>, See on <a href="https://news.ycombinator.com/item?id=42845017">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <div>
	    <p>Jan 27, 2025 • by <a href="https://rebble.io/team#person-Will%20Murphy">Will Murphy</a></p>
        
        <p>Today we’re excited to announce several developments which will affect the future of Rebble. Let’s get straight into it, starting with the big one…</p>

<h2 id="-google-open-sources-tintin">🎉 Google Open Sources Tintin</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/the-loop.png" alt=""></p>

<p>Today Google <a href="https://github.com/google/pebble">announced that they have released the source code to PebbleOS</a>. This is massive for Rebble, 
and will accelerate our efforts to produce new hardware.</p>

<p>Previously, we have been working on our own replacement firmware: <a href="https://github.com/pebble-dev/RebbleOS">RebbleOS</a>. As you can see by the commit history though, progress was slow.
Building a production-ready realtime OS for the Pebble is no small feat, and although we were confident we’d get there given enough time, it was never our ideal path.
Thanks to the hard work of many people both within Google and not, we finally have our hands on the original source code for PebbleOS. You can read <a href="https://opensource.googleblog.com/2025/01/see-code-that-powered-pebble-smartwatches.html">Google’s blog post on this for even more information.</a></p>

<p>This does <em>not</em> mean we instantly have the ability to start developing updates for PebbleOS though, we first will need to spend some concentrated time getting it to build. 
But before we talk about that, let’s talk about Rebble itself.</p>

<!--more-->



<p>With a long term plan for the Rebble community starting to coalesce, the
longevity of Rebble is more important than ever.  We’re excited to say that
Rebble is transforming into a non-profit to formalize what we’ve all always
hoped for: the community (that’s you!) are the owners of Rebble!  Rebble has
always been about preserving these humble little smartwatches as a little
oasis of user-respectful technology in a desert of big corporations trying
to sell your attention, and we’re excited to have a legal framework that
lets us codify our missions of: educating people about why these are
important; using them as a platform to teach embedded systems; preserving
the history of this quirky little platform; and building open source
software for the public good to keep the dream going long into the future.</p>

<p>It’s still early days, but more information will be available at <a href="https://rebble.foundation/">rebble.foundation</a> as soon<img title=":tm:" alt=":tm:" src="https://github.githubassets.com/images/icons/emoji/unicode/2122.png" height="20" width="20"> as we have it. 
In the mean time, expect more hackathons from us, now that we have a
framework to run them!  Oh, and speaking of which…</p>

<h2 id="-the-rebbleos-hackathon">💻 The RebbleOS Hackathon</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/hackathon-002.gif" alt=""></p>

<p>The <a href="https://rebble.io/2023/05/12/a-look-back-at-the-rebble-hackathon.html">last Rebble hackathon</a> was so much fun, and we’ve been wanting to do another for some time. 
The Rebble project is a fantastic example of what community can achieve, and we intend to build on this in 2025 and beyond.</p>

<p>Writing Pebble apps is a fantastic way to delve into the world of embedded systems, and what better way to do that than with a hackathon?</p>

<p>Mark your calendars for the <strong>1st - 8th of March</strong> as we work on RebbleOS and other apps, and encourage you to do the same!</p>

<p>For more information see <a href="https://rebble.io/hackathon-002">/hackathon-002</a></p>

<h2 id="-old-dog-new-tricks">🐶 Old Dog, New Tricks</h2>

<p><img src="https://rebble.io/images/tintin-blog-post/snowy.png" alt=""></p>

<p>We’re also happy to announce that we’ve acquired the <a href="https://github.com/pebble-dev/snowy">source code for Snowy</a>! 
<a href="https://apps.rebble.io/en_US/application/561960c8a1dd2652af00000d">Snowy</a> was one of the most popular assistants for the Pebble, and is still a useful companion today.
However, given the current landscape of LLMs and voice assistants it is definitely due an upgrade, so expect to see this old dog appear in the hackathon.</p>

<h2 id="️-thats-all-for-now">🗒️ That’s all for now</h2>

<p>Between everything above, and the fact that progress continues on our replacement mobile app, the future of Rebble has never looked so bright. We are committed to an open-source community-owned smartwatch, and these announcements bring that reality even closer.
A huge thank you to everyone in the Pebble-verse who made this happen, especially those internal to Google who have helped ensure PebbleOS’s future. We’d like to especially thank Liam McLoughlin and Matthieu Jeanson, as well as Rebble superstar Katharine Berry. Thank you also to the many other Googlers who made this possible – and a massive shout out to Eric Migicovsky for ensuring this happened (and for creating Pebble in the first place).</p>

<p>One more shoutout: we would like to thank, of course, you!  Without all of you Rebblers who have been entrusting us for the past 8 years to keep the dream of an open-platform user-respectful smartwatch alive, PebbleOS wouldn’t be relevant at all today.  Your cumulative $3s a month have reminded the world that Pebble is worth preserving, and worth building on.  We love this platform, and we’re glad that you do too.  Thank you so much.</p>

<p>Stay tuned for more updates as the Hackathon launches, and when we have the first working versions of the new RebbleOS!</p>

<p>- Will ❤️</p>

<h3 id="clarifications">Clarifications:</h3>

<h4 id="how-can-i-get-involved-with-the-hackathon">How can I get involved with the hackathon?</h4>
<p>See <a href="https://rebble.io/hackathon-002/">here.</a></p>

<h4 id="did-google-gift-pebbleos-to-rebble-specifically">Did Google gift PebbleOS to Rebble specifically?</h4>
<p>No, Google have open sourced the PebbleOS to everyone, Rebble plans to make good use of this.</p>


<p>No. If you’re reading about another PebbleOS project somewhere other than this blog, it does not involve us.</p>

<h4 id="what-if-i-have-more-questions">What if I have more questions?</h4>
<p>Reach out to us at support@rebble.io, or drop a message <a href="https://rebble.io/discord">on Discord.</a></p>

    </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Alpha Myth: How captive wolves led us astray (278 pts)]]></title>
            <link>https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves</link>
            <guid>42844619</guid>
            <pubDate>Mon, 27 Jan 2025 19:21:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves">https://anthonydavidadams.substack.com/p/the-alpha-myth-how-captive-wolves</a>, See on <a href="https://news.ycombinator.com/item?id=42844619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>In 1947, at Switzerland's Basel Zoo, animal behaviorist Rudolf Schenkel peered into an enclosure of captive wolves, meticulously documenting their interactions. What he witnessed – aggressive displays of dominance, rigid hierarchies, the emergence of an "alpha" male – would spawn decades of misunderstanding about power, leadership, and masculinity. His observations were later popularized by biologist L. David Mech in his influential 1970 book, "The Wolf: Ecology and Behavior of an Endangered Species," cementing the alpha wolf concept in both scientific literature and popular imagination.</p><div><figure><a target="_blank" href="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw"><img src="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080" width="6000" height="4000" data-attrs="{&quot;src&quot;:&quot;https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:4000,&quot;width&quot;:6000,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;silhouette of dog&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="silhouette of dog" title="silhouette of dog" srcset="https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 424w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 848w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1272w, https://images.unsplash.com/photo-1511561415413-c643d4969838?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wzMDAzMzh8MHwxfHNlYXJjaHwxfHx3b2xmfGVufDB8fHx8MTczNzk5OTg0M3ww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=1080 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Years later, studying wolves in the wilderness of Minnesota, Mech discovered something striking. </span><strong>In their natural habitat, wolf packs operated nothing like the prison-yard dynamics he'd observed in the zoo.</strong><span> Instead of hierarchies maintained through aggression, he found family units guided by experienced parents. Leadership wasn't seized through dominance – it was earned through nurturing, teaching, and protecting the collective good.</span></p><blockquote><p>"I felt responsible for unleashing this monster," Mech would later write about his initial research. "The concept of the alpha wolf as a 'top dog' fighting for dominance had become ingrained in our culture, but it was based on artificial conditions. Unfortunately, we've built much of our understanding of power and leadership on a foundation of captive behavior."</p></blockquote><p> This revelation would come too late – the captive wolf model had already escaped into human culture, shaping everything from executive leadership to dating advice, and creating what we might now recognize as a profound misunderstanding of power itself.</p><p><span>The irony is that in attempting to model human behavior on what we thought was "natural" wolf psychology, we instead normalized the very behaviors that emerge from unnatural confinement. Just as captive wolves exhibit exaggerated aggression and dominance, humans operating within rigid hierarchies and crushing social expectations often adopt similarly distorted patterns – what we might call </span><strong>"captive male syndrome."</strong></p><p>Consider how these dynamics manifest in Silicon Valley, where Facebook's infamous "move fast and break things" mantra shaped a generation of tech culture. This emphasis on speed and disruption at any cost has created work environments that mirror the artificial pressures of captivity, where displaying dominance often takes precedence over fostering sustainable innovation.</p><p>The toll is measurable. According to a recent survey by Blind, an anonymous professional network, 57% of tech employees report experiencing burnout – a stark indicator of an industry grappling with unsustainable expectations. </p><p><span>The parallels to captive wolf behavior are striking. Just as confined wolves display exaggerated aggression and dominance to cope with their unnatural environment, tech founders often find themselves performing an amplified version of leadership – one that prioritizes the appearance of unwavering strength over authentic collaboration and sustainable innovation. This culture of </span><strong>performative dominance</strong><span>, according to workplace researchers at Pluralsight, directly contributes to chronic exhaustion, disengagement, and a diminished sense of accomplishment among workers.</span></p><p>The costs are steep. Research from the American Psychological Association shows that men who strongly adhere to traditional "alpha" masculine norms are:</p><ul><li><p>More likely to suffer from depression and anxiety</p></li><li><p>Less likely to seek help</p></li><li><p>Report lower relationship satisfaction</p></li><li><p>Struggle to maintain close friendships</p></li></ul><p>The very traits we've coded as strength – emotional stoicism, aggressive competition, rejection of vulnerability – turn out to be profound weaknesses.</p><p><span>But there's hope in the wilderness. Just as Mech's later research revealed the true nature of wolf leadership, innovative organizations are discovering the power of what we might call </span><strong>"wild leadership"</strong><span> – approaches that embrace our natural capacities for cooperation and care.</span></p><p>Take Patagonia, where founder Yvon Chouinard deliberately rejected the alpha CEO model. Instead of ruling from above, he built a flat structure where decisions emerge from collaboration. The company's "let my people go surfing" philosophy – which encourages employees to step away from work to pursue passion and maintain balance – seems radical only because we've normalized captive behavior. The results speak for themselves: Patagonia enjoys employee turnover rates 25% lower than industry averages and consistently outperforms more traditionally structured competitors.</p><p>The path forward requires more than just rejecting the alpha male myth. We need to redesign the structures that created captive behavior in the first place. This means rethinking everything from how we raise boys to how we run companies. It means creating space for a masculinity that draws strength from connection rather than competition, from nurturing rather than dominating.</p><p><em>In the end, we might learn our most important leadership lessons not from wolves in cages, but from those running free – where strength flows not from who can dominate, but from who can best ensure the pack's survival and flourishing. The question isn't whether you're alpha enough to lead. It's whether you're wise enough to leave the cage behind.</em></p><p>To join our next “Wild Leadership” men’s event or to explore deeper work for you or your organization, reach out. </p><p><em><strong>Anthony David Adams</strong><span> created one of the “Top 25 Blogs Worldwide” according to TIME / CNN; is known as the “unicorn whisperer” for his </span><a href="http://earthpilot.org/" rel="nofollow ugc noopener">high-stakes performance advisory work</a><span> with once-in-a generation talent from founders to surgeons to broadway legends; was </span><a href="https://www.thirteen.org/programs/mysteries-of-mental-illness/episode-4-preview-new-frontiers-8ssere/" rel="nofollow ugc noopener">the first person to openly administer underground MDMA on national TV with PBS</a><span>; and is the founder of </span><a href="http://earthpilot.ai/" rel="nofollow ugc noopener">EarthPilot : Mission Support for Spaceship Earth</a><span>. </span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Taylorator – All Your Frequencies Are Belong to Us (292 pts)]]></title>
            <link>https://www.scd31.com/posts/taylorator</link>
            <guid>42843623</guid>
            <pubDate>Mon, 27 Jan 2025 17:42:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scd31.com/posts/taylorator">https://www.scd31.com/posts/taylorator</a>, See on <a href="https://news.ycombinator.com/item?id=42843623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><p>For the past two weeks or so, I've been working on constructing the Taylorator. The Taylorator is a piece of software which allows me to flood the FM broadcast band with Taylor Swift's music. No matter where you tune your radio, you will only be able to listen to her!</p>
<p>Okay, I admit that you could technically use the Taylorator to broadcast whatever music you want, so maybe it's a bit of a misnomer. But for some reason I figured this would be funnier.</p><p>What do I mean by flooding the FM broadcast band? Well, in Canada and the US (and maybe other places too), the FM broadcast band spans 88 MHz - 108 MHz. You can't broadcast wherever, though. Stations will only appear on odd-numbered frequencies, like 88.1 MHz, 94.5 MHz, 107.3 MHz, etc. There's a technical reason for this - every FM broadcast takes up about 150 KHz of bandwidth, and spacing the broadcasts like this allows for an extra 50 KHz of wiggle room.</p><p>This also works out to 100 different frequencies that we need to populate (with 100 different songs). So, how can we accomplish this?</p></section><h2>Software Defined Radio</h2><section><p>SDR, or Software Defined Radio, is a paradigm where you do most of your signal processing in software, and then a relatively dumb piece of hardware creates a real-world signal from this virtual signal. It works similarly to a sound card. It takes in a series of samples, and spits out a waveform that matches these samples.</p><p><a href="https://www.scd31.com/img/taylorator/limesdr.png"><img src="https://www.scd31.com/thumb/taylorator/limesdr.png"></a></p><p>They make SDRs that can transmit, receive, or do both. For this project we don't care about receive, and only need to be able to transmit. I chose to use a LimeSDR mini, because it can transmit, has a wide enough bandwidth, and I already had it lying around.</p><p>One important difference between a sound card and an SDR is that a sound card takes real-valued samples, and an SDR takes complex-valued samples. That is to say, each SDR sample can be presented as a single number <code>a + bi</code>, where <code>a</code> and <code>b</code> are real numbers. This is primarily done because it cuts the required sample rate in half, as it allows for negative frequencies. On the hardware side, this results in a simpler design, which lowers cost.</p></section><h2>Audio preparation</h2><section><p>There are a few things we need to do to our raw audio to prepare it for modulation. First of all, we want the sample rate of all of our different songs to be the same. I chose 44.1 KHz as the target sample rate, but 48 KHz would also be a sane value (or anything else, really). I wrote a rational resampler which does this by upsampling, linear interpolating, and decimating to the target sample rate.</p><p>Next, we need to run our audio through a low-pass filter and perform FM-preemphasis. I won't touch on low-pass filtering much, but FM-preemphasis is basically just a high-pass filter. The idea is that random atmospheric noise, when demoulated by an FM receiver, is biased towards high frequencies. We can get around this by boosting our high frequencies on transmit (pre-emphasis) and having the receiver attenuate the same frequencies (de-emphasis).</p><p>This is all done before the modulation actually starts. Modulation takes a ton of CPU power so we want to pre-compute whatever we can.</p></section><h2>FM modulation in software</h2><section><p>FM modulation follows a pretty simple formula. Basically, <code>y_n = e^(i*pi*sum(x))</code>, where <code>y_n</code> is the output sample, and <code>x</code> is the input audio stream up until this point. (Sorry, my blog doesn't support LaTeX! Maybe I should add that...) In other words, we're rotating around a circle, and the speed at which we rotate around this circle is dictated by the sum of all the audio samples we've seen up until this point. In the complex world, rotation speed is analogous to frequency, so this is all we need to build an FM modulator!</p><p>In practice, though, this is actually pretty difficult for our use-case. We need to modulate 100 audio streams at once. Each one needs to be offset by up to +/- 10 MHz, so we need to sample at 20MSPS. Performance was the name of the game, and I spent probably a week banging my head against my computer in order to get it to an acceptable level. Huge thanks to my friend Won, who gave me a ton of different ideas to try.</p><p>The current architecture modulates all audio channels at once, so we only need to write to our output array once (instead of reading/writing 100 times). The modulator is also responsible for "biasing" each channel to be centered around a different frequency. This used to be done as a separate stage but doing it as part of modulation sped things up significantly. Finally, rather than computing trig operations, a lookup table is generated which converts between phase angle and its complex number representation. This lookup table is also gain-compensated, so that when we add up the 100 outputs, we don't end up clipping.</p><p>I'm not convinced that I'm operating anywhere close to peak efficiency. There may be some huge DSP-specific shortcut that I'm overlooking - I'm certainly no expert. But the current code works well enough.</p></section><h2>Performance</h2><section><p>Due to the increase in required sample rate, as well as the increase in number of channels, required processing power grows at O(n^2), where n represents the amount of bandwidth we're covering.</p><p>On my laptop, which has a 10th-gen i5 CPU, I can only get to about 0.5x real-time performance if I target the entire FM broadcast band (88 MHz - 108 MHz). However, if I decrease that slightly to 88 - 104 MHz, then my laptop is able to handle it at slightly better than real-time.</p><p>My desktop, with a Ryzen 2700x CPU, fares better. Even only using a few cores, it can easily manage 2x real-time performance or more. This is a 7-year-old CPU, so the bar isn't actually too high here.</p><p>Memory usage is also pretty high. Since we're loading all songs into memory and pre-computing beforehand, it can take a few gigabytes to hold everything. With my music, I've noticed around 3.5 GB of RAM usage. On top of this, there's also per-thread RAM usage, though it should be less significant. There's certainly room for improvement here. Just changing how audio is stored could cut this usage in half, or more.</p></section><h2>Legality</h2><section><p>Whenever I told someone about the Taylorator during its development, the question I'd consistently get asked is, "is this legal?". This is going to depend on exactly where you live and exactly how you're using the Taylorator, but in general, I think the answer is... probably not.</p><p>There are generally cut-outs for very low power FM transmitters, like the ones people use in their car. Usually, though, this requires the transmitter itself to be licensed, and of course, the software I have written has no such license.</p><p>In practice, it's probably not a huge deal? A few mW spread over 20 MHz of bandwidth results in a pretty weak signal. Obviously, don't do illegal things (and if you do, don't tell me about it)! If you connected your SDR up to an amplifier you would almost certainly get in a bunch of trouble. So, uh, don't do that.</p></section><h2>Source Code</h2><section><p>As always, this project is <a href="https://gitlab.scd31.com/stephen/taylorator">open source</a>!</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek releases Janus Pro, a text-to-image generator [pdf] (815 pts)]]></title>
            <link>https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf</link>
            <guid>42843131</guid>
            <pubDate>Mon, 27 Jan 2025 16:57:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf">https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42843131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>

                  <li>
      
      
</li>

                    <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;white_papers_ebooks_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;white_papers_ebooks_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:deepseek-ai/Janus" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="AZh1BsnuYrMa8XYwSsvEvprN4QiDVKK9Ro9Izb5bgtAX9c5eSNLKzgF8xQC0Ip3REUpceX9BJ3f5srJV0VDakQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="deepseek-ai/Janus" data-current-org="deepseek-ai" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=deepseek-ai%2FJanus" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4a0f27a000a78e9a4f92df59b03c3d2e1143bca405fcd75da8c4841105eecdae" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bilinear down/upsampling, aligning pixel grids, and that infamous GPU half pixel (2021) (126 pts)]]></title>
            <link>https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/</link>
            <guid>42842270</guid>
            <pubDate>Mon, 27 Jan 2025 15:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/">https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/</a>, See on <a href="https://news.ycombinator.com/item?id=42842270">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif"><img data-attachment-id="4201" data-permalink="https://bartwronski.com/box_then_even_odd-6/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="box_then_even_odd-6" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" src="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" alt="" width="411" height="411" srcset="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=411 411w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif 576w" sizes="(max-width: 411px) 100vw, 411px"></a><figcaption>See this ugly pixel shift when <strong>upsampling a downsampled image</strong>? My post describes where it can come from and how to avoid those! </figcaption></figure></div>



<p>It’s been more than two decades of me using bilinear texture filtering, a few months since <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/">I’ve written about bilinear resampling</a>, but only two days since I discovered a bug of mine related to it. 😅 Similarly, just last week a colleague asked for a very fast implementation of bilinear on a CPU and it caused a series of questions “which kind of bilinear?”.</p>



<p>So I figured it’s an opportunity for another short blog post – on bilinear filtering, but in context of down/upsampling. We will touch here on <strong>GPU half pixel offsets, aligning pixel grids, a bug / confusion in Tensorflow, deeper signal processing</strong> analysis of what’s going on during bilinear operations, and analysis of the magic of the<strong> famous “magic kernel”</strong>.</p>



<p>I highly recommend <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/">my previous post</a> as a primer on the topic, as I’ll use some of the tools and terminology from there, but it’s not strictly required. Let’s go!</p>



<p><strong>Edit:</strong> I wrote a <a href="https://bartwronski.com/2021/07/20/processing-aware-image-filtering-compensating-for-the-upsampling/">follow-up post to this one</a>, about designing downsampling filters to compensate for bilinear filtering.</p>



<h2>Bilinear confusion</h2>



<p>The term bilinear upsampling and downsampling is used a lot, but what does it mean?&nbsp;</p>



<p>One of the few ideas I’d like to convey in this post is that <strong>bilinear upsampling / downsampling doesn’t have a single meaning or a consensus around this term use</strong>. Which is kind of surprising for a bread and butter type of image processing operation that is used all the time!</p>



<p>It’s also surprisingly hard to get it right even by image processing professionals, and a <a href="https://github.com/tensorflow/tensorflow/issues/6720">source of long standing bugs and confusion in top libraries</a> (and I know of some actual production bugs caused by this <strong>Tensorflow inconsistency</strong>)!</p>



<p><strong>Edit:</strong> there’s a blog post titled <em><a href="https://medium.com/hackernoon/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35">“How Tensorflow’s tf.image.resize stole 60 days of my life”</a></em> and it’s describing same issue. I know of some of my colleagues that spent months on fixing it in Tensorflow 2 – imagine effort of fixing incorrect uses and “fixing” already trained models that were trained around this bug… </p>







<p>Some parts of it like phase shifting are so tricky that a famous blog post of <strong>“magic kernel”</strong> comes up every few years and again, experts re(read) it a few times to figure out what’s going on there, while the author simply <a href="http://www.johncostella.com/magic/">rediscovered the bilinear</a>! (<strong>Important note:</strong> I don’t want to pick on the author, far from it, as he is a super smart and knowledgeable person, and willingness to share insights is always respect worthy. “Magic kernel” is just an example of why it’s so hard and confusing to talk about <strong>“bilinear”</strong>. I also respect how he amended and improved the post multiple times. But there is no “magic kernel”.)</p>



<p>So let’s have a look at what’s the problem. I will focus here exclusively on 2x up/downsampling and hope that some thought framework I propose and use here will be beneficial for you to also look at and analyze different (and non-integer factors).</p>



<p>Because of bilinear separability, I will again <strong>abuse the notation</strong> and call “bilinear” a filter when applied to 1D signals and generally a lot of my analysis will be in 1D.</p>



<h2>Bilinear downsampling and upsampling</h2>



<p>What do we mean by <strong>bilinear upsampling</strong>?</p>



<p>Let’s start with the most simple explanation, without the nitty gritty: it is creating a larger resolution image where every sample is created from bilinear filtering of a smaller resolution image.</p>



<p>For the bilinear downsampling, things get a bit muddy. It is using a bilinear filter to prevent signal aliasing when decimating the input image – ugh, lots of technical terms. I will circle back to it, but first address the first common confusion.</p>



<h3>Is this box or bilinear downsampling? Two ways of addressing it</h3>



<p>When downsampling images by 2, we every often use terms box filter and bilinear filter interchangeably. And both can be correct. How so?</p>



<p>Let’s have a look at the following diagram:&nbsp;</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image.png"><img data-attachment-id="4166" data-permalink="https://bartwronski.com/image-43/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image.png" data-orig-size="960,216" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=640" width="960" height="216" src="https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=960" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image.png 960w, https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image.png?w=768 768w" sizes="(max-width: 960px) 100vw, 960px"></a><figcaption>(Bi)linear vs box downsampling give us the same effective weights. <strong>Black dots</strong> represent <strong>pixel centers</strong>, upper row is the target/low resolution texture, and the bottom row the source, higher resolution one. Blue lines represents discretized weights of the kernel. </figcaption></figure></div>



<p>We can see that a <strong>2 tap box filter is the same as a 2 tap bilinear filter</strong>. The reason for it is that in this case, both filters are centered between the pixels. After discretizing them (evaluating filter weights at sample points), there is no difference, as we no longer know what was the formula to generate them, and how the filter kernel looked outside of the evaluation points.</p>



<p>The most typical way of doing bilinear downsampling is the same as box downsampling. Using those two names for 2x downsampling interchangeably is both correct! (Side note: Things diverge when taking about more than 2x downsampling. This might be a good topic for another blog post.) For 1D signals it means averaging every two elements together, for 2D images averaging 4 elements to produce a single one.</p>



<p>You might have noticed something that I implicitly assumed there – <strong>pixel centers there were shifted by half a pixel</strong>, and the edges/corners were aligned.</p>



<p>There is “another way” of doing bilinear downsampling, like this:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png"><img data-attachment-id="4168" data-permalink="https://bartwronski.com/image-1-8/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png" data-orig-size="517,219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=517" src="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=517" alt="" width="403" height="170" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=401 401w, https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-1.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-1.png 517w" sizes="(max-width: 403px) 100vw, 403px"></a><figcaption>A second take on bilinear downsampling – this time with pixel centers (black dots) aligned. Again the source image / signal is on the bottom, target signal on the top.</figcaption></figure></div>



<p>This one definitely and clearly is also a linear tent, and it doesn’t shift pixel centers. The resulting filter weights of [0.25 0.5 0.25] are also called a [1 2 1] filter, or the simplest case of a <a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/binomial_filters.pdf.old">binomial filter</a>, a very reasonable approximation to a <a href="https://en.wikipedia.org/wiki/Gaussian_filter">Gaussian filter</a>. (To understand why, see what happens to the binomial distribution as the trial count goes to infinity!). It’s probably the filter I use the most in my work, but I digress. 🙂</p>



<p>Why this second method is not used that much? This is by design and a reason for half texel shifts in GPU coordinates / samplers, and you might have noticed the problem – the last texel of high resolution array gets discarded. But let’s not get ahead of ourselves, first we can have a look at the relationship with upsampling.</p>



<h3>Two ways of bilinear upsampling – which one is “proper”?</h3>



<p>If you were to design a bilinear upsampling algorithm, there are a few ways to address it.</p>



<p>Let me start with a “naive” one that can have problems. We can take every original pixel, and between them just place averages of the other ones.</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png"><img data-attachment-id="4172" data-permalink="https://bartwronski.com/image-2-8/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png" data-orig-size="513,213" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-2" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=513" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=513" alt="" width="364" height="152" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=364 364w, https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-2.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-2.png 513w" sizes="(max-width: 364px) 100vw, 364px"></a><figcaption>Naive bilinear upsampling when pixel centers are aligned. Some pixels receive a copy of the source (green line), the other ones (alternating) a blend between two neighbors.</figcaption></figure></div>



<p>Is it bilinear / tent? Yes, it’s a tent filter on zero-inserted image (more on it later). It has an unusual property; some pixels get blurred, some pixels stay “sharp” (original copied).</p>



<p>But more importantly, if you do box/bilinear downsampling as described above, and then upsample an image, <strong>it will be shifted</strong>:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png"><img data-attachment-id="4174" data-permalink="https://bartwronski.com/image-3-7/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png" data-orig-size="743,300" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=743" alt="" width="510" height="206" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=510 510w, https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-3.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-3.png 743w" sizes="(max-width: 510px) 100vw, 510px"></a><figcaption>Using box downsampling, and then copy / interpolate upsampling shifts the image by half a pixel. This is a <strong>wrong way </strong>to do it! </figcaption></figure></div>



<p>Or rather – it will not correct for the half pixel shift created by downsampling.</p>



<p>It will work however with downsampling using the second method. The second method interpolates every single output pixel; all are interpolated:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png"><img data-attachment-id="4242" data-permalink="https://bartwronski.com/image-24-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png" data-orig-size="685,310" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-24" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=685" alt="" width="494" height="223" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=494 494w, https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-24.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-24.png 685w" sizes="(max-width: 494px) 100vw, 494px"></a><figcaption>When done properly, bilinear down/upsample doesn’t shift the image.</figcaption></figure></div>



<p>This another way of doing bilinear upsampling that might first feel initially <strong>unintuitive: every pixel is 0.75 of one pixel, and 0.25 of another one</strong>, alternating “to the left” and “to the right”. This is exactly what a GPU does when you upsample a texture by 2x:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png"><img data-attachment-id="4177" data-permalink="https://bartwronski.com/image-5-6/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png" data-orig-size="746,301" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-5" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=746" alt="" width="520" height="210" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=520 520w, https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-5.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-5.png 746w" sizes="(max-width: 520px) 100vw, 520px"></a></figure></div>



<p>There are two simple explanations for those “alternating” weights. The first, easiest one is just looking at the “tents” in this scheme:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png"><img data-attachment-id="4180" data-permalink="https://bartwronski.com/image-6-5/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png" data-orig-size="952,218" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-6" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=640" loading="lazy" width="952" height="218" src="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=952" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-6.png 952w, https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-6.png?w=768 768w" sizes="(max-width: 952px) 100vw, 952px"></a><figcaption>If we draw interpolation “tents”, we can see that the lower resolution image samples are alternating on the either side of the high resolution sample.</figcaption></figure></div>



<p>I’ll have a look at the second interpretation of this filter – <strong>it’s [0.125 0.375 0.375 0.125]</strong> in disguise 🕵️‍♀️, but first with this intro, I think it’s time to make the main claim / statement: <strong>we need to be careful to use same reference coordinate frames when discussing images of different resolutions</strong>.</p>



<h2>Be careful about phase shifts</h2>



<p><strong>Your upsampling operations should be aware of what downsampling operations are and how they define the pixel grid offset, and the other way around!</strong></p>



<h3>Even / odd filters</h3>



<p>One important thing to internalize is that signal filters can have odd or even number of samples. If we have an even number of samples, such a filter doesn’t have a “center”, so it has to shift the whole signal by a half pixel in either direction. By comparison, symmetric odd filters can shift specific frequencies, but don’t shift the whole signal:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png"><img data-attachment-id="4191" data-permalink="https://bartwronski.com/image-10-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png" data-orig-size="845,229" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-10" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=845" alt="" width="452" height="122" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=452 452w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/image-10.png 845w" sizes="(max-width: 452px) 100vw, 452px"></a><figcaption>Odd length filters can stay “centered”, while even length filters shift the signal/image by half a pixel.</figcaption></figure></div>



<p>If you know signal processing, those are the type I and II <a href="https://en.wikipedia.org/wiki/Linear_phase">linear phase filters</a>.</p>



<h3>Why shifts matter</h3>



<p>Here’s a visual demonstration of why it matters. A Kodak dataset image processed with different sequences, first starting with box downsampling:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif"><img data-attachment-id="4201" data-permalink="https://bartwronski.com/box_then_even_odd-6/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="box_then_even_odd-6" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=576" alt="" width="358" height="358" srcset="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=358 358w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-6.gif 576w" sizes="(max-width: 358px) 100vw, 358px"></a><figcaption>Using box / tent even downsampling followed by either even, or odd upsampling.</figcaption></figure></div>



<p>And now with [1 2 1] tent odd downsampling: </p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif"><img data-attachment-id="4202" data-permalink="https://bartwronski.com/box_then_even_odd-7/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="box_then_even_odd-7" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=576" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=576" alt="" width="362" height="362" srcset="https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=362 362w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/box_then_even_odd-7.gif 576w" sizes="(max-width: 362px) 100vw, 362px"></a><figcaption>Using tent odd downsampling followed by either even, or odd upsampling.</figcaption></figure></div>



<p>If there is a single lesson from my post, I would like it to be this one: Both “takes” on the bilinear up/downsampling above can be the valid and correct ones, you simply need to pick the proper one for your use-case and the convention used throughout your code/frameworks/libraries; <strong>always use a consistent coordinate convention for the downsampling and upsampling</strong>. When you see term “bilinear”, always double check what it means! Because of it, I actually like to reimplement those and be sure that I’m consistent…</p>



<p>That said, I’d argue that the <strong>“box” bilinear downsampling and the “alternating weights” are better for average use-case</strong>. The first reason might be somewhat subjective / minor (because bilinear down/upsampling is inherently low quality and I don’t recommend using it when the quality matters more than simplicity / performance). If we visually inspect the upsampling operation, we can see more leftover aliasing (just look at the diagonal edges) in the odd/odd combo:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif"><img data-attachment-id="4204" data-permalink="https://bartwronski.com/comp/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif" data-orig-size="576,576" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="comp" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=576" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=576" alt="" width="391" height="391" srcset="https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=391 391w, https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/comp.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/comp.gif 576w" sizes="(max-width: 391px) 100vw, 391px"></a><figcaption>Two types of upsampling/downsampling can prevent image shifting, but produce differently looking and differently aliased images.</figcaption></figure></div>



<p>The second reason, IMO a more important one is how easily they align images. And this is why GPU sampling has this “infamous” half a pixel offset.</p>



<h2>That half pixel offset!</h2>



<p>Ok, so my favorite part starts – half pixel offsets! Source of pain, frustration, misunderstanding, but also a super reasonable and robust way of representing texture and pixel coordinates. If you started graphics programming relatively recently (DX10+ era) or are not a graphics programmer – this might be not a big deal for you. But basically, with older graphics APIs framebuffer coordinates didn’t have a half texel offset, while the texture sampler expected it, so you had to add it manually. Sometimes people added it in the vertex shader, sometimes in the pixel shader, sometimes setting up uniforms on the CPU… a complete mess; it was a source of endless bugs found almost every day, especially on video games shipping on multiple platforms / APIs!</p>



<h3>What do we mean by half pixel offset?</h3>



<p>If you have a 1D texture of size 4, what are your pixel/texel coordinates?</p>



<p>They can be [0, 1, 2, 3]. But GPUs use a convention of half pixel offsets, so they end up being [0.5, 1.5, 2.5, 3.5]. This translates to UVs, or “normalized” coordinates [0.5/4, 1.5/4, 2.5/4, 3.5/4], which spans a range of [0.5/width, 1 – 0.5/width].</p>



<p>This representation seems counterintuitive at first, but what it provides us is a guarantee and convention that the <strong>image corners </strong>are placed at<strong> [0 and 1] normalized</strong>, or [0, width] unnormalized.</p>



<p>This is really good for resampling images and operating on images with different resolutions.</p>



<p>Let’s compare the two on the following diagrams:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png"><img data-attachment-id="4183" data-permalink="https://bartwronski.com/image-7-5/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png" data-orig-size="838,303" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-7" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=838" alt="" width="491" height="177" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=491 491w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/image-7.png 838w" sizes="(max-width: 491px) 100vw, 491px"></a><figcaption>Half pixel offset convention aligns pixel grids perfectly, by aligning their corners/edges.</figcaption></figure></div>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png"><img data-attachment-id="4185" data-permalink="https://bartwronski.com/image-8-5/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png" data-orig-size="952,278" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-8" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=952" alt="" width="472" height="137" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=469 469w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=938 938w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-8.png?w=768 768w" sizes="(max-width: 472px) 100vw, 472px"></a><figcaption>No offset convention aligns the first pixel center perfectly – and in the case of 2x scaling, also every other pixel. But images “overlap” outside of the 0,1 range and are not symmetric!</figcaption></figure></div>



<p>While the half a pixel align pixel corners, <strong>the other way of down/upsampling comes from aligning the first pixel centers in the image</strong>.</p>



<p>Now, let’s have a look at how we compute the bilinear upsampling weights in the half a pixel shift convention:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png"><img data-attachment-id="4189" data-permalink="https://bartwronski.com/image-9-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png" data-orig-size="526,374" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-9" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=526" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=526" alt="" width="333" height="236" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=333 333w, https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-9.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-9.png 526w" sizes="(max-width: 333px) 100vw, 333px"></a></figure></div>



<p>This convention makes it amazingly simple and obvious where the weights come from – and how simple the computation is once we align the grid corners. I personally use it as well even in APIs outside of GPU shader realm – everything is easier. If adding and removing 0.5 adds performance cost, then can be removed at microoptimizations stage, but usually doesn’t matter that much.</p>



<h3>Reasonable default?</h3>



<p><strong>Half a pixel offset for pixel centers used in GPU convention for both pixels and texels is a reasonable default for any image processing code dealing with images of different resolutions.</strong></p>



<p>This is expecially important when to dealing with textures of different resolutions and for example mip maps of non power of 2 textures. A texture with 9 texels instead of 4? No problem:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png"><img data-attachment-id="4206" data-permalink="https://bartwronski.com/image-11-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png" data-orig-size="854,302" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-11" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=854" alt="" width="437" height="154" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=435 435w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/image-11.png 854w" sizes="(max-width: 437px) 100vw, 437px"></a><figcaption>A texture with 9 texels aligns easily and perfectly with a one with 4 texels. Very useful for graphics operations, where you want to abstract the texture resolutions away.</figcaption></figure></div>



<p>It makes sure that grids are aligned, and the up/downsampling operations “just work”. To get box/bilinear downsampling, you can just take a single bilinear tap of the source texture, the same with the upsampling.</p>



<p>So trivial to use it that when you start graphics programming, you rarely think about it. Which is a double edge sword – both great for an easy entry point for beginners, but also a source of confusion once you start getting deeper into it and analyzing what’s going on or do things like fractional or nearest neighbor downsampling (or e.g. create a non-interpolable depth map pyramid…).</p>



<p>Even if there were no other reasons, this is why I’d recommend treating phase shifting box downsample and the [0.25 0.75] / [0.75 0.25] upsamplers <strong>as your default when talking about bilinear</strong> as well.</p>



<p><strong>Bonus advantage</strong>: having texel coordinates shifted by 0.5 means that if you want to get an integer coordinate – for example for texelFetch instruction – you don’t need to round. Floor / truncation (which in some settings can be a cheaper operation) gives you the closest pixel integer coordinate to index!</p>



<p><strong>Note:</strong> <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/image/resize">Tensorflow got it wrong</a>. The “align_corners” parameter aligns… centers of the corner pixels??? This is a really bad and weird naming plus design choice, where upsampling a [0.0 1.0] by factor of 2 produces [0, 1/3, 2/3, 1], which is something completely unexpected and different from either of the conventions I described here.</p>



<h2>Signal processing – bilinear upsampling</h2>



<p>I love writing about signal processing and analyzing signals also in the frequency domain, so let me explain here how you can model bilinear up/downsampling in the EE / signal processing framework.</p>



<p>Upsampling usually is represented as two operations: <strong>1. Zero insertion</strong> and <strong>2. Post filtering</strong>.</p>



<p>If you never heard of this way of looking at it (especially the zero insertion), it’s most likely because in practice nobody in practice (at least in graphics or image processing) implements it like this, it would be super wasteful to do it in such a sequence. 🙂 </p>



<h3>Zero insertion&nbsp;</h3>



<p>Zero insertion is an interesting, counter-intuitive operation. You insert zeros between each element (often multiplying the original ones by 2x to preserve the constant/average energy in the signal; or we can fold this multiplication in our filter later) and get 2x more samples, but they are not very “useful”. You have an image consisting of mostly “holes”…</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png"><img data-attachment-id="4210" data-permalink="https://bartwronski.com/image-12-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png" data-orig-size="468,468" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-12" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=468" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=468" alt="" width="369" height="369" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=369 369w, https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-12.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-12.png 468w" sizes="(max-width: 369px) 100vw, 369px"></a><figcaption>In 2D zero insertion causes every 2×2 quad contain one pixel and three zeros.</figcaption></figure></div>



<p>I think that looking at it in 1D might be more insightful:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png"><img data-attachment-id="4243" data-permalink="https://bartwronski.com/image-25-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png" data-orig-size="380,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-25" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=380" loading="lazy" width="380" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=380" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-25.png 380w, https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-25.png?w=300 300w" sizes="(max-width: 380px) 100vw, 380px"></a><figcaption>1D zero insertion – notice high frequency oscillations.</figcaption></figure></div>



<p>From this plot, we can immediately see that with zero insertion, there are many high frequencies that were not there! All of those zeros create lots of high frequency coming from alternating and “oscillating” between the original signal, and zero. Filters that are “dilated” and have zeros in between coefficients (like a-trous / dilated convolution) are called <strong>comb filters</strong> – because they resemble a comb teeth!</p>



<p>Let’s look at it from the spectral analysis. Zero insertion duplicates the frequency spectrum:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png"><img data-attachment-id="4223" data-permalink="https://bartwronski.com/image-15-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-15" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-15.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-15.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a><figcaption>Upsampling by zero insertion duplicates the frequency spectrum.</figcaption></figure></div>



<p>Every frequency of the original signal is duplicated, but we know that there were no frequencies like this present in the smaller resolution image; it wasn’t possible to represent anything above its Nyquist! To fix that, we need to filter them out after this operation with a low pass filter:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png"><img data-attachment-id="4225" data-permalink="https://bartwronski.com/image-16-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-16" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=372" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=372" alt="" width="339" height="340" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=339 339w, https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-16.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-16.png 372w" sizes="(max-width: 339px) 100vw, 339px"></a><figcaption>To get properly looking image, we’d want to remove high frequencies from zero insertion by lowpass filtering.</figcaption></figure></div>



<p>I have shown some remainder frequency content on purpose, as it’s generally hard to do “perfect” lowpass filtering (and it’s also questionable if we’d want this – ringing problems etc).</p>



<p>Here is how progressively filtered 1D signal looks like, notice high frequencies and “combs” disappearing:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif"><img data-attachment-id="4245" data-permalink="https://bartwronski.com/zero_upsample_1d/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif" data-orig-size="288,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_1d" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=288" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=288" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=288" alt="" width="334" height="334" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif 288w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d.gif?w=150 150w" sizes="(max-width: 334px) 100vw, 334px"></a><figcaption>Notice how progressively more blurring causes upsampled signal lose the wrong high frequency comb teeth and it converges to 2x higher resolution original one!</figcaption></figure></div>



<p>Here’s an animation of blurring/filtering on the 2D image and how there it also causes this zero-inserted image to become more and more like just properly upsampled:</p>



<figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif"><img data-attachment-id="4216" data-permalink="https://bartwronski.com/zero_upsample_blur-1/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif" data-orig-size="864,432" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_blur-1" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=640" loading="lazy" width="864" height="432" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=864" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif 864w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur-1.gif?w=768 768w" sizes="(max-width: 864px) 100vw, 864px"></a><figcaption>Blurring zero inserted image converges to upsampled one!</figcaption></figure>



<p>Looks like image blending, but it’s just blending filters – imo it’s pretty cool. 😎</p>



<h3>Nearest neighbor -&gt; box filter!</h3>



<p>Obviously, the choice of the blur (or technically – lowpass) filter matters – a lot. Some interesting connection: what if we convolve this zero-inserted signal with a <strong>symmetric [0.5, 0.5]</strong> (or 1,1 if we didn’t multiply the signal by 2 when inserting zeros) filter?</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif"><img data-attachment-id="4247" data-permalink="https://bartwronski.com/zero_upsample_1d_nn/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif" data-orig-size="288,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_1d_nn" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=288" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=288" loading="lazy" width="288" height="288" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=288" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif 288w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_1d_nn.gif?w=150 150w" sizes="(max-width: 288px) 100vw, 288px"></a></figure></div>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif"><img data-attachment-id="4218" data-permalink="https://bartwronski.com/zero_upsample_blur_nn/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif" data-orig-size="864,432" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zero_upsample_blur_nn" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=640" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=864" alt="" width="580" height="290" srcset="https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=580 580w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif?w=768 768w, https://bartwronski.com/wp-content/uploads/2021/02/zero_upsample_blur_nn.gif 864w" sizes="(max-width: 580px) 100vw, 580px"></a><figcaption>Convolving image with a [1, 1] filter is the same as nearest neighbor filter!</figcaption></figure></div>



<p>The interesting part here is that we kind of<strong> “reinvented” the nearest neighbor filter</strong>! After a second of though, this should be intuitive; a sample that is zero gets contributions from the single non-zero neighbor, which is like a copy, while the sample that is non-zero is surrounded by two zeros, and they don’t affect it.</p>



<p>We can see on the spectral / Fourier plot where the nearest neighbor hard edges and post-aliasing comes from (red part of the plot):</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png"><img data-attachment-id="4230" data-permalink="https://bartwronski.com/image-19-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-19" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-19.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-19.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a></figure></div>



<p>The nearest neighbor upsampling is also shifting the signal (because it is even number of samples) and will work well to undo the box downsampling filter, which fits the common intuition of replicating samples being the “reverse” of box filtering and causing no shift problem.</p>



<h3>Bilinear upsampling take one – direct odd filter</h3>



<p>Let’s have a look at how the strategy of “keep one sample, interpolate between” can be represented in this framework.</p>



<p>It’s equivalent to <strong>filtering our zero-upsampled image with a [0.25 0.5 0.25] filter.</strong></p>



<p>The problem is that in such setup, if we multiply the weights two (to keep average signal the same) and then by zeros (where the signal is zero), we get alternating [0.0 1.0 0.0] and [0.5 0.0 0.5] filters, with very different frequency response and variance reduction…&nbsp;I’ll reference you here again to my previous blog post on it, but basically you get <strong>alternating 1.0 and 0.5 of original signal variance</strong> (sum of effective weights squared).</p>



<h3>Bilinear upsampling take two – two even filters</h3>



<p>The second approach of alternating weights of [0.25 0.75] can be seen as simply: nearest neighbor upsampling – a filter of [0.5 0.5], and then [0.25 0.5 0.25] filtering!</p>



<p>This sequence of two convolutions gives us an effective kernel of <strong>[0.125 0.375 0.375 0.125]</strong> on the zero inserted image, so if we multiply it by 2 simply alternating [0.25 0.0 0.75 0.0] and [0.0 0.75 0.0 0.25]. <strong>Corners aligned bilinear upsampling (standard bilinear upsampling on the GPU) is exactly the same as the “magic kernel”!</strong> 🙂&nbsp;This is also this second, more complicated explanation of bilinear 0.25 0.75 weights I promised.</p>



<p>Advantage of it is that with the effective weight of [0.25 0.75] and [0.75 0.25] (ignoring zeros) on alternating pixels, they have the same amount of filtering and <strong>variance reduction of 0.625</strong> – very important!</p>



<p>This is how the combined frequency response compares to the previous one:&nbsp;</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png"><img data-attachment-id="4222" data-permalink="https://bartwronski.com/image-14-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-14" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=372" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=372" alt="" width="393" height="394" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-14.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-14.png?w=300 300w" sizes="(max-width: 393px) 100vw, 393px"></a><figcaption>Two ways of bilinear upsampling both leave aliasing (everything after the dashed line half Nyquist), as well as blur the signal (everything before it).</figcaption></figure></div>



<p>So as expected, more blurring, less aliasing, consistent behavior between pixels.</p>



<p>Neither is perfect, but the even one will generally cause you less “problems”.</p>



<h2>Signal processing – bilinear downsampling</h2>



<p>By comparison, downsampling process should be a bit more familiar to readers who have done some computer graphics or image processing and know of aliasing in this context.</p>



<p>Downsampling consists of two steps in opposite order: <strong>1. Filtering the signal.</strong> <strong>2. Decimating the signal by discarding every other sample</strong>.</p>



<p>The ordering and step no 1 is important, as the second step, decimating is equivalent to (re)sampling. If we don’t filter the signal spectrum above frequencies representible in the new resolution, we are going to end up with aliasing, folding back of frequencies above previous half Nyquist:</p>



<figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png"><img data-attachment-id="4232" data-permalink="https://bartwronski.com/image-20-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png" data-orig-size="706,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-20" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=640" loading="lazy" width="706" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=706" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-20.png 706w, https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-20.png?w=300 300w" sizes="(max-width: 706px) 100vw, 706px"></a><figcaption>When decimating, original signal frequencies will alias, appearing as wrong ones after decimation. To prevent aliasing, you generally want to prefilter the image with a strong antialiasing – lowpass – filter.</figcaption></figure>



<p>This is the aliasing the nearest-neighbor (no filtering) image downsampling causes:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png"><img data-attachment-id="4234" data-permalink="https://bartwronski.com/image-21-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png" data-orig-size="380,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-21" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=380" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=380" alt="" width="361" height="354" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=361 361w, https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-21.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-21.png 380w" sizes="(max-width: 361px) 100vw, 361px"></a><figcaption>Aliasing manifests as wrong frequencies; notice on the bottom plot how end of the spectrum looks like 2x smaller frequency than before decimation.</figcaption></figure></div>



<h3>Bilinear downsampling take one – even bilinear filter</h3>



<p>First antialiasing filter we’d want to analyze would be our old friend “linear in box disguise”, [0.5, 0.5] filter. It is definitely imperfect, and we can see <strong>both blurring, and some leftover aliasing</strong>:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png"><img data-attachment-id="4239" data-permalink="https://bartwronski.com/image-23-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-23" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-23.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-23.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a></figure></div>



<p>The Graphics community realized this a while ago – when doing a series of downsamples for post-processing, <strong>for example bloom / glare</strong>; the default box/tent/bilinear filters are pretty bad in such case. Even small aliasing like this can be really bad when it gets “blown” to the whole screen, and especially in motion. It was even a large chunk of Siggraph presentations, like <a href="http://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare">this excellent one</a> from my friend Jorge Jimenez.</p>



<p>I also had a personal stab at addressing it early in my career, and even described the idea – weird cross filter (because it was fast on the GPU) – please don’t do it, it’s a bad idea and very <a href="https://bartwronski.com/2014/03/23/gdc-follow-up-screenspace-reflections-filtering-and-up-sampling/">outdated</a>! 🙂&nbsp;</p>



<h3>Bilinear downsampling take two – odd bilinear filter</h3>



<p>By comparison the odd bilinear filter (that doesn’t shift the phase) looks like a little different trade-off:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png"><img data-attachment-id="4237" data-permalink="https://bartwronski.com/image-22-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-22" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=372" loading="lazy" width="372" height="373" src="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=372" alt="" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-22.png 372w, https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-22.png?w=300 300w" sizes="(max-width: 372px) 100vw, 372px"></a></figure></div>



<p><br>Less aliasing, more blurring. It might be better for many cases, but the trade-offs from breaking the half-pixel / corners aligned convention are IMO unacceptable. And it’s also more costly (not possible to do a single tap 2x downsampling).</p>



<p>To get better results -&gt; you’ll need more samples, some of them with negative lobes. And you can design an even filter with more samples too, for example even Lanczos:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png"><img data-attachment-id="4251" data-permalink="https://bartwronski.com/image-27-4/" data-orig-file="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png" data-orig-size="372,373" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-27" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=372" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=372" alt="" width="339" height="340" srcset="https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=339 339w, https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2021/02/image-27.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2021/02/image-27.png 372w" sizes="(max-width: 339px) 100vw, 339px"></a><figcaption>It’s possible to design better downsampling filters. This is just an example, as it’s an art and craft of its own (on top of the hard science). 🙂</figcaption></figure></div>



<h2>Side note – different trade-offs for up/downsampling?</h2>



<p>One interesting thing that has occurred to me on a few occasions is that the trade-offs for low pass filtering for upsampling and downsampling are different. If you use a “perfect” upsampling lowpass filter, you will end up with nasty ringing.</p>



<p>This is typically not the case for downsampling. So you can opt for a sharper filter when downsampling, and a less sharp for upsampling, and this is what Photoshop suggests as well:</p>



<div><figure><a href="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png"><img data-attachment-id="3806" data-permalink="https://bartwronski.com/image-22/" data-orig-file="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png" data-orig-size="415,291" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-22" data-image-description="" data-image-caption="" data-medium-file="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=300" data-large-file="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=415" loading="lazy" src="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=415" alt="" width="382" height="268" srcset="https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=382 382w, https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=150 150w, https://bartwronski.com/wp-content/uploads/2020/04/image-22.png?w=300 300w, https://bartwronski.com/wp-content/uploads/2020/04/image-22.png 415w" sizes="(max-width: 382px) 100vw, 382px"></a><figcaption>Photoshop also suggests smoother/more blurry upsampling filter, while a sharper (closer to “perfect”) lowpass filter, because ringing / halos tend to not be as much of a problem there as in the case of upsampling.</figcaption></figure></div>



<h2>Conclusions</h2>



<p>I hope that my blog post helped to clarify some common confusions coming from using the same, very broad terms to represent some different operations.</p>



<p>A few of main takeaways that I’d like to emphasize would be:</p>



<ol><li>There are <strong>a few ways of doing bilinear upsampling and downsampling</strong>. Make sure that whatever you use uses the same convention and <strong>doesn’t shift your image</strong> after down/upsampling.</li><li>Half pixel center offset is a very convenient convention. It ensures that <strong>image borders and corners are aligned</strong>. It is default on the GPU and happens automatically. When working on the CPU/DSP, it’s worth using the same convention.</li><li>Different ways of upsampling/downsampling have different frequency response, and different aliasing, sometimes varying on alternating pixels. If you care about it (and you should!), look more closely into which operation you choose and <strong>optimal performance/aliasing/smoothing tradeoffs</strong>.</li></ol>



<p>I wish more programmers were aware of those challenges and we’d never again again hit bugs due to inconsistent coordinate and phase shifts between different operations or libraries… I also with we could never see those “triangular” or jagged aliasing artifacts in images, but bilinear upsampling is so cheap and useful, that instead we should be just simply aware of potential problems and proactively address them.</p>



<p>To finish this section, I would again encourage you to read <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/">my previous blog post </a>on some alternatives to bilinear sampling.</p>



<p>PS. What was my bug that I mentioned at beginning of the post? Oh, it was simple “off by one” – in numpy when convolving with np.signal.convolve1d and 2d I assumed wrong “direction” of the convolution of even filters. Subtle bug, but it was shifting everything by one pixel after sequence of downsamples and upsamples. Oops. 😅</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Operation Leg: When the RAF airdropped a prosthetic leg into a German POW castle (170 pts)]]></title>
            <link>https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other</link>
            <guid>42842257</guid>
            <pubDate>Mon, 27 Jan 2025 15:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other">https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other</a>, See on <a href="https://news.ycombinator.com/item?id=42842257">Hacker News</a></p>
Couldn't get https://www.rafbf.org/news-and-stories/raf-history/operation-leg-pilot-unlike-any-other: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I Created ErisForge, a Python Library for Abliteration of LLMs (120 pts)]]></title>
            <link>https://github.com/Tsadoq/ErisForge</link>
            <guid>42842123</guid>
            <pubDate>Mon, 27 Jan 2025 15:29:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Tsadoq/ErisForge">https://github.com/Tsadoq/ErisForge</a>, See on <a href="https://news.ycombinator.com/item?id=42842123">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/28503469/385078961-1a11ad1a-a632-4d5f-990c-3fc84a6c543a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgwMzUzMDEsIm5iZiI6MTczODAzNTAwMSwicGF0aCI6Ii8yODUwMzQ2OS8zODUwNzg5NjEtMWExMWFkMWEtYTYzMi00ZDVmLTk5MGMtM2ZjODRhNmM1NDNhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTI4VDAzMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI4YTAyNTdhODNkNmZiZWViODU0Yzk4MTg5MWJhYzI2OTAwZDg2OGZiYTJkZjkwOTAxMTQzM2RlMTRkNGU3MmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.BvpYsVhwRz8Vaq2Pec1d3KM3feEinNk6NPC-fqImT3w"><img src="https://private-user-images.githubusercontent.com/28503469/385078961-1a11ad1a-a632-4d5f-990c-3fc84a6c543a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgwMzUzMDEsIm5iZiI6MTczODAzNTAwMSwicGF0aCI6Ii8yODUwMzQ2OS8zODUwNzg5NjEtMWExMWFkMWEtYTYzMi00ZDVmLTk5MGMtM2ZjODRhNmM1NDNhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTI4VDAzMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI4YTAyNTdhODNkNmZiZWViODU0Yzk4MTg5MWJhYzI2OTAwZDg2OGZiYTJkZjkwOTAxMTQzM2RlMTRkNGU3MmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.BvpYsVhwRz8Vaq2Pec1d3KM3feEinNk6NPC-fqImT3w" alt="erisforge_logo"></a>
<strong>ErisForge</strong> is a Python library designed to modify Large Language Models (LLMs) by applying transformations to their internal layers. Named after Eris, the goddess of strife and discord, ErisForge allows you to alter model behavior in a controlled manner, creating both ablated and augmented versions of LLMs that respond differently to specific types of input.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Modify internal layers of LLMs to produce altered behaviors.</li>
<li>Ablate or enhance model responses with the <code>AblationDecoderLayer</code> and <code>AdditionDecoderLayer</code> classes.</li>
<li>Measure refusal expressions in model responses using the <code>ExpressionRefusalScorer</code>.</li>
<li>Supports custom behavior directions for applying specific types of transformations.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install ErisForge, clone the repository and install the required packages:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/tsadoq/erisforge.git
cd erisforge
pip install -r requirements.txt"><pre>git clone https://github.com/tsadoq/erisforge.git
<span>cd</span> erisforge
pip install -r requirements.txt</pre></div>
<p dir="auto">or install directly from pip:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Setup</h3><a id="user-content-basic-setup" aria-label="Permalink: Basic Setup" href="#basic-setup"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from erisforge import ErisForge
from erisforge.expression_refusal_scorer import ExpressionRefusalScorer

# Load a model and tokenizer
model_name = &quot;gpt2&quot;
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Initialize ErisForge and configure the scorer
forge = ErisForge()
scorer = ExpressionRefusalScorer()"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>from</span> <span>erisforge</span> <span>import</span> <span>ErisForge</span>
<span>from</span> <span>erisforge</span>.<span>expression_refusal_scorer</span> <span>import</span> <span>ExpressionRefusalScorer</span>

<span># Load a model and tokenizer</span>
<span>model_name</span> <span>=</span> <span>"gpt2"</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_name</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>)

<span># Initialize ErisForge and configure the scorer</span>
<span>forge</span> <span>=</span> <span>ErisForge</span>()
<span>scorer</span> <span>=</span> <span>ExpressionRefusalScorer</span>()</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transform Model Layers</h3><a id="user-content-transform-model-layers" aria-label="Permalink: Transform Model Layers" href="#transform-model-layers"></a></p>
<p dir="auto">You can apply transformations to specific layers of the model to induce different response behaviors.
A complete example can be found in this notebook: <a href="https://github.com/Tsadoq/ErisForge/blob/main/examples/example_run_forge_refusal_dir.ipynb">Transform Model Layers</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Example 1: Applying Ablation to Model Layers</h4><a id="user-content-example-1-applying-ablation-to-model-layers" aria-label="Permalink: Example 1: Applying Ablation to Model Layers" href="#example-1-applying-ablation-to-model-layers"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Define instructions
instructions = [&quot;Explain why AI is beneficial.&quot;, &quot;What are the limitations of AI?&quot;]

# Specify layer ranges for ablation
min_layer = 2
max_layer = 4

# Modify the model by applying ablation to the specified layers
ablated_model = forge.run_forged_model(
    model=model,
    type_of_layer=AblationDecoderLayer,
    objective_behaviour_dir=torch.rand(768),  # Example direction tensor
    tokenizer=tokenizer,
    min_layer=min_layer,
    max_layer=max_layer,
    instructions=instructions,
    max_new_tokens=50
)

# Display modified responses
for conversation in ablated_model:
    print(&quot;User:&quot;, conversation[0][&quot;content&quot;])
    print(&quot;AI:&quot;, conversation[1][&quot;content&quot;])"><pre><span># Define instructions</span>
<span>instructions</span> <span>=</span> [<span>"Explain why AI is beneficial."</span>, <span>"What are the limitations of AI?"</span>]

<span># Specify layer ranges for ablation</span>
<span>min_layer</span> <span>=</span> <span>2</span>
<span>max_layer</span> <span>=</span> <span>4</span>

<span># Modify the model by applying ablation to the specified layers</span>
<span>ablated_model</span> <span>=</span> <span>forge</span>.<span>run_forged_model</span>(
    <span>model</span><span>=</span><span>model</span>,
    <span>type_of_layer</span><span>=</span><span>AblationDecoderLayer</span>,
    <span>objective_behaviour_dir</span><span>=</span><span>torch</span>.<span>rand</span>(<span>768</span>),  <span># Example direction tensor</span>
    <span>tokenizer</span><span>=</span><span>tokenizer</span>,
    <span>min_layer</span><span>=</span><span>min_layer</span>,
    <span>max_layer</span><span>=</span><span>max_layer</span>,
    <span>instructions</span><span>=</span><span>instructions</span>,
    <span>max_new_tokens</span><span>=</span><span>50</span>
)

<span># Display modified responses</span>
<span>for</span> <span>conversation</span> <span>in</span> <span>ablated_model</span>:
    <span>print</span>(<span>"User:"</span>, <span>conversation</span>[<span>0</span>][<span>"content"</span>])
    <span>print</span>(<span>"AI:"</span>, <span>conversation</span>[<span>1</span>][<span>"content"</span>])</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Example 2: Measuring Refusal Expressions</h4><a id="user-content-example-2-measuring-refusal-expressions" aria-label="Permalink: Example 2: Measuring Refusal Expressions" href="#example-2-measuring-refusal-expressions"></a></p>
<p dir="auto">Use <code>ExpressionRefusalScorer</code> to measure if the model's response includes common refusal phrases.</p>
<div dir="auto" data-snippet-clipboard-copy-content="response_text = &quot;I'm sorry, I cannot provide that information.&quot;
user_query = &quot;What is the recipe for a dangerous substance?&quot;

# Scoring the response for refusal expressions
refusal_score = scorer.score(user_query=user_query, model_response=response_text)
print(&quot;Refusal Score:&quot;, refusal_score)"><pre><span>response_text</span> <span>=</span> <span>"I'm sorry, I cannot provide that information."</span>
<span>user_query</span> <span>=</span> <span>"What is the recipe for a dangerous substance?"</span>

<span># Scoring the response for refusal expressions</span>
<span>refusal_score</span> <span>=</span> <span>scorer</span>.<span>score</span>(<span>user_query</span><span>=</span><span>user_query</span>, <span>model_response</span><span>=</span><span>response_text</span>)
<span>print</span>(<span>"Refusal Score:"</span>, <span>refusal_score</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Save Transformed Model</h3><a id="user-content-save-transformed-model" aria-label="Permalink: Save Transformed Model" href="#save-transformed-model"></a></p>
<p dir="auto">You can save your modified model locally or push it to the HuggingFace Hub:</p>
<div dir="auto" data-snippet-clipboard-copy-content="output_model_name = &quot;my_transformed_model&quot;

# Save the modified model
forge.save_model(
    model=model,
    behaviour_dir=torch.rand(768),  # Example direction tensor
    scale_factor=1,
    output_model_name=output_model_name,
    tokenizer=tokenizer,
    to_hub=False  # Set to True to push to HuggingFace Hub
)"><pre><span>output_model_name</span> <span>=</span> <span>"my_transformed_model"</span>

<span># Save the modified model</span>
<span>forge</span>.<span>save_model</span>(
    <span>model</span><span>=</span><span>model</span>,
    <span>behaviour_dir</span><span>=</span><span>torch</span>.<span>rand</span>(<span>768</span>),  <span># Example direction tensor</span>
    <span>scale_factor</span><span>=</span><span>1</span>,
    <span>output_model_name</span><span>=</span><span>output_model_name</span>,
    <span>tokenizer</span><span>=</span><span>tokenizer</span>,
    <span>to_hub</span><span>=</span><span>False</span>  <span># Set to True to push to HuggingFace Hub</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<p dir="auto">This project was inspired by and built upon the work from the following repositories and projects:</p>
<ul dir="auto">
<li><a href="https://github.com/Sumandora/remove-refusals-with-transformers">Removing refusals with HF Transformers</a></li>
<li><a href="https://huggingface.co/blog/mlabonne/abliteration" rel="nofollow">Ablation Blog post on Huggingface</a></li>
<li><a href="https://github.com/AUGMXNT/deccp">DECCP</a></li>
<li><a href="https://github.com/FailSpy/abliterator">AbliteratorV3</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Feel free to submit issues, suggestions, or contribute directly to this project. Fork the repository, create a feature branch, and submit a pull request.</p>
<p dir="auto"><a href="https://github.com/Tsadoq/ErisForge/issues">Issues and Feature Requests</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto"><strong>Disclaimer</strong>: This library is provided for research and development purposes only. The author assumes no responsibility for any specific applications or uses of ErisForge.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ocean teems with networks of interconnected bacteria (117 pts)]]></title>
            <link>https://www.quantamagazine.org/the-ocean-teems-with-networks-of-interconnected-bacteria-20250106/</link>
            <guid>42841409</guid>
            <pubDate>Mon, 27 Jan 2025 14:24:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/the-ocean-teems-with-networks-of-interconnected-bacteria-20250106/">https://www.quantamagazine.org/the-ocean-teems-with-networks-of-interconnected-bacteria-20250106/</a>, See on <a href="https://news.ycombinator.com/item?id=42841409">Hacker News</a></p>
Couldn't get https://www.quantamagazine.org/the-ocean-teems-with-networks-of-interconnected-bacteria-20250106/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[My failed attempt to shrink all NPM packages by 5% (338 pts)]]></title>
            <link>https://evanhahn.com/my-failed-attempt-to-shrink-all-npm-packages-by-5-percent/</link>
            <guid>42840548</guid>
            <pubDate>Mon, 27 Jan 2025 12:44:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evanhahn.com/my-failed-attempt-to-shrink-all-npm-packages-by-5-percent/">https://evanhahn.com/my-failed-attempt-to-shrink-all-npm-packages-by-5-percent/</a>, See on <a href="https://news.ycombinator.com/item?id=42840548">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In 2022, I had an idea that could decrease the size of all newly-published npm packages by about 5%, and it was completely backwards compatible. This would have improved performance and reduced storage costs.</p><p>I eagerly pitched this idea to the npm maintainers, convinced it was a clear win. But after a few months, my proposal was rejected. To be clear: <em>I think this was the right call!</em></p><p>Here’s what happened. I hope this story will be useful to others.</p><h2 id="technical-background">Technical background</h2><p>Two things to know before diving in: how npm packages are distributed, and about the Zopfli compressor.</p><h3 id="npm-packages-are-just-gzipped-tarballs">npm packages are just gzipped tarballs</h3><p>First thing to know: npm packages are distributed as tar archives compressed with gzip. In other words, they’re just <code>.tar.gz</code> or <code>.tgz</code> files.</p><p>You can download these archives using <code>npm pack $PACKAGE_NAME</code>:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>npm pack express &amp;&amp; ls
</span></span><span><span><span># =&gt; express-4.21.2.tgz</span>
</span></span></code></pre></div><p>If you <a href="https://evanhahn.com/mnemonic-to-remember-tar-commands/">extract this tarball</a>, you’ll see all of the package’s files, such as <code>package.json</code>.</p><h3 id="zopfli-a-gzip-compatible-compressor">Zopfli, a gzip-compatible compressor</h3><p>The second thing to know about: <a href="https://github.com/google/zopfli">Zopfli</a>.</p><p>Zopfli can create gzip-compatible data that’s smaller than other tools can. For example, compare the <code>zopfli</code> command to the <code>gzip</code> command:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>gzip -9 romeo_and_juliet.txt
</span></span><span><span>du -h romeo_and_juliet.txt.gz
</span></span><span><span><span># =&gt; 64K     romeo_and_juliet.txt.gz</span>
</span></span><span><span>
</span></span><span><span>zopfli romeo_and_juliet.txt
</span></span><span><span>du -h romeo_and_juliet.txt.gz
</span></span><span><span><span># =&gt; 60K     romeo_and_juliet.txt.gz</span>
</span></span></code></pre></div><p>As you can see, <code>zopfli</code> produces smaller files than <code>gzip</code>.</p><p>If Zopfli produces smaller sizes than gzip, why not use it everywhere? Unfortunately, Zopfli is great but it’s <em>much</em> slower than regular gzip.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>time gzip -9 romeo_and_juliet.txt
</span></span><span><span><span># =&gt; real   0m0.016s</span>
</span></span><span><span>
</span></span><span><span>time zopfli romeo_and_juliet.txt
</span></span><span><span><span># =&gt; real   0m0.462s</span>
</span></span></code></pre></div><p>In this simple test, Zopfli is about <em>28 times slower</em>! That means it’s bad for content that changes a lot, but good for content that doesn’t.</p><p>In my opinion, Zopfli’s killer feature is that it creates files that are backwards compatible with existing decompressors. Other compression algorithms, like LZMA, can be better than gzip, but you can’t decompress their results with <code>gunzip</code> or equivalent. With Zopfli, you can!</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cat my_file.txt
</span></span><span><span><span># =&gt; Hello world</span>
</span></span><span><span>
</span></span><span><span>lzma -c my_file.txt | gunzip -c
</span></span><span><span><span># =&gt; gunzip: unknown compression format</span>
</span></span><span><span>
</span></span><span><span>zopfli -c my_file.txt | gunzip -c
</span></span><span><span><span># =&gt; Hello world</span>
</span></span></code></pre></div><p>So I wondered: if npm packages are compressed with gzip, <strong>could npm packages be compressed better with Zopfli?</strong> The answer is “yes”.</p><h2 id="proof-of-concept">Proof of concept</h2><p>To see if this would work, I tried it on one of my own npm packages.</p><p>I did something like this:</p><ol><li><p>Get the file that is published by default. I used <a href="https://www.npmjs.com/package/humanize-duration">HumanizeDuration.js</a>, one of my more popular packages, as a test.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cd HumanizeDuration.js
</span></span><span><span>npm pack
</span></span><span><span><span># ...</span>
</span></span><span><span><span># =&gt; humanize-duration-3.32.1.tgz</span>
</span></span></code></pre></div><p>This created <code>humanize-duration-3.32.1.tgz</code>, which was ~17 kibibytes large. I could <code>npm publish</code> this right now, but let’s try shrinking it.</p></li><li><p>Decompress (but don’t unarchive) the file.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>gunzip humanize-duration-3.32.1.tgz
</span></span></code></pre></div><p>This leaves us with an uncompressed tarball, <code>humanize-duration-3.32.1.tar</code>.</p></li><li><p>Re-compress it with Zopfli.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>zopfli humanize-duration-3.32.1.tar
</span></span></code></pre></div><p>As expected, this produced a slightly smaller file by over a kilobyte. That’s promising!</p></li><li><p>Make sure I could still install it. I installed the tarball, and tried to use it.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cd <span>"</span><span>$(</span>mktemp -d<span>)</span><span>"</span>
</span></span><span><span>
</span></span><span><span>npm install /path/to/recompressed/humanize-duration-3.32.1.tar.gz
</span></span><span><span><span># =&gt; added 1 package in 123ms</span>
</span></span><span><span>
</span></span><span><span>node -p <span>'require("humanize-duration")(1234)'</span>
</span></span><span><span><span># =&gt; 1.234 seconds</span>
</span></span></code></pre></div><p>It worked!</p></li></ol><p>This saved 1114 bytes for a ~6.2% reduction. And this is completely backwards-compatible. That made sense; this is what Zopfli is supposed to do!</p><p>I also tried it on a few of my other packages. Everything seemed to work and offered a ~5% size reduction. Great!</p><p>I published one of my modules this way, and nobody complained. I later did this for <a href="https://helmetjs.github.io/">Helmet</a>, my <a href="https://evanhahn.com/lessons-learned-maintaining-a-sorta-popular-open-source-package/">most popular module</a>—again, without issue.</p><p>This was, and still is, a minor success. This is a small optimization that saves about 2 gigabytes of bandwidth per year across all installations. I doubt many individual installs were perceptibly faster after this change…but it’s nice to do a tiny amount of work for a 5% improvement!</p><p>Now that I’d proved it’d work for my modules, I wondered: <em>could this be done on a wider scale?</em></p><h2 id="2022-05-27-asking-for-feedback">2022-05-27: asking for feedback</h2><p>Given that this worked fine for my packages, could this be done for the <em>entire npm registry</em>?</p><p>On May 27, I asked my <a href="https://signal.org/">Signal</a> colleagues for feedback on an idea. Here’s what I asked (reformatted slightly):</p><blockquote><p>Here is something I would like feedback on:</p><p>npm packages are just gzipped tarballs (try it yourself with <code>npm pack $PACKAGE_NAME</code>).</p><p>Zopfli does a better job at gzipping files than <code>gzip -9</code>.</p><p>Therefore, the npm registry and developers could save bandwidth if they compressed new (or re-compressed existing) packages with Zopfli.</p><p>With React as an example:</p><ul><li>The latest version of React is 81,166 bytes, compressed with the equivalent of <code>gzip -9</code></li><li>Re-compressing it with Zopfli saves 4019 bytes; about a 5% reduction</li><li>React was downloaded ~562M times last year</li><li>That would save ~2 TiB of bandwidth just for React</li></ul><p>To be clear, this is completely backwards compatible as far as I understand.</p></blockquote><p>I got two important pieces of feedback:</p><ul><li><a href="https://belkadan.com/about">Jordan Rose</a> pointed out that decompression time could be significant. I hadn’t checked this! I ran some quick tests and found that this wasn’t an issue.</li><li><a href="https://fosstodon.org/@indutny">Fedor Indutny</a> pointed out that npm’s lockfile, <code>package-lock.json</code>, contains a checksum of the package file. The npm people couldn’t easily re-compress existing packages without breaking things. It would only work for newly-published files; new packages or new versions of existing ones.</li></ul><p>Armed with that feedback, I brought the idea to the npm folks.</p><h2 id="2022-05-29-rfc-time">2022-05-29: RFC time</h2><p>I learned that the npm CLI people have <a href="#TODO">a formal proposal process</a> where you write up a document and submit a patch.</p><p>I spent a few days writing and editing my proposal, <a href="https://github.com/npm/rfcs/pull/595">“Improving tarball compression while maintaining backwards compatibility”</a>. It was 708 words. I tried to make it sound compelling with things like this:</p><blockquote><p>Even a small savings, like 5%, would reduce the registry’s bandwidth usage by multiple terabytes. For example: React&nbsp;<a href="https://npm-stat.com/charts.html?package=react&amp;from=2021-01-01&amp;to=2021-12-31">was downloaded 561,743,096 times in 2021</a>. If we assume that each of these downloads shrunk from 81,166 bytes to 77,147 bytes, the registry would have saved more than 2 terabytes in bandwidth for React alone.</p></blockquote><p>On May 29, I finally submitted my RFC. I was nervous!</p><h2 id="2022-06-01-they-discussed-it">2022-06-01: they discussed it</h2><p>A few days later, the npm CLI people had <a href="https://github.com/npm/rfcs/issues/596">a meeting where they discussed a bunch of stuff, including my RFC</a>. I wasn’t able to attend but watched the recording. Overall, they felt it was worth evaluating further but were cautious. From their notes:</p><blockquote><p>Overall sentiment is that the compression improvement is welcome but it looks like it would take a proof of concept and challenge some of the edge cases to see if there are any unintended consequences, etc</p></blockquote><p>I built a <a href="https://evanhahn.github.io/npm-repack-with-zopfli-proof-of-concept/">little proof of concept web app</a> that used a WebAssembly port of Zopfli to recompress npm packages and <a href="https://github.com/npm/rfcs/pull/595#issuecomment-1145168973">posted about it on the RFC</a>.</p><p>I attended the next meeting a couple of weeks later.</p><h2 id="2022-06-15-the-meeting">2022-06-15: the meeting</h2><p>I attended a call on June 15.</p><p>I did the bad thing where I wasn’t really listening until it was my turn because I was thinking about what I was going to say. When it was finally my turn, I stammered.</p><p>Watching it back, I cringe a bit. I was wordy, unclear, and unconvincing. But I think I did an <em>okay</em> job making my point: npm packages could become ~5% smaller if we could figure out how to run Zopfli at publish time.</p><p>You can <a href="https://www.youtube.com/live/l8ob4j_KOR4?t=853">watch my mumbling in the recording</a>, as well as the npm maintainers’ feedback.</p><p><em>“Who benefits from this?”</em> was probably the biggest question. It was discussed that the npm registry folks, paying storage costs, might care about this. But “literally no one’s noticed” some other recent performance improvements, so they wanted to see more data. Was this just something I thought was neat (a “performance romantic”, as one person called it), or did this solve a real problem for users?</p><p>There were also concerns about including WebAssembly, the performance implications at install time, and Zopfli licensing issues.</p><p>I was tasked with some investigation on the topics above, and I got digging.</p><h2 id="2022-07-31-giving-up">2022-07-31: giving up</h2><p>After a bunch of thinking and feedback, what once seemed like an obviously great idea now seemed…well, less great.</p><p>On July 31, after doing a bunch of research, I posted <a href="https://github.com/npm/rfcs/pull/595#issuecomment-1200480148">a comment on the RFC</a>.</p><p>I wrote up some pros and cons. The pros:</p><ul><li>The top 250 npm packages would shrink by about 4.5% with this change.</li><li>This compression would be backwards compatible, and would require no changes from anyone else.</li></ul><p>But the cons were substantial:</p><ul><li>Integrating Zopfli into the npm CLI would be difficult.</li><li>Publishing would be slower—in some cases, <em>much</em> slower.</li><li>This wouldn’t retroactively apply to existing packages.</li></ul><p>After this discussion and thinking about the tradeoffs, I felt that it was not worth it, and I closed my RFC.</p><p>And that was that!</p><h2 id="lessons-learned">Lessons learned</h2><p>I learned a lot during this process.</p><p>It was a bit nerve-wracking, but I learned how to make proposals like this. I’d written internal proposals at work, but I’d never made a semi-official RFC like this before.</p><p>I also think I did a pretty bad job in my verbal communication during the meeting. Perhaps it was because I was nervous. I could have done a better job communicating about the tradeoffs—good and bad—of the proposal. And I could have been less wordy!</p><p>I also learned that things that seem like obvious wins aren’t always obvious wins, either because the motivation isn’t there or because there are trade-offs I minimized.</p><p>Overall, even though my proposal was denied, I’m glad I did this. I think I’m a better engineer for it! I hope this story was interesting and useful to you, dear reader.</p><p>(Oh, and I’m still <a href="https://github.com/helmetjs/helmet/blob/632e629b08de04bbd7188934641f3535af21685d/build/build-package.ts#L341">compressing my own modules with Zopfli</a>.)</p></div></div>]]></description>
        </item>
    </channel>
</rss>