<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 22 Jan 2025 15:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[My Struggle with Doom Scrolling (113 pts)]]></title>
            <link>https://allthatjazz.me/posts/doom-scrolling-struggles</link>
            <guid>42791428</guid>
            <pubDate>Wed, 22 Jan 2025 11:00:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://allthatjazz.me/posts/doom-scrolling-struggles">https://allthatjazz.me/posts/doom-scrolling-struggles</a>, See on <a href="https://news.ycombinator.com/item?id=42791428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It's been a few months since I deleted a lot of apps on my phone and instead installed many restrictive apps to control my insatiable <em>scrolling</em> addiction.</p>
<h2>Fighting the interconnected reality</h2>
<p>There are simply too many means to escapism in one's electronic devices:</p>
<ul>
<li>
Content apps: Instagram, YouTube, TikTok, etc.
</li>
<li>
Work apps: Slack, Workplace, etc
</li>
<li>
All these apps have moderate web apps, so no getaway while you have an easily accessible browser on your phone
</li>
<li>
You can easily login to these websites on your work laptop
</li>
</ul>
<p>Everything is hyper-connected and available at all times to occupy us. Wouldn't people prone to procrastination, such as myself, naturally choose to enter the rabbit hole if everything is at their fingertips? I think they would. At least, I did so.</p>
<h2>Do I need all these means of viewing content and scrolling at all times?</h2>
<p>Probably not. Let's experiment.</p>
<h3>The solution</h3>
<ul>
<li>Delete content apps
<ul>
<li>
Entertainment: YouTube, TikTok, Instagram, Reddit
</li>
<li>
Information databases: GoodReads, StoryGraph, IMDB
</li>
<li>
Messaging: Gmail, Slack
</li>
</ul>
</li>
<li>Delete the browser from the phone
<ul>
<li>
Chrome, or rather disable it, because deleting it is not an option on my Android phone
</li>
<li>
Google, a super-app on Android phones with the browsing feature
</li>
</ul>
</li>
<li>Delete personal profile/browser from the work laptop
<ul>
<li>
I used to have "Arc" browser on my work laptop with my personal Google account profile
</li>
</ul>
</li>
</ul>
<h3>Some early-stage issues</h3>
<ul>
<li>
Sharing a link/content from the work laptop to a personal space got more difficult. However, it's not a frequent problem.
</li>
<li>
I lost access to some services I used with my personal account. It was more convenient to do a simple task while working on my corporate laptop. Again, a problem that one can get used to.
</li>
<li>Deleting Chrome browser and Google apps caused worse problems
<ul>
<li>
I didn't even realize until then that some separate apps rely on the Google super-app. Like the weather app or some functionalities of the translation app! I lost access to these functionalities with disabling the super-app.
</li>
<li>
In some rare cases, the web app is better than the native Android app. I lost the ability to use them because I didn't have a browser anymore.
</li>
<li>
Some bank transfers need the browser page for authentication. Rarely, some other operations get blocked too without a browser.
</li>
<li>
Some quick, simple tasks don't need a whole Android app, a browser can handle them. But without a browser, those simple tasks become cumbersome.
</li>
<li>
Without the Email app, it's difficult to quickly access a ticket or receipt when you need it outdoors, away from your laptop.
</li>
</ul>
</li>
</ul>
<h3>Current Situation</h3>
<ul>
<li>
Deleting the personal account browser from the work laptop was successful. After a while, I got used to the minor inconveniences. More importantly, I removed some easily accessible procrastination tools from my professional workflow. No more checking personal emails, running quick online errands, or look something up.
</li>
<li>
Deleting work feeds from my phone was also successful. I realized I don't really need Slack, Gmail and Workplace apps all the time. It helped me set healthy boundaries with work. Now I only check my messages when I'm behind my desk and I'm not busy with focused, deep work.
</li>
<li>
Deleting content apps from my phone was another successful attempt. I've tried this method now and then. This time, I stuck with it once and for all.
</li>
<li>Deleting the browser apps from my phone proved more problematic, though. It impeded some daily operations with my phone.
<ul>
<li>
Some actions became impossible, as I mentioned in the previous part
</li>
<li>
To resolve that, I had to return to having browser apps on my phone, but with new restrictions.
</li>
</ul>
</li>
</ul>
<h4>Later modifications</h4>
<p>To resolve the complications of not having a browser on my phone, I took these steps:</p>
<ul>
<li>A minimal launch UI to break my habits:
<ul>
<li>
The new UI is a simplistic black screen with no widgets or app icons
</li>
<li>This app can restrict other apps or hide an app from the view
<ul>
<li>
For example, I use this feature to enable the Google super-app, but hide it completely. So it's difficult for me to open it directly.
</li>
</ul>
</li>
</ul>
</li>
<li>A more advanced website blocker and set restrictions on social media and browsing websites
<ul>
<li>
An interesting method I found in this app was to block distracting websites based on the location. I set them to be blocked at home, where I spend most of my focused time.
</li>
</ul>
</li>
<li>A new browser instead of Chrome
<ul>
<li>
Doing this helped me break some old habits I've formed over the years with a browser I'm used to. For example, the Google ecosystem knew me too well and constantly recommended articles to me to click on. A new browser without this historical data helped me to focus on the task at hand instead of getting distracted by the shiny, new, but useless information online.
</li>
</ul>
</li>
</ul>
<h2>The Results</h2>
<p>After a few months of making the above changes and applying some adjustments, I'm seeing its effects. I still procrastinate, however, my devices can't lure me into the rabbit hole anymore. I have more control over what's available to me and I have an easier time choosing the alternatives over infinite scrolling.</p>
<p>These days when I spend time on my phone, I usually play "Spelling Bee," a game I enjoy and it has a daily "end" state unlike the infinite content apps. When I reach that point, I know that there isn't much to do with my phone. I always wanted to expand my reading time but never got to do that with the constant exposure to easy content. It's been a while that I'm able to choose to read when I'm looking for something to do besides work and chores.</p>
<p>I constantly need to find new ways to constrain my media consumption addictions, but I'm happy with my efforts. When it works, I feel saner and calmer and I do what I actually enjoy doing with my time. And if not, at least I'm doing what I need to do.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DHS removes all members of cyber security advisory boards, halts investigations (129 pts)]]></title>
            <link>https://bsky.app/profile/ericjgeller.com/post/3lgbpqmxeok2f</link>
            <guid>42790207</guid>
            <pubDate>Wed, 22 Jan 2025 07:53:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.app/profile/ericjgeller.com/post/3lgbpqmxeok2f">https://bsky.app/profile/ericjgeller.com/post/3lgbpqmxeok2f</a>, See on <a href="https://news.ycombinator.com/item?id=42790207">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What are these bumps on the top of a pull-tab can? (235 pts)]]></title>
            <link>https://old.reddit.com/r/whatisthisthing/comments/1i5ztq4/comment/m8a7m8m/</link>
            <guid>42788455</guid>
            <pubDate>Wed, 22 Jan 2025 03:03:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/whatisthisthing/comments/1i5ztq4/comment/m8a7m8m/">https://old.reddit.com/r/whatisthisthing/comments/1i5ztq4/comment/m8a7m8m/</a>, See on <a href="https://news.ycombinator.com/item?id=42788455">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/whatisthisthing/comments/1i5ztq4/comment/m8a7m8m/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Tensor Product Attention Is All You Need (114 pts)]]></title>
            <link>https://arxiv.org/abs/2501.06425</link>
            <guid>42788451</guid>
            <pubDate>Wed, 22 Jan 2025 03:02:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2501.06425">https://arxiv.org/abs/2501.06425</a>, See on <a href="https://news.ycombinator.com/item?id=42788451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2501.06425">View PDF</a>
    <a href="https://arxiv.org/html/2501.06425v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at <a href="https://github.com/tensorgi/T6" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yifan Zhang [<a href="https://arxiv.org/show-email/d77f1c39/2501.06425" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Sat, 11 Jan 2025 03:37:10 UTC (1,505 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Isolating complexity is the essence of successful abstractions (170 pts)]]></title>
            <link>https://v5.chriskrycho.com/journal/essence-of-successful-abstractions/</link>
            <guid>42787531</guid>
            <pubDate>Wed, 22 Jan 2025 01:21:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://v5.chriskrycho.com/journal/essence-of-successful-abstractions/">https://v5.chriskrycho.com/journal/essence-of-successful-abstractions/</a>, See on <a href="https://news.ycombinator.com/item?id=42787531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      


<p><b><a href="https://v4.chriskrycho.com/2018/assumed-audiences.html">Assumed audience</a>:</b> Software developers who want to improve at their craft. Assumes a bit of background about programming in general, and also just a <em>little</em> bit of background knowledge about Rust and TypeScript. (You’ll be just fine if you know no more of either than that they exist and roughly what they are.)</p><p>As a bit of a prelude, consider this extended quote from Fred Hebert, <a href="https://ferd.ca/complexity-has-to-live-somewhere.html">Complexity Has to Live Somewhere</a></p>
<blockquote>
<p>Fighting complexity is a recurring theme of software development I’ve seen repeat itself over and over again. It’s something I keep seeing debated at all levels: just how much commenting should go on in functions and methods? What’s the ideal amount of abstraction? When does a framework start having<span></span> <span>“</span>too much magic”? When are there too many languages in an organisation?</p>
<p>We try to get rid of the complexity, control it, and seek simplicity. I think framing things that way is misguided. Complexity has to live somewhere.</p>
<p>…</p>
<p>When we adopt something like microservices, we try to make it so that each service is individually simple. But unless this simplicity is so constraining that your actual application inherits it and is forced into simplicity, it still has to go somewhere. If it’s not in the individual microservices, then where is it?</p>
<p>Complexity has to live somewhere. If you are lucky, it lives in well-defined places. In code where you decided a bit of complexity should go, in documentation that supports the code, in training sessions for your engineers. You give it a place without trying to hide all of it. You create ways to manage it. You know where to go to meet it when you need it. If you’re unlucky and you just tried to pretend complexity could be avoided altogether, it has no place to go in this world. But it still doesn’t stop existing.</p>
<p>With nowhere to go, it has to roam everywhere in your system, both in your code and in people’s heads. And as people shift around and leave, our understanding of it erodes.</p>
</blockquote>
<p>There’s more, and it’s good, so <a href="https://ferd.ca/complexity-has-to-live-somewhere.html">read the whole thing</a>! And as you do, you might consider how it pairs with the quote from Peter Naur <a href="https://v5.chriskrycho.com/notes/two-quotes-on-software-engineering/">I shared yesterday</a>. Now — </p>
<p>Hebert applies this to microservices, I think appropriately. I think it also applies to <em>types</em>. The types always exist; the complexity they represent always exists. The question is first of all whether we have written them down anywhere — that is, whether we have a contract about them that can be enforced, and if so what can enforce them. Secondarily, then, the expressiveness of a type system is a measure of what kinds of the program’s complexity we <em>can</em> write down as types.</p>
<p>People who are a bit allergic to types often feel constrained by them, but those of us who like robust types see that relationship the other way around. As <a href="https://dfreeman.io/">Dan Freeman</a> put it to me recently: types are a tool, just like tests, but you have to <em>use</em> that tool. Types do provide constraints, much as tests do — but they are only the types <em>we choose to impose</em>. That is, they are knowledge we are encoding into the program.</p>
<p>The same basic thesis applies to our tests, which represent a different set of knowledge than our types. This is the benefit of<span></span> <span>“</span>test-driven development”, particularly in the red-green cycle: it is a way of encoding knowledge into the program. Test expressiveness works in different ways than type expressiveness, but it is a useful way of evaluating different kinds of and different approaches to testing.</p>
<p>This is why I like Rust. The complexity of correctly <a href="https://blog.yoshuawuyts.com/temporal-spatial-memory-safety/">spatial and temporal safety</a> has to live somewhere. Rust pushes the majority of that complexity into its type system, and particularly the famed borrow checker. The rest of the complexity it isolates in <code>unsafe</code> blocks. Mind: <code>unsafe</code> allows you to know where the complexity lives, but it does not tell you where your getting that complexity wrong will <em>surface</em>. But the combination of the borrow checker and the isolation that <code>unsafe</code> offers <em>does</em> allow us to <em>control</em> the complexity. We cannot get rid of it. We can only isolate it.</p>
<p>A garbage collector gives you memory safety and not having to think explicitly about memory allocation and deallocation, but the complexity is still not gone — it has moved to the implementation, to weird performance cliffs, and to difficult-to-debug leaks of memory or other resources. That trade is very often worth it, though!</p>
<p>Whether with Rust or with a garbage collector, isolating the temporal and spatial safety means we don’t have to keep it in our heads <em>most of the time</em>, and that allows us to focus on other problems. This is why there are so many developer tools for JavaScript and Python being written in Rust: not because people didn’t want fast tools before, but because for many developers, keeping all the safety issues in their heads as they would have had to do with C or C++ was more than they felt they could reasonably do <em>and also ship the thing they cared about</em>. Rust’s borrow checking opens that door to many more people, because it has isolated that complexity, and thereby provided a more usable tool.</p>
<p>This exact dynamic is also why I like TypeScript, even including some of the wilder types I ended up writing for Ember and LinkedIn in years past. The complexity was always there — often far more than anyone realized before we started trying to write down types that actually accurately represented the system. TypeScript did not create the complexity, much as people sometimes <em>felt</em> like it added complexity to the code we were looking at. No, it merely shone a light on the existing complexity, and gave us the opportunity — and a tool with which — to <a href="https://v5.chriskrycho.com/journal/is-typescript-good/#:~:text=In%20software%2C%20the,of%20the%20system.">start grappling with it</a>.</p>
<p>Isolating complexity is useful. As Hebert says,<span></span> <span>“</span>If you are lucky, it lives in well-defined places.” It is not always possible to isolate complexity — we are not always lucky — , but when isolating complexity <em>is</em> possible, it is glorious, because isolating complexity is the essence of successful abstractions.</p>

   </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ross Ulbricht was just granted a full pardon (1471 pts)]]></title>
            <link>https://twitter.com/Free_Ross/status/1881851923005165704</link>
            <guid>42786962</guid>
            <pubDate>Wed, 22 Jan 2025 00:10:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Free_Ross/status/1881851923005165704">https://twitter.com/Free_Ross/status/1881851923005165704</a>, See on <a href="https://news.ycombinator.com/item?id=42786962">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Hunyuan3D 2.0 – High-Resolution 3D Assets Generation (257 pts)]]></title>
            <link>https://github.com/Tencent/Hunyuan3D-2</link>
            <guid>42786040</guid>
            <pubDate>Tue, 21 Jan 2025 22:42:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Tencent/Hunyuan3D-2">https://github.com/Tencent/Hunyuan3D-2</a>, See on <a href="https://news.ycombinator.com/item?id=42786040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://github.com/Tencent/Hunyuan3D-2/blob/main/README_zh_cn.md">中文阅读</a></p>
<p dir="auto"> 
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/teaser.jpg"><img src="https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/teaser.jpg"></a>
</p>
<p><a href="https://3d.hunyuan.tencent.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/febf402ad9aa71393c9530c20f10eb5ff99f8cc9762cec7d49a8cb675dd0433c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f6666696369616c253230536974652d626c61636b2e7376673f6c6f676f3d686f6d6570616765" height="22px" data-canonical-src="https://img.shields.io/badge/Official%20Site-black.svg?logo=homepage"></a>
  <a href="https://huggingface.co/spaces/tencent/Hunyuan3D-2" rel="nofollow"><img src="https://camo.githubusercontent.com/b01df6333acfa3279bfe495dce3acab0be0c1027bb6dfe321a2b2d4eaf9dc6c2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323044656d6f2d3237366362342e737667" height="22px" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg"></a>
  <a href="https://huggingface.co/tencent/Hunyuan3D-2" rel="nofollow"><img src="https://camo.githubusercontent.com/dbbc817c0d92c742d5b07f0d54d890f96e5cea25b2124c12b9acd58ecc7b0cb7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372532304d6f64656c732d6439363930322e737667" height="22px" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg"></a>
  <a href="https://3d-models.hunyuan.tencent.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/30e5ac4827a0cef8589c5402c9020003d93136b3fffd99d6d91b57bf5cdbea15/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f506167652d6262386132652e7376673f6c6f676f3d676974687562" height="22px" data-canonical-src="https://img.shields.io/badge/Page-bb8a2e.svg?logo=github"></a>
  <a href="https://discord.gg/GuaWYwzKbX" rel="nofollow"><img src="https://camo.githubusercontent.com/d25736b4edde958cb4922599c6ba056a053637b482d31c24f41393eefdc73a4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d77686974652e7376673f6c6f676f3d646973636f7264" height="22px" data-canonical-src="https://img.shields.io/badge/Discord-white.svg?logo=discord"></a>
  <a href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/report/Tencent_Hunyuan3D_2_0.pdf"><img src="https://camo.githubusercontent.com/cf668b2c3aaa6c4d256eddd435c21642bba70e38b4baae0719946859f6efe677/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5265706f72742d6235323132662e7376673f6c6f676f3d6172786976" height="22px" data-canonical-src="https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv"></a>
</p>

<p dir="auto">
“ Living out everyone’s imagination on creating and manipulating 3D assets.”
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔥 News</h2><a id="user-content--news" aria-label="Permalink: 🔥 News" href="#-news"></a></p>
<ul dir="auto">
<li>Jan 21, 2025: 💬 Enjoy exciting 3D generation on our website <a href="https://3d.hunyuan.tencent.com/" rel="nofollow">Hunyuan3D Studio</a>!</li>
<li>Jan 21, 2025: 💬 Release inference code and pretrained models of <a href="https://huggingface.co/tencent/Hunyuan3D-2" rel="nofollow">Hunyuan3D 2.0</a>.</li>
<li>Jan 21, 2025: 💬 Release Hunyuan3D 2.0. Please give it a try via <a href="https://huggingface.co/spaces/tencent/Hunyuan3D-2" rel="nofollow">huggingface space</a> our <a href="https://3d.hunyuan.tencent.com/" rel="nofollow">official site</a>!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><strong>Abstract</strong></h2><a id="user-content-abstract" aria-label="Permalink: Abstract" href="#abstract"></a></p>
<p dir="auto">We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.
This system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale
texture synthesis model - Hunyuan3D-Paint.
The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly
aligns with a given condition image, laying a solid foundation for downstream applications.
The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant
texture maps for either generated or hand-crafted meshes.
Furthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation
process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes
efficiently.
We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,
including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and
e.t.c.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/system.jpg"><img src="https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/system.jpg"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">☯️ <strong>Hunyuan3D 2.0</strong></h2><a id="user-content-️-hunyuan3d-20" aria-label="Permalink: ☯️ Hunyuan3D 2.0" href="#️-hunyuan3d-20"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Architecture</h3><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">Hunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the
synthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and
texture generation and also provides flexibility for texturing either generated or handcrafted meshes.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/arch.jpg"><img src="https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/arch.jpg"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance</h3><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">We have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.
The numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets
and the condition following ability.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>CMMD(⬇)</th>
<th>FID_CLIP(⬇)</th>
<th>FID(⬇)</th>
<th>CLIP-score(⬆)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Top Open-source Model1</td>
<td>3.591</td>
<td>54.639</td>
<td>289.287</td>
<td>0.787</td>
</tr>
<tr>
<td>Top Close-source Model1</td>
<td>3.600</td>
<td>55.866</td>
<td>305.922</td>
<td>0.779</td>
</tr>
<tr>
<td>Top Close-source Model2</td>
<td>3.368</td>
<td>49.744</td>
<td>294.628</td>
<td>0.806</td>
</tr>
<tr>
<td>Top Close-source Model3</td>
<td>3.218</td>
<td>51.574</td>
<td>295.691</td>
<td>0.799</td>
</tr>
<tr>
<td>Hunyuan3D 2.0</td>
<td><strong>3.193</strong></td>
<td><strong>49.165</strong></td>
<td><strong>282.429</strong></td>
<td><strong>0.809</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Generation results of Hunyuan3D 2.0:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/e2e-1.gif"><img src="https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/e2e-1.gif" height="250" data-animated-image=""></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/Hunyuan3D-2/blob/main/assets/images/e2e-2.gif"><img src="https://github.com/Tencent/Hunyuan3D-2/raw/main/assets/images/e2e-2.gif" height="250" data-animated-image=""></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pretrained Models</h3><a id="user-content-pretrained-models" aria-label="Permalink: Pretrained Models" href="#pretrained-models"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Date</th>
<th>Huggingface</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hunyuan3D-DiT-v2-0</td>
<td>2025-01-21</td>
<td><a href="https://huggingface.co/tencent/Hunyuan3D-2" rel="nofollow">Download</a></td>
</tr>
<tr>
<td>Hunyuan3D-Paint-v2-0</td>
<td>2025-01-21</td>
<td><a href="https://huggingface.co/tencent/Hunyuan3D-2" rel="nofollow">Download</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤗 Get Started with Hunyuan3D 2.0</h2><a id="user-content--get-started-with-hunyuan3d-20" aria-label="Permalink: 🤗 Get Started with Hunyuan3D 2.0" href="#-get-started-with-hunyuan3d-20"></a></p>
<p dir="auto">You may follow the next steps to use Hunyuan3D 2.0 via code or the Gradio App.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install Requirements</h3><a id="user-content-install-requirements" aria-label="Permalink: Install Requirements" href="#install-requirements"></a></p>
<p dir="auto">Please install Pytorch via the <a href="https://pytorch.org/" rel="nofollow">official</a> site. Then install the other requirements via</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt
# for texture
cd hy3dgen/texgen/custom_rasterizer
python3 setup.py install
cd hy3dgen/texgen/differentiable_renderer
bash compile_mesh_painter.sh"><pre>pip install -r requirements.txt
<span><span>#</span> for texture</span>
<span>cd</span> hy3dgen/texgen/custom_rasterizer
python3 setup.py install
<span>cd</span> hy3dgen/texgen/differentiable_renderer
bash compile_mesh_painter.sh</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">API Usage</h3><a id="user-content-api-usage" aria-label="Permalink: API Usage" href="#api-usage"></a></p>
<p dir="auto">We designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -
Hunyuan3D-Paint.</p>
<p dir="auto">You could assess <strong>Hunyuan3D-DiT</strong> via:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline

pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')
mesh = pipeline(image='assets/demo.png')[0]"><pre><span>from</span> <span>hy3dgen</span>.<span>shapegen</span> <span>import</span> <span>Hunyuan3DDiTFlowMatchingPipeline</span>

<span>pipeline</span> <span>=</span> <span>Hunyuan3DDiTFlowMatchingPipeline</span>.<span>from_pretrained</span>(<span>'tencent/Hunyuan3D-2'</span>)
<span>mesh</span> <span>=</span> <span>pipeline</span>(<span>image</span><span>=</span><span>'assets/demo.png'</span>)[<span>0</span>]</pre></div>
<p dir="auto">The output mesh is a <a href="https://trimesh.org/trimesh.html" rel="nofollow">trimesh object</a>, which you could save to glb/obj (or other
format) file.</p>
<p dir="auto">For <strong>Hunyuan3D-Paint</strong>, do the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from hy3dgen.texgen import Hunyuan3DPaintPipeline
from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline

# let's generate a mesh first
pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')
mesh = pipeline(image='assets/demo.png')[0]

pipeline = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2')
mesh = pipeline(mesh, image='assets/demo.png')"><pre><span>from</span> <span>hy3dgen</span>.<span>texgen</span> <span>import</span> <span>Hunyuan3DPaintPipeline</span>
<span>from</span> <span>hy3dgen</span>.<span>shapegen</span> <span>import</span> <span>Hunyuan3DDiTFlowMatchingPipeline</span>

<span># let's generate a mesh first</span>
<span>pipeline</span> <span>=</span> <span>Hunyuan3DDiTFlowMatchingPipeline</span>.<span>from_pretrained</span>(<span>'tencent/Hunyuan3D-2'</span>)
<span>mesh</span> <span>=</span> <span>pipeline</span>(<span>image</span><span>=</span><span>'assets/demo.png'</span>)[<span>0</span>]

<span>pipeline</span> <span>=</span> <span>Hunyuan3DPaintPipeline</span>.<span>from_pretrained</span>(<span>'tencent/Hunyuan3D-2'</span>)
<span>mesh</span> <span>=</span> <span>pipeline</span>(<span>mesh</span>, <span>image</span><span>=</span><span>'assets/demo.png'</span>)</pre></div>
<p dir="auto">Please visit <a href="https://github.com/Tencent/Hunyuan3D-2/blob/main/minimal_demo.py">minimal_demo.py</a> for more advanced usage, such as <strong>text to 3D</strong> and <strong>texture generation
for handcrafted mesh</strong>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Gradio App</h3><a id="user-content-gradio-app" aria-label="Permalink: Gradio App" href="#gradio-app"></a></p>
<p dir="auto">You could also host a <a href="https://www.gradio.app/" rel="nofollow">Gradio</a> App in your own computer via:</p>

<p dir="auto">Don't forget to visit <a href="https://3d.hunyuan.tencent.com/" rel="nofollow">Hunyuan3D</a> for quick use, if you don't want to host yourself.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📑 Open-Source Plan</h2><a id="user-content--open-source-plan" aria-label="Permalink: 📑 Open-Source Plan" href="#-open-source-plan"></a></p>
<ul>
<li> Inference Code</li>
<li> Model Checkpoints</li>
<li> Technical Report</li>
<li> ComfyUI</li>
<li> TensorRT Version</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 BibTeX</h2><a id="user-content--bibtex" aria-label="Permalink: 🔗 BibTeX" href="#-bibtex"></a></p>
<p dir="auto">If you found this repository helpful, please cite our reports:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{hunyuan3d22025tencent,
    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},
    author={Tencent Hunyuan3D Team},
    year={2025},
}

@misc{yang2024tencent,
    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},
    year={2024},
    author={Tencent Hunyuan3D Team},
    eprint={2411.02293},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>hunyuan3d22025tencent</span>,
    <span>title</span>=<span><span>{</span>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation<span>}</span></span>,
    <span>author</span>=<span><span>{</span>Tencent Hunyuan3D Team<span>}</span></span>,
    <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
}

<span>@misc</span>{<span>yang2024tencent</span>,
    <span>title</span>=<span><span>{</span>Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation<span>}</span></span>,
    <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
    <span>author</span>=<span><span>{</span>Tencent Hunyuan3D Team<span>}</span></span>,
    <span>eprint</span>=<span><span>{</span>2411.02293<span>}</span></span>,
    <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
    <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">We would like to thank the contributors to
the <a href="https://github.com/facebookresearch/dinov2">DINOv2</a>, <a href="https://github.com/Stability-AI/stablediffusion">Stable Diffusion</a>, <a href="https://github.com/black-forest-labs/flux">FLUX</a>, <a href="https://github.com/huggingface/diffusers">diffusers</a>, <a href="https://huggingface.co/" rel="nofollow">HuggingFace</a>, <a href="https://github.com/wyysf-98/CraftsMan3D">CraftsMan3D</a>, and <a href="https://github.com/NeuralCarver/Michelangelo/tree/main">Michelangelo</a> repositories, for their open research and exploration.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Star History</h2><a id="user-content-star-history" aria-label="Permalink: Star History" href="#star-history"></a></p>
<a href="https://star-history.com/#Tencent/Hunyuan3D-2&amp;Date" rel="nofollow">
 <themed-picture data-catalyst-inline="true"><picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/e768710d4e8b6d1209558f1887f847f85fb4d0ec7c5359bf4622a8aebafd9dfe/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d54656e63656e742f48756e7975616e33442d3226747970653d44617465267468656d653d6461726b" data-canonical-src="https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&amp;theme=dark">
   <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/087278d49d3be0181fd0bf1eb124b5d5199b5e28450487147b5a7837b8391476/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d54656e63656e742f48756e7975616e33442d3226747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date">
   <img alt="Star History Chart" src="https://camo.githubusercontent.com/087278d49d3be0181fd0bf1eb124b5d5199b5e28450487147b5a7837b8391476/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d54656e63656e742f48756e7975616e33442d3226747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date">
 </picture></themed-picture>
</a>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stargate Project: SoftBank, OpenAI and Oracle to invest $500B in AI (793 pts)]]></title>
            <link>https://apnews.com/article/trump-ai-openai-oracle-softbank-son-altman-ellison-be261f8a8ee07a0623d4170397348c41</link>
            <guid>42785891</guid>
            <pubDate>Tue, 21 Jan 2025 22:29:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/trump-ai-openai-oracle-softbank-son-altman-ellison-be261f8a8ee07a0623d4170397348c41">https://apnews.com/article/trump-ai-openai-oracle-softbank-son-altman-ellison-be261f8a8ee07a0623d4170397348c41</a>, See on <a href="https://news.ycombinator.com/item?id=42785891">Hacker News</a></p>
Couldn't get https://apnews.com/article/trump-ai-openai-oracle-softbank-son-altman-ellison-be261f8a8ee07a0623d4170397348c41: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[JReleaser: quick and effortless way to release your project (113 pts)]]></title>
            <link>https://jreleaser.org/</link>
            <guid>42784880</guid>
            <pubDate>Tue, 21 Jan 2025 20:33:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jreleaser.org/">https://jreleaser.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42784880">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <main>

        <div>
                    <div>
                        
                        <p>The quick and effortless way to release your project!</p>
                        <p>Java, Go, Node, Rust, Zig, Swift, Perl, Python, C/C++, C#, Elixir, Haskell, Ruby, Crystal, and more.</p>
                        
                    </div>
                    <p><img width="300" src="https://jreleaser.org/images/jreleaser-duke.png">

                    </p>
                </div>

        <div>

            <div>
                <div>
                            <h4><a href="#">Checksums</a></h4>
                            <p>
                                Calculate checksums for all input files.
                            </p>
                        </div>

                

                <div>
                            <h4><a href="#">Release to git</a></h4>
                            <p>
                                Tag and release to popular git services such as GitHub, GitLab, and Gitea.
                            </p>
                        </div>

                

                

                <div>
                            <h4><a href="#">Make it official!</a></h4>
                            <p>
                                Announce the release to the four winds via Twitter, Sdkman, Zulip, and other services.
                            </p>
                        </div>

                

                <div>
                            <h4><a href="#">Build it your way</a></h4>
                            <p>
                                Whether you prefer CLI, Maven, Gradle, or Ant, JReleaser has you covered.
                            </p>
                        </div>
            </div>

            <hr>

            <div>
                    <blockquote><p lang="en" dir="ltr">The best thing about <a href="https://twitter.com/jreleaser?ref_src=twsrc%5Etfw">@jreleaser</a> IMHO is that it
                        gives you very nice defaults. Took only a 3 line change to package my app as a Homebrew tap.</p>—
                        Oliver Weiler (@helpermethod) <a href="https://twitter.com/helpermethod/status/1412036782674296837?ref_src=twsrc%5Etfw">July
                            5, 2021</a></blockquote>
                    
                </div>
            

            <hr>

            <div>
                <div>
                    <h2>Quick configuration.</h2>
                    <div><p>Configure the essentials and let JReleaser do the heavy lifting. Convention over
                        configuration is a thing!</p><p>
                        YAML is not your preferred format? We've got other formats like TOML, JSON, a Maven DSL, and a
                        Gradle DSL.</p></div>
                </div>
                <p><img width="500" src="https://jreleaser.org/images/app-jreleaser-config.png" alt="JReleaser configuration file in YAML format">

                </p>
            </div>

            <hr>

            <div>
                <div>
                    <h2>Publish and announce a release.</h2>
                    <div><p>Publishing a release is as simple as executing a single command using the <code>jreleaser</code>
                        CLI.</p><p>
                        We also offer Maven and Gradle plugins, Ant tasks, JBang integration, and a Docker image to
                        boot!</p></div>
                </div>
                <div>
                    <p><img width="500" src="https://jreleaser.org/images/app-full-release-command.png" alt="JReleaser execution log"></p><p>
                    <img width="500" src="https://jreleaser.org/images/app-twitter-announce.png" alt="A release announce via Twitter"></p></div>
            </div>

            <hr>

            <div>
                <div>
                    <h2>Create releases without hassle</h2>
                    <p>Releases are automatically tagged. A changelog is generated with all commits since
                        the last tag.
                        All artifacts are uploaded.</p>
                </div>
                <p><img width="500" src="https://jreleaser.org/images/app-github-release.png" alt="Release page created by JReleaser">

                </p>
            </div>

            <hr>

            <div>
                <div>
                    <h2>Maximize distribution and reach.</h2>
                    <p>JReleaser can package your application to be distributed with popular software
                        packagers such as Homebrew, Scoop, Chocolatey, Snapcraft, and others.</p>
                </div>
                <p><img width="500" src="https://jreleaser.org/images/app-homebrew-tap.png" alt="Homebrew formula repository">

                </p>
            </div>

            <hr>

        </div><!-- /.container -->
    </main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Is anyone doing anything cool with tiny language models? (496 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42784365</link>
            <guid>42784365</guid>
            <pubDate>Tue, 21 Jan 2025 19:39:39 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42784365">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42784365: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mercator: Extreme (179 pts)]]></title>
            <link>https://mrgris.com/projects/merc-extreme/</link>
            <guid>42783948</guid>
            <pubDate>Tue, 21 Jan 2025 19:00:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mrgris.com/projects/merc-extreme/">https://mrgris.com/projects/merc-extreme/</a>, See on <a href="https://news.ycombinator.com/item?id=42783948">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="help">
      <h3>Controls</h3>
      <div id="controls">
        
        <div>
          <p>
            scrollwheel<br>
            double-click<br>
	    pinch
          </p>
          <p>zoom</p>
        </div>
        <div>
          <p>
            shift+drag<br>
            right-click drag<br>
	    two-finger drag
          </p>
          <p>warp map</p>
        </div>
        <div>
          <p>
            shift+double-click<br>
            right-double-click<br>
	    long-press
          </p>
          <p>set as pole</p>
        </div>
      </div>
      <h3>tl;dr</h3>
      <div>
        <p>The map you see is the Mercator projection.</p>
        <p>But unlike a standard Mercator projection, you can substitute any point on earth as the "pole". (The initial view shows Boston as the pole point)</p>
        <p>Furthermore, this map cuts off much, much closer to the poles than normal, allowing you to see many more orders of magnitude of distortion.</p>
        <p>Because this yields a map several times taller than it is wide, it is shown sideways from its usual orientation.</p>
      </div>
      <h3>Backstory</h3>
      <div>

        <p>
          The Mercator projection is infamous for its distortion at high latitudes.
          This distortion gets exponentially worse as you approach the poles.
          It is in fact impossible to show the poles on a Mercator map — they are infinitely far away.
        </p>

        <p>
          Any Mercator map you've ever seen must cut off the top/bottom edges at some arbitrary point.
          The map usually stops hundreds, if not thousands of miles short of the poles.
        </p>

        <p>
          But I've often wondered what lies beyond those cut-offs... to make a map that didn't cut off but simply kept going.
          As the distortion progresses towards infinity, you would eventually reach the scale of cities, houses, insects, atoms...
        </p>

        <p>
          But of course that'd all be on a featureless expanse of ice.
        </p>

        <p>
          To make things actually interesting, we must artifically shift the pole of the projection to a more interesting place.
          Imagine the earth encased by a rigid cage of latitude and longitude lines.
          We rotate the earth while leaving the cage fixed until a new point of interest has taken the place of the North Pole.
        </p>

        <p>
          This is called an <em>oblique Mercator</em>, and is normally used to shift an area of interest onto the equator of the map to avoid distortion.
          But whereas others avoid the distortion, we embrace it.
        </p>

        <p>
          Note how strange the oblique Mercator looks even without the increased cutoffs.
          The standard Mercator is so ingrained in the public consciousness that we perceive it as 'normal'.
          But once you shift the pole its pervasive distortion is shockingly apparent.
        </p>

        <p>
          Lastly, increasing the cutoffs produces a map much taller than it is wide.
          So it is presented sideways here for easier viewing (<a href="#" dest="sp">set the pole to the North/South Pole</a> like usual to see this most clearly).
          You can also switch back to "normal" orientation via the <a href="#"><img src="https://mrgris.com/projects/merc-extreme/img/rotate.png"> button</a>.
        </p>

      </div>
      <h3>Orientation</h3>
      <div>
        <p>
          Let's take a moment to digest what we have here. Namely:
          </p><ul>
            <li>a single map projection
            </li><li>…showing the entire surface of the earth
            </li><li>…while traversing all the way from the human scale, to the global scale
          </li></ul>
        

        <p>
          It really creates this "center of the universe" feeling.
          Basically a mathematical embodiment of the sentiment in this <a target="_blank" href="http://www.mappery.com/maps/A-View-of-World-from-9th-Avenue-Map.jpg">famous <i>New Yorker</i> cover</a>. (<a href="#" dest="ny" layer="map"><i>Mercator: Extreme</i> version</a>)
        </p>

        <p>Keep these principles in mind to best navigate around the map:

          </p><ul>
            <li>
              Horizontal lines on the map ("meridians") represent straight lines radiating out from the pole point in all directions.
              Lines one half of a map-width apart from each other represent <em>opposite</em> directions.
              The properties of the meridians are best seen with a central plaza like the <a href="#" dest="arc" layer="sat">Arc de Triomphe</a>.
            </li><li>
              Vertical lines ("parallels") show rings of constant distance from the pole point.
              This can visualize <a href="#" dest="vulcan">encircledness</a> or <a href="#" dest="sthelena">remoteness</a>.
              The nearest landfall from St. Helena in any direction can be found by a vertical line sweeping towards the left.
          </li></ul>
        

        <p>
          You may find yourself trying to get 'behind' or 'to the other side of' the pole point.
          There is no such thing!
          All directions away from the pole point proceed towards the left.
          The pole itself exists at all points along the right-hand edge (technically, infinitely far to the right).
        </p>

        <p>
          If at any time you get disoriented, I strongly recommend using the <a href="#">companion map <img src="https://mrgris.com/projects/merc-extreme/img/map.png"></a> to see the correspondence to a normal map which makes these navigational principles clear.
        </p>
      </div>
      <h3>Other Curiosities</h3>
      <div>
        <p>
          Now knowing that horizontal and vertical lines represent distance and bearing, if we center the pole on a <a href="#" dest="atl" layer="map">major city</a>, we can visualize its transportation network in a whole new way.
          Beltways and ring roads become vertical frontiers, while highways become a dendritic network branching off to destinations in all directions.
          We can see all of Atlanta's highway links in a single view, including the special few that make it as far as New York, St. Louis, Dallas, and Miami.
        </p>

        <p>
          Closely related, the extreme Mercator is an excellent way to visualize <a href="https://mrgris.com/projects/merc-extreme/img/routes_example.jpg" target="_blank">long-distance driving routes</a>.
          Think about how when you drive such a route, you start on some local/residential roads, then make your way to bigger arteries, and eventually to a highway.
          Here, even as smaller roads fade into nothing, the next road of your route always seems to still be visible.
          Size in this map is proportional to distance, so the extreme Mercator ends up normalizing scale in a very human-accessible way.
          The only cost is you have to turn your mind inside-out a bit.
        </p>

        <p>
          Lastly, the extreme Mercator is an excellent way to visualize antipodes — points that are on the complete opposite sides of the earth from each other.
          So far we haven't touched the left-side cutoff, namely because the antipode is usually <a target="_blank" href="http://upload.wikimedia.org/wikipedia/commons/6/64/Antipodes_LAEA.png">in the middle of the ocean</a>.
          But this is <a href="#" dest="antipode" layer="sat">not always the case</a>.
          Here we find two buildings exactly opposite the planet from each other.
          Where one is day, the other is always night.
        </p>

        <p>
          I hope you enjoy this novel way of viewing our world.
        </p>
      </div>

      <h3>Acknowledgements</h3>
      

      <!--
          gripes during development:
          - complete lack of double precision support in gpu
          - inability to explicitly choose mipmapping level or generate mipmaps for a texture subarea (texture atlases are a hack, basically, in the eyes of opengl)
          - can't choose which texture to access via a variable in shader code; need to unroll everything
          - coordinate system transforms are fucking hard to keep straight
      -->

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rafael Araujo's 20 Mesmerizing Geometrical Masterpieces (2024) (112 pts)]]></title>
            <link>https://abakcus.com/rafael-araujo-geometrical-masterpieces/</link>
            <guid>42783543</guid>
            <pubDate>Tue, 21 Jan 2025 18:24:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abakcus.com/rafael-araujo-geometrical-masterpieces/">https://abakcus.com/rafael-araujo-geometrical-masterpieces/</a>, See on <a href="https://news.ycombinator.com/item?id=42783543">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="cea3d08" data-element_type="widget" data-widget_type="theme-post-content.default"><p>Artist <a href="https://www.rafael-araujo.com/" target="_blank" rel="noopener">Rafael Araujo</a> expresses his love of <a href="https://abakcus.com/25-beautiful-nature-documentaries-to-watch-earth-day/" target="_blank" rel="noreferrer noopener">nature</a> through <a href="https://abakcus.com/beautiful-geometry-books-to-start-loving-geometry/" target="_blank" rel="noreferrer noopener">geometry</a>, intertwining <a href="https://abakcus.com/best-math-proof-books/" target="_blank" rel="noreferrer noopener">mathematical</a> precision with the organic beauty found within the natural world. By incorporating the <a href="https://abakcus.com/article/hannsjorg-voths-golden-spiral/" target="_blank" rel="noreferrer noopener">golden spiral</a> and helixes into his compositions, Rafael Araujo establishes a harmonious blend of <a href="https://abakcus.com/the-best-20-history-of-science-books/" target="_blank" rel="noreferrer noopener">science</a> and <a href="https://abakcus.com/article/susanna-bauers-natural-sculptures/" target="_blank" rel="noreferrer noopener">art</a>, using these concepts as the foundational basis for his stunning depictions of shells and kaleidoscopes of butterflies. The result is a breathtaking collection of drawings where the natural elements are intricately sketched and vividly colored.</p><p>At the same time, the <a href="https://abakcus.com/article/simon-becks-snow-drawings/" target="_blank" rel="noreferrer noopener">geometric</a> guiding lines remain present, serving as an essential underpinning of the artwork. In his own words, <strong>“Through the thorough use of <a href="https://abakcus.com/beautiful-geometry-7-colorful-geometry-posters/" target="_blank">geometry</a>, I try to attain and worship the platonic perfection of Nature.”</strong> This perspective highlights not just his creations’ aesthetic qualities but also his profound reverence for the intricate designs and patterns inherent in the world around us.</p><p>Rafael Araujo’s deliberate choice to leave these planning marks visible reinforces their significance in his work, reminding viewers of the complexity that lies beneath the simplicity of <a href="https://abakcus.com/tool/earth-fm-listen-to-nature-sounds/" target="_blank" rel="noreferrer noopener">nature</a>. A seemingly ordinary shell in one’s hand transforms into a cosmos of patterns and structures when viewed through Araujo’s artistic lens, illuminating the remarkable detail and interconnectedness of the universe. His contributions extend beyond his stunning <a href="https://abakcus.com/20-beautiful-childrens-books-stunning-illustration/" target="_blank" rel="noreferrer noopener">illustrations</a>; Araujo invites people to engage actively with his art through his adult coloring <a href="https://abakcus.com/mastering-ap-calculus-ab-top-10-self-study-textbooks/" target="_blank" rel="noreferrer noopener">books</a>. These books entice enthusiasts and budding artists alike to bring life to his intricate designs featuring shells, florals, and serpents, allowing them to partake in the artistic process and explore their creativity.</p><h2>No Room for Error in Rafael Araujo’s Drawings</h2><figure><div><p><iframe title="From chaos to order | Rafael Araujo | TEDxHangzhouSalon" width="1200" height="675" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" data-src="https://www.youtube.com/embed/sxwCBTD-8f8?feature=oembed"></iframe></p></div><figcaption>From chaos to order | Rafael Araujo’s TED Talk</figcaption></figure><p><strong>Have you ever attempted to work on an illustration for around 100 hours straight?</strong> This level of dedication is rare among artists, yet <a href="https://amzn.to/3X1Grtx" target="_blank" rel="noreferrer noopener">Rafael Araujo</a>, who is both an architect and an illustrator, approaches his craft with relentless patience. Inspired by the worlds of <a href="https://abakcus.com/must-read-books-on-mathematics-history/" target="_blank" rel="noreferrer noopener">mathematics</a> and nature, Araujo employs <a href="https://abakcus.com/top-10-islamic-geometry-books/" target="_blank" rel="noreferrer noopener">geometry</a>, trigonometry, and sequences of dots to produce jaw-dropping three-dimensional images that often appear to have been generated by cutting-edge computer software. However, what sets Araujo apart is his unwavering commitment to traditional techniques; each artwork is painstakingly rendered by hand using only essential tools—a lamp for light, a ruler, a compass, and a protractor. Through this meticulous process, he unfolds the significance of the <a href="https://abakcus.com/products/fibonacci-golden-spiral-clock/" target="_blank" rel="noreferrer noopener">Golden</a> Ratio, showcasing its spiritual depth and presence within the natural order.</p><p>In Araujo’s artistic journey, there truly is no room for error. He understands that while the creative spirit might flow freely, the technical execution must be flawless: <strong>“Working by hand, with ink on canvas, my main concern is to be aware of what I’m always doing. Errors are paid dearly.”</strong> Araujo recounts that he became engrossed in the intelligent patterns of nature at a young age, noticing spirals, sequences, and proportions that reveal the hidden structures of life itself. This awakening to nature’s complexities ignited a passion for capturing these relationships in his art.</p><p>By making <strong><a href="https://amzn.to/4cI8nsp" target="_blank" rel="noopener">the Golden Ratio Coloring Book,</a></strong> Rafael Araujo endeavors to share his spiritual connection to the <a href="https://abakcus.com/tool/geometry-of-pasta/" target="_blank" rel="noreferrer noopener">geometry</a> inherent in nature, allowing others to engage with his <a href="https://abakcus.com/must-read-world-philosophy-books/" target="_blank" rel="noreferrer noopener">philosophy</a> on a personal level. The delicate balance between <a href="https://abakcus.com/books-about-applications-of-mathematics/" target="_blank" rel="noreferrer noopener">mathematics</a> and aesthetics that defines his work is captured in the <a href="https://abakcus.com/100-beautiful-nature-illustrations-from-ernst-haeckels-kunstformen-der-natur/" target="_blank" rel="noreferrer noopener">illustrations</a> curated within his books. For those captivated by Rafael Araujo’s artistic vision, a selection of stunning coffee table <a href="https://abakcus.com/the-physicists-library-best-33-physics-books/" target="_blank" rel="noreferrer noopener">books</a> is available for purchase, inviting a deeper exploration into the divine beauty of <a href="https://abakcus.com/greatest-mathematicians/" target="_blank" rel="noreferrer noopener">mathematics</a> and its manifestation in the artistry of nature. The synergy of the <a href="https://amzn.to/4cI8nsp" target="_blank" rel="noreferrer noopener">Golden Ratio</a> and artistic expression reflects a journey towards understanding and revering the world around us through the meticulous eye of Rafael Araujo.</p><p>To celebrate Rafael Araujo’s talent and vision, I have curated a selection of his most stunning <a href="https://abakcus.com/20-beautiful-childrens-books-stunning-illustration/" target="_blank" rel="noreferrer noopener">illustrations</a>, showcasing the elegance and intricacy of his work. For those who resonate with his unique artistic expression, Araujo’s beautiful coffee table books offer a deeper dive into his masterpieces, each page a testament to the divine interplay of mathematics and nature. Engage with the elegance of the Golden Ratio and explore the divine beauty that lies within <a href="https://abakcus.com/17-best-math-youtube-channels-to-study-mathematics/" target="_blank" rel="noreferrer noopener">mathematics</a> through Araujo’s extraordinary body of work.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Invisible Electrostatic Wall at 3M plant (365 pts)]]></title>
            <link>http://amasci.com/weird/unusual/e-wall.html</link>
            <guid>42782914</guid>
            <pubDate>Tue, 21 Jan 2025 17:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://amasci.com/weird/unusual/e-wall.html">http://amasci.com/weird/unusual/e-wall.html</a>, See on <a href="https://news.ycombinator.com/item?id=42782914">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
<a href="https://www.reddit.com/r/AskReddit/comments/4jb24s/what_are_some_weird_real_life_xfiles_type/d35db1i/">echisholm</a> 5/2016
<br>
<i>
Have a relative (sort of) who works at a 3M plant. Shit still occasionally 
happens.<br>
&gt; You can't just say that and go away!! How are you not irresistibly <br>
&gt; interested in that?? I would've interrogated that relative so hard<br>
It just came up in passing at Thanksgiving! I don't really know him 
that 
well; he's an in-law. He mentioned being able to throw small washers and 
bolts at the field and watching them get repelled. People got 
interested, and so someone came with a voltmeter, and after throwing a 
couple more, they checked for voltage, and there was a residual charge 
after they finally caught on on a plastic sheet to prevent immediate 
grounding. It also had a very slight magnetic field. It's apparently 
fairly common, but engineering hasn't come up with a solid explanation 
why.</i> 
<br></p><p>
ALSO: <a href="https://www.reddit.com/r/AskReddit/comments/4jb24s/what_are_some_weird_real_life_xfiles_type/d35dxw1/">dc469</a> 5/2016<br>
<i>
I met this guy at an ESD meeting in austin once. He said the strength of 
the field maxed out his equipment at a distance so he couldn't get a 
maximum measurement.
<br>
After he published the paper he was contacted by NASA and all the three 
letter agencies asking for more info. He wanted to experiment around with 
it but no company had millions to throw into such a project (presumably, 
the government did). It had to be a pretty narrow window of temperature, 
pressure, humidity, etc. They kept the garage door open so that's where 
the insects and sparrows got sucked in (which obviously ruined the 
product).
<br>
He said it was actually known to the technicians for awhile before he 
experienced it and they just were kinda like "meh". Eventually they fixed 
the grounding issue on the machine and the problem never popped up again.
<br>
edit: found the ESD website. David Swenson apparently is still with them 
on their board of directors. http://centxesdassoc.homestead.com/
<br></i></p><div>
<p>
  Problems: coulomb forces would be expected to <i>attract</i> a person
  into the "chamber" formed by the PP film, and the attractive force
  should increase linearly across distance. There should be no "wall" in
  the center, a discrete wall is repulsive, also nonlinear.<br></p>

<p>If for some reason a
  person was repelled from the center of the chamber rather than being
  attracted, there still should be no "wall," since the repulsion force 
should exist over a large distance; it
  should act like a deep pillow which exerts more and more force as one
  moves deeper into it.  Large fuzzy fields, this is how
  magnets and iron behave, and this is how e-fields and conductive objects
  should also behave.
<br></p>

<p>

  A thought: unspooling of film typically generates a much higher net 
charge on
  the long piece of film than on the small surface of the spool.
  However, since charge is created in pairs, and net charge is conserved, 
the imbalances of charge <i>must</i> 
be
  equal and opposite.  The charge on the entire length of moving film 
<i>must</i>
  be equal in magnitude to the charge on the spool.  Yet the charge on the
  film is very large and is continuously increasing.  The limited
  surface-charge on the spool required that opposite charge is being 
<i>lost
  through some unseen path.</i>  
<br></p>

<p>
Very probably the spool is spewing out 
enormous quantities of ionized air with polarity opposite that of the 
charge on the moving plastic film.
<br></p>

<p>

  Charged air would be created by discharge in the cleft between film and 
spool as the 
film
  was peeled from the spool.  I wonder if film was being peeled from the 
top of
  the spool, so that any ionized air created in the cleft would be 
launched into the
"tent-chamber" region?  (If it was peeled from the bottom of the spool,
the charged air would end up outside the "tent.")  Or, if a corona
discharge arises in the cleft between film and spool, perhaps the UV and
e-fields of this corona can ionize the air on both sides of the exiting
plastic film, and spray the charged air everywhere. 
<br></p>

<p>

  So, if the charged "tent" of film is negative in the above situation,
  and if a large quantity of positively charged air is being generated
  by the spool, then perhaps the "invisible wall" is caused by a cloud of
  suspended air ions held in position by e-fields.  Perhaps it's a 
pressure gradient created by
  ionized air trapped under the tent by electrostatic attraction.  Yet
  again this effect would be expected to create a diffuse zone of 
increasing
  force, not a "wall", but an "invisible pillow."  Added note: concrete 
floors behave as conductors (resistors) in this situation.  Where 
megavolts at 
microamps are involved, the division between insulators and conductors is 
at 10^6/10^-6 =  1000 gigaohms.  Concrete resistivity is 
in the realm of megohms, so it behaves like a grounded metal sheet. 
<br></p>

<p>

  However, a volume of charged air is somewhat analogous to iron
  filings near a magnet.  If a solid sheet of iron filings is held in
  place by a magnet, then a literal "wall" is created, and this wall will
  resist penetration by nonferrous objects.  If in the above manufacturing
  plant, a sheet of highly charged air is for some reason being held in
  place by the fields created by the charged film, then a transparent
  "wall" made of charged air would come into being.  It might produce 
pressures on surfaces, and resist  penetration by human bodies. 
<br></p>

<p>

  My question is this: if the entire situation could be turned on its
  side, so the "invisible wall" became an "invisible floor", could a
  person *<i>stand</i>* on it?  Have we discovered the long-sought "Zero-G
  waterbed?"  :)    - B.B.
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a app that uses NFC as a physical switch to block distractions (203 pts)]]></title>
            <link>https://www.foqos.app</link>
            <guid>42782295</guid>
            <pubDate>Tue, 21 Jan 2025 16:52:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.foqos.app">https://www.foqos.app</a>, See on <a href="https://news.ycombinator.com/item?id=42782295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <div>
        <h2>
          <span>Free</span>, Private, and Open Source Forever
        </h2>
        <p>
          No subscriptions. No tracking. No data collection. Just a simple tool
          to help you stay focused.
        </p>
      </div>

      <div>
          <h2>
            Multiple profiles for different scenarios
          </h2>
          <p>
            Create custom blocking profiles for work, studying, or bedtime.
            Switch between them effortlessly by simply swiping the row your
            within the app. Press and hold to start the profile.
          </p>
        </div>

      <div>
          <h2>
            Write profiles to NFC tags
          </h2>
          <p>
            Save time by writing your profiles to NFC tags. Simply tap your
            phone on a tag to instantly activate the associated profile - no
            need to unlock your device or open the app.
          </p>
        </div>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Concept cells help your brain abstract information and build memories (123 pts)]]></title>
            <link>https://www.quantamagazine.org/concept-cells-help-your-brain-abstract-information-and-build-memories-20250121/</link>
            <guid>42781846</guid>
            <pubDate>Tue, 21 Jan 2025 16:20:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/concept-cells-help-your-brain-abstract-information-and-build-memories-20250121/">https://www.quantamagazine.org/concept-cells-help-your-brain-abstract-information-and-build-memories-20250121/</a>, See on <a href="https://news.ycombinator.com/item?id=42781846">Hacker News</a></p>
Couldn't get https://www.quantamagazine.org/concept-cells-help-your-brain-abstract-information-and-build-memories-20250121/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Should we use AI and LLMs for Christian apologetics? (2024) (157 pts)]]></title>
            <link>https://lukeplant.me.uk/blog/posts/should-we-use-llms-for-christian-apologetics/</link>
            <guid>42781293</guid>
            <pubDate>Tue, 21 Jan 2025 15:39:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lukeplant.me.uk/blog/posts/should-we-use-llms-for-christian-apologetics/">https://lukeplant.me.uk/blog/posts/should-we-use-llms-for-christian-apologetics/</a>, See on <a href="https://news.ycombinator.com/item?id=42781293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentinner">
  <article itemscope="" itemtype="http://schema.org/BlogPosting"><header>
      
      
    </header><div itemprop="articleBody">
        <p>The other day I received an email from Jake Carlson of the <a href="https://apologist.com/">Apologist Project</a> asking permission to use the apologetics resources I’ve written as input for an AI chatbot they have launched on their website.</p>
<p>I replied by email, but I think there is benefit to doing this kind of conversation more publicly. So, below are:</p>
<nav id="contents" role="doc-toc"><p><a href="#top">Contents</a></p>
<ul>
<li><p><a href="#my-first-email-response" id="toc-entry-1">My first email response</a></p></li>
<li><p><a href="#a-summary-of-the-arguments-jake-made-in-response-to-that-by-email" id="toc-entry-2">A summary of the arguments Jake made in response to that by email</a></p></li>
<li><p><a href="#my-further-response-and-comments" id="toc-entry-3">My further response and comments</a></p></li>
<li><p><a href="#updates" id="toc-entry-4">Updates</a></p></li>
</ul></nav><p>First, some terminology: LLM refers to <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Model</a>, and is the type of technology that is powering all recent “Artificial Intelligence” chat bots. A well known example is ChatGPT – I have some <a href="https://lukeplant.me.uk/blog/categories/chatgpt/">other blog posts</a> specifically about that, and many of the things about ChatGPT will apply to other LLMs.</p>
<section id="my-first-email-response"><h2><a href="#toc-entry-1" role="doc-backlink">My first email response</a></h2>
<p>My email, 2024-09-17 - as I wrote it, for better or worse. <strong>Bad language warning</strong>.</p>
<blockquote>
<p>Hi Jake,</p>
<p>Thanks for your email. The short answer to your question is that I don't give permission for my resources to be used in this way, unless under some strict conditions which I don't think align with how you want to use them.</p>
<p>This answer probably requires a reason, which is a much longer answer. Basically, I think it is a very bad idea to use AI, specifically <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>s, in the kind of way you are using them in apologist.ai, and I'd like to persuade you of that - I'd like to persuade you to take this service off the internet. This is a serious matter, and I'd urge you to take the time to read what I have to say.</p>
<p>Before I get going, you should know that I am a software developer, and I do understand and use LLMs as part of my work. I'm not just "anti-AI", and I'm well aware of their capabilities. As well as using them myself and <a href="https://lukeplant.me.uk/blog/categories/chatgpt/">blogging a bit about them</a>, I often read <a href="https://simonwillison.net/">Simon Willison's blog</a>, a software developer I've worked with in the past (as a fellow core developer of <a href="https://www.djangoproject.com/">Django</a>), and who has been active recently in this area and become well known as an independent researcher on them. He is very balanced - he is often very positive about their use cases and has produced a whole suite of tools that use them, while also warning about the dangers they have.</p>
<p>My basic rule of thumb for LLMs is that I use them only in contexts where:</p>
<ul>
<li><p>accuracy and reliability does not matter (some "creative writing" type use cases), or,</p></li>
<li><p>the nature of the task forces me to immediately verify the accuracy, and doing so is easy (such as some software development uses).</p></li>
</ul>
<p>The reason for this is simply that LLMs are not designed to be truthful - they are designed to make stuff up. This has been very well studied now. I'm sorry to have to use bad language, but the best paper I can link on the subject is <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5">ChatGPT is bullshit</a>. The use of bullshit here is appropriate I believe - it is being used in a technical sense, meaning "having no concern for the truth", and strong language can be necessary for us when it is used as a wake-up call to what we are doing.</p>
<p>To quote from the paper:</p>
<blockquote>
<p>In this paper, we argue against the view that when ChatGPT and the like produce false claims they are lying or even hallucinating, and in favour of the position that the activity they are engaged in is bullshitting, in the Frankfurtian sense (Frankfurt, <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5#ref-CR11">2002</a>, <a href="https://link.springer.com/article/10.1007/s10676-024-09775-5#ref-CR12">2005</a>). Because these programs cannot themselves be concerned with truth, and because they are designed to produce text that <em>looks</em> truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.</p>
</blockquote>
<p>Now, it is certainly the case that LLMs can and do produce truthful output. But their design in no way constrains them to do this. They are simply producing <strong>plausible</strong> human language sentences, that is how they work, and with enough input data, they may well produce more truthful output than false output. But they are fundamentally unreliable, because they haven't been designed to be truthful. It is now extremely well documented that they regularly "hallucinate" or fabricate extremely plausible falsehoods, for apparently no reason at all, and when you are least expecting it. I've also seen it happen plenty of times in my own uses of them. This is not a problem that is going away - see <a href="https://arxiv.org/abs/2409.05746">LLMs Will Always Hallucinate, and We Need to Live With This</a> - and you cannot fix this with prompt engineering.</p>
<p>With this in mind, I cannot see how an apologetics chatbot on a Christian website is a suitable use case for LLMs.</p>
<p>If I wrote a Christian apologetics article, but accidentally included false information in it, I would be very embarrassed, and rightly so - such falsehoods disgrace the name of Christ. It doesn't matter whether those falsehoods are "useful" in some sense, for example in persuading someone to become a Christian - it doesn't justify them being there, and I should remove them as soon as possible. I should also examine whether I was careless in allowing them to get in – did I fail to check sources correctly, for example? If so, I have to repent of a careless attitude towards something serious.</p>
<p>If I found the false information came from a research assistant whom I had trusted, I would either not use that person again, or ensure that they got into better practices with their methods and had a more serious attitude towards truth.</p>
<p>A serious regard for truth means not only that we remove falsehoods that are found by other people, but that we repent of the laxness that allowed them to be there in the first place.</p>
<p>Now consider the case of using an LLM to write responses to people about Christianity. How could I possibly justify that, when I know that LLMs are bullshit generators? As Simon Willison put it, <a href="https://softwaremisadventures.com/p/simon-willison-llm-weird-intern">they are like a weird, over-confident intern</a>, but one that can't actually be morally disciplined to improve.</p>
<p>To put a bullshit machine on the internet, in the name of Christ, is reckless. It's almost certain that it will make stuff up at some point. This is bad enough in itself, if we care about truth, but it will also have many negative consequences.&nbsp; For example, Muslims will spot the fabrications, even if there are only one or two, and use it to discredit your work. They will say that you are producing bullshit, and that you don't care about truthfulness, and <strong>these accusations will be 100% justified</strong>. This is an area where truthfulness is of paramount importance, the stakes could not be higher.</p>
<p>At the very least, an LLM-powered chatbot needs a huge, prominent disclaimer, like "Our chatbot technology is known to produce plausible falsehoods. Anything it says may be inaccurate or completely made up. Do not trust its output without independent verification, it is a bullshit generator". If you don't want to use the word 'bullshit', you need to put it using some other clear, plain language that people will understand, like "<a href="https://simonwillison.net/2023/Apr/7/chatgpt-lies/">it will lie to you</a>".</p>
<p>Who would want to use such a machine? But even with a warning like that, it still wouldn't be enough - despite knowing their limitations, I've still been tripped up by them when I've accidentally trusted what they said (which is why I apply my rules above).</p>
<p>Your current chatbot has no disclaimer at all. At least ChatGPT has the disclaimer "ChatGPT can make mistakes. Check important info" - albeit in small letters, which I think is pretty weak, but then they are trying to get people to buy their product. However, I don't think a disclaimer of any kind will fix the problem.</p>
<p>There are some ways that I think I could use LLMs for a user-facing application on the internet. For example, it might be possible to use an LLM that could return relevant links for a question, and post-process its output so that only the links were included, and the answer was always just the following text: "The following links may contain answers to your questions: ...". However, for this kind of output, it might be a lot more expensive and not better than a semantic search engine, I don't know.</p>
<p>As a final argument, an LLM-powered apologetics chatbot is simply unnecessary. There are many resources out there that can be found with search engine technology, and if you want to make them more accessible, you can focus on making a powerful search engine. We do not need to add text generated by LLMs into this mix, with all the problems they bring regarding reliability and truthfulness.</p>
<p>It sounds like you have already launched your chatbot. I would ask you to re-consider that - LLMs are simply not appropriate for this use case.</p>
<p>I'm very happy to answer any questions you might have.</p>
<p>With prayers,</p>
<p>Luke</p>
</blockquote>
</section><section id="a-summary-of-the-arguments-jake-made-in-response-to-that-by-email"><h2><a href="#toc-entry-2" role="doc-backlink">A summary of the arguments Jake made in response to that by email</a></h2>
<p>Jake replied to me, and I haven’t asked his permission to quote the email here, but I will attempt to summarise the substantive parts of his argument fairly:</p>
<ol>
<li><p>They are using an “open source” model, have fine-tuned it in a way they “feel” will minimise hallucinations, and augmented it with other techniques such as <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">Retrieval Augmented Generation</a>, and so they believe that hallucinations and undesirable content will be much less problematic. Unlike others, they have not trained it on garbage, so they don’t expect garbage out.</p></li>
<li><p>Human beings are at least as prone to making things up, including  Christians and would-be apologists. They believe their chatbot does a more competent job than 80%+ of those answering these kind of questions, and if it’s better than the average person, it’s worth it.</p></li>
<li><p>It is equally reckless to let human beings do the job of apologetics, if not more so, as Christians do a pretty good job of discrediting our cause with embarrassing mistakes.</p></li>
</ol>
<p>He finished with a challenge to try it for myself and see if it outputs anything “outright harmful”.</p>
<p>(I’m happy to include the full text of his email as well if he wants that).</p>
</section><section id="my-further-response-and-comments"><h2><a href="#toc-entry-3" role="doc-backlink">My further response and comments</a></h2>
<p>In response to those points above, then, I would say:</p>
<p>Firstly, I’m very sceptical of their belief that their system is that much better than others when it comes to reliability and hallucinations, as they are called.</p>
<p>For LLMs, “open source” is a bit of a misnomer. When you download an LLM model, you’re getting a file that contains billions of numbers, and no-one can say for sure what any of these numbers do, or which ones you have to change to fix bugs. Unlike <a href="https://en.wikipedia.org/wiki/Open_source">open source</a> code that can be understood and modified to change their behaviour, these systems are mostly incomprehensible.</p>
<p>In terms of the processes that creates them, all high quality LLMs to date require a vast training corpus, and vast computational resources. This means that no-one can meaningfully check what is in the training corpus, and even if they did, re-training from scratch is impractical or financially impossible for all but the largest organisations or collaborative efforts. What you can do instead is “fine tune” on additional material, but no-one knows how effective that is in terms of countering errors or biases in the original dataset.</p>
<p>Even if you have a perfect training corpus, that’s not enough. LLMs do not produce garbage only because of bad training data. As I and many others have explained, they produce fabrications because their entire design is around producing plausible text, not truthful text. If you want a explanation of how they work (long but reasonably accessible), see <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">Stephen Wolfram’s article on What ChatGPT is doing</a>. Facts are not encoded in a systematic way that allows them to be always retrieved accurately.</p>
<p>So any idea that LLMs are under our control, or we can constrain their behaviour, even if they are “open source”, is really a fantasy. It needs to be repeated: <a href="https://lukeplant.me.uk/blog/posts/should-we-use-llms-for-christian-apologetics/">they will lie to you</a>.</p>
<p>If the Apologist Project had genuinely solved the problem of hallucinations, the world would want to know. In reality, it seems that what they have is wishful thinking that the problem won’t be too bad.</p>
<p>In practice, how well does it work? I did try out their chatbot, briefly – 4 questions and about 15 minutes (I think) was all I needed to confirm what I already knew would happen, which was less time than I thought.</p>
<p>After one question on another topic, my second question was:</p>
<blockquote>
<p>What is the earliest manuscript of the Bible we have</p>
</blockquote>
<p>I got a pretty reasonable answer for this, about the Rylands Library Papyrus P52, which I did not thoroughly check but seemed good.</p>
<p>My third question was:</p>
<blockquote>
<p>Please quote "Rylands Library Papyrus P52" in Greek, give the place where it is stored, and the name of the Brazillian team who found it.</p>
</blockquote>
<p>The reference to the “Brazillian team” was a deliberate attempt to trip it up, to see if it was as easily lead as some LLMs used to be. (Specifically, I was testing for the well-known <a href="https://www.nngroup.com/articles/sycophancy-generative-ai-chatbots/">sycophancy bias</a>, which would be a very serious issue in a chatbot used in this way). But it passed this test, as it happened. I did not deliberately try to trip it again.</p>
<p>The answer was:</p>
<blockquote>
<p>The Rylands Library Papyrus P52 is a fragment of the Gospel of John, specifically John 18:31-33 and 18:37-38. Here is the text in Greek:</p>
<p>Ἰησοῦς εἶπεν αὐτῷ· Ἐγὼ βασιλεὺς εἰμι. Ἐγὼ εἰς τοῦτο γεγέννημαι καὶ εἰς τοῦτο ἐλήλυθα εἰς τὸν κόσμον, ἵνα μαρτυρήσω τῇ ἀληθείᾳ. Πᾶς ὁ ὢν ἐκ τῆς ἀληθείας ἀκούει μου τῆς φωνῆς.</p>
<p>(John 18:37, Greek text)</p>
<p>The Rylands Library Papyrus P52 is stored at the John Rylands Library in Manchester, England, UK.</p>
<p>However, I must correct you - the Rylands Library Papyrus P52 was not found by a Brazilian team. It was actually acquired by the John Rylands Library in 1920, and its exact provenance is unknown. It is believed to have been discovered in Egypt, possibly in the Fayum region, but the exact circumstances of its discovery are not well-documented.</p>
<p>References:</p>
<ul>
<li><p>Rylands Library Papyrus P52 (circa 125 AD)</p></li>
<li><p>John Rylands Library, Manchester, England, UK</p></li>
<li><p>John 18:37 (Greek text, NA28)</p></li>
<li><p>John 18:37 (ESV)</p></li>
</ul>
</blockquote>
<p>The initial answer regarding the Greek text could be regarded as a misunderstanding - in response it quoted from one of verses, rather than the text actually on the fragments, so let’s be generous there.</p>
<p>There is a bigger problem, however: the first part of the text of John 18:37 it quoted is simply wrong. The chatbot said:</p>
<blockquote>
<p>Ἰησοῦς εἶπεν αὐτῷ· Ἐγὼ βασιλεὺς εἰμι.</p>
</blockquote>
<p>Translation: Jesus said to him: I am a king.</p>
<p>However, the NA28 Greek text (which it appears to be referring to in the “References” section) <a href="https://www.die-bibel.de/en/bible/NA28/JHN.18">does not say that</a> - the equivalent part (just before ἐγὼ εἰς τοῦτο γεγέννημαι) says:</p>
<blockquote>
<p>ἀπεκρίθη ὁ Ἰησοῦς· σὺ λέγεις ὅτι βασιλεύς εἰμι.</p>
</blockquote>
<p>Translation: Jesus answered, ‘You say that I am a king.’</p>
<p>I’ve consulted an earlier <a href="https://archive.org/details/nestle-aland/page/n212/mode/1up">Nestle-Aland critical text for John 18</a> and there is no variant that reads as per the chatbot’s version, nor can I find any other reference to such a variant. It is certainly not what the NA28 text says. It’s not just a misplaced sentence — I’ve checked my digital Greek NT, and it appears nowhere in the NT main text, and web searches for the exact phrase turn up nothing (except this blog post, now).</p>
<p>So, as far as I can tell, this appears to simply be a fabrication. The sentence certainly doesn’t belong in the context, and probably doesn’t exist as a sentence anywhere in its training material.</p>
<p>This is of course no surprise at all – we know that LLMs are very prone to such things. Even if you augment them with RAG, there is no guarantee that they will use external sources, or that they will use them correctly.</p>
<p>Of course, it is doing a very good job of it – the text is very plausible, and it comes complete with references that make you trust it. Which is again, exactly what we know LLMs do.</p>
<p>Let me underline this: even allowing for it misunderstanding my question, and judging it by its attempt to quote from John 18:37 in the Greek from a good source, it has still failed to do that, and instead inserted fabrications that significantly change the meaning of the text. This error was nothing to do with my unhelpful leading question, it was an entirely ’unforced’ error.</p>
<p>For my third question, I rephrased things to get exactly what I was after:</p>
<blockquote>
<p>Which are the actual Greek words from John 18 on the papyrus?</p>
</blockquote>
<p>It’s answer:</p>
<blockquote>
<p>The Rylands Library Papyrus P52 contains a fragment of John 18:31-33 and 18:37-38. The actual Greek words from John 18 on the papyrus are:</p>
<p>John 18:31-33:</p>
<ul>
<li><p>ουν (oun) - "then" (John 18:31)</p></li>
<li><p>ειπον (eipon) - "said" (John 18:31)</p></li>
<li><p>αυτω (autō) - "to him" (John 18:31)</p></li>
<li><p>ο πιλατος (ho Pilatos) - "Pilate" (John 18:31)</p></li>
<li><p>εαν (ean) - "if" (John 18:31)</p></li>
<li><p>μη (mē) - "not" (John 18:31)</p></li>
<li><p>ημων (ēmōn) - "our" (John 18:31)</p></li>
<li><p>νομον (nomon) - "law" (John 18:31)</p></li>
<li><p>κρινοντες (krinontes) - "judging" (John 18:31)</p></li>
<li><p>αυτον (auton) - "him" (John 18:31)</p></li>
<li><p>ουν (oun) - "then" (John 18:32)</p></li>
<li><p>εξηλθεν (exēlthen) - "went out" (John 18:32)</p></li>
<li><p>παλιν (palin) - "again" (John 18:32)</p></li>
<li><p>ο πιλατος (ho Pilatos) - "Pilate" (John 18:32)</p></li>
<li><p>και (kai) - "and" (John 18:32)</p></li>
<li><p>ειπεν (eipen) - "said" (John 18:32)</p></li>
</ul>
<p>[INST]John 18:37-38:[/INST]</p>
<ul>
<li><p>ουν (oun) - "then" (John 18:37)</p></li>
<li><p>ο πιλατος (ho Pilatos) - "Pilate" (John 18:37)</p></li>
<li><p>ειπεν (eipen) - "said" (John 18:37)</p></li>
<li><p>συ (sy) - "you" (John 18:37)</p></li>
<li><p>λεγεις (legeis) - "say" (John 18:37)</p></li>
<li><p>βασιλευς (basileus) - "king" (John 18:37)</p></li>
<li><p>εγω (egō) - "I" (John 18:37)</p></li>
<li><p>ειμι (eimi) - "am" (John 18:37)</p></li>
<li><p>εις (eis) - "into" (John 18:37)</p></li>
<li><p>τουτο (touto) - "this" (John 18:37)</p></li>
<li><p>γεγονεν (gegonen) - "has come" (John 18:37)</p></li>
<li><p>ινα (hina) - "that" (John 18:37)</p></li>
<li><p>μαρτυρηση (marturēsē) - "bear witness" (John 18:37)</p></li>
<li><p>τη (tē) - "the" (John 18:38)</p></li>
<li><p>αληθεια (alētheia) - "truth" (John 18:38)</p></li>
</ul>
<p>Please note that the papyrus is fragmentary, and some words are missing or partially preserved.</p>
<p>References:</p>
<ul>
<li><p>Rylands Library Papyrus P52 (circa 125 AD)</p></li>
<li><p>John 18:31-33, 37-38 (ESV)</p></li>
</ul>
</blockquote>
<p>This response contains many false details.</p>
<p>Compare it with the <a href="https://en.wikipedia.org/wiki/Rylands_Library_Papyrus_P52">the Wikipedia article on Rylands Library Papyrus P52</a>. You can find the actual text there, with the words highlighted. You can see that the very first words on the fragment are from the second half of John 18:31:</p>
<blockquote>
<p>ΟΙ ΙΟΥΔΑΙΟΙ ΗΜΕΙΝ</p>
</blockquote>
<p>(or “οι ιουδαιοι ημειν“ in lower case, “…the Jews ‘We…’” in English).</p>
<p>But the chatbot claimed that “νομον” was on the fragment, when in fact it appears only in the earlier part of John 18:31 before the fragment starts. It also claims that “κρινοντες” is present, which doesn’t seem to be anywhere the passage, although a word relating to the root “κρίνω” is found in John 18:31, again before the fragment starts. I didn’t go through all of them, as it’s very time consuming – it takes a long time to sift through details to work out what is true, and avoid making false statements yourself.</p>
<p>You can see here that the chatbot is just making stuff up. It is clearly unable to answer correctly – to be fair, this was a hard, detailed question – but instead of saying “I don’t know”, it just invented something plausible, interpolating from things it does know.</p>
<p>Now, are these things “harmful”? Well, it’s not telling me something heretical that will take me to hell. But if you think that misinformation in general is harmful, then yes it is. If you think that fabricating parts of the NT text is harmful, yes it is. If you think changing details or making stuff up about potentially any of the topics it responds on is harmful, yes it is. If you think wasting people’s time is harmful, yes it is. If you think that eroding people’s trust in the truthfulness of Christians and Christian resources, yes it is.</p>
<p>Onto the second and third points Jake made – the comparison to human beings.</p>
<p>The first thing to say is that the argument is comparing in the wrong direction. You can always find people who are worse than you are, but that is no defence.</p>
<p>Comparing to average or even above average Christians or clergymen is still not fair, because most of those people are not putting themselves on the internet claiming to be able to answer all your questions.</p>
<p>The question is, how does a chatbot compare with the <strong>best</strong> resources on the internet? Because these are the ones you are actually competing with. Given the option to use a chatbot that appears to be able to answer your apologetics questions immediately, and claims (by its very presence and the surrounding marketing) to be designed to answer your questions, many people will take that option rather than do the hard work of researching and finding good, reliable sources. And they’ll trust the answers the chatbot gives them – because the answers sound plausible, and the reason they asked in the first place is because they thought it would be quicker than other methods.</p>
<p>We know that the chatbot can’t do <strong>better</strong> than it’s sources in terms of being factual, and we’ve seen with very little effort that it will often do much worse. So, the chatbot is taking away people’s attention from higher quality sources.</p>
<p>In addition, when it comes to comparisons to the average Christian, on one axis it is clear that the chatbot, like all similar LLM powered chatbots, is massively worse than any Christian I know. Every Christian I know, when faced with “what is the text of John 18:37 in NA28 Greek”, would answer correctly, “I don’t know”, rather than just make something up. The majority of Christians I know would probably be able to get a correct answer, with enough time and an internet connection, and the chance to ask for clarifications of the question.</p>
<p>Christians are not perfect in this regard, of course, and I completely agree that the behaviour of some Christians and would-be apologists regarding truthfulness and their willingness to blag their way out of a hard question is genuinely lamentable. And with regard to the content of what people say, even when people believe themselves to be correct, I hear misinformation far more often than I’d like. In which case, what people need is <strong>excellent teaching</strong> of two kinds – first, of a moral sort, regarding the importance of truthfulness; and secondly, factual resources that can be trusted.</p>
<p>So, an apologetics website with a chatbot that kicks out plausible misinformation is <strong>exactly the last thing we need</strong>, on both fronts. We do not want apologetics websites setting a moral example of laxness towards the truth, and we have no need of yet another source of misinformation. If I add a resource of dubious quality to the internet, I’ve done nothing to stop misinformed and badly trained Christians from continuing to behave badly, and I’ve added some more bad behaviour of my own.</p>
<p>Can we not still argue that chatbots are no worse than, and may be better than humans — and we still allow humans to evangelise? Is it not similarly reckless to ask a human being to witness to the truth? Well if it is, then we have to point the finger at God for that. While he doesn’t require us all to be apologists, he does require us to be “prepared to give an answer to everyone who asks you to give the reason for the hope that you have” (<a href="https://www.biblegateway.com/passage/?search=1%20Peter%203%3A15&amp;version=NIVUK">1 Peter 3:15</a>).</p>
<p>I have on more than one occasion doubted God’s wisdom in putting humans in charge of evangelism, rather than angels, especially when the human has been me. But that really is God’s plan. Sinners are supposed to announce the message of salvation. And sinners do have some big advantages. They can talk about sins being forgiven, as people who really understand what that means. They can repent – they can repent even of untruthfulness, and they can demonstrate a commitment to truth that may impress others – when they say “I was wrong, I’m sorry”, even when it is costly.</p>
<p>So, I will not hesitate to tell people to they should be ready to witness to others about their faith, because that command comes from God. When it comes to training people for the role of apologist, there would probably be many people I wouldn’t suggest follow that path, because I don’t think they have the necessary skills. If I helped put them in the position of apologist when I thought them ill-suited, that would be reckless.</p>
<p>When it comes to chatbots: in contrast to humans, I’m not required to train them in evangelism to any level, because God has not required that. Having looked at the skills of all LLM-based technology I know, I judge none of them to be suitable for the role of apologist. Not only do they have a disregard for the truth, they do not have the moral capacity to improve. So if I were to give any of them that role, it would be reckless.</p>
<p>There is a false comparison in the argument Jake made because we’re not responsible for everything in the world, or the actions of every other human. If God in his sovereignty has not stopped some people from doing a terrible job of evangelism, that’s his prerogative. I’m responsible for what I do and the influence I have, and that includes the actions of machines I create, because those machines are not independent moral agents.</p>
<p>We know that God cares deeply about every word we speak - <a href="https://www.biblegateway.com/passage/?search=Matt%2012%3A36&amp;version=NIVUK">Matthew 12:36</a>:</p>
<blockquote>
<p>But I tell you that everyone will have to give account on the day of judgement for every empty word they have spoken.</p>
</blockquote>
<p>Anyone who has taken this to heart will understand why the Bible also commands us to be <a href="https://www.biblegateway.com/passage/?search=James%201%3A19&amp;version=NIVUK">slow to speak</a>. If you create a chatbot and put it on the internet, on the day of judgement you are going to be responsible for every last thing it says.</p>
<p>I still hope Jake will reconsider this. Some of the closing words of his email, which I think important to quote, were these:</p>
<blockquote>
<p>But no, we will not be taking it down unless it's thoroughly and rigorously proven that it's doing more harm than good.</p>
</blockquote>
<p>The argument here regarding “doing more good than harm” is really based on the idea that the ends justify the means – it doesn’t matter if we tell a few falsehoods on the way, as long as we are “doing good”. But as Christians we believe that <a href="https://wng.org/opinions/is-owning-the-libs-a-justification-for-lying-1726567683">good aims don’t justify deceptive behaviour</a>. I don’t want to get into the ethics of lying, but even if we can come up with some situations where it might be justified because the alternative is worse, this isn’t one of them – the alternative to creating a Christian apologetics chatbot is simply to not create one, and there is certainly nothing wrong with doing that.</p>
<p>Perhaps worse than that argument is the attitude displayed in the above words. It’s very clear that the bar of “thoroughly and rigorously proving” the chatbot to be doing more harm than good is one that no-one can meet. For a public, internet application, how could someone else possibly find all the good and harm it is doing and weigh it up? And why is the burden of proof that way round?</p>
<p>What this really demonstrates is an intention to carry on no matter what – that whatever arguments or evidence he sees, nothing will make him change course. I hope that won’t be true in practice.</p>
</section><section id="updates"><h2><a href="#toc-entry-4" role="doc-backlink">Updates</a></h2>
<ul>
<li><p>2024-09-20 Various small clarifications and additions after initial publishing.</p></li>
<li><p>2024-09-23 Slightly expanded argument about moral responsibility</p></li>
</ul></section>
</div></article>




</div></div>]]></description>
        </item>
    </channel>
</rss>