<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 20 Jul 2023 21:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[IRS moves forward with a new free-file tax return system (254 pts)]]></title>
            <link>https://www.pbs.org/newshour/politics/irs-moves-forward-with-a-new-free-file-tax-return-system-that-has-both-supporters-and-critics-mobilizing</link>
            <guid>36804710</guid>
            <pubDate>Thu, 20 Jul 2023 18:35:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pbs.org/newshour/politics/irs-moves-forward-with-a-new-free-file-tax-return-system-that-has-both-supporters-and-critics-mobilizing">https://www.pbs.org/newshour/politics/irs-moves-forward-with-a-new-free-file-tax-return-system-that-has-both-supporters-and-critics-mobilizing</a>, See on <a href="https://news.ycombinator.com/item?id=36804710">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="articleBody">
                <div>
                    <p>WASHINGTON (AP) — An IRS plan to&nbsp;test drive a new electronic free-file tax return system&nbsp;next year has got supporters and critics of the idea mobilizing to sway the public and Congress over whether the government should set up a permanent program to help people file their taxes without needing to pay somebody else to figure out what they owe.</p>
<p>On one side, civil society groups this week launched a coalition to promote the move toward a government-run free-file program. On the other, tax preparation firms like Intuit — the <a href="https://www.pbs.org/newshour/economy/ftc-sues-intuit-to-stop-turbotax-ads-for-free-filings">parent company of TurboTax</a> — and H&amp;R Block have been pouring millions into trying to stop the idea cold.</p>
<p>The advocacy groups are exponentially out-monied.</p>
<p>An April AP analysis found that overall, Intuit, H&amp;R Block, and other private companies and advocacy groups for large tax preparation businesses, as well as proponents in favor of electronic free file, have reported spending $39.3 million since 2006 to lobby on “free-file” and other matters. Federal law doesn’t require domestic lobbyists to itemize expenses by specific issue, so the sums are not limited to free-file.</p>
<p>Intuit spent at least $25.6 million since 2006 on lobbying, H&amp;R Block about $9.6 million and the conservative Americans for Tax Reform roughly $3 million.</p>
<p>In contrast, the NAACP has spent $140,000 lobbying on “free-file” since 2006 and Public Citizen has spent $110,000 in the same time frame.</p>
<p>“What we have on our side is public opinion,” said Igor Volsky, executive director of the liberal Groundwork Action advocacy group.</p>
<p><a href="https://www.pbs.org/newshour/politics/irs-reduces-tax-return-backlog-by-80-percent-and-improves-customer-service"><strong>READ MORE:</strong> IRS reduces tax return backlog by 80 percent and improves customer service, report says</a></p>
<p>Volsky’s organization and leaders from Public Citizen, the Center for the Study of Social Policy, Code for America, the Economic Security Project and others launched the “Coalition for Free and Fair Filing” on Wednesday. The group’s mission is to “ensure all U.S. taxpayers can easily file tax returns and get the tax credits they deserve by safeguarding and expanding” the new IRS program.</p>
<p>“The overwhelming majority of people demand a free-file option,” Volsky said. “Now the question for us is how do you channel that into effective political pressure.”</p>
<p>The IRS in May&nbsp;<a href="https://apnews.com/article/tax-irs-taxpayers-direct-file-ef2e9f92ad45984487fd368b851773af">released a report</a>&nbsp;that said most taxpayers are interested in filing their taxes directly to the IRS for free, and concurrently announced plans to launch the pilot program for the 2024 filing season. The goal is to test a direct file system that will help the IRS decide whether to move forward with a more permanent program.</p>
<p>That idea has faced the immediate threat of budget cuts from congressional Republicans.</p>
<p>Republicans on the House Appropriations Committee in June proposed a budget rider that would&nbsp;<a href="https://appropriations.house.gov/sites/republicans.appropriations.house.gov/files/documents/FY24%20Financial%20Services%20and%20General%20Government%20-%20Full%20Committee%20Mark.pdf">prohibit funds</a>&nbsp;to be used for the IRS to create a government-run tax preparation software, unless approved by a group of House and Senate committees.</p>
<p>The move “safeguards the IRS from an obvious conflict of interest where the tax collector becomes the tax preparer,” the bill’s summary states.</p>
<p>A&nbsp;Government Accountability Report in April 2022 found that 70 percent of taxpayers were eligible to use an existing free-file program but just 3 percent actually used the service. That program consists of a public-private partnership of tax software companies that offers free services to certain taxpayers outside of the IRS website.</p>
<p>Additionally, anyone can prepare and mail in their taxes for free, but the tax code is so complex that almost 50 percent of Americans use a tax prep company. IRS officials have estimated individual taxpayers pay an average of $140 preparing their tax returns each year.</p>
<p>Derrick Plummer, a spokesman for Intuit, stressed the free options that already were available.</p>
<p>“An IRS direct-to-e-file system is redundant and will not be free – not free to build, not free to operate, and not free for taxpayers,” Plummer said, adding that it “will unnecessarily cost taxpayers billions of dollars.”</p>
<p>H&amp;R Block said in a statement the direct e-file pilot “continues to be a solution in search of a problem.”</p>
<p>Citing the free-filing options for Americans under a certain income threshold through the existing&nbsp;<a href="https://www.irs.gov/e-file-providers/about-the-free-file-alliance">Free File Alliance</a>, H&amp;R Block said, “this pilot is unnecessary and faces significant barriers to providing comprehensive tax preparation services.”</p>
<p>H&amp;R Block came under fire after congressional Democrats last week released a report stating that it was one of three large tax preparation firms that sent “extraordinarily sensitive” information on tens of millions of taxpayers to&nbsp;Facebook parent company Meta&nbsp;and Google over the course of at least two years.</p>
<p>Susan Harley, Congress Watch managing director at Public Citizen, said “we’re outgunned as far as money being spent, but we have the moral higher ground” in supporting the free-file program over third-party tax preparers.</p>
<p>Nations like Germany, Japan, the U.K. and other Organization for Economic Cooperation and Development countries already offer their taxpayers some form of pre-populated tax document.</p>
<p>Some countries also use “tax agency reconciliation,” where taxpayers who opt to participate provide the government with basic employment status information and the tax administrator sends them a return with their calculated tax liability.</p>
<p><a href="https://www.nber.org/papers/w30008">Research conducted</a> last year by a group of Treasury, Federal Reserve and other academics shows that the IRS could pre-populate 42 to 48 percent of all tax returns.</p>
<p>The IRS has already seen cuts to its funding since the passage of the&nbsp;Inflation Reduction Act&nbsp;that&nbsp;President Joe Biden signed&nbsp;last August gave the agency $80 billion to modernize and hire more workers and move toward the free-file program.</p>
<p>House Republicans built a $1.4 billion reduction to the IRS into the debt ceiling and budget cuts package passed by Congress this summer. The White House said the debt deal also has a separate agreement to take $20 billion from the IRS over the next two years and divert that money to other non-defense programs.</p>

                </div>
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In 1961 a Gallup poll showed only 33% of Americans in favor of moon landing (140 pts)]]></title>
            <link>https://newsletter.pessimistsarchive.org/p/the-moon-landing-was-opposed-by-majority</link>
            <guid>36804394</guid>
            <pubDate>Thu, 20 Jul 2023 18:12:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.pessimistsarchive.org/p/the-moon-landing-was-opposed-by-majority">https://newsletter.pessimistsarchive.org/p/the-moon-landing-was-opposed-by-majority</a>, See on <a href="https://news.ycombinator.com/item?id=36804394">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><strong>Today marks the 54th anniversary of the moon landing.</strong></em><span> In retrospect it has almost unanimous support and adoration in the US. </span><em>However</em><span>, before that giant leap for mankind actually took place - less than one third of Americans were in favor of a moon landing: in 1961 a Gallup poll showed only </span><a href="https://www.newspapers.com/article/arizona-republic/128550438/" rel="">33%</a><span> of Americans in favor.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg" width="956" height="701.8983516483516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg&quot;,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:1069,&quot;width&quot;:1456,&quot;resizeWidth&quot;:956,&quot;bytes&quot;:1419223,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42503f30-47ac-4af7-9a27-da9a1913fc9d_7932x5826.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>George Gallup would publish the findings, in an article syndicated in newspapers across the US.</figcaption></figure></div><p><span>This isn’t something you often </span><a href="https://www.theatlantic.com/technology/archive/2012/09/moondoggle-the-forgotten-opposition-to-the-apollo-program/262254/" rel="">hear about</a><span> regarding the history of the moon mission. It is </span><a href="https://www.theatlantic.com/technology/archive/2012/09/moondoggle-the-forgotten-opposition-to-the-apollo-program/262254/" rel="">conveniently ignored</a><span> in the popular mind, in lieu of a story of collective triumph against the USSR, a narrative that pleases the left as an example of the power of public funding and the right, as a triumph of the </span><em>capitalist west</em><span> against the </span><em>communist east</em><span>. The retroactive support now is as bi-partisan as the opposition was then.</span></p><p>Noted fiscal hawk Barry Goldwater dismissed the lofty ambitions of lunar exploration as a "wasteful endeavor," an ironic stance given he voiced his criticism at a glitzy dinner that cost each attendee a cool $100 - close to $1000 in 2023. In the very same year, he decried the United States as "moon struck." President Kennedy’s early suggestion of teaming up with Russia to reach the moon drew opposition from Republicans too.  </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg" width="1026" height="617.2912087912088" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg&quot;,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:876,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1026,&quot;bytes&quot;:484490,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0a04b25-35e0-4d21-9147-5c8b47ad0ebd_4772x2870.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Ex-president Dwight D. Eisenhower - the very man responsible for birthing and funding NASA - expressed his own reservations, dismissing Kennedy's lunar ambition as </span><em>"almost hysterical"</em><span> saying </span><em>“Anybody who would spend $40 billion in a race to the moon for national prestige is nuts…” </em><span>Other Republicans labelled it a Moondoggle - a term coined in 1961 by AI pioneer (and </span><a href="https://newsletter.pessimistsarchive.org/p/the-original-ai-doomer-dr-norbert" rel="">original AI doomer</a><span>) Norbert Wiener - who held </span><a href="https://links.org.au/socialism-norbert-wiener#:~:text=Wiener%20was%20also%20a%20highly,which%20he%20linked%20to%20capitalism." rel="">noted</a><span> left wing views. More left wing opposition revolved around the desire for more spending on the needs of citizens.</span></p><p><a href="https://site.nhd.org/73136591/uploaded/Public_Opinion_Polls_and_Perceptions_of.pdf" rel="">Aggregations</a><span> of opinion polls in the 1960s and 70s have shown approval of the moon landing was consistently lower than disapproval. Even astronomers polled, were majority against the mission. Only in the weeks before the moon mission was approval </span><a href="https://www.newspapers.com/article/the-bangor-daily-news/128556617/" rel="">recorded</a><span> at 51% in one Harris poll.</span></p><p><span>In the aftermath of the moon landing, approval for that specific mission didn’t </span><a href="https://www.theatlantic.com/technology/archive/2012/09/moondoggle-the-forgotten-opposition-to-the-apollo-program/262254/" rel="">meaningfully budge</a><span>. 47% said it was worth it a decade later, in 1979 and it would take 20 years for amnesia to set it and this number to reach 77% in 1989. Meanwhile opposition to further moon missions remained higher than support for one until at least the mid-1990s. The US hasn’t been back to the moon since 1972.</span></p><p>Lack of ambition by NASA was one of the reasons that prompted some of America’s wealthiest to move space exploration beyond the realms of nation states, with numerous private individuals exploring the star with some - like Jeff Bezos and Elon Musk securing NASA contracts for among other things, a new moon landing.</p><p><span>This time around, fiscal conservatives are less opposed, since private efficiencies  like re-usable rockets have lowered the tax burden of space exploration. Left wing critics like Bernie Sanders </span><a href="https://www.theguardian.com/commentisfree/2022/apr/22/jeff-bezos-space-elon-musk-billionaires-bernie-sanders" rel="">complain</a><span> - argue in outlets like The Guardian - that those private dollars invested in space ought to have been taxed away to fund public programs like Apollo - ignoring the fact those kind of programs have always been politically unpopular. Sanders would have likely made the </span><a href="https://www.thedailybeast.com/bernie-sanders-would-have-voted-against-the-moon-landing" rel="">same arguments</a><span> about Apollo had he been a Senator in the 1960s.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png" width="496" height="585.3619834710744" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1428,&quot;width&quot;:1210,&quot;resizeWidth&quot;:496,&quot;bytes&quot;:1502806,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cc99d32-898c-404a-8897-6ce2c750ba16_1210x1428.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Ironically in July 1969, that same out let in which Sanders wrote - </span><em>The Guardian -</em><span> </span><a href="https://twitter.com/PessimistsArc/status/1417468837151264772" rel="">called</a><span> a teachers’ union officer a </span><em>“cynic”</em><span> for calling America’s moonshot </span><em>“A trivial prestige exercise”</em><span> that </span><em>“ignored the social conditions existing in the world”,</em><span> reporting that when man first set foot on the moon, he had not even been watching it on TV.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://newsletter.pessimistsarchive.org/?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share Pessimists Archive Newsletter&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://newsletter.pessimistsarchive.org/?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share Pessimists Archive Newsletter</span></a></p><p><span>We recommend </span><em><a href="http://newart.press/" rel="">Newart.Press</a><span>, </span></em><span>a publication exploring the past of creativity and technology, to help us understand the future.</span></p><p><em>Recommended reads:</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif" width="272" height="56.41481481481482" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:56,&quot;width&quot;:270,&quot;resizeWidth&quot;:272,&quot;bytes&quot;:1081645,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F929ec824-cdc2-46d0-8e1b-67e6b1fe0d77_270x56.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div data-attrs="{&quot;url&quot;:&quot;e_trim:10:transparent/h_56,c_limit,f_auto,q_auto:good,fl_progressive:steep/https://substack-post-media.s3.amazonaws.com/public/images/effe6ef7-3afa-4fd2-924a-d4c83951ae0a_757x157.gif&quot;}" data-component-name="AssetErrorToDOM"><picture><img src="https://newsletter.pessimistsarchive.org/img/missing-image.png" height="455" width="728"></picture></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running Stable Diffusion in 260MB of RAM (136 pts)]]></title>
            <link>https://github.com/vitoplantamura/OnnxStream</link>
            <guid>36803408</guid>
            <pubDate>Thu, 20 Jul 2023 17:01:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vitoplantamura/OnnxStream">https://github.com/vitoplantamura/OnnxStream</a>, See on <a href="https://news.ycombinator.com/item?id=36803408">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">OnnxStream</h2>
<p dir="auto">The challenge is to run <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a>, which includes a large transformer model with almost 1 billion parameters, on a <a href="https://www.raspberrypi.com/products/raspberry-pi-zero-2-w/" rel="nofollow">Raspberry Pi Zero 2</a>, which is a microcomputer with 512MB of RAM, without adding more swap space and without offloading intermediate results on disk. The recommended minimum RAM/VRAM for Stable Diffusion is typically 8GB.</p>
<p dir="auto">Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream.</p>
<p dir="auto">OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from <code>WeightsProvider</code>. A <code>WeightsProvider</code> specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom <code>WeightsProvider</code> can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word "Stream" in "OnnxStream"). Two default <code>WeightsProviders</code> are available: <code>DiskNoCache</code> and <code>DiskPrefetch</code>.</p>
<p dir="auto"><strong>OnnxStream can consume even 55x less memory than OnnxRuntime while being only 0.5-2x slower</strong> (on CPU, see the Performance section below).</p>
<h2 tabindex="-1" dir="auto">Stable Diffusion</h2>
<p dir="auto">These images were generated by the Stable Diffusion example implementation included in this repo, using OnnxStream, at different precisions of the VAE decoder. The VAE decoder is the only model of Stable Diffusion that could not fit into the RAM of the Raspberry Pi Zero 2 in single or half precision. This is caused by the presence of residual connections and very big tensors and convolutions in the model. The only solution was static quantization (8 bit). The third image was generated by my RPI Zero 2 in about 3 hours. The first image was generated on my PC using the same latents generated by the RPI Zero 2, for comparison:</p>
<p dir="auto">VAE decoder in W16A16 precision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png" alt="W16A16 VAE Decoder"></a></p>
<p dir="auto">VAE decoder in W8A32 precision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png" alt="W8A32 VAE Decoder"></a></p>
<p dir="auto">VAE decoder in W8A8 precision (generated by my RPI Zero 2 in about 3 hours):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png" alt="W8A8 VAE Decoder"></a></p>
<h2 tabindex="-1" dir="auto">Features of OnnxStream</h2>
<ul dir="auto">
<li>Inference engine decoupled from the <code>WeightsProvider</code></li>
<li><code>WeightsProvider</code> can be <code>DiskNoCache</code>, <code>DiskPrefetch</code> or custom</li>
<li>Attention slicing</li>
<li>Dynamic quantization (8 bit unsigned, asymmetric, percentile)</li>
<li>Static quantization (W8A8 unsigned, asymmetric, percentile)</li>
<li>Easy calibration of a quantized model</li>
<li>FP16 support (with or without FP16 arithmetic)</li>
<li>24 ONNX operators implemented (the most common)</li>
<li>Operations executed sequentially but all operators are multithreaded</li>
<li>Single implementation file + header file</li>
<li>XNNPACK calls wrapped in the <code>XnnPack</code> class (for future replacement)</li>
</ul>
<p dir="auto">OnnxStream depends on <a href="https://github.com/google/XNNPACK">XNNPACK</a> for some (accelerated) primitives: MatMul, Convolution, element-wise Add/Sub/Mul/Div, Sigmoid and Softmax.</p>
<h2 tabindex="-1" dir="auto">Performance</h2>
<p dir="auto">Stable Diffusion consists of three models: <strong>a text encoder</strong> (672 operations and 123 million parameters), the <strong>UNET model</strong> (2050 operations and 854 million parameters) and the <strong>VAE decoder</strong> (276 operations and 49 million parameters). Assuming that the batch size is equal to 1, a full image generation with 10 steps, which yields good results (with the Euler Ancestral scheduler), requires 2 runs of the text encoder, 20 (i.e. 2*10) runs of the UNET model and 1 run of the VAE decoder.</p>
<p dir="auto">This table shows the various inference times of the three models of Stable Diffusion, together with the memory consumption (i.e. the <code>Peak Working Set Size</code> in Windows or the <code>Maximum Resident Set Size</code> in Linux).</p>
<table>
<thead>
<tr>
<th>Model / Library</th>
<th>1st run</th>
<th>2nd run</th>
<th>3rd run</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 UNET / OnnxStream</td>
<td>0.133 GB - 18.2 secs</td>
<td>0.133 GB - 18.7 secs</td>
<td>0.133 GB - 19.8 secs</td>
</tr>
<tr>
<td>FP16 UNET / OnnxRuntime</td>
<td>5.085 GB - 12.8 secs</td>
<td>7.353 GB - 7.28 secs</td>
<td>7.353 GB - 7.96 secs</td>
</tr>
<tr>
<td>FP32 Text Enc / OnnxStream</td>
<td>0.147 GB - 1.26 secs</td>
<td>0.147 GB - 1.19 secs</td>
<td>0.147 GB - 1.19 secs</td>
</tr>
<tr>
<td>FP32 Text Enc / OnnxRuntime</td>
<td>0.641 GB - 1.02 secs</td>
<td>0.641 GB - 0.06 secs</td>
<td>0.641 GB - 0.07 secs</td>
</tr>
<tr>
<td>FP32 VAE Dec / OnnxStream</td>
<td>1.004 GB - 20.9 secs</td>
<td>1.004 GB - 20.6 secs</td>
<td>1.004 GB - 21.2 secs</td>
</tr>
<tr>
<td>FP32 VAE Dec / OnnxRuntime</td>
<td>1.330 GB - 11.2 secs</td>
<td>2.026 GB - 10.1 secs</td>
<td>2.026 GB - 11.1 secs</td>
</tr>
</tbody>
</table>
<p dir="auto">In the case of the UNET model (when run in FP16 precision, with FP16 arithmetic enabled in OnnxStream), OnnxStream can consume even 55x less memory than OnnxRuntime while being 0.5-2x slower.</p>
<p dir="auto">Notes:</p>
<ul dir="auto">
<li>The first run for OnnxRuntime is a warm up inference, since its <code>InferenceSession</code> is created before the first run and reused for all the subsequent runs. No such thing as a warm up exists for OnnxStream since it is purely eager by design (however subsequent runs can benefit from the caching of the weights files by the OS).</li>
<li>At the moment OnnxStream doesn't support inputs with a batch size != 1, unlike OnnxRuntime, which can greatly speed up the whole diffusion process using a batch size = 2 when running the UNET model.</li>
<li>In my tests, changing OnnxRuntime's <code>SessionOptions</code> (like <code>EnableCpuMemArena</code> and <code>ExecutionMode</code>) produces no significant difference in the results.</li>
<li>Performance of OnnxRuntime is very similar to that of NCNN (the other framework I evaluated), both in terms of memory consumption and inference time. I'll include NCNN benchmarks in the future, if useful.</li>
<li>Tests were run on my development machine: Windows Server 2019, 16GB RAM, 8750H cpu (AVX2), 970 EVO Plus SSD, 8 virtual cores on VMWare.</li>
</ul>
<h2 tabindex="-1" dir="auto">Attention Slicing and Quantization</h2>
<p dir="auto">The use of "attention slicing" when running the UNET model and the use of W8A8 quantization for the VAE decoder were crucial in reducing memory consumption to a level that allowed execution on a RPI Zero 2.</p>
<p dir="auto">While there is a lot of information on the internet about quantizing neural networks, little can be found about "attention slicing". The idea is simple: the goal is to avoid materializing the full <code>Q @ K^T</code> matrix when calculating the scaled dot-product attention of the various multi-head attentions in the UNET model. With an attention head count of 8 in the UNET model, <code>Q</code> has a shape of (8,4096,40), while <code>K^T</code> has a shape of (8,40,4096): so the result of the first MatMul has a final shape of (8,4096,4096), which is a 512MB tensor (in FP32 precision):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png" alt="Attention Slicing"></a></p>
<p dir="auto">The solution is to split <code>Q</code> vertically and then to proceed with the attention operations normally on each chunk of <code>Q</code>. <code>Q_sliced</code> has a shape of (1,x,40), where x is 4096 (in this case) divided by <code>onnxstream::Model::m_attention_fused_ops_parts</code> (which has a default value of 2, but can be customized). This simple trick allows to lower the overall consumed memory of the UNET model from 1.1GB to 300MB (when the model is run in FP32 precision). A possible alternative, certainly more efficient, would be to use FlashAttention, however FlashAttention would require writing a custom kernel for each supported architecture (AVX, NEON etc), bypassing XnnPack in our case.</p>
<h2 tabindex="-1" dir="auto">How OnnxStream works</h2>
<p dir="auto">This code can run a model defined in the <code>path_to_model_folder/model.txt</code>: (all the model operations are defined in the <code>model.txt</code> text file; OnnxStream expects to find all the weights files in that same folder, as a series of <code>.bin</code> files)</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;onnxstream.h&quot;

using namespace onnxstream;

int main()
{
    Model model;

    //
    // Optional parameters that can be set on the Model object:
    //
    // model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)
    // model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)
    // model.write_range_data( ... ); // writes a range data file (useful after calibration)
    // model.m_range_data_calibrate = true; // calibrates the model
    // model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)
    // model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference
    // model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)
    // model.m_fuse_ops_in_attention = true; // enables attention slicing
    // model.m_attention_fused_ops_parts = ... ; // see the &quot;Attention Slicing&quot; section above
    //

    model.read_file(&quot;path_to_model_folder/model.txt&quot;);

    tensor_vector<float> data;
    
    ... // fill the tensor_vector with the tensor data. &quot;tensor_vector&quot; is just an alias to a std::vector with a custom allocator.

    Tensor t;
    t.m_name = &quot;input&quot;;
    t.m_shape = { 1, 4, 64, 64 };
    t.set_vector(std::move(data));
    model.push_tensor(std::move(t));

    model.run();
    
    auto&amp; result = model.m_data[0].get_vector<float>();
    
    ... // process the result: &quot;result&quot; is a reference to the first result of the inference (a tensor_vector<float> as well).

    return 0;
}"><pre>#<span>include</span> <span><span>"</span>onnxstream.h<span>"</span></span>

<span>using</span> <span>namespace</span> <span>onnxstream</span><span>;</span>

<span>int</span> <span>main</span>()
{
    Model model;

    <span><span>//</span></span>
    <span><span>//</span> Optional parameters that can be set on the Model object:</span>
    <span><span>//</span></span>
    <span><span>//</span> model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)</span>
    <span><span>//</span> model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)</span>
    <span><span>//</span> model.write_range_data( ... ); // writes a range data file (useful after calibration)</span>
    <span><span>//</span> model.m_range_data_calibrate = true; // calibrates the model</span>
    <span><span>//</span> model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)</span>
    <span><span>//</span> model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference</span>
    <span><span>//</span> model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)</span>
    <span><span>//</span> model.m_fuse_ops_in_attention = true; // enables attention slicing</span>
    <span><span>//</span> model.m_attention_fused_ops_parts = ... ; // see the "Attention Slicing" section above</span>
    <span><span>//</span></span>

    model.<span>read_file</span>(<span><span>"</span>path_to_model_folder/model.txt<span>"</span></span>);

    tensor_vector&lt;<span>float</span>&gt; data;
    
    ... <span><span>//</span> fill the tensor_vector with the tensor data. "tensor_vector" is just an alias to a std::vector with a custom allocator.</span>

    Tensor t;
    t.<span>m_name</span> = <span><span>"</span>input<span>"</span></span>;
    t.<span>m_shape</span> = { <span>1</span>, <span>4</span>, <span>64</span>, <span>64</span> };
    t.<span>set_vector</span>(<span>std::move</span>(data));
    model.<span>push_tensor</span>(<span>std::move</span>(t));

    model.<span>run</span>();
    
    <span>auto</span>&amp; result = model.<span>m_data</span>[<span>0</span>].<span>get_vector</span>&lt;<span>float</span>&gt;();
    
    ... <span><span>//</span> process the result: "result" is a reference to the first result of the inference (a tensor_vector&lt;float&gt; as well).</span>

    <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto">The <code>model.txt</code> file contains all the model operations in ASCII format, as exported from the original ONNX file. Each line corresponds to an operation: for example this line represents a convolution in a quantized model:</p>
<div data-snippet-clipboard-copy-content="Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1"><pre><code>Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1
</code></pre></div>
<p dir="auto">In order to export the <code>model.txt</code> file and its weights (as a series of <code>.bin</code> files) from an ONNX file for use in OnnxStream, a notebook (with a single cell) is provided (<code>onnx2txt.ipynb</code>).</p>
<p dir="auto">Some things must be considered when exporting a Pytorch <code>nn.Module</code> (in our case) to ONNX for use in OnnxStream:</p>
<ol dir="auto">
<li>When calling <code>torch.onnx.export</code>, <code>dynamic_axes</code> should be left empty, since OnnxStream doesn't support inputs with a dynamic shape.</li>
<li>It is strongly recommended to run the excellent <a href="https://github.com/daquexian/onnx-simplifier">ONNX Simplifier</a> on the exported ONNX file before its conversion to a <code>model.txt</code> file.</li>
</ol>
<h2 tabindex="-1" dir="auto">How to Build the Stable Diffusion example on Linux/Mac/Windows</h2>
<ul dir="auto">
<li><strong>Windows only</strong>: start the following command prompt: <code>Visual Studio Tools</code> &gt; <code>x64 Native Tools Command Prompt</code>.</li>
<li><strong>Mac only</strong>: make sure to install cmake: <code>brew install cmake</code>.</li>
</ul>
<p dir="auto">First you need to build <a href="https://github.com/google/XNNPACK">XNNPACK</a>.</p>
<p dir="auto">Since the function prototypes of XnnPack can change at any time, I've included a <code>git checkout</code> ​​that ensures correct compilation of OnnxStream with a compatible version of XnnPack at the time of writing:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/google/XNNPACK.git
cd XNNPACK
git rev-list -n 1 --before=&quot;2023-06-27 00:00&quot; master
git checkout <COMMIT_ID_FROM_THE_PREVIOUS_COMMAND>
mkdir build
cd build
cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..
cmake --build . --config Release"><pre><code>git clone https://github.com/google/XNNPACK.git
cd XNNPACK
git rev-list -n 1 --before="2023-06-27 00:00" master
git checkout &lt;COMMIT_ID_FROM_THE_PREVIOUS_COMMAND&gt;
mkdir build
cd build
cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..
cmake --build . --config Release
</code></pre></div>
<p dir="auto">Then you can build the Stable Diffusion example.</p>
<p dir="auto"><code>&lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&gt;</code> is for example <code>/home/vito/Desktop/XNNPACK</code> or <code>C:\Projects\SD\XNNPACK</code> (on Windows):</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/vitoplantamura/OnnxStream.git
cd OnnxStream
cd src
mkdir build
cd build
cmake -DXNNPACK_DIR=<DIRECTORY_WHERE_XNNPACK_WAS_CLONED> ..
cmake --build . --config Release"><pre><code>git clone https://github.com/vitoplantamura/OnnxStream.git
cd OnnxStream
cd src
mkdir build
cd build
cmake -DXNNPACK_DIR=&lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&gt; ..
cmake --build . --config Release
</code></pre></div>
<p dir="auto">Now you can run the Stable Diffusion example. The weights for the example can be downloaded from the Releases of this repo. These are the command line options of the Stable Diffusion example:</p>
<div data-snippet-clipboard-copy-content="--models-path       Sets the folder containing the Stable Diffusion models.
--ops-printf        During inference, writes the current operation to stdout.
--output            Sets the output PNG file.
--decode-latents    Skips the diffusion, and decodes the specified latents file.
--prompt            Sets the positive prompt.
--neg-prompt        Sets the negative prompt.
--steps             Sets the number of diffusion steps.
--save-latents      After the diffusion, saves the latents in the specified file.
--decoder-calibrate Calibrates the quantized version of the VAE decoder.
--decoder-fp16      During inference, uses the FP16 version of the VAE decoder.
--rpi               Configures the models to run on a Raspberry Pi Zero 2."><pre><code>--models-path       Sets the folder containing the Stable Diffusion models.
--ops-printf        During inference, writes the current operation to stdout.
--output            Sets the output PNG file.
--decode-latents    Skips the diffusion, and decodes the specified latents file.
--prompt            Sets the positive prompt.
--neg-prompt        Sets the negative prompt.
--steps             Sets the number of diffusion steps.
--save-latents      After the diffusion, saves the latents in the specified file.
--decoder-calibrate Calibrates the quantized version of the VAE decoder.
--decoder-fp16      During inference, uses the FP16 version of the VAE decoder.
--rpi               Configures the models to run on a Raspberry Pi Zero 2.
</code></pre></div>
<h2 tabindex="-1" dir="auto">Credits</h2>
<ul dir="auto">
<li>The Stable Diffusion implementation in <code>sd.cpp</code> is based on <a href="https://github.com/fengwang/Stable-Diffusion-NCNN">this project</a>, which in turn is based on <a href="https://github.com/EdVince/Stable-Diffusion-NCNN">this project</a> by @EdVince. The original code was modified in order to use OnnxStream instead of NCNN.</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TypeChat (141 pts)]]></title>
            <link>https://microsoft.github.io/TypeChat/blog/introducing-typechat/</link>
            <guid>36803124</guid>
            <pubDate>Thu, 20 Jul 2023 16:41:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://microsoft.github.io/TypeChat/blog/introducing-typechat/">https://microsoft.github.io/TypeChat/blog/introducing-typechat/</a>, See on <a href="https://news.ycombinator.com/item?id=36803124">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<p><em>July 20, 2023 by Anders Hejlsberg, Steve Lucco, Daniel Rosenwasser, Pierce Boggan, Umesh Madan, Mike Hopcroft, and Gayathri Chandrasekaran</em></p>
<p>In the last few months, we've seen a rush of excitement around the newest wave of large language models.
While chat assistants have been the most direct application, there's a big question around how to best integrate these models into existing app interfaces.</p>
<p>In other words, how do we <em>augment</em> traditional UI with natural language interfaces?
How do we use AI to take a user request and turn it into something our apps can operate on?
And how do we make sure our apps are safe, and doing work that developers and users alike can trust?</p>
<p>Today we're releasing <strong>TypeChat</strong>, an experimental library that aims to answer these questions.
It uses the type definitions in your codebase to retrieve structured AI responses that are type-safe.</p>
<p>You can get up and running with TypeChat today by running</p>
<pre tabindex="0"><code><span><span>npm install typechat</span></span>
<span><span></span></span></code></pre>
<p>and hooking it up with any language model to work with your app.</p>
<p>But let's first quickly explore why TypeChat exists.</p>
<h2>Pampering and Parsing</h2>
<p>The current wave of LLMs default to conversational <em>natural</em> language — languages that humans communicate in like English.
Parsing natural language is an extremely difficult task, no matter how much you pamper a prompt with rules like "respond in the form a bulleted list".
Natural language might have structure, but it's hard for typical software to reconstruct it from raw text.</p>
<p>Surprisingly, we can ask LLMs to respond in the form of JSON, and they generally respond with something sensible!</p>
<blockquote>
<p><strong>User:</strong></p>
<p>Translate the following request into JSON.</p>
<blockquote>
<p>Could I get a blueberry muffin and a grande latte?</p>
</blockquote>
<p>Respond only in JSON like the following:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>    </span><span>"items"</span><span>: [</span></span>
<span><span>        { </span><span>"name"</span><span>: </span><span>"croissant"</span><span>, </span><span>"quantity"</span><span>: </span><span>2</span><span>  },</span></span>
<span><span>        { </span><span>"name"</span><span>: </span><span>"latte"</span><span>, </span><span>"quantity"</span><span>: </span><span>1</span><span>, </span><span>"size"</span><span>: </span><span>"tall"</span><span> }</span></span>
<span><span>    ]</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p><strong>ChatBot:</strong></p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>    </span><span>"items"</span><span>: [</span></span>
<span><span>        {</span></span>
<span><span>            </span><span>"name"</span><span>: </span><span>"blueberry muffin"</span><span>,</span></span>
<span><span>            </span><span>"quantity"</span><span>: </span><span>1</span></span>
<span><span>        },</span></span>
<span><span>        {</span></span>
<span><span>            </span><span>"name"</span><span>: </span><span>"latte"</span><span>,</span></span>
<span><span>            </span><span>"quantity"</span><span>: </span><span>1</span><span>,</span></span>
<span><span>            </span><span>"size"</span><span>: </span><span>"grande"</span></span>
<span><span>        }</span></span>
<span><span>    ]</span></span>
<span><span>}</span></span>
<span></span></code></pre>
</blockquote>
<p>This is good — though this example shows the best-case response.
While examples can help guide structure, they don't define what an AI should return extensively, and they don't provide anything we can validate against.</p>
<h2>Just Add Types!</h2>
<p>Luckily <strong>types</strong> do precisely that.
What we've found is that because LLMs have seen so many type definitions in the wild, types also act as a great guide for how an AI should respond.
Because we're typically working with JSON — <em>JavaScript</em> Object Notation — and because it's is very near and dear to our hearts, we've been using TypeScript types in our prompts.</p>
<blockquote>
<p><strong>User:</strong></p>
<p>Translate the following request into JSON.</p>
<blockquote>
<p>Could I get a blueberry muffin and a grande latte?</p>
</blockquote>
<p>Respond only in JSON that satisfies the <code>Response</code> type:</p>
<pre tabindex="0"><code><span><span>type</span><span> </span><span>Response</span><span> = {</span></span>
<span><span>    </span><span>items</span><span>: </span><span>Item</span><span>[];</span></span>
<span><span>};</span></span>
<span></span>
<span><span>type</span><span> </span><span>Item</span><span> = {</span></span>
<span><span>    </span><span>name</span><span>: </span><span>string</span><span>;</span></span>
<span><span>    </span><span>quantity</span><span>: </span><span>number</span><span>;</span></span>
<span><span>    </span><span>size</span><span>?: </span><span>string</span><span>;</span></span>
<span><span>    </span><span>notes</span><span>?: </span><span>string</span><span>;</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p><strong>ChatBot:</strong></p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  </span><span>"items"</span><span>: [</span></span>
<span><span>    {</span></span>
<span><span>      </span><span>"name"</span><span>: </span><span>"blueberry muffin"</span><span>,</span></span>
<span><span>      </span><span>"quantity"</span><span>: </span><span>1</span></span>
<span><span>    },</span></span>
<span><span>    {</span></span>
<span><span>      </span><span>"name"</span><span>: </span><span>"latte"</span><span>,</span></span>
<span><span>      </span><span>"quantity"</span><span>: </span><span>1</span><span>,</span></span>
<span><span>      </span><span>"size"</span><span>: </span><span>"grande"</span></span>
<span><span>    }</span></span>
<span><span>  ]</span></span>
<span><span>}</span></span>
<span></span></code></pre>
</blockquote>
<p>This is pretty great!
TypeScript has shown that it's well-suited to precisely describe JSON.
But what happens when a language model stumbles and makes up a response that doesn't conform to our types?</p>
<p>Well because these types are valid TypeScript code, we can validate the response against them using the TypeScript compiler itself!
In fact, the error feedback from the compiler can even be used to guide repairs.
When put together, we can get a robust process for getting well-typed responses that our apps can further massage, validate with a user, etc.</p>
<p>In other words, <strong>types are all you need</strong>.</p>
<h2>Enter TypeChat</h2>
<p>The technique of combining a human prompt and a "response schema" is not necessarily unique — but it is promising.
And as we've focused on translating user intent to structured data, we've found that TypeScript is very well-suited for the task.
We've grown more confident with this approach, and in order to prove it out, we're releasing a library called TypeChat to help make it easier to use in your apps.
<a href="https://npmjs.com/package/typechat">TypeChat is already on npm</a> if you want to try it now, and provides tools for prompt prototyping, schema validation, repair, and more.</p>
<p>Here's the basic code to hook TypeChat up to an LLM and decide if a sentence is negative, neutral, or positive.</p>
<pre tabindex="0"><code><span><span>// ./src/sentimentSchema.ts</span></span>
<span></span>
<span><span>// The following is a schema definition for determining the sentiment of a some user input.</span></span>
<span></span>
<span><span>export</span><span> </span><span>interface</span><span> </span><span>SentimentResponse</span><span> {</span></span>
<span><span>    </span><span>/** The sentiment of the text. */</span></span>
<span><span>    </span><span>sentiment</span><span>: </span><span>"negative"</span><span> | </span><span>"neutral"</span><span> | </span><span>"positive"</span><span>;</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<pre tabindex="0"><code><span><span>// ./src/main.ts</span></span>
<span></span>
<span><span>import</span><span> </span><span>*</span><span> </span><span>as</span><span> </span><span>fs</span><span> </span><span>from</span><span> </span><span>"fs"</span><span>;</span></span>
<span><span>import</span><span> </span><span>*</span><span> </span><span>as</span><span> </span><span>path</span><span> </span><span>from</span><span> </span><span>"path"</span><span>;</span></span>
<span><span>import</span><span> </span><span>dotenv</span><span> </span><span>from</span><span> </span><span>"dotenv"</span><span>;</span></span>
<span><span>import</span><span> </span><span>*</span><span> </span><span>as</span><span> </span><span>typechat</span><span> </span><span>from</span><span> </span><span>"typechat"</span><span>;</span></span>
<span><span>import</span><span> { </span><span>SentimentResponse</span><span> } </span><span>from</span><span> </span><span>"./sentimentSchema"</span><span>;</span></span>
<span></span>
<span><span>// Load environment variables.</span></span>
<span><span>dotenv</span><span>.</span><span>config</span><span>({ </span><span>path:</span><span> </span><span>path</span><span>.</span><span>join</span><span>(</span><span>__dirname</span><span>, </span><span>"../.env"</span><span>) });</span></span>
<span></span>
<span><span>// Create a language model based on the environment variables.</span></span>
<span><span>const</span><span> </span><span>model</span><span> = </span><span>typechat</span><span>.</span><span>createLanguageModel</span><span>(</span><span>process</span><span>.</span><span>env</span><span>);</span></span>
<span></span>
<span><span>// Load up the contents of our "Response" schema.</span></span>
<span><span>const</span><span> </span><span>schema</span><span> = </span><span>fs</span><span>.</span><span>readFileSync</span><span>(</span><span>path</span><span>.</span><span>join</span><span>(</span><span>__dirname</span><span>, </span><span>"sentimentSchema.ts"</span><span>), </span><span>"utf8"</span><span>);</span></span>
<span><span>const</span><span> </span><span>translator</span><span> = </span><span>typechat</span><span>.</span><span>createJsonTranslator</span><span>&lt;</span><span>SentimentResponse</span><span>&gt;(</span><span>model</span><span>, </span><span>schema</span><span>, </span><span>"SentimentResponse"</span><span>);</span></span>
<span></span>
<span><span>// Process requests interactively.</span></span>
<span><span>typechat</span><span>.</span><span>processRequests</span><span>(</span><span>"😀&gt; "</span><span>, </span><span>/*inputFile*/</span><span> </span><span>undefined</span><span>, </span><span>async</span><span> (</span><span>request</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>    </span><span>const</span><span> </span><span>response</span><span> = </span><span>await</span><span> </span><span>translator</span><span>.</span><span>translate</span><span>(</span><span>request</span><span>);</span></span>
<span><span>    </span><span>if</span><span> (!</span><span>response</span><span>.</span><span>success</span><span>) {</span></span>
<span><span>        </span><span>console</span><span>.</span><span>log</span><span>(</span><span>response</span><span>.</span><span>message</span><span>);</span></span>
<span><span>        </span><span>return</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>    </span><span>console</span><span>.</span><span>log</span><span>(</span><span>`The sentiment is </span><span>${</span><span>response</span><span>.</span><span>data</span><span>.</span><span>sentiment</span><span>}</span><span>`</span><span>);</span></span>
<span><span>});</span></span>
<span></span></code></pre>
<p>TypeChat can be used in a number of different ways.
The way we've discussed here so far is all about using a "data schema" to turn some user intent into a structured response;
however, TypeChat also makes it possible to use an "API schema" to construct basic programs.
We have some <a href="https://microsoft.github.io/TypeChat/docs/">docs</a> and <a href="https://microsoft.github.io/TypeChat/docs/examples/">examples</a> to get a sense of the different ways you can use TypeChat.</p>
<h2>Open and Pluggable</h2>
<p>First of all, TypeChat is open-source.
We're MIT-licensed and you can <a href="https://github.com/Microsoft/TypeChat">find us on GitHub</a> where we're eager to hear your thoughts, share our ideas, and build with you.</p>
<p>Second, TypeChat is built in a way that is meant to be model-neutral.
While we have some very basic integration with the OpenAI API and the Azure OpenAI service for convenience, this approach should work for any chat completion-style API that you want to use — though note that at the moment, TypeChat works best with models that have been trained on both prose and code.</p>
<h2>Try It Today!</h2>
<p>We'd love to know if TypeChat is something that's useful and interests you!
As we mentioned, we'll be welcoming you on <a href="https://github.com/Microsoft/TypeChat">GitHub</a> if you have any question, suggestions, and more.</p>
<p>Happy Hacking!</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Banned journalism housed in virtual Minecraft architecture (2022) (142 pts)]]></title>
            <link>https://99percentinvisible.org/article/uncensored-library-virtual-minecraft-architecture-houses-banned-journalism/</link>
            <guid>36802021</guid>
            <pubDate>Thu, 20 Jul 2023 15:32:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://99percentinvisible.org/article/uncensored-library-virtual-minecraft-architecture-houses-banned-journalism/">https://99percentinvisible.org/article/uncensored-library-virtual-minecraft-architecture-houses-banned-journalism/</a>, See on <a href="https://news.ycombinator.com/item?id=36802021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <main>
                    <article id="article">
                            <header>
                
<h4>Article by Kurt Kohlstedt</h4>






              </header>
                            
              <div>
                <p>When schools ban books, the strategy often <a href="https://www.theguardian.com/education/2021/dec/23/us-book-bans-conservative-parents-reading" target="_blank" rel="noopener noreferrer">backfires</a> on would-be censors, resulting in greater interest around illicit literature. Similarly,&nbsp; when governments censor the media, groups like Reporters Without Borders spearhead efforts to make such censored material extra visible. Their <a href="https://www.uncensoredlibrary.com/en" target="_blank" rel="noopener noreferrer">Uncensored Library</a> project brings together architecture and journalism in an unlikely virtual reality space: the interactive gaming world of Minecraft.</p>
<p><a href="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-flags.jpg"><img decoding="async" src="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-flags-600x338.jpg" alt="" width="600" height="338" srcset="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-flags-600x338.jpg 600w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-flags-728x410.jpg 728w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-flags-300x169.jpg 300w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-flags.jpg 1280w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>On the surface, Minecraft is a game of collaborative construction and its low-res look may not appear conducive to elements like: reading articles or even entire books in-game. But there are “items” within the game that effectively work like books with a theoretically infinite number of pages. Creators can transcribe text into these “books,” rendering them legible and downloadable. The books are then put in “chests” and organized in the virtual space for accessibility.</p>
<p><a href="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze.png"><img decoding="async" loading="lazy" src="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze-600x424.png" alt="" width="600" height="424" srcset="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze-600x424.png 600w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze-728x515.png 728w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze-300x212.png 300w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze-1536x1086.png 1536w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-gallery-maze.png 1920w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>The idea, in part, is to work around normally filtered channels. The non-profit <a href="https://rsf.org/en" target="_blank" rel="noopener noreferrer">Reporters Without Borders</a> has experience on this front with projects like the audio-centric Uncensored Playlist, which evaded censors by operating through music streaming services. In the Uncensored Library, the spatial design makes finding material easier, and allows for other forms of creation and interaction as well, such as a memorial to murdered journalists housed within the library’s walls.</p>
<p><a href="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view.png"><img decoding="async" loading="lazy" src="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view-600x338.png" alt="" width="600" height="338" srcset="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view-600x338.png 600w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view-728x410.png 728w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view-300x169.png 300w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view-1536x864.png 1536w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-aerial-view.png 1920w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>“The style is Classical and formal, the kind of architecture you’d see in the British Museum and New York Public Library,” says project lead James Delaney of Blockworks. “That was deliberate, because this architectural style is usually used by governments to reinforce their own positions of authority.” The design team “wanted to take that and turn it on its head. Yes, we’re using this formalistic, authoritarian style, but instead it’s filled with free information.”</p>
<p><a href="https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples.jpg"><img decoding="async" loading="lazy" src="https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples-600x331.jpg" alt="" width="600" height="331" srcset="https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples-600x331.jpg 600w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples-728x402.jpg 728w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples-300x166.jpg 300w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples-1536x848.jpg 1536w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-book-text-examples.jpg 2048w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>“The <a href="https://99percentinvisible.org/episode/weeding-is-fundamental/">criteria for inclusion</a> is handled by Reporters Without Borders, which ensures the library’s content is accurate, truthful, and sensitive,” reports <a href="https://www.theverge.com/2020/3/18/21184041/minecraft-library-censored-journalism-reporters-without-borders" target="_blank" rel="noopener noreferrer">Cian Mahar</a>. And it seems to be working: the library gets regular visitors from countries where information is less than free, including Russia, Egypt, Mexico, Saudi Arabia, and Vietnam.</p>
<p><a href="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns.png"><img decoding="async" loading="lazy" src="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns-600x338.png" alt="" width="600" height="338" srcset="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns-600x338.png 600w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns-728x410.png 728w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns-300x169.png 300w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns-1536x864.png 1536w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-neoclassical-columns.png 1920w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>Maps and flags adorn the interior of the virtual library’s architecture, highlighting regions where censorship is more pronounced and helping with virtual wayfinding within the stacks. The design choices are also made to welcome <a href="https://99percentinvisible.org/episode/goodnight-nobody/">younger audiences</a> who are familiar with Minecraft and help educate them about information suppression. Not all countries are taking this lying down — some are pushing back, trying to get the main server banned. But as with most things on the internet: once the cat (gif) is out of the bag, it’s game over. Anyone can download and reupload the entire library to another server at any time.</p>
<p><a href="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument.png"><img decoding="async" loading="lazy" src="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument-600x424.png" alt="" width="600" height="424" srcset="https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument-600x424.png 600w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument-728x515.png 728w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument-300x212.png 300w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument-1536x1086.png 1536w, https://99percentinvisible.org/app/uploads/2022/02/uncensored-library-entry-monument.png 1920w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p><em><strong>From the <a href="https://www.uncensoredlibrary.com/en" target="_blank" rel="noopener noreferrer">Uncensored Library</a> website:</strong> </em>“In many countries, websites, social media and blogs are controlled by oppressive leaders. Young people, in particular, are forced to grow up in systems where their opinion is heavily manipulated by governmental disinformation campaigns. But even where almost all media is blocked or controlled, the world’s most successful computer game is still accessible. Reporters Without Borders (RSF) uses this loophole to bypass internet censorship to bring back the truth – within Minecraft.”</p>

              </div>
              
          </article>
                  </main><!-- /.main -->
                  <!-- /.sidebar -->
              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FedNow Is Live (385 pts)]]></title>
            <link>https://www.federalreserve.gov/newsevents/pressreleases/other20230720a.htm</link>
            <guid>36801491</guid>
            <pubDate>Thu, 20 Jul 2023 15:00:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.federalreserve.gov/newsevents/pressreleases/other20230720a.htm">https://www.federalreserve.gov/newsevents/pressreleases/other20230720a.htm</a>, See on <a href="https://news.ycombinator.com/item?id=36801491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
    
    <div id="article">
            <div>
                
                <p>July 20, 2023</p>
                <h3>Federal Reserve announces that its new system for instant payments, the FedNow® Service, is now live</h3>              
                
                <p>For release at 10:00 a.m. EDT                     
                
                </p><ul>
                    
                </ul>
 
            </div>
            
            <div>
                
                <p>The Federal Reserve on Thursday announced that its new system for instant payments, the FedNow<sup>® </sup>Service, is now live. Banks and credit unions of all sizes can sign up and use this tool to instantly transfer money for their customers, any time of the day, on any day of the year.</p>

<p>"The Federal Reserve built the FedNow Service to help make everyday payments over the coming years faster and more convenient," said Federal Reserve Chair Jerome H. Powell. "Over time, as more banks choose to use this new tool, the benefits to individuals and businesses will include enabling a person to immediately receive a paycheck, or a company to instantly access funds when an invoice is paid."</p>

<p>To start, 35 early-adopting banks and credit unions, as well as the U.S. Department of the Treasury's Bureau of the Fiscal Service, are ready with instant payments capabilities via the FedNow Service. In addition, 16 service providers are ready to support payment processing for banks and credit unions.</p>

<p>When fully available, instant payments will provide substantial benefits for consumers and businesses, such as when rapid access to funds is useful, or when just-in-time payments help manage cash flows in bank accounts. For example, individuals can instantly receive their paychecks and use them the same day, and small businesses can more efficiently manage cash flows without processing delays. Over the coming years, customers of banks and credit unions that sign up for the service should be able to use their financial institution's mobile app, website, and other interfaces to send instant payments quickly and securely.</p>

<p>As an interbank payment system, the FedNow Service operates alongside other longstanding Federal Reserve payment services such as Fedwire<sup>®</sup> and FedACH<sup>®</sup>. The Federal Reserve is committed to working with the more than 9,000 banks and credit unions across the country to support the widespread availability of this service for their customers over time.</p>

<p>A list of early adopters with instant payment capabilities is attached. Additional information is available on the Federal Reserve Financial Services <a href="https://www.frbservices.org/financial-services/fednow/organizations">website</a>.</p>

<p>For media inquiries, please email <a href="https://www.federalreserve.gov/cdn-cgi/l/email-protection#254840414c44654357470b424a53"><span data-cfemail="375a52535e567751455519505841">[email&nbsp;protected]</span></a> or call 202-452-2955.</p>
                  
                                
                             
            </div>
            
        </div>
    <div>
        <p>Last Update:
            July 20, 2023
        </p>
        
    </div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JPEG XL: How It Started, How It’s Going (254 pts)]]></title>
            <link>https://cloudinary.com/blog/jpeg-xl-how-it-started-how-its-going</link>
            <guid>36801448</guid>
            <pubDate>Thu, 20 Jul 2023 14:57:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloudinary.com/blog/jpeg-xl-how-it-started-how-its-going">https://cloudinary.com/blog/jpeg-xl-how-it-started-how-its-going</a>, See on <a href="https://news.ycombinator.com/item?id=36801448">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<p>Last month at the annual Worldwide Developers Conference, Apple announced its support of JPEG XL. As someone who helped create JPEG XL, I don’t think anyone was more pleased than me to hear this news.</p>



<p>For a standard that’s not even three years old, this was a major win. Or rather, it’s a win for the web community, for photographers and artists, as well as for those of us who created the codec.</p>



<p>Before diving into what the Apple announcement means for JPEG XL moving forward, let’s take a quick look at its origin.</p>








<p>JPEG XL development began in 2018, when the JPEG committee launched a call for proposals on next-generation image compression, to which seven proposals were submitted. Of the seven two stood out: Google’s PIK and Cloudinary’s FUIF proposal. The ingredients from both proposals were eventually combined and refined to design a new codec that was better than the sum of its parts.</p>



<p>By the end of 2020, the main technical work was done and the bitstream was frozen, i.e., no more changes would be made that would change the format from the decoder point of view.</p>



<p>In November 2020 Cloudinary added JXL support and early in 2021, I wrote the blog post <a href="https://cloudinary.com/blog/time_for_next_gen_codecs_to_dethrone_jpeg" target="_blank" rel="noreferrer noopener">Time for Next-Gen Codecs to Dethrone JPEG</a>. In the piece I argued that modern codecs like JPEG XL can bring many benefits, and expressed the hope that they would be widely adopted.</p>



<p>In early April 2021, the Chrome browser added experimental support (behind a flag), even before the JPEG XL standard was officially published. (The final draft had been submitted to ISO, but it would still take until March 2022 before it was approved and published as the international standard ISO/IEC 18181.) Firefox followed suit quickly and added experimental support. Things were looking good.</p>



<p>Then, on Halloween 2022, Chrome developers suddenly announced that they would be removing JPEG XL support. This decision was quite unexpected and controversial. In my blog <a href="https://cloudinary.com/blog/the-case-for-jpeg-xl" target="_blank" rel="noreferrer noopener">The Case for JPEG XL</a>, I argued why this decision should be reversed. In December, Chrome developers provided test results that were used to justify the decision and invited feedback. I analyzed the results and <a href="https://cloudinary.com/blog/contemplating-codec-comparisons" target="_blank" rel="noreferrer noopener">pointed out</a> several methodological flaws and oversights. So far, my feedback has been ignored.</p>



<p>Beyond browsers, adoption of JPEG XL continued, in particular in image authoring software like Serif Affinity, Adobe Camera Raw, GIMP, Krita, etc. Unfortunately, Chrome’s decision has slowed wider adoption in web browsers of JPEG XL.</p>








<p>On June 5, 2023, at Apple’s annual Worldwide Developers Conference (WWDC23), a slide was presented listing new features of the Safari browser, and “JPEG XL” was on the slide. Not only is Safari 17 adding JPEG XL support, the new versions of iOS, iPadOS, macOS, watchOS, and visionOS will support JPEG XL.</p>



<figure><img width="1999" height="1124" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1124/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA" alt="speaker presenting new Safari browser features, with &quot;JPEG XL&quot; listed" data-public-id="Web_Assets/blog/JPEG-XL_2.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1689095239" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"></figure>



<p>This was unexpectedly good news. None of the JPEG XL developers had anticipated that the first browser to support this format that was co-created by Google, would be Safari. This is, of course, great news for the adoption of JPEG XL and leads me to think the momentum is back!</p>








<p>Image compression is a crucial part of what we do at Cloudinary, and for that we need a deep understanding of when to use what codec — or rather, what encoder and settings. For that reason, we did a large experiment to create the Cloudinary Image Dataset (CID22), a big dataset of human-annotated compressed images. This helps us to better understand the impact compression has on perceptual quality.</p>



<p>Comparing image encoders is not an easy task. The distortions caused by different encoders are different and it is hard to predict the perceived image quality. Subjective testing is still the gold standard of image quality assessment and the testing methodology is still an active area of research, in which Cloudinary participates. (For example, in the AIC-3 project of the JPEG committee.)</p>



<p>Based on datasets of human-annotated images, objective metrics like SSIMULACRA 2 can be validated. While not perfect, metrics can be used at scale to test many more images and encoder configurations than what would be feasible in a subjective experiment.</p>



<p>In image compression, there is a trade-off to be made between three things:</p>



<ul>
<li><strong>Compression.</strong> How much can the file size be reduced?</li>



<li><strong>Quality. </strong>To what extent is the image degraded in visual quality?</li>



<li><strong>Speed. </strong>How long does it take to encode an image?</li>
</ul>



<p>The importance of these elements is relative to one another and depends on the specific use case. For the same visual quality, we can look at the performance of various encoders in terms of compression and speed. We use the SSIMULACRA v2.1 metric as an indicator of visual quality on the horizontal axis. On the vertical axis, we have the compression performance (as a percentage saving compared to unoptimized 4:4:4 JPEG) on the main plot on the left hand side. On the plot on the right hand side we have encoding speed expressed in megapixels per second (when using a single cpu core). The plot shows JPEG XL (red), AVIF/libaom (yellow), WebP (green) and mozjpeg (white), each at their default speed setting. The dotted lines use 4:2:0 chroma subsampling (yuv420) while the full lines use 4:4:4 (yuv444). The range goes from a very low quality (around q30 in libjpeg terms) to a very high quality (around q95 in libjpeg terms).</p>



<figure><img width="1999" height="1347" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1347/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA" alt="graph showing compression, quality, and encode speed trade-offs according to SSIMULACRA_2.1" data-public-id="Web_Assets/blog/JPEG-XL_1.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1689094929" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"><figcaption><em>Compression gains and encode speed for JPEG XL (red), AVIF (yellow), WebP (green) and mozjpeg (white), compared to unoptimized JPEG</em></figcaption></figure>



<p>The plot shows aggregated data corresponding to about 300 different source images. Some conclusions:</p>



<ul>
<li>WebP does bring a significant gain compared to unoptimized JPEG: a saving of 25 to 35% at the lower end of the quality spectrum, though at the higher end the savings diminish, mostly because WebP’s forced limited-range yuv420 does become a limiting factor (at q90+, it struggles to even reach the quality of JPEG, which can do full-range yuv444).</li>



<li>Compared to the optimized mozjpeg encoder though, which also brings significant gains compared to unoptimized JPEG, the additional gains WebP brings are not as large: only about 3 to 5% additional savings.</li>



<li>AVIF does bring a significant additional saving over WebP of about 10 to 15%. Moreover, since AVIF does not have forced yuv420, it can still bring significant savings over unoptimized JPEG even in the high-quality range, unlike WebP. However, these savings do come at a cost in speed: AVIF encoding at default speed is an order of magnitude slower than WebP or mozjpeg.</li>



<li>JPEG XL brings an additional saving over AVIF of about 5 to 10%. The additional saving is larger in the high-quality range than in the lower-quality range. Moreover, it does this at a more reasonable speed.</li>
</ul>








<p>Because Cloudinary was involved in the development of JPEG XL, we were the first to provide support. To convert an image to JXL with Cloudinary, simply add <code>f_jxl</code> to the url or change its extension to <code>.jxl</code>. Especially if you have significant traffic from visitors using iOS or Safari, it will likely be worthwhile to serve JPEG XL images (as soon as the new iOS and Safari have landed), even if you still need AVIF, WebP, or JPEG fallbacks for other visitors.</p>



<p>While in terms of average compression performance, JPEG XL is currently the best codec available, there is a lot of image-dependent variation. For some images, JPEG XL is a lot better than the other codecs, while for other images, AVIF is better than JPEG XL. For this reason, we are currently working on an AI-powered new version of our <code>f_auto,q_auto</code> feature, which can automatically select the optimal format to use on an image-by-image basis.</p>



<p>Have any questions or want to discuss the topic of this blog in greater detail? The <a href="https://community.cloudinary.com/" target="_blank" rel="noreferrer noopener">Cloudinary Community</a> is a great place to share your questions, ideas, and suggestions. You can also ping me on the <a href="https://discord.gg/cloudinary" target="_blank" rel="noreferrer noopener">Cloudinary Community Discord server</a> here: Jon: @_wb_</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Become Ungoogleable (195 pts)]]></title>
            <link>https://joeyh.name/blog/entry/become_ungoogleable/</link>
            <guid>36800789</guid>
            <pubDate>Thu, 20 Jul 2023 14:16:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joeyh.name/blog/entry/become_ungoogleable/">https://joeyh.name/blog/entry/become_ungoogleable/</a>, See on <a href="https://news.ycombinator.com/item?id=36800789">Hacker News</a></p>
<div id="readability-page-1" class="page"><article class="page">







<section id="pagebody" role="main">
<p>I've <a href="https://joeyh.name/robots.txt">removed my website from indexing by Google</a>.
The proximate cause is Google's new effort to
<a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md">DRM the web</a>,
but there is of course so much more.</p>

<p>This is a unique time, when it's actually feasible to become ungoogleable
without losing much. Nobody really expects to be able to find anything of
value in a Google search now, so if they're looking for me or something I've
made and don't find it, they'll use some other approach.</p>

<p>I've looked over the kind of traffic that Google refers to my website, and
it will not be a significant loss even if those people fail to find me by
some other means. Over 30% of the traffic to this website is rss feeds.
Google just doesn't matter on the modern web.</p>

<p>The web will end one day. But let's not let Google kill it.</p>

</section>



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple says it'll remove iMessage and FaceTime in UK rather than break encryption (1246 pts)]]></title>
            <link>https://9to5mac.com/2023/07/20/apple-imessage-facetime-remove-security-law/</link>
            <guid>36800297</guid>
            <pubDate>Thu, 20 Jul 2023 13:41:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5mac.com/2023/07/20/apple-imessage-facetime-remove-security-law/">https://9to5mac.com/2023/07/20/apple-imessage-facetime-remove-security-law/</a>, See on <a href="https://news.ycombinator.com/item?id=36800297">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=1600" srcset="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="iMessage" fetchpriority="high">
	
	</figure>

<p>Facing possible legislation that would require messaging services to offer backdoors in end-to-end encryption, Apple is saying it would rather remove apps like iMessage and FaceTime entirely from the UK market (<a href="https://www.bbc.co.uk/news/technology-66256081">via BBC News</a>).</p>



<p>The new Online Safety Bill is currently under review. Apple, WhatsApp, Signal, and other services have voiced their opposition to the proposal.</p>



<p>The UK government <a href="https://9to5mac.com/2023/06/27/apple-online-safety-bill-encryption/">wants the ability to scan end-to-end encrypted messages</a>, for child-abuse material and other illegal content. They argue the existing law accommodates this but is technically outdated by the security provisions of modern technology.</p>



<p>Apple has submitted a nine-page opposition to the planned bill. It strongly objects to requirements such as backdoors for end-to-end encryption, reporting changes to product security features before they are released and being forced to disable security features before an appeals process can take place.</p>



<p>The company said it would not make changes for one country that would weaken security for all of its users, threatening instead to disable iMessage and FaceTime for UK customers.</p>



<p>The proposed law is currently undergoing an eight-week consultation period. Obviously, Apple and others hope the government will revise the bill in response to the criticism.</p>



<p><a href="https://9to5mac.com/2022/12/07/apple-confirms-that-it-has-stopped-plans-to-roll-out-csam-detection-system/">Apple previously withdrew plans</a> for its own CSAM-scanning feature for iCloud Photos, following pushback from customers and human rights groups. Apple’s solution was more privacy-preserving than what is now proposed by the UK government.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Mac to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3PWlVJb"><img src="https://9to5mac.com/wp-content/uploads/sites/6/2023/07/PRBanner750x1501-1.png" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Accidentally Load Bearing (287 pts)]]></title>
            <link>https://www.jefftk.com/p/accidentally-load-bearing</link>
            <guid>36800151</guid>
            <pubDate>Thu, 20 Jul 2023 13:29:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jefftk.com/p/accidentally-load-bearing">https://www.jefftk.com/p/accidentally-load-bearing</a>, See on <a href="https://news.ycombinator.com/item?id=36800151">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div>

    <p><span>

Sometimes people will talk about Chesterton's Fence, the idea that if
you want to change something—removing an apparently useless
fence—you should first determine why it was set up that way:

</span></p>
<blockquote>

The gate or fence did not grow there. It was not set up by
somnambulists who built it in their sleep. It is highly improbable
that it was put there by escaped lunatics who were for some reason
loose in the street. Some person had some reason for thinking it would
be a good thing for somebody. And until we know what the reason was,
we really cannot judge whether the reason was reasonable. It is
extremely probable that we have overlooked some whole aspect of the
question, if something set up by human beings like ourselves seems to
be entirely meaningless and mysterious. — <i>G. K. Chesterton, The
Drift From Domesticity</i>

</blockquote>

<p>

Figuring out something's designed purpose can be helpful in evaluating
changes, but a risk is that it puts you in a frame of mind where what
matters is the role the original builders intended.

</p>
<p>

A few years ago I was <a href="https://www.jefftk.com/p/bathroom-construction-framing">rebuilding a bathroom in our
house</a>, and there was a vertical stud that was in the way.  I could
easily tell why it was there: it was part of a partition for a closet.
And since I knew its designed purpose and no longer needed it for
that anymore, the Chesterton's Fence framing would suggest that it was
fine to remove it.  Except that over time it had become accidentally
load bearing: through other (ill conceived) changes to the structure
this stud was now helping hold up the second floor of the house.  In
addition to considering why something was created, you also need to
consider what additional purposes it may have since come to serve.

</p>
<p>

This is a concept I've run into a lot when making changes to complex
computer systems.  It's useful to look back through the change
history, read original design documents, and understand why a
component was built the way it was.  But you also need to look closely
at how the component integrates into the system today, where it can
easily have taken on additional roles.

  </p>
</div>


  
<p>Comment via: <a href="https://www.facebook.com/jefftk/posts/pfbid02i81EvJpGSRdnfgT8kbtgPtFKJ4t9c4uYCnNvwmZiCyVvYLbcVKkshj9YkoC1at3Zl">facebook</a>, <a href="https://lesswrong.com/posts/QBeN49SoKpDMX3kKk">lesswrong</a>, <a href="https://news.ycombinator.com/item?id=36800151">hacker news</a>, <a href="https://mastodon.mit.edu/@jefftk/110707644099272630">mastodon</a></p>

</div><section>
  <h3>Recent posts on blogs I like:</h3>
  <section>
    
    <div>
      <h4>
        <a href="https://www.lilywise.com/fiddle">Fiddle</a>
      </h4>
      <p>
  

I first started playing fiddle when I was five, just around my
birthday.  I had really wanted a fiddle because I wanted to learn how
to play it and my parents got me one for my birthday so I started
taking lessons.  Though after a couple of lessons I start…</p>
      <p><small>
        via <a href="https://www.lilywise.com/">Lily Wise's Blog Posts</a>
      </small>
      <small>July 13, 2023</small>
    </p></div>
    
    <div>
      <h4>
        <a href="https://juliawise.net/the-best-kind-of-music-to-learn-is-social-music/">The best kind of music to learn is social music</a>
      </h4>
      <p>Will they ever stay up late playing this kind of music for fun?
The post The best kind of music to learn is social music appeared first on Otherwise.
</p>
      <p><small>
        via <a href="https://juliawise.net/">Otherwise</a>
      </small>
      <small>July 1, 2023</small>
    </p></div>
    
    <div>
      <h4>
        <a href="https://lincolnquirk.com/2023/07/01/altruist_perks.html">Why altruists can’t have nice things</a>
      </h4>
      <p>I posted this on the Effective Altruism forum as part of the EA Strategy Fortnight. I’m cross posting it here.</p>
      <p><small>
        via <a href="https://lincolnquirk.com/">Home</a>
      </small>
      <small>July 1, 2023</small>
    </p></div>
    
  </section>
  <p>
    <a href="https://www.jefftk.com/ring">more</a>
    &nbsp;&nbsp;&nbsp;
    (<a href="https://git.sr.ht/~sircmpwn/openring">via openring</a>)
  </p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Crazily Unconstitutional New Laws Trying to Criminalize Filming Cops (171 pts)]]></title>
            <link>https://slate.com/news-and-politics/2023/07/jarrell-garris-bodycam-footage-filming-cops-law-indiana-florida.html</link>
            <guid>36800144</guid>
            <pubDate>Thu, 20 Jul 2023 13:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://slate.com/news-and-politics/2023/07/jarrell-garris-bodycam-footage-filming-cops-law-indiana-florida.html">https://slate.com/news-and-politics/2023/07/jarrell-garris-bodycam-footage-filming-cops-law-indiana-florida.html</a>, See on <a href="https://news.ycombinator.com/item?id=36800144">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-uri="slate.com/_components/article/instances/clka449ke00bvmgm5svypmy3c@published" data-has-roadblock="false" data-rubric="jurisprudence" itemscope="" itemtype="http://schema.org/Article">
  

  

<header>

  <a href="https://slate.com/news-and-politics/jurisprudence">      Jurisprudence</a>

  




    </header>
<div>
      <figure data-uri="slate.com/_components/image/instances/clka449ke00bomgm59by9x02o@published" data-editable="imageInfo"><p><img loading="lazy" src="https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0" alt="Someone filming a line of cops in riot gear on a tablet." width="1560" height="1040" srcset="https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=320 320w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=480 480w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=600 600w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=840 840w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=960 960w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1280 1280w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1440 1440w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1600 1600w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1920 1920w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=2200 2200w">
        
      </p>
<figcaption>
<span>Videos like this give justice a fighting chance.</span>
<span>Ramin Talaie/Corbis via Getty Images</span>
</figcaption>
</figure>

  </div>
  

  <section>
      


      

    <div itemprop="mainEntityOfPage">
          <p data-word-count="102" data-uri="slate.com/_components/slate-paragraph/instances/clka449ke00bpmgm5yclnkvuw@published">Few seem to have noticed that a dangerous legislative campaign is underway. Its aim: to shield police brutality by shutting down our cameras. On July 1, an Indiana law <a href="https://iga.in.gov/pdf-documents/123/2023/house/bills/HB1186/HB1186.05.ENRS.pdf">went into effect</a> making it a crime to come within 25 feet of an on-duty police officer if ordered to stay back. Legislators in <a href="https://www.flsenate.gov/Session/Bill/2023/1539/BillText/c2/PDF">Florida</a>, <a href="https://legis.la.gov/legis/ViewDocument.aspx?d=1329883">Louisiana</a>, and <a href="https://legislation.nysenate.gov/pdf/bills/2021/s3464">New York</a> have produced similar laws. Other states have promised to follow suit. Under these laws, if you come close enough to film a police officer beating someone up, there’s a good chance you’ll spend two months in jail and end up with a criminal record.</p>

  

  <p data-word-count="92" data-uri="slate.com/_components/slate-paragraph/instances/clka44kg9000r3b6w7a1kmxpd@published">In 2023 alone, the police have killed <a href="https://mappingpoliceviolence.org/">more than 500 people</a> in the United States. Among them was <a href="https://www.nytimes.com/2023/07/13/nyregion/new-rochelle-police-shooting-jarrell-garris.html">Jarrell Garris</a>, who died last week in New Rochelle, New York, after police shot him during an arrest for allegedly stealing a banana and some grapes. Garris was unarmed, and tackled by three officers, handcuffed, and shot. The police claim he was reaching for an officer’s gun. They’ve released bodycam footage <a href="https://www.youtube.com/watch?v=KhrQl2BBLus">that mysteriously stops</a> just before the shooting. They want to make sure you don’t see exactly what happened. So do the new laws.</p>

  



  <p data-word-count="74" data-uri="slate.com/_components/slate-paragraph/instances/clka44kkg000s3b6woijmcqf3@published">The Indiana, Florida, and Louisiana laws are only the most recent offensives in an ongoing campaign to shield police violence. As the Black Lives Matter protests caught fire in 2020, right-wing organizations jumped into action. They had three goals: quell the protests; silence critics of the police; and keep scenes like George Floyd’s murder out of the public eye, freeing up officers like convicted murderer Derek Chauvin to use traditional techniques to break suspects.</p>

  


  <p data-word-count="100" data-uri="slate.com/_components/slate-paragraph/instances/clka44kos000t3b6wqxef7e18@published">These efforts have been a resounding success. In the wake of the protests, 42 states have enacted laws <a href="https://www.icnl.org/usprotestlawtracker/">creating extra penalties</a> for protesters, with new crimes such as “aggravated riot” and “mob intimidation.” Many states have passed hate-speech laws that <a href="https://scholarship.law.campbell.edu/cgi/viewcontent.cgi?article=1695&amp;context=clr">protect the police</a> from verbal or symbolic abuse, such as stomping on a “Back the Blue” sign while “smirking in an intimidating manner” (the crime for which a woman in Utah <a href="https://www.npr.org/2021/07/15/1016431004/a-woman-is-facing-a-hate-crime-charge-for-stomping-on-a-back-the-blue-sign-in-ut">was prosecuted</a>). Although some of these laws have been vetoed or deemed unconstitutional, their promoters have only grown more brazen. The Indiana, Louisiana, and Florida laws are the result.</p>

  


  <p data-word-count="76" data-uri="slate.com/_components/slate-paragraph/instances/clka44kr8000u3b6wuq8ceyv9@published">These laws take their cue not only from the anti-protest and police protection laws but from a 2022 <a href="https://www.azleg.gov/legtext/55leg/2R/bills/HB2319S.pdf">Arizona law</a> making it a crime to film the police from 8 feet or closer. That law’s sponsor, then–state Rep. John Kavanagh (a former police officer), was frank about his intentions. “There are groups hostile to the police that follow them around to <a href="https://www.azcentral.com/story/opinion/op-ed/2022/03/24/hb-2319-videotape-police-8-feet-during-violent-encounters/7130071001/">videotape police incidents</a>,” he declared. The new law would put a stop to this nuisance.</p>

  


  


  


  <p data-word-count="59" data-uri="slate.com/_components/slate-paragraph/instances/clka44ktm000v3b6wl4r9ygn7@published">After the bill passed, the <a href="https://www.aclu.org/cases/arizona-broadcasters-association-v-brnovich#:~:text=Summary,accountability%20tools%20against%20police%20wrongdoing.">ACLU and 10 media organizations</a> sued, arguing that the law was a patent violation of the First Amendment. Federal District Court Judge John Tuchi agreed, <a href="https://www.acluaz.org/sites/default/files/2319_pi_order_9.9.22.pdf">issuing a preliminary injunction</a>. We have a clearly established constitutional right to record the police, he explained. Allowing the law to stand for even a moment “constitutes irreparable injury.”</p>

  <p data-word-count="55" data-uri="slate.com/_components/slate-paragraph/instances/clka44kwc000w3b6wx4geiizh@published">The law may not stand. But <a href="https://www.azcentral.com/story/news/local/arizona/2022/09/09/federal-judge-halts-arizona-ban-filming-police-within-8-feet/8035972001/">Kavanagh has vowed</a> that if it is ruled unconstitutional, he will simply redraft it. “If the judge says, you know, ‘This is bad because of X,’ next year, though, the bill will come in ‘Bill minus X.’ ” In short, he will massage it to make it <em>appear</em> constitutional.</p>

  <p data-word-count="92" data-uri="slate.com/_components/slate-paragraph/instances/clka44kyl000x3b6w5sscdsff@published">This is precisely what Indiana, Florida, and Louisiana have tried to do. Anticipating constitutional objections, the new laws don’t mention video. “Any suggestion that this … denies somebody the opportunity to film is just not accurate,” protests the Louisiana law’s sponsor, <a href="https://lailluminator.com/2023/05/09/bill-that-would-make-it-illegal-to-be-within-25-feet-of-police-advances-to-house-floor/">state Rep. Mike Johnson</a>, who wrote the bill on behalf of the Fraternal Order of Police. (The Louisiana Fraternal Order named Johnson <a href="https://www.kalb.com/2022/07/27/rep-mike-johnson-recognized-legislator-year/">legislator of the year</a> in 2022.) The new law, he insists, merely creates “a bodily separation” that prevents “anyone, friend or foe,” from “walking up to” officers and interfering.</p>

  


  


  <p data-word-count="57" data-uri="slate.com/_components/slate-paragraph/instances/clka44l2m000y3b6wcl9voher@published">Such claims should fool no one. “Cops have long tried claiming that [filming them] <a href="https://reason.com/2021/07/27/florida-bill-criminalize-filming-cops-first-amendment/">obstructs their ability</a> to do their job,” observes First Amendment lawyer Ari Cohn. “Now that this argument failed, they are … transparently trying to create a safe space from observation.” The new laws are nothing but “a targeted assault on First Amendment activity.”</p>

  


  <p data-word-count="94" data-uri="slate.com/_components/slate-paragraph/instances/clka44l50000z3b6w2k844ec1@published">The new laws don’t in fact criminalize interference (already illegal in all 50 states). They criminalize only the proximity necessary for filming. They give police the <a href="https://www.reentry.net/ny/help/item.2915-Getting_Property_Back_After_an_Arrest#:~:text=Safekeeping%3A%20your%20property%20can%20be,related%20to%20an%20ongoing%20case.">right to seize</a> cameras in the no-approach zone. They create a visual and auditory buffer likely to produce reasonable doubt in the courtroom. Is the officer’s knee on the suspect’s shoulder, or on his windpipe? Is the suspect fighting, or flailing? From 25 feet, with officers blocking visibility, phone cameras can’t capture crucial details. From 25 feet, they can’t capture choking sounds or someone crying “I can’t breathe.”</p>

  


  

  <p data-word-count="92" data-uri="slate.com/_components/slate-paragraph/instances/clka44l7y00103b6wfrfvhi05@published">By now, we shouldn’t be surprised that the footage released by New Rochelle police contained critical gaps. Videos give the lie to official reports. The fatal shooting of <a href="https://www.nytimes.com/2023/06/28/world/europe/france-police-shooting-paris-nanterre.html">Nahel Merzouk</a> in France on June 27 offers an object lesson. The police initially claimed they shot Merzouk because he was driving his car directly at them, but the video <a href="https://www.youtube.com/watch?v=YSfcLfoNkFk">shows him driving away</a>. Such lies are routine in the United States: We were told that <a href="https://pix11.com/news/exclusive-police-report-in-eric-garners-death-conflicts-with-videos-witnesses/">Eric Garner</a> died of a heart attack and <a href="https://www.cnn.com/2021/04/21/us/minneapolis-police-george-floyd-death/index.html">George Floyd</a> of “medical distress.” Then we saw the videos.</p>

  


  


  <p data-word-count="27" data-uri="slate.com/_components/slate-paragraph/instances/clka44lcs00113b6wakreh9nm@published">Such videos may be imperfect witnesses. Prosecutors may ignore them, attorneys may manipulate them, judges and jurors may misread them. But they give justice a fighting chance.</p>

  <p data-word-count="89" data-uri="slate.com/_components/slate-paragraph/instances/clka44lew00123b6w39kpejke@published">The Florida bill, backed by Gov. Ron DeSantis, is likely to be passed at the next legislative session. <a href="https://www.kalb.com/2023/06/28/gov-edwards-vetoes-state-rep-mike-johnsons-house-bill-85/">Johnson will reintroduce</a> the Louisiana bill after the Democratic governor who vetoed it leaves office in January. These new laws are marching forward, one state at a time. They are brazen attempts to silence protest, designed to dodge the Constitution and cover up criminal violence. They are assaults on the First Amendment’s central function, the sine qua non of democracy: the freedom to protest abuses of power. They must not <span>pass.</span></p>

  

  

</div>

      <ul>
<li>
            <a href="https://slate.com/tag/black-lives-matter">
              Black Lives Matter
            </a>
          </li><li>
            <a href="https://slate.com/tag/criminal-justice">
              Criminal Justice
            </a>
          </li><li>
            <a href="https://slate.com/tag/florida">
              Florida
            </a>
          </li><li>
            <a href="https://slate.com/tag/indiana">
              Indiana
            </a>
          </li><li>
            <a href="https://slate.com/tag/jurisprudence">
              Jurisprudence
            </a>
          </li><li>
            <a href="https://slate.com/tag/arizona">
              Arizona
            </a>
          </li><li>
            <a href="https://slate.com/tag/ron-desantis">
              Ron DeSantis
            </a>
          </li>      </ul>

  </section>

      

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Project Aria 'Digital Twin' Dataset by Meta (138 pts)]]></title>
            <link>https://www.projectaria.com/datasets/adt/</link>
            <guid>36800041</guid>
            <pubDate>Thu, 20 Jul 2023 13:21:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.projectaria.com/datasets/adt/">https://www.projectaria.com/datasets/adt/</a>, See on <a href="https://news.ycombinator.com/item?id=36800041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="project-aria-content"><header id="u_0_2_Nk"><div><p><a href="https://www.projectaria.com/" data-ms="{&quot;creative&quot;:&quot;logo&quot;,&quot;creative_detail&quot;:&quot;click_internal-link_logo&quot;}"><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/348237963_660017879294690_4357723783463620244_n.png?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=UD93VuRidvoAX8uoFUm&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBrZejyEjBoL5IUYzjy8RN2bSyX69A4wNu7sc2_PylEPQ&amp;oe=64BEA65B" alt="Aria logo"></a></p><ul><li><a href="https://www.projectaria.com/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;home&quot;}" target="_self">Home</a></li><li><a href="https://www.projectaria.com/glasses/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;glasses&quot;}" target="_self">Glasses</a></li><li data-key="2" aria-expanded="false"><a href="#" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;datasets&quot;}" target="_self" role="button">Datasets</a><svg width="9" height="6" viewBox="0 0 9 6" fill="none"><path d="M1 1L4.5 4.5L8 1" stroke="white" stroke-linecap="round"></path></svg></li><li><a href="https://www.projectaria.com/research-kit/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;partners&quot;}" target="_self">Partners</a></li><li data-key="4" aria-expanded="false"><a href="#" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;Resources&quot;}" target="_blank" rel="noreferrer noopener" role="button" id="u_0_3_eG">Resources</a><svg width="9" height="6" viewBox="0 0 9 6" fill="none"><path d="M1 1L4.5 4.5L8 1" stroke="white" stroke-linecap="round"></path></svg></li><li><a href="https://github.com/facebookresearch/projectaria_tools" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;github&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_4_ZN">GitHub</a></li></ul></div><div><div data-key="2"><ul><li data-key="0"><a href="https://www.projectaria.com/datasets/apd/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;aria-pilot-dataset&quot;}" target="_self">Aria Pilot Dataset</a></li><li data-key="1"><a href="https://www.projectaria.com/datasets/adt/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;aria-digital-twin&quot;}" target="_self">Aria Digital Twin</a></li><li data-key="2"><a href="https://www.projectaria.com/datasets/ase/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;aria-synthetic-environments&quot;}" target="_self">Aria Synthetic Environments</a></li></ul><ul><li data-key="0"><a href="https://www.projectaria.com/datasets/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;learn-about-datasets&quot;}" target="_self">Learn About Datasets</a></li><li data-key="1"><a href="https://www.projectaria.com/challenges/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;learn-about-challenges&quot;}" target="_self">Learn About Challenges</a></li></ul></div><div data-key="4"><ul><li data-key="0"><a href="https://facebookresearch.github.io/projectaria_tools/docs/intro" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;Docs&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_5_/7">Docs</a></li><li data-key="1"><a href="https://drive.google.com/file/d/1eAgYMXbI6zNtTC6c9eEOctMGG8u43rJS/view" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;paper&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_6_KX">Paper</a></li></ul></div></div><div aria-expanded="false" id="u_0_7_7K"><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/348237963_660017879294690_4357723783463620244_n.png?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=UD93VuRidvoAX8uoFUm&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBrZejyEjBoL5IUYzjy8RN2bSyX69A4wNu7sc2_PylEPQ&amp;oe=64BEA65B" alt="Aria logo"></p></div><ul><li><a href="https://www.projectaria.com/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;home&quot;}" target="_self">Home</a></li><li><a href="https://www.projectaria.com/glasses/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;glasses&quot;}" target="_self">Glasses</a></li><li data-key="2"><svg width="9" height="6" viewBox="0 0 9 6" fill="none"><path d="M1 1L4.5 4.5L8 1" stroke="white" stroke-linecap="round"></path></svg></li><div data-key="2"><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/348237963_660017879294690_4357723783463620244_n.png?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=UD93VuRidvoAX8uoFUm&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBrZejyEjBoL5IUYzjy8RN2bSyX69A4wNu7sc2_PylEPQ&amp;oe=64BEA65B" alt="Aria logo"></p></div><div><li>Datasets</li><ul><li data-key="0"><a href="https://www.projectaria.com/datasets/apd/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;aria-pilot-dataset&quot;}" target="_self">Aria Pilot Dataset</a></li><li data-key="1"><a href="https://www.projectaria.com/datasets/adt/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;aria-digital-twin&quot;}" target="_self">Aria Digital Twin</a></li><li data-key="2"><a href="https://www.projectaria.com/datasets/ase/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;aria-synthetic-environments&quot;}" target="_self">Aria Synthetic Environments</a></li></ul><ul><li data-key="0"><a href="https://www.projectaria.com/datasets/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;learn-about-datasets&quot;}" target="_self">Learn About Datasets</a></li><li data-key="1"><a href="https://www.projectaria.com/challenges/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;learn-about-challenges&quot;}" target="_self">Learn About Challenges</a></li></ul></div></div><li><a href="https://www.projectaria.com/research-kit/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;partners&quot;}" target="_self">Partners</a></li><li data-key="4"><svg width="9" height="6" viewBox="0 0 9 6" fill="none"><path d="M1 1L4.5 4.5L8 1" stroke="white" stroke-linecap="round"></path></svg></li><div data-key="4"><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/348237963_660017879294690_4357723783463620244_n.png?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=UD93VuRidvoAX8uoFUm&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBrZejyEjBoL5IUYzjy8RN2bSyX69A4wNu7sc2_PylEPQ&amp;oe=64BEA65B" alt="Aria logo"></p></div><li><a href="https://github.com/facebookresearch/projectaria_tools" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_header&quot;,&quot;creative_detail&quot;:&quot;github&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_b_sg">GitHub</a></li></ul></div></header><div><p>A real-world dataset, with hyper-accurate digital counterpart &amp; comprehensive ground-truth annotation</p></div><div><div><h5>WHAT IS IT?</h5><p>An egocentric dataset with extensive and accurate ground-truth</p><div><p>Aria Digital Twin is an egocentric dataset captured using Aria glasses, with extensive simulated ground truth for devices, objects and environment.</p><p>This dataset sets a new standard for egocentric machine perception research, and accelerates research into a number of challenges including 3D object detection and tracking, scene reconstruction and understanding, sim-to-real learning, human pose prediction, while also inspiring new machine perception tasks for augmented reality (AR) applications.</p></div><a data-ga-category="c8" data-ga-label="learn-more-about-project-aria" href="#download-dataset" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal-link&quot;,&quot;creative_detail&quot;:&quot;Download the dataset below&quot;}" target="_self"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="#2C0D00"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#2C0D00"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#2C0D00"></path></svg><h5>DOWNLOAD THE DATASET BELOW</h5></a><div><h4>Dataset Content</h4><div><ul><li>200 sequences (~400 mins)</li><li>398 objects (324 stationary, 74 dynamic)</li><li>2 real indoor scenes</li><li>Single + multi-user activities</li></ul></div><h4>Sensor Data per device</h4><div><ul><li>2 x outward-facing monochrome camera streams</li><li>1 x outward-facing RGB camera stream</li><li>2 x IMU streams</li><li>2 x Internal-facing eye tracking cameras</li><li>Complete sensor calibrations</li></ul></div><h4>Annotations</h4><div><ul><li>6DoF device trajectory</li><li>3D object pose</li><li>3D human skeleton</li><li>3D eye gaze</li><li>2D Photo-realistic synthetic rendering</li><li>2D bounding box</li><li>2D instance segmentation</li><li>2D depth map</li></ul></div></div></div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="" id="u_0_k_2H"></p></div></div><div><div><h5>HOW WAS IT CREATED?</h5><p>A digital twin for a physical world</p><p>The Aria Digital Twin Dataset was captured in 2 different locations within Meta offices in North America, each with extensive ground-truth survey.</p></div><div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350317528_2601383460028681_2770023034813992523_n.jpg?_nc_cat=111&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=nUufpzD3Pd4AX9IHH2l&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfCKe_t6L0gWUP1O_ziyK0rZyfYOgShtxkp9d_8VFyYHmQ&amp;oe=64BDEBD3" alt="" id="u_0_l_jc"></p><div><p>Photo-realistic object reconstruction</p><p>Each object within the Aria Digital Twin are laser scanned to reconstruct highly precise geometry. Object material are modeled using a photogrammetry pipeline and fine-tuned to ensure the images rendered from the models accurately match real images of the object.</p></div></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350970329_3430970527165884_1016978064637225669_n.jpg?_nc_cat=102&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=y7OOeRUBrGQAX-1eBDG&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfCaDttD5Mpp3UP8XiyHQJ-wMPqTogCHIkgnptxRYim2qw&amp;oe=64BE3C65" alt="" id="u_0_m_5m"></p><div><p>Hyper-accurate scene digitization</p><p>The Aria Digital Twin Dataset was captured in 2 different locations within Meta offices in North America. Each room was laser scanned &amp; modelled to ensure a high quality ground truth for each environment.</p></div></div></div></div><div><div><h5>HOW IS THE DATASET ANNOTATED?</h5><p>Comprehensive ground-truth of the real-world environment</p><p>For every frame of motion in the real-world footage, the Aria Digital Twin Dataset has a complete set of ground-truth data at the human, object, and scene level.</p></div><div><div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350990263_942180560335987_3637854262120131676_n.jpg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=As6Wtaji2noAX8TUy9S&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfDQPTQY7e9lYnE1ZDl4MJJjJNGRi04ewUwo2kSvIJkW4A&amp;oe=64BE48D7" alt="" id="u_0_n_cB"></p><div><p>High-quality device and object 6DoF poses</p><p>Camera and object trajectories are provided for every sequence, aligned to the same reference-frame as the scene geometry, allowing annotations to be understood within the same context.</p></div></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350851399_6403222636458407_5156413340978131931_n.jpg?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=zi27eiMTDbEAX_NT6CB&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfDjtN1b-kciAfXTHN1uQD7wnhw5vvBvqQtiK_5BnTFviQ&amp;oe=64BD8077" alt="" id="u_0_o_ev"></p><div><p>High quality depth-maps and object segmentation</p><p>Aria Digital Twin derives depth maps and object segmentation by leveraging the complete scene reconstruction and dynamic object tracking. This data provides researchers with additional knowledge of objects and scene.</p></div></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350852660_584456103826983_590547595068134511_n.jpg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=b_7Quxo0puoAX90xzQW&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfDW3zNMFeSsQqzEKR4i81lPFY06O2T8nVkT5bvinR-7BQ&amp;oe=64BE09B4" alt="" id="u_0_p_Vv"></p><div><p>3D human poses</p><p>In addition to camera poses, each Aria wearer is outfitted with a full body motion capture suit, to estimate the joint positions of the wearer. This allows dataset users to explore methods for full body pose estimation.</p></div></div></div><div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350960577_230485879731797_5599307775926344381_n.jpg?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=5cThEwqt9l8AX9fd845&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfDYzCGcq3r2MGSTFmySg5mi9qsWzB1s8xDvbjDDbHziZg&amp;oe=64BD7C6D" alt="" id="u_0_q_FN"></p><div><p>Faithfully-simulated synthetic sensor data</p><p>Each real-world sequence is accompanied by a synthetic sequence matching the sensor characteristics of the RGB and monochrome sensors on Aria glasses, at photo-realistic quality.</p></div></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350807087_805798264013804_6615975676322708521_n.jpg?_nc_cat=106&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=Yj7eciTaUrMAX-l1WX_&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBhsyw2ozeUbcFo-2MsghiF6z6eO-7vFb8eT29bh7wZgw&amp;oe=64BD9D95" alt="" id="u_0_r_Pd"></p><div><p>3D eye gaze vectors</p><p>Using data from Project Aria’s eye-tracking cameras, Aria Digital Twin includes an estimate of the wearer's eye-gaze as a 3D vector with depth information. This introduces additional user-object interaction besides hands.</p></div></div></div></div></div><div><div><h5>ARIA DIGITAL TWIN DATASET TOOLS</h5><p>Comprehensive tools to load and visualize data easily</p><div><p>Tools for working with Aria Digital Twin allow researchers to access, interact with, and visualize all raw data and annotations available in the dataset.</p><p>We provide both C++ and python interfaces to load data, so that researchers can access data in the way best suited to their needs. We also provide tools for querying dataset contents, so that specific types of data can be surfaced.</p></div><a data-ga-category="c8" data-ga-label="responsible-innovation-principles" href="https://github.com/facebookresearch/projectaria_tools/tree/main/projects/AriaDigitalTwinDatasetTools" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal-link&quot;,&quot;creative_detail&quot;:&quot;view-aria-digital-twin-tools&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_s_mt"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="#18EED4"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#18EED4"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#18EED4"></path></svg><h5>VIEW ARIA DIGITAL TWIN TOOLS ON GITHUB</h5></a></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350935573_795907125172765_9048504922745545510_n.jpg?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=SHF0HJMFgugAX_8kKr9&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfA3yJb9dLXqrbMVEfg5JOxYXu2IHXRhodk7r8ujuuROMw&amp;oe=64BE733F" alt="" id="u_0_t_tp"></p></div></div><div><div><h5>HOW WILL THE DATASET BE USED?</h5><p>Use Aria Digital Twin to participate in Object Detection Challenges</p><div><p>Aria Digital Twin is designed to catalyze research related to object detection and spatialization.</p><p>Use the dataset to participate in open benchmark challenges and accelerate progress with the open community.</p></div><a data-ga-category="c8" data-ga-label="responsible-innovation-principles" href="https://www.projectaria.com/challenges/detection/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal-link&quot;,&quot;creative_detail&quot;:&quot;object-detection-challenges&quot;}" target="_self"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="#2C0D00"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#2C0D00"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#2C0D00"></path></svg><h5>VIEW THE OBJECT DETECTION ARIA CHALLENGE</h5></a></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350960283_1743429122780783_2418185418004781374_n.jpg?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=rajmx5k6wOoAX_kNlHg&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfAZ5RQwuaiKaJuKhTETn5rxmNZrCUiCEZ8OGSq2Hji0dA&amp;oe=64BF4734" alt="" id="u_0_u_rv"></p></div></div><div><div><p>Enabling innovation, responsibly</p><p>All sequences within the Aria Digital Twin Dataset have been captured using fully consented researchers in controlled environments in Meta offices.</p><a data-ga-category="c8" data-ga-label="responsible-innovation-principles" href="https://about.meta.com/metaverse/responsible-innovation/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal-link&quot;,&quot;creative_detail&quot;:&quot;responsible-innovation-principles&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_v_hN"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="#2C0D00"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#2C0D00"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#2C0D00"></path></svg><h5>RESPONSIBLE INNOVATION PRINCIPLES</h5></a></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350814322_285915773793521_3186947146087364278_n.jpg?_nc_cat=106&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=FVvmr0X9JkUAX9PZza2&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfDQAgMjlvrYKouH2QKX4QgGKRao9ZG6Uwl7uSZHDW-TgA&amp;oe=64BD7410" alt="" id="u_0_w_IJ"></p></div></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/350845397_921462402488827_2281702135775367083_n.jpg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=S_fDOFx6txUAX9LHmQB&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfDH7M4Io-C7jvRjpggkE7zp1RsBQZ6xT2xW0O2bZc9F-g&amp;oe=64BE9689" alt="" id="u_0_y_Im"></p></div><div><div><p>Access Aria Digital Twin Dataset</p><p>If you are a researcher in AI or ML research, access the Aria Digital Twin Dataset and accompanying tools here.</p></div><div><p>By submitting your email and accessing the Aria Digital Twin Dataset, you agree to abide by the <a data-ga-category="c8" data-ga-label="project-aria-pilot-dataset-form" href="https://www.projectaria.com/datasets/adt/license/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal-link&quot;,&quot;creative_detail&quot;:&quot;project-aria-pilot-dataset-form&quot;}" target="_self">dataset license agreement</a> and to receive emails in relation to the dataset.</p></div></div><div><h2>Subscribe to Project Aria Updates</h2><p>Stay in the loop with the latest datasets and challenges from Project Aria.</p><a href="http://eepurl.com/ipQYdI" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;button&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie"><p>SIGN UP FOR EMAIL UPDATES</p></a></div><div><p><a data-ga-category="c8" data-ga-label="footer_meta-link" href="https://www.projectaria.com/" aria-label="Meta logo" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_footer&quot;,&quot;creative_detail&quot;:&quot;footer_meta-link&quot;}" target="_self"><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/250802775_421829349379303_5128561417639041188_n.png?_nc_cat=1&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=m8-u9jirGyIAX8HKqVk&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBChbDiVKaQ5f57vHOaj4XkNkzd8oNFvycVZMG2HPTNIA&amp;oe=64BE7239" height="17" width="87" alt="Meta logo, homepage link" id="u_0_10_qC"></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No-more-secrets: recreate the decryption effect seen in the 1992 movie Sneakers (704 pts)]]></title>
            <link>https://github.com/bartobri/no-more-secrets</link>
            <guid>36799776</guid>
            <pubDate>Thu, 20 Jul 2023 12:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bartobri/no-more-secrets">https://github.com/bartobri/no-more-secrets</a>, See on <a href="https://news.ycombinator.com/item?id=36799776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/52a166beedb33d757d4d8508d58e75f59ca0ef3fb07ca710cbb75f48bedec82e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302e312d677265656e2e737667"><img src="https://camo.githubusercontent.com/52a166beedb33d757d4d8508d58e75f59ca0ef3fb07ca710cbb75f48bedec82e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302e312d677265656e2e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-1.0.1-green.svg"></a></p>
<p dir="auto">Like this project? Consider tipping me: <a href="https://github.com/sponsors/bartobri">https://github.com/sponsors/bartobri</a></p>
<h2 tabindex="-1" dir="auto">No More Secrets</h2>
<p dir="auto">This project provides a command line tool called <code>nms</code> that recreates the
famous data decryption effect seen on screen in the 1992 hacker movie Sneakers.
For reference, you can see this effect at 0:35 in <a href="https://www.youtube.com/watch?v=F5bAa6gFvLs&amp;t=35" rel="nofollow">this movie clip</a>.</p>
<p dir="auto">This command works on piped data. Pipe any ASCII or UTF-8 text to <code>nms</code>,
and it will apply the Hollywood effect, initially showing encrypted data,
then starting a decryption sequence to reveal the original plain-text characters.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/68bbd6c555003c61a5e865edaf317f370a7b35544abb59a85ce179fde58f626f/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f6e6d732e676966"><img src="https://camo.githubusercontent.com/68bbd6c555003c61a5e865edaf317f370a7b35544abb59a85ce179fde58f626f/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f6e6d732e676966" alt="Screenshot" data-animated-image="" data-canonical-src="https://www.brianbarto.info/static/nms/nms.gif"></a></p>
<p dir="auto">Also included in this project is a program called <code>sneakers</code> that recreates
what we see in the above movie clip. Note that this program requires the
user to select one of the menu options before it terminates.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b1a797b0e5dc49c8f1714bad79660a89b2472cd67576fd1dfddeb03036f1601c/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f736e65616b6572732e676966"><img src="https://camo.githubusercontent.com/b1a797b0e5dc49c8f1714bad79660a89b2472cd67576fd1dfddeb03036f1601c/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f736e65616b6572732e676966" alt="Screenshot" data-animated-image="" data-canonical-src="https://www.brianbarto.info/static/nms/sneakers.gif"></a></p>
<p dir="auto">By default, this project has no dependencies, but it does rely on ANSI/VT100
terminal escape sequences to recreate the effect. Most modern terminal
programs support these sequences so this should not be an issue for most
users. If yours does not, this project also provides a ncurses implementation
which supports non-ANSI terminals, but at the expense of losing the inline
functionality (ncurses will always clear the screen prior to displaying output).</p>
<h2 tabindex="-1" dir="auto">Table of Contents</h2>
<ol dir="auto">
<li><a href="#download-and-install">Download and Install</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#the-nms-library">The NMS Library</a></li>
<li><a href="#license">License</a></li>
</ol>
<h2 tabindex="-1" dir="auto">Download and Install</h2>
<p dir="auto">More and more Unix/Linux platforms are including this project in their
package manager. You may wish to search your package manager to see if it
is an installation option. If you install from a package manager, please
check that you have the latest version (<code>nms -v</code>). If not, I suggest
installing from source by following the instructions below.</p>
<p dir="auto">To install this project from source, you will need to have the tools <code>git</code>,
<code>gcc</code>, and <code>make</code> to download and build it. Install them from your package
manager if they are not already installed.</p>
<p dir="auto">Once you have the necessary tools installed, follow these instructions:</p>
<h4 tabindex="-1" dir="auto">Install:</h4>
<div data-snippet-clipboard-copy-content="$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms
$ make sneakers             ## Optional
$ sudo make install"><pre><code>$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms
$ make sneakers             ## Optional
$ sudo make install
</code></pre></div>
<h4 tabindex="-1" dir="auto">Uninstall:</h4>

<h4 tabindex="-1" dir="auto">Install with Ncurses Support</h4>
<p dir="auto">If your terminal does not support ANSI/VT100 escape sequences, the effect
may not render properly. This project provides a ncurses implementation
for such cases. You will need the ncurses library installed. <a href="https://github.com/bartobri/no-more-secrets/blob/master/NCURSES.md">Install this
library from your package manager</a>. Next, follow these instructions:</p>
<div data-snippet-clipboard-copy-content="$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms-ncurses
$ make sneakers-ncurses     ## Optional
$ sudo make install"><pre><code>$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms-ncurses
$ make sneakers-ncurses     ## Optional
$ sudo make install
</code></pre></div>
<h2 tabindex="-1" dir="auto">Usage</h2>
<p dir="auto"><code>nms</code> works on piped data. Pipe any ASCII or UTF-8 characters to it and
enjoy the magic. In the below examples, I use a simple directory listing.</p>
<div data-snippet-clipboard-copy-content="$ ls -l | nms
$ ls -l | nms -a           // Set auto-decrypt flag
$ ls -l | nms -s           // Set flag to mask space characters
$ ls -l | nms -f green     // Set foreground color to green
$ ls -l | nms -c           // Clear screen
$ nms -v                   // Display version"><pre><code>$ ls -l | nms
$ ls -l | nms -a           // Set auto-decrypt flag
$ ls -l | nms -s           // Set flag to mask space characters
$ ls -l | nms -f green     // Set foreground color to green
$ ls -l | nms -c           // Clear screen
$ nms -v                   // Display version
</code></pre></div>
<p dir="auto">Note that by default, after the initial encrypted characters are displayed,
<code>nms</code> will wait for the user to press a key before initiating the decryption
sequence. This is how the it is depicted in the movie.</p>
<h4 tabindex="-1" dir="auto">Command Line Options</h4>
<p dir="auto"><code>-a</code></p>
<p dir="auto">Set the auto-decrypt flag. This will automatically start the
decryption sequence without requiring a key press.</p>
<p dir="auto"><code>-s</code></p>
<p dir="auto">Set a flag to mask space characters. This will only mask single blank space
characters. Other space characters such as tabs and newlines will not be masked.</p>
<p dir="auto"><code>-f &lt;color&gt;</code></p>
<p dir="auto">Set the foreground color of the decrypted text to the color
specified. Valid options are white, yellow, black, magenta, blue, green,
or red. This is blue by default.</p>
<p dir="auto"><code>-c</code></p>
<p dir="auto">Clear the screen prior to printing any output. Specifically,
it saves the state of the terminal (all current output), and restores it
once the effect is completed. Note that when using this option, <code>nms</code> requires
the user to press a key before restoring the terminal.</p>
<p dir="auto"><code>-v</code></p>
<p dir="auto">Display version info.</p>
<h2 tabindex="-1" dir="auto">The NMS Library</h2>
<p dir="auto">For those who would like to use this effect in their own projects, I have
created a C library that provides simple interface and can easily be used
for any program that runs from the command line.</p>
<p dir="auto">See <a href="https://github.com/bartobri/libnms">LibNMS</a> for more info.</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">This program is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License. See <a href="https://github.com/bartobri/no-more-secrets/blob/master/LICENSE">LICENSE</a> for
more details.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anytype – open-source, local-first, P2P Notion alternative (291 pts)]]></title>
            <link>https://anytype.io/?hn</link>
            <guid>36799548</guid>
            <pubDate>Thu, 20 Jul 2023 12:30:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anytype.io/?hn">https://anytype.io/?hn</a>, See on <a href="https://news.ycombinator.com/item?id=36799548">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The past is not true (308 pts)]]></title>
            <link>https://sive.rs/pnt</link>
            <guid>36798854</guid>
            <pubDate>Thu, 20 Jul 2023 10:52:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sive.rs/pnt">https://sive.rs/pnt</a>, See on <a href="https://news.ycombinator.com/item?id=36798854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<header>


<small>2023-07-20</small>
</header>

<p>
	When I was 17, I was driving recklessly and crashed into an oncoming car.
	I found out that I broke the other driver’s spine, and she’ll never walk again.
</p><p>
	I carried that burden with me everywhere, and felt so horrible about it for so many years that at age 35 I decided to find this woman to apologize.
	I found her name and address, went to her house, knocked on the door, and a middle-aged woman answered.
	As soon as I said, “I’m the teenager that hit your car eighteen years ago and broke your spine”, I started sobbing - a big ugly cry, surfacing years of regret.
	She was so sweet, and hugged me saying, “Oh sweetie, sweetie! Don’t worry. I’m fine!”
	Then she walked me into her living room.
	Walked.
</p><p>
	Turns out the news had been miscommunicated.
	Yes she fractured a couple vertebrae but it never stopped her from walking.
	She said “that little accident” helped her pay more attention to her fitness, lose weight, and since then has been in better health than ever.
	Then she apologized for causing the accident in the first place.
	Apologized.
</p><p>
	I said, “Well, no, it was my fault for ignoring the yield sign.”
</p><p>
	She said, “No, it was my fault because I was eating while driving and not watching the road. You didn’t hit me. I hit you.”
</p><p>
	Seems we had both been told the accident was our fault, and had spent eighteen years feeling bad about it.
	This time she started crying, sniffled, grabbed a tissue to wipe her eyes and said, “It’s so <em>stupid</em> - these stories.”
</p>
<hr>
<p>
	Aim a laser pointer at the moon, then move your hand the tiniest bit, and it’ll move a thousand miles at the other end.
	The tiniest misunderstanding long ago, amplified through time, leads to piles of misunderstandings in the present.
</p><p>
	We think of the past like it’s a physical fact - like it’s real.
	But the past is what we call our memory and stories about it.
	Imperfect memories, and stories built on one interpretation of incomplete information.
	That’s “<em>the</em> past”.
</p><p>
	History is not true.
<strong>
	You can change history.
</strong>
	The actual factual events are such a small part of the story.
	Everything else is interpretation.
</p><p>
	It’s never too late to change a story.
</p>
<figure>
<a href="https://www.flickr.com/photos/ashclements/290493334/"><img src="https://sive.rs/images/crashedcar.jpg" alt="crashed car photo by Ashley Jonathan Clements"></a>
<figcaption>photo by <a href="https://www.flickr.com/photos/ashclements/290493334/">Ashley Jonathan Clements</a></figcaption>
</figure>


</article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cool Retro Terminal (237 pts)]]></title>
            <link>https://github.com/Swordfish90/cool-retro-term</link>
            <guid>36798774</guid>
            <pubDate>Thu, 20 Jul 2023 10:37:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Swordfish90/cool-retro-term">https://github.com/Swordfish90/cool-retro-term</a>, See on <a href="https://news.ycombinator.com/item?id=36798774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">cool-retro-term</h2>
<table>
<thead>
<tr>
<th>&gt; Default Amber</th>
<th>C:\ IBM DOS</th>
<th>$ Default Green</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/121322/32070717-16708784-ba42-11e7-8572-a8fcc10d7f7d.gif"><img src="https://user-images.githubusercontent.com/121322/32070717-16708784-ba42-11e7-8572-a8fcc10d7f7d.gif" alt="Default Amber Cool Retro Term" data-animated-image=""></a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/121322/32070716-16567e5c-ba42-11e7-9e64-ba96dfe9b64d.gif"><img src="https://user-images.githubusercontent.com/121322/32070716-16567e5c-ba42-11e7-9e64-ba96dfe9b64d.gif" alt="IBM DOS" data-animated-image=""></a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/121322/32070715-163a1c94-ba42-11e7-80bb-41fbf10fc634.gif"><img src="https://user-images.githubusercontent.com/121322/32070715-163a1c94-ba42-11e7-80bb-41fbf10fc634.gif" alt="Default Green Cool Retro Term" data-animated-image=""></a></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Description</h2>
<p dir="auto">cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens.
It has been designed to be eye-candy, customizable, and reasonably lightweight.</p>
<p dir="auto">It uses the QML port of qtermwidget (Konsole): <a href="https://github.com/Swordfish90/qmltermwidget">https://github.com/Swordfish90/qmltermwidget</a>.</p>
<p dir="auto">This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.</p>
<p dir="auto">Settings such as colors, fonts, and effects can be accessed via context menu.</p>
<h2 tabindex="-1" dir="auto">Screenshots</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/26bae23283b3b91bf2409aae0de2a032408a50e97ca79aa62513b471626b5c3b/68747470733a2f2f692e696d6775722e636f6d2f544e756d6b446e2e706e67"><img src="https://camo.githubusercontent.com/26bae23283b3b91bf2409aae0de2a032408a50e97ca79aa62513b471626b5c3b/68747470733a2f2f692e696d6775722e636f6d2f544e756d6b446e2e706e67" alt="Image" data-canonical-src="https://i.imgur.com/TNumkDn.png"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/49da3e0faf874c2c0503632693a74efd0e5ee836b69821bbd676c8f8b8eebdd1/68747470733a2f2f692e696d6775722e636f6d2f68666a574f4d342e706e67"><img src="https://camo.githubusercontent.com/49da3e0faf874c2c0503632693a74efd0e5ee836b69821bbd676c8f8b8eebdd1/68747470733a2f2f692e696d6775722e636f6d2f68666a574f4d342e706e67" alt="Image" data-canonical-src="https://i.imgur.com/hfjWOM4.png"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a88479dfa171e61f02f831bff623eab504f495f61bc42b6c29830820ba0dd357/68747470733a2f2f692e696d6775722e636f6d2f47595244507a4a2e6a7067"><img src="https://camo.githubusercontent.com/a88479dfa171e61f02f831bff623eab504f495f61bc42b6c29830820ba0dd357/68747470733a2f2f692e696d6775722e636f6d2f47595244507a4a2e6a7067" alt="Image" data-canonical-src="https://i.imgur.com/GYRDPzJ.jpg"></a></p>
<h2 tabindex="-1" dir="auto">Install</h2>
<p dir="auto">If you want to get a hold of the latest version, just go to the Releases page and grab the latest AppImage (Linux) or dmg (macOS).</p>
<p dir="auto">Alternatively, most distributions such as Ubuntu, Fedora or Arch already package cool-retro-term in their official repositories.</p>
<h2 tabindex="-1" dir="auto">Building</h2>
<p dir="auto">Check out the wiki and follow the instructions on how to build it on <a href="https://github.com/Swordfish90/cool-retro-term/wiki/Build-Instructions-(Linux)">Linux</a> and <a href="https://github.com/Swordfish90/cool-retro-term/wiki/Build-Instructions-(macOS)">macOS</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Docuseal: Open-source DocuSign alternative. Create, fill, sign digital documents (437 pts)]]></title>
            <link>https://github.com/docusealco/docuseal</link>
            <guid>36798593</guid>
            <pubDate>Thu, 20 Jul 2023 10:04:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/docusealco/docuseal">https://github.com/docusealco/docuseal</a>, See on <a href="https://news.ycombinator.com/item?id=36798593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">
  <p><a href="https://www.docuseal.co/" rel="nofollow">
      <img alt="DocuSeal" src="https://github.com/docusealco/docuseal/assets/5418788/c12cd051-81cd-4402-bc3a-92f2cfdc1b06" width="80">
      <br>
    </a>
    DocuSeal
  </p>
</h2>
<h3 tabindex="-1" dir="auto">
  Open source document filling and signing
</h3>
<p dir="auto">
  <a href="https://hub.docker.com/r/docuseal/docuseal" rel="nofollow">
    <img alt="Docker releases" src="https://camo.githubusercontent.com/bf6921e570de205b15b2ffccdcfdb86a2ce7f56610b9a57d1239fe9563404387/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f646f63757365616c2f646f63757365616c" data-canonical-src="https://img.shields.io/docker/v/docuseal/docuseal">
  </a>
  <a href="https://discord.gg/qygYCDGck9" rel="nofollow">
    <img src="https://camo.githubusercontent.com/56c2ca2c4d2134ea4912b743333a79e1b57e985bfaca0e665406fd8d96932b44/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313132353131323634313137303434383435343f6c6f676f3d646973636f7264" data-canonical-src="https://img.shields.io/discord/1125112641170448454?logo=discord">
  </a>
  <a href="https://twitter.com/intent/follow?screen_name=docusealco" rel="nofollow">
    <img src="https://camo.githubusercontent.com/d31021dc3163b73ba470c870765521480715dfdc9c36fa4e2800376b87abae62/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f646f63757365616c636f3f7374796c653d736f6369616c" alt="Follow @docusealco" data-canonical-src="https://img.shields.io/twitter/follow/docusealco?style=social">
  </a>
</p>
<p dir="auto">
DocuSeal is an open source platform that provides secure and efficient digital document signing and processing. Create PDF forms to have them filled and signed online on any device with an easy-to-use, mobile-optimized web tool.
</p>
<h2 tabindex="-1" dir="auto">
  <a href="https://demo.docuseal.co/" rel="nofollow"><g-emoji alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">✨</g-emoji> Live Demo</a>
  <span>|</span>
  <a href="https://docuseal.co/sign_up" rel="nofollow"><g-emoji alias="cloud" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2601.png">☁️</g-emoji> Try in Cloud</a>
</h2>
<p dir="auto"><a href="https://demo.docuseal.co/" rel="nofollow"><gh:secured-asset-reference resource_type="UserAsset" resource_id="251969508"></gh:secured-asset-reference></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/docusealco/docuseal/assets/5418788/d8703ea3-361a-423f-8bfe-eff1bd9dbe14"><img src="https://github.com/docusealco/docuseal/assets/5418788/d8703ea3-361a-423f-8bfe-eff1bd9dbe14" alt="Demo"></a></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul>
<li> PDF form fields builder (WYSIWYG)</li>
<li> 10 field types available (Signature, Date, File, Checkbox etc.)</li>
<li> Multiple submitters per document</li>
<li> Automated emails via SMTP</li>
<li> Files storage on AWS S3, Google Storage, or Azure</li>
<li> Automatic PDF eSignature</li>
<li> PDF signature verification</li>
<li> Users management</li>
<li> Mobile-optimized</li>
<li> Easy to deploy in minutes</li>
</ul>
<h2 tabindex="-1" dir="auto">Deploy</h2>
<table>
<thead>
<tr>
<th>Heroku</th>
<th>Railway</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://heroku.com/deploy?template=https://github.com/docusealco/docuseal-heroku" rel="nofollow"><img alt="Deploy on Heroku" src="https://camo.githubusercontent.com/6979881d5a96b7b18a057083bb8aeb87ba35fc279452e29034c1e1c49ade0636/68747470733a2f2f7777772e6865726f6b7563646e2e636f6d2f6465706c6f792f627574746f6e2e737667" height="40" data-canonical-src="https://www.herokucdn.com/deploy/button.svg"></a></td>
<td><a href="https://railway.app/template/IGoDnc?referralCode=ruU7JR" rel="nofollow"><img alt="Deploy on Railway" src="https://camo.githubusercontent.com/081df3dd8cff37aab35044727b02b94a8e948052487a8c6253e190f5940d776d/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" height="40" data-canonical-src="https://railway.app/button.svg"></a></td>
</tr>
<tr>
<td><strong>DigitalOcean</strong></td>
<td><strong>Render</strong></td>
</tr>
<tr>
<td><a href="https://cloud.digitalocean.com/apps/new?repo=https://github.com/docusealco/docuseal-digitalocean/tree/master&amp;refcode=421d50f53990" rel="nofollow"><img alt="Deploy on DigitalOcean" src="https://camo.githubusercontent.com/df21703b4229f8d44f76c2d56073657a4ab450ca4566ba5d24d05bf528c298f8/68747470733a2f2f7777772e6465706c6f79746f646f2e636f6d2f646f2d62746e2d626c75652e737667" height="40" data-canonical-src="https://www.deploytodo.com/do-btn-blue.svg"></a></td>
<td><a href="https://render.com/deploy?repo=https://github.com/docusealco/docuseal-render" rel="nofollow"><img alt="Deploy to Render" src="https://camo.githubusercontent.com/3cae4655a3792a1d58dcd0e3f8815853cc88543acd4eb5d8c534ea24e0e46f89/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667" height="40" data-canonical-src="https://render.com/images/deploy-to-render-button.svg"></a></td>
</tr>
</tbody>
</table>
<h4 tabindex="-1" dir="auto">Docker</h4>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name docuseal -p 3000:3000 -v.:/data docuseal/docuseal"><pre>docker run --name docuseal -p 3000:3000 -v.:/data docuseal/docuseal</pre></div>
<p dir="auto">By default DocuSeal docker container uses an SQLite database to store data and configurations. Alternatively, it is possible use PostgreSQL or MySQL databases by specifying the <code>DATABASE_URL</code> env variable.</p>
<h4 tabindex="-1" dir="auto">Docker Compose</h4>
<p dir="auto">Download docker-compose.yml into your private server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://raw.githubusercontent.com/docusealco/docuseal/master/docker-compose.yml > docker-compose.yml"><pre>curl https://raw.githubusercontent.com/docusealco/docuseal/master/docker-compose.yml <span>&gt;</span> docker-compose.yml</pre></div>
<p dir="auto">Run the app under a custom domain over https using docker compose (make sure your DNS points to the server to automatically issue ssl certs with Caddy):</p>
<div dir="auto" data-snippet-clipboard-copy-content="HOST=your-domain-name.com docker-compose up"><pre>HOST=your-domain-name.com docker-compose up</pre></div>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">DocuSeal is released under the GNU Affero General Public License v3.0.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MDN Playground (216 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/play</link>
            <guid>36798157</guid>
            <pubDate>Thu, 20 Jul 2023 08:39:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/play">https://developer.mozilla.org/en-US/play</a>, See on <a href="https://news.ycombinator.com/item?id=36798157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="references-menu" aria-labelledby="references-button"><li><a href="https://developer.mozilla.org/en-US/docs/Web"><div><p>Overview / Web Technology</p><p>Web technology reference for developers</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML"><div><p>HTML</p><p>Structure of content on the web</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS"><div><p>CSS</p><p>Code used to describe document style</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript"><div><p>JavaScript</p><p>General-purpose scripting language</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/HTTP"><div><p>HTTP</p><p>Protocol for transmitting web resources</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/API"><div><p>Web APIs</p><p>Interfaces for building web applications</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions"><div><p>Web Extensions</p><p>Developing extensions for web browsers</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web"><div><p>Web Technology</p><p>Web technology reference for developers</p></div></a></li></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Happened to Dolphin on Steam? (225 pts)]]></title>
            <link>https://dolphin-emu.org/blog/2023/07/20/what-happened-to-dolphin-on-steam/</link>
            <guid>36798092</guid>
            <pubDate>Thu, 20 Jul 2023 08:29:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dolphin-emu.org/blog/2023/07/20/what-happened-to-dolphin-on-steam/">https://dolphin-emu.org/blog/2023/07/20/what-happened-to-dolphin-on-steam/</a>, See on <a href="https://news.ycombinator.com/item?id=36798092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header> 
<img src="https://dolphin-emu.org/m/user/blog/steam-release/steamwhat.jpg"> 
<img src="https://dolphin-emu.org/m/user/blog/steam-release/steamwhatmini.jpg"> 
</header>

<p>Well that blew up, huh?  If you follow emulation or just gaming on the whole, you've probably heard about the controversy around the Dolphin Steam release and the Wii Common Key.  There's been a lot of conclusions made, and while we've wanted to defend ourselves, we thought it would be prudent to contact lawyers first to make sure that our understanding of the situation was legally sound. That took some time, which was frustrating to ourselves and to our users, but now we are educated and ready to give an informed response.</p>
<p><a id="cuthere"></a>
We'd like to thank Kellen Voyer of <a href="https://voyerlaw.com/">Voyer Law</a> for providing us with legal council for this matter. And to be clear, all of the analysis below is specifically regarding US law. Without further delay, let's begin.</p>
<h3 id="what-actually-happened">What actually happened?<a href="#what-actually-happened" title="Permanent link">¶</a></h3>
<p>First things first - Nintendo <strong>did not</strong> send Valve or Dolphin a Digital Millenium Copyright Act (DMCA) section 512(c) notice (commonly known as a DMCA Takedown Notice) against our Steam page. Nintendo has not taken any legal action against Dolphin Emulator or Valve.</p>
<p>What actually happened was that Valve's legal department contacted Nintendo to inquire about the announced release of Dolphin Emulator on Steam.  In reply to this, a lawyer representing Nintendo of America requested Valve prevent Dolphin from releasing on the Steam store, citing the DMCA as justification. Valve then forwarded us the statement from Nintendo's lawyers, and told us that we had to come to an agreement <em>with Nintendo</em> in order to release on Steam.  Considering the strong legal wording at the start of the document and the citation of DMCA law, we took the letter very seriously.  We wanted to take some time and formulate a response, however after being flooded with questions, we wrote a <a href="https://dolphin-emu.org/blog/2023/05/27/dolphin-steam-indefinitely-postponed/">fairly frantic statement</a> on the situation as we understood it at the time, which turned out to only fuel the fires of speculation. </p>
<p>So, after a long stay of silence, we have a difficult announcement to make.  We are abandoning our efforts to release Dolphin on Steam.  Valve ultimately runs the store and can set any condition they wish for software to appear on it. But given Nintendo's long-held stance on emulation, we find Valve's requirement for us to get <em>approval</em> from Nintendo for a Steam release to be impossible. Unfortunately, that's that.  But there are some more serious matters to discuss, some that are much bigger than Dolphin's Steam Release.</p>
<h3 id="what-about-the-key">What about the key?<a href="#what-about-the-key" title="Permanent link">¶</a></h3>
<p>Over the past few weeks, a lot has been said about Dolphin including the Wii Common Key. As you may know, Wii games are encrypted, and the Wii uses the "common key" that is burned into the console to decrypt Wii discs. Wii software does not have any access to the key whatsoever, however, some smart engineers and a pair of tweezers was all it took to extract the key. If you haven't heard this story before, we highly recommend <a href="https://media.ccc.de/v/25c3-2799-en-console_hacking_2008_wii_fail">checking out the 25c3 presentation on the actual Tweezer Exploit that gave Team Twiizers its original name</a>.  It's an incredibly entertaining video that's worth your time.  If you aren't familiar with Team Twiizers, perhaps you know them under their modern name: <a href="https://fail0verflow.com/">fail0verflow</a>.</p>
<p>The extraction of the Wii Common Key did not elicit any kind of legal response from anyone.  It was <a href="https://hackmii.com/2008/04/keys-keys-keys/">freely shared</a> <em>everywhere</em>, and eventually made its way into Dolphin's codebase <a href="https://github.com/dolphin-emu/dolphin/commit/6ff164585ce5b63ef91a4c6635e35a7fe938ea1f">more than 15 years ago</a> (committed by a Team Twiizers member no less). </p>
<p>These keys have been publicly available for years and no one has really cared.  US law regarding this has not changed, yet a lot of armchair lawyers have come out talking about how foolish we were to ship the Wii Common Key.  Fueling this is Nintendo's letter to Valve, which cites the anti-circumvention provisions of the DMCA (<a href="https://www.law.cornell.edu/uscode/text/17/1201">17 U.S.C. § 1201</a>), particularly because Dolphin has to decrypt Wii games. </p>
<blockquote>Wii and Nintendo GameCube game files, or ROMs, are encrypted using proprietary cryptographic keys. The Dolphin emulator operates by incorporating these cryptographic keys without Nintendo’s authorization and decrypting the ROMs at or immediately before runtime. Thus, use of the Dolphin emulator unlawfully “circumvent[s] a technological measure that effectively controls access to a work protected under” the Copyright Act. 17 U.S.C. § 1201(a)(1). Distribution of the emulator, whether by the Dolphin developers or other third-party platforms, constitutes unlawful “traffic[king] in a[] technology . . . that . . . is primarily designed or produced for the purpose of circumventing a technological measure . . . .” 17 U.S.C. § 1201(a)(2)(A).<sup>3</sup></blockquote>
<p>This sounds extremely bad at a glance (and we certainly had a moment of panic after first reading it), but now that we have done our homework and talked to a lawyer, we are no longer concerned.</p>
<p>We have a very strong argument that Dolphin is <strong>not</strong> primarily designed or produced for the purpose of circumventing protection.  Dolphin is designed to recreate the GameCube and Wii hardware as software, and to provide the means for a user to interact with this emulated environment. Only an <em>incredibly tiny</em> portion of our code is actually related to circumvention. Additionally, GameCube games aren't actually encrypted at all, and Dolphin can also play homebrew and is used in the development of game mods.  There are even homebrew and mods that specifically target Dolphin as its own platform, given that it has the ability to emulate more memory and processing power than is possible on the original consoles.  That's why there are "Dolphin modes" in many modern homebrew games!</p>
<p>Considering that only a small fraction of what we do involves circumvention, we think that the claim that we are "primarily for circumvention" is a <strong>reach</strong>. We do not believe this angle would be successful in a US courtroom, if it were ever to come to that. The reason the lawyers representing Nintendo would make such a leap is because they wished to create a narrative where the DMCA's exemptions do not apply to us, as these exemptions are powerful and widely in our favor. Of particular note for Dolphin is the reverse engineering exemption in <a href="https://www.law.cornell.edu/uscode/text/17/1201#e">17 U.S.C. § 1201(f)</a> which states that:</p>
<blockquote>
...a person may develop and employ technological means to circumvent a technological measure, or to circumvent protection afforded by a technological measure, in order to enable the identification and analysis under paragraph (1), or for the purpose of enabling interoperability of an independently created computer program with other programs, if such means are necessary to achieve such interoperability, to the extent that doing so does not constitute infringement under this title.
</blockquote>
<p>Dolphin is an independently created computer program that is circumventing Wii disc encryption for interoperability with Wii software. According to this exemption, this does not constitute infringement under 17 U.S.C. § 1201. This exemption even allows distribution of information collected through circumvention, like encryption keys, if it is for software interoperability.</p>
<blockquote>The information acquired through the acts permitted under paragraph (1), and the means permitted under paragraph (2), may be made available to others if the person referred to in paragraph (1) or (2), as the case may be, provides such information or means solely for the purpose of enabling interoperability of an independently created computer program with other programs, and to the extent that doing so does not constitute infringement under this title or violate applicable law other than this section.
</blockquote>
<p>17 U.S.C. § 1201(f) is a significant legal protection for emulation in the US, and it is why Nintendo has yet to legally challenge any emulator with the DMCA anti-circumvention clauses despite the law going into effect <em>25 years ago</em>. Unless a court rules that our understanding of the law is incorrect, we have every reason to believe that our decryption of Wii game discs is covered by this exemption.</p>
<p>After this situation blew up, we received <em>many</em> requests, and even some demands, to remove all Wii keys from our codebase. We're disappointed that so many people on YouTube and social media <em>didn't even consider</em> that maybe the team had done their research and risk analysis before including the keys, and just assumed that now that it was "pointed out to us" we would remove them. However, <strong>we do not think that including the Wii Common Key actually matters</strong> - the law could easily be interpreted to say that circumventing a Wii disc's encryption <em>by any means</em> is a violation.  As such, it is our interpetation that <strong>removing the Wii keys would not change whether the exemption in 17 U.S.C. § 1201(f) applies to us or not.</strong></p>
<p>In fact, we think that offloading decryption tasks onto a potential 3rd party application would make the situation worse for everyone. As such, we believe leaving the keys as they are is the best course of action.</p>
<p>And to all the armchair lawyers out there, the letter to Valve did not make any claims that we were violating a US copyright by including the Wii Common Key, as a <em>short string of entirely random letters and numbers generated by a machine</em> <a href="https://www.copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf">is not copyrightable</a> under current US copyright law. If that ever changes, the world will be far too busy to think about emulation.</p>
<h3 id="what-happens-now">What happens now?<a href="#what-happens-now" title="Permanent link">¶</a></h3>
<p><strong>We do not believe that Dolphin is in any legal danger</strong>.  We can look to the end of the message Valve forwarded to us to show this. After all of the scary language, Nintendo made no demands and made only a single request to Valve.</p>
<blockquote>We specifically request that Dolphin’s “coming soon” notice be removed and that you ensure the emulator does not release on the Steam store moving forward.</blockquote>
<p>In the end, Valve is the one running the Steam store front, and they have the right to allow or disallow anything they want on said store front for any reason.  As for Nintendo, this incident just continues their existing stance towards emulation.  We don't think that this incident should change anyone's view of either company.</p>
<p>As a silver lining, some of the features being developed for the Steam release will still work in Dolphin's normal builds, and are still being developed.  One of the features we are most excited for is a full "Big Picture" GUI that can be used directly with a controller.  That is still going to happen <em>regardless</em> of a Steam release, alongside several smaller features that were meant to be quality of life improvements for Steam builds.</p>
<p>The last thing we'd like to do before signing off is thank the developers who put a lot of effort into the Steam release. <strong><a href="https://github.com/OatmealDome">OatmealDome</a></strong> in particular was the architect of Dolphin's Steam Integration, working with Dolphin's infrastructure and Steam to take it from theory all the way to a fully-functional version of Dolphin. We'd also like to thank <strong><a href="https://github.com/delroth">delroth</a></strong> for the immense amount of CI work the past few months, which gave OatmealDome a solid foundation to build from. Finally, <strong><a href="https://github.com/MayImilae">MayImilae</a></strong> put in a large amount of media work toward the Steam release despite also working on a major upcoming feature.</p>


    
    
    

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Something in space has been lighting up every 20 minutes since 1988 (132 pts)]]></title>
            <link>https://arstechnica.com/science/2023/07/new-slow-repeating-radio-source-we-have-no-idea-what-it-is/</link>
            <guid>36797231</guid>
            <pubDate>Thu, 20 Jul 2023 05:42:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2023/07/new-slow-repeating-radio-source-we-have-no-idea-what-it-is/">https://arstechnica.com/science/2023/07/new-slow-repeating-radio-source-we-have-no-idea-what-it-is/</a>, See on <a href="https://news.ycombinator.com/item?id=36797231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/GettyImages-1291354392-800x523.jpg" alt="image of a bright blue sphere on a dark background, with spikes of light emitted by two poles.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/07/GettyImages-1291354392.jpg" data-height="1399" data-width="2142">Enlarge</a> <span>/</span> Most of the explanations for this phenomenon involve a neutron star, depicted above. These explanations are uniformly terrible.</p></figcaption>  </figure>

  




<!-- cache hit 187:single/related:66376822a460a81ec816b18266551f55 --><!-- empty -->
<p>On Wednesday, researchers announced the discovery of a new astronomical enigma. The new object, GPM J1839–10, behaves a bit like a pulsar, sending out regular bursts of radio energy. But the physics that drives pulsars means that they'd stop emitting if they slowed down too much, and almost every pulsar we know of blinks at least once per minute.</p>
<p>GPM J1839–10 takes 22 minutes between pulses. We have no idea what kind of physics or what kind of objects can power that.</p>
<h2>A persistent transient</h2>
<p>GPM J1839–10 was discovered in a search of the galactic plane for transient objects—something that's not there when you first look, but appears the next time you check. The typical explanation for a transient object is something like a supernova, where a major event gives something an immense boost in brightness. They're found at the radio end of the spectrum—<a href="https://arstechnica.com/science/2020/11/its-coming-from-inside-the-galaxy-first-fast-radio-burst-source-idd/">fast radio bursts</a>—but are also very brief, and so fairly difficult to spot.</p>                                            
                                                        
<p>In any case, GPM J1839–10 showed up in the search in a rather unusual way: It appeared as a transient item twice in the same night of observation. Rather than delivering a short burst of immense energy, such as a fast radio burst, GPM J1839–10 was much lower energy and spread out over a 30-second-long burst.</p>
<p>Follow-on observations showed that the object repeated pretty regularly, with a periodicity of about 1,320 seconds (more commonly known as 22 minutes). There's a window of about 400 seconds centered on that periodicity, and a burst can appear anywhere within the window and will last anywhere from 30 to 300 seconds. While active, the intensity of GPM J1839–10 can vary, with lots of sub-bursts within the main signal. Occasionally, a window will also go by without any bursts.</p>
<p>A search through archival data showed that signals had been detected at the site as far back as 1988. So, whatever is producing this signal is not really a transient, in the sense that the phenomenon that's producing these bursts isn't a one-time-only event.</p>
<p>The list of known objects that can produce this sort of behavior is short and consists of precisely zero items.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Memoriam: Hans Petter William Sirevåg Selasky (110 pts)]]></title>
            <link>https://lists.freebsd.org/archives/freebsd-announce/2023-July/000076.html</link>
            <guid>36797178</guid>
            <pubDate>Thu, 20 Jul 2023 05:32:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.freebsd.org/archives/freebsd-announce/2023-July/000076.html">https://lists.freebsd.org/archives/freebsd-announce/2023-July/000076.html</a>, See on <a href="https://news.ycombinator.com/item?id=36797178">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="main">
    <label for="invert"></label>
    <header>
    
    <ul>
    
    
    <li><strong><i>Go to: </i></strong> [ <a href="#footer">bottom of page</a> ] [ <a href="https://lists.freebsd.org/archives/freebsd-announce/index.html">top of archives</a> ] [ <a href="https://lists.freebsd.org/archives/freebsd-announce/2023-July/index.html">this month</a> ] </li>
    </ul>
    </header>
    <strong><i>From:</i></strong> Sergio Carlavilla  &lt;carlavilla_at_freebsd.org&gt;<br>
    <strong><i>Date:</i></strong> Thu, 20 Jul 2023 03:58:14 UTC <br>
    <pre>The FreeBSD community was saddened this month by the tragic death of
one of its most prolific contributors.  We learned that Hans Petter
Selasky passed away in a traffic accident in Lillesand, Norway on June
23, 2023 at the age of 41.  Hans was an incredibly brilliant and kind
person, and made many valuable contributions to FreeBSD.  He was
preceded in death by his father Gordon, and is survived by his mother,
Inger Elisabeth, his brothers Mark and Leif Conrad, and his nieces and
nephews Petra, David and Signe.

Hans began contributing to FreeBSD roughly 25 years ago, with fixes to
FreeBSD’s ISDN support.  He was a FreeBSD committer for nearly 15
years, and was best known for re-writing and maintaining the USB
stack.  Hans wrote the webcamd package which supports running Linux
webcam drivers in userspace on FreeBSD, and which enables those of us
using FreeBSD on the desktop to participate in modern
teleconferencing.  Most recently, he worked for Mellanox (now Nvidia)
to support their ConnectX series of high speed NICs on FreeBSD.
Hans’s work included major contributions to the kernel TLS framework,
as well as support for NIC kTLS send and receive offload in the mce(4)
driver, and many improvements to the Linux device driver compatibility
layer.

I first met Hans in 2015, in the context of his work on the mce(4)
driver for Mellanox NICs.  We worked together to make the mce(4)
driver one of highest performance NIC drivers in FreeBSD.  It was
during this time that I learned how brilliant Hans was.  He often had
ideas that sounded “crazy”, but which were actually brilliant.  One
example of this was his idea to sort incoming TCP packets using the
NIC provided RSS flow identifiers in order to present LRO with all
packets from the same TCP connection back to back.  This idea, which I
initially discounted as impractical, was crucial to Netflix being able
to meet our performance target of serving 100Gb/s of video traffic
from a single machine, and continues to save Netflix a large amount of
CPU resources.

Hans was a very kind and welcoming person.  The first time I attended
EuroBSDCon was in 2019 in Lillehammer, Norway where Hans insisted on
playing host to me.  Hans had driven across Norway from his home in
Grimstad to EuroBSDCon in Lillehammer with his father, and took me
around to see the Olympic ski jump, along with several other sites in
the town.  He then took me out to dinner, and back to the house he’d
rented with his father for an evening of great conversation.

Outside of FreeBSD, Hans’s hobbies included music and mathematics.  He
was active in his church, and contributed to its sound team.  He was a
loving and dedicated uncle to his nieces and nephews.  He loved
animals, especially his cat Pumba.

Even if you don’t use FreeBSD yourself, odds are good that Han’s work
touches on your daily life. For example, if you use a Playstation,
chances are you are using Hans’ USB stack.  If you watch Netflix, the
odds are good that the show you’re watching was delivered to you by a
ConnectX NIC running Hans’s mce(4) driver.

Hans, if you are reading this, know that you will be missed.

-- Drew Gallatin
</pre>
    
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sidebery – A Firefox extension for managing tabs and bookmarks in sidebar (158 pts)]]></title>
            <link>https://github.com/mbnuqw/sidebery</link>
            <guid>36796422</guid>
            <pubDate>Thu, 20 Jul 2023 02:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mbnuqw/sidebery">https://github.com/mbnuqw/sidebery</a>, See on <a href="https://news.ycombinator.com/item?id=36796422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Sidebery</h2>
<p dir="auto">Firefox extension for managing tabs and bookmarks in sidebar.</p>
<h2 tabindex="-1" dir="auto">Install</h2>
<p dir="auto"><strong>Stable</strong> (4.10.2):
<a href="https://github.com/mbnuqw/sidebery/releases/tag/v4.10.2">Release page</a> |
<a href="https://addons.mozilla.org/firefox/addon/sidebery/" rel="nofollow">Addon page</a> |
<a href="https://addons.mozilla.org/firefox/downloads/file/3994928/sidebery-4.10.2.xpi" rel="nofollow">Install</a><br>
<strong>Beta</strong> (v5.0.0rc4):
<a href="https://github.com/mbnuqw/sidebery/releases/tag/v5.0.0rc4">Release page</a> |
<a href="https://github.com/mbnuqw/sidebery/releases/download/v5.0.0rc4/sidebery-5.0.0rc4.xpi">Install</a></p>
<blockquote>
<p dir="auto">Note: Before installing the beta version make sure to save backup of the Add-on data (Sidebery settings / Help / Export).</p>
</blockquote>
<h2 tabindex="-1" dir="auto">About</h2>
<p dir="auto">Sidebery is a highly configurable sidebar with panels of different types. Some of the key features:</p>
<ul dir="auto">
<li>Vertical tabs panels with tree or flat layout</li>
<li>Bookmarks panels</li>
<li>(v5) History panel</li>
<li>(v5) Search in panels</li>
<li>Customizable context menu</li>
<li>Customizable styles</li>
<li>Snapshots (saved windows/panels/tabs)</li>
<li>...and more</li>
</ul>
<h2 tabindex="-1" dir="auto">Build</h2>
<blockquote>
<p dir="auto">Prerequisites: latest LTS Node.js version</p>
</blockquote>
<ol dir="auto">
<li>Install dependencies: <code>npm install</code></li>
<li>Build all parts of Add-on: <code>npm run build</code></li>
<li>Create Add-on archive in <code>./dist</code>: <code>npm run build.ext</code></li>
</ol>
<p dir="auto">After creating the Add-on archive, you can then use the version in Firefox as follows:</p>
<ol dir="auto">
<li>Open Firefox</li>
<li>Go to <code>about:debugging</code></li>
<li>Go to "This Firefox"</li>
<li>At "Temporary Extensions" click on "Load Temporary Add-on..."</li>
<li>Select the <code>.zip</code> file in the <code>dist</code> directory.</li>
<li>Close the settings tab</li>
<li>Your Firefox now always runs with the development version</li>
<li>For updating: Repeat all steps.</li>
</ol>
<h2 tabindex="-1" dir="auto">Development</h2>
<blockquote>
<p dir="auto">Prerequisites: latest LTS Node.js version</p>
</blockquote>
<p dir="auto">Install dependencies: <code>npm install</code><br>
Build and watch for changes: <code>npm run dev</code><br>
Run browser with Add-on: <code>npm run dev.run -- &lt;firefox-executable&gt;</code></p>
<h2 tabindex="-1" dir="auto">Donate</h2>
<p dir="auto">You can donate to this project, which will motivate me to answer questions, fix reported bugs, implement requested features and generally will speed up development process. Thank you.</p>
<details><summary><b> Bitcoin (BTC) </b></summary>
<div data-snippet-clipboard-copy-content="bc1q2drx3x5pfl0c68urwztvjrwgksg9u3l7mn4g4m"><pre><code>bc1q2drx3x5pfl0c68urwztvjrwgksg9u3l7mn4g4m
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/6276694/215584021-b1eee3ab-ca62-4a81-acb4-cd69c27c734a.png"><img src="https://user-images.githubusercontent.com/6276694/215584021-b1eee3ab-ca62-4a81-acb4-cd69c27c734a.png" alt="btc-bc1q2drx3x5pfl0c68urwztvjrwgksg9u3l7mn4g4m"></a></p>
</details>
<details><summary><b> Ethereum (ETH), USDT (ERC20), USDC (ERC20) </b></summary>
<div data-snippet-clipboard-copy-content="0x11667D20AB328194AEEc68F9385CCcf713607929"><pre><code>0x11667D20AB328194AEEc68F9385CCcf713607929
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/6276694/215587549-39505f92-0f80-43ec-bec1-42bf8cd570c4.png"><img src="https://user-images.githubusercontent.com/6276694/215587549-39505f92-0f80-43ec-bec1-42bf8cd570c4.png" alt="eth-0x11667D20AB328194AEEc68F9385CCcf713607929"></a></p>
</details>
<details><summary><b> USDT (TRC20), USDC (TRC20) </b></summary>
<div data-snippet-clipboard-copy-content="TJEdp1TnsN7Jfhfi9Db8yXKDK8NEUovCZb"><pre><code>TJEdp1TnsN7Jfhfi9Db8yXKDK8NEUovCZb
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/6276694/247570039-bbdefadc-3430-4537-94f1-447244d0e72f.png"><img src="https://user-images.githubusercontent.com/6276694/247570039-bbdefadc-3430-4537-94f1-447244d0e72f.png" alt="TJEdp1TnsN7Jfhfi9Db8yXKDK8NEUovCZb"></a></p>
</details>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><a href="https://github.com/mbnuqw/sidebery/blob/v5/LICENSE">MIT</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kevin Mitnick has died (3404 pts)]]></title>
            <link>https://www.dignitymemorial.com/obituaries/las-vegas-nv/kevin-mitnick-11371668</link>
            <guid>36795173</guid>
            <pubDate>Wed, 19 Jul 2023 23:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dignitymemorial.com/obituaries/las-vegas-nv/kevin-mitnick-11371668">https://www.dignitymemorial.com/obituaries/las-vegas-nv/kevin-mitnick-11371668</a>, See on <a href="https://news.ycombinator.com/item?id=36795173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                

                <h2 id="obit-name">Kevin David Mitnick</h2>

                

                    <p><img src="https://d3eguztg5751m.cloudfront.net/as/assets-mem-com/cmi/8/6/6/1/11371668/20230719_173037764_0_orig.jpg/-/kevin-mitnick-las-vegas-nv-obituary.jpg?crop=%28171.58333333333331%2C0%2C544.9583333333333%2C464%29&amp;cropxunits=696&amp;cropyunits=464&amp;maxheight=650" alt="Obituary of Kevin David Mitnick" layout="fill">
                    </p>

                

    


                <p>
Kevin David Mitnick, 59, died peacefully on Sunday, July 16, 2023, after valiantly battling pancreatic cancer for more than a year. Kevin is survived by his beloved wife, Kimberley Mitnick, who remained by his side throughout their 14-month ordeal. Kimberley is pregnant with their first child. Kevin was ecstatic about this new chapter in his and Kimberley's life together, which has now been sadly cut short.    

Kevin was preceded in death by his devoted mother, Shelly Jaffe, and his loving grandmother, Reba Vartanian, his father, Alan Mitnick, and his half-brother, Adam Mitnick. 

He is survived by his brother-in-law, Ricky Barry and his wife Roxy, and their three children: Millie, Winston, and George, his mother-in-law and father-in-law, Daisy and Andrew Tibbs, his stepmother Nanci King, his great aunt Sophie "Chickie" Leventhal and her longtime partner, Dr. Bob Berkowitz, Kevin's cousins Mitch Leventhal, Karen van den Berg, Jolie Mitnick, Mark Mitnick and Wendy Cohen.

Kevin would also want to acknowledge the tremendous love and support over the years of his dear long-time friends: Michael Morris who showed tremendous dedication to Kevin over many decades, Paul Dryman, Roy Eskapa, Shawn Nunley, Darci and Brianna Wood, Amy Gray, Alex Kasper (Kasperavicius), David Kennedy, David Fugate, Dr. Nick Spirtos, Stu Sjouwerman, and Apollo Robbins. It is impossible to list all of Kevin’s close friends. He was blessed to have so many. You know who you are. Your impact on Kevin was profound. Kevin was also very grateful for the legions of fans who in the mid-to-late 1990’s fueled the global “FREE KEVIN” movement. 

Kevin was an original; much of his life reads like a fiction story. The word that most of us who knew him would use – magnificent. 

He grew up brilliant and restless in the San Fernando Valley in California, an only child with a penchant for mischief, a defiant attitude toward authority, and a love for magic. Kevin's intelligence and delight in holding the rapt attention of audiences revealed themselves early in his childhood and continued throughout his life. In time, he transitioned from pranks and learning magic tricks to phone phreaking, social engineering, and computer hacking.  

When his desire to push boundaries led him too far astray, he landed in juvenile detention and eventually served a couple of stints in prison. His time on the FBI's Most Wanted List was well documented in his New York Times bestselling book, The Ghost in the Wires: My Adventures as the World's Most Wanted Hacker, and his other titles: The Art of Deception, The Art of Intrusion, both co-authored with William Simon, and The Art of Invisibility with Robert Vamosi.  

Kevin emerged from his final prison term, which he deemed a 'vacation,' in January 2000. He was a changed individual, and began constructing a new career, as a White Hat hacker and security consultant. He became a highly sought-after global public speaker, a writer, and established the successful Mitnick Security Consulting. In November 2011, he became the Chief Hacking Officer and part owner of security awareness training company KnowBe4, founded by close friend and business partner Stu Sjouwerman.  

Kevin attracted attention and support from unlikely sources. The bus driver who saw young Kevin memorize the bus schedules, punch cards and punch tool systems so he could ride the buses all day for free testified as a character witness for Kevin during his federal trial. The federal prosecutor offered his testimony that Kevin never tried to take one dime from any of his “victims.” The probation officer assigned to monitor Kevin after prison gave Kevin permission to write his first book on a laptop when he was not yet supposed to have access to computers. Shawn Nunley, the star witness in the FBI's case against Kevin, became so disillusioned with the government's treatment of Kevin that he contacted Kevin's defense team, helped garner Kevin's release, and became one of Kevin's dearest friends. Kevin had an irresistible way of converting foes to friends and keeping them as friends forever.  

To know Kevin was to be enthralled, exasperated, amazed, amused, irritated, and utterly charmed - in equal measure. He was insistent upon being kept updated at all times - even when it meant dozens of phone calls in a single day to the same person - just to be sure he had all the facts. He set incredibly high standards for himself and those who worked with him, and would get lost for hours in complex problems encountered in his work. He spent much of his time working with his Global Ghost Team, an elite pentesting team that spans Argentina, Spain, Germany, Canada, and beyond. Self-educated and driven by eagerness, intense drive, immense curiosity, and seemingly endless energy, he continually expanded his skills as a hacker. He was insatiable in pushing himself, and his team, to pursue excellence in their tradecraft. Kevin was a visionary and an expert at finding a way into every organization he was authorized to hack. He used this knowledge for the greater good and to develop hacking demonstrations that educated the business world and everyday people on how to protect themselves. Kevin’s body of work inspired many individuals to pursue a career in cybersecurity - the industry upon which he leaves an indelible mark and an incredible legacy.

Kevin applied that same relentless tenacity to attempting to beat pancreatic cancer.  He and Kimberley invested thousands of hours in searching for the very best treatments, finding the cutting edge research, and working with the most talented and aggressive doctors and surgeons. That search led him to the University of Pittsburgh Medical Center and Dr. Amer Zureikat, Dr. Randall Brand, and their incredible staff. Each individual did their utmost to help Kevin beat the odds and survive and for that we will always be grateful.  

Kevin was a gentleman: well-mannered and respectful, astoundingly generous with those he loved. He had a unique and unforgettable laugh - a delightful, loud, booming one - which he unleashed unexpectedly and often, frequently accompanied by a mischievous twinkle in his eyes. He saw the funny side of his compulsive perfectionism and work ethic, and enjoyed laughing at his own expense - a rare quality among the best of us. 

We knew him simply as Kev, our beloved friend, a devoted husband, and a trustworthy confidante. Kevin Mitnick crammed a dozen lifetimes into a single prematurely short one. He wanted nothing more than to live -- to keep enjoying the little "BIG" things like quality time with his wife and their growing family, his in-laws, his relatives, and his longtime friends. 

He had so much living left to do. And we know, with broken hearts, that there will never ever be anyone like him again. We will miss him for the rest of our days, hear his voice in our minds, and look forward to reconnecting with him in whatever version of the 'beyond' we each believe in. To imagine that Kev could be there to greet us, likely playing a prank, or inviting us to share an extraordinary meal and conversation, will be heaven indeed. We are each so deeply grateful for the time we had with this truly great man.  

We celebrate that a part of Kevin will live on with the upcoming birth of his and Kimberley’s child. We can only hope that the child knows, as he or she grows,  that around the world, the many friends of his father will be holding them in their hearts. 

Rest in peace, Kev, you are well loved and will be missed always.  

A private memorial and burial service will be held for close friends and family members.  

Donations can be made in Kevin's memory to The National Pancreas Foundation https://pancreasfoundation.org/ or The Equal Justice Initiative  https://eji.org/ 
These are two causes of great importance to Kimberley and Kevin; both organizations  put the majority of donated funds to work in the communities they serve.


                </p>
                <p><a href="#" onclick="return false;">
                    <span>See more</span>
                    
                </a>
            </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Danger of Popcorn Polymer: Incident at the TPC Group Chemical Plant [video] (181 pts)]]></title>
            <link>https://www.youtube.com/watch?v=6-3BFXpBcjc</link>
            <guid>36794756</guid>
            <pubDate>Wed, 19 Jul 2023 23:05:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=6-3BFXpBcjc">https://www.youtube.com/watch?v=6-3BFXpBcjc</a>, See on <a href="https://news.ycombinator.com/item?id=36794756">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[LinkedIn adopts protocol buffers and reduces latency up to 60% (177 pts)]]></title>
            <link>https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/</link>
            <guid>36794430</guid>
            <pubDate>Wed, 19 Jul 2023 22:33:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/">https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/</a>, See on <a href="https://news.ycombinator.com/item?id=36794430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<p>LinkedIn <a href="https://engineering.linkedin.com/blog/2023/linkedin-integrates-protocol-buffers-with-rest-li-for-improved-m">adopted Protocol Buffers for exchanging data between microservices</a> more efficiently across its platform and integrated it with <a href="https://linkedin.github.io/rest.li/">Rest.li</a>, their open-source REST framework. After the company-wide rollout, they reduced the latency by up to 60% and improved resource utilization at the same time.</p>

<p>The LinkedIn platform employs a microservices architecture, and for years now, <a href="https://en.wikipedia.org/wiki/JSON">JSON</a> has been used as the serialization format for over 50 thousand API endpoints exposed by microservices at LinkedIn. To help their teams build consistent interactions between services, the company created a Java framework called Rest.li, which became open-sourced.</p>

<p>The framework helps create servers and clients that use the <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a> style of communication and abstracts away many aspects of data exchange, including networking, serialization, or service discovery. It primarily supports Java and Python but can also work with Scala, Kotlin, JavaScript, Go, etc.</p>

<p><img alt="" data-src="news/2023/07/linkedin-protocol-buffers-restli/en/resources/1RestLiClientServerFlow-1689612288438.jpeg" src="https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/07/linkedin-protocol-buffers-restli/en/resources/1RestLiClientServerFlow-1689612288438.jpeg" rel="share"></p>

<p><em>Data and Control Flow Between a Rest.li Server and Client (Source: <a href="https://linkedin.github.io/rest.li/user_guide/server_architecture">Rest.li Documentation</a>)</em></p>

<p>JSON is the default serialization format in Rest.li and has been selected due to its wide language support and being human-readable. The last property, however beneficial, introduces problems from the performance (and particularly latency) point of view.</p>

<p><a href="https://www.linkedin.com/in/karthikrg/">Karthik Ramgopal</a> and <a href="https://www.linkedin.com/in/aman1309/">Aman Gupta</a>, engineers at LinkedIn, share challenges with using JSON for inter-service communication:</p>

<blockquote>
<p>The first challenge is that JSON is a textual format, which tends to be verbose. This results in increased network bandwidth usage and higher latencies, which is less than ideal. [...] The second challenge we faced was that due to the textual nature of JSON, serialization and deserialization latency and throughput were suboptimal.</p>
</blockquote>

<p>The team has been considering alternatives to JSON, looking for a compact payload size and high serialization efficiency to reduce latency and increase throughput. They also didn’t want to limit the number of supported language stacks and enable gradual migration by integrating the new serialization mechanism into Rest.li. Finally, after a comprehensive review, they decided to go with <a href="https://protobuf.dev/">Protocol Buffers (Protobuf)</a>, which scored the highest, based on the defined criteria.</p>

<p>The main difficulty around integrating Protocol Buffers into Rest.li was the dynamic schema generation based on the framework's custom schema definition system, <a href="https://linkedin.github.io/rest.li/pdl_schema">PDL</a>. The solution involved generating a symbol table that is used to generate Protobuf schema definition dynamically, but the method for delivering symbol tables varied depending on the type of client. Backend clients fetch and cache symbol tables on-demand, while for web/mobile apps, symbol tables are generated at build-time and included as versioned dependencies.</p>

<p>After changes to the framework were rolled out, the team gradually reconfigured the clients to enable Protobuf instead of JSON using HTTP headers. The result of Protocol Buffers adoption was an average increase in throughput by 6.25% for responses and 1.77% for requests. The team also observed up to 60% latency reduction for large payloads.</p>

<p><img alt="" data-src="news/2023/07/linkedin-protocol-buffers-restli/en/resources/1linkedin-restli-protobuf-1689612288438.jpeg" src="https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/07/linkedin-protocol-buffers-restli/en/resources/1linkedin-restli-protobuf-1689612288438.jpeg" rel="share"></p>

<p><em>Latency comparison between JSON and Protobuf (Source: <a href="https://engineering.linkedin.com/blog/2023/linkedin-integrates-protocol-buffers-with-rest-li-for-improved-m">LinkedIn Integrates Protocol Buffers With Rest.li for Improved Microservices Performance</a>)</em></p>

<p>Based on the learnings from the Protocol Buffers rollout, the team is planning to follow up with migration from Rest.li to <a href="https://grpc.io/">gRPC</a>, which also uses Protocol Buffers but additionally supports streaming and has a large community behind it.</p>

<p>See also the&nbsp;InfoQ Podcast:&nbsp;<a href="https://www.infoq.com/podcasts/api-showdown-rest-graphql-grpc/">API Showdown: REST vs. GraphQL vs. gRPC – Which Should You Use?</a></p>

								









  
    <div> <!-- main wrapper for authors section -->
        <h2>About the Author</h2> <!-- section title -->

        
            
                
            
            <div data-id="author-Rafal-Gancarz">
                    <h4><strong>Rafal Gancarz</strong></h4>
                    
                </div>
        
    </div>

							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The open source learning curve for AI researchers (149 pts)]]></title>
            <link>https://www.supervised.news/p/the-open-source-learning-curve-for</link>
            <guid>36793881</guid>
            <pubDate>Wed, 19 Jul 2023 21:43:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.supervised.news/p/the-open-source-learning-curve-for">https://www.supervised.news/p/the-open-source-learning-curve-for</a>, See on <a href="https://news.ycombinator.com/item?id=36793881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png" width="576" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:576,&quot;bytes&quot;:1672153,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>a group of scientists playing in a rock band in front of a large crowd — midjourney</figcaption></figure></div><p>Tri Dao, the creator of an increasingly popular technique in language model development, is running head-first into a new problem for AI researchers: working with the incredibly fast LLM open source community.</p><p><span>Dao is one of the creators of FlashAttention, a technique now adopted by some developers to increase the amount of information that can go into a context window for a large language model more efficiently. The theory goes that if you can find ways to get larger amounts of useful information into that window (</span><a href="https://arxiv.org/pdf/2307.03172.pdf" rel="">without going overboard</a><span>), you could achieve better outcomes because the model has a better idea of what you’re trying to do. FlashAttention got an update this week to FlashAttention-2.</span></p><p><span>Dao this week </span><a href="https://together.ai/blog/tri-dao-flash-attention" rel="">joined</a><span> </span><a href="https://together.ai/" rel="">Together</a><span>, a startup that aims to build open source language models and associated technology, where he’ll work as chief scientist. Together </span><a href="https://www.supervised.news/p/the-most-popular-technique-in-ai" rel="">raised $20 million </a><span>in a round earlier this year led by Lux Capital (an investor in MosaicML which was very recently acquired by Databricks for $1.3 billion). SV Angel, First Round Capital, and Jakob Uszkoreit, one of the co-writers of the Transformers paper, are also investors.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png" width="524" height="354.85164835164835" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:986,&quot;width&quot;:1456,&quot;resizeWidth&quot;:524,&quot;bytes&quot;:490622,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1456w" sizes="100vw"></picture></div></a><figcaption>Tri Dao, chief scientist of Together</figcaption></figure></div><p>FlashAttention, which speeds up training and fine-tuning of LLMs, could help solve an important piece of the puzzle for making large language models address more practical use cases. For complicated tasks, users could hypothetically inject a lot of instructions and examples for how to address the task.</p><p>Dao is now one of many academic researchers that are now finding their work catching fire on the internet among hobbyists and non-academic practitioners, with developers quickly finding widely-applicable use cases. That’s different than the typical environment of academia, which is often concerned with finding novel technologies and resolving novel use cases that don’t necessarily consider concerns around performance, cost, or practicality.</p><p>“This is probably my first exposure to open source,” he told me in an interview. “Previously we had research where almost all our papers have just made code available, but it was more on the research-y side. And then some interesting researchers would use that and improve the methods. With FlashAttention it was solving a problem that people need and immediately got integrated into a bunch of frameworks like Hugging Face and PyTorch. It’s available in PyTorch and it benefits a huge number of folks. As a result we get great feedback.”</p><p>That, of course, comes with a learning curve to shift to a more production-level mindset that scientists like Dao now face. And it’s one that AI researchers will continue to face going forward as their work gets quickly adapted and iterated on in the open source community.</p><p><span>Dao’s interest in Together started with his colleagues working at the company in the first place. Chris Ré, </span><a href="https://arxiv.org/abs/2205.14135" rel="">a co-writer on the paper for FlashAttention</a><span>, is a co-founder of Together, and co-writer Dan Fu is an academic partner of the company. Together at its core is built around open source methodologies in contrast to closed-source model developers like OpenAI.</span></p><p>“What appealed to me about Together  was their focus on open source,” Dao said. “Philosophically, I think that aligns well with what I wanted to do and how I see the future looking like. The future might not be a few companies offering really good AI models. The future is probably a bunch of really good open source models with a bunch of players in this field. The models will be accessible and lots of people can contribute and improve them, or contribute data and things like that.”</p><p>But the biggest learning curve to academics entering the open source ecosystem is that production-grade open source tools are mostly focused on practicality, rather than achieving some optimal solution without taking concerns about resources, costs, or other considerations like that into account, Bob Van Luijt, CEO of open source vector database startup Weaviate, told me.</p><p>“So, as a researcher, you might squeeze a couple of microseconds out of your&nbsp;algorithm with double the memory footprint, making the index immutable, etc. and eureka! You are state-of-the-art,” he said. “The only downside is that the rather pragmatic community doesn't really adopt it because it's unusable. Pragmatically, the cheapest to run, good enough solution often wins.”</p><p><span>Many of the most widely-used technologies in the open source ecosystem found their roots in academia. One of the most notable ones is Apache Spark, coming out of UC Berkeley. That led to the birth of Databricks, a now $38 billion company that’s </span><a href="https://www.bloomberg.com/news/articles/2023-06-13/databricks-hits-1-billion-in-annual-sales-while-adding-data-warehouse-tool?in_source=embedded-checkout-banner" rel="">generating $1 billion in revenue</a><span> annually. But it certainly wasn’t an easy transition.</span></p><p>“I think it’s been great for Databricks,” CEO Ali Ghodsi told me. “We innovate largely thanks to our academic roots. In academia it’s all about novelty.”</p><p>In the case of Spark, it built up a foundational layer for managing colossal amounts of data. Databricks has also popularize the use of data lake architectures (and the “lakehouse” paradigm by extending functionality to data warehouses) and built essentially a one-stop shop for developing and deploying machine learning models. </p><p>Both have extremely practical use cases for businesses—and can lead to even more new open source technologies, like its open source Delta Lake framework. But it also meant growing beyond producing novel technologies and solving for niche use cases and focusing more on how it could have a measurable impact on products and businesses.</p><p>“I actually actively had to put hurdles in so people would stop innovating,” Ghodsi said. “It’s so ingrained in the bloodstream of the company. The original 20 people were researchers and their modus operandi was, ‘how can we improve how people do things in a novel way?’”</p><p><span>It’s one thing to appeal to data wonks looking to derive insights from their vast piles of data and build functionality around it. It’s another thing to run head-on into a community of avid hobbyists and practitioners that find immediate use cases that aren’t just practical—they’re widely accessible and, perhaps more importantly, </span><em>fun to play with</em><span>. </span></p><p><span>Large language models have captured the attention of those same hobbyists and practitioners in the same way they’ve captured the attention of the general public. That’s partly thanks to ChatGPT, but the </span><a href="https://www.supervised.news/p/the-race-for-open-source-language" rel="">explosion of open source development around language models like LLaMA</a><span> has enabled a vast array of new fun use cases for the technology that’s very accessible to non-scientist types. </span></p><p><span>Meta </span><a href="https://www.supervised.news/p/llama-2-has-entered-the-chat" rel="">released Llama 2, the first version of LLaMA that’s available for commercial use, just yesterday</a><span>. Within just a few hours, </span><a href="https://huggingface.co/TheBloke" rel="">quantized GGML versions of Llama 2 started appearing on Hugging Face</a><span> that would work with </span><a href="https://github.com/ggerganov/llama.cpp" rel="">Llama.cpp</a><span>, a package that enables users to run the models locally on a MacBook Pro. That technology is built using ggml, </span><em><a href="https://github.com/ggerganov/ggml" rel="">another</a></em><a href="https://github.com/ggerganov/ggml" rel=""> open source tensor library</a><span> built </span><a href="https://github.com/ggerganov" rel="">by Llama.cpp creator Georgi Gerganov</a><span>.</span></p><p><span>Tools like that, though, are focused on </span><em>practicality</em><span>—sacrificing certain levels of accuracy in a large language model for the sake of running them locally on a laptop. An emphasis on novelty takes a back seat to managing the tradeoffs to create a technology that has the most widely applicable capabilities.</span></p><p>“If academia wants to adapt to users, they need to broaden the scope, which in turn conflicts with creating state-of-the-art algorithms,” Luijt said. “It's very common that the academic work that's the easiest to use causing the least amount of friction to end users wins.”</p><p><span>Together is particularly focused on that tradeoff. It created the RedPajamas data set to mimic the data set used to train the original LLaMA, </span><a href="https://together.ai/models" rel="">as well as two accompanying open source models</a><span> in the form of RedPajamas-INCITE, to address a much broader set of use cases. It’s part of the appeal of scientists joining a startup like Together, where they can potentially have a larger impact than incrementally advancing the field with advanced research.</span></p><p>But there is an enormous appeal to research and academia, especially as we start to run into walls around the performance of Transformers, the most popular technique for AI today. The restrictions around context windows is one example that FlashAttention addresses. But Dao also said he’s exploring completely novel use cases that go beyond Transformers at Together—which tries to strike a balance between research and developing practical tools.</p><p>“If we want to understand why Transformers is so good, we should try to develop alternatives and see if we can come up with something just as good,” He said. “If we can’t maybe there’s something special about Transformers. If theres evidence theres alternatives, that gives us info about what’s important for models to perform well.”</p><p data-attrs="{&quot;url&quot;:&quot;https://www.supervised.news/p/the-open-source-learning-curve-for?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.supervised.news/p/the-open-source-learning-curve-for?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><strong><a href="https://www.bloomberg.com/news/articles/2023-07-19/apple-preps-ajax-generative-ai-apple-gpt-to-rival-openai-and-google#xj4y7vzkg" rel="">Apple Tests ‘Apple GPT,’ Develops Generative AI Tools to Catch OpenAI (Bloomberg)</a><span>:</span></strong><span> Mark Gurman at Bloomberg basically confirms what we thought to be true with Apple building an in-house LLM for its products. Apple noted that it was using transformers-based approaches for language modeling in autocorrect, which was a clear indicator that there was something new under the hood. It also, per Bloomberg, built a framework called Ajax to build LLMs. Perhaps Apple is interested in personalized fine-tuned models for every user hosted directly on-device?</span></p><p><strong><a href="https://arxiv.org/abs/2307.09009" rel="">How is ChatGPT's behavior changing over time? (ArXiv)</a><span>: </span></strong><span>A team out of Stanford and Berkeley, including Databricks CTO Mateo Zaharia, investigate what seemed to be a pet theory on the internet that the quality of OpenAI’s language models was decreasing over time. The paper suggests that, indeed, that may be the case. OpenAI said it was updating its models earlier this year, including new feedback (typically RLHF) to “improve” the model. But we’re seeing lately that any changes and tuning can have a big downstream impact on the actual responses from the model.</span></p><p><strong><a href="https://www.newcomer.co/p/unstructured-raises-25-million-to" rel="">Unstructured Raises $25 Million to Bring Order to the Chaos for Language Model Data (Newcomer)</a></strong><span>: It wasn’t going to be long before we started getting new tools that make vector databases even more useful. Unstructured improves the process of transitioning company data into a vector database like Weaviate, Pinecone, or Chroma. Unstructured is getting $25 million in a round led by Madrona and including Bain Capital Ventures. Weaviate CEO Bob van Luijt and LangChain creator/CEO Harrison Chase also participated in the round. </span></p><p><strong><a href="https://www.nytimes.com/2023/07/18/technology/openai-chatgpt-facial-recognition.html" rel="">OpenAI Worries About What Its Chatbot Will Say About People’s Faces (New York Times)</a></strong><em><strong>:</strong></em><span> The Times details some reasoning behind OpenAI restricting its multimodal capabilities for GPT-4—namely, avoiding use cases that could invade privacy. But with all the major developers racing to adapt multi-modal capabilities into next-generation models it’s really only a matter of time before one comes out that’ll wade into that very complicated territory.</span></p><p><em>If you have any tips, please send me a note at m@supervised.news or contact me directly on Signal at +1-415-690-7086. As always, please send any and all feedback my way.</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geothermal Ahead of Schedule (158 pts)]]></title>
            <link>https://twitter.com/TimMLatimer/status/1681304496234991620</link>
            <guid>36793372</guid>
            <pubDate>Wed, 19 Jul 2023 21:05:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/TimMLatimer/status/1681304496234991620">https://twitter.com/TimMLatimer/status/1681304496234991620</a>, See on <a href="https://news.ycombinator.com/item?id=36793372">Hacker News</a></p>
Couldn't get https://twitter.com/TimMLatimer/status/1681304496234991620: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
    </channel>
</rss>