<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 15 Jun 2024 14:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Tesla's FSD – A Useless Technology Demo (231 pts)]]></title>
            <link>https://tomverbeure.github.io/2024/05/20/Tesla-FSD-First-and-Last-Impressions.html</link>
            <guid>40688001</guid>
            <pubDate>Sat, 15 Jun 2024 07:04:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomverbeure.github.io/2024/05/20/Tesla-FSD-First-and-Last-Impressions.html">https://tomverbeure.github.io/2024/05/20/Tesla-FSD-First-and-Last-Impressions.html</a>, See on <a href="https://news.ycombinator.com/item?id=40688001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#rules-of-engagement" id="markdown-toc-rules-of-engagement">Rules of Engagement</a></li>
  <li><a href="#test-ride-1-from-kings-beach-to-truckee-11-miles" id="markdown-toc-test-ride-1-from-kings-beach-to-truckee-11-miles">Test Ride 1: from Kings Beach to Truckee (11 miles)</a></li>
  <li><a href="#test-ride-2-i-80-from-truckee-to-blue-canyon-36-miles" id="markdown-toc-test-ride-2-i-80-from-truckee-to-blue-canyon-36-miles">Test Ride 2: I-80 from Truckee to Blue Canyon (36 miles)</a></li>
  <li><a href="#test-ride-3-from-west-valley-college-to-i-85-entrance-1-mile" id="markdown-toc-test-ride-3-from-west-valley-college-to-i-85-entrance-1-mile">Test Ride 3: from West-Valley College to I-85 Entrance (1 mile)</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In the past months, Tesla has been offering a free, one-month trial of their full 
self-driving (FSD) system to all current owners. It looks like they are rolling this 
out in stages, because I only got mine only a few days ago.</p>

<p>I’ve had a Model Y for more than 3 years now, well before Elon revealed himself as the 
kind of person he really is, and I’ve been happy with it. The odometer is now well 
above 50,000 miles, a significant part of those were spent on I-80 while driving between the 
SF South Bay and the Lake Tahoe area.</p>

<p>For long distance interstate driving, autopilot (in other words: lane centering and 
adaptive cruise control) has been amazing. I use it all the time, and have little to 
complain about. Early on, I had one case where the autopilot started slowing down 
for no good reason, but since I distrust these kind of systems and since phantom braking 
has been reported quite a bit in the press, I try to keep attention to what the car is doing 
at all times. I immediately pressed the accelerator, and that was that.</p>

<p>I don’t know how prevalent phantom breaking really is. One time is still too many, and 
disconcerting. It doesn’t help that you can’t anticipate it, there must by a bunch of different 
factors to trigger it: version of the car, weather, light conditions etc. All I can say, after 
so many miles, is that autopilot has been amazing for me.</p>

<p>When I buy my next car, my requirements will be simple: I want an EV, an extensive charger 
network along I-80, and an autosteer that’s at least as good as what I have today. Let’s hope 
there’ll be decent Tesla alternatives by then.</p>

<p>But let’s get to FSD.</p>

<p>During his last financial conference call, Musk claimed that he wants to focus more on robo-taxis. 
With no driver in the car at all, such a system better be pretty much flawless. But with the 
YouTube videos that are out there, often posted by Tesla fans, that show the system making 
ridiculous errors, I highly doubt that the system is close to ready. I would never pay for FSD, 
not only do I not trust it, I also don’t really see the point, but with with a free trial, 
I couldn’t resist checking it out.</p>

<p>Here are my impressions.</p>

<h2 id="rules-of-engagement">Rules of Engagement</h2>

<p>During 3 short tests, I watched FSD the way a helicopter parent watches a toddler who’s first 
exploring the world: allow it do what it wants to do, but intervene the moment you feel things 
are not going the way you like it.</p>

<p>It’s common on social media to see comments like this: “if the driver had waited a bit more, 
FSD would still have corrected itself.” I’m having none of that. The moment I’m sensing it’s on 
its way to do something, anything, wrong, I intervene.</p>

<p>While it’s possible that benign cases are dinged as interventions, I don’t think any what I 
describe below can be considered as such. They were real mistakes that should never have happened.</p>

<h2 id="test-ride-1-from-kings-beach-to-truckee-11-miles">Test Ride 1: from Kings Beach to Truckee (11 miles)</h2>

<p>I first switched on FSD for an 11 mile drive 
from a mountain biking trailhead in Kings Beach to the I-80 entrance in Truckee, with a stop 
at a gas station to get some snacks.</p>

<p><a href="https://www.google.com/maps/dir/39.249095,-120.0300215/39.338854,-120.1698876/@39.3190228,-120.1747248,13z/data=!4m9!4m8!1m5!3m4!1m2!1d-120.1522306!2d39.3168207!3s0x809961d34ddd1f31:0xc7a37e232df2aeae!1m0!3e0?entry=ttu"><img src="https://tomverbeure.github.io/assets/fsd/kings_beach_to_truckee.png" alt="Map from Kings Beach to Truckee"></a>
<em>Click to open in Google Maps</em></p>

<p>This is not a complicated tasks. Other than the gas stop, it’s just driving straight along state 
route 267 with a 3 traffic lights. What could possibly go wrong? Well, FSD managed to make 2 mistakes.</p>

<p><strong>Mistake 1: select the wrong exit lane</strong></p>

<p>During the first mistake, instead of turning right at the gas station, it made the decision to prepare 
to exit one street early, switched on its indicator, and started moving to the right exit lane. Note
that there is no way to get to the gas station through that first exit.</p>

<p><img src="https://tomverbeure.github.io/assets/fsd/mistake1.jpg" alt="Mistake one: go right too early"></p>

<p><img src="https://tomverbeure.github.io/assets/fsd/mistake1_streetview.jpg" alt="Mistake one: streetview"></p>

<p>If I hadn’t immediately interrupted that maneuver (see <em>Rules of Engagement</em>), I assume it would 
have corrected itself eventually and gone back onto the main lane. But if I had been the driver behind, 
I’d have questioned the antics of the driver in front of me.</p>

<p>FSD managed to screw up its very first maneuver. Not a good look.</p>

<p><strong>Mistake 2: selecting the right turn lane when going straight</strong></p>

<p>The second mistake happened less than a mile later.</p>

<p>At the intersection with Old Brockway Rd, the car was supposed to continue straight. There are 3 
lanes at the traffic light: left, middle, and right, and only the middle lane can be used 
to go straight.</p>

<p><img src="https://tomverbeure.github.io/assets/fsd/mistake2_streetview.jpg" alt="Mistake two: streetview"></p>

<p>For whatever reason, FSD initiated a move to go to the right lane. Another case where I’m sure it 
would have corrected itself eventually, but it’s clear that the system had no clue about the 
traffic situation in front of it.</p>

<p>While both cases are not life-or-death situations, it’s truly impressive that FSD managed to make 2 
easily avoided mistakes before my first 10 miles of using it!</p>

<h2 id="test-ride-2-i-80-from-truckee-to-blue-canyon-36-miles">Test Ride 2: I-80 from Truckee to Blue Canyon (36 miles)</h2>

<p>For the second test, my wife reluctantly gave me permission to try out FSD for interstate driving, 
which should be its best case scenario.</p>

<p><a href="https://www.google.com/maps/dir/39.338854,-120.1698876/39.2543823,-120.7372512/@39.2945489,-120.4893074,10z/data=!4m2!4m1!3e0?entry=ttu"><img src="https://tomverbeure.github.io/assets/fsd/truckee_to_blue_canyon.png" alt="Map from Truckee to Blue Canyon"></a>
<em>Click to open in Google Maps</em></p>

<p>It’s a bit disconcerting to see the car make a decision to change lanes to pass someone, but I guess 
that’s something you’ll get used to.</p>

<p>But what was baffling was the way in which it behaved worse than autopilot. 
There were two nearly identical cases, where the 2-lane road was dead straight, with excellent paint marks, 
and with cars right of me, yet FSD made nervous left-right oscillation-like corrections that I have never 
experienced before in autopilot mode. It was not a case of FSD wanting to change lanes, no right indicator 
was ever switched on.</p>

<p><img src="https://tomverbeure.github.io/assets/fsd/mistake3_streetview.jpg" alt="Mistake three: streetview"></p>

<p>The first time, my wife questioned what was going on. The second time, on a section just past
the Whitmore Caltrans station near Alta, she ordered me to switch off FSD. In the past 3 years, she 
never once asked me to switch off autopilot.</p>

<p>One would think that autopilot and FSD have the same core lane tracking algorithms, but one way or the other 
the experience was radically different. I switched back to autopilot. The remaining 3 hours were uneventful.</p>

<h2 id="test-ride-3-from-west-valley-college-to-i-85-entrance-1-mile">Test Ride 3: from West-Valley College to I-85 Entrance (1 mile)</h2>

<p>The final test happened yesterday, while driving back from the 
<a href="https://www.electronicsfleamarket.com/">Silicon Valley Electronics Flea Market</a>
back home.</p>

<p>These are always held on a Sunday, start very early at 6am, and I’m usually out before 9am, so there’s 
almost nobody on the road.</p>

<p><a href="https://www.google.com/maps/dir/37.2651941,-122.0128022/37.2768284,-122.0072917/@37.2710772,-122.0107086,15.68z/data=!4m2!4m1!3e0?entry=ttu"><img src="https://tomverbeure.github.io/assets/fsd/west_valley_college_saratoga.png" alt="Map from West Valley College to Route 85 Canyon"></a>
<em>Click to open in Google Maps</em></p>

<p>FSD managed to turn right out of the parking lot just fine and get past the first traffic light.</p>

<p><a href="https://tomverbeure.github.io/assets/fsd/no_turn_on_right.jpg"><img src="https://tomverbeure.github.io/assets/fsd/no_turn_on_right.jpg" alt="Mistake four: streetview"></a></p>

<p>The second traffic light has a don’t-turn-on-red sign. The light was red, the Tesla came to a full stop, 
and then pressed on the gas to move on while the light was still red. (According to my colleague, police 
often lay in wait at this location to catch violators.)</p>

<p>By now I fully expected it to make that mistake, so I was ready to press the brake.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The way it currently behaves, FSD is a system that can’t be trusted to make the right decisions. It makes 
the most basic mistakes and it makes many of them.</p>

<p>Without FSD, you pay attention to the road and everything else is within your control. With FSD, you still 
need to pay attention but now there’s the additional cognitive load to monitor an unpredictable system over 
which you don’t have direct control. Forget about just being focused, you need to be hyper-focused, and you 
need to pay $99 per month or a one time fee of $12,000 for the privilege. With the limited functionality of autopilot, 
you hit the sweet spot: adaptive cruise control and lane centering work reliably, and you don’t need to 
worry about any other mischief.</p>

<p>Maybe one day I’ll be able to drive to Lake Tahoe by typing in the address, sit back, take a nap,
or play on my phone. Until then, it’s just a fancy technology demo with little practical value.</p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Voyager 1 is back online! NASA spacecraft returns data from all 4 instruments (564 pts)]]></title>
            <link>https://www.space.com/voyager-1-fully-operational</link>
            <guid>40687660</guid>
            <pubDate>Sat, 15 Jun 2024 05:12:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/voyager-1-fully-operational">https://www.space.com/voyager-1-fully-operational</a>, See on <a href="https://news.ycombinator.com/item?id=40687660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-320-80.jpg" alt="artwork of voyager 1 spacecraft in black space background" srcset="https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/Aevg5Lf897c5wagmjmsp8Z.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: NASA)</span>
</figcaption>
</div>

<div id="article-body">
<p>All right, everyone —&nbsp;we can all breathe a sigh of relief. NASA's <a data-analytics-id="inline-link" href="https://www.space.com/17688-voyager-1.html" data-before-rewrite-localise="https://www.space.com/17688-voyager-1.html"><u>Voyager 1</u></a> spacecraft is fully operational once more, with all four science instruments returning usable data to Earth.</p><p>The problems began in November 2023, when Voyager 1 lost its ability to "speak" with us. More specifically, it started sending to <a data-analytics-id="inline-link" href="https://www.space.com/54-earth-history-composition-and-atmosphere.html" data-before-rewrite-localise="https://www.space.com/54-earth-history-composition-and-atmosphere.html"><u>Earth</u></a> unintelligible data instead of its normal 0s and 1s of binary code. Of course, Voyager 1 is 46 years old —&nbsp;ancient for a spacecraft —&nbsp;so it wasn't <em>entirely</em> a surprise that its health might be waning. And that's not to mention that it's in entirely uncharted interstellar territory, some 15 billion miles (24 billion kilometers) from Earth.&nbsp;</p><p>Voyager 1's dogged team was determined to not only figure out what went wrong, but also to fix the problem. And they've succeeded! Controllers identified where the issue was located: the flight data subsystem (FDS), used to "package" data to be sent to Earth. Further sleuthing revealed the exact chip causing the problem, which allowed them to find a workaround. After the team relocated the code to a new location in the FDS, Voyager 1 finally sent back intelligible data on April 20, 2024&nbsp;— but only from two of its four science instruments. Now, just two months later, Voyager 1's remaining two science instruments are back up and running, communicating effectively with mission control on Earth.</p><p>Even if Voyager 1 had gone dark for good, however, the mission would still have been a wild success. After it launched in 1977, its primary mission was to study <a data-analytics-id="inline-link" href="https://www.space.com/7-jupiter-largest-planet-solar-system.html" data-before-rewrite-localise="https://www.space.com/7-jupiter-largest-planet-solar-system.html"><u>Jupiter</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/48-saturn-the-solar-systems-major-ring-bearer.html" data-before-rewrite-localise="https://www.space.com/48-saturn-the-solar-systems-major-ring-bearer.html"><u>Saturn</u></a> —&nbsp;that was accomplished by 1980. (Its twin spacecraft, <a data-analytics-id="inline-link" href="https://www.space.com/voyager-2" data-before-rewrite-localise="https://www.space.com/voyager-2"><u>Voyager 2</u></a>, went on to study <a data-analytics-id="inline-link" href="https://www.space.com/45-uranus-seventh-planet-in-earths-solar-system-was-first-discovered-planet.html" data-before-rewrite-localise="https://www.space.com/45-uranus-seventh-planet-in-earths-solar-system-was-first-discovered-planet.html"><u>Uranus</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/41-neptune-the-other-blue-planet-in-our-solar-system.html" data-before-rewrite-localise="https://www.space.com/41-neptune-the-other-blue-planet-in-our-solar-system.html"><u>Neptune</u></a>.) But Voyager 1 is on an unstoppable path. Continuing its journey away from Earth, the spacecraft entered <a data-analytics-id="inline-link" href="https://www.space.com/interstellar-space-definition-explanation" data-before-rewrite-localise="https://www.space.com/interstellar-space-definition-explanation"><u>interstellar space</u></a> in 2012, returning crucial data about this mysterious realm.</p><p>Now that Voyager 1 is back online, the team will continue to "touch up" the spacecraft to get it back in top form, including resynchronizing its timekeeping software to execute commands at the right <a data-analytics-id="inline-link" href="https://www.space.com/time-how-it-works" data-before-rewrite-localise="https://www.space.com/time-how-it-works"><u>time</u></a>, as well as performing maintenance on the digital tape recorder that measures plasma waves. And hopefully, Voyager 1 will have a long, happy life ahead.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-zXjotydkMTeij63e2Mmbt7"><section><p>Sign up to our newsletter for the latest updates on rocket launches, skywatching events and more!</p></section></div>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>
<div id="slice-container-authorBio-zXjotydkMTeij63e2Mmbt7"><p>Space.com contributing writer Stefanie Waldek is a self-taught space nerd and aviation geek who is passionate about all things spaceflight and astronomy. With a background in travel and design journalism, as well as a Bachelor of Arts degree from New York University, she specializes in the budding space tourism industry and Earth-based astrotourism. In her free time, you can find her watching rocket launches or looking up at the stars, wondering what is out there. Learn more about her work at <a href="https://mailtrack.io/trace/link/841a96809cf8444a85db7a2b318410433580a8f6?url=https%3A%2F%2Fwww.stefaniewaldek.com&amp;userId=2756587&amp;signature=ba54abdc159873b3" target="_blank">www.stefaniewaldek.com</a>.</p></div>


</section>




<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snowden: "They've gone full mask-off: do not ever trust OpenAI or its products" (206 pts)]]></title>
            <link>https://twitter.com/Snowden/status/1801610725229498403</link>
            <guid>40685644</guid>
            <pubDate>Fri, 14 Jun 2024 22:13:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Snowden/status/1801610725229498403">https://twitter.com/Snowden/status/1801610725229498403</a>, See on <a href="https://news.ycombinator.com/item?id=40685644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Exponentially Better Rotations (225 pts)]]></title>
            <link>http://thenumb.at/Exponential-Rotations/</link>
            <guid>40684901</guid>
            <pubDate>Fri, 14 Jun 2024 20:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://thenumb.at/Exponential-Rotations/">http://thenumb.at/Exponential-Rotations/</a>, See on <a href="https://news.ycombinator.com/item?id=40684901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>If you’ve done any 3D programming, you’ve likely encountered the zoo of techniques and representations used when working with 3D rotations. Some of them are better than others, depending on the situation.</p>

<p><em>Based on <a href="http://15462.courses.cs.cmu.edu/">CMU 15-462</a> course materials by <a href="https://www.cs.cmu.edu/~kmcrane/">Keenan Crane</a>.</em></p>

<ul>
  <li><a href="#representations">Representations</a>
    <ul>
      <li><a href="#rotation-matrices">Rotation Matrices</a></li>
      <li><a href="#euler-angles">Euler Angles</a></li>
      <li><a href="#quaternions">Quaternions</a></li>
      <li><a href="#axisangle-rotations">Axis/Angle</a></li>
    </ul>
  </li>
  <li><a href="#the-exponential-and-logarithmic-maps">The Exponential and Logarithmic Maps</a>
    <ul>
      <li><a href="#axisangle-in-2d">Axis/Angle in 2D</a></li>
      <li><a href="#axisangle-in-3d">Axis/Angle in 3D</a></li>
      <li><a href="#averaging-rotations">Averaging Rotations</a></li>
      <li><a href="#quaternions-again">Quaternions (Again)</a></li>
    </ul>
  </li>
  <li><a href="#further-reading">Further Reading</a></li>
</ul>

<h2 id="representations">Representations</h2>

<h2 id="rotation-matrices">Rotation Matrices</h2>

<p>Linear-algebra-wise, the most straightforward representation is an orthonormal 3x3 matrix (with positive determinant). The three columns of a rotation matrix specify where the x, y, and z axes end up after the rotation.</p>

<p>Rotation matrices are particularly useful for transforming points: just multiply! Even better, rotation matrices can be composed with any other linear transformations via matrix multiplication. That’s why we use rotation matrices when actually drawing things on screen: only one matrix multiplication is required to transform a point from world-space to the screen. However, rotation matrices are not so useful for actually working with <em>rotations</em>: because they don’t form a vector space, adding together two rotation matrices will not give you a rotation matrix back. For example, animating an object by linearly interpolating between two rotation matrices adds scaling:</p>





<table><tbody><tr>
<td>$$ R_0 = \begin{bmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix} $$</td>
<td id="matrix_interp_mj">$$ R(0.00) = \begin{bmatrix}\phantom{-}1.00&amp;\phantom{-}0.00&amp;\phantom{-}0.00\\\phantom{-}0.00&amp;\phantom{-}1.00&amp;\phantom{-}0.00\\\phantom{-}0.00&amp;\phantom{-}0.00&amp;\phantom{-}1.00\end{bmatrix} $$</td>
<td id="matrix_interp_m1">$$ R_1 = \begin{bmatrix}-1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;-1\end{bmatrix} $$</td>
</tr></tbody></table>

<h2 id="euler-angles">Euler Angles</h2>

<p>Another common representation is Euler angles, which specify three separate rotations about the x, y, and z axes (also known as pitch, yaw, and roll). The order in which the three component rotations are applied is an arbitrary convention—here we’ll apply x, then y, then z.</p>

<canvas id="euler"></canvas>
<table>
<tbody><tr><td>$$\theta_x$$</td><td></td></tr>
<tr><td>$$\theta_y$$</td><td></td><td></td></tr>
<tr><td>$$\theta_z$$</td><td></td></tr>
</tbody></table>

<p>Euler angles are generally well-understood and often used for authoring rotations. However, using them for anything else comes with some significant pitfalls. While it’s possible to manually create splines that nicely interpolate Euler angles, straightforward interpolation often produces undesirable results.</p>

<p>Euler angles suffer from <em>gimbal lock</em> when one component causes the other two axes of rotation to become parallel. Such configurations are called <em>singularities</em>. At a singularity, changing either of two ‘locked’ angles will cause the same output rotation. You can demonstrate this phenomenon above by pressing the ‘lock’ button and adjusting the x/z rotations (a quarter rotation about y aligns the z axis with the x axis).</p>

<p>Singularities break interpolation: if the desired path reaches a singularity, it gains a degree of freedom with which to represent its current position. Picking an arbitrary representation to continue with causes discontinuities in the interpolated output: even within an axis, interpolation won’t produce a constant angular velocity. That can be a problem if, for example, you’re using the output to drive a robot. Furthermore, since each component angle is cyclic, linear interpolation won’t always choose the shortest path between rotations.</p>

<canvas id="euler_interp"></canvas>


<table><tbody><tr>
<td>$$ \mathbf{\theta}_0 = \begin{bmatrix}0\\0\\0\end{bmatrix} $$</td>
<td id="euler_interp_mj">$$ \mathbf{\theta}(0.00) = \begin{bmatrix}\phantom{-}0.00\\\phantom{-}0.00\\\phantom{-}0.00\end{bmatrix} $$</td>
<td id="euler_interp_m1">$$ \mathbf{\theta}_1 = \begin{bmatrix}-3.14\\0.00\\-3.14\end{bmatrix} $$</td>
</tr></tbody></table>

<p>Thankfully, interpolation is smooth if the path doesn’t go through a singularity, so these limitations <em>can</em> be worked around, especially if you don’t need to represent ‘straight up’ and ‘straight down.’</p>

<h2 id="quaternions">Quaternions</h2>

<p>At this point, you might be expecting yet another article on quaternions—don’t worry, we’re not going to delve into hyper-complex numbers today. It suffices to say that unit quaternions are the standard tool for composing and interpolating rotations, since <a href="https://en.wikipedia.org/wiki/Slerp">spherical linear interpolation</a> (slerp) chooses a constant-velocity shortest path between any two quaternions. However, unit quaternions also don’t form a vector space, are unintuitive to author, and can be computationally costly to interpolate<a href="http://number-none.com/product/Hacking%20Quaternions/">*</a>. Further, there’s no intuitive notion of scalar multiplication, nor averaging.</p>

<p>But, they’re still fascinating! If you’d like to understand quaternions more deeply (or, perhaps, learn what they are in the first place), read <a href="https://eater.net/quaternions">this</a>.</p>

<canvas id="quat_interp"></canvas>


<table><tbody><tr>
<td>$$ Q_0 = \begin{bmatrix}1\\0\\0\\0\end{bmatrix}\ \ $$</td>
<td id="quat_interp_mj">$$ Q(0.00) = \begin{bmatrix}\phantom{-}1.00\\\phantom{-}0.00\\\phantom{-}0.00\\\phantom{-}0.00\end{bmatrix}\ \ $$</td>
<td id="quat_interp_m1">$$ Q_1 = \begin{bmatrix}0\\0\\1\\0\end{bmatrix} $$</td>
</tr></tbody></table>

<p>Note that since quaternions double-cover the space of rotations, sometimes \(Q(1)\) will go to \(-Q_1\).</p>

<h2 id="axisangle-rotations">Axis/Angle Rotations</h2>

<p>An axis/angle rotation is a 3D vector of real numbers. Its direction specifies the axis of rotation, and its magnitude specifies the angle to rotate about that axis. For convenience, we’ll write axis/angle rotations as \(\theta\mathbf{u}\), where \(\mathbf{u}\) is a unit-length vector and \(\theta\) is the rotation angle.</p>

<canvas id="axis_angle"></canvas>

<table>
<tbody><tr><td>$$\mathbf{u}_x$$</td><td></td></tr>
<tr><td>$$\mathbf{u}_y$$</td><td></td></tr>
<tr><td>$$\mathbf{u}_z$$</td><td></td></tr>
<tr><td>$$\theta$$</td><td></td></tr>
</tbody></table>

<p>Since axis/angle rotations are simply 3D vectors, they form a vector space: we can add, scale, and interpolate them to our heart’s content. Linearly interpolating between any two axis/angle rotations is smooth and imparts constant angular velocity. However, note that linearly interpolating between axis-angle rotations does not necessarily choose the shortest path: it depends on which axis/angle you use to specify the target rotation.</p>

<canvas id="axis_angle_interp"></canvas>


<table><tbody><tr>
<td>$$ \theta_0\mathbf{u}_0 = \begin{bmatrix}0\\0\\0\end{bmatrix}\ \ $$</td>
<td id="axis_angle_interp_mj">$$ \theta\mathbf{u}(0.00) = \begin{bmatrix}\phantom{-}0.00\\\phantom{-}0.00\\\phantom{-}0.00\end{bmatrix}\ \ $$</td>
<td id="axis_angle_interp_m1">$$ \theta_1\mathbf{u}_1 = \begin{bmatrix}0\\3.14\\0\end{bmatrix} $$</td>
</tr></tbody></table>

<p>Like quaternions, axis/angle vectors double-cover the space of rotations: sometimes \(\theta\mathbf{u}(1)\) will go to \((2\pi - \theta_1)(-\mathbf{u}_1)\).</p>

<h2 id="the-exponential-and-logarithmic-maps">The Exponential and Logarithmic Maps</h2>

<p>Ideally, we could freely convert rotations between these diverse representations based on our use case. We will always want to get a rotation matrix out at the end, so we’ll consider matrices the ‘canonical’ form. Enter the <strong>exponential map</strong>: a function that takes a different kind of rotation object and gives us back an equivalent rotation matrix. The corresponding <strong>logarithmic map</strong> takes a rotation matrix and gives us back a rotation object. How these maps relate to the scalar <code>exp</code> and <code>log</code> functions will hopefully become clear later on.</p>

<p><img src="http://thenumb.at/assets/exp-rotations/map.png"></p>

<p>Below, we’ll define an <code>exp</code> and <code>log</code> map translating between rotation matrices and axis/angle vectors. But first, to build up intuition, let us consider how rotations work in two dimensions.</p>

<h2 id="axisangle-in-2d">Axis/Angle in 2D</h2>

<p>In 2D, there’s only one axis to rotate around: the one pointing out of the plane. Hence, our ‘axis/angle’ rotations can be represented by just \(\theta\).</p>

<p>Given a 2D point \(\mathbf{p}\), how can we rotate \(\mathbf{p}\) by \(\theta\)? One way to visualize the transformation is by forming a coordinate frame in which the output is easy to describe. Consider \(\mathbf{p}\) and its quarter (\(90^\circ\)) rotation \(\mathcal{J}\mathbf{p}\):</p>

<p><img src="http://thenumb.at/assets/exp-rotations/2d_rot.svg"></p>

<p>Using a bit of trigonometry, we can describe the rotated \(\mathbf{p}_\theta\) in two components:</p><p>

\[\begin{align*} \mathbf{p}_\theta &amp;= \mathbf{p}\cos\theta + \mathcal{J}\mathbf{p}\sin\theta \\
 &amp;= (\cos(\theta)\mathcal{I} + \sin(\theta)\mathcal{J})\mathbf{p} \end{align*}\]

</p><p>But, what actually are \(\mathcal{I}\) and \(\mathcal{J}\)? The former should take a 2D vector and return it unchanged: it’s the 2x2 identity matrix. The latter should be similar, but swap and negate the two components:</p><p>

\[\begin{align*} \mathcal{I} &amp;= \begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix} \\ \mathcal{J} &amp;= \begin{bmatrix}0&amp;-1\\1&amp;0\end{bmatrix} \end{align*}\]

</p><p>Just to make sure we got \(\mathcal{J}\) right, let’s check what happens if we apply it twice (via \(\mathcal{J}^2\)):</p><p>

\[\mathcal{J}^2 = \begin{bmatrix}0&amp;-1\\1&amp;0\end{bmatrix}\begin{bmatrix}0&amp;-1\\1&amp;0\end{bmatrix} = \begin{bmatrix}-1&amp;0\\0&amp;-1\end{bmatrix}\]

</p><p>We got \(\mathcal{J}^2 = -\mathcal{I}\), which is a 180-degree rotation. So, \(\mathcal{J}\) indeed represents 90-degree rotation.</p>

<p>Now, what does our transform look like?</p><p>

\[\begin{align*}
\mathbf{p}_\theta &amp;= (\cos(\theta)\mathcal{I} + \sin(\theta)\mathcal{J})\mathbf{p} \\
                  &amp;= \left(\cos(\theta)\begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix} + \sin(\theta)\begin{bmatrix}0&amp;-1\\1&amp;0\end{bmatrix}\right)\mathbf{p} \\
                  &amp;= \begin{bmatrix}\cos\theta&amp;-\sin\theta\\\sin\theta&amp;\cos\theta\end{bmatrix}\mathbf{p}
\end{align*}\]

</p><p>That’s the standard 2D rotation matrix. What a coincidence!</p>

<h3 id="complex-rotations">Complex Rotations</h3>

<p>If you’re familiar with complex numbers, you might notice that our first transform formula feels eerily similar to Euler’s formula, \(e^{ix} = \cos x + i\sin x\):</p><p>

\[\begin{align*}
\mathbf{p}_\theta &amp;= (\cos(\theta)\mathcal{I} + \sin(\theta)\mathcal{J})\mathbf{p} \\
e^{i\theta}p &amp;= (\cos(\theta) + i\sin(\theta))p
\end{align*}\]

</p><p><img src="http://thenumb.at/assets/exp-rotations/expi.svg"></p>

<p>Where \(i\) and \(\mathcal{J}\) both play the role of a quarter turn. We can see that in complex arithmetic, multiplying by \(i\) in fact has that effect:</p><p>

\[\mathcal{J}\mathbf{p} = \begin{bmatrix}0&amp;-1\\1&amp;0\end{bmatrix}\begin{bmatrix}a\\b\end{bmatrix} = \begin{bmatrix}-b\\a\end{bmatrix}\]

\[ip = i(a + bi) = ai + bi^2 = -b + ai\]

</p><p>So, there must be some connection to the exponential function here.</p>

<h3 id="the-2d-exponential-map">The 2D Exponential Map</h3>

<p>Recall the definition of the exponential function (or equivalently, its Taylor series):</p><p>

\[e^x = \sum_{k=0}^\infty \frac{x^k}{k!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots\]

</p><p>Using Euler’s formula, \(e^{i\theta}\) gave us a complex number representing a 2D rotation by \(\theta\). Can we do the same with \(e^{\theta\mathcal{J}}\)? If we plug a matrix into the above definition, the arithmetic still works out: 2x2 matrices certainly support multiplication, addition, and scaling. (More on matrix exponentiation <a href="https://www.youtube.com/watch?v=O85OWBJ2ayo">here</a>.)</p>

<p>Let \(\theta\mathcal{J} = A\) and plug it in:</p><p>

\[e^A = \sum_{k=0}^\infty \frac{A^k}{k!} = \mathcal{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \dots\]

</p><p>Let’s pull out the first four terms to inspect further:</p><p>

\[\begin{align*}
e^A &amp;= \mathcal{I} + A + \frac{1}{2!}A^2 + \frac{1}{3!}A^3 \\
    &amp;= \mathcal{I} + A\left(\mathcal{I} + \frac{1}{2}A\left(\mathcal{I} + \frac{1}{3}A\right)\right) \\
    &amp;= \mathcal{I} + A\left(\mathcal{I} + \frac{1}{2}A\begin{bmatrix}1&amp;\frac{-\theta}{3}\\\frac{\theta}{3}&amp;1\end{bmatrix}\right) \\
    &amp;= \mathcal{I} + A\begin{bmatrix}1-\frac{\theta^2}{6}&amp;\frac{-\theta}{2}\\\frac{\theta}{2}&amp;1-\frac{\theta^2}{6}\end{bmatrix} \\
    &amp;= \begin{bmatrix}1-\frac{\theta^2}{2}&amp;-\theta+\frac{\theta^3}{6}\\\theta-\frac{\theta^3}{6}&amp;1-\frac{\theta^2}{2}\end{bmatrix}
\end{align*}\]

</p><p>These entries look familiar. Recall the Taylor series that describe the functions \(\sin\) and \(\cos\):</p><p>

\[\begin{align*} \sin x &amp;= x - \frac{x^3}{3!} + \frac{x^5}{5!} - \dots  \\
\cos x &amp;= 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \dots \end{align*}\]

</p><p>If we write out all the terms of \(e^A\), we’ll recover the expansions of \(\sin\theta\) and \(\cos\theta\)! Therefore:</p><p>

\[e^A = e^{\theta\mathcal{J}} = \begin{bmatrix}\cos\theta&amp;-\sin\theta\\\sin\theta&amp;\cos\theta\end{bmatrix}\]

</p><p>We’ve determined that the exponential function \(e^{\theta\mathcal{J}}\) converts our angle \(\theta\) into a corresponding 2D rotation matrix. In fact, we’ve proved a version of Euler’s formula with 2x2 matrices instead of complex numbers:</p><p>

\[\begin{align*} &amp;&amp; e^{\theta\mathcal{J}} &amp;= (\cos(\theta)\mathcal{I} + \sin(\theta)\mathcal{J})\\
    &amp;\implies&amp; \mathbf{p}_\theta &amp;= e^{\theta\mathcal{J}}\mathbf{p}
\end{align*}\]

</p><h3 id="the-2d-logarithmic-map">The 2D Logarithmic Map</h3>

<p>The logarithmic map should naturally be the inverse of the exponential:</p><p>

\[R = \exp(\theta\mathcal{J}) \implies \log(R) = \theta\mathcal{J}\]

</p><p>So, given \(R\), how can we recover \(\theta\mathcal{J}\)?</p><p>

\[\begin{align*}
&amp;&amp; R &amp;= \begin{bmatrix}\cos\theta&amp;-\sin\theta\\\sin\theta&amp;\cos\theta\end{bmatrix}\\
&amp;\implies&amp; \theta &amp;= \text{atan2}(R_{21},R_{11})\\
&amp;\implies&amp; \log(R) &amp;= \begin{bmatrix}0&amp;-\theta\\\theta&amp;0\end{bmatrix}
\end{align*}\]

</p><p>Note that in general, our exponential map is not injective. Clearly, \(\exp(\theta\mathcal{J}) = \exp((\theta + 2\pi)\mathcal{J})\), since adding an extra full turn will always give us back the same rotation matrix. Therefore, our logarithmic map can’t be surjective—we’ll define it as returning the <em>smallest</em> angle \(\theta\mathcal{J}\) corresponding to the given rotation matrix. Using \(\text{atan2}\) implements this definition.</p>

<h3 id="interpolation">Interpolation</h3>

<p>Consider two 2D rotation angles \(\theta_0\) and \(\theta_1\). The most obvious way to interpolate between these two rotations is to interpolate the angles and create the corresponding rotation matrix. This scheme is essentially a 2D version of axis-angle interpolation.</p><p>

\[\begin{align*} \theta(t) &amp;= (1-t)\theta_0 + t\theta_1\\
R_\theta(t) &amp;= \begin{bmatrix}\cos(\theta(t))&amp;-\sin(\theta(t))\\\sin(\theta(t))&amp;\cos(\theta(t))\end{bmatrix}
\end{align*}\]

</p><p>However, if \(\theta_0\) and \(\theta_1\) are separated by more than \(\pi\), this expression will take the long way around: it’s not aware that angles are cyclic.</p>

<canvas id="2d_angle_interp"></canvas>


<table><tbody><tr>
<td>$$ \theta_0 = 0\ \ $$</td>
<td id="2d_angle_interp_mj">$$ \theta(0.00) = 0.00\ \ $$</td>
<td id="2d_angle_interp_m1">$$ \theta_1 = 4.71 $$</td>
</tr></tbody></table>

<p>Instead, let’s devise an interpolation scheme based on our <code>exp</code>/<code>log</code> maps. Since we know the two rotation matrices \(R_0\), \(R_1\), we can express the rotation that takes us directly from the initial pose to the final pose: \(R_1R_0^{-1}\), i.e. first undo \(R_0\), then apply \(R_1\).</p>

<p>Using our logarithmic map, we can obtain the smallest angle that rotates from \(R_0\) to \(R_1\): \(\log(R_1R_0^{-1})\). Since \(\log\) gives us an axis-angle rotation, we can simply scale the result by \(t\) to perform interpolation. After scaling, we can use our exponential map to get back a rotation matrix. This matrix represents a rotation \(t\) of the way from \(R_0\) to \(R_1\).</p>

<p>Hence, our final parametric rotation matrix is \(R(t) = \exp(t\log(R_1R_0^{-1})))R_0\).</p><p>

\[\begin{align*} R(0) &amp;= \exp(0)R_0 = R_0\\
R(1) &amp;= \exp(\log(R_1R_0^{-1}))R_0 = R_1R_0^{-1}R_0 = R_1 \end{align*}\]

</p><canvas id="2d_exp_interp"></canvas>


<table><tbody><tr>
<td>$$ R_0 = \begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix} $$</td>
<td id="2d_exp_interp_mj">$$ R(0.00) = \begin{bmatrix}\phantom{-}1.00&amp;\phantom{-}0.00\\\phantom{-}0.00&amp;\phantom{-}1.00\end{bmatrix} $$</td>
<td id="2d_exp_interp_m1">$$ R_1 = \begin{bmatrix}0&amp;-1\\1&amp;0\end{bmatrix} $$</td>
</tr></tbody></table>

<p>Using <code>exp</code>/<code>log</code> for interpolation might seem like overkill for 2D—we could instead just check how far apart the angles are. But below, we’ll see how this interpolation scheme generalizes—without modification—to 3D, and in fact any number of dimensions.</p>

<h2 id="axisangle-in-3d">Axis/Angle in 3D</h2>

<p>We’re finally ready to derive an exponential and logarithmic map for 3D rotations. In 2D, our map arose from exponentiating \(\theta\mathcal{J}\), i.e. \(\theta\) times a matrix representing a counter-clockwise quarter turn about the axis of rotation. We will be able to do the same in 3D—but what transformation encodes a quarter turn about a 3D unit vector \(\mathbf{u}\)?</p>

<p>The cross product \(\mathbf{u}\times\mathbf{p}\) is typically defined as a vector normal to the plane containing both \(\mathbf{u}\) and \(\mathbf{p}\). However, we could also interpret \(\mathbf{u}\times\mathbf{p}\) as a quarter turn of the <em>projection</em> of \(\mathbf{p}\) into the plane with normal \(\mathbf{u}\), which we will call \(\mathbf{p}_\perp\):</p>

<p><img src="http://thenumb.at/assets/exp-rotations/crossu.svg"></p>

<p>So, if we can compute the quarter rotation of \(\mathbf{p}_\perp\), it should be simple to recover the quarter rotation of \(\mathbf{p}\). Of course, \(\mathbf{p}=\mathbf{p}_\perp+\mathbf{p}_\parallel\), so we’ll just have to add back the parallel part \(\mathbf{p}_\parallel\). This is correct because a rotation about \(\mathbf{u}\) preserves \(\mathbf{p}_\parallel\):</p>

<p><img src="http://thenumb.at/assets/exp-rotations/crossu2.svg"></p>

<p>However, “\(\mathbf{u} \times\)” is not a mathematical object we can work with. Instead, we can devise a matrix \(\mathbf{\hat{u}}\) that when multiplied with a a vector \(\mathbf{p}\), outputs the same result as \(\mathbf{u} \times \mathbf{p}\):</p><p>

\[\begin{align*}
\mathbf{u} \times \mathbf{p} &amp;= \begin{bmatrix} u_yp_z - u_zp_y \\ u_zp_x - u_xp_z \\ u_xp_y - u_yp_x \end{bmatrix} \\
 &amp;= \begin{bmatrix}0&amp;-u_z&amp;u_y\\u_z&amp;0&amp;-u_x\\-u_y&amp;u_x&amp;0\end{bmatrix}\begin{bmatrix}p_x\\p_y\\p_z\end{bmatrix} \\
 &amp;= \mathbf{\hat{u}}\mathbf{p}
\end{align*}\]

</p><p>We can see that \(\mathbf{\hat{u}}^T = -\mathbf{\hat{u}}\), so \(\mathbf{\hat{u}}\) is a <em>skew-symmetric</em> matrix. (i.e. it has zeros along the diagonal, and the two halves are equal but negated.) Note that in the 2D case, our quarter turn \(\mathcal{J}\) was also skew-symmetric, and sneakily represented the 2D cross product! We must be on the right track.</p>

<p>The reason we want to use axis/angle rotations in the first place is because they form a vector space. So, let’s make sure our translation to skew-symmetric matrices maintains that property. Given two skew-symmetric matrices \(A_1\) and \(A_2\):</p><p>

\[(A_1 + A_2)^T = A_1^T + A_2^T = -A_1 - A_2 = -(A_1 + A_2)\]

</p><p>Their sum is also a skew-symmetric matrix. Similarly:</p><p>

\[(cA)^T = c(A^T) = -cA_1\]

</p><p>Scalar multiplication also maintains skew-symmetry. The other vector space properties follow from the usual definition of matrix addition.</p>

<p>Finally, note that \(\mathbf{u} \times (\mathbf{u} \times (\mathbf{u} \times \mathbf{p})) = -\mathbf{u} \times \mathbf{p}\). Taking the cross product three times would rotate \(\mathbf{p}_\perp\) three-quarter turns about \(\mathbf{u}\), which is equivalent to a single negative-quarter turn. More generally, \(\mathbf{\hat{u}}^{k+2} = -\mathbf{\hat{u}}^k\) for any \(k&gt;0\). We could prove this by writing out all the terms, but the geometric argument is easier:</p>

<p><img src="http://thenumb.at/assets/exp-rotations/crossu3.svg"></p>

<h3 id="the-3d-exponential-map">The 3D Exponential Map</h3>

<p>Given an axis/angle rotation \(\theta\mathbf{u}\), we can make \(\theta\mathbf{\hat{u}}\) using the above construction. What happens when we exponentiate it? Using the identity \(\mathbf{\hat{u}}^{k+2} = -\mathbf{\hat{u}}^k\):</p><p>

\[\begin{align*}
e^{\theta\mathbf{\hat{u}}} &amp;= \mathcal{I} + \theta\mathbf{\hat{u}} + \frac{1}{2!}\theta^2\mathbf{\hat{u}}^2 + \frac{1}{3!}\theta^3\mathbf{\hat{u}}^3 + \frac{1}{4!}\theta^4\mathbf{\hat{u}}^4 + \frac{1}{5!}\theta^5\mathbf{\hat{u}}^5 + \dots \\
&amp;= \mathcal{I} + \theta\mathbf{\hat{u}} + \frac{1}{2!}\theta^2\mathbf{\hat{u}}^2 - \frac{1}{3!}\theta^3\mathbf{\hat{u}} - \frac{1}{4!}\theta^4\mathbf{\hat{u}}^2 + \frac{1}{5!}\theta^5\mathbf{\hat{u}} + \dots \\
&amp;= \mathcal{I} + \left(\theta - \frac{1}{3!}\theta^3 + \frac{1}{5!}\theta^5 - \dots\right)\mathbf{\hat{u}} + \left(1 - \left(1 - \frac{1}{2!}\theta^2 + \frac{1}{4!}\theta^4 - \dots\right)\right)\mathbf{\hat{u}}^2 \\
&amp;= \mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2
\end{align*}\]

</p><p>In the last step, we again recover the Taylor expansions of \(\sin\theta\) and \(\cos\theta\). Our final expression is known as <em>Rodrigues’ formula</em>.</p>

<p>This formula is already reminiscent of the 2D case: the latter two terms are building up a 2D rotation in the plane defined by \(\mathbf{u}\). To sanity check our 3D result, let’s compute our transform for \(\theta = 0\):</p><p>

\[e^{0\mathbf{\hat{u}}}\mathbf{p} = (\mathcal{I} + 0\mathbf{\hat{u}} + (1-1)\mathbf{\hat{u}}^2)\mathbf{p} = \mathbf{p}\]

</p><p>Rotating by \(\theta = 0\) preserves \(\mathbf{p}\), so the formula works. Then compute for \(\theta = \frac{\pi}{2}\):</p><p>

\[\begin{align*} e^{\frac{\pi}{2}\mathbf{\hat{u}}}\mathbf{p} &amp;= (\mathcal{I} + 1\mathbf{\hat{u}} + (1-0)\mathbf{\hat{u}}^2)\mathbf{p} \\ &amp;= \mathbf{p} + \mathbf{\hat{u}}\mathbf{p} + \mathbf{\hat{u}}^2\mathbf{p} \\
&amp;= \mathbf{p} + \mathbf{u}\times\mathbf{p} + \mathbf{u}\times(\mathbf{u}\times\mathbf{p})\\
&amp;= (\mathbf{p}_\perp + \mathbf{p}_\parallel) + \mathbf{u}\times\mathbf{p} - \mathbf{p}_\perp\\
&amp;= \mathbf{u}\times\mathbf{p} + \mathbf{p}_\parallel
\end{align*}\]

</p><p>Above, we already concluded \(\mathbf{u}\times\mathbf{p} + \mathbf{p}_\parallel\) is a quarter rotation. So, our formula is also correct at \(\theta = \frac{\pi}{2}\). Then compute for \(\theta = \pi\):</p><p>

\[\begin{align*} e^{\pi\mathbf{\hat{u}}}\mathbf{p} &amp;= (\mathcal{I} + 0\mathbf{\hat{u}} + (1-(-1))\mathbf{\hat{u}}^2)\mathbf{p} \\
&amp;= \mathbf{p} + 2\mathbf{\hat{u}}^2\mathbf{p} \\
&amp;= (\mathbf{p}_\perp + \mathbf{p}_\parallel) - 2\mathbf{p}_\perp \\
&amp;= -\mathbf{p}_\perp + \mathbf{p}_\parallel
\end{align*}\]

</p><p><img src="http://thenumb.at/assets/exp-rotations/crossu4.svg"></p>

<p>We end up with \(-\mathbf{p}_\perp + \mathbf{p}_\parallel\), which is a half rotation. Hence \(\theta = \pi\) is also correct.</p>

<p>So far, our formula checks out. Just to be sure, let’s prove that our 3D result is a rotation matrix, i.e. it’s orthonormal and has positive determinant. A matrix is orthonormal if \(A^TA = \mathcal{I}\), so again using \(\mathbf{\hat{u}}^{k+2} = -\mathbf{\hat{u}}^k\):</p><p>

\[\begin{align*}
&amp;\left(\mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2\right)^T\left(\mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2\right) \\
=&amp; \left(\mathcal{I}^T + \sin(\theta)\mathbf{\hat{u}}^T + (1-\cos(\theta))\left(\mathbf{\hat{u}}^T\right)^2\right)\left(\mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2\right) \\
=&amp;\ (\mathcal{I} - \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2)\left(\mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2\right) \\
=&amp;\ \mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2 - \sin(\theta)\mathbf{\hat{u}} - \sin^2(\theta)\mathbf{\hat{u}}^2 - \sin(\theta)(1-\cos(\theta))\mathbf{\hat{u}}^3 \\&amp;+ (1-\cos(\theta))\mathbf{\hat{u}}^2 + \sin(\theta)(1-\cos(\theta))\mathbf{\hat{u}}^3 + (1-\cos(\theta))^2\mathbf{\hat{u}}^4 \\
=&amp;\ \mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2 - \sin(\theta)\mathbf{\hat{u}} - \sin^2(\theta)\mathbf{\hat{u}}^2 + \sin(\theta)(1-\cos(\theta))\mathbf{\hat{u}} \\&amp;+ (1-\cos(\theta))\mathbf{\hat{u}}^2 - \sin(\theta)(1-\cos(\theta))\mathbf{\hat{u}} - (1-\cos(\theta))^2\mathbf{\hat{u}}^2\\
=&amp;\ \mathcal{I} + 2(1-\cos(\theta))\mathbf{\hat{u}}^2  - \sin^2(\theta)\mathbf{\hat{u}}^2 - (1-\cos(\theta))^2\mathbf{\hat{u}}^2\\
=&amp;\ \mathcal{I} + (-\sin^2(\theta) + 2(1-\cos(\theta)) - (1-\cos(\theta))^2)\mathbf{\hat{u}}^2\\
=&amp;\ \mathcal{I} + (-\sin^2(\theta)+1-\cos^2(\theta))\mathbf{\hat{u}}^2\\
=&amp;\ \mathcal{I} + (1-(\sin^2(\theta)+\cos^2(\theta)))\mathbf{\hat{u}}^2\\
=&amp;\ \mathcal{I}
\end{align*}\]

</p><p>Therefore, \(e^{\theta\mathbf{\hat{u}}}\) is orthonormal. We could show its determinant is positive (and therefore \(1\)) by writing out all the terms, but it suffices to argue that:</p>

<ul>
  <li>Clearly, \(\begin{vmatrix}\exp(0\mathbf{\hat{u}})\end{vmatrix} = \begin{vmatrix}\mathcal{I}\end{vmatrix} = 1\)</li>
  <li>There is no \(\theta\), \(\mathbf{\hat{u}}\) such that \(\begin{vmatrix}\exp(\theta\mathbf{\hat{u}})\end{vmatrix} = 0\), since \(\mathbf{\hat{u}}\) and \(\mathbf{\hat{u}}^2\) can never cancel out \(\mathcal{I}\).</li>
  <li>\(\exp\) is continuous with respect to \(\theta\) and \(\mathbf{\hat{u}}\)</li>
</ul>

<p>Therefore, \(\begin{vmatrix}\exp(0\mathbf{\hat{u}})\end{vmatrix}\) can never become negative. That means \(\exp(\theta\mathbf{\hat{u}})\) is a 3D rotation matrix!</p>

<h3 id="the-3d-logarithmic-map">The 3D Logarithmic Map</h3>

<p>Similarly to the 2D case, the 3D exponential map is not injective, so the 3D logarithmic map will not be surjective. Instead, we will again define it to return the <em>smallest magnitude</em> axis-angle rotation corresponding to the given matrix. Our exponential map gave us:</p><p>

\[R = \exp(\theta\mathbf{\hat{u}}) = \mathcal{I} + \sin(\theta)\mathbf{\hat{u}} + (1-\cos(\theta))\mathbf{\hat{u}}^2\]

</p><p>We can take the <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)">trace</a> (sum along the diagonal) of both sides:</p><p>

\[\operatorname{tr}(R) = \operatorname{tr}(\mathcal{I}) + \sin(\theta)\operatorname{tr}(\mathbf{\hat{u}}) + (1-\cos(\theta))\operatorname{tr}(\mathbf{\hat{u}}^2)\]

</p><p>Clearly \(\operatorname{tr}(\mathcal{I}) = 3\), and since \(\mathbf{\hat{u}}\) is skew-symmetric, its diagonal sum is zero. That just leaves \(\mathbf{\hat{u}}^2\):</p><p>

\[\mathbf{\hat{u}}^2 = \begin{bmatrix}-u_y^2-u_z^2&amp;u_xu_y&amp;u_xu_z\\u_xu_y&amp;-u_x^2-u_z^2&amp;u_yu_z\\u_xu_z&amp;u_yu_z&amp;-u_x^2-u_y^2\end{bmatrix}\]

</p><p>We can see \(\operatorname{tr}(\mathbf{\hat{u}}^2) = -2u_x^2-2u_y^2-2u_z^2 = -2\|\mathbf{u}\|^2 = -2\). (We originally defined \(\mathbf{u}\) as a unit vector.) Our final trace becomes:</p><p>

\[\begin{align*} &amp;&amp; \operatorname{tr}(R) &amp;= 3 + 0\sin(\theta) - 2(1-\cos(\theta)) \\ &amp;&amp;&amp;= 1 + 2\cos\theta \\
&amp;\implies&amp; \theta &amp;= \arccos\left(\frac{\operatorname{tr}(R)-1}{2}\right) \end{align*}\]

</p><p>That’s half of our logarithmic map. To recover \(\mathbf{\hat{u}}\), we can antisymmetrize \(R\). Recall \(\mathbf{\hat{u}}^T = -\mathbf{\hat{u}}\), and that \(\mathbf{\hat{u}}^2\) is symmetric (above).</p><p>

\[\begin{align*}
&amp;&amp; R - R^T &amp;= \mathcal{I} - \mathcal{I}^T + \sin(\theta)(\mathbf{\hat{u}}-\mathbf{\hat{u}}^T) + (1-\cos(\theta))(\mathbf{\hat{u}}^2-(\mathbf{\hat{u}}^2)^T) \\
&amp;&amp;&amp;= \sin(\theta)(\mathbf{\hat{u}}+\mathbf{\hat{u}}) + (1-\cos(\theta))(\mathbf{\hat{u}}^2-\mathbf{\hat{u}}^2) \\
&amp;&amp;&amp;= 2\sin(\theta)\mathbf{\hat{u}} \\
&amp;\implies&amp; \mathbf{\hat{u}} &amp;= \frac{1}{2\sin\theta}(R-R^T) \\
\end{align*}\]

</p><p>Finally, to get \(\mathbf{u}\), we can pull out the entries of \(\mathbf{\hat{u}}\), which we just derived:</p><p>

\[\mathbf{u} = \frac{1}{2\sin\theta}\begin{bmatrix}R_{32}-R_{23}\\ R_{13}-R_{31}\\R_{21}-R_{12}\end{bmatrix}\]

</p><p>We now have our full 3D logarithmic map!</p>

<h3 id="interpolation-1">Interpolation</h3>

<p>Now equipped with our 3D <code>exp</code> and <code>log</code> maps, we can use them for interpolation. The exact same formula as the 2D case still applies:</p><p>

\[R(t) = \exp(t\log(R_1R_0^{-1})))R_0\]

</p><canvas id="3d_exp_interp"></canvas>


<table><tbody><tr>
<td>$$ R_0 = \begin{bmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix} $$</td>
<td id="3d_exp_interp_mj">$$ R(0.00) = \begin{bmatrix}\phantom{-}1.00&amp;\phantom{-}0.00&amp;\phantom{-}0.00\\\phantom{-}0.00&amp;\phantom{-}1.00&amp;\phantom{-}0.00\\\phantom{-}0.00&amp;\phantom{-}0.00&amp;\phantom{-}1.00\end{bmatrix} $$</td>
<td id="3d_exp_interp_m1">$$ R_1 = \begin{bmatrix}-0.42&amp;-0.59&amp;-0.69\\0.51&amp;-0.79&amp;0.36\\-0.75&amp;-0.20&amp;0.63\end{bmatrix} $$</td>
</tr></tbody></table>

<p>Our interpolation scheme produces all the nice properties of axis/angle rotations—<strong>and</strong> chooses the shortest path every time. This wouldn’t look so smooth with Euler angles!</p>

<h2 id="averaging-rotations">Averaging Rotations</h2>

<p>However, we would have gotten an equally good interpolation scheme by just using quaternions instead of messing about with all this matrix math. Let’s consider something interesting we can <em>only</em> easily do with axis/angle rotations: averaging a set of rotation matrices.</p>

<p>The most straightforward method is to convert each matrix into an axis/angle rotation, average the resulting vectors, and convert back. That is certainly a valid strategy, but the resulting behavior won’t be very intuitive:</p>

<canvas id="rot_avg"></canvas>


<p>In particular, summing axis-angle vectors can result in “catastrophic cancellation.” An extreme example is averaging \(\begin{bmatrix}\pi&amp;0&amp;0\end{bmatrix}\) and \(\begin{bmatrix}-\pi&amp;0&amp;0\end{bmatrix}\), resulting in zero—which is clearly not representative of the two equivalent rotations.</p>

<p>To find an alternative, let’s first consider a slightly unconventional way of averaging points in the plane. The average of a set of points is the point that minimizes total squared distance to all others. Hence, there’s an optimization-based algorithm for finding it. Given \(x_0, \dots, x_n\), we can iterate the following procedure:</p>

<ul>
  <li>Pick an initial guess \(\bar{x} \in \mathbb{R}^2\) (can be one of the points).</li>
  <li>Repeat:
    <ul>
      <li>For each point, get its translation from the guess: \(\mathbf{u}_i \leftarrow x_i - \bar{x}\)</li>
      <li>Average the vectors: \(\mathbf{u} \leftarrow \frac{1}{n} \sum_{i=1}^n \mathbf{u}_i\)</li>
      <li>Step toward the average direction: \(\bar{x} \leftarrow \bar{x} + \tau\mathbf{u}\)</li>
    </ul>
  </li>
  <li>while \(\|\mathbf{u}\| &gt; \epsilon\).</li>
</ul>

<p><img src="http://thenumb.at/assets/exp-rotations/pointavg.svg"></p>

<p>As we run this procedure, \(\bar{x}\) will converge to the average point. Of course, we could have just averaged the points directly, but we’ll be able to translate this idea to the rotational case rather nicely.</p>

<p>Our logarithmic map lets us convert rotation matrices to axis axis/angle rotations, which are themselves just 3D points. So, what if we use the point averaging algorithm on rotations \(R_0, \dots, R_n\)?</p>

<ul>
  <li>Pick an initial guess \(\bar{R} \in \mathbb{R}^{3\times3}\) (can be \(\mathcal{I}\)).</li>
  <li>Repeat:
    <ul>
      <li>For each matrix, get its axis/angle rotation from the guess: \(\mathbf{u}_i \leftarrow \log(R_i\bar{R}^{-1})\)</li>
      <li>Average the vectors: \(\mathbf{u} \leftarrow \frac{1}{n} \sum_{i=1}^n \mathbf{u}_i\)</li>
      <li>Step toward the average rotation: \(\bar{R} \leftarrow \exp(\tau\mathbf{u})\bar{R}\)</li>
    </ul>
  </li>
  <li>while \(\|\mathbf{u}\| &gt; \epsilon\).</li>
</ul>

<canvas id="karcher"></canvas>


<p>The result of this algorithm is formally known as the <em>Karcher mean</em>. Just like how the average point minimizes total squared distance from all other points, the Karcher mean is a rotation that minimizes squared <em>angular</em> distance from all other rotations. Therefore, it won’t be subject to catastrophic cancellation—we’ll always end up with a non-zero in-between rotation.</p>

<p>Try comparing the two averaging algorithms—randomizing will keep them in sync. While the results are often similar, the Karcher mean exhibits more consistent behavior.</p>

<h2 id="quaternions-again">Quaternions (Again)</h2>

<p><em>Warning: section assumes knowledge of quaternions</em></p>

<p>Okay, I couldn’t resist talking about quaternions at least a little bit, given how closely they’re related to axis/angle rotations. Just like how complex exponentiation turned out to be equivalent to (skew-symmetric) 2D matrix exponentiation, quaternion exponentiation is equivalent to (skew-symmetric) 3D matrix exponentiation.</p>

<p>In 2D, an axis/angle rotation was simply \(\theta\). We created a pure-imaginary complex number \(i\theta\) and exponentiated it:</p><p>

\[e^{i\theta} = \cos\theta + i\sin\theta\]

</p><p>We got back a complex number that when multiplied with a point, rotates it by \(\theta\). It’s always the case that \(\|\cos\theta + i\sin\theta\| = 1\), so 2D rotations can be represented as unit-norm complex numbers.</p>

<p>In 3D, an axis/angle rotation is a vector \(\mathbf{u}\) such that \(\|\mathbf{u}\| = \theta\). What happens if we create a pure-imaginary quaternion \(\mathbf{q} = u_x\mathbf{i} + u_y\mathbf{j} + u_z\mathbf{k}\) and exponentiate it, too?</p>

<p>To make evaluating \(e^\mathbf{q}\) easier, first derive the following using the quaternion <a href="https://en.wikipedia.org/wiki/Quaternion#Multiplication_of_basis_elements">multiplication rules</a>:</p><p>

\[\begin{align*}
\mathbf{q}^2 &amp;= (u_x\mathbf{i} + u_y\mathbf{j} + u_z\mathbf{k})(u_x\mathbf{i} + u_y\mathbf{j} + u_z\mathbf{k}) \\
             &amp;= u_x^2\mathbf{i}^2 + u_xu_y\mathbf{i}\mathbf{j} + u_xu_z\mathbf{i}\mathbf{k} + u_yu_x\mathbf{j}\mathbf{i} + u_y^2\mathbf{j}^2 + u_yu_z\mathbf{j}\mathbf{k} + u_zu_x\mathbf{k}\mathbf{i} + u_zu_y\mathbf{k}\mathbf{j} + u_z^2\mathbf{k}^2 \\
             &amp;= -u_x^2 + u_xu_y\mathbf{k} - u_xu_z\mathbf{j} - u_yu_x\mathbf{k} - u_y^2 + u_yu_z\mathbf{i} + u_zu_x\mathbf{j} - u_zu_y\mathbf{i} - u_z^2 \\
             &amp;= -u_x^2-u_y^2-u_z^2\\
             &amp;= -\|\mathbf{q}\|^2\\
             &amp;= -\theta^2
\end{align*}\]

</p><p>Which is highly reminiscent of the skew-symmetric matrix identity used above. Therefore…</p><p>

\[\begin{align*}
e^\mathbf{q} &amp;= 1 + \mathbf{q} + \frac{\mathbf{q}^2}{2!} + \frac{\mathbf{q}^3}{3!} + \frac{\mathbf{q}^4}{4!} + \frac{\mathbf{q}^5}{5!} + \dots \\
             &amp;= 1 + \frac{\theta\mathbf{q}}{\theta} - \frac{\theta^2}{2!} - \frac{\theta^3\mathbf{q}}{3!\theta} + \frac{\theta^4}{4!} + \frac{\theta^5\mathbf{q}}{5!\theta} \dots \\
             &amp;= \left(1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \dots\right) + \frac{\mathbf{q}}{\theta}\left(\theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \dots\right) \\
             &amp;= \cos\theta + \frac{\mathbf{q}}{\theta}\sin\theta \\
             &amp;\approx \cos\theta + \frac{\mathbf{u}}{\|\mathbf{u}\|}\sin\theta
\end{align*}\]

</p><p>Our result looks almost exactly like the 2D case, just with three imaginary axes instead of one. In 2D, our axis/angle rotation became a unit-norm complex number. In 3D, it became a unit-norm quaternion. Now we can use this quaternion to rotate 3D points! Pretty cool, right?</p>

<p>One advantage of using quaternions is how easy the exponential map is to compute—if you don’t need a rotation matrix, it’s a good option. The quaternion logarithmic map is similarly simple:</p><p>

\[\theta = \arccos(\Re(\mathbf{q})), \mathbf{u} = \frac{1}{\sin\theta}\Im(\mathbf{q})\]

</p><p>Finally, note that the way to rotate a point \(\mathbf{p}\) by a quaternion \(\mathbf{q}\) is by evaluating the conjugation \(\mathbf{q}\mathbf{p}\mathbf{q}^{-1}\), where \(\mathbf{p} = p_x\mathbf{i} + p_y\mathbf{j} + p_z\mathbf{k}\) is another pure-imaginary quaternion representing our point. The conjugation technically rotates the point by \(2\theta\) about \(\mathbf{u}\), but that’s easily accounted for by making \(\|\mathbf{u}\| = \frac{\theta}{2}\) in the beginning.</p>

<h2 id="further-reading">Further Reading</h2>

<p>Made it this far? Well, there’s even more to learn about rotations.</p>

<p>Learn about quaternions <a href="https://eater.net/quaternions">here</a>, and why geometric algebra is more intuitive <a href="https://marctenbosch.com/quaternions/">here</a>.</p>

<p>Beyond understanding the four representations covered here (plus geometric algebra), it can be enlightening to learn about the algebraic structure underlying all 3D rotations: the group \(SO(3)\). I found <a href="https://www.youtube.com/watch?v=ACZC_XEyg9U">this video</a> to be a great resource: it explains \(SO(3)\) both intuitively and visually, demonstrating how it relates it to the group \(SU(2)\) as well as why quaternions and axis/angle rotations double-cover 3D rotation matrices.</p>

<p>The <a href="https://en.wikipedia.org/wiki/3D_rotation_group">wikipedia page on SO(3)</a> is also informative, though <em>very</em> math heavy. It touches on connections with axis/angle rotations, topology, \(SU(2)\), quaternions, and Lie algebra. It turns out the vector space of skew-symmetric matrices we derived above makes up \(\mathfrak{so}(3)\), the Lie algebra that corresponds to \(SO(3)\)—but I don’t know what that entails.</p>




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Search: The Bitter-Er Lesson (268 pts)]]></title>
            <link>https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d</link>
            <guid>40683697</guid>
            <pubDate>Fri, 14 Jun 2024 18:47:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d">https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d</a>, See on <a href="https://news.ycombinator.com/item?id=40683697">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's Recall AI feature is now indefinitely delayed (400 pts)]]></title>
            <link>https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/</link>
            <guid>40683210</guid>
            <pubDate>Fri, 14 Jun 2024 18:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/">https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/</a>, See on <a href="https://news.ycombinator.com/item?id=40683210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-index="0" data-js="panel" data-type="wysiwyg-with-aside" data-modular-content="" data-modular-content-collection="">
<p><em><b>Update: June 13, 2024:</b> Today, we are communicating an additional update on the Recall (preview) feature for Copilot+ PCs. Recall will now shift from a preview experience broadly available for Copilot+ PCs on June 18, 2024, to a preview available first in the <a href="https://www.microsoft.com/windowsinsider/">Windows Insider Program (WIP)</a> in the coming weeks. Following receiving feedback on Recall from our Windows Insider Community, as we typically do, we plan to make Recall (preview) available for all Copilot+ PCs coming soon. &nbsp;</em></p>
<p><em>We are adjusting the release model for Recall to leverage the expertise of the Windows Insider community to ensure the experience meets our high standards for quality and security. This decision is rooted in our commitment to providing a trusted, secure and robust experience for all customers and to seek additional feedback prior to making the feature available to all Copilot+ PC users. Additionally, as we shared in our <a href="https://www.microsoft.com/en-us/security/blog/2024/05/03/security-above-all-else-expanding-microsofts-secure-future-initiative/">May 3 blog</a>, security is our top priority at Microsoft, in line with our <a href="https://www.microsoft.com/en-us/microsoft-cloud/resources/secure-future-initiative">Secure Future Initiative (SFI)</a>. This is reflected in additional security protections we are providing for Recall content, including “just in time” decryption protected by <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security">Windows Hello Enhanced Sign-in Security (ESS)</a>, so Recall snapshots will only be decrypted and accessible when the user authenticates. The development of Copilot+ PCs, Recall and Windows will continue to be guided by SFI.&nbsp;</em></p>
<p><em>When Recall (preview) becomes available in the Windows Insider Program, we will publish a blog post with details on how to get the preview. To try Recall (preview) WIP customers will need a Copilot+ PC due to our <a href="https://support.microsoft.com/en-us/topic/copilot-pcs-hardware-requirements-35782169-6eab-4d63-a5c5-c498c3037364">hardware requirements</a>. We look forward to hearing Windows Insider feedback.&nbsp;&nbsp;&nbsp;</em></p>
<p>Today, we are sharing an update on the Recall (preview) feature for Copilot+ PCs, including more information on the set-up experience, privacy controls and additional details on our approach to security.</p>
<p>On May 20, <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">we introduced Copilot+ PCs</a>, our fastest, most intelligent Windows PCs ever. Copilot+ PCs have been reimagined from the inside out to deliver better performance and all new AI experiences to help you be more productive, creative and communicate more effectively. One of the new experiences exclusive to Copilot+ PCs is Recall, a new way to instantly find something you’ve previously seen on your PC. To create an explorable visual timeline, Recall periodically takes a snapshot of what appears on your screen. These images are encrypted, stored and analyzed locally, using on-device AI capabilities to understand their context. When logged into your Copilot+ PC, you can easily <a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c">retrace your steps</a> visually using Recall to find things from apps, websites, images and documents that you’ve seen, operating like your own virtual and completely private “photographic memory.” You are always in control of what’s saved. You can disable saving snapshots, pause temporarily, filter applications and delete your snapshots at any time.</p>
<p>As AI becomes more prevalent, we are rearchitecting Windows to give customers and developers more choice to leverage both the cloud and the power of local processing on the device made possible by the neural processing unit (NPU). This distributed computing model offers choice for both privacy and security. All of this work will continue to be guided by our <a href="https://www.microsoft.com/en-us/microsoft-cloud/resources/secure-future-initiative">Secure Future Initiative (SFI)</a>.</p>
<p>Our team is driven by a relentless desire to empower people through the transformative potential of AI and we see great utility in Recall and the problem it can solve. We also know for people to get the full value out of experiences like Recall, they have to trust it. That’s why we are launching Recall in preview on Copilot+ PCs – to give customers a choice to engage with <span data-contrast="auto">the feature</span><span data-contrast="auto"> early, or not, and to give </span>us an opportunity to learn from the types of <span data-ccp-parastyle="paragraph">real world</span><span data-ccp-parastyle="paragraph"> scenario</span>s customers and the Windows community finds most useful.</p>
<h3><strong>Listening to and acting on customer feedback</strong></h3>
<p>Even before making Recall available to customers, we have heard a clear signal that we can make it easier for people to choose to enable Recall on their Copilot+ PC and improve privacy and security safeguards. With that in mind we are announcing updates that will go into effect before Recall (preview) ships to customers on June 18.</p>
<ul>
<li>First, we are updating the set-up experience of Copilot+ PCs to give people a clearer choice to opt-in to saving snapshots using Recall. If you don’t proactively choose to turn it on, it will be off by default.<br>
<img fetchpriority="high" decoding="async" src="https://blogs.windows.com/wp-content/uploads/prod/sites/2/2024/06/OOBEAsset_1920-1024x649.png" alt="Recall user interface" width="1024" height="649"></li>
</ul>
<ul>
<li>Second, Windows Hello enrollment is required to enable Recall. In addition, proof of presence is also required to view your timeline and search in Recall.<br>
<img decoding="async" src="https://blogs.windows.com/wp-content/uploads/prod/sites/2/2024/06/Hello-1920-1024x581.png" alt="Windows Hello user interface" width="1024" height="581"></li>
</ul>
<ul>
<li>Third, we are adding additional layers of data protection including “just in time” decryption protected by <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security">Windows Hello Enhanced Sign-in Security (ESS)</a> so Recall snapshots will only be decrypted and accessible when the user authenticates. In addition, we encrypted the search index database.</li>
</ul>
<h3><strong>Secure by design and secure by default</strong></h3>
<p>In line with Microsoft’s SFI principles, before the preview release of Recall to customers, we are taking steps to increase data protection. Copilot+ PCs will launch with “just in time” decryption protected by Windows Hello Enhanced Sign-in Security (ESS), so Recall snapshots will only be decrypted and accessible when the user authenticates. This gives an additional layer of protection to Recall data in addition to other default enabled Window Security features like SmartScreen and Defender which use advanced AI techniques to help prevent malware from accessing data like Recall.</p>
<p>We also know the best way to secure information on a PC is to secure the whole PC itself. We want to reinforce what has previously been shared from David Weston, vice president of Enterprise and OS Security, about how <a href="https://www.microsoft.com/en-us/security/blog/2024/05/20/new-windows-11-features-strengthen-security-to-address-evolving-cyberthreat-landscape/">Copilot+ PCs have been designed to be secure by default</a> and share additional details about our security approach. Some notable examples of security enhancements include:</p>
<ul>
<li>All Copilot+ PCs will be Secured-core PCs, bringing advanced security to both commercial and consumer devices. In addition to the layers of protection in Windows 11, Secured-core PCs provide advanced firmware safeguards and dynamic root-of-trust measurement to help protect from chip to cloud.</li>
<li>Microsoft Pluton security processor will be enabled by default on all Copilot+ PCs. Pluton is a chip-to-cloud security technology – designed by Microsoft and built by silicon partners – with <a href="https://www.microsoft.com/security/business/zero-trust">Zero Trust</a> principles at the core. This helps protect credentials, identities, personal data and encryption keys, making them significantly harder to remove from the device, even if a user is tricked into installing malware or an attacker has physical possession of the PC.</li>
<li>All Copilot+ PCs will ship with <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security">Windows Hello Enhanced Sign-in Security (ESS)</a>. This provides more secure biometric sign ins and eliminates the need for a password.</li>
</ul>
<h3><strong>Protecting your privacy on Copilot+ PCs</strong></h3>
<p>In our early internal testing, we have seen different people use Recall in the way that works best for them. Some love the way it makes remembering what they’ve seen across the web so much easier to find than reviewing their browser history. Others like the way it allows them to better review an online course or find a PowerPoint. And people are taking advantage of the controls to exclude apps they don’t want captured in snapshots, from communication apps or Teams calls, or to delete some or all their snapshots. This is why we built Recall with fine-grained controls to allow each person to customize the experience to their comfort level, ensuring your information is protected and that you are in control of when, what and how it is captured.</p>
<ul>
<li><strong>Snapshots are stored locally.</strong> Copilot+ PCs have powerful AI that works on your device itself. No internet or cloud connections are used to store and process snapshots. Recall’s AI processing happens exclusively on your device, and your snapshots are kept safely on your local device only. Your snapshots are yours and they are not used to train the AI on Copilot+ PCs.</li>
<li><strong>Snapshots are not shared.</strong> Recall does not send your snapshots to Microsoft. Snapshots are not shared with any other companies or applications. Recall doesn’t share snapshots with other users who are signed into the same device, and per-user encryption ensures even administrators cannot view other users’ snapshots.</li>
<li><strong>You will know when Recall is saving snapshots.</strong> You’ll see Recall pinned to the taskbar when you reach your desktop. You’ll have a Recall snapshot icon on the system tray letting you know when Windows is saving snapshots.</li>
<li><strong>Digital rights managed or InPrivate browsing snapshots are not saved.</strong> Recall does not save snapshots of digital rights managed content or InPrivate browsing in <a href="https://support.microsoft.com/en-us/windows/privacy-and-control-over-your-recall-experience-d404f672-7647-41e5-886c-a3c59680af15#:~:text=Privacy%20and%20control%20over%20your%20Recall%20experience%201,stays%20local%204%20Built-in%20security%205%20Related%20articles">supported web browsers</a>.</li>
<li><strong>You can pause, filter and delete what’s saved at any time. </strong>You’re always in control of what’s saved as a snapshot. You can disable saving snapshots, pause them temporarily, filter applications and websites from being in snapshots, and delete your snapshots at any time.</li>
<li><strong>Enterprise and customer choice. </strong>For customers using managed work devices, your IT administrator is provided the control to disable the ability to save snapshots. However, your IT administrator cannot enable saving snapshots on your behalf. The choice to enable saving snapshots is solely yours.</li>
</ul>
<h3><strong>Empowering people with experiences they can trust</strong></h3>
<p>We are on a journey to build products and experiences that live up to our company mission to empower people and organizations to achieve more, and are driven by the critical importance of maintaining our customers’ privacy, security and trust. As we always do, we will continue to listen to and learn from our customers, including consumers, developers and enterprises, to evolve our experiences in ways that are meaningful to them.</p>
<p>We are excited for the upcoming launch of Copilot+ PCs on June 18 and for the innovative new features and benefits this entirely new category of PCs will bring. We will continue to build these new capabilities and experiences for our customers by prioritizing privacy, safety and security first. We remain grateful for the vibrant community of customers who continue to share their feedback with us.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Turning the Tables on AI (107 pts)]]></title>
            <link>https://ia.net/topics/turning-the-tables-on-ai</link>
            <guid>40682959</guid>
            <pubDate>Fri, 14 Jun 2024 17:42:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ia.net/topics/turning-the-tables-on-ai">https://ia.net/topics/turning-the-tables-on-ai</a>, See on <a href="https://news.ycombinator.com/item?id=40682959">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Artificial Intelligence has assimilated Microsoft, Facebook, Google, and now Apple. There is no escape: As soon as you open your computer you’re invited to skip thinking and let the machine fill the void. But what if we used AI not to think less but <em>more</em>?</p>
<p>AI has one trait: It makes everything the same. Every tech company now offers more or less the same service: “Think less.” How about Apple? Think different? Not anymore.</p>
<p>Tech companies big and small sell AI as something that thinks for us. It <em>does</em> replace thought with statistics—but it is not intelligent. No one knows what the future will bring. But is a future without thought a better future?</p>
<p>Now, with a tool that might help us think… How about using AI not to think less but <em>more</em>?</p>
<p><em>But I don’t know where to start. But I really like that ChatGPT sentence there. But I can’t say it better. But I’m stuck. But I’m worried I wrote something stupid. But I just need to correct some commas and typos. But I need a second opinion.</em></p>
<p>Let’s turn the tables on our excuses.</p>
<h2>1. Don’t ask AI, let AI ask you</h2>
<h3>a) But I don’t know where to start</h3>
<p>Give ChatGPT an outline and let it write for you. Let’s turn the tables and have ChatGPT prompt <em>us</em>. Tell AI to ask you questions about what you’re writing. Push yourself to express in clear terms what you really want to say. Like this, for example:</p>
<blockquote>
<p>I want to write [format] about [topic]. Ask me questions one at a time that force me to explain my idea.</p>
</blockquote>
<p>Keep asking until your idea is clear to you. Here’s how such a dialog where ChatGPT prompts <em>you</em> would look:</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/write-for-me.png" alt="">
<figcaption><b>Write for me:</b> This is how AI is sold to us. But why write if you have nothing to say? Who would read it, and why?</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/ask-ChatGPT.png" alt="">
<figcaption><b>Ask me instead:</b> Make AI ask *you* questions about your text. Copy-paste your answers. First draft done.</figcaption>
</figure>
<p>Once you have enough to work with, paste <em>your</em> answers into your text editor. You have just written your first draft without cheating.</p>
<p>ChatGPT may have asked or said some clever things along the way. Feeling tempted to steal it? Don’t. Rather use it as a chance to think and express what you feel.</p>
<h2>2. Don’t sell stolen goods—make your own</h2>
<h3>b) But I really like that ChatGPT sentence there</h3>
<p><em>What if the ChatGPT generated something useful that I want to keep?</em> Paste it as a note <em>Marked as AI.</em> Use quotes, use markup, and note its origin.</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/paste-as-AI-from-ChatGPT.png" alt="">
<figcaption><b>Paste as AI:</b> Want to use AI-generated output? In Writer, right-click and select Paste as AI.</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/text-marked-as-ai.png" alt="">
<figcaption><b>External sources in Writer:</b> iA Writer greys out text that you marked as AI so you can always discern what is yours and what isn’t.</figcaption>
</figure>
<h2>3. Don’t pretend. Create</h2>
<h3>c) But I can’t say it better</h3>
<p><em>What if Artificial Intelligence says exactly what I wanted to say?</em> Rethink and rewrite it to make it your own. Ask if what has been generated is really true. At first glance, it may look tight, but if you really think it through, you’ll find mistakes. Feel it, mean it, think it through. Say it your way.</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/cheating-with-ia-writer.png" alt="">
<figcaption><b>It’s not yours (yet):</b> Writer brings your own words to the foreground as you type over AI-generated text. But don’t just change a word and claim it as yours. Rethink and rewrite until you own it.</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/don-t-cheat-think.png" alt="">
<figcaption><b>This is yours:</b> Don’t sweat it if you have one or two common words left over from ChatGPT (here it’s “cheat,” in grey). What matters is that you thought it through by yourself.</figcaption>
</figure>
<p>Continue working on your first draft by cutting and ordering your thoughts. Focus on what you want to say and get yourself into the flow. And you can forget ChatGPT for the next few hours. The story is yours now.</p>
<h2>4. Editing: Cut, clarify, simplify</h2>
<p>Idea, structure, first draft, editing, and publishing seem to be separate steps. In reality, they’re connected. Your idea takes form as you write. Writing and editing go hand in hand until the idea wants to be shared. And even while you’re editing, Gemini, ChatGPT, and similar tools can help again.</p>
<h3>d) But I’m stuck</h3>
<p><em>Can’t find the right way to say something?</em> Tell ChatGPT to regenerate parts of your document, make it shorter and in the style of your favorite writer to help contrast your writing with somebody else’s.</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/rewrite-as-tom-wolfe.png" alt="">
<figcaption><b>The same in Tom Wolfe’s Style:</b> Your thoughts in a different style. Don’t copy-paste. Compare and improve.</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/rewrite-as-george-carlin.png" alt="">
<figcaption><b>The same in George Carlin’s Style:</b> Using comedians to see your thoughts from another angle is a nice way to catch a break, especially when you write about dark matters.</figcaption>
</figure>
<h3>e) But I’m worried I wrote something stupid</h3>
<p><em>Worried you missed something obvious?</em> You can use ChatGPT as an editor to list potential flaws, like long words, clichés, or factual errors.</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/list-long-words-with-alternatives.png" alt="">
<figcaption><b>Find long words:</b> List long words and suggest shorter alternatives. Don’t simply find and replace. Rethink the sentences.</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/complete-rewrite.png" alt="">
<figcaption><b>Don’t replace words, rethink, rewrite:</b> Don’t fix your writing by replacing words. Use the opportunity to rethink and rewrite.</figcaption>
</figure>
<p>Use “list” rather than rewrite. Rewriting can lead to all sorts of unwanted changes. They can be hard to spot because ChatGPT seems so smooth, grammatically and orthographically.</p>
<h3>f) But I just need to correct some commas and typos</h3>
<p><em>But what if it’s really just commas or typos?</em> Then paste ChatGPT’s text over your own as edits. With Authorship enabled, iA Writer will show you what ChatGPT changed. This way you’ll make sure that it doesn’t rewrite things you said.</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/fix-typos-with-chatgpt.png" alt="">
<figcaption><b>Paste edits as:</b> You can paste the edited text over the text you have written.</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/fixed-comma.png" alt="">
<figcaption><b>AI changes are grey:</b> In this case, ChatGPT has added a comma. Make sure you agree with every change, no matter how small.</figcaption>
</figure>
<h3>g) But I need a second opinion</h3>
<p><em>What weaknesses have I missed in my writing?</em> Ask ChatGPT for progressively harsher critiques of your writing. That can be unpleasant, but that’s how you find and fix your blind spots, without the shame.</p>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/criticize-as-hard-as-you-can.png" alt="">
<figcaption><b>Criticize my text:</b> Ask for harsh feedback, otherwise ChatGPT will be too forgiving.</figcaption>
</figure>
<figure>
<img decoding="async" src="https://ia.net/wp-content/uploads/2024/06/syntax-highlight.png" alt="">
<figcaption><b>Far from done:</b> ChatGPT will find weaknesses. But rather than blindly obeying, rethink weaknesses that you agree with. Use iA Writer’s Syntax Highlight to find more flaws (did you spot the two “now’s”?).</figcaption>
</figure>
<h2>Why bother?</h2>
<p>Enthusiasts present AI as a magic wand that can solve humanity’s biggest problems. In the meantime, it uses an exponential amount of energy to make everything the same.</p>
<p>With every thought we outsource, we miss out on a chance to grow. Love it or hate it, AI is here to stay. However we use it, we need to think more, not less.</p>
<p>
<iframe width="100%" height="30%" src="https://www.youtube.com/embed/OpPjqj4OOE4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vercel ends open-source sponsorship program giving projects 24hr notice (110 pts)]]></title>
            <link>https://vercel.com/guides/can-vercel-sponsor-my-open-source-project</link>
            <guid>40682711</guid>
            <pubDate>Fri, 14 Jun 2024 17:19:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vercel.com/guides/can-vercel-sponsor-my-open-source-project">https://vercel.com/guides/can-vercel-sponsor-my-open-source-project</a>, See on <a href="https://news.ycombinator.com/item?id=40682711">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><article><p>Our open-source sponsorship program is <b>currently paused</b> as we make improvements.</p><p>We’re proud to have supported <b>over 700 teams</b> and individuals so far with almost <b>$2M dollars</b> of Vercel credits. This includes popular open-source projects like Tailwind CSS, Nuxt, Svelte, styled-components, as well as non-profits like the Red Cross.</p><p>While we aren’t accepting <i>new team sponsorships</i> at the moment, individuals can still get started building on Vercel completely free. See the <a href="https://vercel.com/docs/limits/fair-use-guidelines#typical-monthly-usage-guidelines" rel="noopener" target="_blank">free tier usage guidelines</a> to learn more. Thank you.</p></article></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The sun's magnetic field is about to flip (336 pts)]]></title>
            <link>https://www.space.com/sun-magnetic-field-flip-solar-maximum-2024</link>
            <guid>40682401</guid>
            <pubDate>Fri, 14 Jun 2024 16:48:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/sun-magnetic-field-flip-solar-maximum-2024">https://www.space.com/sun-magnetic-field-flip-solar-maximum-2024</a>, See on <a href="https://news.ycombinator.com/item?id=40682401">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<p><img src="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-320-80.gif" alt="Image of the sun with a bar magnet graphic rotating in front of it " srcset="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-320-80.gif 320w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-480-80.gif 480w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-650-80.gif 650w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-970-80.gif 970w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-1024-80.gif 1024w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-1200-80.gif 1200w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-1920-80.gif 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN.gif" data-pin-media="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN.gif">
</p>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN.gif">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span>The sun's magnetic field is about to flip.</span>
<span itemprop="copyrightHolder">(Image credit: Created in Canva by Daisy Dobrijevic)</span>
</figcaption>
</div>

<div id="article-body">
<p>The sun is on the verge of a significant event: a magnetic field reversal.&nbsp;</p><p>This phenomenon happens roughly every 11 years and marks an important stage in the <a data-analytics-id="inline-link" href="https://www.space.com/solar-cycle-frequency-prediction-facts" data-before-rewrite-localise="https://www.space.com/solar-cycle-frequency-prediction-facts"><u>solar cycle</u></a>. The shift in polarity indicates the halfway point of <a data-analytics-id="inline-link" href="https://www.space.com/what-is-solar-maximum-and-when-will-it-happen" data-before-rewrite-localise="https://www.space.com/what-is-solar-maximum-and-when-will-it-happen"><u>solar maximum</u></a>, the height of solar activity, and the beginning of the shift toward solar minimum.&nbsp;</p><p>The last time <a data-analytics-id="inline-link" href="https://www.space.com/58-the-sun-formation-facts-and-characteristics.html" data-before-rewrite-localise="https://www.space.com/58-the-sun-formation-facts-and-characteristics.html"><u>the sun</u></a>'s magnetic field flipped was toward the end of 2013. But what causes this switch in polarity, and is it dangerous? Let's take a deep look at the sun's magnetic field reversal and investigate the effects it could have on <a data-analytics-id="inline-link" href="https://www.space.com/54-earth-history-composition-and-atmosphere.html" data-before-rewrite-localise="https://www.space.com/54-earth-history-composition-and-atmosphere.html"><u>Earth</u></a>.</p><p><strong>Related: </strong><a data-analytics-id="inline-link" href="https://www.space.com/giant-sunspot-ar3664-solar-storms-aurora" data-before-rewrite-localise="https://www.space.com/giant-sunspot-ar3664-solar-storms-aurora"><u>How a giant sunspot unleashed solar storms that spawned global auroras that just dazzled us all</u></a></p><p>To understand the magnetic field's reversal, first, it's important to be familiar with the solar cycle. This approximately 11-year cycle of solar activity is driven by the sun's magnetic field and is indicated by the frequency and intensity of <a data-analytics-id="inline-link" href="https://www.space.com/sunspots-formation-discovery-observations" data-before-rewrite-localise="https://www.space.com/sunspots-formation-discovery-observations"><u>sunspots</u></a> visible on the surface. The height of solar activity during a given solar cycle is known as solar maximum, and current estimates predict it will occur between <a data-analytics-id="inline-link" href="https://www.swpc.noaa.gov/products/solar-cycle-progression#:~:text=The%20Prediction%20Panel%20predicted%20Cycle,November%202024%20and%20March%202026.&amp;text=SWPC%20Space%20Weather%20Operations%20(SWO)%2C%20Daily%20Observations." data-url="https://www.swpc.noaa.gov/products/solar-cycle-progression#:~:text=The%20Prediction%20Panel%20predicted%20Cycle,November%202024%20and%20March%202026.&amp;text=SWPC%20Space%20Weather%20Operations%20(SWO)%2C%20Daily%20Observations." target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>late 2024 and early 2026</u></a>.</p><p>But there is another very important, albeit lesser-known, cycle that encapsulates two 11-year solar cycles. Known as the Hale cycle, this magnetic cycle lasts approximately 22 years, through which the sun's magnetic field reverses and then reverts to its original state, <a data-analytics-id="inline-link" href="https://www.space.com/author/ryan-french" data-before-rewrite-localise="https://www.space.com/author/ryan-french"><u>Ryan French</u></a>, a solar astrophysicist and Space.com contributing writer, told Space.com.&nbsp;</p><p>During solar minimum, the sun's magnetic field is close to a dipole, with one north pole and one south pole, similar to <a data-analytics-id="inline-link" href="https://www.space.com/earths-magnetic-field-explained" data-before-rewrite-localise="https://www.space.com/earths-magnetic-field-explained"><u>Earth's magnetic field</u></a>. But as we shift toward solar maximum, "the sun's magnetic field becomes more complex, without a clear north-south pole separation," French said. By the time solar maximum passes and solar minimum arrives, the sun has returned to a dipole, albeit with a flipped polarity.&nbsp;</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-SQesLi2fRyiUNYk4wTvBaW"><section><p>Sign up to our newsletter for the latest updates on rocket launches, skywatching events and more!</p></section></div><p>The upcoming switch in polarity will be from the northern to southern magnetic field in the Northern Hemisphere and vice versa in the Southern Hemisphere. "This will bring it to a similar magnetic orientation to Earth, which also has its southern-pointing magnetic field in the Northern Hemisphere," French explained.</p><h2 id="what-causes-the-switch-in-polarity-xa0-3">What causes the switch in polarity?&nbsp;</h2><p>The reversal is driven by sunspots, magnetically complex regions of the sun's surface that can spawn significant solar events, such as <a data-analytics-id="inline-link" href="https://www.space.com/solar-flares-effects-classification-formation" data-before-rewrite-localise="https://www.space.com/solar-flares-effects-classification-formation"><u>solar flares</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/coronal-mass-ejections-cme" data-before-rewrite-localise="https://www.space.com/coronal-mass-ejections-cme"><u>coronal mass ejections</u></a> (CMEs) — large blasts of plasma and magnetic field.</p><a target="_blank" data-url="" href="" data-hl-processed="none"><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-320-80.png" alt="Diagram of where sunspots are located on the sun at different times during the solar cycle." srcset="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK.png"></picture></p></div><figcaption itemprop="caption description"><span>During solar maximum a large number of sunspots are visible at mid-latitudes and during solar minimum a very small number (sometimes zero) of sunspots are visible at the equator.&nbsp; </span><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure></a><p>As sunspots emerge close to the equator, they will have an orientation matching the old magnetic field, while sunspots forming closer to the poles will have a magnetic field matching the incoming magnetic orientation, French said. This is called Hale's law.&nbsp;</p><p>"The magnetic field from active regions makes its way toward the poles and eventually causes the reversal," solar physicist Todd Hoeksema, director of the Wilcox Solar Observatory at Stanford University, <a data-analytics-id="inline-link" href="https://www.space.com/22310-sun-magnetic-field-flip-mystery.html" data-before-rewrite-localise="https://www.space.com/22310-sun-magnetic-field-flip-mystery.html"><u>previously told Space.com</u></a>.</p><p>But the exact underlying cause of such a flip in polarity remains mysterious. "That gets into the whole [solar] cycle, and wondering what that is," Stanford University solar physicist Phil Scherrer previously told Space.com. "We still don't have a really self-consistent mathematical description of what's happening. And until you can model it, you don't really understand it — it's hard to really understand it."</p><p>It really depends on where the magnetic field comes from. "Are there going to be many sunspots? And are the sunspots going to contribute to the magnetic field of the pole, or are they going to kind of cancel locally?" Hoeksema said. "That question we don't yet know how to answer."</p><h2 id="how-quickly-does-the-switch-occur-xa0-3">How quickly does the switch occur?&nbsp;</h2><p>What we do know is that the solar magnetic field flip is not instantaneous. It's a gradual transition from a dipole to a complex magnetic field, to a reversed dipole over the entire 11-year solar cycle. "In short, there is no specific 'moment' in which the sun's poles flip," French said. "It's not like the Earth, where the flip is measured by the migration of the North/South pole."&nbsp;</p><p>It generally takes a year or two for a complete reversal, but it can vary significantly. For example, the north polar field of Solar Cycle 24, which ended in December 2019, took nearly five years to reverse, according to the <a data-analytics-id="inline-link" href="https://nso.edu/blog/polar-magnetic-field-reversal" target="_blank" data-url="https://nso.edu/blog/polar-magnetic-field-reversal" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>National Solar Observatory</u></a>.</p><p>The magnetic field flip is so gradual, you won't even notice when it happens. And no, however dramatic it might sound, it is not the sign of an impending apocalypse. "The world will not end tomorrow," Scherrer <a data-analytics-id="inline-link" href="https://www.space.com/22289-sun-magnetic-field-flip-earth-effects.html" data-before-rewrite-localise="https://www.space.com/22289-sun-magnetic-field-flip-earth-effects.html"><u>previously told</u></a> Space.com.</p><p>However, we will experience some of the polarity flip's side effects.&nbsp;</p><h2 id="how-does-the-sun-apos-s-magnetic-flip-affect-us-xa0-3">How does the sun's magnetic flip affect us? &nbsp;</h2><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-320-80.jpg" alt="the sun is at the center and a twirling purple sheet extends out far into the solar system. It looks a bit like a dancers skirt rippling and ruffling as they spin." srcset="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT.jpg"></picture></p></div><figcaption itemprop="caption description"><span>Artist's impression of the heliospheric current sheet which becomes more wavy when the sun's magnetic field flips. </span><span itemprop="copyrightHolder">(Image credit: NASA)</span></figcaption></figure><p>There is no doubt that the sun has been incredibly active recently, firing out numerous powerful solar flares and CMEs, triggering strong geomagnetic storms on Earth, which, in turn, have produced some <a data-analytics-id="inline-link" href="https://www.space.com/solar-storms-may-2024-strongest-auroras-500-years" data-before-rewrite-localise="https://www.space.com/solar-storms-may-2024-strongest-auroras-500-years"><u>incredible auroral displays of late</u></a>.&nbsp;</p><p>However, the increased severity of <a data-analytics-id="inline-link" href="https://www.space.com/space-weather" data-before-rewrite-localise="https://www.space.com/space-weather"><u>space weather</u></a> is not the direct cause of the flip in polarity. Rather, these things tend to occur together, Hoeksema told Space.com in 2013.</p><p>Space weather is typically the strongest during solar maximum, when the sun's magnetic field is also the most complex, according to French.&nbsp;</p><p>One side effect of the magnetic field shift is slight but primarily beneficial: It can help shield Earth from galactic <a data-analytics-id="inline-link" href="https://www.space.com/32644-cosmic-rays.html" data-before-rewrite-localise="https://www.space.com/32644-cosmic-rays.html"><u>cosmic rays</u></a> — high-energy subatomic particles that travel at near light speed and can damage spacecraft and harm orbiting astronauts who are outside Earth's protective atmosphere.&nbsp;</p><p>As the sun's magnetic field shifts, the "current sheet" — a sprawling surface that radiates billions of miles outward from the sun's equator — <a data-analytics-id="inline-link" href="https://www.nasa.gov/science-research/heliophysics/the-suns-magnetic-field-is-about-to-flip/" target="_blank" data-url="https://www.nasa.gov/science-research/heliophysics/the-suns-magnetic-field-is-about-to-flip/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>becomes very wavy</u></a>, providing a better barrier against cosmic rays. &nbsp;</p><h2 id="predicting-future-solar-cycle-strengths-xa0-3">Predicting future solar cycle strengths &nbsp;</h2><p>Scientists will be keeping a watchful eye on the sun's magnetic field reversal and seeing how long it takes for it to bounce back into a dipole configuration. If that happens within the next couple of years, the next 11-year cycle will be relatively active, but if the buildup is slow, the cycle will be relatively weak, like the previous Solar Cycle 24.&nbsp;</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>
<div id="slice-container-authorBio-SQesLi2fRyiUNYk4wTvBaW"><p>Daisy Dobrijevic joined <a href="https://www.space.com/" data-before-rewrite-localise="https://www.space.com/">Space.com</a> in February 2022 having previously worked for our sister publication <a href="https://www.space.com/43203-all-about-space-free-issue.html" target="_blank" data-before-rewrite-localise="https://www.space.com/43203-all-about-space-free-issue.html">All About Space</a> magazine as a staff writer. Before joining us, Daisy completed an editorial internship with the BBC Sky at Night Magazine and worked at the <a href="https://spacecentre.co.uk/?gclid=Cj0KCQjw3f6HBhDHARIsAD_i3D-mw3nFf8xOLVjxawebnwAOTgxjs-OAi0TTmNvL8gD1MjcujcBiU6QaAlJ1EALw_wcB" target="_blank">National Space Centre</a> in Leicester, U.K., where she enjoyed communicating space science to the public. In 2021, Daisy completed a PhD in plant physiology and also holds a Master's in Environmental Science, she is currently based in Nottingham, U.K. Daisy is passionate about all things space, with a penchant for solar activity and space weather. She has a strong interest in astrotourism and loves nothing more than a good northern lights chase!&nbsp;</p></div>


</section>




<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Effect – Build robust apps in TypeScript (115 pts)]]></title>
            <link>https://effect.website/</link>
            <guid>40682149</guid>
            <pubDate>Fri, 14 Jun 2024 16:20:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://effect.website/">https://effect.website/</a>, See on <a href="https://news.ycombinator.com/item?id=40682149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2><span>The best way to</span><br><span><span>ship faster</span><span>ship faster</span></span><span>in TypeScript</span></h2><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Maximum Type-safety (incl. error handling)</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Makes your code more composable, reusable and testable</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Extensive library with a rich ecosystem of packages</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Clustering and Workflows (Alpha)</span></li></ul></div><div><h2>Make the hard things easy</h2><p>As your application grows, Effect scales with it - keeping your code simple and maintainable.</p><div><svg viewBox="0 0 577 211" fill="none" xmlns="http://www.w3.org/2000/svg"><rect x="0.5" y="187" width="576" height="1" fill="url(#paint0_linear_280_1304)"></rect><line x1="0.957328" y1="168.502" x2="502.957" y2="125.502" stroke="url(#paint1_linear_280_1304)"></line><path d="M1.5 177C149.455 159.787 424 116 502.5 1" stroke="url(#paint2_linear_280_1304)"></path><defs><linearGradient id="paint0_linear_280_1304" x1="0.499992" y1="187.954" x2="576.5" y2="187.81" gradientUnits="userSpaceOnUse"><stop stop-color="#18181B"></stop><stop offset="0.177083" stop-color="#71717A"></stop><stop offset="1" stop-color="#09090B"></stop></linearGradient><linearGradient id="paint1_linear_280_1304" x1="1" y1="169" x2="503" y2="126" gradientUnits="userSpaceOnUse"><stop stop-color="#3178C6" stop-opacity="0.25"></stop><stop offset="0.515625" stop-color="#3178C6"></stop><stop offset="1" stop-color="#3178C6"></stop></linearGradient><linearGradient id="paint2_linear_280_1304" x1="502.5" y1="0.9998" x2="2.49996" y2="187" gradientUnits="userSpaceOnUse"><stop stop-color="#F97583"></stop><stop offset="0.489583" stop-color="#F97583"></stop><stop offset="1" stop-color="#F97583" stop-opacity="0.25"></stop></linearGradient></defs></svg><p><span><span>Complexity</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11 14"><path d="M9.31328 2.625H9.75078V3.0625V9.1875V9.625H8.87578V9.1875V4.11797L1.96602 11.0277L1.65703 11.3367L1.03906 10.7187L1.34805 10.4098L8.25781 3.5H3.18828H2.75078V2.625H3.18828H9.31328Z"></path></svg></span><span>(Lower is better)</span><span><span>–</span><span>Without Effect</span></span><span><span>–</span><span>With Effect</span></span></p></div><div><div><h3>Without Effect</h3></div><div><h3>With <span>Effect</span><svg viewBox="0 0 149 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M29.8022 24.317C30.2747 24.05 30.4361 23.4582 30.1636 22.9953C29.891 22.5329 29.2873 22.3741 28.8148 22.6411L15.9211 29.9362L3.07463 22.6683C2.60281 22.4012 1.999 22.5594 1.72597 23.0225C1.45347 23.4854 1.61541 24.077 2.08741 24.3441L15.3897 31.8698C15.5053 31.9353 15.6327 31.9771 15.7645 31.9929C15.8963 32.0087 16.0299 31.9981 16.1576 31.9617C16.278 31.9433 16.3941 31.9031 16.5002 31.8431L29.8022 24.317Z" fill="white"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M31.1298 16.6012C31.1974 16.1929 31.0061 15.7675 30.6177 15.5488L16.555 7.63105C16.4443 7.56873 16.3234 7.52682 16.198 7.50732C16.0631 7.46888 15.922 7.45758 15.7827 7.47405C15.6434 7.49053 15.5088 7.53446 15.3865 7.60332L1.32289 15.5214C0.913972 15.7518 0.723686 16.2117 0.824499 16.6391C0.780205 16.9913 0.91787 17.3598 1.32768 17.5916L15.3904 25.5478C15.5127 25.6169 15.6475 25.661 15.7869 25.6776C15.9263 25.6942 16.0675 25.6829 16.2026 25.6445C16.3297 25.6253 16.4522 25.583 16.5642 25.5197L30.6275 17.563C31.0408 17.329 31.1776 16.9562 31.1298 16.6012ZM28.2266 16.5591L15.9459 9.64453L3.67206 16.5554L15.9528 23.5034L28.2266 16.5591Z" fill="white"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M31.3429 10.6097C31.8677 10.3131 32.0476 9.65608 31.7442 9.14178C31.4416 8.62819 30.7712 8.45201 30.2464 8.74854L15.9269 16.8501L1.66063 8.77876C1.13584 8.48152 0.465408 8.65787 0.162793 9.172C-0.14053 9.68541 0.0391253 10.3432 0.564095 10.6397L15.337 18.9976C15.4654 19.0702 15.607 19.1165 15.7534 19.1339C15.8998 19.1514 16.0482 19.1395 16.19 19.0991C16.3236 19.0791 16.4524 19.0347 16.5701 18.9681L31.3429 10.6097Z" fill="white"></path><path d="M2.7403 9.6795L15.8991 1.62024L29.0577 9.67879L15.8989 17.2013L2.7403 9.6795Z" fill="white"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M31.3255 8.49027C31.8513 8.78627 32.0333 9.44279 31.7325 9.95692C31.4307 10.4707 30.7603 10.6474 30.2344 10.3514L15.9128 2.28787L1.64317 10.3224C1.11731 10.6184 0.44688 10.4415 0.145328 9.92794C-0.15587 9.41381 0.0262664 8.75729 0.55159 8.46129L15.325 0.143725C15.4534 0.0713274 15.5949 0.0251207 15.7412 0.00776107C15.8875 -0.00959854 16.0358 0.00223134 16.1775 0.0425706C16.3093 0.0631109 16.4364 0.107185 16.5526 0.172702L31.3255 8.49027Z" fill="white"></path><path d="M109.663 25.038C106.779 25.038 105.211 23.694 105.211 20.53V13.698H103.167V10.478H105.211V7.538L108.991 7.146V10.478H112.071V13.698H108.991V20.334C108.991 21.258 109.439 21.678 110.167 21.678H111.735V25.038H109.663Z" fill="white"></path><path d="M96.4072 25.402C92.2352 25.402 88.9032 21.986 88.9032 17.758C88.9032 13.53 92.1512 10.142 96.6032 10.114C99.2632 10.086 101.419 11.122 102.651 12.83L99.7392 15.21C99.0672 14.202 97.9472 13.558 96.6592 13.558C94.2512 13.558 92.6832 15.49 92.6832 17.758C92.6832 20.026 94.3352 21.958 96.7432 21.958C98.1992 21.958 99.1512 21.23 99.9072 20.222L102.651 22.602C101.279 24.366 99.2632 25.402 96.4072 25.402Z" fill="white"></path><path d="M80.6032 25.402C76.0952 25.402 73.0712 21.986 73.0712 17.786C73.0712 13.586 76.2352 10.114 80.6032 10.114C84.9712 10.114 87.9952 13.586 87.9952 17.786C87.9952 18.206 87.9672 18.654 87.8832 19.13H76.9912C77.4112 20.838 78.6992 22.042 80.6032 22.042C82.2272 22.042 83.4872 21.202 84.1592 20.082L87.0992 22.294C85.9232 24.114 83.4872 25.402 80.6032 25.402ZM76.9912 16.302H84.1872C83.7672 14.678 82.3952 13.362 80.5472 13.362C78.7552 13.362 77.4112 14.566 76.9912 16.302Z" fill="white"></path><path d="M66.328 9.498C66.328 6.558 68.232 4.598 71.172 4.598H72.74V7.818H71.76C70.64 7.818 70.108 8.406 70.108 9.554V10.478H73.076V13.698H70.108V25.038H66.328V13.698H64.284V10.478H66.328V9.498Z" fill="white"></path><path d="M56.9764 9.498C56.9764 6.558 58.8804 4.598 61.8204 4.598H63.3884V7.818H62.4084C61.2884 7.818 60.7564 8.406 60.7564 9.554V10.478H63.7244V13.698H60.7564V25.038H56.9764V13.698H54.9324V10.478H56.9764V9.498Z" fill="white"></path><path d="M41.8917 25.038V5.438H53.3717V8.966H45.8117V13.362H53.2317V16.778H45.8117V21.51H53.4277V25.038H41.8917Z" fill="white"></path></svg></h3></div></div></div><div><h2>The missing standard library for TypeScript</h2><p>TypeScript/JavaScript, the most popular programming language, is still missing a standard library. Effect is filling this gap by providing a solid foundation of data structures, utilities, and abstractions to make building applications easier.
</p><a target="_blank" href="https://2022.stateofjs.com/en-US/opinions/#top_currently_missing_from_js"><span>See 2022 State of JavaScript survey</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11 14"><path d="M9.31328 2.625H9.75078V3.0625V9.1875V9.625H8.87578V9.1875V4.11797L1.96602 11.0277L1.65703 11.3367L1.03906 10.7187L1.34805 10.4098L8.25781 3.5H3.18828H2.75078V2.625H3.18828H9.31328Z"></path></svg></a></div><section><div><div><div><h3>Powerful building blocks</h3></div><p>Every part of the Effect ecosystem is designed to be composable. The Effect primitives can be combined in many different ways to tackle the most complex problems.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Immutable data structures</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Asynchronous queues &amp; pub-sub</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Configuration &amp; dependency management</span></li></ul><a href="https://effect.website/docs"></a></div><div><div><h3>No more one-off dependencies</h3></div><p>With Effect the batteries are included. Regardless of the application, your package.json will have never been this small.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Data validation &amp; serialization</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Frameworks for CLI &amp; HTTP applications</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Powerful abstractions for every platform</span></li></ul><a href="https://effect.website/docs"></a></div><div><div><h3>Never try &amp; catch again</h3></div><p>Effect doesn't shy away from errors — it embraces them as a fact of life. Successfully handle failure with the built-in error-handling primitives.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Type-safe errors as values</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Powerful retry &amp; recovery APIs</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Tools for logging &amp; tracing</span></li></ul><a href="https://effect.website/docs"></a></div></div><hr></section><div><h2>Let's see some<!-- --> <span>example code</span></h2><p>Doing the right thing in TypeScript is hard. Effect makes it easy!<br>We have collated some examples to show you how Effect can be utilized in your next project.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Effect helps you with handling errors, async code, concurrency, streams and much more.</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Effect provides a unified replacement for many one-off dependencies.</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Effect integrates deeply with your current tech stack.</span></li></ul></div><section><div><h2>Effect gives you new Superpowers</h2><div><div><p><h3>Composable &amp; Reusable</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Everything in the Effect ecosystem is designed to work together. Building applications with Effect feels like playing with Lego.</p></div><div><p><h3>Maximum Type-Safety</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Effect leverages the full power of TypeScript to give you confidence in your code. From errors to managing dependencies — if it compiles, it works.</p></div><div><p><h3>Built-In Tracing</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>The built-in support for tracing is first-class. Effect integrates seamlessly with OpenTelemetry to give you full visibility over your application's performance.</p></div><div><p><h3>Built-In Metrics</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Effect has built-in support for metrics. You can use the provided OpenTelemetry exporter to integrate with a wide range of dashboards.</p></div><div><p><h3>Easy to refactor</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Make changes to your app with confidence. Effect's powerful abstractions make it easy to refactor your code without breaking anything.</p></div></div></div><hr></section><div><h2>What Effect users are saying</h2></div><div><h2>Okay, so what's the catch?</h2><div><div><div><h3>Learning curve</h3></div><p>Learning Effect can be quite daunting but it doesn’t take long for you to be productive. It’s similar to learning TypeScript. It’s worth it.</p></div><div><div><h3>Different programming style</h3></div><p>To ensure a high level of type-safety, composabilty and tree-shakability, Effect's programming style might be different from what you're used to.</p></div><div><div><h3>Extensive API surface</h3></div><p>Effect has an API for every situation. While it may take some time to learn them all, a handful of the most common ones will get you a long way.</p></div></div></div><section><div><h2>Frequently asked questions</h2><div><div><h3>Can I incrementally adopt Effect?</h3><p>Yes! Adopting Effect you can start by refactoring small portions of your app, usually the ones with higher complexity, and keep going as you see fit</p></div><div><h3>Does Effect scale?</h3><p>Effect was built for production since the very beginning, we take good care of making everything as performant as possible, by providing you with better ways to deal with concurrency and great observability finding bottlenecks in your program becomes easy</p></div><div><h3>Do I have to know functional programming?</h3><p>No! While Effect makes usage of Functional Programming principles and patterns internally you can be proficient in Effect by simply using it as a smart Promise and forget that there is even a thing called Functional Programming</p></div><div><h3>The library is huge, do I have to know it all?</h3><p>No! Every module in Effect is made with a specific problem in mind that is deemed to be common enough but you don't need to know everything, in fact you can harness 80% of the productivity gain by just learning a few functions and 2-3 core modules.</p></div><div><h3>What's the minimum bundle size?</h3><p>The core of Effect is a runtime system that weights about 15k when compressed and tree-shaken, the rest scales with usage. If you end up using 100k of Effect code there is a good chance your app would have been 1Mb if not using Effect</p></div><div><h3>Any more questions?</h3><p>Feel free to reach out in our Discord community!</p><a rel="noopener noreferrer" target="_blank" href="https://discord.gg/effect-ts"></a></div></div></div><hr></section><div><h2>Join our welcoming community</h2><div><div><p><img alt="Community member" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=640&amp;q=75 640w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=750&amp;q=75 750w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=828&amp;q=75 828w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=1080&amp;q=75 1080w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=1200&amp;q=75 1200w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=1920&amp;q=75 1920w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=2048&amp;q=75 2048w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=3840&amp;q=75 3840w" src="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=3840&amp;q=75"></p></div><div><p><img alt="Community member" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=640&amp;q=75 640w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=750&amp;q=75 750w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=828&amp;q=75 828w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=1080&amp;q=75 1080w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=1200&amp;q=75 1200w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=1920&amp;q=75 1920w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=2048&amp;q=75 2048w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=3840&amp;q=75 3840w" src="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=3840&amp;q=75"></p></div></div></div><div><h2><span>Start to</span><br><span><span>ship faster</span></span><br><span>in TypeScript</span></h2><p>Effect makes it easy to build typed, robust &amp; scalable applications. Check out our friendly documentation to get started.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nemotron-4-340B (118 pts)]]></title>
            <link>https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/</link>
            <guid>40682000</guid>
            <pubDate>Fri, 14 Jun 2024 16:01:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/">https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/</a>, See on <a href="https://news.ycombinator.com/item?id=40682000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	<main id="main" role="main">
			<div>
		

<article id="post-72165" data-url="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/" data-identifier="72165 https://blogs.nvidia.com/?p=72165" data-title="NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models">

	<!-- .entry-header -->

	<!-- META -->
	<!-- .entry-meta -->

	<div>
		<p dir="ltr">NVIDIA today announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry.</p>
<p dir="ltr">High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access.</p>
<p dir="ltr">Through a uniquely permissive <a href="https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf" target="_blank" rel="noopener">open model license</a>, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.</p>
<p dir="ltr">The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with <a href="https://github.com/NVIDIA/NeMo" target="_blank" rel="noopener">NVIDIA NeMo</a>, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source <a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener">NVIDIA TensorRT-LLM</a> library.</p>
<p dir="ltr">Nemotron-4 340B can be downloaded now from <a href="https://huggingface.co/collections/nvidia/nemotron-4-340b-666b7ebaf1b3867caf2f1911" target="_blank" rel="noopener">Hugging Face</a>. Developers will soon be able to access the models at <a href="http://ai.nvidia.com/" target="_blank" rel="noopener">ai.nvidia.com</a>, where they’ll be packaged as an <a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/" target="_blank" rel="noopener">NVIDIA NIM</a> microservice with a standard application programming interface that can be deployed anywhere.</p>
<h2 dir="ltr">Navigating Nemotron to Generate Synthetic Data</h2>
<p dir="ltr">LLMs can help developers generate synthetic training data in scenarios where access to large, diverse labeled datasets is limited.</p>
<p dir="ltr">The <a href="https://huggingface.co/nvidia/Nemotron-4-340B-Instruct" target="_blank" rel="noopener">Nemotron-4 340B Instruct</a> model creates diverse synthetic data that mimics the characteristics of real-world data, helping improve data quality to increase the performance and robustness of custom LLMs across various domains.</p>
<p dir="ltr">Then, to boost the quality of the AI-generated data, developers can use the <a href="https://huggingface.co/nvidia/Nemotron-4-340B-Reward" target="_blank" rel="noopener">Nemotron-4 340B Reward</a> model to filter for high-quality responses. Nemotron-4 340B Reward grades responses on five attributes: helpfulness, correctness, coherence, complexity and verbosity. It’s currently first place on the <a target="_blank" href="https://huggingface.co/spaces/allenai/reward-bench">Hugging Face RewardBench leaderboard</a>, created by <a href="https://allenai.org/" target="_blank" rel="noopener">AI2</a>, for evaluating the capabilities, safety and pitfalls of reward models.</p>
<figure id="attachment_72168" aria-describedby="caption-attachment-72168"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-scaled.jpg"><img fetchpriority="high" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-672x378.jpg" alt="nemotron synthetic data generation pipeline diagram" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px"></a><figcaption id="caption-attachment-72168">In this synthetic data generation pipeline, (1) the Nemotron-4 340B Instruct model is first used to produce synthetic text-based output. An evaluator model, (2) Nemotron-4 340B Reward, then assesses this generated text — providing feedback that guides iterative improvements and ensures the synthetic data is accurate, relevant and aligned with specific requirements.</figcaption></figure>
<p dir="ltr">Researchers can also create their own instruct or reward models by customizing the <a href="https://huggingface.co/nvidia/Nemotron-4-340B-Base" target="_blank" rel="noopener">Nemotron-4 340B Base</a> model using their proprietary data, combined with the included <a href="https://huggingface.co/datasets/nvidia/HelpSteer2" target="_blank" rel="noopener">HelpSteer2 dataset</a>.</p>
<h2 dir="ltr">Fine-Tuning With NeMo, Optimizing for Inference With TensorRT-LLM</h2>
<p dir="ltr">Using open-source NVIDIA NeMo and NVIDIA TensorRT-LLM, developers can optimize the efficiency of their instruct and reward models to generate synthetic data and to score responses.</p>
<p dir="ltr">All Nemotron-4 340B models are optimized with TensorRT-LLM to take advantage of tensor parallelism, a type of model parallelism in which individual weight matrices are split across multiple GPUs and servers, enabling efficient inference at scale.</p>
<p dir="ltr">Nemotron-4 340B Base, trained on 9 trillion tokens, can be customized using the NeMo framework to adapt to specific use cases or domains. This fine-tuning process benefits from extensive pretraining data and yields more accurate outputs for specific downstream tasks.</p>
<p dir="ltr">A variety of customization methods are available through the NeMo framework, including supervised fine-tuning and parameter-efficient fine-tuning methods such as low-rank adaptation, or LoRA.</p>
<p dir="ltr">To boost model quality, developers can align their models with <a href="https://github.com/NVIDIA/NeMo-Aligner" target="_blank" rel="noopener">NeMo Aligner</a> and datasets annotated by Nemotron-4 340B Reward. Alignment is a key step in training LLMs, where a model’s behavior is fine-tuned using algorithms like reinforcement learning from human feedback (RLHF) to ensure its outputs are safe, accurate, contextually appropriate and consistent with its intended goals.</p>
<p dir="ltr">Businesses seeking enterprise-grade support and security for production environments can also access NeMo and TensorRT-LLM through the cloud-native <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" target="_blank" rel="noopener">NVIDIA AI Enterprise</a> software platform, which provides accelerated and efficient runtimes for generative AI foundation models.</p>
<h2 dir="ltr">Evaluating Model Security and Getting Started</h2>
<p dir="ltr">The Nemotron-4 340B Instruct model underwent extensive safety evaluation, including adversarial tests, and performed well across a wide range of risk indicators. Users should still perform careful evaluation of the model’s outputs to ensure the synthetically generated data is suitable, safe and accurate for their use case.</p>
<p dir="ltr">For more information on model security and safety evaluation, read the model card.</p>
<p dir="ltr">Download Nemotron-4 340B models via&nbsp;<a href="https://huggingface.co/collections/nvidia/nemotron-4-340b-666b7ebaf1b3867caf2f1911" target="_blank" rel="noopener">Hugging Face</a>. For more details, read the <a target="_blank" href="https://research.nvidia.com/publication/2024-06_nemotron-4-340b">research papers on the model</a> and <a href="https://arxiv.org/abs/2406.08673" target="_blank" rel="noopener">dataset</a>.</p>
<p><i>See </i><a href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/" target="_blank" rel="noopener"><i>notice</i></a><i> regarding software product information.</i></p>

		<!-- .entry-footer -->

	</div><!-- .entry-content -->
	
<!-- #secondary -->
	

</article><!-- #post-## -->
			<!-- .navigation -->
					</div>
			</main><!-- #main -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple set to be first Big Tech group to face charges under EU digital law (112 pts)]]></title>
            <link>https://www.ft.com/content/31a996d5-b472-4357-953e-ace078494604</link>
            <guid>40681920</guid>
            <pubDate>Fri, 14 Jun 2024 15:49:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/31a996d5-b472-4357-953e-ace078494604">https://www.ft.com/content/31a996d5-b472-4357-953e-ace078494604</a>, See on <a href="https://news.ycombinator.com/item?id=40681920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="barrier-page">
<div data-component="articleHeaderHeroOffer" data-component-unique-name="CHE-Print"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2>Try unlimited access<br><strong>Only CHF1 for 4 weeks</strong></h2></p><p>Then CHF85 per month.<br>Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.</p></div></div>
<div id="recommendedOffers-CHE-Print" data-component="recommendedOffers" data-component-unique-name="CHE-Print"><p><h2 data-tiles="" data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-offer-type="premium" data-o-grid-colspan="12" data-tile=""><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw///financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p data-offer-type="premium"><h3>Standard Digital</h3><h3>FT Digital Edition</h3><h3>Premium Digital</h3><h3>Print + Premium Digital</h3><h3>Print</h3><h3>Trial</h3><h3>FT Edit</h3><h3>FT Professional</h3><h3>Weekend Print + Standard Digital</h3><h3>Weekend Print + Premium Digital</h3></p></div><p>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</p></div><div data-offer-type="bundle" data-o-grid-colspan="12" data-tile=""><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw///financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_bundle.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p data-offer-type="bundle"><h3>Standard Digital</h3><h3>FT Digital Edition</h3><h3>Premium Digital</h3><h3>Print + Premium Digital</h3><h3>Print</h3><h3>Trial</h3><h3>FT Edit</h3><h3>FT Professional</h3><h3>Weekend Print + Standard Digital</h3><h3>Weekend Print + Premium Digital</h3></p></div><p>Billed Quarterly at CHF379. Complete digital access plus the FT newspaper delivered Monday-Saturday.</p></div><div data-offer-type="" data-o-grid-colspan="12" data-tile=""><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw///financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p data-offer-type=""><h3>Standard Digital</h3><h3>FT Digital Edition</h3><h3>Premium Digital</h3><h3>Print + Premium Digital</h3><h3>Print</h3><h3>Trial</h3><h3>FT Edit</h3><h3>FT Professional</h3><h3>Weekend Print + Standard Digital</h3><h3>Weekend Print + Premium Digital</h3></p></div></div></div>
<div data-component="subscriptionOptionsV2" data-component-unique-name="UK-Print"><h2>Explore our full range of subscriptions.</h2></div>
<div data-component="whyFT" data-component-unique-name="default"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft">Find out why</a></p></div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cost of self hosting Llama-3 8B-Instruct (227 pts)]]></title>
            <link>https://blog.lytix.co/posts/self-hosting-llama-3</link>
            <guid>40681784</guid>
            <pubDate>Fri, 14 Jun 2024 15:30:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.lytix.co/posts/self-hosting-llama-3">https://blog.lytix.co/posts/self-hosting-llama-3</a>, See on <a href="https://news.ycombinator.com/item?id=40681784">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><article dir="ltr"><h2>Cost Of Self Hosting Llama-3 8B-Instruct</h2><div><p>Sid Premkumar</p><!-- --><p>,<time datetime="2024-06-13T00:00:00.000Z">Thu Jun 13 2024</time><span>•</span><a href="https://blog.lytix.co/tags/llama-3">llama-3</a></p></div><img src="https://blog.lytix.co/posts/self-hosting-llama-3/cover.webp" alt="Cover Image">
<h2>How much does it cost to self host a LLM?<a href="#how-much-does-it-cost-to-self-host-a-llm" id="how-much-does-it-cost-to-self-host-a-llm" aria-label="Permalink for this section"></a></h2>
<h4>⚡️ TLDR: Assuming 100% utilization of your model <code dir="ltr">Llama-3 8B-Instruct</code> model costs about $17 dollars per 1M tokens when self hosting with EKS, vs ChatGPT with the same workload can offer $1 per 1M tokens. Choosing to self host the hardware can make the cost &lt;$0.01 per 1M token that takes ~5.5 years to break even.<a href="#️-tldr-assuming-100-utilization-of-your-model-llama-3-8b-instruct-model-costs-about-17-dollars-per-1m-tokens-when-self-hosting-with-eks-vs-chatgpt-with-the-same-workload-can-offer-1-per-1m-tokens-choosing-to-self-host-the-hardware-can-make-the-cost-001-per-1m-token-that-takes-55-years-to-break-even" id="️-tldr-assuming-100-utilization-of-your-model-llama-3-8b-instruct-model-costs-about-17-dollars-per-1m-tokens-when-self-hosting-with-eks-vs-chatgpt-with-the-same-workload-can-offer-1-per-1m-tokens-choosing-to-self-host-the-hardware-can-make-the-cost-001-per-1m-token-that-takes-55-years-to-break-even" aria-label="Permalink for this section"></a></h4>
<p>A question we often get is how do you scale with ChatGPT. One thing we wanted to try is to determine the cost of self hosting an open model such as Llama-3.</p>
<br>
<h5>Determining the best hardware<a href="#determining-the-best-hardware" id="determining-the-best-hardware" aria-label="Permalink for this section"></a></h5>
<p><em>Context</em>: All tests were run on a EKS cluster. Each test was allocated the resources of the entire node, nothing else was running on that node other than system pods (prometheus, nvidia-daemon, etc.)</p>
<p>We first wanted to start small, can we run it on a single Nvidia Tesla T4 GPU? I started with AWS’s <code dir="ltr">g4dn.2xlarge</code> instance. Its specs were as follows (<a href="https://aws.amazon.com/ec2/instance-types/g4/" target="_blank" rel="noreferrer">source<span> (opens in a new tab)</span></a>):</p>
<ul>
<li>1 NVidia Tesla T4</li>
<li>32GB Memory.</li>
<li>8 vCPUs</li>
</ul>
<p>The short answer is that running either the 8B param or the 70B param version of Llama 3 did not work on this hardware. We initially tried the 70B param version of Llama 3 and constantly ran into OOM issues.
We then sobered up and tried the 8B param version. Although this time we was able to get a response, generating the response took what felt like ~10 minutes. I checked <code dir="ltr">nvidia-smi</code> and the single GPU was being used, it just wasn’t enough.</p>
<p>For context 8B vs 70B refers to the parameters in the model and generally translate to performance of the model. The advantage of the 8B param is its small size and resource footprint.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/cuda-oom.png" alt="CUDA OOM">
<p>So I decided to switch to the <code dir="ltr">g4dn.16xlarge</code> instance type. The specs of that were as follows (<a href="https://aws.amazon.com/ec2/instance-types/g4/" target="_blank" rel="noreferrer">source<span> (opens in a new tab)</span></a>):</p>
<ul>
<li>4 NVidia Tesla T4s</li>
<li>192gb memory</li>
<li>48 vCPUs</li>
</ul>
<h5>Initial Implementation<a href="#initial-implementation" id="initial-implementation" aria-label="Permalink for this section"></a></h5>
<p>My initial implementation involved trying to copy and paste the code present on the llama-3 hugging face (<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank" rel="noreferrer">see<span> (opens in a new tab)</span></a>)</p>

<p>These results seemed a lot more promising as the response time lowered from ~10m to sub 10 seconds consistently.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/inital-implementation-results.png" alt="Initial Implementation Results">
<p>I was able to get a response in a much more reasonable 5-7 seconds. At this point I wanted to start calculating the cost of this request.</p>
<p><code dir="ltr">g5dn.12xlarge</code> costs $3.912 per hour on demand (<a href="https://aws.amazon.com/ec2/instance-types/g5/" target="_blank" rel="noreferrer">see<span> (opens in a new tab)</span></a>).</p>
<p>If we assume a full month of use, that costs:</p>

<p>I had trouble with getting the token count in hugging face so ended up using <a href="https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/" target="_blank" rel="noreferrer">llama-tokenizer-js<span> (opens in a new tab)</span></a> to get an approximation of tokens used.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/llama-3-tokenizer-js.png" alt="Llama-3 Tokenizer JS">
<p><em><em>Note: This ended up being incorrect way of calculating the tokens used.</em></em></p>
<p>If we look at this result, we assume we used 39 tokens over ~6 seconds. Assuming Im processing tokens 24/7, extrapolating that we get:</p>

<p>With 6.5 tokens per second, over the month I can process:</p>

<p>With 16,848,000 tokens per month, every million tokens costs:</p>

<p>😵Yikes. It is not looking good. Lets compare this to what ChatGPT 3.5 can get us.</p>
<p>Looking at their <a href="https://openai.com/api/pricing/" target="_blank" rel="noreferrer">pricing<span> (opens in a new tab)</span></a>, ChatGPT 3.5 Turbo charges $0.5 per 1M input token, and $1.5 per 1M output token. For sake of simplicity, assuming an average input:output ratio, that means per 1M tokens they charge $1 and thats the number to beat. Far from my $167.17.</p>
<h5>Realization Something Was Wrong<a href="#realization-something-was-wrong" id="realization-something-was-wrong" aria-label="Permalink for this section"></a></h5>
<p>At this point I felt I was doing something wrong. Llama 3 with 8B params is hard to run, but I didn’t feel it should be this hard, especially considering I have 4 GPUs available 🤔.</p>
<p>I decided to try to use vLLM to host an API server instead of attempting to do it myself via hugging face libraries.</p>
<p>This was dead simple and just involved me installing <code dir="ltr">ray</code> and <code dir="ltr">vllm</code> via <code dir="ltr">pip3</code> and then changing my docker entry point to:</p>

<p>Specifically noting that I call <code dir="ltr">—tensor-parallel-size 4</code> to enforce that we use all 4 GPUs.</p>
<p>Using this got <em>significantly</em> better results.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/significantly-better-results.png" alt="Llama-3 Tokenizer JS">
<p>In this example you can see the query took 2044ms to complete. This was also the time I realized my previous method of calculating token usage was incorrect. vLLM returned the tokens used which was perfect.</p>
<p>Going back to cost, now we can calculate tokens per second:</p>

<p>Again assuming I produce tokens 24/7, this means that I can ingest a total of:</p>

<p>Which means the cost per 1 million tokens would cost me:</p>

<p>Unfortunately this is still not below the value that ChatGPT offers, you’d lose about $17 a day 😭</p>
<h5>An Unconventional Approach<a href="#an-unconventional-approach" id="an-unconventional-approach" aria-label="Permalink for this section"></a></h5>
<p>Instead of using AWS another approach involves self hosting the hardware as well. Even after factoring in energy, this does dramatically lower the price.</p>
<p>Assuming we want to mirror our setup in AWS, we’d need 4xNVidia Tesla T4s. You can buy them for about $700 dollars on eBay</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/ebay-listing.png" alt="Ebay Listing">
<p>Add in $1,000 to setup the rest of the rig and you have a final price of around:</p>

<p>If we calculate energy for this, we get about ~$50 bucks. I determined power consumption by 70W per GPU + 20W overhead:</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/energy-cost.png" alt="Energy Cost">
<p>After factoring in the ~$3,800 fixed cost, you have a monthly cost of ~$50 bucks, lets round up to ~$100 to factor any misc items we might have missed.</p>
<p>Re-calculating our cost per 1M tokens now:</p>

<p>Which is <strong>significantly</strong> cheaper than our ChatGPT costs.</p>
<p>Trying to determine when you’d break even, assuming you want to produce 157,075,200 tokens with ChatGPT, you’re looking at a bill of:</p>

<p>You have a fixed cost of ~$100 a month, which results in a ‘profit’ of $57. To make up your initial server cost of $3,800, you’d need about 66 months or 5.5 years to benefit from this approach.</p>
<p>Although this approach does come with negatives such as having to manage and scale your own hardware, it does seem to be possible to undercut the prices that ChatGPT offer by a significant amount in theory. In pracitice however, you'd have to evaluate how often you are utilizing your LLM, these hypotheticals all assume 100% utilization which is not realistic and would have to be tailord per use case.</p><small><time>2024</time> © lytix.ai.</small></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan Passes Law to Allow Third-Party App Stores on the iPhone (130 pts)]]></title>
            <link>https://www.macrumors.com/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/</link>
            <guid>40681375</guid>
            <pubDate>Fri, 14 Jun 2024 14:41:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/">https://www.macrumors.com/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/</a>, See on <a href="https://news.ycombinator.com/item?id=40681375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/"><p>New <a href="https://www.jftc.go.jp/file/240612EN3.pdf">legislation in Japan</a> requires Apple to allow third-party app stores and payment providers on the <a href="https://www.macrumors.com/guide/iphone/">iPhone</a>.</p>
<p><img src="https://images.macrumors.com/t/9vxUsPNctk90_yUedrMMhPKdGA8=/400x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg?lossy" srcset="https://images.macrumors.com/t/9vxUsPNctk90_yUedrMMhPKdGA8=/400x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg?lossy 400w,https://images.macrumors.com/t/gBGXBeSG7RiXuc1kweuddC_OT9M=/800x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg?lossy 800w,https://images.macrumors.com/t/5lUXiumA7jjt-2sOhx-q2-SS25M=/1600x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg 1600w,https://images.macrumors.com/t/lfOnH7SoaWstF9bMLPrzjm-JL-Y=/2500x0/filters:no_upscale()/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple japan new year promotion 2022" width="1614" height="944"><br>The Japanese parliament has passed the Act on Promotion of Competition for Specified Smartphone Software, a law that compels Apple to allow access to third-party app stores and payment providers on devices that run iOS. The legislation, which was passed by Japan's upper house and will be enforced following Cabinet approval within the next eighteen months, seeks to curb the dominance of major tech firms like Apple in the smartphone market.</p>
<p>The law requires Apple to make several significant changes to its business practices. The company will have to permit third-party app stores on its devices, just like it does in the EU. App developers will be allowed to use third-party payment services. There are also provisions to allow users to change default settings via new choice screens during setup, such as for selecting a default browser.</p>
<p>Apple will be forbidden from giving its own services preferential treatment in search results without a justifiable reason. The law also prohibits the use of data acquired about competing software to benefit its own apps. Additionally, the law requires that third-party developers have access to the same features as Apple's own apps and services, such as NFC for contactless payments.</p>
<p>Failure to comply with these new regulations could lead to fines amounting to 20 percent of relevant turnover, with the figure increasing to 30 percent for repeat offenses. In a statement to <a href="https://www.theverge.com/2024/6/13/24177599/apple-google-japan-law-third-party-app-stores-competition"><em>The Verge</em></a>, Apple said:</p>
<blockquote><p>The Japanese government made a number of changes to the legislation that will help protect user privacy, data security, innovation, and our intellectual property. We will continue our engagement with the JFTC during the implementation period as we remain concerned about how the law will impact Japanese consumers and the secure and private iPhone experience our users have come to expect.</p></blockquote>
<p>The law is expected to be fully implemented by the end of 2025. <a href="https://www.macrumors.com/guide/epic-games/">Epic Games</a> has already announced plans to bring Fortnite and its game store platform to iOS in Japan by late 2025.</p>
<p>Japan's move follows a trend of international legislative efforts aimed at regulating the dominance of major tech companies. The European Union's Digital Markets Act (DMA) and the UK's Digital Markets, Competition and Consumers Bill are similar initiatives designed to foster competition and prevent monopolistic practices. Various antitrust cases in the United States are also targeting similar issues.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/06/11/ios-18-pop-out-button-animation/">iOS 18 Adds Pop-Out Bezel Animation When Pressing iPhone Buttons</a></h3><p>Tuesday June 11, 2024 10:40 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>iOS 18 includes a small but interesting change for the buttons on the iPhone, adding more of a visual element when changing volume, activating the Action button, or locking the screen. When you press an iPhone button in iOS 18, the display bezel bulges outward slightly. This feature is available for the volume buttons, Action button and the power button, and it will also likely be used for...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/ios-18-compatible-with-these-iphone-models/">Revealed: iOS 18 Works With These iPhone Models</a></h3><p>iOS 18 will be compatible with the same iPhone models as iOS 17, according to a post on X today from a private account with a proven track record of sharing build numbers for upcoming iOS updates. iOS 18 will be compatible with the iPhone XR, and hence also the iPhone XS and iPhone XS Max models with the same A12 Bionic chip, but older iPhone models will miss out. Here is the full...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/wwdc-2024-next-generation-carplay-images/">Apple Provides Updated Look at Next-Generation CarPlay at WWDC 2024</a></h3><p>Apple today shared a few WWDC 2024 coding sessions related to its upcoming next-generation CarPlay system ahead of its launch later this year. The sessions include lots of updated next-generation CarPlay images, with one revealing new Vehicle, Media, and Climate apps in action for the first time. MacRumors previously discovered evidence of these apps in the iOS 17.4 beta. Next-generation...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/apple-announces-ios-18-with-new-customization-features-and-more/">Apple Announces iOS 18 With New Customization Features, Redesigned Photos App, and More</a></h3><p>Apple today previewed iOS 18, the next major update to the operating system for the iPhone, with new customization features, a redesigned Photos app, and more. iOS 18 features new customization tools for the Home Screen. App icons now feature Dark Mode and users can tint them with a color to create a unique look. Apps can also now be placed anywhere on the Home Screen freely. The Control...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/macos-sequoia-and-ipados-18-compatibility/">macOS Sequoia and iPadOS 18 Drop Support for These Macs and iPads</a></h3><p>macOS Sequoia is still compatible with several Intel-based Macs, but it does drop support for 2018 and 2019 models of the MacBook Air. macOS Sequoia is compatible with the following Macs, according to Apple: MacBook Pro: 2018 and later MacBook Air: 2020 and later Mac mini: 2018 and later iMac: 2019 and later iMac Pro: 2017 Mac Studio: 2022 and later Mac Pro: 2019 and later The...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/wwdc-2024-recap/">Everything Apple Announced at WWDC 2024 in Nine Minutes</a></h3><p>Monday June 10, 2024 7:59 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple crammed an overwhelming number of new features into its WWDC 2024 keynote event, introducing Apple Intelligence, iOS 18, iPadOS 18, macOS Sequoia, visionOS 2, watchOS 11, and tvOS 18. It was hard to keep up with everything that Apple highlighted, so we did a video of all of the new additions you won't want to miss. Subscribe to the MacRumors YouTube channel for more videos. We've also...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/09/ios-18-these-iphones-wont-support-ai/">Massive iPhone Upgrade Coming This Week But These Devices Will Miss Out</a></h3><p>Apple is planning a major AI overhaul in iOS 18, with a feature set it is referring to as "Apple Intelligence." However, these new features will not work on older iPhones, even if they do appear on the new operating system's device compatibility list. Apple's initial AI roadmap for iOS 18 is said to come in two parts: Basic AI features that will be processed on-device, and more advanced...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sleep deprivation disrupts memory (223 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-01732-y</link>
            <guid>40681345</guid>
            <pubDate>Fri, 14 Jun 2024 14:39:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-01732-y">https://www.nature.com/articles/d41586-024-01732-y</a>, See on <a href="https://news.ycombinator.com/item?id=40681345">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_27197870.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_27197870.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A fluorescence light micrograph of brain hippocampus neurons, shown in green and blue" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_27197870.jpg">
  <figcaption>
   <p><span>Neurons (artificially coloured) in the hippocampus play a part in learning and memory.</span><span>Credit: Cell Applications Inc/Science Photo Library</span></p>
  </figcaption>
 </picture>
</figure><p>A crucial brain signal linked to <a href="https://www.nature.com/articles/d41586-022-02298-3" data-track="click" data-label="https://www.nature.com/articles/d41586-022-02298-3" data-track-category="body text link">long-term memory</a> falters in rats when they are deprived of sleep — which might help to explain why <a href="https://www.nature.com/articles/d41586-023-01849-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-01849-6" data-track-category="body text link">poor sleep disrupts memory formation</a><sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Even a night of normal slumber after a poor night’s sleep <a href="https://www.nature.com/articles/d41586-019-00723-8" data-track="click" data-label="https://www.nature.com/articles/d41586-019-00723-8" data-track-category="body text link">isn’t enough to fix the brain signal</a>.</p><p>These results, published today in <i>Nature</i>, suggest that there is a “critical window for memory processing”, says Loren Frank, a neuroscientist at the University of California, San Francisco, who was not involved with the study. “Once you’ve lost it, you’ve lost it.”</p><p>In time, these findings could lead to targeted <a href="https://www.nature.com/articles/d41586-023-02833-w" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02833-w" data-track-category="body text link">treatments to improve memory</a>, says study co-author Kamran Diba, a computational neuroscientist at the University of Michigan Medical School in Ann Arbor.</p><h2>Firing in lockstep</h2><p>Neurons in the brain seldom act alone; they are highly interconnected and often fire together in a rhythmic or repetitive pattern. One such pattern is the <a href="https://www.nature.com/articles/d41586-021-02122-4" data-track="click" data-label="https://www.nature.com/articles/d41586-021-02122-4" data-track-category="body text link">sharp-wave</a><a href="https://www.nature.com/articles/d41586-021-02122-4" data-track="click" data-label="https://www.nature.com/articles/d41586-021-02122-4" data-track-category="body text link"> ripple</a>, in which a large group of neurons fire with extreme synchrony, then a second large group of neurons does the same and so on, one after the other at a particular tempo. These ripples occur in a brain area called the <a href="https://www.nature.com/articles/d41586-019-02211-5" data-track="click" data-label="https://www.nature.com/articles/d41586-019-02211-5" data-track-category="body text link">hippocampus, which is key to memory formation</a>. The patterns are thought to facilitate communication with the neocortex, where long-term memories are later stored.</p><p>One clue to their function is that some of these ripples are accelerated re-runs of brain-activity patterns that occurred during past events. For example, when an animal visits a particular spot in its cage, <a href="https://www.nature.com/articles/d41586-020-03164-w" data-track="click" data-label="https://www.nature.com/articles/d41586-020-03164-w" data-track-category="body text link">a specific group of neurons in the hippocampus fires in unison</a>, creating a neural representation of that location. Later, these same neurons might participate in sharp-wave ripples — as if they were rapidly replaying snippets of that experience.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-01626-x" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_23192898.jpg"><p>Unpicking the link between smell and memories</p></a>
 </article><p>Previous research<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> found that, when these ripples were disturbed, mice struggled on a memory test. And when the ripples were prolonged, their performance on the same test improved<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>, leading György Buzsáki, a systems neuroscientist at NYU Langone Health in New York City, who has been researching these bursts since the 1980s, to call the ripples a ‘cognitive biomarker’ for memory and learning.</p><p>Researchers also noticed<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> that sharp-wave ripples tend to occur during deep sleep as well as during waking hours, and that those bursts during slumber seem to be particularly important for transforming short-term knowledge into long-term memories<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>. These links between the ripples, sleep and memory are well-documented, but there have been few studies that have directly manipulated sleep to determine how it affects these ripples, and in turn memory, Diba says.</p><h2>Wake-up call</h2><p>To understand how poor sleep affects memory, Diba and his colleagues recorded hippocampal activity in seven rats as they explored mazes over the course of several weeks. The researchers regularly disrupted the sleep of some of the animals and let others sleep at will.</p><p>To Diba’s surprise, rats that were woken up repeatedly had similar, or even higher, levels of sharp-wave-ripple activity than the rodents that got normal sleep did. But the firing of the ripples was weaker and less organized, showing a marked decrease in repetition of previous firing patterns. After the sleep-deprived animals recovered over the course of two days, re-creation of previous neural patterns rebounded, but never reached levels found in those which had normal sleep.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-00930-y" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_26984650.jpg"><p>Memories are made by breaking DNA — and fixing it</p></a>
 </article><p>This study makes clear that “memories continue to be processed after they’re experienced, and that post-experience processing is really important”, Frank says. He adds that it could explain why cramming before an exam or pulling an all-nighter might be an ineffective strategy.</p><p>It also teaches researchers an important lesson: the content of sharp-wave ripples is more important than its quantity, given that rats that got normal sleep and rats that were sleep-deprived had a similar number of ripples, he says.</p><h2>Ripple effects</h2><p>Buzsáki says that these findings square with data his group published in March<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup> that found that sharp-wave ripples that occur while an animal is awake might help to select which experiences enter long-term memory.</p><p>It’s possible, he says, that the disorganized sharp-wave ripples of sleep-deprived rats don’t allow them to effectively flag experiences for long-term memory. As a result, the animals might be unable to replay the neural firing of those experiences at a later time.</p><p>This means that sleep disruption could be used to prevent memories from entering long-term storage, which could be useful for people who have recently experienced something traumatic, such as those with post-traumatic stress disorder, Buzsáki says.</p>
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Giri, B. <i>et al.</i> <i>Nature</i> https://doi.org/10.1038/s41586-024-07538-2 (2024).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-024-07538-2" data-track-item_id="10.1038/s41586-024-07538-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-024-07538-2" aria-label="Article reference 1" data-doi="10.1038/s41586-024-07538-2">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-024-07538-2&amp;publication_year=2024&amp;author=Giri%2CB.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Jadhav, S. P., Kemere, C., German, P. W. &amp; Frank, L. <i>M. Science</i> <b>336</b>, 1454–1458 (2012).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1126/science.1217230." data-track-item_id="10.1126/science.1217230." data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1217230." aria-label="Article reference 2" data-doi="10.1126/science.1217230.">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=M.%20Science&amp;doi=10.1126%2Fscience.1217230.&amp;volume=336&amp;pages=1454-1458&amp;publication_year=2012&amp;author=Jadhav%2CS.%20P.&amp;author=Kemere%2CC.&amp;author=German%2CP.%20W.&amp;author=Frank%2CL.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Fernández-Ruiz, A. <i>et al.</i> <i>Science</i> <b>364</b>, 1082–1086 (2019).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1126/science.aax0758" data-track-item_id="10.1126/science.aax0758" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aax0758" aria-label="Article reference 3" data-doi="10.1126/science.aax0758">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.aax0758&amp;volume=364&amp;pages=1082-1086&amp;publication_year=2019&amp;author=Fern%C3%A1ndez-Ruiz%2CA.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Eschenko, O., Ramadan, W., Mölle, M., Born, J. &amp; Sara, S. J. <i>Learn. Mem.</i> <b>15</b>, 222–228 (2008).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1101/lm.726008" data-track-item_id="10.1101/lm.726008" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1101%2Flm.726008" aria-label="Article reference 4" data-doi="10.1101/lm.726008">Article</a>&nbsp;
    <a data-track="click||click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18385477" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Learn.%20Mem.&amp;doi=10.1101%2Flm.726008&amp;volume=15&amp;pages=222-228&amp;publication_year=2008&amp;author=Eschenko%2CO.&amp;author=Ramadan%2CW.&amp;author=M%C3%B6lle%2CM.&amp;author=Born%2CJ.&amp;author=Sara%2CS.%20J.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">Ramadan, W., Eschenko, O. &amp; Sara, S. J.<i> PLoS ONE</i> <b>4</b>, e6697 (2009).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pone.0006697" data-track-item_id="10.1371/journal.pone.0006697" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pone.0006697" aria-label="Article reference 5" data-doi="10.1371/journal.pone.0006697">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=PLOS%20ONE&amp;doi=10.1371%2Fjournal.pone.0006697&amp;volume=4&amp;publication_year=2009&amp;author=Ramadan%2CW.&amp;author=Eschenko%2CO.&amp;author=Sara%2CS.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="6."><p id="ref-CR6">Yang, W. <i>et al.</i> <i>Science</i> <b>383</b>, 1478–1483 (2024).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1126/science.adk8261" data-track-item_id="10.1126/science.adk8261" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.adk8261" aria-label="Article reference 6" data-doi="10.1126/science.adk8261">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.adk8261&amp;volume=383&amp;pages=1478-1483&amp;publication_year=2024&amp;author=Yang%2CW.">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-024-01732-y?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[H.264 Is Magic (2016) (277 pts)]]></title>
            <link>https://sidbala.com/h-264-is-magic/</link>
            <guid>40681306</guid>
            <pubDate>Fri, 14 Jun 2024 14:35:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sidbala.com/h-264-is-magic/">https://sidbala.com/h-264-is-magic/</a>, See on <a href="https://news.ycombinator.com/item?id=40681306">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <!--kg-card-begin: markdown--><p>H.264 is a video compression codec standard. It is ubiquitous - internet video, Blu-ray, phones, security cameras, drones, everything. Everything uses H.264 now.</p>
<p>H.264 is a remarkable piece of technology. It is the result of 30+ years of work with one single goal: To reduce the bandwidth required for transmission of full-motion video.</p>
<p>Technically, it is very interesting. This post will give insight into some of the details at a high level - I hope to not bore you too much with the intricacies. Also note that many of the concepts explained here apply to video compression in general, and not just H.264.</p>
<blockquote>
<p>Why even compress anything?</p>
</blockquote>
<p>A simple uncompressed video file will contain an array of 2D buffers containing pixel data for each frame. So it's a 3D (2 spatial dimensions and 1 temporal) array of bytes. Each pixel takes 3 bytes to store - one byte each for the three primary colors (red, green and blue).</p>
<p>1080p @ 60 Hz = 1920x1080x60x3 =&gt; ~<strong>370 MB/sec</strong> of raw data.</p>
<p>This is next to impossible to deal with. A 50GB Blu-ray disk will only hold ~2 mins. You can't move it anywhere fast. Even SSDs have trouble dumping this straight from RAM to Disk[^1].</p>
<p>So yeah. We need compression.</p>
<blockquote>
<p>Why <em>H.264</em> compression?</p>
</blockquote>
<p>Yes, I will answer this. But first let me show you something. Here is the Apple Homepage:</p>
<p><img src="https://sidbala.com/content/images/2016/11/HomePage.png" alt="" loading="lazy"></p>
<p>I captured the screen of this home page and produced two files:</p>
<ul>
<li><a href="https://sidbala.com/content/images/2016/11/outputFrame.png">PNG screenshot of the Apple homepage</a> <strong>1015KB</strong></li>
<li><a href="https://s3-us-west-2.amazonaws.com/sidbala-blog/VideoH264.mp4?ref=sidbala.com">5 Second 60fps H.264 video of the same Apple homepage</a> <strong>175KB</strong></li>
</ul>
<blockquote>
<p>Eh. What? Those file sizes look switched.</p>
</blockquote>
<p>No, they're right. The H.264 video, 300 frames long is 175KB. A single frame of that video in PNG is 1015KB.</p>
<p>It looks like we're storing 300 times the amount of data in the video. But the file size is a fifth. So H.264 would seem to be 1500x as efficient as PNG.</p>
<blockquote>
<p>How is this even possible? All right, what's the trick?</p>
</blockquote>
<p>There are very many tricks! H.264 uses all the tricks you can think of (and tons you can't think of). Let's go through the important ones.</p>
<h6 id="sheddingweight">Shedding weight</h6>
<p>Imagine you're building a car for street racing. You need to go faster. What is the first thing you do? You shed some weight. Your car weighs 3000 lbs. You throw away stuff you don't need. Those back seats? pfft. Chuck those. That subwoofer? Gone. No music for you. Air Conditioning? Yeah, ditch it. Transmission? Ye..no. Wait! We're gonna need that.</p>
<p>You remove everything except the things that matter.</p>
<p>This concept of throwing away bits you don't need to save space is called <strong>lossy</strong> compression. H.264 is a lossy codec - it throws away less important bits and only keeps the important bits.</p>
<p>PNG is a <strong>lossless</strong> codec. It means that nothing is thrown away. Bit for bit, the original source image can be recovered from a PNG encoded image.</p>
<blockquote>
<p>Important bits? How does the algorithm know what bits in my frame are important?</p>
</blockquote>
<p>There are few obvious ways to trim out images. Maybe the top right quadrant is useless all the time. So maybe we can zero out those pixels and discard that quadrant. We would use only 3/4th of the space we need. ~2200 lbs now. Or maybe we can crop out a thick border around the edges of the frame, the important stuff is in the middle anyway. Yes, you could do these. But H.264 doesn't do this.</p>
<blockquote>
<p>What does H.264 actually do?</p>
</blockquote>
<p>H.264, like other lossy image algorithms, discards detail information. Here is a close-up of the original compared with the image post-discard.</p>
<p><img src="https://sidbala.com/content/images/2016/11/CompressedImage-1.jpg" alt="" loading="lazy"></p>
<p>See how the compressed one does not show the holes in the speaker grills in the MacBook Pro? If you don't zoom in, you would even notice the difference. The image on the right weighs in at <strong>7%</strong> the size of the original - and we haven't even compressed the image in the traditional sense. Imagine your car weighed just 200 lbs!</p>
<blockquote>
<p>7% wow! How do you discard detail information like that?</p>
</blockquote>
<p>For this we need a quick math lesson.</p>
<h6 id="informationentropy">Information Entropy</h6>
<p>Now we're getting to the juicy bits! Ha puns! If you took an information theory class, you might remember information entropy. Information entropy is the number of bits required to represent some information. Note that it is not simply the size of some dataset. It is minimum number of bits that must be used to represent all the information contained in a dataset.</p>
<p>For example, if your dataset is the result of a single coin toss, you need 1 bit of entropy. If you have record two coin tosses, you'll need 2 bits. Makes sense?</p>
<p>Suppose you have some strange coin - you've tossed it 10 times, and every time it lands on heads. How would you describe this information to someone? You wouldn't say HHHHHHHHH. You would just say "10 tosses, all heads" - bam! You've just compressed some data! Easy. I saved you hours of mindfuck lectures. This is obviously an oversimplification, but you've transformed some data into another shorter representation of the same information. You've reduced data <strong>redundancy</strong>. The information entropy in this dataset has not changed - you've just converted between representations. This type of encoder is called an <strong>entropy encoder</strong> - it's a general-purpose lossless encoder that works for any type of data.</p>
<h6 id="frequencydomain">Frequency Domain</h6>
<p>Now that you understand information entropy, let's move on to transformations of data. You can represent data in some fundamental units. If you use binary, you have 0 and 1. If you use hex, you have 16 characters. You can easily transform between the two systems. They are essentially equivalent. So far so good? Ok!</p>
<p>Now, some imagination! Imagine you can transform any dataset that varies over space(or time) - something like the brightness value of an image, into a different coordinate space. So instead of x-y coordinates, let's say we have frequency coordinates. freqX and freqY are the axes now. This is called a <strong>frequency domain</strong> representation. There is another mindfuck mathematical theorem[^2] that states that you can do this for any data and you can achieve a perfect lossless transformation as long as freqX and freqY are high enough.</p>
<blockquote>
<p>Okay, but what the freq are freqX and freqY?</p>
</blockquote>
<p>freqX and freqY are some other set of basis units. Just like when we switch from binary to hex, we have a different fundamental unit, we're switching from the familiar X-Y to freqX and freqY. Hex 'A' looks different from binary '1010'. Both mean the same thing, but <strong>look</strong> different. So here is what our image looks like in the frequency domain:</p>
<p><img src="https://sidbala.com/content/images/2016/11/BasicFFT-2.png" alt="" loading="lazy"></p>
<p>The fine grill on that MacBook pro has a high information content in the higher frequency components of that image. Finely varying content = high frequency components. Any sort of gradual variation in the color and brightness - such as gradients are low frequency components of that image. Anything in between falls in between. So fine details = high freq. Gentle gradients = low freq. Makes sense?</p>
<p>In the frequency domain representation, the low frequency components are near the center of that image. The higher frequency components are towards of the edges of the image.</p>
<blockquote>
<p>Okay. Kinda makes sense. But why do all this?</p>
</blockquote>
<p>Because now, you can take that frequency domain image and then mask out the edges - discard information which will contain the information with high frequency components. Now if you convert back to your regular x-y coordinates, you'll find that the resulting image looks similar to the original but has lost some of the fine details. But now, the image only occupies a fraction of the space. By controlling how big your mask is, you can now tune precisely how detailed you want your output images to be.</p>
<p>Here is the close-up of the laptop in the home page again. Except now, there is a circular border mask that's been applied.</p>
<p><img src="https://sidbala.com/content/images/2016/11/QuantizationHorizontalWithMasks-1.jpg" alt="" loading="lazy"></p>
<p>The numbers represent the information entropy of that image as a fraction of the original. Even at 2%, you won't notice the difference unless you're at this zoom level. 2%! - your car now weighs 60 lbs!</p>
<p>So that's how you shed weight. This process in lossy compression is called <strong>quantization</strong>[^3].</p>
<blockquote>
<p>Okay. Impressive, I guess. What else you got?</p>
</blockquote>
<h6 id="chromasubsampling">Chroma Subsampling.</h6>
<p>The human/eye brain system is not very good at resolving finer details in color. It can detect minor variations in brightness very easily but not color. So there must be some way to discard color information to shed even more weight.</p>
<p>In a TV signal, R+G+B color data gets transformed to Y+Cb+Cr. The Y is the luminance (essentially black and white brightness) and the Cb and Cr are the chrominance (color) components. RGB and YCbCr are equivalent in terms of information entropy.</p>
<blockquote>
<p>Why unnecessarily complicate? RGB not good enough for you?</p>
</blockquote>
<p>Back before we had color TV, we only had the Y signal. And when color TVs just started coming along, engineers had to figure out a way to transmit RGB color along with Y. Instead of using two separate data streams, they wisely decided to encode the color information into Cb and Cr and transmit that along with the Y information. That way, BW TVs would only look at the Y component. Color TVs will, in addition, look at the chrominance components and convert to RGB internally.</p>
<p>But check out the trick: the Y component gets encoded at full resolution. The C components only at a quarter resolution. Since the eye/brain is terrible at detecting color variations, you can get away with this. By doing this, you reduce total bandwidth by one half, with very little visual difference. Half! Your car now weighs 30 lbs!</p>
<p>This process of discarding some of the color information is called <strong>Chroma Subsampling</strong>[^4]. While not specific to H.264 and has been around for decades itself, it is used almost universally.</p>
<p>Those are the big weight shedders for lossy compression. Our frames are now tiny - since we discarded most of the detail information and half of the color information.</p>
<blockquote>
<p>Wait. That's it? Can we do something more?</p>
</blockquote>
<p>Yes. Weight shedding is only the first step. So far we're only looking at the spatial domains within a single frame. Now it's time to explore temporal compression - where we look at a group of frames across time.</p>
<h6 id="motioncompensation">Motion compensation</h6>
<p>H.264 is a motion compensation compression standard.</p>
<blockquote>
<p>Motion compensation? What now?</p>
</blockquote>
<p>Imagine you're watching a tennis match. The camera is fixed at a certain angle. The only thing moving is the ball back and forth. How would you encode this information? You do what you always do, right? You have a 3D array of pixels, two dimensions in space and one in time. Right?</p>
<p>Nah. Why would you? Most of the image is the same anyway. The court, the net, the crowds, all are static. The only real action is the ball moving. What if you could just have one static image of everything in the background, and then one moving image of just the ball? Wouldn't that save a lot of space? You see where I am going with this? Get it? See where I am going? Motion estimation?</p>
<p>Lame jokes aside, this is exactly what H.264 does. H.264 splits up the image into macro-blocks - typically 16x16 pixel blocks that it will use for motion estimation. It encodes one static image - typically called an <strong>I-frame</strong>(Intra frame). This is a full frame - containing all the bits it required to construct that frame. And then subsequent frames are either <strong>P-frames</strong>(predicted) or <strong>B-frames</strong>(bi-directionally predicted). P-frames are frames that will encode a motion vector for each of the macro-blocks from the previous frame. So a P-frame has to be constructed by the decoder based on previous frames. It starts with the last I-frame in the video stream and then walks through every subsequent frame - adding up the motion vector deltas as it goes along until it arrives at the current frame.</p>
<p>B-frames are even more interesting, where the prediction happens bi-directionally, both from past frames and from future frames. So you can imagine now why that Apple home page video is so well compressed. Because it's really just three I-frames in which the macro blocks are being panned around.</p>
<p>Let's say you've been playing a video on YouTube. You missed the last few seconds of dialog, so you scrub back a few seconds. Have you noticed that it doesn't instantly start playing from that timecode you just selected. It pauses for a few moments and then plays. It's already buffered those frames from the network, since you just played it, so why that pause?</p>
<blockquote>
<p>Yeah that annoys the shit out of me. Why does it do that?</p>
</blockquote>
<p>Because you've asked the decoder to jump to some arbitrary frame, the decoder has to redo all the calculations - starting from the nearest I-frames and adding up the motion vector deltas to the frame you're on - and this is computationally expensive, and hence the brief pause. Hopefully you'll be less annoyed now, knowing it's actually doing hard work and not just sitting around just to annoy you.</p>
<p>Since you're only encoding motion vectors deltas, this technique is extremely space-efficient for any video with motion, at the cost of some computation.</p>
<p>Now we've covered both spatial and temporal compression! So far we have a shitton of space saved in Quantization. Chroma subsampling further halved the space required. On top of that, we have motion compensation that stores only 3 actual frames for the ~300 that we had in that video.</p>
<blockquote>
<p>Looks pretty good to me. Now what?</p>
</blockquote>
<p>Now we wrap up and seal the deal. We use a traditional lossless entropy encoder. Because why not? Let's just slap that on there for good measure.</p>
<h6 id="entropycoder">Entropy Coder</h6>
<p>The I-frames, after the lossy steps, contain redundant information. The motion vectors for each of the macro blocks in the P and B-frames - there are entire groups of them with the same values - since several macro blocks move by the same amount when the image pans in our test video.</p>
<p>An entropy encoder will take care of this redundancy. And since it is a general purpose lossless encoder, we don't have to worry about what tradeoffs it's making. We can recover all the data that goes in.</p>
<p>And, we're done! At the core of it, this is how video compression codecs like H.264 work. These are its tricks.</p>
<blockquote>
<p>Ok great! But I am curious to know how much our car weighs now.</p>
</blockquote>
<p>The original video was captured at an odd resolution of 1232x1154. If we apply the math here, we get:</p>
<p>5 secs @ 60 fps = 1232x1154x60x3x5 =&gt; <strong>1.2 GB</strong><br>
Compressed video =&gt; <strong>175 KB</strong></p>
<p>If we apply the same ratio to our 3000 lb car, we get <strong>0.4 lbs</strong> as the final weight. 6.5 ounces!</p>
<p><strong>Yeah. It's magic!</strong></p>
<p>Obviously, I am massively oversimplifying several decades of intense research in this field. If you want to know more, the <a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC?ref=sidbala.com">Wikipedia Page</a> is pretty descriptive.</p>
<p>Have comments? Did I get something wrong? Not a fan of the lame jokes? Offended by the swearing? Use <a href="https://news.ycombinator.com/item?id=12871403&amp;ref=sidbala.com"><strong>HackerNews</strong></a> or <a href="https://www.reddit.com/r/programming/comments/5b31gt/h264_is_magic/?ref=sidbala.com"><strong>Reddit</strong></a> for voicing your opinion!</p>
<p>Or hit me up on <a href="https://twitter.com/SidBaIa?ref=sidbala.com"><strong>Twitter</strong></a> or <a href="https://www.linkedin.com/in/sidbalasubramanian?ref=sidbala.com"><strong>LinkedIn</strong></a> if you want to chat.</p>
<p>[^1]<a href="http://www.anandtech.com/show/8747/samsung-ssd-850-evo-review/8?ref=sidbala.com">SSD Benchmarks</a></p>
<p>[^2]<a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem?ref=sidbala.com">Nyquist-Shannon Sampling Theorem</a></p>
<p>[^3][Quantization](<a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)?ref=sidbala.com">https://en.wikipedia.org/wiki/Quantization_(signal_processing)</a></p>
<p>[^4]<a href="https://en.wikipedia.org/wiki/Chroma_subsampling?ref=sidbala.com">Chroma Subsampling</a></p>
<!--kg-card-end: markdown-->
    </section></div>]]></description>
        </item>
    </channel>
</rss>