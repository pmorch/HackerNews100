<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 24 Aug 2025 12:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A German ISP tampered with their DNS – specifically to sabotage my website (244 pts)]]></title>
            <link>https://lina.sh/blog/telefonica-sabotages-me</link>
            <guid>45003033</guid>
            <pubDate>Sun, 24 Aug 2025 10:27:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lina.sh/blog/telefonica-sabotages-me">https://lina.sh/blog/telefonica-sabotages-me</a>, See on <a href="https://news.ycombinator.com/item?id=45003033">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <p>One of Germany's biggest ISPs changed how their DNS works, right after I exposed an organization that they’re part of.</p>
            </section><article>
                <h2>My website: Publishing Germany's secret internet blocklist</h2>
<p>In Germany, we have the <em>Clearingstelle Urheberrecht im Internet</em> (CUII) - literally 'Copyright Clearinghouse for the Internet', 
a private organization that decides what websites to block, corporate interests rewriting our free internet.
No judges, no transparency, just a bunch of ISPs and major copyright holders deciding what your eyes can see.<br>
I decided to create a website, <a href="https://cuiiliste.de/">cuiiliste.de</a>, to find blocked domains, as the CUII refuses to publish such a list.
To read more about the CUII, check out <a href="https://lina.sh/blog/exposing-the-cuii">one of my previous blog posts</a>. Germany's four biggest ISPs 
(Telekom, Vodafone, 1&amp;1 and Telefonica (o2)) are all part of the CUII.</p>
<h2>Yet another slip-up by the CUII</h2>
<p>This week, Netzpolitik.org published an article about the CUII's latest blunder<sup id="fnref:1"><a href="#fn:1">1</a></sup>, based on information I gathered. 
They managed to block domains that no longer even existed: websites that had already been seized and taken offline when they were blocked.
It's not the first time the CUII has tripped over its own feet, and this mistake likely didn’t sit well with them.
In the past, it was <em>really</em> easy to find out if a domain was blocked by the CUII.
If you asked an ISP's DNS server (basically the internet's phone book) for a site and got a CNAME to <code>notice.cuii.info</code>, you knew it was blocked.<br>
What this basically means in case you're not a tech nerd:<br>
You can check the phone book of an ISP (the "DNS server") where to find a website, and you'd receive a note saying "This site is blocked by the CUII" if the page is blocked.
Automating this was simple, I could basically just ask "Hey, where can I find this site?" and immediately knew if it was blocked.
The CUII apparently did <em>not</em> like the fact that it was so easy for me to check if a domain was blocked. They want to keep their list secret.<br>
ISPs like Telekom, 1&amp;1 and Vodafone actually all stopped using this response a few months ago, 
after older articles about the CUII's past failures were published. Instead, they started pretending that blocked sites didn't exist at all.
Straight up erasing entries from the phone book. You could not tell if a site was blocked or just didn't exist.
Telefonica (the parent company of for example o2, Germany's <strong>fourth-biggest ISP</strong><sup id="fnref:2"><a href="#fn:2">2</a></sup>), apparently didn't get this memo, and they still used <code>notice.cuii.info</code> in their DNS responses.  </p>
<p>On cuiiliste.de, anyone can enter a domain, and see if it is blocked by the CUII, and which ISPs block it specifically.</p>
<h3>I get a new visitor</h3>
<p>Telefonica modified their DNS servers, specifically saying that <code>blau-sicherheit.info</code> was blocked by the CUII.
At 11:06 AM last Friday, someone from Telefonica's network checked if <code>blau-sicherheit.info</code> was blocked on my site. 
The twist? Telefonica seems to own this domain. Blau is one of their brands<sup id="fnref:3"><a href="#fn:3">3</a></sup>, and <code>blau-sicherheit.info</code> wasn’t some piracy hub -
it appears to be a test domain of theirs. 
My tool flagged it as blocked because Telefonica's DNS servers said so. 
Why would they block their own domain?</p>
<p><a href="https://lina.sh/assets/blog/blau-sicherheit-probe.png" target="_blank"><img alt="Telefonica's DNS response" src="https://lina.sh/assets/blog/blau-sicherheit-probe.png"></a>
To recap:</p>
<ul>
<li> Telefonica blocks their own domain</li>
<li> Someone from Telefonica visits my website to check if I detect this</li>
<li> I <i>do</i> in fact detect this</li>
</ul>
<h3>Telefonica modifies how their blocking works... to mess specifically with my website</h3>
<p>Two hours after this suspicious query, I was bombarded with Notifications. 
My program thought that the CUII had suddenly unblocked hundreds of domains.<br>
The reason: Telefonica had altered their DNS servers to stop redirecting blocked domains to <code>notice.cuii.info</code>.
Now they pretend that the domain doesn't exist at all, after they <em>specifically</em> blocked their own domain, likely to find out how my website works.<br>
I had to spend my entire Friday afternoon fixing this mess, and now everything is fully operational again.
<a href="https://lina.sh/assets/blog/git-pull.png" target="_blank"><img alt="Git pull" src="https://lina.sh/assets/blog/git-pull.png"></a>
The fix worked, but there’s a catch: without the <code>notice.cuii.info</code> redirect, it's harder to confirm if a block is actually the CUII's doing. 
Sometimes ISPs block sites for other reasons, like terrorism content (<a href="https://lina.sh/blog/german-isps-block-terrorist-content">I wrote about that too</a>). 
I try to compensate this by cross-checking domains against a list of known non-CUII-blocks.
<a href="https://lina.sh/assets/blog/almanar-blocked.png" target="_blank"><img alt="Probing a site blocked by ISPs, but not by the CUII" src="https://lina.sh/assets/blog/almanar-blocked.png"></a></p>
<h3>Why sabotage my website?</h3>
<p>The timing is more than suspicious. 
Right after Netzpolitik’s article exposed the CUII for blocking non-existent domains, they make it harder to track their mistakes. 
Coincidence? Or a move to bury future slip-ups? 
We can only speculate.
Regardless of intent, the result is the same: less transparency and harder oversight. And that benefits the CUII, not the public.  </p>
<p>In this context, Netzpolitik.org released another article (German): 
<a href="https://netzpolitik.org/2025/netzsperren-provider-verstecken-welche-domains-sie-sperren/">Netzpolitik.org: Provider verstecken, welche Domains sie sperren</a></p>
<h3>Sources</h3>

            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Turning Claude Code into My Best Design Partner (107 pts)]]></title>
            <link>https://betweentheprompts.com/design-partner/</link>
            <guid>45002315</guid>
            <pubDate>Sun, 24 Aug 2025 08:06:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://betweentheprompts.com/design-partner/">https://betweentheprompts.com/design-partner/</a>, See on <a href="https://news.ycombinator.com/item?id=45002315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When I first started using Claude Code, I had a naive approach to working with it. I would describe the task directly in the prompt, press Enter, and cross my fingers. If the agent made mistakes, I would tell it how to fix them. For small tasks, this can be good enough, but as the task grows in complexity, this approach reveals several significant drawbacks.</p><h2 id="when-simple-doesnt-scale">When Simple Doesn’t Scale</h2><p>The first problem is that the conversation becomes the only source of truth about the task. This means a new message can override instructions from an old one, but it isn’t always clear when this happens, which can cause mistakes by the agent.</p><p>Additionally, the context size for the agent is limited. The more the conversation grows, the more information from the beginning can be “forgotten,” even though Claude Code has a capacity for “compacting” the conversation that’s supposed to improve this issue.</p><p>For these reasons, I started experimenting with an approach I’d heard about: asking Claude Code to start by writing a plan document. This document becomes the source of truth instead of a sprawling conversation. When I find the plan document good enough, I’ve taken the habit of clearing the conversation to start fresh with just the plan as context.</p><h2 id="creating-the-initial-plan">Creating the Initial Plan</h2><p>My first prompt is usually to give Claude Code a description of the feature it should implement (or bug it should fix, or the refactoring it should do) with all the details I have in mind. If I already have an idea of the implementation, I can give it some pointers to existing files for reference. However, I try not to give it too many implementation instructions because I want it to make suggestions and contribute to the design process.</p><blockquote><p>I want to implement a query builder. The page will be displayed as two columns. In the first one, a first box will let the user select a view (for now only one view: “Volume Metrics”), a second box to select fields (“field 1”, “field 2”, “field 3”), and a third one to add filters (don’t fill it yet). The right column will display first the query as human readable, then a table with the query results. (to be continued)</p></blockquote><p>I also try to refer to existing plans for features implemented previously. I don’t have a formal template, but I don’t really need one as long as I have other plans to reference as examples.</p><blockquote><p>(continuing) Check out the previous plan in @plans/chat-playground.md to know about routing and architecture details. (to be continued)</p></blockquote><p>I expect to see several key elements in the document. First, a rephrasing of the feature description I gave it, which helps ensure we’re aligned on requirements. Second, details on how the feature will be implemented—usually it includes some pieces of code or pseudo-code without me having to tell it to do so. Finally, commands to run to make sure the code quality is acceptable, including type checking, linting, and tests.</p><blockquote><p>(continuing) Write a plan in @plans/query-builder.md, and let me validate it before starting the implementation.</p></blockquote><h2 id="the-collaborative-design-process">The Collaborative Design Process</h2><p>Sometimes, I’m not satisfied with the suggested implementation. In this case, instead of updating the plan, I tell it why it’s wrong, expecting it to change its approach.</p><blockquote><p>The page should be a subroute of /explore, not /review. Also, it should be accessible only to users with the “admin” role.</p></blockquote><p>It also happens that after a few back-and-forth exchanges, I realize the first suggested approach was better than the one I had in mind. This process is much more efficient than if I had started writing the code by myself and realized my approach was wrong later.</p><p>It’s a bit like discussing the plan with a colleague each time I’m about to start a new feature. More specifically, it’s like challenging my implementation plan with a junior colleague (or one who doesn’t know the codebase as well) who will question my choices. The dynamic reminds me of the rubber duck debugging technique, where explaining your approach helps you think through problems. However, it won’t suggest a radically different approach unless I specifically ask it to, which I have never tried.</p><p>But the plan document isn’t just a blueprint for the implementation. I discovered that Claude Code is much more efficient when it considers it as a living document.</p><h2 id="the-living-document-approach">The Living Document Approach</h2><p>The key insight is that I don’t just ask Claude Code to write the plan—I also ask it to make it a living document while implementing the feature. I explicitly ask it to update the plan during implementation because the implementation process, and especially the type checking, linting, or test processes, can reveal that some parts of the original plan were incorrect.</p><p>I’ve developed the habit of asking it to check that the plan is up to date each time it commits code, treating plan updates the same way as running quality checks.</p><blockquote><p>Make sure the plan is up to date, and commit changes.</p></blockquote><p>This living document approach solves a fundamental problem with AI development: context limits. With an up-to-date plan document, I can start a fresh conversation and simply ask Claude Code to continue the implementation. This usually works great—just the document is usually enough context for a new session to pick up exactly where the previous one left off.</p><blockquote><p>Continue the implementation documented in @plans/query-builder.md.</p></blockquote><h2 id="my-review-process">My Review Process</h2><p>When the implementation starts, I review changes along the way to ensure everything is progressing correctly. However, if I’m satisfied with the progress, I can let it continue without checking as often. When reviewing the final code, the updated plan document gives me helpful hints about the technical choices that were made during implementation.</p><p>Surprisingly, I think that the fact that I need to plan my features carefully before rushing into implementation is making me a better developer overall. This happens simply because it forces me to document the implementation and think it through before jumping into code. I also find myself explaining my reasoning more clearly because I have to write it down for the AI, whereas with colleagues I would typically discuss things in person or via video call.</p><h2 id="from-chaos-to-system">From Chaos to System</h2><p>This workflow systematically addresses the fundamental problems I encountered with the naive approach: it creates a clear source of truth, eliminates context limit issues, and forces better architectural thinking. The living document becomes both the specification and the implementation log, creating a complete record of not just what was built, but why and how it was built.</p><p>The result is a development process that’s more thoughtful, more documented, and more reliable.</p><p>AI isn’t just serving as implementer.</p><p>It becomes a collaborative design partner.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valve Software handbook for new employees [pdf] (2012) (120 pts)]]></title>
            <link>https://cdn.akamai.steamstatic.com/apps/valve/Valve_NewEmployeeHandbook.pdf</link>
            <guid>45002301</guid>
            <pubDate>Sun, 24 Aug 2025 08:02:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cdn.akamai.steamstatic.com/apps/valve/Valve_NewEmployeeHandbook.pdf">https://cdn.akamai.steamstatic.com/apps/valve/Valve_NewEmployeeHandbook.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45002301">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How to build a coding agent (245 pts)]]></title>
            <link>https://ghuntley.com/agent/</link>
            <guid>45001051</guid>
            <pubDate>Sun, 24 Aug 2025 03:21:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ghuntley.com/agent/">https://ghuntley.com/agent/</a>, See on <a href="https://news.ycombinator.com/item?id=45001051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div><p>😎</p><p>The following was developed last month and has already been delivered at two conferences. If you would like for me to run a workshop similar to this at your employer, please <a href="https://ghuntley.com/workshops/" rel="noreferrer">get in contact</a>.</p></div><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent: A workshop that teaches you how to build your own coding agent. Similar to Roo code, Cline, Amp, Cursor, Windsurf or OpenCode.</p><p>A workshop that teaches you how to build your own coding agent. Similar to Roo code, Cline, Amp, Cursor, Windsurf or OpenCode. - ghuntley/how-to-build-a-coding-agent</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-23.svg" alt=""><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-2" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>workshop materials and source code for the workshop can be found here</span></p></figcaption></figure><p>Hey everyone, I'm here today to teach you how to build a coding agent.  By this stage of the conference, you may be tired of hearing the word "agent". </p><p>You hear the word frequently. However, it appears that everyone is using this term loosely without a clear understanding of what it means or how these coding agents operate internally. It's time to pull back the hood and show that there is no moat.</p><p>Learning how to build a coding agent is one of the best things you can do for your personal development in 2025, as it teaches you the fundamentals. Once you understand these fundamentals, you'll move from being a consumer of AI to a producer of AI who can automate things with AI.</p><p>Let me open with the following facts:</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.002.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.002.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.002.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.002.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.002.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>it's not that hard</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.003.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.003.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.003.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.003.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.003.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>to build a coding agent</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.004.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.004.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.004.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.004.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.004.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>it's 300 lines of code</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.005.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.005.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.005.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.005.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.005.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>running in a loop</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.006.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.006.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.006.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.006.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.006.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>With LLM tokens, that's all it is. </p><p>300 lines of code running in a loop with LLM tokens. You just keep throwing tokens at the loop, and then you've got yourself an agent.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.007.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.007.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.007.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.007.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.007.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Today, we're going to build one. We're going to do it live, and I'll explain the fundamentals of how it all works. As we are now in 2025, it has become the norm to work concurrently with AI assistance. So, what better way to demonstrate the point of this talk than to have an agent build me an agent whilst I deliver this talk? </p><figure data-kg-thumbnail="https://ghuntley.com/content/media/2025/08/Let-s-build-an-agent-AFK_thumb.jpg" data-kg-custom-thumbnail="https://ghuntley.com/content/images/2025/08/Screenshot-2025-08-24-125648.png">
            <div>
                <video src="https://ghuntley.com/content/media/2025/08/Let-s-build-an-agent-AFK.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:22</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://ghuntley.com/content/media/2025/08/Let-s-build-an-agent-AFK_thumb.jpg"></figure><p>Cool. We're now building an agent. This is one of the things that's changing in our industry, because work can be done concurrently and whilst you are away from your computer. </p><p>The days of spending a week or a couple of days on a research spike are now over because you can turn an idea into execution just by speaking to your computer. </p><p>The next time you're on a Zoom call, consider that you could've had an agent building the work that you're planning to do during that Zoom call. If that's not the norm for you, and it is for your coworkers, then you're naturally not going to get ahead.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.009.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.009.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.009.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.009.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.009.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>please build your own</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.010.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.010.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.010.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.010.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.010.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>as the knowledge</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.011.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.011.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.011.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.011.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.011.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>will transform you</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.012.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.012.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.012.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.012.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.012.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>from being a consumer</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.013.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.013.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.013.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.013.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.013.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>to a producer that can</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.014.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.014.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.014.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.014.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.014.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>automate things</span></figcaption></figure><p>The tech industry is almost like a conveyor belt - we always need to be learning new things. </p><p>If I were to ask you what a primary key is, you should know what a primary key is. That's been the norm for a long time.  </p><p>In 2024, it is essential to understand what a primary key is. </p><p>In 2025, you should be familiar with what a primary key is and how to create an agent, as knowing what this loop is and how to build an agent is now fundamental knowledge that employers are looking for in candidates before they'll let you in the door.</p><figure><a href="https://www.canva.dev/blog/engineering/yes-you-can-use-ai-in-our-interviews/?ref=ghuntley.com"><div><p>Yes, You Can Use AI in Our Interviews. In fact, we insist - Canva Engineering Blog</p><p>How We Redesigned Technical Interviews for the AI Era</p><p><img src="https://ghuntley.com/content/images/icon/apple-touch-180x180.b6aae4a9-3.webp" alt=""><span>canva.dev</span><span>Simon Newton</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/thumbnail.1c7590f6-3.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>As this knowledge will transform you from being a consumer of AI to being a producer of AI that can orchestrate your job function. Employers are now seeking individuals who can automate tasks within their organisation.</p><p>If you're joining me later this afternoon for the conference closing (see below), I'll delve a bit deeper into the above. </p><figure><a href="https://ghuntley.com/six-month-recap/"><div><p>the six-month recap: closing talk on AI at Web Directions, Melbourne, June 2025</p><p>Welcome back to our final session at WebDirections. We’re definitely on the glide path—though I’m not sure if we’re smoothly landing, about to hit turbulence, or perhaps facing a go-around. We’ll see how it unfolds. Today, I’m excited to introduce Geoffrey Huntley. I discovered Geoff earlier this year through</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-42.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/the-future-belongs-to-people-who-do-things.001-2.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>the conference closing talk</span></p></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.015.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.015.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.015.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.015.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.015.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Right now, you'll be somewhere on the journey above. </p><p>On the top left, we've got 'prove it to me, it's not real,' 'prove it to me, show me outcomes', 'prove it to me that it's not hype', and a bunch of 'it's not good enough' folks who get stuck up there on that left side of the cliff, completely ignoring that there are people on the other side of the cliff, completely automating their job function.</p><blockquote>In my opinion, any disruption or job loss related to AI is not a result of AI itself, but rather a consequence of a lack of personal development and self-investment. If your coworkers are hopping between multiple agents, chewing on ideas, and running in the background during meetings, and you're not in on that action, then naturally you're just going to fall behind.</blockquote><figure><a href="https://ghuntley.com/ngmi/"><div><p>What do I mean by some software devs are “ngmi”?</p><p>At “an oh fuck moment in time”, I closed off the post with the following quote. N period on from now, software engineers who haven’t adopted or started exploring software assistants, are frankly not gonna make it. Engineering organizations right now are split between employees who have had that “oh</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-45.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/fruits-2.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>don't be the person on the left side of the cliff.</span></p></figcaption></figure><p>The tech industry's conveyor belt continues to move forward. If you're a DevOps engineer in 2025 and you don't have any experience with AWS or GCP, then you're going to find it pretty tough in the employment market. </p><p>What's surprising to software and data engineers is just how fast this is elapsing. It has been eight months since the release of the first coding agent, and most people are still unaware of how straightforward it is to build one, how powerful this loop is, and its disruptive implications for our profession.</p><p>So, my name's Geoffrey Huntley. I was the tech lead for developer productivity at Canva, but as of a couple of months ago, I'm one of the engineers at Sourcegraph building <a href="https://ampcode.com/?ref=ghuntley.com" rel="noreferrer">Amp</a>. It's a small core team of about six people. We build AI with AI.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.016.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.016.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.016.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.016.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.016.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>ampcode.com</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.017.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.017.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.017.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.017.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.017.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>cursor</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.018.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.018.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.018.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.018.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.018.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>windsurf</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.019.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.019.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.019.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.019.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.019.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>claude code</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.020.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.020.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.020.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.020.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.020.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>github co-pilot</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.022.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.022.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.022.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.022.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.022.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>are lines of code running in a loop with LLM tokens</span></figcaption></figure><p>Cursor, Windsurf, Claude Code, GitHub Copilot, and Amp are just a small number of lines of code running in a loop of LLM tokens. I can't stress that enough. The model does all the heavy lifting here, folks. It's the model that does it all. </p><p>You are probably five vendors deep in product evaluation, right now, trying to compare all these agents to one another. But really, you're just chasing your tail. </p><p>It's so easy to build your own...</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.023.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.023.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.023.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.023.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.023.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>There are just a few key concepts you need to be aware of.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.024.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.024.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.024.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.024.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.024.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Not all LLMs are agentic. </p><p>The same way that you have different types of cars, like you've got a <a href="https://en.wikipedia.org/wiki/Toyota_Land_Cruiser_(J40)?ref=ghuntley.com">40 series</a> if you want to go off-road, and then you've also got people movers, which exist for transporting people. </p><p>The same principle applies to LLMs, and I've been able to map their behaviours into a quadrant. </p><blockquote>A model is either high safety, low safety, an oracle, or agentic. It's never both or all.</blockquote><p>If I were to ask you to do some security research, which model would you use? </p><p>That'd be Grok. That's a low safety model.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.025.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.025.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.025.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.025.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.025.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>If you want something that's "ethics-aligned", it's Anthropic or OpenAI. So that's high safety. Similarly, you have oracles. Oracles are on the polar opposite of agentic. Oracles are suitable for summarisation tasks or require a high level of thinking. </p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.026.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.026.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.026.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.026.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.026.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Meanwhile, you have providers like Anthropic, and their Claude Sonnet is a digital squirrel (see below). </p><figure><a href="https://ghuntley.com/cars/"><div><p>Claude Sonnet is a small-brained mechanical squirrel of &lt;T&gt;</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-43.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Traditional-tattoo-style-print.--Low-angle--dramatic-scene-a-mechanical-pump-amidst-a-stormy-sky--pumping-generic-tools.-Vivid-electric-colors--complex-ornamental-designs--retro-feel--white-background.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>The first robot used to chase tennis balls. The first digital robot chases tool calls.</span></p></figcaption></figure><p>Sonnet is a robotic squirrel that just wants to do tool calls. It doesn't spend too much time thinking; it biases towards action, which is what makes it agentic. Sonnet focuses on incrementally obtaining success instead of pondering for minutes per turn before taking action.</p><p>It seems like every day, a new model is introduced to the market, and they're all competing with one another. But truth be told, they have their specialisations and have carved out their niches. </p><p>The problem is that, unless you're working with these models at an intimate level, you may not have this level of awareness of the specialisations of the models, which results in consumers just comparing the models on two basic primitives:</p><ol><li>The size of the context window</li><li>The cost</li></ol><p>It's kind of like looking at a car, whether it has two doors or three doors, whilst ignoring the fact that some vehicles are designed for off-roading, while others are designed for passenger transport. </p><p>To build an agent, the first step is to choose a highly agentic model. That is currently Claude Sonnet, or Kimi K2. </p><p>Now, you might be wondering, "What if you want a higher level of reasoning and checking of work that the incremental squirrel does?". Ah, that's simple. You can wire other LLMs in as tools into an existing agentic LLM. This is what we do at Amp.</p><p>We call it the Oracle. The Oracle is just GPT wired in as a tool that Claude Sonnet can function call for guidance, to check work progress, and to conduct research/planning.</p><figure><a href="https://ampcode.com/news/oracle?ref=ghuntley.com"><div><p>Oracle</p><p><img src="https://ghuntley.com/content/images/icon/amp-mark-color-9.svg" alt=""><span>Amp</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/oracle.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>Amp's oracle is just another LLM registered in as a tool to an agentic LLM that it can function call</span></p></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.027.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.027.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.027.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.027.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.027.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The next important thing to learn is that you should only use the context window for one activity. When you're using Cursor or any one of these tools, it's essential to clear the context window after each activity (see below).</p><figure><a href="https://ghuntley.com/gutter/"><div><p>autoregressive queens of failure</p><p>Have you ever had your AI coding assistant suggest something so off-base that you wonder if it’s trolling you? Welcome to the world of autoregressive failure. LLMs, the brains behind these assistants, are great at predicting the next word—or line of code—based on what’s been fed into</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-44.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/A-traditional-tattoo-style-print-of-a-bowling-ball-split-in-the-gutter--rendered-in-vibrant-colors-with-bold-lines-and-diffused-shadows.--The-image-features-a-retro-flair-and-complex-ornamental-details-against-a-white-background-2.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>LLM outcomtes are a needle in a haystack of what you've allocated into the haystack.</span></p></figcaption></figure><p>If you start an AI-assisted session to build a backend API controller, then reuse that session to research facts about meerkats. Then it should be no surprise when you tell it to redesign the website in the active session; the website might end up with facts about your API or meerkats, or both.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.028.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.028.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.028.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.028.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.028.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>nb. the context window for Sonnet since delivering this workshop has increased to 1m</span></figcaption></figure><p>Context windows are very, very small. It's best to think of them as a Commodore 64, and as such, you should be treating it as a computer with a limited amount of memory. The more you allocate, the worse your outcome and performance will be.</p><p>The advertised context window for Sonnet is 200k. However, you don't get to use all of that because the model needs to allocate memory for the system-level prompt. Then the harness (Cursor, Windsurf, Claude Code, Amp) also needs to allocate some additional memory, which means you end up with approximately 176k tokens usable.</p><p>You probably heard a lot about the Model Context Protocols (MCPs). They are the current hot thing, and the easiest way to think about them is as a function with a description allocated to the context window that tells it how to invoke that function.</p><p>A common failure scenario I observe is people installing an excessive number of MCP servers or failing to consider the number of tools exposed by a single MCP tool or the aggregate context window allocation of all tools.</p><p>There is a cardinal rule that is not as well understood as it should be. The more you allocate to a context window, the worse the performance of the context window will be, and your outcomes will deteriorate.</p><p>Avoid excessively allocating to the context window with your agent or through MCP tool consumption. It's very easy to fall into a trap of allocating an additional 76K of tokens just for MCP tools, which means you only have 100K usable. </p><blockquote>Less is more, folks. Less is more. </blockquote><p>I recommend dropping by and reading the blog post below if you want to understand when to use MCP and when not to.</p><figure><a href="https://ghuntley.com/allocations/"><div><p>too many model context protocol servers and LLM allocations on the dance floor</p><p>This blog post intends to be a definitive guide to context engineering fundamentals from the perspective of an engineer who builds commercial coding assistants and harnesses for a living. Just two weeks ago, I was back over in San Francisco, and there was a big event on Model Context Protocol</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-52.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Depict-AI-robots-dancing-on-a-rose-patterned-dance-floor-in-a-traditional-tattoo-art-style.-The-print-uses-a-vibrant-color-scheme--soft-lighting--and-diffused-shadows--creating-a-calming-atmosphere.-Complex-orna.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>When you should use MCP, when you should not use MCP, and how allocations work in the context window.</span></p></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.029.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.029.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.029.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.029.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.029.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Let's head back and check on our agent that's being built in the background. If you look at it closely enough, you can see the loop and how it's invoking other tools. </p><p>Essentially, how this all works is outlined in the loop below.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.030.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.030.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.030.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.030.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.030.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>For every piece of input from the user or result of a tool call that gets allocated to the response, and that response is sent off for inferencing:</p><figure><img src="https://ghuntley.com/content/images/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png" alt="" loading="lazy" width="2000" height="382" srcset="https://ghuntley.com/content/images/size/w600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 1600w, https://ghuntley.com/content/images/size/w2400/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>The inferencing loop (minus tool registrations)</span></figcaption></figure><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-19.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials (above) and run the basic chat application via:</p><pre><code>go run chat.go</code></pre><blockquote><strong>You:</strong> You are a weather expert. When I ask you about the weather in a given location, I want you to reply with <code>get_weather(&lt;location_name&gt;)</code>. I will then tell you what the weather in that location is. Understood?<p><strong>Claude:</strong> I understand your instructions. When you ask me about the weather in a specific location, I will respond with <code>get_weather(&lt;location_name&gt;)</code>. Then you will provide me with the weather information for that location, and I can discuss it with you from there. I'm ready whenever you'd like to ask about the weather somewhere.</p></blockquote><p>So, let's ask what the weather is today here in Melbourne, Australia?</p><blockquote><strong>You:</strong> What's the weather like today in Melbourne, Australia?<p><strong>Claude:</strong>  <code>get_weather(Melbourne, Australia)</code></p></blockquote><p>boom! There we go! </p><p>We've registered our first tool into the model, and any time that we tickle the latent space with a query about the weather, it's going to function call our new tool. </p><p>It's a stub tool currently, but you can see it; we have a method, and the LLM is passing the parameters into the method from our query.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.031.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.031.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.031.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.031.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.031.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>When we talk about agentic, we're referring to something that's tool-calling, something that has been trained on its reward function to call a tool. There are models where they've been explicitly trained; all they want to do is chase nuts, much like a squirrel (see below). It's not nuts, it's tools.</p><figure><a href="https://ghuntley.com/cars/"><div><p>Claude Sonnet is a small-brained mechanical squirrel of &lt;T&gt;</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-46.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Traditional-tattoo-style-print.--Low-angle--dramatic-scene-a-mechanical-pump-amidst-a-stormy-sky--pumping-generic-tools.-Vivid-electric-colors--complex-ornamental-designs--retro-feel--white-background-1.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>Whilst visiting the Computer History Museum in San Francisco, I stumbled upon the first robotic squirrel, and it got me thinking: "Wait a sec, is Sonnet our first digital squirrel?"</span></p></figcaption></figure><p>In the simplest way to explain what MCP is or a tool, it's just a function with a billboard on top that nudges the LLM's latent space to invoke that function. It is that simple. You register the tool, you describe how to invoke it, and then it just works.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.032.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.032.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.032.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.032.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.032.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Let's start the short journey towards making this a real coding agent. We have a mock function and a mock tool, but <code>get_weather(Melbourne, Australia)</code> currently does nothing, so let's fix that.</p><p>What we need to do is create a tool and then adjust our inferencing loop to check whether it wants to execute the tool (see line 5 above). If it does, execute the tool (see line 6 above) and then allocate the outcome of that tool back (see line 7 above) to the loop for more inferencing.</p><p>Here's what our updated sequence diagram looks like when tools have been registered.</p><figure><img src="https://ghuntley.com/content/images/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png" alt="" loading="lazy" width="1714" height="3840" srcset="https://ghuntley.com/content/images/size/w600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 1600w, https://ghuntley.com/content/images/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 1714w" sizes="(min-width: 720px) 720px"><figcaption><span>this is what a coding agent is in it's purest form without any UI</span></figcaption></figure><p>As we're not making a weather application today, let's switch gears to build the first primitive of a coding agent.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.034.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.034.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.034.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.034.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.034.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The first primitive behind any coding agent is a read tool. All coding agents have a read tool that reads a file into the context window, either the whole file or, in the case of a larger file, in chunks.</p><pre><code>var ReadFileDefinition = ToolDefinition{
	Name:        "read_file",
	Description: "Read the contents of a given relative file path. Use this when you want to see what's inside a file. Do not use this with directory names.",
	InputSchema: ReadFileInputSchema,
	Function:    ReadFile,
}</code></pre><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and create a file:</p><pre><code>cat &lt;&lt; EOF
I have a mane but I'm not a lion,
I have four legs but I'm not a table,
I can gallop but I'm not running,
People say I'm disagreeable because I always say "neigh."
What am I?
EOF &gt; riddle.txt</code></pre><p>Then let's run the read file tool:</p><pre><code>go run read.go</code></pre><blockquote><strong>You:</strong> Claude, mate. What's the answer to this riddle in <code>riddle.txt</code>?<p><strong>Claude:</strong> I'll help you solve the riddle. Let me first read the contents of the <code>riddle.txt</code> file to see what the riddle is.</p><p><strong>tool:</strong> <code>read_file({"path":"riddle.txt"})</code></p><p>Reading file: <code>riddle.txt</code><br>Successfully read file <code>riddle.txt</code> (175 bytes)</p><p>result: I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p><strong>Claude:</strong> After reading the riddle, I can solve it for you:</p><p>The answer to the riddle is: a horse.</p><p>Let's analyze the clues:</p><p>"I have a mane but I'm not a lion" - Horses have manes, which is hair growing from the top of the neck.</p><p>"I have four legs but I'm not a table" - Horses walk on four legs.</p><p>"I can gallop but I'm not running" - Galloping is a specific gait of horses.</p><p>"People say I'm disagreeable because I always say 'neigh'" - "Neigh" is the sound a horse makes, and it sounds like "nay" (which means "no" or disagreement).</p><p>All of these clues point to a horse as the answer to the riddle.</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.033.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.033.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.033.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.033.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.033.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The second primitive behind any coding agent is a list tool. All coding agents have a list files tool, which lists all the files and directories in a given path.</p><pre><code>var ListFilesDefinition = ToolDefinition{
	Name:        "list_files",
	Description: "List files and directories at a given path. If no path is provided, lists files in the current directory.",
	InputSchema: ListFilesInputSchema,
	Function:    ListFiles,
}</code></pre><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and run the basic chat application via:</p><pre><code>go run list_files.go</code></pre><blockquote><strong>You:</strong> Claude, mate. Is there  a <code>README.md</code> in this folder? Also what is the contents of <code>riddle.txt</code>?<p><strong>Claude:</strong>  I'll help you check for a README.md file and show you the contents of <code>riddle.txt</code>.</p><p>First, let me list the files in the current directory to see if <code>README.md</code> exists:</p><p><strong>tool:</strong> <code>list_files({})</code><br>Listing files in directory: .<br>Successfully listed 88 items in .</p><p>result:</p><p><code>[".devenv.flake.nix",".direnv/",".envrc",".gitignore","AGENT.md","Makefile","README.md","bash_tool.go","chat.go","code_search_tool.go","devenv.lock","devenv.nix","devenv.yaml","edit_tool.go","go.mod","go.sum","list_files.go","prompts/","prompts/00-weather.md","prompts/01-read_file.md","prompts/02-list_files.md","prompts/03-bash_tool.md","prompts/04-edit_tool.md","read.go","renovate.json","riddle.txt"]</code></p><p><strong>Claude:</strong> Yes, there is a <code>README.md</code> file in this folder.</p><p>Now, let me show you the contents of <code>riddle.txt</code>:</p><p>tool: <code>read_file({"path":"riddle.txt"})</code><br>result: I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p><strong>Claude:</strong> The contents of <code>riddle.txt is</code> a riddle:</p><p>I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p>The answer to this riddle appears to be a horse.</p></blockquote><p>Boom! We now have a read file tool and a list files tool, and you can see that their functions are called and chained together from our original prompt.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.035.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.035.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.035.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.035.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.035.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The third primitive behind any coding harness is a bash tool, which does precisely what its name implies. It enables the LLM to execute commands on your computer.</p><pre><code>var BashDefinition = ToolDefinition{
	Name:        "bash",
	Description: "Execute a bash command and return its output. Use this to run shell commands.",
	InputSchema: BashInputSchema,
	Function:    Bash,
}</code></pre><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and run the bash tool:</p><pre><code>go run bash_tool.go</code></pre><blockquote><strong>You:</strong> Hey Claude, provide a summary of all the processes running on this computer.<p><strong>Claude:</strong> I'll provide a summary of all the processes running on this computer. Let me use the <code>ps</code> command to gather this information.</p><p>tool: <code>bash({"command":"ps aux"})</code>precisely<br>Executing bash command: ps aux<br>Bash command succeeded: ps aux (output: 87344 bytes)</p><p>result: &lt;snipped&gt;</p><p><strong>Claude:</strong> Based on the output of the <code>ps aux</code> command, here's a summary of the processes running on this computer:</p><p>System Summary:</p><p>This is a multi-CPU system with 96 cores/threads (visible from the many CPU-related kernel threads)</p><p>Running Linux with systemd as the init system</p><p>The system has been up since August 20th (seen in START column)</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.036.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.036.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.036.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.036.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.036.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The fourth primitive behind any coding harness is the edit tool.  The edit tool also does precisely as the name implies. It takes the results back from inference and applies edits to the file.</p><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and run edit tool:</p><pre><code>go run edit_file.go</code></pre><blockquote><strong>You:</strong> Claude, mate. Create <code>fizzbuzz.js</code> with nodejs that does fizzbuzz up to 20 and execute it to ensure it works<p><strong>Claude:</strong> I'll create a Node.js implementation of FizzBuzz that counts up to 20 and then run it to make sure it works correctly.</p><p>First, let me create the <code>fizzbuzz.js</code>, file:</p><p>tool: <code>edit_file({"path":"fizzbuzz.js","old_str":"","new_str":"// FizzBuzz implementation in Node.js\n// Prints numbers from 1 to 20\n// If number is divisible by 3, print "Fizz"\n// If number is divisible by 5, print "Buzz"\n// If number is divisible by both 3 and 5, print "FizzBuzz"\n\nfor (let i = 1; i &lt;= 20; i++) {\n if (i % 3 === 0 &amp;&amp; i % 5 === 0) {\n console.log("FizzBuzz");\n } else if (i % 3 === 0) {\n console.log("Fizz");\n } else if (i % 5 === 0) {\n console.log("Buzz");\n } else {\n console.log(i);\n }\n}"})</code></p><p>Editing file: <code>fizzbuzz.js</code> (replacing 0 chars with 485 chars)</p><p>File does not exist, creating new file: <code>fizzbuzz.js</code><br>Creating new file: <code>fizzbuzz.js</code> (485 bytes)<br>Successfully created file <code>fizzbuzz.js</code></p><p>result: Successfully created file <code>fizzbuzz.js</code></p><p><strong>Claude:</strong> Now let me verify the file was created:<br>tool: <code>read_file({"path":"fizzbuzz.js"})</code></p><p>Reading file: <code>fizzbuzz.js</code><br>Successfully read file <code>fizzbuzz.js</code> (485 bytes)</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/08/fizzbuzz.js--1--1.png" alt="" loading="lazy" width="2000" height="1331" srcset="https://ghuntley.com/content/images/size/w600/2025/08/fizzbuzz.js--1--1.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/fizzbuzz.js--1--1.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/fizzbuzz.js--1--1.png 1600w, https://ghuntley.com/content/images/2025/08/fizzbuzz.js--1--1.png 2080w" sizes="(min-width: 720px) 720px"></figure><blockquote><strong>Claude:</strong> Now I'll execute the script to make sure it works correctly:<p>tool: <code>bash({"command":"node fizzbuzz.js"})</code><br>Executing bash command: <code>node fizzbuzz.js</code></p><p>Bash command executed successfully, output length: 77 chars<br>result: </p><p>1<br>2<br>Fizz<br>4<br>Buzz<br>Fizz<br>7<br>8<br>Fizz<br>Buzz<br>11<br>Fizz<br>13<br>14<br>FizzBuzz<br>16<br>17<br>Fizz<br>19<br>Buzz</p><p><strong>Claude:</strong> Great! The <code>fizzbuzz.js</code> file has been created and executed successfully. The output shows:</p><p>Numbers not divisible by 3 or 5 are printed as is (1, 2, 4, 7, 8, 11, 13, 14, 16, 17, 19)</p><p>Numbers divisible by 3 (but not 5) print "Fizz" (3, 6, 9, 12, 18)</p><p>Numbers divisible by 5 (but not 3) print "Buzz" (5, 10, 20)</p><p>Number 15 prints "FizzBuzz" as it's divisible by both 3 and 5</p></blockquote><p>And voila, we now have a coding agent. Is it this point that we could throw some other prompts in and say, "Hey, let's do FizzBuzz to 100, and it will just work."</p><figure><img src="https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.039.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/08/how-to-build-a-agent.039.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/how-to-build-a-agent.039.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/how-to-build-a-agent.039.png 1600w, https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.039.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The fifth and final primitive behind any coding harness is a search tool. What if I were to tell you that there is no magic for indexing source code or any intelligence? </p><pre><code>var CodeSearchDefinition = ToolDefinition{
	Name: "code_search",
	Description: `Search for code patterns using ripgrep (rg).

Use this to find code patterns, function definitions, variable usage, or any text in the codebase.
You can search by pattern, file type, or directory.`,
	InputSchema: CodeSearchInputSchema,
	Function:    CodeSearch,
}</code></pre><p>Nearly every coding tool currently available uses the open source <code>ripgrep</code> binary under the hood. See the two blog posts below if you want more proof:</p><figure><a href="https://ghuntley.com/amazon-kiro-source-code/"><div><p>source code analysis of Amazon Kiro</p><p>It’s an uncertain time for our profession, but one thing is certain—things will change. Drafting used to require a room of engineers, but then CAD came along…</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-53.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Tattoo-art-print-depicting-reverse-engineered-Amazon-systems--warm-autumn-colors--white-background--intricate-lines-and-vibrant-colors--retro-flair--complex-ornamental-details--charming-and-cute-style.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://ghuntley.com/overton/"><div><p>from Luddites to AI: the Overton Window of disruption</p><p>I’ve been thinking about Overton Windows lately, but not of the political variety. You see, the Overton window can be adapted to model disruptive innovation by framing the acceptance of novel technologies, business models, or ideas within a market or society. So I’ve been pondering about where, when and how</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-51.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/download-3.jpeg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again and run the code search tool:</p><pre><code>go run code_search_tool.go</code></pre><blockquote><strong>You:</strong> Claude, mate. Search the source code for a riddle<p><strong>Claude:</strong> I'd be happy to search the source code for a riddle. Let me use the code search tool to look for mentions of riddles in the codebase.</p><p>tool: <code>code_search({"pattern":"riddle"})</code></p><p>Searching for pattern: <code>riddle</code><br>Found 6 matches for pattern: <code>riddle</code></p><p>result: </p><p><code>./README.md:332:- <strong>riddle.txt</strong>: A simple riddle for content analysis</code></p><p><strong>Claude:</strong> I've found references to a riddle file called "<code>riddle.txt</code>" in the codebase. Let me check if this file exists by listing the files in the current directory.</p><p>tool: <code>list_files({})</code><br>Listing files in directory: .<br>Successfully listed 25 files in .<br>result: </p><p><code>["./go.mod","./renovate.json","./devenv.yaml","./AGENT.md","./.devenv.flake.nix","./.envrc","./bash_tool.go","./fizzbuzz.js","./code_search_tool.go","./list_files.go","./Makefile","./devenv.nix","./devenv.lock","./prompts/01-read_file.md","./prompts/02-list_files.md","./prompts/00-weather.md","./prompts/03-bash_tool.md","./prompts/04-edit_tool.md","./chat.go","./go.sum","./riddle.txt","./edit_tool.go","./README.md","./.gitignore","./read.go"]</code></p><p>Claude: Great! I can see that "<code>riddle.txt</code>" exists in the current directory. Let me read its contents to see what the riddle is.</p><p>tool: <code>read_file({"path":"riddle.txt"})</code></p><p>Reading file: <code>riddle.txt</code><br>Successfully read file <code>riddle.txt</code> (175 bytes)<br>result: I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p><strong>Claude:</strong> I found the riddle in the file <code>riddle.txt</code>!</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.037.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.037.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.037.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.037.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.037.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Everything from this point forward is just a matter of tuning your prompts. Due to time constraints, we didn't delve into the notion of the harness prompt. The harness prompt is where your tool registrations go. It contains information such as the operating system you're running, so it knows whether to use PowerShell (Windows) or bash (Linux or macOS). </p><p>It also contains instructions on how the agent should operate. I say should is because LLMs are non-deterministic. You can include the guidance, and it's just guidance. However, through prompt evaluation, tuning, and spending time playing with the models to understand how they behave, you can develop effective prompts.</p><p>Drop by the blog post below if you want to see an example of a fully fledged coding agent:</p><figure><a href="https://ghuntley.com/amazon-kiro-source-code/"><div><p>source code analysis of Amazon Kiro</p><p>It’s an uncertain time for our profession, but one thing is certain—things will change. Drafting used to require a room of engineers, but then CAD came along…</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-55.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Tattoo-art-print-depicting-reverse-engineered-Amazon-systems--warm-autumn-colors--white-background--intricate-lines-and-vibrant-colors--retro-flair--complex-ornamental-details--charming-and-cute-style-1.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>There are plenty of open-source coding agents already, such as SST Open Code</p><figure><a href="https://github.com/sst/opencode?ref=ghuntley.com"><div><p>GitHub - sst/opencode: AI coding agent, built for the terminal.</p><p>AI coding agent, built for the terminal. Contribute to sst/opencode development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-21.svg" alt=""><span>GitHub</span><span>sst</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/6aa80c72-cede-4e0d-b724-d3ed79320a7e" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Or this 100-line agent, which scored really high on the SWE Bench.</p><figure><a href="https://github.com/SWE-agent/mini-swe-agent?ref=ghuntley.com"><div><p>GitHub - SWE-agent/mini-swe-agent: The 100 line AI agent that solves GitHub issues or helps you in your command line. Radically simple, no huge configs, no giant monorepo—but scores 68% on SWE-bench verified!</p><p>The 100 line AI agent that solves GitHub issues or helps you in your command line. Radically simple, no huge configs, no giant monorepo—but scores 68% on SWE-bench verified! - SWE-agent/mini-swe-agent</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-24.svg" alt=""><span>GitHub</span><span>SWE-agent</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/mini-swe-agent" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>And if you want some inspiration, there are many repositories on GitHub with leaked developer tooling harness and tool prompts.</p><figure><a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools?ref=ghuntley.com"><div><p>GitHub - x1xhlol/system-prompts-and-models-of-ai-tools: FULL v0, Cursor, Manus, Augment Code, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent, VSCode Agent, Dia Browser, Xcode, Trae AI, Cluely &amp; Orchids.app (And other Open Sourced) System Prompts, Tools &amp; AI Models.</p><p>FULL v0, Cursor, Manus, Augment Code, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent, VSCode Agent, Dia Browser, Xcode, Trae AI, Cluely &amp;amp; Orchids.app (And other Open Sourced) System Pro…</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-22.svg" alt=""><span>GitHub</span><span>x1xhlol</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/system-prompts-and-models-of-ai-tools" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><img src="https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.041.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/08/how-to-build-a-agent.041.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/how-to-build-a-agent.041.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/how-to-build-a-agent.041.png 1600w, https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.041.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>In recap. What you just built was a coding agent. Perhaps you don't want to create a coding agent. What if you're in the data engineering profession? What would that look like? Think about all of the activities that you do day-to-day, where having the capability to automate using these primitives could be handy or valuable to your employer.</p><p>Your current workers are going to take your job, not AI. </p><p>If you're concerned about AI, the answer is straightforward: just invest in yourself. It really is that simple. This year is a particularly challenging time to be asleep at the wheel when it comes to personal development.  </p><figure><a href="https://ghuntley.com/six-month-recap/"><div><p>the six-month recap: closing talk on AI at Web Directions, Melbourne, June 2025</p><p>Welcome back to our final session at WebDirections. We’re definitely on the glide path—though I’m not sure if we’re smoothly landing, about to hit turbulence, or perhaps facing a go-around. We’ll see how it unfolds. Today, I’m excited to introduce Geoffrey Huntley. I discovered Geoff earlier this year through</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-54.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/the-future-belongs-to-people-who-do-things.001-3.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>conference lock-note</span></p></figcaption></figure><p>I hope to see you later this afternoon for the conference for the locknote (see above). </p><blockquote>Go forward and build.</blockquote><p>ps. socials</p><ul><li>X - <a href="https://x.com/GeoffreyHuntley/status/1959453609240007052?ref=ghuntley.com">https://x.com/GeoffreyHuntley/status/1959453609240007052</a></li><li>Linkedin : <a href="https://www.linkedin.com/posts/geoffreyhuntley_how-to-build-an-agent-activity-7365220712707485697-EiJO?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI">https://www.linkedin.com/posts/geoffreyhuntley_how-to-build-an-agent-activity-7365220712707485697-EiJO</a></li><li>Bluesky: <a href="https://bsky.app/profile/ghuntley.com/post/3lx4jmqsclk2q?ref=ghuntley.com">https://bsky.app/profile/ghuntley.com/post/3lx4jmqsclk2q</a></li></ul>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla insiders have sold more than 50% of their shares in the last year (102 pts)]]></title>
            <link>https://electrek.co/2025/08/18/tesla-tsla-insiders-have-sold-more-than-50-of-their-shares-in-the-last-year/</link>
            <guid>45000248</guid>
            <pubDate>Sun, 24 Aug 2025 00:36:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/08/18/tesla-tsla-insiders-have-sold-more-than-50-of-their-shares-in-the-last-year/">https://electrek.co/2025/08/18/tesla-tsla-insiders-have-sold-more-than-50-of-their-shares-in-the-last-year/</a>, See on <a href="https://news.ycombinator.com/item?id=45000248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="766" src="https://electrek.co/wp-content/uploads/sites/3/2025/04/Tesla-family-hero.png?w=1600" alt="Tesla family hero" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/04/Tesla-family-hero.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/04/Tesla-family-hero.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/04/Tesla-family-hero.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/04/Tesla-family-hero.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Tesla insiders have been unloading their shares at an impressive rate. Excluding CEO Elon Musk, Tesla executives and board members have sold more than 50% of their TSLA shares over the last year.</p>



<p>And that might only be part of the story.</p>



<p>Public companies are required to report insider trading by key executives and board members.</p>



<p>In recent years, Tesla’s number of key executives has dwindled to now only three:</p>	
	



<ul>
<li>Elon Musk</li>



<li>Tom Zhu</li>



<li>Vaibhav Taneja</li>
</ul>



<p>Here’s Tesla’s corporate governance page on its investor relations website:</p>



<figure><img decoding="async" height="573" width="1024" src="https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?w=1024" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png 2444w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=150,84 150w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=300,168 300w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=768,430 768w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=1024,573 1024w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=1536,860 1536w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=2048,1146 2048w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=350,196 350w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=140,78 140w, https://electrek.co/wp-content/uploads/sites/3/2025/08/Screenshot-2025-08-18-at-10.23.57-AM.png?resize=1600,896 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>That’s partly due to several of them leaving in the last year, including Drew Baglino, who was the de facto head of engineering and was listed as a key executive before leaving last year.</p>



<p>It’s also because Musk is known to micro-manage, resulting in him having many direct reports who would generally go through other department heads.</p>



<p>The result is that only two Tesla executives, in addition to Musk, who would have to report his transactions even if he weren’t CEO, since he owns more than 10% of the company, are required to report their stock transactions.</p>



<p>Based on Tesla’s 2024 proxy statement, here were the insider ownership last year:</p>



<figure><table><thead><tr><th>TSLA Insider Ownership</th><th>Total</th><th>Shares</th><th>Options</th></tr></thead><tbody><tr><td>Elon Musk</td><td>715,022,706</td><td>303,960,630</td><td>411,062,076</td></tr><tr><td>Vaibhav Taneja</td><td>1,063,544</td><td>105,032</td><td>958,512</td></tr><tr><td>Andrew Baglino</td><td>1,218,669</td><td>31,230</td><td>1,187,439</td></tr><tr><td>Tom Zhu</td><td>1,996,983</td><td>63,171</td><td>1,933,812</td></tr><tr><td>Robyn Denholm</td><td>1,490,069</td><td>15,000</td><td>1,475,069</td></tr><tr><td>Ira Ehrenpreis</td><td>1,681,005</td><td>1,571,005</td><td>110,000</td></tr><tr><td>Joe Gebbia</td><td>111</td><td>111</td><td>0</td></tr><tr><td>James Murdoch</td><td>1,427,295</td><td>157,275</td><td>1,270,020</td></tr><tr><td>Kimbal Musk</td><td>1,950,470</td><td>1,608,720</td><td>341,750</td></tr><tr><td>Kathleen Wilson‑Thompson</td><td>771,255</td><td>5,400</td><td>765,855</td></tr><tr><td><strong>TOTAL</strong></td><td><strong>726,622,107</strong></td><td><strong>307,517,574</strong></td><td><strong>419,104,533</strong></td></tr><tr><td><strong>TOTAL Excl. Elon Musk</strong></td><td><strong>11,599,401</strong></td><td><strong>3,556,944</strong></td><td><strong>8,042,457</strong></td></tr></tbody></table></figure>



<p>Now here’s the ownership of Tesla shares and options from insiders based on the 2025 proxy statement:</p>



<figure><table><thead><tr><th>Name</th><th><strong>Total</strong></th><th><strong>Shares</strong></th><th><strong>Options</strong></th><th><strong>As‑of (filing)</strong></th></tr></thead><tbody><tr><td>Elon Musk</td><td>714,754,706</td><td>410,794,076</td><td>303,960,630</td><td>12/31/2024 (10‑K/A filed 4/30/2025)</td></tr><tr><td>Vaibhav Taneja</td><td>830,844</td><td>116,924</td><td>713,920</td><td>7/8/2025</td></tr><tr><td>Andrew Baglino</td><td>520,005</td><td>31,230</td><td>488,775</td><td>4/1/2024 (latest)</td></tr><tr><td>Tom Zhu</td><td>348,250</td><td>67,600</td><td>280,650</td><td>6/12/2025</td></tr><tr><td>Robyn Denholm</td><td>85,000</td><td>85,000</td><td>0</td><td>5/6/2025</td></tr><tr><td>Ira Ehrenpreis</td><td>855,394</td><td>855,394</td><td>0</td><td>5/27/2025</td></tr><tr><td>Joe Gebbia</td><td>4,111</td><td>4,111</td><td>0</td><td>4/24/2025</td></tr><tr><td>James Murdoch</td><td>1,282,519</td><td>884,306</td><td>398,213</td><td>3/10/2025</td></tr><tr><td>Kimbal Musk</td><td>1,463,220</td><td>1,463,220</td><td>0</td><td>5/27/2025</td></tr><tr><td>Kathleen Wilson‑Thompson</td><td>5,400</td><td>5,400</td><td><strong>0</strong></td><td>5/1/2025 (options canceled)</td></tr><tr><td><strong>TOTAL (sum of listed rows)</strong></td><td><strong>720,149,449</strong></td><td><strong>414,307,261</strong></td><td><strong>305,842,188</strong></td><td></td></tr><tr><td><strong>TOTAL excl. Elon Musk</strong></td><td><strong>5,394,743</strong></td><td><strong>3,513,185</strong></td><td><strong>1,881,558</strong></td><td></td></tr></tbody></table></figure>



<p>As you can see, excluding Musk, Tesla insiders sold more than half their shares in the company over the last year.</p>



<p>To be fair, some of those changes also include cancellations of stock options for board members, who settled <a href="https://electrek.co/2025/01/08/tesla-board-members-officially-settle-excessive-compensation-case-for-nearly-1-billion/" target="_blank" rel="noreferrer noopener">a shareholders’ lawsuit for having overcompensated themselves</a>.</p>



<p>However, it doesn’t account for the reduction in ownership of more than 6 million shares and stock options, worth approximately $2 billion at today’s share price.</p>



<p>There are also some specific examples of non-board members liquidating their stakes.</p>



<p>Tom Zhu, who has led Tesla’s manufacturing efforts and was at times seen as Musk’s number 2 at Tesla, reduced his stake by 82% in a single year.</p>



<p>This happened while Musk claimed that Tesla will become the most valuable company in the world and roughly 10x its current stock price due to autonomous driving and robots, a claim most unbiased analysts have been highly skeptical about.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>More than 50% reduction in ownership in a single year is wild.</p>



<p>But as I hinted at the beginning of the article, this is only what we can see. Other Tesla execs, managers, and employees also have shares and stock options, and they could potentially be selling at an even higher rate. We simply don’t know.</p>




	<p>The single reporting person to have bought shares is Joe Gebbia, who only bought about $1 million worth, and he is a multi-billionaire. It would be the equivalent of me buying a few hundred dollars’ worth of Electrek shares – not a great show of confidence in my company.</p>



<p> I’m not in the business of predicting Tesla’s share price. I think it trades mainly on gullible Tesla retail shareholders believing Musk’s lies.</p>



<p>But I believe that Tesla will likely face several challenging quarters in the next few years and may even start incurring losses.  I think many executives also see this coming and don’t believe that autonomy and humanoid robots will have a positive financial contribution for a few years, as Musk claims.</p>



<p>Meanwhile, Tesla’s EV business is struggling, and there’s little hope of reversing the trend without fresh new models and innovation – the pace of which appears to have greatly slowed at Tesla, unfortunately.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AGI is an engineering problem, not a model training problem (156 pts)]]></title>
            <link>https://www.vincirufus.com/posts/agi-is-engineering-problem/</link>
            <guid>45000176</guid>
            <pubDate>Sun, 24 Aug 2025 00:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vincirufus.com/posts/agi-is-engineering-problem/">https://www.vincirufus.com/posts/agi-is-engineering-problem/</a>, See on <a href="https://news.ycombinator.com/item?id=45000176">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>We’ve reached an inflection point in AI development. The scaling laws that once promised ever-more-capable models are showing diminishing returns. GPT-5, Claude, and Gemini represent remarkable achievements, but they’re hitting asymptotes that brute-force scaling can’t solve. The path to artificial general intelligence isn’t through training ever-larger language models—it’s through building engineered systems that combine models, memory, context, and deterministic workflows into something greater than their parts.</p>
<p>Let me be blunt: <strong>AGI is an engineering problem, not a model training problem.</strong></p>
<h2 id="the-plateauing-reality">The Plateauing Reality</h2>
<p>The current generation of large language models has hit a wall that’s become increasingly obvious to anyone working with them daily. They’re impressive pattern matchers and text generators, but they remain fundamentally limited by their inability to maintain coherent context across sessions, their lack of persistent memory, and their stochastic nature that makes them unreliable for complex multi-step reasoning.</p>
<p>We’ve seen this movie before. Every technology wave follows the same trajectory: initial breakthrough, rapid scaling, then increasing marginal costs for decreasing marginal gains. The semiconductor industry hit this wall in the early 2000s when clock speed scaling became impossible. The solution then wasn’t to brute-force faster processors—it was to fundamentally rethink the architecture with multi-core designs.</p>
<p><strong>AI is at the same inflection point.</strong> We need to stop asking “how do we make the model bigger?” and start asking “how do we make the system smarter?”</p>
<h2 id="the-systems-approach-to-agi">The Systems Approach to AGI</h2>
<p>The human brain isn’t a single neural net—it’s a collection of specialized systems working in concert: memory formation, context management, logical reasoning, spatial navigation, language processing. Each system has evolved specific purposes, and they operate asynchronously with complex feedback loops between them.</p>
<p><strong>True AGI requires us to engineer similar systems. Here’s what we actually need to build:</strong></p>
<h3 id="1-context-management-as-infrastructure">1. Context Management as Infrastructure</h3>
<p>Current models have attention spans measured in thousands of tokens. Human context span extends across years of lived experience. The gap isn’t just quantitative—it’s qualitative. We need context management systems that can:</p>
<ul>
<li><strong>Retrieve and filter relevant information</strong> on-demand using sophisticated retrieval systems</li>
<li><strong>Maintain coherent world models</strong> that persist across sessions and evolve with new information</li>
<li><strong>Bridge context gaps</strong> between different specialized knowledge domains</li>
<li><strong>Handle conflicting information</strong> with probabilistic weighting and uncertainty quantification</li>
</ul>
<p>This requires moving beyond simple vector similarity searches to building operational knowledge graphs that can be updated, queried, and reasoned about in real-time. <a href="https://www.vincirufus.com/blog/context-engineering">Our work on Context Engineering</a> provides a foundation for these systems.</p>
<h3 id="2-memory-as-a-service">2. Memory as a Service</h3>
<p>LLMs don’t have memory—they engage in elaborate methods to fake it through prompt engineering and context stuffing. Real AGI needs memory systems that:</p>
<ul>
<li><strong>Update beliefs</strong> when contradicted by new evidence</li>
<li><strong>Consolidate information</strong> across multiple experiences into general principles</li>
<li><strong>Forget irrelevant details</strong> without catastrophic forgetting</li>
<li><strong>Generate meta-knowledge</strong> about the reliability and source of stored information</li>
</ul>
<p>This isn’t just database persistence—it’s building memory systems that evolve the way human memory does: strengthening with use, decaying with disuse, and reorganizing based on new understanding. The <a href="https://www.vincirufus.com/blog/adr">architectural patterns from software systems</a> show us how to design such evolving structures.</p>
<h3 id="3-deterministic-workflows-with-probabilistic-components">3. Deterministic Workflows with Probabilistic Components</h3>
<p>The real breakthrough in AGI will come from building deterministic frameworks that can incorporate probabilistic components when appropriate. Think of it like building a compiler: the overall flow is rigid and predictable, but individual steps can use heuristics and probabilistic optimization.</p>
<p><strong>We need systems that can:</strong></p>
<ul>
<li><strong>Route problems</strong> to appropriate specialized solvers based on problem characteristics</li>
<li><strong>Execute multi-step workflows</strong> with rollback and recovery capabilities</li>
<li><strong>Validate outputs</strong> through deterministic checks before accepting probabilistic results</li>
<li><strong>Compose capabilities</strong> in predictable ways while maintaining the benefits of stochastic generation</li>
</ul>
<p>Our <a href="https://www.vincirufus.com/blog/deterministic-vs-propapbistic">research on deterministic vs. probabilistic systems</a> demonstrates how we can build these hybrid architectures effectively. The key insight is that <strong>uncertainty should be a first-class concept</strong> in system design, not something we try to eliminate.</p>
<h3 id="4-specialized-models-as-modular-components">4. Specialized Models as Modular Components</h3>
<p>The future isn’t one model to rule them all—it’s hundreds or thousands of specialized models working together in orchestrated workflows. Language models remain excellent at linguistic tasks, but they’re terrible at:</p>
<ul>
<li><strong>Symbolic manipulation</strong> and exact calculation</li>
<li><strong>Visual-spatial reasoning</strong> beyond basic pattern matching</li>
<li><strong>Temporal reasoning</strong> and planning complex sequences</li>
<li><strong>Intentional agent behavior</strong> with persistent goals</li>
</ul>
<p>Instead of waiting for a breakthrough that makes language models good at everything, we should be building systems that:</p>
<ul>
<li><strong>Route problems</strong> to models optimized for specific domains (<a href="https://www.vincirufus.com/blog/thinking-in-agents">thinking in agents</a> demonstrates this approach)</li>
<li><strong>Combine outputs</strong> from different model types into coherent solutions</li>
<li><strong>Maintain compatibility</strong> while allowing individual components to evolve independently</li>
<li><strong>Handle failure gracefully</strong> when individual models underperform</li>
</ul>
<h2 id="the-engineering-challenge">The Engineering Challenge</h2>
<p>This brings us to the core insight: <strong>building AGI is a distributed systems problem, not a machine learning problem.</strong> We’ve been fooled into thinking that because data center-scale training clusters are distributed systems, we’re already doing systems engineering. Nothing could be further from the truth.</p>
<p><strong>The real engineering challenge is building:</strong></p>
<ul>
<li><strong>Fault-tolerant pipelines</strong> where component failures don’t cascade into system failures</li>
<li><strong>Monitoring and observability</strong> systems that can detect when model outputs are drifting or becoming unreliable</li>
<li><strong>Deployment systems</strong> that allow for rolling updates without breaking existing integrations</li>
<li><strong>Testing frameworks</strong> that can validate system behavior across thousands of model and parameter combinations</li>
</ul>
<p>This is the kind of engineering challenge that requires decades of distributed systems experience, not just machine learning expertise. The solutions will come from infrastructure engineers who understand how to build reliable, scalable systems at the intersection of hardware, software, and AI models.</p>
<h2 id="what-we-should-be-building-instead">What We Should Be Building Instead</h2>
<p>While everyone else is focused on scaling the next model, we should be building the infrastructure that makes general intelligence possible. Here’s my roadmap:</p>
<h3 id="phase-1-foundation-layer">Phase 1: Foundation Layer</h3>
<ul>
<li><strong>Context Management Service</strong>: Persistent, queryable, versioned knowledge graphs with real-time updates</li>
<li><strong>Memory Service</strong>: Episodic and semantic memory systems with learned consolidation patterns</li>
<li><strong>Workflow Engine</strong>: Deterministic orchestration of probabilistic components with rollback capabilities</li>
<li><strong>Agent Coordination Layer</strong>: Multi-agent systems with negotiated consensus and conflict resolution</li>
</ul>
<h3 id="phase-2-capability-layer">Phase 2: Capability Layer</h3>
<ul>
<li><strong>Specialized Model Controls</strong>: Fine-tuned models for specific reasoning domains with standardized interfaces</li>
<li><strong>Symbolic Reasoning Engine</strong>: Exact calculation and symbolic manipulation capabilities that work with probabilistic components</li>
<li><strong>Planning and Goal Management</strong>: Systems that can break complex objectives into executable sub-plans</li>
<li><strong>Cross-modal Integration</strong>: Systems that combine sensory inputs (text, vision, audio) into unified representations</li>
</ul>
<h3 id="phase-3-emergence-layer">Phase 3: Emergence Layer</h3>
<p>This is where real AGI emerges—from the interaction of all these components working together, not from any single breakthrough model. The system’s capabilities will exceed those of its individual parts through emergent properties that arise from careful architectural design.</p>
<h2 id="the-path-forward">The Path Forward</h2>
<p>The path to AGI isn’t through training a bigger transformer—it’s through building distributed systems that can orchestrate hundreds of specialized models, maintain coherent context across sessions, execute deterministic workflows around probabilistic components, and provide fault-tolerant operation at production scale.</p>
<p>This is fundamentally <strong>engineering work</strong>, requiring decades of experience building reliable distributed systems. The breakthroughs will come from infrastructure engineers who understand how to build context paths, memory systems, workflow orchestration, and model coordination at scale.</p>
<p><strong>The race to AGI isn’t being won by the team with the biggest GPU cluster—it’s being won by the team that understands how to build reliable, engineered AI systems that can actually reason across domains while maintaining consistent behavior.</strong></p>
<p>The models we have now are sufficient. The missing piece is the systems engineering that turns them into general intelligence.</p>
<p>We’ve been asking the wrong question. It’s not “how do we get to the next model breakthrough?” It’s “how do we build the system architecture that makes general intelligence inevitable with the models we already have?”</p>
<p>The answer is systems engineering. The future of AGI is architectural, not algorithmic.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good EU regulations (133 pts)]]></title>
            <link>https://www.actuallygoodregulations.eu/</link>
            <guid>44999419</guid>
            <pubDate>Sat, 23 Aug 2025 21:51:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.actuallygoodregulations.eu/">https://www.actuallygoodregulations.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=44999419">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The cost of interrupted work (2023) (205 pts)]]></title>
            <link>https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html</link>
            <guid>44999373</guid>
            <pubDate>Sat, 23 Aug 2025 21:45:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html">https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html</a>, See on <a href="https://news.ycombinator.com/item?id=44999373">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main_content_wrap">
        
<p>2023-11-05</p>

<p>You’ve likely read lots of blog posts stating that it takes 23 minutes and 15 seconds to get back to work after an interruption, context switch, or meeting. Thus, “do you have five minutes” ends up not only costing those few minutes, but instead about half an hour. But where does that number come from?</p>

<p>I just wanted to quickly reference this fact to a colleague. Quick search for the reference, copy’n’paste it, in and out, 20 minutes adventure. I quickly found a reference to a paper. For sanity sake, I just wanted to verify where it states the 23 minutes. Open the paper, Ctrl+F for “23”, “no results”. Huh?</p>

<h3 id="papers">Papers</h3>

<p>Most of the posts mentioning the number refer to the paper <a href="https://ics.uci.edu/~gmark/chi08-mark.pdf">The Cost of Interrupted Work: More Speed and Stress</a>. The authors performed a study investigating different effects of interruptions during long tasks.<br>
Contrary to the quoted number, the study found out that the time spent on only the original task was in fact lower when interruptions were present (20.31 and 20.60 min) compared to no interruptions (22.77 min), albeit with much higher experienced stress. The paper never goes into details regarding the recovery time between finishing the interruption and getting back to the original task. The paper never mentions the number <code>23</code>.</p>

<p>Maybe it’s in a different paper? Related Work? References?</p>
<ul>
  <li><a href="http://erichorvitz.com/taskdiary.pdf">A Diary Study of Task Switching and Interruptions</a> let participants record interruption diaries. The paper does not include or mention task switch recovery time. Its primary result is that the average person has 50 task switches per week.</li>
  <li><a href="https://erichorvitz.com/CHI_2007_Iqbal_Horvitz.pdf">Disruption and Recovery of Computing Tasks: Field Study, Analysis, and Directions</a> states that it takes 11 - 16 minutes to resolve an interruption until getting back to the original task. Some of that time is spent to get the mind back into the original task. However, no further investigation of the recovery period has been performed.</li>
  <li><a href="https://interruptions.net/literature/Adamczyk-CHI04-p271-adamczyk.pdf">If Not Now, When?: The Effects of Interruption at Different Moments Within Task Execution</a> states “An approximate value for Resumption Lag, the time a subject takes to switch focus back to primary task after interruption, was also collected.” However, the paper never provides any value for that number and doesn’t discuss it further.</li>
  <li><a href="https://ics.uci.edu/~gmark/CHI2005.pdf">No Task Left Behind? Examining the Nature of Fragmented Work</a> focuses on the probability that a task was resumed on the same day in regards to recovery. There is no mention of a specific recovery time.</li>
</ul>

<h3 id="blog-posts">Blog Posts</h3>

<p>The search continued. In addition to the 5 papers I (fittingly) read through a total of 23 posts.</p>
<ul>
  <li>9 posts incorrectly referred to one of the papers; one of them even included a quote that is nowhere to be found within the referenced paper</li>
  <li>2 posts correctly referred to the first paper for its actual results</li>
  <li>9 posts directly or indirectly refer to three interviews with Gloria Mark (the author of the original paper), in which she stated the 23 minutes and 15 seconds recovery time</li>
  <li>2 posts refer to the Wall Street Journal, which directly quotes Gloria Mark with the 23 minutes 15 seconds figure</li>
</ul>

<p>So in the end, where do the 23 minutes and 15 seconds come from? They are mentioned in interviews multiple times by Gloria Mark. But I wasn’t able to find a primary printed source. There are <a href="https://ics.uci.edu/~gmark/Home_page/Publications.html">many more publications by Gloria Mark</a>, but none of them turned up while searching for the 23 minutes 15 seconds figure. If someone knows a paper or study where that figure originally appears in, please tell me.</p>

<hr>

<p>Discussion on <a href="https://www.reddit.com/r/programming/comments/17ooxwe/interruptions_cost_23_minutes_15_seconds_right/">r/programming</a>.</p>

<hr>

<p>Here is the reference graph of all posts and papers I’ve mentioned in this post and a list of their links.</p>

<p><img src="https://blog.oberien.de/assets/2023-11-05-references.svg" alt=""></p>

<ul>
  <li>Dev Interrupted – <a href="https://devinterrupted.substack.com/p/3-proven-ways-to-improve-dev-focus">https://devinterrupted.substack.com/p/3-proven-ways-to-improve-dev-focus</a></li>
  <li>Loom – <a href="https://www.loom.com/blog/cost-of-context-switching">https://www.loom.com/blog/cost-of-context-switching</a></li>
  <li>Paladinic – <a href="https://www.paladininc.com/blog/detail/6299/dealing-with-work-interruptions">https://www.paladininc.com/blog/detail/6299/dealing-with-work-interruptions</a></li>
  <li>Lifehacker – <a href="https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353">https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353</a></li>
  <li>The Muse – <a href="https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted">https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted</a></li>
  <li>Fast Company – <a href="https://www.fastcompany.com/944128/worker-interrupted-cost-task-switching">https://www.fastcompany.com/944128/worker-interrupted-cost-task-switching</a></li>
  <li>idonethis – <a href="https://blog.idonethis.com/distractions-at-work/">https://blog.idonethis.com/distractions-at-work/</a></li>
  <li>idea to value – <a href="https://www.ideatovalue.com/curi/nickskillicorn/2023/07/it-takes-23-minutes-to-regain-focus-after-a-distraction-task-switching/">https://www.ideatovalue.com/curi/nickskillicorn/2023/07/it-takes-23-minutes-to-regain-focus-after-a-distraction-task-switching/</a></li>
  <li>LeadDev – <a href="https://leaddev.com/process/managing-chaos-context-switching">https://leaddev.com/process/managing-chaos-context-switching</a></li>
  <li>getabstract – <a href="https://journal.getabstract.com/en/2022/03/17/twenty-three-minutes/">https://journal.getabstract.com/en/2022/03/17/twenty-three-minutes/</a></li>
  <li>gallup – <a href="https://news.gallup.com/businessjournal/23146/too-many-interruptions-work.aspx">https://news.gallup.com/businessjournal/23146/too-many-interruptions-work.aspx</a></li>
  <li>JournalStar – <a href="https://eu.pjstar.com/story/news/2013/01/25/frequent-emails-phone-call-interruptions/42450766007/">https://eu.pjstar.com/story/news/2013/01/25/frequent-emails-phone-call-interruptions/42450766007/</a></li>
  <li>togglblog – <a href="https://toggl.com/blog/how-to-get-back-on-track-when-you-get-distracted-at-work">https://toggl.com/blog/how-to-get-back-on-track-when-you-get-distracted-at-work</a></li>
  <li>Presentation by Gloria Mark – <a href="https://slideplayer.com/slide/1409624/">https://slideplayer.com/slide/1409624/</a></li>
  <li>Productivityjunkie – <a href="https://www.linkedin.com/pulse/productivityjunkie-23-minutes-15-seconds-mystery-sugar-inga-bieli%C5%84ska">https://www.linkedin.com/pulse/productivityjunkie-23-minutes-15-seconds-mystery-sugar-inga-bieli%C5%84ska</a></li>
  <li>Bright Developers – <a href="https://www.brightdevelopers.com/the-cost-of-interruption-for-software-developers/">https://www.brightdevelopers.com/the-cost-of-interruption-for-software-developers/</a></li>
  <li>Wall Street Journal – <a href="https://www.wsj.com/articles/SB10001424127887324339204578173252223022388">https://www.wsj.com/articles/SB10001424127887324339204578173252223022388</a></li>
  <li>Ironistic – <a href="https://www.linkedin.com/pulse/cost-distractions-developers-ironistic-com">https://www.linkedin.com/pulse/cost-distractions-developers-ironistic-com</a></li>
  <li>Jazz Hanley – <a href="https://www.linkedin.com/pulse/reclaim-23-minutes-you-lose-every-time-youre-work-jazz-hanley">https://www.linkedin.com/pulse/reclaim-23-minutes-you-lose-every-time-youre-work-jazz-hanley</a></li>
  <li>devmio – <a href="https://devm.io/careers/aaaand-gone-true-cost-interruptions-128741">https://devm.io/careers/aaaand-gone-true-cost-interruptions-128741</a></li>
  <li>Stephanie C. Mitchell – <a href="https://www.stephaniecmitchell.com/articles/whats-distracting-you-from-writing">https://www.stephaniecmitchell.com/articles/whats-distracting-you-from-writing</a></li>
  <li>devbizops – <a href="https://devbizops.medium.com/getting-into-the-developer-flow-state-7b0e5c98eb8a">https://devbizops.medium.com/getting-into-the-developer-flow-state-7b0e5c98eb8a</a></li>
  <li>Hardvard Business Review – <a href="https://hbr.org/2014/04/help-your-employees-find-flow">https://hbr.org/2014/04/help-your-employees-find-flow</a></li>
</ul>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Romhack.ing's Internet Archive Mirror No Longer Available (151 pts)]]></title>
            <link>https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw</link>
            <guid>44998982</guid>
            <pubDate>Sat, 23 Aug 2025 20:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw">https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw</a>, See on <a href="https://news.ycombinator.com/item?id=44998982">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A 2k-year-old sun hat worn by a Roman soldier in Egypt (143 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/</link>
            <guid>44998514</guid>
            <pubDate>Sat, 23 Aug 2025 19:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/">https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/</a>, See on <a href="https://news.ycombinator.com/item?id=44998514">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What makes Claude Code so damn good (341 pts)]]></title>
            <link>https://minusx.ai/blog/decoding-claude-code/</link>
            <guid>44998295</guid>
            <pubDate>Sat, 23 Aug 2025 19:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minusx.ai/blog/decoding-claude-code/">https://minusx.ai/blog/decoding-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44998295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Claude Code is the most delightful AI agent/workflow I have used so far. Not only does it make targeted edits or vibe coding throwaway tools less annoying, using Claude Code makes me happy. It has enough autonomy to do interesting things, while not inducing a jarring loss of control like some other tools do. Of course most of the heavy lifting is done by the new Claude 4 model (especially interleaved thinking). But I find Claude Code objectively less annoying to use compared to Cursor, or Github Copilot agents even with the same underlying model! What makes it so damn good? If you're reading this and nodding along, I'm going to try and provide some answers.</p>
<p><strong>Note</strong>: This is not a blogpost with Claude Code's architecture dump (there are some good ones out there). This blogpost is meant to be a guide for building delightful LLM agents, based on my own experience using and tinkering with Claude Code over the last few months (and all the logs we intercepted and analyzed). You can find <a href="#appendix">prompts</a> and <a href="#appendix">tools</a> in the <a href="#appendix">Appendix section</a>. This post is ~2k words long, so strap in! If you're looking for some quick takeaways, the <a href="#how-to-build-a-claude-code-like-agent-tldr">TL;DR</a> section is a good place to start.</p>

<p><img src="https://minusx.ai/images/claude-code/prompts.png" alt="prompts">
</p><p>You can clearly see the different Claude Code updates.</p>

<p>Claude Code (CC) feels great to use, because it <em>just simply works</em>. CC has been crafted with a fundamental understanding of what the LLM is good at and what it is terrible at. Its prompts and tools cover for the model's stupidity and help it shine in its wheelhouse. The control loop is extremely simple to follow and trivial to debug.</p>
<p>We started using CC at MinusX as soon as it launched. To look under the hood, <a href="https://x.com/ppsreejith_">Sreejith</a> wrote a logger that intercepts and logs every network request made. The following analysis is from my extensive use over the last couple of months. <strong>This post attempts to answer the question - "What makes Claude Code so good, and how can you give a CC-like experience in your own chat-based-LLM agent?"</strong> We've incorporated most of these into MinusX already and I'm excited to see you do it too!</p>

<p><img src="https://minusx.ai/images/claude-code/tools.png" alt="prompts">
</p><p>Edit is the most frequent tool, followed by Read and ToDoWrite</p>
<br>
<h2 id="how-to-build-a-claude-code-like-agent-tldr"><a href="#how-to-build-a-claude-code-like-agent-tldr">How to build a Claude Code like agent: TL;DR</a></h2>
<p>If there is one thing to take away from this, it is this - <strong>Keep Things Simple, Dummy</strong>. LLMs are terrible enough to debug and evaluate. Any additional complexity you introduce (multi-agents, agent handoffs or complex RAG search algorithms) only makes debugging 10x harder. If such a fragile system works at all, you'll be terrified of making drastic changes to it later. So, keep everything in one file, avoid excessive boilerplate scaffolding and rip it all out at least a couple of times :)</p>
<p>Here are the main takeaways from Claude Code to implement in your own system.</p>
<h4 id="1-control-loop"><a href="#1-control-loop">1. Control Loop</a></h4>
<ul>
<li>1.1 <a href="#11-keep-one-main-loop">Keep one main loop (with max one branch) and one message history</a></li>
<li>1.2 <a href="#12-use-a-smaller-model-for-everything">Use a smaller model for all sorts of things. All. The. Frickin. Time.</a></li>
</ul>
<h4 id="2-prompts"><a href="#2-prompts">2. Prompts</a></h4>
<ul>
<li>2.1 <a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">Use claude.md pattern to collaborate on and remember user preferences</a></li>
<li>2.2 <a href="#22-special-xml-tags-markdown-and-lots-of-examples">Use special XML Tags, Markdown, and lots of examples</a></li>
</ul>
<h4 id="3-tools"><a href="#3-tools">3. Tools</a></h4>
<ul>
<li>3.1 <a href="#31-llm-search---rag-based-search">LLM search &gt;&gt;&gt;  RAG based search</a></li>
<li>3.2 <a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">How to design good tools? (High vs Low level tools)</a></li>
<li>3.3 <a href="#33-let-the-agent-manage-a-todo-list">Let your agent manage its own todo list</a></li>
</ul>
<h4 id="4-steerability"><a href="#4-steerability">4. Steerability</a></h4>
<ul>
<li>4.1 <a href="#41-tone-and-style">Tone and style</a></li>
<li>4.2 <a href="#42-this-is-important-is-still-state-of-the-art">"<strong>PLEASE THIS IS IMPORTANT</strong>" is unfortunately still state of the art</a></li>
<li>4.3 <a href="#43-write-the-algorithm-with-heuristics-and-examples">Write the algorithm, with heuristics and examples</a></li>
</ul>
<br>
<blockquote>
<p>Claude Code choses architectural simplicity at every juncture - one main loop, simple search, simple todolist, etc. Resist the urge to over-engineer, build good harness for the model let it cook! Is this end-to-end self-driving all over again? Bitter lesson much?</p>
</blockquote>
<hr>
<h2 id="1-control-loop-design"><a href="#1-control-loop-design">1. Control Loop Design</a></h2>
<h3 id="11-keep-one-main-loop"><a href="#11-keep-one-main-loop">1.1 Keep One Main Loop</a></h3>
<p>Debuggability &gt;&gt;&gt; complicated hand-tuned multi-agent lang-chain-graph-node mishmash.</p>
<p>Despite multi agent systems being all the rage, Claude Code has just one main thread. It uses a few different types of prompts periodically to summarize the git history, to clobber up the message history into one message or to come up with some fun UX elements. But apart from that, it maintains a flat list of messages. An interesting way it handles hierarchical tasks is by spawning itself as a sub-agent without the ability to spawn more sub-agents. There is a maximum of one branch, the result of which is added to the main message history as a "tool response".</p>
<p>If the problem is simple enough, the main loop just handles it via iterative tool calling. But if there are one or more tasks that are complex, the main agent creates clones of itself. The combination of the max-1-branch and the todo list makes sure the agent has the ability to break the problem into sub-problems, but also keep the eye on the final desired outcome.</p>
<p>I highly doubt your app needs a multi-agent system. With every layer of abstraction you make your system harder to debug, and more importantly you deviate from the general-model-improvement trajectory.</p>
<p><img src="https://minusx.ai/images/claude-code/control_loop.gif" alt="Control Loop"></p>
<h3 id="12-use-a-smaller-model-for-everything"><a href="#12-use-a-smaller-model-for-everything">1.2 Use a Smaller model for <em>everything</em></a></h3>
<p>Over 50% of all important LLM calls made by CC are to claude-3-5-haiku. It is used to read large files, parse web pages, process git history and summarize long conversations. It is also used to come up with the one-word processing label - literally for every key stroke! The smaller models are 70-80% cheaper than the standard ones (Sonnet 4, GPT-4.1). Use them liberally!</p>
<h2 id="2-prompts-1"><a href="#2-prompts-1">2. Prompts</a></h2>
<p>Claude Code has extremely elaborate prompts filled with heuristics, examples and IMPORTANT (tch-tch) reminders. The system prompt is ~2800 tokens long, with the Tools taking up a whopping 9400 tokens. The user prompt always contains the claude.md file, which can typically be another 1000-2000 tokens. The system prompt contains sections on tone, style, proactiveness, task management, tool usage policy and doing tasks. It also contains the date, current working directory, platform and OS information and recent commits.</p>
<p><a href="#appendix"><strong>Go read the entire prompt</strong></a>!</p>
<h3 id="21-use-claudemd-for-collaborating-on-user-context-and-preferences"><a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">2.1 Use claude.md for collaborating on user context and preferences</a></h3>
<p>One of the major patterns most coding agent creators have settled on is the context file (aka Cursor Rules / claude.md / agent.md). The difference in Claude Code's performance with and without claude.md is night and day. It is a great way for the developers to impart context that cannot be inferred from the codebase and to codify all strict preferences. For example, you can force the LLM to skip some folders, or use specific libraries. CC sends the entire contents of the claude.md with every user request</p>
<p>We recently introduced <a href="https://minusx.ai/blog/memory/">minusx.md in MinusX</a> which is fast becoming the de-facto context file for our agents to codify user and team preferences.</p>
<h3 id="22-special-xml-tags-markdown-and-lots-of-examples"><a href="#22-special-xml-tags-markdown-and-lots-of-examples">2.2 Special XML Tags, Markdown, and lots of examples</a></h3>
<p>It is fairly established that XML tags and Markdown are two ways to structure a prompt. CC uses both, extensively. Here are a few notable XML tags in Claude Code:</p>
<ul>
<li><code>&lt;system-reminder&gt;</code>: This is used at the end of many prompt sections to remind the LLM of thing it presumably otherwise forgets. Example:</li>
</ul>
<pre><code>&lt;system-reminder&gt;This is a reminder that your todo list is currently empty. DO NOT mention this to the user explicitly because they are already aware. If you are working on tasks that would benefit from a todo list please use the TodoWrite tool to create one. If not, please feel free to ignore. Again do not mention this message to the user.&lt;/system-reminder&gt;
</code></pre>
<ul>
<li><code>&lt;good-example&gt;</code>, <code>&lt;bad-example&gt;</code>: These are used to codify heuristics. They can be especially useful when there is a fork in the road with multiple seemingly reasonable paths/tool_calls the model can choose. Examples can be used to contrast the cases and make it very clear which path is preferable. Example:</li>
</ul>
<pre><code>Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of `cd`. You may use `cd` if the User explicitly requests it.
&lt;good-example&gt;
pytest /foo/bar/tests  
&lt;/good-example&gt;
&lt;bad-example&gt;
cd /foo/bar &amp;&amp; pytest tests
&lt;/bad-example&gt;
</code></pre>
<p>CC also uses markdown to demarcate clear sections in the system prompt. Example markdown headings include:</p>
<ul>
<li>Tone and style</li>
<li>Proactiveness</li>
<li>Following conventions</li>
<li>Code style</li>
<li>Task Management</li>
<li>Tool use policy</li>
<li>Doing Tasks</li>
<li>Tools</li>
</ul>
<h2 id="3-tools-1"><a href="#3-tools-1">3. Tools</a></h2>
<p><a href="#appendix"><strong>Go read the entire tools prompt</strong></a> - it is a whopping 9400 tokens long!</p>
<h3 id="31-llm-search---rag-based-search"><a href="#31-llm-search---rag-based-search">3.1 LLM search &gt;&gt;&gt;  RAG based search</a></h3>
<p>One significant way in which CC deviates from other popular coding agents is in its rejection of RAG. Claude Code searches your code base just as you would, with really complex <code>ripgrep</code>, <code>jq</code> and <code>find</code> commands. Since the LLM understands code really well, it can use sophisticated regex to find pretty much any codeblock it deems relevant. Sometimes it ends up reading whole files with a smaller model.</p>
<p>RAG sounds like a good idea in theory, but it introduces new (and more importantly, hidden) failure modes. What is the similarity function to use? What reranker? How do you chunk the code? What do you do with large JSON or log files? With LLM Search, it just looks at 10 lines of the json file to understand its structure. If it wants, it looks at 10 more lines - just like you would. Most importantly, this is RL learnable - something BigLabs are already working on. The model does most of the heavy lifting - as it should, dramatically reducing the number of moving parts in the agent. Also, having two complicated, intelligent systems wired this way is just ugly. I was recently kidding with a friend saying this is the Camera vs Lidar of the LLM era and I'm only half joking.</p>
<h3 id="32-how-to-design-good-tools-low-level-vs-high-level-tools"><a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">3.2 How to design good tools? (Low level vs High level tools)</a></h3>
<p>This question keeps anyone who is building an LLM agent up at night. Should you give the model generic tasks (like meaningful actions) or should it be low level (like type and click and bash)? The answer is that it depends (and you should use both).</p>
<p>Claude Code has low level (Bash, Read, Write), medium level (Edit, Grep, Glob) and high level tools (Task, WebFetch, exit_plan_mode). CC can use bash, so why give a separate Grep tool? The real trade-off here is in how often you expect your agent to use the tool vs accuracy of the agent in using the tool. CC uses grep and glob so frequently that it makes sense to make separate tools out of them, but at the same time, it can also write generic bash commands for special scenarios.</p>
<p>Similarly, there are even higher level tools like WebFetch or 'mcp__ide__getDiagnostics' that are extremely deterministic in what they do. This saves the LLM from having to do multiple low level clicking and typing and keeps it on track. Help the poor model out, will ya!? Tool descriptions have elaborate prompts with plenty of examples. The system prompt has information about ‘when to use a tool' or how to choose between two tools that can do the same task.</p>
<p><strong>Tools in Claude Code:</strong></p>
<div><div><ul>
<li><a href="#appendix">Task</a></li>
<li><a href="#appendix">Bash</a></li>
<li><a href="#appendix">Glob</a></li>
<li><a href="#appendix">Grep</a></li>
<li><a href="#appendix">LS</a></li>
<li><a href="#appendix">ExitPlanMode</a></li>
<li><a href="#appendix">Read</a></li>
<li><a href="#">Edit</a></li>
</ul></div><div><ul>
<li><a href="#appendix">MultiEdit</a></li>
<li><a href="#appendix">Write</a></li>
<li><a href="#appendix">NotebookEdit</a></li>
<li><a href="#appendix">WebFetch</a></li>
<li><a href="#appendix">TodoWrite</a></li>
<li><a href="#appendix">WebSearch</a></li>
<li><a href="#">mcp__ide__getDiagnostics</a></li>
<li><a href="#">mcp__ide__executeCode</a></li>
</ul></div></div>
<h3 id="33-let-the-agent-manage-a-todo-list"><a href="#33-let-the-agent-manage-a-todo-list">3.3 Let the agent manage a todo list</a></h3>
<p>There are many reasons why this is a good idea. Context rot is a common problem in long-running LLM agents. They enthusiastically start out tackling a difficult problem, but over time lose their way and devolve into garbage. There are a few ways current agent designs tackle this.
Many agents have experimented with explicit todos (one model generates todos, another model implements them) or with Multi-agent handoff + verification (PRD/PM agent -&gt; implementer agent -&gt; QA agent)</p>
<p>We already know multi-agent handoff is not a good idea, for many many reasons. CC uses an explicit todo list, but one that the model maintains. This keeps the LLM on track (it has been heavily prompted to refer to the todo list frequently), while at the same time giving the model the flexibility to course correct mid-way in an implementation. This also effectively leverages the model's interleaved thinking abilities to either reject or insert new todo items on the fly.</p>
<h2 id="4-steerability-1"><a href="#4-steerability-1">4. Steerability</a></h2>
<h3 id="41-tone-and-style"><a href="#41-tone-and-style">4.1 Tone and Style</a></h3>
<p>CC explicitly attempts to control the aesthetic behavior of the agent. There are sections in the system prompt around tone, style and proactiveness - full of instructions and examples. This is  why Claude Code “feels” tasteful in its comments and eagerness. I recommend just copying large sections of this into your app as is.</p>
<pre><code># Some examples of tone and style
- IMPORTANT: You should NOT answer with unnecessary preamble or postamble (such as explaining your code or summarizing your action), unless the user asks you to.
Do not add additional code explanation summary unless requested by the user.

- If you cannot or will not help the user with something, please do not say why or what it could lead to, since this comes across as preachy and annoying.

- Only use emojis if the user explicitly requests it. Avoid using emojis in all communication unless asked.
</code></pre>
<h3 id="42-this-is-important-is-still-state-of-the-art"><a href="#42-this-is-important-is-still-state-of-the-art">4.2 "THIS IS IMPORTANT" is still State of the Art</a></h3>
<p>Unfortunately CC is no better when it comes to asking the model to not do something. IMPORTANT, VERY IMPORTANT, NEVER and ALWAYS seem to be the best way to steer the model away from landmines. I expect the models to get more steerable in the future and avoid this ugliness. But for now, CC uses this liberally, and so should you. Some examples:</p>
<pre><code>- IMPORTANT: DO NOT ADD ***ANY*** COMMENTS unless asked

- VERY IMPORTANT: You MUST avoid using search commands like `find` and `grep`. Instead use Grep, Glob, or Task to search. You MUST avoid read tools like `cat`, `head`, `tail`, and `ls`, and use Read and LS to read files.\n  - If you _still_ need to run `grep`, STOP. ALWAYS USE ripgrep at `rg` first

- IMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the URLs are for helping the user with programming. You may use URLs provided by the user in their messages or local files.

</code></pre>
<h3 id="43-write-the-algorithm-with-heuristics-and-examples"><a href="#43-write-the-algorithm-with-heuristics-and-examples">4.3 Write the Algorithm (with heuristics and examples)</a></h3>
<p>It is extremely important to identify the most important task the LLM needs to perform and write out the algorithm for it. Try to role-play as the LLM and work through examples, identify all the decision points and write them explicitly. It helps if this is in the form of a flow-chart. This helps structure the decision making and aids the LLM in following instructions. One thing to definitely avoid is a big soup of Dos and Don'ts. They are harder to keep track, and keep mutually exclusive. If your prompt is several thousand tokens long, you will inadvertently have conflicting Dos and Don'ts. The LLM becomes extremely fragile in this case and it becomes impossible to incorporate new use cases.</p>
<p><code>Task Management</code>, <code>Doing Tasks</code> and <code>Tool Usage Policy</code> sections in Claude Code's system prompt clearly walk through the algorithm to follow. This is also the section to add lots of heuristics and examples of various scenarios the LLM might encounter.</p>
<h2 id="bonus-why-pay-attention-to-biglab-prompts"><a href="#bonus-why-pay-attention-to-biglab-prompts">Bonus: Why pay attention to BigLab prompts?</a></h2>
<p>A lot of the effort in steering LLMs is trying to reverse engineer their post-training / RLHF data distribution. Should you use JSON or XML? Should the tool descriptions be in the system prompt or just in tools? What about your app's current state? It helps to see what they do in their own apps and use it to inform yours. Claude Code design is very opinionated and it helps to use that in forming your own.</p>
<br>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>The main takeaway, again, is to keep things simple. Extreme scaffolding frameworks will hurt more than help you. Claude Code really made me believe that an "agent" can be simple and yet extremely powerful. We've incorporated a bunch of these lessons into MinusX, and are continuing to incorporate more.</p>
<p>If you're interested in Claude-Codifying your own LLM agent, I'd love to chat - ping me on <a href="https://x.com/nuwandavek">twitter</a>! If you want trainable Claude Code like data agents for your Metabase, check out <a href="https://minusx.com/">MinusX</a> or set up a demo with me <a href="https://minusx.com/demo">here</a>. Happy (Claude) Coding!</p>

<br>
<hr>
<br>
<h2 id="appendix"><a href="#appendix">Appendix</a></h2>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFK Jr demanded a vaccine study be retracted – the journal said no (112 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-02682-9</link>
            <guid>44997435</guid>
            <pubDate>Sat, 23 Aug 2025 17:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-02682-9">https://www.nature.com/articles/d41586-025-02682-9</a>, See on <a href="https://news.ycombinator.com/item?id=44997435">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="access-teaser"> <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-02682-9/d41586-025-02682-9_51368256.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-02682-9/d41586-025-02682-9_51368256.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="HHS Secretary Robert F. Kennedy Jr. gestures while speaking into microphones at a podium during a press conference." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-02682-9/d41586-025-02682-9_51368256.jpg"><figcaption><p><span>Robert F. Kennedy Jr wants a study about vaccines retracted.</span><span>Credit: Tom Williams/CQ Roll Call/Sipa US via Alamy</span></p></figcaption></picture></figure><p>US health secretary and vaccine sceptic Robert F. Kennedy Jr has called for the retraction of a Danish study that found no link between aluminium in vaccines and chronic diseases in children — a rare move for a US public official. Aluminium has been used for almost a century to enhance the immune system’s response to some vaccines. But some people claim the ingredient is linked to rising rates of childhood disorders such as autism.</p><p>Public-health officials in Kennedy’s position rarely request that studies be retracted, says Ivan Oransky, a specialist in academic publishing and co-founder of the media organization Retraction Watch. Through this request, “Secretary Kennedy has demonstrated that he wants the scientific literature to bend to his will”, says Oransky.</p><p>The study<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> in question, published in <i>Annals of Internal Medicine</i> in July, is one of the largest of its kind, looking at 1.2 million children born over more than two decades in Denmark. The authors reported that no significant risk of developing autoimmune, allergic or neurodevelopmental disorders was associated with exposure to aluminium compounds in vaccines. </p><p>In an <a href="https://www.trialsitenews.com/a/flawed-science-bought-conclusions-the-aluminum-vaccine-study-the-media-wont-question-aaec2793" data-track="click" data-label="https://www.trialsitenews.com/a/flawed-science-bought-conclusions-the-aluminum-vaccine-study-the-media-wont-question-aaec2793" data-track-category="body text link">opinion piece</a> published on TrialSite News on 1 August, Kennedy called into question the study’s methodology, analysis and results. Since his appointment as head of the US Department of Health and Human Services, Kennedy has <a href="https://www.nature.com/articles/d41586-025-01852-z" data-track="click" data-label="https://www.nature.com/articles/d41586-025-01852-z" data-track-category="body text link">bypassed normal scientific review processes</a> to change vaccine recommendations and <a href="https://www.nature.com/articles/d41586-025-02612-9" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02612-9" data-track-category="body text link">terminated grants</a> for projects on mRNA vaccines.</p><p><i>Annals of Internal Medicine</i> says it stands by the study and has no plans to retract it. Christine Laine, editor in chief for the journal, wrote in a comment on the study’s web page on 11 August that “retraction is warranted only when serious errors invalidate findings or there is documented scientific misconduct, neither of which occurred here”.</p><p>The Department of Health and Human Services said that Kennedy’s article spoke for itself, and that the department did not have any further comment in response to <i>Nature</i>’s questions about Kennedy's request for a retraction.</p><h2>Widely used</h2><p>Aluminium, in the form of salts, such as potassium aluminium sulfate, have been administered in vaccines — for diseases ranging from whooping cough to pneumonia — to millions of people worldwide, and the vaccines have been widely studied for safety issues<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup><sup>,</sup><sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>. Gary Grohmann, an independent virologist in Canberra, says there is no evidence of significant side effects caused by the small amount of aluminium in vaccines.</p><p>But in 2011, a study<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> published in the <i>Journal of Inorganic Biochemistry</i> claimed to show a causal relationship between rising autism diagnoses in children and increased exposure to aluminium-containing vaccines. In 2012, the World Health Organization’s Global Advisory Committee on Vaccine Safety said the study and another by the same authors were “<a href="https://www.who.int/groups/global-advisory-committee-on-vaccine-safety/topics/adjuvants" data-track="click" data-label="https://www.who.int/groups/global-advisory-committee-on-vaccine-safety/topics/adjuvants" data-track-category="body text link">seriously flawed</a>” because they used inappropriate study designs, incorrect assumptions and questionable data.</p><p>Since then, Grohmann says, the claim that aluminium in vaccines causes autism has been debunked “again and again”. “If there was a mechanism of action where a particular vaccine caused autism, we’d see it in 80, 90, 100% of people receiving the vaccine, and we don’t,” he says. Any association between autism and vaccines is probably a coincidence of timing, he says. “In other words, vaccines might be given at the age of two, and autism genetically might also kick in at the age of two,” he adds.</p><p>Allen Cheng, an epidemiologist at Monash University in Melbourne, Australia, says the Danish study adds to the evidence that vaccines containing aluminium are safe.</p><h2>Kennedy’s concerns</h2><p>Among Kennedy’s criticisms of the Danish study are that the analysis excluded children who had died before the age of two. According to Kennedy, this means that the children “most likely to reveal injuries” associated with aluminum exposure were excluded.</p><p>Kennedy also criticized the fact that the authors did not compare vaccinated and unvaccinated children to determine whether any aluminium exposure causes harm, even though they had some data on unvaccinated children.</p><p>Other critiques posted on the journal website overlapped with Kennedy’s criticisms, says Anders Hviid, the senior author and an epidemiologist at the Statens Serum Institut in Copenhagen, Denmark’s public-health agency. Hviid says he and his colleagues addressed the critiques “one by one”. He also published a <a href="https://www.trialsitenews.com/a/data-vs.-doubt-danish-scientist-responds-to-u.s.-hhs-secretary-critique-of-aluminum-vaccine-study-290120e9" data-track="click" data-label="https://www.trialsitenews.com/a/data-vs.-doubt-danish-scientist-responds-to-u.s.-hhs-secretary-critique-of-aluminum-vaccine-study-290120e9" data-track-category="body text link">rebuttal</a> of Kennedy’s article on TrialSite News on 3 August.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I hacked Monster Energy (182 pts)]]></title>
            <link>https://bobdahacker.com/blog/monster-energy</link>
            <guid>44997145</guid>
            <pubDate>Sat, 23 Aug 2025 16:42:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bobdahacker.com/blog/monster-energy">https://bobdahacker.com/blog/monster-energy</a>, See on <a href="https://news.ycombinator.com/item?id=44997145">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <div>
                <h2>The Energy Drink Giant That Forgot to Lock Its Doors</h2>
<p>As a hacker who likes energy drinks, I decided to check out Monster Energy's corporate infrastructure. What I found was completely exposed and making terrible security decisions.</p>
<h3>Monster University: Where Security Goes to Die</h3>
<p>Monster University (<code>mu.monsterenergy.com</code>) is where Monster employees go to learn about their brand. It's also where I learned that changing <code>/login</code> to <code>/register</code> in the URL is apparently Monster's idea of "authentication."</p>
<p>The registration form appeared but wouldn't submit. So I went straight to the JavaScript to find the actual API endpoint. The API helpfully told me exactly which fields were missing from my registration attempt.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/register_api.png" alt="Monster University Registration API"></p>
<p>Once I called the API directly with the right fields, boom, I was in. Full access to Monster University, complete with all their training materials, including this absolute masterpiece about their target demographic:</p>
<h3>This Is What Monster Thinks You Look Like</h3>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/consumer_demographics.jpg" alt="Monster's " consumer"="" demographics"=""></p>
<p>I'm not kidding. This is from their actual brand training guide. According to Monster Energy, their "Core Brand Family Consumer" is:</p>
<p><em>"Monster Green shoppers are likely younger (Gen-Z/Millennial/Gen-X) male, lower income &amp; Caucasian (skews Hispanic)."</em></p>
<p>And they included this photo of five people in Monster gear looking like they're being held hostage in a marketing photoshoot. This is literally what Monster corporate thinks their average customer looks like. I can't make this stuff up.</p>
<h3>The Irony: Their Own Cybersecurity Training</h3>
<p>The best part? Monster University has a cybersecurity course they bought from a third-party vendor. The same platform with no authentication has lessons teaching employees about phishing and basic security:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/cybersecurity_course.png" alt="Monster Cybersecurity Course"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/gamelist.png" alt="Monster Cybersecurity Game List"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/amplify_game.png" alt="Amplify Cybersecurity Game"></p>
<p>The irony of hosting a cybersecurity course about phishing on a completely unsecured platform is just <em>chef's kiss</em>.</p>
<h3>Meanwhile in Monster Corporate: Walmart Zoom Calls and "ULTIMATE BEAST" Badges</h3>
<p>While exploring Monster University, I found some gems about their corporate culture. You can view their entire Zoom meeting schedule and even get the join links:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/walmart_meeting.png" alt="Walmart Office Hours Event"></p>
<p>And check out their employee achievement system - you can earn badges for everything from "BEAST" to "ULTIMATE BEAST":</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/monster_badges.png" alt="Monster Achievement Badges"></p>
<h3>Beast Bux: Monster's Internal Currency System</h3>
<p>But wait, it gets better. I found their internal employee rewards system called "Beast Bux." Here's their actual training video explaining it:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/video_section.png" alt="Monster University Video Section"></p>

<p>Essentially, employees get Beast Bux yearly and can give them to other employees to show appreciation. These can be used to buy Monster merch at their internal store: <a target="_blank" rel="noopener noreferrer" href="https://bleedgreenshop.monsterenergy.com/">https://bleedgreenshop.monsterenergy.com/</a></p>
<h3>The Real Treasure: Exposed Corporate File System</h3>
<p>The scariest part wasn't the training portal or the questionable customer profiling. It was finding their OpenText API completely exposed with no authentication required:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/search/?page=1&amp;pageSize=1000&amp;searchTerms=
</code></pre>
<p>This endpoint allows anyone to search through Monster's entire file system. No password. No authentication. Nothing.</p>
<p>Want to see internal contracts? Sure, here's one I found:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/images/7e02a7602d8cee4aaf5b999850c243df9d0a184b
</code></pre>
<p>(Don't let the "images" in the URL fool you - it serves all file types, not just images)</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/contract.png" alt="Monster Contract Document"></p>
<p>The API returns full document metadata, file paths, and direct download links for everything in their system. Contracts, internal documents, you name it.</p>
<h3>Even Worse: The ClickUp Integration Disaster</h3>
<p>On a subdomain called Kermometer (<code>kermometer.monsterenergy.com</code>), I discovered Monster had integrated ClickUp into their workflow, but they made a critical mistake: they exposed an admin's private account token directly in their website's JavaScript.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_token.png" alt="ClickUp Token Exposed in JavaScript"></p>
<p>This token would allow anyone to:</p>
<ul>
<li>Access their entire ClickUp workspace</li>
<li>View all private documents and projects</li>
<li>Invite themselves to the workspace</li>
<li>Potentially modify or delete critical project data</li>
</ul>
<p>To prove it worked, I invited myself to their workspace:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_invite.png" alt="ClickUp Workspace Invitation"></p>
<p>For the lulz, I wrote a script to share everything with myself using their admin token:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/sharing_script.png" alt="Script Sharing Everything With Me"></p>
<p>Don't worry, I left their workspace immediately after proving the vulnerability. I'm not trying to steal Monster's secret energy drink formulas or anything.</p>
<h3>The Response (Or Lack Thereof)</h3>
<p>I tried contacting Monster Energy directly about these vulnerabilities. No response.</p>
<p>They did fix the Monster University registration issue, but I don't think they even read my emails - they probably just noticed someone had signed up through their broken system and patched it.</p>
<p>Finally, I told ClickUp themselves about the exposed token on Monster's site. They investigated and contacted Monster, getting it fixed in less than a week. </p>
<p>But Monster? They never even acknowledged any of my reports. And as you can see, they left their entire file system API wide open.</p>
<p><strong>The OpenText API is STILL ACTIVE as of writing this post.</strong></p>
<h3>To Monster Energy</h3>
<p>Your energy drinks might "Unleash the Beast," but your security is definitely asleep.</p>
<p>Maybe spend less time creating stereotypical customer profiles and more time securing your infrastructure? Just a thought.</p>
<p>Also, a security contact email would be nice. You know, for next time.</p>
<hr>

            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Line scan camera image processing for train photography (353 pts)]]></title>
            <link>https://daniel.lawrence.lu/blog/y2025m09d21/</link>
            <guid>44996938</guid>
            <pubDate>Sat, 23 Aug 2025 16:09:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.lawrence.lu/blog/y2025m09d21/">https://daniel.lawrence.lu/blog/y2025m09d21/</a>, See on <a href="https://news.ycombinator.com/item?id=44996938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><div><ol><li><a href="#s1"><span>1</span> <span>Principle of operation</span></a></li><li><a href="#s2"><span>2</span> <span>About the camera</span></a></li><li><a href="#s3"><span>3</span> <span>Detecting the region of interest</span></a></li><li><a href="#s4"><span>4</span> <span>Speed estimation</span></a></li><li><a href="#s5"><span>5</span> <span>Resampling</span></a></li><li><a href="#s6"><span>6</span> <span>Demosaicing</span></a></li><li><a href="#s7"><span>7</span> <span>Getting rid of the vertical stripes</span></a></li><li><a href="#s8"><span>8</span> <span>Denoising</span></a></li><li><a href="#s9"><span>9</span> <span>Skew correction</span></a></li><li><a href="#s10"><span>10</span> <span>Color calibration</span></a></li><li><a href="#s11"><span>11</span> <span>Implementation details</span></a><ol><li><a href="#s11.1"><span>11.1</span> <span>Vibe coding experience</span></a></li></ol></li><li><a href="#s12"><span>12</span> <span>Other people’s line scan photography of trains</span></a><ol><li><a href="#s12.1"><span>12.1</span> <span>Adam Magyar</span></a></li><li><a href="#s12.2"><span>12.2</span> <span>KR64’s blog</span></a></li></ol></li></ol></div></header><p>I use my line scan camera to take cool pictures of trains and other stuff.</p><p>But there’s a lot that goes into properly processing the images.</p><div><figure id="fig1"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_a8414d74520f4baa.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_450228d70b868eb2.jpg" alt=""></a><figcaption><a href="#fig1">FIGURE 1</a> A cool tram.</figcaption></figure></div><div><figure id="fig2"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_0d9fee240b0c6e5f.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_5df7185ac60d16b4.jpg" alt=""></a><figcaption><a href="#fig2">FIGURE 2</a> A cool train, the Renfe AVE Class 102, nicknamed <em>Pato</em> because of its duck bill-like appearance.</figcaption></figure></div><div><figure id="fig3"><a href="https://i.dllu.net/rgb_9_prod_no_denoise_20704300acae5f1e.jpg"><img src="https://i.dllu.net/rgb_9_prod_no_denoise_16c81b9b2f43f4bb.jpg" alt=""></a><figcaption><a href="#fig3">FIGURE 3</a> Cool diesel locomotive.</figcaption></figure></div><div><figure id="fig4"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_dd93f40ada264e00.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_7e6bd0d3e527a2d0.jpg" alt=""></a><figcaption><a href="#fig4">FIGURE 4</a> Nice CRH6A intercity electric multiple unit.</figcaption></figure></div><div><figure id="fig5"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_685eacc3349ba19a.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_b45d3300018b8368.jpg" alt=""></a><figcaption><a href="#fig5">FIGURE 5</a> Awesome CR400AF. Super fast.</figcaption></figure></div><h2 id="s1"><a href="#s1">1</a> <span>Principle of operation</span></h2><p>The way it works is that the camera has a single column of pixels (or in this case, two columns), that scans at a super high speed.
The camaera is stationary, but as a train moves past it, it gets scanned.</p><p>This is essentially also how a <a href="https://en.wikipedia.org/wiki/Photo_finish">photo finish camera</a> works.</p><p>Since the background is static, it gets repeated for every column of the image, giving it its distinctive striped look.</p><p>Line scan cameras are very suitable for capturing trains, since I can capture the full length of the train with minimal perspective distortion.
This is super nice for train nerds who want to make models of the trains.
Also, as you keep the camera running, you can get incredibly high resolution photos that span over 100,000 pixels wide.</p><p>By the way, film photo finish cameras and strip cameras behave almost the same as line scan cameras but with one subtle distinction, which is that you have to pull the film across a strip that’s somewhat wider than a single column of pixels.
This is because film is less sensitive than modern digital image sensors.
However, you’ll need to know the approximate speed of the subject and pull the film across at roughly the right speed.</p><h2 id="s2"><a href="#s2">2</a> <span>About the camera</span></h2><p>I’m using an <a href="https://www.alkeria.com/products/necta-series">Alkeria Necta N4K2-7C</a>.
It has a 4096×2 <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a> image sensor.
I’m saving its raw data in 16 bit binary arrays.</p><div><figure id="fig8"><a href="https://i.dllu.net/2024-09-12-18-31-00_DSCF0249_d644edd144f26bdabd7a876eda224c2a673ad9ec_d6e13378a10243e5.jpg"><img src="https://i.dllu.net/2024-09-12-18-31-00_DSCF0249_d644edd144f26bdabd7a876eda224c2a673ad9ec_7281c09603aff005.jpg" alt=""></a><figcaption><a href="#fig8">FIGURE 8</a> My camera.</figcaption></figure></div><iframe width="320" height="400" src="https://www.youtube.com/embed/r-GHYwkQD1o" title="line scan photography of Shanghai Transrapid" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><div><figure id="fig9"><a href="https://i.dllu.net/IMG_6801_654e71a21ee44795.jpg"><img src="https://i.dllu.net/IMG_6801_9c68db912482ad71.jpg" alt=""></a><figcaption><a href="#fig9">FIGURE 9</a> Waiting for a subway train to roll by in Brooklyn, New York.</figcaption></figure></div><h2 id="s3"><a href="#s3">3</a> <span>Detecting the region of interest</span></h2><p>Sometimes, I keep the line scan camera running for a while, and it generates tons of boring data of the background.
To detect moving things, I compute an “energy function” that’s defined as</p><p>where <img src="https://daniel.lawrence.lu/texcache/4ba7d3cff9565fbc8341f413e7711422cb61cc27i.svg" alt="\text{max}_\mathbf{I}"> is the maximum pixel value of the image, and the partial derivative are the <a href="https://en.wikipedia.org/wiki/Image_gradient">image gradient</a>.</p><div><figure id="fig10"><a href="https://i.dllu.net/score_sample_91b1560a90ba63f2.png"><img src="https://i.dllu.net/score_sample_8c65dd9f93d6c9aa.png" alt=""></a><figcaption><a href="#fig10">FIGURE 10</a> Example energy image.</figcaption></figure></div><p>This is because, for a static background, it will be full of horizontal stripes.
By weighing the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-direction (time direction) gradient against the total gradient norm, we can find areas where it’s a more vertical-ish structure rather than a horizontal structure.
However, doing this by itself risks noisy gradients in empty (but noisy) areas where the gradient direction is completely random.
The maximum pixel value term ensures that whatever gradient we see is salient.</p><p>The image is divided into chunks and the score of a chunk is the 99th percentile energy.</p><p>Finally, chunks containing moving objects are defined to be ones where the score is at least 1.5× that of the minimum score.</p><p>This heuristic took me longer than I would like to admit to figure out.
Previously, I came up with heuristics that worked well on one capture but couldn’t generalize well to other captures.
Sometimes, the background will contain slowly moving foliage waving in the wind, that would screw up other methods of detection.
That resulted in a lot of wasted time because time spent processing empty regions seriously slows down iteration speed when developing the later steps.</p><h2 id="s4"><a href="#s4">4</a> <span>Speed estimation</span></h2><p>The most common question I get is, how do I estimate the speed of the subject?
If I don’t do it properly, it will appear stretched out, squished, or flipped.</p><p>Typically, I just set the camera to scan as fast as possible while maintaining a decent exposure, so the scan rate is independent of the subject.
Faster subjects will appear squished, and slower subjects will appear stretched out.</p><p>For most of my earlier works, I just eyeballed it. A good rule of thumb is to look for round things such as the wheels and “no smoking” signs.
But now I have a fully automated technique that works fairly robustly.</p><p>The key idea is to exploit the fact that the line scan camera actually has two lines in a <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a>, where one line is red, green, red, green, and the second line is green, blue, green blue.
By comparing the two green channels, we are able to figure out how fast stuff is moving.</p><p>The problem is that the data is very noisy, and salient features are sparse.
Here’s the general approach:</p><ul><li>Divide image into chunks.</li><li>Compute the absolute difference between the 2 green channels of each chunk for various small shifts (from -7 to +7). This gives us a cost array for each chunk.</li><li>Perform subpixel peak interpolation in the cost array using an iteratively reweighted Gaussian, <a href="https://en.wikipedia.org/wiki/Mean_shift">mean shift</a> style. This gives us a shift estimate per chunk.</li><li>Fit a robust spline to the shift estimates.</li></ul><div><figure id="fig11"><a href="https://i.dllu.net/mean_shift_0080_e3d219d77b34d22e.png"><img src="https://i.dllu.net/mean_shift_0080_c375801a598f46a4.png" alt=""></a><figcaption><a href="#fig11">FIGURE 11</a> Interpolating to find the peak using mean shift.</figcaption></figure></div><div><figure id="fig12"><a href="https://i.dllu.net/spline_8457468cc8c5d06a.png"><img src="https://i.dllu.net/spline_49ca48ada37ab8d2.png" alt=""></a><figcaption><a href="#fig12">FIGURE 12</a> Sample spacing and fitted spline.</figcaption></figure></div><p>As you can see, the data is noisy, but we have surprisingly decent granularity for this very subpixel case where we were scanning slower than needed so the spacing is like 0.5.</p><p>The value of the spline is actually the <em>sample spacing</em>. It tells us how close together or far apart the sample points in the original time series we should be using.
This leads us to the next section.</p><div><figure id="fig13"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_b83579bd26d10381.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_2e790d956d6045d7.jpg" alt=""></a><figcaption><a href="#fig13">FIGURE 13</a> Uncorrected left end. It’s squished!!!</figcaption></figure></div><div><figure id="fig14"><a href="https://i.dllu.net/rgb_2_prod_no_denoise_32cd95888d846ffa.jpg"><img src="https://i.dllu.net/rgb_2_prod_no_denoise_aeefb00dbe18bf6f.jpg" alt=""></a><figcaption><a href="#fig14">FIGURE 14</a> Uncorrected right end. It’s slightly squished but not nearly as much.</figcaption></figure></div><div><figure id="fig15"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_e458634bcd3cc936.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_b489258f43a8a411.jpg" alt=""></a><figcaption><a href="#fig15">FIGURE 15</a> Left end of New York subway train.</figcaption></figure></div><div><figure id="fig16"><a href="https://i.dllu.net/rgb_3_prod_no_denoise_fa356ca82a22b333.jpg"><img src="https://i.dllu.net/rgb_3_prod_no_denoise_6c643c2fa36cc924.jpg" alt=""></a><figcaption><a href="#fig16">FIGURE 16</a> Right end of New York subway train.</figcaption></figure></div><p>Hmm, I think my speed estimation still isn’t perfect. It could be off by about 10%.
For future work, I think I might be able to extract features correspondences such as SIFT or LightGlue. Trains are full of repeating elements that are supposed to be evenly spaced. I can detect those, and add a cost function to evenly space them, and optimize.
Another idea is to use a circle Hough transform to find circles.</p><h2 id="s5"><a href="#s5">5</a> <span>Resampling</span></h2><p>From the spline that gives us the sample spacing, we can basically generate the samples as such:</p><div><pre><span></span><span>samples</span> <span>=</span> <span>[]</span>
<span>sample_position</span> <span>=</span> <span>0.0</span>
<span>while</span> <span>sample_position</span> <span>&lt;</span> <span>raw_width</span><span>:</span>
    <span>samples</span><span>.</span><span>append</span><span>(</span><span>sample_position</span><span>)</span>
    <span>sample_position</span> <span>+=</span> <span>spline</span><span>(</span><span>sample_position</span><span>)</span>
</pre></div>
<p>However, there are a few gotchas:</p><ul><li>If the spline is negative-valued, it means the subject is going the other way, i.e. the image is flipped. In this case, I start with <code>sample_position</code> set to <code>raw_width</code> and go from right to left.</li><li>If the spline goes to zero, we are doomed because the while loop will never terminate. I clamp the steps to at least 0.1 and throw an error if the spline has both positive and negative values.</li><li>This is sort of a naive integration compared to the trapezoidal rule or something. However, given that the spline moves very slowly, it is fine.</li></ul><p>Now, for each sample position, we also store the sample width, which is the value of the spline.
If we were to simply extract a single column from the raw data, we would be throwing away a lot of data, and the result wouldn’t be antialiased.
Instead, it is better to pick a window of width proportional to the sample spacing.
I chose a <a href="https://en.wikipedia.org/wiki/Hann_function">Hann window</a>.</p><div><figure id="fig17"><a href="https://i.dllu.net/2025-08-19-12-10-58_b5c58484522b4579.png"><img src="https://i.dllu.net/2025-08-19-12-10-58_26c04e2f3b5d16e1.png" alt=""></a><figcaption><a href="#fig17">FIGURE 17</a> Naively selecting columns instead of using a windowing function.</figcaption></figure></div><div><figure id="fig18"><a href="https://i.dllu.net/2025-08-19-12-11-19_006f733f714d000c.png"><img src="https://i.dllu.net/2025-08-19-12-11-19_1c8c1f1331cf0c62.png" alt=""></a><figcaption><a href="#fig18">FIGURE 18</a> Using a rectangular window.</figcaption></figure></div><p>Not only is the first image very grainy, but the rapidly blinking LED display showing the characters for 筲箕灣 is completely illegible without proper sampling.</p><div><figure id="fig19"><a href="https://i.dllu.net/2025-08-21-23-01-25_b836f51e7b72da06.png"><img src="https://i.dllu.net/2025-08-21-23-01-25_f6d4fd8579a3ee52.png" alt=""></a><figcaption><a href="#fig19">FIGURE 19</a> Upsampling using a rectangular window.</figcaption></figure></div><div><figure id="fig20"><a href="https://i.dllu.net/2025-08-21-23-00-57_cf42ac3a31b6cdf8.png"><img src="https://i.dllu.net/2025-08-21-23-00-57_89de76f62fa2ad89.png" alt=""></a><figcaption><a href="#fig20">FIGURE 20</a> Upsampling using a Hann window.</figcaption></figure></div><p>As you can see, the rectangular window performs very poorly when upsampling and introduces horrible jagged artifacts. The Hann window does better. Some other windows like the Sinc are even better supposedly.</p><h2 id="s6"><a href="#s6">6</a> <span>Demosaicing</span></h2><p>Recall that the camera has two lines forming a Bayer array.</p><p>If we simply create an image of half resolution (i.e. 2048 pixels tall instead of 4096), by grouping each RGGB group into one pixel, we would have some nasty fringing problems since the red and blue pixels are offset.</p><div><figure id="fig21"><a href="https://i.dllu.net/2025-08-19-17-57-15_ea319c5f717e47df.png"><img src="https://i.dllu.net/2025-08-19-17-57-15_35d9b99165d33e0a.png" alt=""></a><figcaption><a href="#fig21">FIGURE 21</a> Fringing due to bad demosaicing.</figcaption></figure></div><div><figure id="fig22"><a href="https://i.dllu.net/2025-08-19-17-57-35_41dc85cc57217678.png"><img src="https://i.dllu.net/2025-08-19-17-57-35_92ba71ba7bbc7152.png" alt=""></a><figcaption><a href="#fig22">FIGURE 22</a> Better.</figcaption></figure></div><p>Instead we should write out the image with careful attention to offsets, interpolating as necessary.
Note that the horizontal offsets must be done <em>after</em> speed estimation, because, before speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-axis is time, and after speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-axis is space.
But the 2-pixel wide Bayer array is physically a <em>spatial</em> offset.</p><p>I implemented a basic interpolation scheme that uses bilinear interpolation.
This fixes most of the fringing, although we can do even better. That will be left for future work.</p><p>Unlike a traditional Bayer array, here we have the possibility that the green channels cover 100% of the pixels, so we can potentially do better than traditional demosaicing algorithms.
But there’s currently an annoying problem, which is that the two green channels on my line scan camera don’t match.</p><h2 id="s7"><a href="#s7">7</a> <span>Getting rid of the vertical stripes</span></h2><p>Vertical stripes in the image are common and are due to two main reasons:</p><ul><li>Clock jitter. The exposure time of each column may be randomly slightly off for some reason.</li><li>I’ve noticed that when a dark object shows up, like the coupling between train cars, the whole slice of the image there is brighter.</li></ul><p>To fix this, I use linear regression to fit a basic model of the form:</p><p>where <img src="https://daniel.lawrence.lu/texcache/86f7e437faa5a7fce15d1ddcb9eaeaea377667b8i.svg" alt="a">, <img src="https://daniel.lawrence.lu/texcache/e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98i.svg" alt="b">, and <img src="https://daniel.lawrence.lu/texcache/84a516841ba77a5b4648de2cd0dfcb30ea46dbb4i.svg" alt="c"> are scalar parameters of the model, <img src="https://daniel.lawrence.lu/texcache/5fafb73566c2a261cb4f5b35b6642b16d8284d3di.svg" alt="\mathbf{x}"> is a 2048-element vector containing the luminance value of the column (mean over the 4 channels), and <img src="https://daniel.lawrence.lu/texcache/ba360d8ea75d540ca5e1b1ce009ddbe6cd0691e4i.svg" alt="\mathbf{k}"> is the row index (aka the 2048-element vector of <img src="https://daniel.lawrence.lu/texcache/6b8d068c8ae69b58d2484e0ad9b0f6576d9bd529i.svg" alt="\begin{bmatrix}0, 1, \cdots, 2047\end{bmatrix}">.</p><p>You can compose models as such:</p><p>This gives us a new model <img src="https://daniel.lawrence.lu/texcache/c1086bb06dd2795dc0b7c7de2b953422fa2bc22di.svg" alt="\text{model}_{12}"> with parameters:</p><p>The associative property of the composition operator is left as an exercise for the reader.</p><p>There is also the identity model, i.e. one that does nothing, which is</p><p>and there’s also the inverse:</p><p>so the set of these models forms a mathematical group.</p><p>I fit a model to each consecutive pair of columns using weighted least squares, where we assign each row element a weight based on a Gaussian.
The weight would be:</p><p>In other words, the residual would be</p><p>After fitting this model, we redo the steps again several times, where the <img src="https://daniel.lawrence.lu/texcache/5d3a9194bf53d4e2741ee6cf67ae186b5d22770ai.svg" alt="\mathbf{w}"> vector is recalculated each time.
This is known as <em>iteratively-reweighted least squares</em> and is pretty good at rejecting outliers.</p><div><figure id="fig23"><a href="https://i.dllu.net/jitter_debug_082121e08533fc28.png"><img src="https://i.dllu.net/jitter_debug_c28f675b057b422f.png" alt=""></a><figcaption><a href="#fig23">FIGURE 23</a> The first plot shows the current column’s luminance and previous column’s luminance, as well as the previous column corrected by the model. The second plot shows the weight. The third plot shows the weighted initial and final error.</figcaption></figure></div><p>This all gives us <em>relative</em> models between the previous column and the current column, but we want <em>global</em>models that tell us how to correct each column overall.
We could set the global models by just composing them forever, but they would soon start to drift arbitrarily far away from the identity model.</p><p>You could prevent them from drifting away by solving a band-diagonal linear system where you have residuals of two types:</p><ul><li>prior residual, penalizing the difference between each model from the identity</li><li>relative model residual, penalizing the difference between the delta between adjacent models and the relative model we computed</li></ul><p>This can be solved in <img src="https://daniel.lawrence.lu/texcache/ebc75cd71fe8ecc45d16e8fbe4ca608d05d1efe0i.svg" alt="O(n)">. However, it is a bit of work to implement. In practice, you can mitigate most high frequency stripes by just doing <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential smoothing</a>, which basically acts as a high-pass filter</p><p>for some small <img src="https://daniel.lawrence.lu/texcache/b3931f1ce298c536432fd324b3a1ab4337120689i.svg" alt="\lambda">, in this case hardcoded to be 0.02.</p><div><figure id="fig24"><a href="https://i.dllu.net/2025-08-21-23-06-05_7f2b75d765c999be.png"><img src="https://i.dllu.net/2025-08-21-23-06-05_c5de9c2fbf369030.png" alt=""></a><figcaption><a href="#fig24">FIGURE 24</a> Before. You can see rather subtle stripes in the dark area.</figcaption></figure></div><div><figure id="fig25"><a href="https://i.dllu.net/2025-08-21-23-06-21_d6e64e01704c96d1.png"><img src="https://i.dllu.net/2025-08-21-23-06-21_a3b1e6747c4f6c1c.png" alt=""></a><figcaption><a href="#fig25">FIGURE 25</a> After.</figcaption></figure></div><p>Previously, I also had some success by directly fitting the model to line up each column with the first column. However, it doesn’t work for captures where the background isn’t static (e.g. rotating line scan panoramas, and pointing the line scan camera out of a moving train).</p><p>By the way, I should point out that vertical stripes getting rid of should be done <em>before</em> speed estimation, since it happens in the time domain at capture time.
If a train were speeding up, it would appear stretched out at first, and squished at the end, and the striping would affect the end a lot more than the start.</p><h2 id="s8"><a href="#s8">8</a> <span>Denoising</span></h2><p>I implemented a patch-based denoiser, also known as <a href="https://en.wikipedia.org/wiki/Block-matching_and_3D_filtering">block matching</a>.
It works by making the observation that you often have repeated textures in a line scan photo of a train.
Technically, you also have lots of self-similarity in general photos, so patch-based denoising is a common method for denoising in general.
However, one important distinction is that most denoisers only look in a small neighborhood around the current patch, but mine looks along the entire row.</p><p>What I do is, for each row, we process it independently.
From each 3×3 pixel patch, we can construct a <em>feature vector</em> of size 27 (9 times 3 channels, RGB).
Then, we collect all these features and sort them by mean value.
Now, for each position along the row, we search in the window of size 128 in the sorted vector.
The sorted vector will have similar-looking patches nearby, but we further weigh them by Gaussian similarity to the current patch.
Then, we compute the weighted average of the center pixel of each of those patches.</p><p>Another trick is to realize that the noise is Poisson-distributed which has a standard deviation that scales with the square root of the signal.
But if I just square root the input data first, then we just need to compare it to a constant.</p><p>This works decently, but is incredibly slow.
Let me know if you think of any faster ways to do it. A KD tree in feature space would die from the curse of dimensionality. Perhaps a hash table? To keep things lightweight, we can limit the population in each cell.</p><div><figure id="fig26"><a href="https://i.dllu.net/2025-08-20-16-08-55_264411aaba797e0c.png"><img src="https://i.dllu.net/2025-08-20-16-08-55_648c53661a3709d9.png" alt=""></a><figcaption><a href="#fig26">FIGURE 26</a> Noisy watch.</figcaption></figure></div><div><figure id="fig27"><a href="https://i.dllu.net/2025-08-20-16-08-30_6c3c0102c46a5803.png"><img src="https://i.dllu.net/2025-08-20-16-08-30_7db8ed848f17bc87.png" alt=""></a><figcaption><a href="#fig27">FIGURE 27</a> Denoised watch.</figcaption></figure></div><div><figure id="fig28"><a href="https://i.dllu.net/2025-08-20-16-09-34_8be193ad950c4151.png"><img src="https://i.dllu.net/2025-08-20-16-09-34_4440c8997d90f81e.png" alt=""></a><figcaption><a href="#fig28">FIGURE 28</a> Noisy passenger.</figcaption></figure></div><div><figure id="fig29"><a href="https://i.dllu.net/2025-08-20-16-09-59_dbce64ce63ce8080.png"><img src="https://i.dllu.net/2025-08-20-16-09-59_49cd78feafeaebf8.png" alt=""></a><figcaption><a href="#fig29">FIGURE 29</a> Denoised passenger.</figcaption></figure></div><p>The good thing about the patch-based denoiser is that unique features like this passenger remain virtually unchanged.</p><p>Previously, I also tried using a <a href="https://en.wikipedia.org/wiki/Total_variation_denoising">total variation denoiser</a>, processing each row and column independently.
It worked decently but would often destroy fine detail in textures.</p><h2 id="s9"><a href="#s9">9</a> <span>Skew correction</span></h2><p>If the camera isn’t perfectly upright, the resulting image may be slightly skewed.
I’m planning on implementing automatic skew correction.
But here are two caveats:</p><ul><li>skew detection must be done after speed estimation</li><li>proper sampling should happen after skew detection, since the skew transformation introduces generation loss and we can sample directly from the raw data instead.</li></ul><p>So basically we’d need to generate a quick, poorly sampled version, run skew detection on it, and then sample it properly afterwards.
We can implement skew detection using a <a href="https://daniel.lawrence.lu/blog/y2025m09d21/Hough%20transform">Hough transform</a>.
Generally, I do a decent job of keeping the camera upright, so we just need to correct for very small skews, so a Hough transform is suitable (since the complexity scales with the number of bins of the histogram).
We can also use the energy function from the region of interest detector to primarily care about vertical structures.</p><h2 id="s10"><a href="#s10">10</a> <span>Color calibration</span></h2><p>I kinda just eyeballed this color calibration matrix.</p><p>But to be honest it looks fairly decent.</p><div><figure id="fig30"><a href="https://i.dllu.net/rgb_7_prod_no_denoise_2474ac2d89335c15.jpg"><img src="https://i.dllu.net/rgb_7_prod_no_denoise_e7ce15b1872fe528.jpg" alt=""></a><figcaption><a href="#fig30">FIGURE 30</a> People’s skin tones look fine to me.</figcaption></figure></div><h2 id="s11"><a href="#s11">11</a> <span>Implementation details</span></h2><p>The code is implemented in Python using numpy.</p><p><a href="https://github.com/dllu/nectar/blob/master/python/preview.py">the code</a></p><p>Due to the large size of the data (4096 rows and hundreds of thousands of columns), it is sometimes impossible to fit all of it in memory, so the code takes several passes and outputs in chunks.
Actually, it is probably okay to fit it in a few gigabytes of RAM, but you’d have to chunk up the storage (there’s no way a contiguous numpy array of 4096 by 100,000 could be allocated).</p><h2 id="s11.1"><a href="#s11.1">11.1</a> <span>Vibe coding experience</span></h2><p>I tried using AI to help with a lot of the implementation. However, the results were mixed.</p><p>AI would often accidentally make things quadratic for no reason when a linear time algorithm would suffice. For example, when trying to implement spline-based resampling, ChatGPT 5 came up with horribly slow (but vectorized) code that constructed a giant tensor with a mask across the entire width of the image for <em>every single sample</em>. Since there are 100,000 samples, and each mask was 100,000 columns wide, you can imagine it would take millennia to run. I ended up reimplementing it from scratch by hand. Then Grok 4 implemented weighted least squares regression by materializing the entire weight vector with <code>np.diag</code> instead of simply pre-multiplying each row of <code>A</code> and <code>x</code> with the square root of the weight before doing <code>np.linalg.solve(A, x)</code>. Again, with 100,000 elements, making the square matrix with <code>np.diag</code> would have instantly run out of memory.</p><p>Both Grok 4 Expert and ChatGPT 5 Thinking also completely failed to implement the band-diagonal least squares to my vertical stripes strategy, but as mentioned, the exponential smoothing trick works okay for now.</p><p>However, for some other stuff, AI was quite helpful.
It created a class that dynamically loads chunks from disk but provides the API to index and slice it.
That was neat.
AI was also incredibly good at helping with Matplotlib’s arcane syntax.</p><h2 id="s12"><a href="#s12">12</a> <span>Other people’s line scan photography of trains</span></h2><h2 id="s12.1"><a href="#s12.1">12.1</a> <span>Adam Magyar</span></h2><p><a href="https://www.magyaradam.com/wp/">Adam Magyar</a> uses a black and white digital line scan camera for his “Stainless” project, and another derived from a scanner for his “Urban Flow” project.</p><p>His camera must have much better sensitivity than mine since he managed to capture fairly clean images even for underground trains (whereas I generally require sunlight for mine).
Apparently, he had to scout out many subway stations to find ones where the lights don’t flicker at 60 Hz.</p><h2 id="s12.2"><a href="#s12.2">12.2</a> <span>KR64’s blog</span></h2><p>At <a href="https://web.archive.org/web/20250715102540/https://kr64.seesaa.net/">kr64.seesaa.net</a> you can find a mind boggling collection of high quality line scan photos of trains from all across Japan.</p><p>They probably do this full time as the variety of trains is far greater than I can ever hope to achieve.
I believe they use a film slit scan camera.</p><p>Unfortunately, their website has a bunch of technical issues and often goes down. I would be happy to help them out but my Japanese is very poor and I don’t see any way to contact them.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFC 9839 and Bad Unicode (251 pts)]]></title>
            <link>https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</link>
            <guid>44995640</guid>
            <pubDate>Sat, 23 Aug 2025 12:54:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839">https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</a>, See on <a href="https://news.ycombinator.com/item?id=44995640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">Unicode is good. If you’re designing a data structure or protocol that has text fields, they should contain
    Unicode characters encoded in UTF-8. There’s another question, though:
    “<em>Which</em> Unicode characters?” The 
    answer is “Not all of them, please exclude some.”</p>

<p>This issue keeps coming up, so Paul Hoffman and I put together an individual-submission draft
    to the IETF and now (where by “now” I mean “two years later”) it’s been published as
    <a href="https://www.rfc-editor.org/rfc/rfc9839.html">RFC 9839</a>. It explains which characters are bad, and why, then offers
    three plausible less-bad subsets that you might want to use.
    Herewith a bit of background, but…</p>

<p id="p-2"><span>Please</span> · 
If you’re actually working on something new that will have text fields, please read the RFC. It’s only ten pages long, and that’s
    with all the IETF boilerplate. It’s written specifically for software and networking people.</p>

<p id="p-3"><span>The smoking gun</span> · 
The badness that 9839 focuses on is “problematic characters”, so let’s start with a painful example of what that means.
    Suppose you’re designing a protocol that uses JSON and one of your constructs has a <code>username</code> field.
    Suppose you get this message (I omit all the non-<code>username</code> fields). It’s 
    a perfectly legal JSON text:</p>

<div><pre><span></span><span>{</span>
  <span>  </span><span>"username"</span><span>:</span><span> </span><span>"\u0000\u0089\uDEAD\uD9BF\uDFFF"</span>
<span>}</span>    </pre></div>
    <p>Unpacking all the JSON escaping gibberish reveals that the value of the <code>username</code> field contains four 
    numeric “code points” identifying Unicode characters:</p>

    <ol>
      <li><p>The first code point is zero, in Unicode jargon <code>U+0000</code>. In human-readable text it
      has no meaning, but it will interfere with the operation of certain programming languages.</p>
</li>
      <li><p>Next is Unicode <code>U+0089</code>, official name “CHARACTER TABULATION WITH JUSTIFICATION”. It’s what Unicode calls a
      <a href="https://en.wikipedia.org/wiki/C0_and_C1_control_codes">C1
      control code</a>, inherited from ISO/IEC 6429:1992, adopted from 
      <a href="https://www.ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA 48</a> (1991), which calls it
      “HTJ” and says: <i>HTJ causes the contents of the active field (the field in the presentation component that contains the
      active presentation position) to be shifted forward so that it ends at the character position preceding the
      following character tabulation stop. The active presentation position is moved to that following character
      tabulation stop. The character positions which precede the beginning of the shifted string are put into the
      erased state.</i></p>

      <p>Good luck with that.</p>
</li>
      <li><p>The third code point, <code>U+DEAD</code>, in Unicode lingo, is an “unpaired surrogate”.  To understand,
      you’d have to learn how Unicode’s much-detested
      <a href="https://en.wikipedia.org/wiki/UTF-16">UTF-16</a> encoding works.
      I recommend not bothering.</p>

      <p>All you need to know is that surrogates are only meaningful when they come in pairs in UTF-16 encoded text. There is
      effectively no such text on the wire and thus no excuse for tolerating surrogates in your data. In fact, the UTF-8 specification
      says that you mustn’t use UTF-8 to encode surrogates. But the real problem is that different libraries in different
      programming languages don’t always do the same things when they encounter this sort of fœtid interloper.</p>
    </li>
      <li><p>Finally, <code>\uD9BF\uDFFF</code> is JSON for the code point <code>U+7FFFF</code>.
      Unicode has a category called “noncharacter”, containing a few dozen code points that, for a variety of
      reasons, some good, 
      don’t represent anything and must not be interchanged on the wire. <code>U+7FFFF</code> is one of those.</p>
</li>
    </ol>
    <p>The four code points in the example are all clearly problematic. 
    The just-arrived RFC 9839 formalizes the notion of “problematic” and
    offers easy-to-cite language saying which of these problematic types you want to
    exclude from your text fields. Which, if you’re going to use JSON, you should probably do.</p>

    <p id="p-6"><span>Don’t blame Doug</span> · 
    Doug Crockford I mean, the inventor of JSON.  If he (or I or really anyone careful) were inventing JSON now that Unicode is
    mature, he’d have been fussier about its character repertoire. Having said that, we’re stuck with JSON-as-it-is forever, so we
    need a good way to say which of the problematic characters we’re going to exclude even if JSON allows them.</p>

    <p id="p-5"><span>PRECISion</span> · 
    You may find yourself wondering why the IETF waited until 2025 to provide help with Bad Unicode.
    It didn’t; here’s
    <a href="https://www.rfc-editor.org/rfc/rfc8264.html">RFC 8264</a>: <cite>PRECIS Framework: Preparation, Enforcement, and
    Comparison of Internationalized Strings in Application Protocols</cite>; the first PRECIS predecessor was published in 2002.
    8264 is 43 pages long, containing a <em>very</em>
    thorough discussion of many more potential Bad Unicode issues than 9839 does.</p>

    <p>Like 9839, PRECIS specifies subsets of the Unicode character repertoire and goes further, providing a mechanism for defining
    more.</p>

    <p>Having said that, PRECIS doesn’t seem to be very widely used by people who are defining new data structures and protocols. My
    personal opinion is that there are two problems which make it hard to adopt. First, it’s large and 
    complex, with many moving parts, and requires careful study to understand. Developers are (for good reason) lazy.</p>

    <p>Second, using PRECIS ties you to a specific version of Unicode. In particular, it forbids the use of the (nearly a million)
    unassigned code points. Since each release of Unicode includes new code point assignments, that means that a sender and receiver
    need to agree on exactly which version of Unicode they’re both going to use if they want reliably interoperable behavior. This
    makes life difficult for anyone writing a general-purpose code designed to be used in lots of different applications.</p>

    <p>I personally think that the only version of Unicode anybody wants to use is “as recent as possible”, so they can be confident
    of having all the latest emojis.</p>

    <p>Anyhow, 9839 is simpler and dumber than PRECIS. But I think some people will find it useful and now the IETF agrees.</p>

    <p id="p-7"><span>Source code</span> · 
    I’ve written a little Go-language library to validate incoming text fields against each of the three subsets that 9839
    specifies,
    <a href="https://github.com/timbray/RFC9839">here</a>.  I don’t claim it’s optimal, but it is well-tested.</p>

    <p>It doesn’t have a version number or release just yet, I’ll wait till a few folk have had a chance to spot any dumb mistakes I
    probably made.</p>

    <p id="p-9"><span>Details</span> · 
    Here’s a compact summary of the world of problematic Unicode code points and data formats and standards.</p>

    <table>
      <tbody><tr><td></td><th colspan="3">Problematic classes excluded?</th></tr>
      <tr><td></td><th>Surrogates</th><th>Legacy controls</th><th>Noncharacters</th></tr>
      <tr><td>CBOR</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>I-JSON</td><td>yes</td><td>no</td><td>yes</td></tr>
      <tr><td>JSON</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>Protobufs</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>TOML</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial [1]</td><td>partial [2]</td></tr>
      <tr><td>YAML</td><td>yes</td><td>mostly [3]</td><td>partial [2]</td></tr>
      <tr><td></td><th colspan="3">RFC 9839 Subsets</th></tr>
      <tr><td>Scalars</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial</td><td>partial</td></tr>
      <tr><td>Assignables</td><td>yes</td><td>yes</td><td>yes</td></tr>
    </tbody></table>
    <p>Notes:</p>

    <p><b>[1]</b> XML allows C1 controls.</p>

    <p><b>[2]</b> XML and YAML don’t exclude the noncharacters outside the Basic Multilingual Pane.</p>

    <p><b>[3]</b> YAML excludes all the legacy controls except for the mostly-harmless <code>U+0085</code>, another version of
    <code>\n</code> used in IBM mainframe documents.</p>

    <p id="p-8"><span>Thanks!</span> · 
    9839 is not a solo production. It received an extraordinary amount of discussion and improvement from a lot of smart and
    well-informed people 
    and the published version, 15 draft revisions later, is immensely better than my initial draft. My sincere thanks go to my
    co-editor Paul Hoffman and to all those mentioned in the RFC’s “Acknowledgements” section.</p>

    <p id="p-4"><span>On individual submissions</span> · 
    9839 is the second “individual submission” RFC I’ve pushed through the IETF (the other is
    <a href="https://datatracker.ietf.org/doc/html/rfc7725">RFC 7725</a>, which registers the HTTP 451 status code).  While it’s nice
    to decide something is worth standardizing and eventually have that happen, it’s really a lot of work. Some of that work is
    annoying.</p>

    <p>I’ve been involved in
    other efforts as Working-Group member, WG chair, and WG specification editor, and I can report authoritatively that creating an
    RFC the traditional way, through a Working Group, is easier and better.</p>

    <p>I feel discomfort advising others not to follow in my footsteps, but in this case I think it’s the right advice.</p>

  <hr>


</div></div>]]></description>
        </item>
    </channel>
</rss>