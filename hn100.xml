<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 19 Jan 2025 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Haskell: A Great Procedural Language (132 pts)]]></title>
            <link>https://entropicthoughts.com/haskell-procedural-programming</link>
            <guid>42754098</guid>
            <pubDate>Sun, 19 Jan 2025 05:50:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://entropicthoughts.com/haskell-procedural-programming">https://entropicthoughts.com/haskell-procedural-programming</a>, See on <a href="https://news.ycombinator.com/item?id=42754098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#side-effects-as-first-class-values">Side effects as first class values</a></li>
<li><a href="#de-mystifying-do-blocks">De-mystifying do blocks</a></li>
<li><a href="#functions-that-operate-on-side-effects">Functions that operate on side effects</a></li>
<li><a href="#leaning-into-the-first-classiness-of-effects">Leaning into the first classiness of effects</a></li>
<li><a href="#things-you-never-need-to-care-about">Things you never need to care about</a></li>
<li><a href="#appendix-a--avoiding-success-and-uselessness">Appendix A: Avoiding success and uselessness</a></li>
<li><a href="#appendix-b--why-fmap-maps-over-both-side-effects-and-lists">Appendix B: Why fmap maps over both side effects and lists</a></li>
<li><a href="#appendix-c--foldable-and-traversable">Appendix C: Foldable and Traversable</a></li>
</ul>
</div>
</div>

<p>
There are many catchphrases about Haskell.
</p>

<ul>
<li>Haskell is useless.</li>
<li>Haskell aims to avoid success at all costs.</li>
<li>Haskell is the best procedural language in the world.</li>
</ul>

<p>
These sound like dismissals or absurdities from the outside, but once you learn
what they really mean, they take on a new light. In this article, I want to
explain the third. (See the appendix if you are curious about the first two.)
</p>

<p>
This article really, really tried to become a monad <abbr>i/o</abbr> tutorial, but I think
I stopped it in time.<label for="fn.1">1</label><span><sup>1</sup> By that I mean I had to rewrite it twice and delete
large chunks of monad <abbr>i/o</abbr> tutorial material.</span> Here, we are going to jump right
in and focus on the interesting stuff.
</p>
<section id="outline-container-side-effects-as-first-class-values">
<h2 id="side-effects-as-first-class-values">Side effects as first class values</h2>
<div id="text-orgd4b0318">
<p>
Effectful computations in Haskell are first class values. This means we can
store them in variables or data structures for later use. There is a Haskell
function
</p>

<p><label>In[1]:</label></p><div>
<pre><span>randomRIO</span> <span>::</span> (<span>Int</span>, <span>Int</span>) <span>-&gt;</span> <span>IO</span> <span>Int</span>
</pre>
</div>

<p>
which, when given two integers as arguments, picks a random integer between
them. We can put calls to this function into a list, like so:
</p>

<p><label>In[2]:</label></p><div>
<pre><span>some_dice</span> <span>=</span> [ randomRIO(1, 6), randomRIO(1, 6) ]
</pre>
</div>

<p>
This is a list of two calls to <code>randomRIO</code>. What surprises non-Haskellers is
that when this list is created, no random numbers are generated. Coming from
other programming languages, we are used to side effects (such as random
generation) being executed directly when the side effectful function is
called.<label for="fn.2">2</label><span><sup>2</sup> You may think Haskell is different here due to laziness, but that’s
also not true. Even if we put these calls into a strict data structure, no
randomness would happen.</span>
</p>

<p>
We can add more random generation to the list:
</p>

<p><label>In[3]:</label></p><div>
<pre><span>more_dice</span> <span>=</span> some_dice <span>&lt;&gt;</span> [ randomRIO(1, 6) ]
</pre>
</div>

<p>
and still no random numbers will be generated. We can go ahead and manipulate
this list in all sorts of ways, and <i>still</i> no random numbers would be
generated.
</p>

<p>
To be clear, the <code>randomRIO</code> function could well be called<label for="fn.3">3</label><span><sup>3</sup> Whether this
actually happens is a question of lazy evaluation, optimisation, etc.</span>, and when
it is called it returns a value of type <code>IO Int</code>. It’s just that this value <i>is
not an integer</i>. If anything, we can think of it as a set of instructions for
eventually, somehow, getting an integer. It’s not an actual integer. It’s an
object encapsulating a side effect. When this side effect object executes, it
will produce a random integer, but the object itself just describes the
computation, it is not an integer.
</p>

<p>
In other words, in Haskell, it is not enough to call a side effectful function
to execute its side effects. When we call the side effectful function, it
produces an object encapsulating the side effect, and this object can be
executed in the future to produce the result of the side effect.<label for="fn.4">4</label><span><sup>4</sup> Readers
familiar with JavaScript promises will recognise this concept. Indeed,
promises are modeled after side effects in Haskell.</span>
</p>

<p>
The common way we teach beginners to do execute side effect objects is by
calling them from a <code>do</code> block, using the special <code>&lt;-</code> assignment operator to
extract their result. As a first approximation, we can think of the following
code as the way to force side effects to execute.
</p>

<p><label>In[4]:</label></p><div>
<pre><span>dice_print</span> <span>=</span> <span>do</span>
  side <span>&lt;-</span> randomRIO(1, 6)
  printf <span>"It landed on %d\n"</span> side
</pre>
</div>

<p>
We can imagine that the <code>&lt;-</code> arrow executes the side effect object returned by
<code>randomRIO</code> and captures the value it produces. Similarly, the side effect
object returned by <code>printf</code> gets executed, but we don’t capture the result; we
don’t care about the value produced by it, we only care about the side effect
itself.
</p>

<p>
The lie-to-children here is that we pretend the <code>do</code> block is magical and that
when it executes, it also executes side effects of functions called in it. This
mental model will take the beginner a long way, but at some point, one will want
to break free of it. That is when Haskell starts to really shine as a procedural
language.
</p>

<p>
This article features another lie-to-children: it will have type signatures
specialised to <code>IO a</code> and <code>[a]</code>. All the functions I mention are more generic
than I’m letting on.
</p>

<ul>
<li>Anywhere this article says <code>IO a</code> it will work with any type of side effect
(like <code>Maybe a</code>, <code>Rand g a</code>, <code>StateT s m a</code>, etc.)</li>
<li>Anywhere this article says <code>[a]</code> it probably also works with other
collection/container types (like <code>Maybe a</code>, <code>Array i a</code>, <code>HashMap k a</code>, <code>Tree
  a</code>, etc.)</li>
</ul>

<p>
The reason this article uses more direct type signatures is to hopefully be
readable also to someone who does not use Haskell.<label for="fn.5">5</label><span><sup>5</sup> If the reader has never
seen <abbr>ml</abbr> style syntax before, I think the most important thing to know is that
function calls aren’t written like <code>isInfixOf("llo", "hello, world\n")</code> but
rather with spaces, as in <code>isInfixOf "llo" "hello, world\n"</code>.</span>
</p>
</div>
</section>
<section id="outline-container-de-mystifying-do-blocks">
<h2 id="de-mystifying-do-blocks">De-mystifying do blocks</h2>
<p>
In order to drive the point home, we’ll start by seeing what it is the <code>do</code>
block actually does, because it’s not magic at all. In fact, every do block can
be converted to just two operators. If you
already know this, skip ahead to the next section.
</p>
<div id="outline-container-then">
<h2 id="then">then</h2>
<div id="text-orgef76d50">
<p>
If we want to combine two side effects into one, we can use the <code>*&gt;</code> operator,
which is pronounced <i>then</i> or <i>sequence right</i>. It takes two side effect objects
and creates a new side effect object that executes both when itself is executed.
The value produced by this new composite object is going to be the value
produced by the second object its constructed from. In that sense, the <code>*&gt;</code>
operator is a lot like the comma operator in C: it chains together statements,
and returns the result of the last.
</p>

<p><label>In[5]:</label></p><div>
<pre>(<span>*&gt;</span>) <span>::</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> b <span>-&gt;</span> <span>IO</span> b
</pre>
</div>

<p>
For example, here we combine two print statements into a single side effectful
function.
</p>

<p><label>In[6]:</label></p><div>
<pre><span>greeting</span> <span>::</span> <span>IO</span> <span>()</span>
<span>greeting</span> <span>=</span>
  putStr <span>"hello, "</span> <span>*&gt;</span> putStrLn <span>"world"</span>
</pre>
</div>

<p>
This is a single side effect object called <code>greeting</code>, but its execution will
involve the execution of two separate print statements.
</p>

<p>
We teach beginners to write this as
</p>

<p><label>In[7]:</label></p><div>
<pre><span>greeting</span> <span>=</span> <span>do</span>
  putStr <span>"hello, "</span>
  putStrLn <span>"world"</span>
</pre>
</div>

<p>
which is the exact same thing, although arguably easier to read. The interesting
thing is the implication for how we look at <code>do</code> blocks. It turns out they may
not be magical at all; maybe they are just inserting implicit commas, i.e. a
pretty way of taking multiple side effect objects and combining them into a
single, bigger, side effect object.
</p>
</div>
</div>
<div id="outline-container-bind">
<h2 id="bind">bind</h2>
<div id="text-org642d69b">
<p>
The one thing we cannot do with <code>*&gt;</code> is take the result from the left-hand side
effect and use it to influence the right-hand side function call, because the
<code>*&gt;</code> operator discards the first result before executing the second effect –
just like the comma operator in C. The die throwing code we saw previously,
</p>

<p><label>In[8]:</label></p><div>
<pre><span>dice_print</span> <span>=</span> <span>do</span>
  side <span>&lt;-</span> randomRIO(1, 6)
  printf <span>"It landed on %d\n"</span> side
</pre>
</div>

<p>
cannot be implemented with just <code>*&gt;</code>. We need the additional operator <code>&gt;&gt;=</code>
which takes a side effect object and plugs the value it produces into another
side effectful function.<label for="fn.6">6</label><span><sup>6</sup> This operator is widely known as <i>bind</i>.</span>
</p>

<p><label>In[9]:</label></p><div>
<pre>(<span>&gt;&gt;=</span>) <span>::</span> <span>IO</span> a <span>-&gt;</span> (a <span>-&gt;</span> <span>IO</span> b) <span>-&gt;</span> <span>IO</span> b
</pre>
</div>

<p>
Using this operator, we could write the above do block as
</p>

<p><label>In[10]:</label></p><div>
<pre><span>print_side</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>IO</span> <span>()</span>
<span>print_side</span> side <span>=</span>
  printf <span>"It landed on %d\n"</span> side

<span>dice_print</span> <span>::</span> <span>IO</span> <span>()</span>
<span>dice_print</span> <span>=</span>
  randomRIO(1, 6) <span>&gt;&gt;=</span> print_side
</pre>
</div>

<p>
and this would take the result of executing the side effect of <code>randomRIO</code> and
plugging it into another side effectful function, namely <code>print_side</code>.
</p>
</div>
</div>
<div id="outline-container-two-operators-are-all-of-do-blocks">
<h2 id="two-operators-are-all-of-do-blocks">Two operators are all of do blocks</h2>
<div id="text-org71d7321">
<p>
This illustrates that <code>do</code> blocks are built from only two operators. If we stick
with <code>do</code> blocks for all side effects, we will never learn why Haskell is the
greatest procedural programming language in the world, because we are limiting
ourselves to just two operators for dealing with side effects.
</p>

<p>
Let’s lift our gaze and see what happens when we look beyond those. There are
more functions for dealing with side effects.
</p>
</div>
</div>
</section>
<section id="outline-container-functions-that-operate-on-side-effects">
<h2 id="functions-that-operate-on-side-effects">Functions that operate on side effects</h2>
<p>
We’ll start with the basics and work our way up.
</p>
<div id="outline-container-pure">
<h2 id="pure">pure</h2>
<div id="text-orgc2ed192">
<p>
If we want to construct a new side effect object that always produces a
specific value, we can use the function <code>pure</code>.
</p>

<p><label>In[11]:</label></p>

<p>
For example, this creates a side effect object that always produces the integer 4.
</p>

<p><label>In[12]:</label></p><div>
<pre><span>loaded_die</span> <span>::</span> <span>IO</span> <span>Int</span>
<span>loaded_die</span> <span>=</span>
  <span>-- </span><span>Chosen by fair dice roll.</span>
  <span>-- </span><span>Guaranteed to be random.</span>
  pure 4
</pre>
</div>

<p>
Creating side effect objects that always produce a known value might not seem
very useful, but it comes up all the time when bridging the worlds of pure code
and side effects.
</p>
</div>
</div>
<div id="outline-container-fmap">
<h2 id="fmap">fmap</h2>
<div id="text-orgd7cf409">
<p>
One of the most used functions when working with side effects in Haskell is
<code>fmap</code>.
</p>

<p><label>In[13]:</label></p><div>
<pre><span>fmap</span> <span>::</span> (a <span>-&gt;</span> b) <span>-&gt;</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> b
</pre>
</div>

<p>
This takes a pure function, and a side effect object, and returns a new side effect
object that is similar to the one it got, except the value produced will be
transformed by the function first.
Transforming the results of side effects is
so common that <code>fmap</code> has an operator alias: <code>&lt;$&gt;</code>. For example, to get the
length of the path to the user’s home directory, we can do
</p>

<p><label>In[14]:</label></p><div>
<pre><span>path_length</span> <span>::</span> <span>IO</span> <span>Int</span>
<span>path_length</span> <span>=</span> length <span>&lt;$&gt;</span> getEnv <span>"HOME"</span>
<span>-- </span><span>equivalent to</span>
<span>-- </span><span>path_length = fmap length (getEnv "HOME")</span>
</pre>
</div>

<p>
This creates a new side effect object which will produce the result of applying
the <code>length</code> function to the result of the side effect of <code>getEnv</code>.
</p>
</div>
</div>
<div id="outline-container-lifta2--lifta3---">
<h2 id="lifta2--lifta3---">liftA2, liftA3, …</h2>
<div id="text-org7e05c97">
<p>
Where <code>fmap</code> allows us to transform the value produced by a single side effect,
sometimes we need to create a side effect object that produces something based
on multiple other side effects.
This is where <code>liftA2</code> and friends come in.
</p>

<p><label>In[15]:</label></p><div>
<pre><span>liftA2</span> <span>::</span> (a <span>-&gt;</span> b <span>-&gt;</span> c)      <span>-&gt;</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> b <span>-&gt;</span> <span>IO</span> c
<span>liftA3</span> <span>::</span> (a <span>-&gt;</span> b <span>-&gt;</span> c <span>-&gt;</span> d) <span>-&gt;</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> b <span>-&gt;</span> <span>IO</span> c <span>-&gt;</span> <span>IO</span> d
</pre>
</div>

<p>
These can be thought of like fmap, except they don’t transform the result of
just one side effect, but they combine the results of multiple side effects with
one function, and create a new side effect object that produces the return value
of that function.
This is also common enough that it can be written with two
operators: the <code>&lt;$&gt;</code> we already saw for the first argument, and then <code>&lt;*&gt;</code> for
the rest of the arguments. To check if a user’s home directory contains the user
name, we do
</p>

<p><label>In[16]:</label></p><div>
<pre><span>home_username</span> <span>::</span> <span>IO</span> <span>Bool</span>
<span>home_username</span> <span>=</span> isInfixOf <span>&lt;$&gt;</span> getEnv <span>"LOGNAME"</span> <span>&lt;*&gt;</span> getEnv <span>"HOME"</span>
<span>-- </span><span>equivalent to</span>
<span>-- </span><span>home_username = liftA2 isInfixOf (getEnv "LOGNAME") (getEnv "HOME")</span>
</pre>
</div>

<p>
This has created a new side effect object that will produce true if <code>$LOGNAME</code>
is a part of <code>$HOME</code>, and false otherwise.
</p>
</div>
</div>
<div id="outline-container-intermission--what-s-the-point-">
<h2 id="intermission--what-s-the-point-">Intermission: what’s the point?</h2>
<div id="text-org88fd136">
<p>
I am raving ecstatic about this. Combining the results of side effects through
<code>liftA2</code> and friends is such a fundamental technique that the favicon of this
website is a stylised version of the <code>&lt;*&gt;</code> operator.
</p>

<p>
But the reader may understandably be a little underwhelmed. It seems like we
have only learned to do in Haskell what we can do in Python and every other
programming language already. All popular programming languages let us use the
results of side effects as arguments to other functions. There are already two
reasons we should care about Haskell, though, even before we see what’s to come.
These are refactorability and discipline.
</p>



<p>
Biggest benefit is probably refactorability. We might have the following code
that throws two dice and sums them up:
</p>

<p><label>In[17]:</label></p><div>
<pre><span>sum_dice</span> <span>::</span> <span>IO</span> <span>Int</span>
<span>sum_dice</span> <span>=</span>
  liftA2 (<span>+</span>) (randomRIO(1,6)) (randomRIO(1,6))
</pre>
</div>

<p>
and we think the repetition is annoying, so we put the actual die-tossing code
into a variable.
</p>

<p><label>In[18]:</label></p><div>
<pre><span>sum_dice</span> <span>::</span> <span>IO</span> <span>Int</span>
<span>sum_dice</span> <span>=</span>
  <span>let</span>
    toss_die <span>=</span> randomRIO(1,6)
  <span>in</span>
    liftA2 (<span>+</span>) toss_die toss_die
</pre>
</div>

<p>
If we did this in Python, we would accidentally store the <i>result</i> of the die
toss in the <code>toss_dice</code> variable! If the die lands on 3, we will compute 3+3,
rather than the intention of summing two different tosses. With the way side
effects are first class values in Haskell, we are always free to blindly (!)
extract code into a variable name, and this will never change how the code
runs.<label for="fn.7">7</label><span><sup>7</sup> This is called <i>equational reasoning</i> and it’s incredibly powerful and one of
the things that make Haskell such a strong enterprise language, and so nice to
work with procedural code in.</span>
</p>

<p>
A second benefit is that by being structured in how we allow side effects to
affect subsequent computation, we have a lower risk of accidentally introducing
side effects where they were not intended. We also have greater control over
what can affect what, reducing the risk of interaction bugs.
</p>

<p>
The reader might, for example, argue that the previous refactoring example works
just fine in Python, by using an anonymous function as a faux side effect object:
</p>

<p><label>In[19]:</label></p><div>
<pre><span>def</span> <span>sum_dice</span>():
    <span>toss_die</span> = <span>lambda</span>: random.randint(1, 7)
    <span>return</span> toss_die() + toss_die()
</pre>
</div>

<p>
but this (a) requires being careful in refactoring, (b) does not prevent
accidentally triggering the side effect where it was not intended, and (c)
requires a language feature (<i>sequence points</i>) to disambiguate in which order
side effects get executed. With Haskell, we just don’t have to care. We can rest
easy in knowing that the compiler and libraries have our backs.
</p>
</div>
</div>
<div id="outline-container-sequencea">
<h2 id="sequencea">sequenceA</h2>
<div id="text-org0d35ca6">
<p>
We are starting to see the benefits of side effects as first class values, so
let’s shift to a higher gear. We have seen that Haskell allows us to store side
effect objects in variables without accidentally executing their effects. The
next step is storing these side effect objects in data structures.
</p>

<p>
We could, for example, create a list of three different ways to get the username
of the currently logged in user.
</p>

<p><label>In[20]:</label></p><div>
<pre><span>getting_usernames</span> <span>::</span> [<span>IO</span> (<span>Maybe</span> <span>String</span>)]
<span>getting_usernames</span> <span>=</span> [lookupEnv <span>"USER"</span>, lookupEnv <span>"LOGNAME"</span>, lookupEnv <span>"SUDO_USER"</span>]
</pre>
</div>

<p>
This list cannot be executed for its side effects directly, because the list
itself is not a side effect – it’s a list of side effects. There are library
functions to deal with this, though. One is
</p>

<p><label>In[21]:</label></p><div>
<pre><span>sequenceA</span> <span>::</span> [<span>IO</span> a] <span>-&gt;</span> <span>IO</span> [a]
</pre>
</div>

<p>
This is what we need in this case: it takes a list of side effect objects and
creates a new side effect object that executes all the side effects of the list,
and then produces a list of all the values produced by those side effects. To
make a side effect that produces a list of candidate usernames, we define
</p>

<p><label>In[22]:</label></p><div>
<pre><span>actual_usernames</span> <span>::</span> <span>IO</span> [<span>Maybe</span> <span>String</span>]
<span>actual_usernames</span> <span>=</span> sequenceA getting_usernames
</pre>
</div>

<p>
If we execute this side effect and print the result (either by connecting it up
with <code>print</code> using the <code>&gt;&gt;=</code> operator, or in a <code>do</code> block), then on my system we
get the result
</p>

<p><label>Out[1]:</label></p><pre>[Just "kqr", Just "kqr", Nothing]
</pre>


<p>
Sometimes we have a list of side effects but we don’t care about the value they
produce. This might be the case if we have collected a bunch of log statements
from a pure function. We want to execute their side effects (the actual logging
action) but we don’t care about what the log function itself returns (it is
usually a void or unit-type value.)
</p>

<p><label>In[23]:</label></p><div>
<pre><span>log_statements</span> <span>::</span> [<span>IO</span> <span>()</span>]
<span>log_statements</span> <span>=</span> [
    log <span>Info</span> <span>"Creating user"</span>,
    log <span>Warn</span> <span>"User already found"</span>
    log <span>Info</span> <span>"Updating user"</span>
]
</pre>
</div>

<p>
As a reminder: these function calls to the <code>log</code> function do not cause anything
to be logged. The <code>log</code> function returns a side effect object that, when
executed, makes the logging happen. The <code>log_statements</code> variable contains a
list of such side effect objects – it is not itself a side effect object.
</p>

<p>
To execute these, we can again combine the side effects in the list into one
side effect object with <code>sequenceA</code>. When we do so, we get a side effect object
that produces the value <code>[(), (), ()]</code>. To get the code to type check, we may
have to discard this value. We already know how to do this, because discarding
values is what the <code>*&gt;</code> operator does.
</p>

<p><label>In[24]:</label></p><div>
<pre><span>execute_logged</span> <span>::</span> <span>IO</span> <span>()</span>
<span>execute_logged</span> <span>=</span>
  sequenceA log_statements <span>*&gt;</span> pure <span>()</span>
</pre>
</div>

<p>
When the side effect of <code>execute_logged</code> runs, it will run the side effects of
the log statements and then discard the dummy values produced in the process.
</p>

<p>
Remember that loaded die from before? Now we can check that it indeed always
returns the same number. First we create a list by repeating the same
side effect object an infinite number of times.
</p>

<p><label>In[25]:</label></p><div>
<pre><span>many_loaded_dice</span> <span>::</span> [<span>IO</span> <span>Int</span>]
<span>many_loaded_dice</span> <span>=</span>
  repeat loaded_die
</pre>
</div>

<p>
Then we construct a side effect object that executes the first few of these and
keeps the values they produced.
</p>

<p><label>In[26]:</label></p><div>
<pre><span>some_thrown_dice</span> <span>::</span> <span>IO</span> [<span>Int</span>]
<span>some_thrown_dice</span> <span>=</span>
  sequenceA (take 20 many_loaded_dice)
</pre>
</div>

<p>
If we connect this up with a print (again, <code>do</code> block or the <code>&gt;&gt;=</code>
operator) and execute it, we get
</p>

<p><label>Out[2]:</label></p><pre>[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4]
</pre>


<p>
We could do the same thing with a better die:
</p>

<p><label>In[27]:</label></p><div>
<pre><span>many_good_dice</span> <span>::</span> [<span>IO</span> <span>Int</span>]
<span>many_good_dice</span> <span>=</span>
  repeat (randomRIO (1,6))

<span>some_thrown_dice</span> <span>::</span> <span>IO</span> [<span>Int</span>]
<span>some_thrown_dice</span> <span>=</span>
  sequenceA (take 20 many_good_dice)
</pre>
</div>

<p>
If we print this once, we get
</p>

<p><label>Out[3]:</label></p><pre>[2,1,4,1,3,2,2,2,1,4,6,4,1,4,6,4,5,3,4,6]
</pre>


<p>
If we print it again, we might get
</p>

<p><label>Out[4]:</label></p><pre>[4,5,3,2,4,2,5,4,6,1,1,5,1,3,6,4,4,5,1,4]
</pre>


<p>
Even though we constructed the list by repeating <i>the same</i> side effect object,
we get a fresh set of random numbers every time the composite side effect object
is executed. This is additional proof that what we store in the list is not the
result of the side effect, but the side effect itself.
</p>

<p>
But also note what we did. We used list functions (<code>repeat</code>, <code>take 20</code>) to
manipulate a data structure of side effect objects as if they were regular
values – because they are! Then we used a side effect manipulation function
(<code>sequenceA</code>) to combine the side effects in the list into one new side effect
object that executes all of them. This is a kind of meta programming, except
it’s not using a special macro language but performed at the level of regular
values.
</p>
</div>
</div>
<div id="outline-container-interlude--convenience-functions">
<h2 id="interlude--convenience-functions">Interlude: convenience functions</h2>
<div id="text-org4d4d886">
<p>
The Haskell standard libraries also contain a few convenience functions to do
what we have done above except easier. For example, when we discarded the value
produced by a side effect, we used <code>object *&gt; pure ()</code>. This exists as a
shortcut function called <code>void</code>:
</p>

<p><label>In[28]:</label></p>

<p>
We could wrap the call in this instead:
</p>

<p><label>In[29]:</label></p><div>
<pre><span>execute_logged</span> <span>::</span> <span>IO</span> <span>()</span>
<span>execute_logged</span> <span>=</span>
  void (sequenceA log_statements)
</pre>
</div>

<p>
But there is an even more convenient way to do it. Many of the functions I will
show have a variant that ends with underscore. These variants throw away the
result. So we could instead have written
</p>

<p><label>In[30]:</label></p><div>
<pre><span>execute_logged</span> <span>::</span> <span>IO</span> <span>()</span>
<span>execute_logged</span> <span>=</span>
  sequenceA_ log_statements
</pre>
</div>

<p>
and this would run the effects and throw away the results. One benefit of using
this variant is that it actually works with slightly more collection types than
the one without underscore but the drawback is, of course, that we don’t get the
results produced, only the side effects.<label for="fn.8">8</label><span><sup>8</sup> See the appendix for more details.</span>
</p>

<p>
Separately, we used list manipulation functions <code>repeat</code> and <code>take 20</code> to create
an appropriately-sized list of side effect objects, and then executed it with
<code>sequenceA</code>. This specific combination is common enough to exist as a library
function
</p>

<p><label>In[31]:</label></p><div>
<pre><span>replicateM</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> [a]
</pre>
</div>

<p>
With this, we can draw 20 loaded dice quite succinctly.
</p>

<p><label>In[32]:</label></p><div>
<pre><span>some_thrown_dice</span> <span>::</span> <span>IO</span> [<span>Int</span>]
<span>some_thrown_dice</span> <span>=</span>
  replicateM 20 loaded_die
</pre>
</div>

<p>
This also exist as a result-discarding variant that runs a side effect object
\(n\) times but does not collect the values produced by the side effect.
</p>

<p><label>In[33]:</label></p><div>
<pre><span>replicateM_</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> <span>()</span>
</pre>
</div>

<p>
Back when I was in university I had a coursemate who had just learned
programming and found the following joke very funny:
</p>

<blockquote>
<p>
Teacher: Write “I will not cheat again” 500 times on the blackboard. That should
get you to learn your lesson.
</p>

<p>
Student: <code>for (int counter = 0; counter &lt; 500; counter++) {
System.out.println("I will not cheat again."); }</code>
</p>
</blockquote>

<p>
I didn’t find it funny – I found it sad. Is this really the type of low-level
programming we are teaching students? Here’s how it should go.
</p>

<blockquote>
<p>
Teacher: Write “I will not cheat again” 500 times on the blackboard. That should
get you to learn your lesson.
</p>

<p>
Student: <code>replicateM_ 500 (putStrLn "I will not cheat again.")</code>
</p>
</blockquote>

<p>
As a programmer, I do not want to babysit my computer and instruct it in exactly
how to count to perform a side effect 500 times. I just want to tell it to
perform the side effect 500 times and the standard libraries should know what
the detailed steps are.
</p>

<p>
Anyway, let’s get back to the good stuff. Now we’re going into advanced
techniques.
</p>
</div>
</div>
<div id="outline-container-traverse">
<h2 id="traverse">traverse</h2>
<div id="text-org1ebf799">
<p>
In a previous example, we had code that amounted to
</p>

<p><label>In[34]:</label></p><div>
<pre><span>usernames</span> <span>::</span> <span>IO</span> [<span>Maybe</span> <span>String</span>]
<span>usernames</span> <span>=</span>
  sequenceA [lookupEnv <span>"USER"</span>, lookupEnv <span>"LOGNAME"</span>, lookupEnv <span>"SUDO_USER"</span>]
</pre>
</div>

<p>
This contains some duplication, namely the application of <code>lookupEnv</code> to each of
the strings. We could write this instead as
</p>

<p><label>In[35]:</label></p><div>
<pre><span>usernames</span> <span>::</span> <span>IO</span> [<span>Maybe</span> <span>String</span>]
<span>usernames</span> <span>=</span>
  sequenceA (fmap lookupEnv [<span>"USER"</span>, <span>"LOGNAME"</span>, <span>"SUDO_USER"</span>])
</pre>
</div>

<p>
relying on the fact that <code>fmap</code> works on lists as well<label for="fn.9">9</label><span><sup>9</sup> For a brief
explanation of this, see the appendix.</span>, but there is a more efficient way to
express this, though, and it’s using the function
</p>

<p><label>In[36]:</label></p><div>
<pre><span>traverse</span> <span>::</span> (a <span>-&gt;</span> <span>IO</span> b) <span>-&gt;</span> [a] <span>-&gt;</span> <span>IO</span> [b]
</pre>
</div>

<p>
which constructs a side effect object that produces the result of applying a
side effectful function to each element in a collection. The above side effect
object could then be constructed as
</p>

<p><label>In[37]:</label></p><div>
<pre><span>usernames</span> <span>::</span> <span>IO</span> [<span>Maybe</span> <span>String</span>]
<span>usernames</span> <span>=</span>
  traverse lookupEnv [<span>"USER"</span>, <span>"LOGNAME"</span>, <span>"SUDO_USER"</span>]
</pre>
</div>

<p>
This is a fairly fundamental operation, and we can define e.g. <code>sequenceA</code> in
terms of <code>traverse</code>:
</p>

<p><label>In[38]:</label></p><div>
<pre><span>sequenceA</span> <span>::</span> [<span>IO</span> a] <span>-&gt;</span> <span>IO</span> [a]
<span>sequenceA</span> <span>=</span> traverse identity
</pre>
</div>

<p>
where <code>identity</code> is the identity function that returns its argument unchanged.
</p>

<p>
As with <code>sequenceA</code>, there exists an underscored version of <code>traverse</code> that
throws away the values produced by the side effect, for cases where we are only
interested in the side effect itself.
</p>

<p><label>In[39]:</label></p><div>
<pre><span>traverse_</span> <span>::</span> (a <span>-&gt;</span> <span>IO</span> b) <span>-&gt;</span> [a] <span>-&gt;</span> <span>IO</span> <span>()</span>
</pre>
</div>

<p>
We can use this e.g. to create a side effect object that emits several log
messages when it executes.
</p>

<p><label>In[40]:</label></p><div>
<pre><span>mass_log</span> <span>::</span> <span>IO</span> <span>()</span>
<span>mass_log</span> <span>=</span>
  traverse_ (log <span>Info</span>) [
    <span>"System startup in experimental mode."</span>
    <span>"This means it will attempt to divine in which order to run startup scripts."</span>
    <span>"The startup procedure may crash if things are performed in the wrong order."</span>
    <span>"Please see the manual for more details."</span>
  ]
</pre>
</div>
</div>
</div>
<div id="outline-container-for">
<h2 id="for">for</h2>
<div id="text-orga458cb5">
<p>
We have learned that when we have a collection of things, and we want to perform
some side effectful work for each of those things, we use <code>traverse</code>. However,
when we call <code>traverse</code> we have to give the function first, and the collection
second. Sometimes it’s convenient to be able to pass the arguments the other way
around, and for this, we have an alias called <code>for</code> that is exactly like
<code>traverse</code> except with its arguments swapped around. In other words, this takes
a collection as its first argument, and the scare-quotes “loop body” last.
</p>

<p><label>In[41]:</label></p><div>
<pre><span>for</span> <span>::</span> [a] <span>-&gt;</span> (a <span>-&gt;</span> <span>IO</span> b) <span>-&gt;</span> <span>IO</span> [b]
</pre>
</div>

<p>
As with <code>traverse</code>, it exists in an underscored version that throws away the
values produced by the side effect.
</p>

<p><label>In[42]:</label></p><div>
<pre><span>for_</span> <span>::</span> [a] <span>-&gt;</span> (a <span>-&gt;</span> <span>IO</span> b) <span>-&gt;</span> <span>IO</span> <span>()</span>
</pre>
</div>

<p>
This swapped-parameters version of <code>traverse</code> is convenient whenever we need
something that resembles a for loop in other languages. The general pattern of
the Haskell <code>for</code> loop is
</p>

<p><label>In[43]:</label></p><div>
<pre><span>for</span> collection <span>$</span> <span>\</span>item <span>-&gt;</span> <span>do</span>
  things
</pre>
</div>

<p>
This, again, highlights how side-effects-as-first-class-values lets us do meta
programming. The <code>for</code> loop is not syntax. There is nothing (other than side
effects) built into the language supporting this construct. It is made entirely
from library functions.
</p>

<p>
My imagination is running dry, so we will use a convoluted example to illustrate
this. We are trying to break weak cryptography, and we have a list of numbers
for which we want to get the largest prime factor of each. This is a pure
function (no side effects) at its core, but we know factorisation is expensive,
and we suspect we will receive the same numbers repeatedly, so we will cache the
results of the computation. Once we involve the cache, the function becomes side
effectful. Here’s a first draft of such a function.
</p>

<p><label>In[44]:</label></p><div>
<pre><span>factorise</span> <span>::</span> [<span>Int</span>] <span>-&gt;</span> <span>IO</span> [(<span>Int</span>, <span>Maybe</span> <span>Int</span>)]
<span>factorise</span> numbers <span>=</span> <span>do</span>
  <span>-- </span><span>Create a mutable variable with an empty dictionary.</span>
  cache <span>&lt;-</span> newIORef M.empty
  <span>-- </span><span>Loop through all numbers we received.</span>
  for numbers <span>$</span> <span>\</span>n <span>-&gt;</span> <span>do</span>
    <span>-- </span><span>Get what's in the cache for this number.</span>
    cached <span>&lt;-</span> M.lookup n <span>&lt;$&gt;</span> readIORef cache

    factor <span>&lt;-</span> <span>case</span> cached <span>of</span>
      <span>-- </span><span>If the cache contained the factor for n, use it</span>
      <span>-- </span><span>as it is.</span>
      <span>Just</span> f <span>-&gt;</span> pure f
      <span>-- </span><span>If the cache did not contain the factor, compute</span>
      <span>-- </span><span>it fresh and cache it.</span>
      <span>Nothing</span> <span>-&gt;</span> <span>do</span>
        <span>let</span> f <span>=</span> greatest_factor n
        modifyIORef cache (M.insert n f)
        pure f

    <span>-- </span><span>If the greatest factor is the number itself, it is</span>
    <span>-- </span><span>prime. Otherwise, it can be divided by factor.</span>
    <span>if</span> factor <span>==</span> n <span>then</span>
      pure (n, <span>Nothing</span>)
    <span>else</span>
      pure (n, <span>Just</span> factor)
</pre>
</div>

<p>
This seems rather … procedural. Even though we get all the nice guarantees of
working with side effectful functions in Haskell, the code itself reads like any
other procedural language would. With Haskell, we get the best of both worlds.
</p>
</div>
</div>
</section>
<section id="outline-container-leaning-into-the-first-classiness-of-effects">
<h2 id="leaning-into-the-first-classiness-of-effects">Leaning into the first classiness of effects</h2>
<div id="text-orge387ca8">
<p>
We can really lean into this and simplify the code further, though.
</p>

<ul>
<li>We create the helper function <code>caching_compute</code> which computes the
greatest factor of a number, inserts it into the cache, and then returns the
computed factor. This allows us to replace the big pattern match with a call
to the <code>maybe</code> function. This works because – you guessed it – side effects
are first class values in Haskell.</li>
<li>We reduce duplication in the last few lines, by recognising that the first
part of the return tuple is always the number we started with, and the second
is either <code>Just factor</code> or <code>Nothing</code> based on a condition.</li>
</ul>

<p><label>In[45]:</label></p><div>
<pre><span>factorise</span> <span>::</span> [<span>Int</span>] <span>-&gt;</span> <span>IO</span> [(<span>Int</span>, <span>Maybe</span> <span>Int</span>)]
<span>factorise</span> numbers <span>=</span>
  <span>let</span>
    caching_compute cache n <span>=</span> <span>do</span>
      <span>let</span> computed_factor <span>=</span> greatest_factor n
      <span>-- </span><span>Cache the computed factor and return it.</span>
      modifyIORef cache (M.insert n computed_factor)
      pure computed_factor
  <span>in</span> <span>do</span>
    cache <span>&lt;-</span> newIORef M.empty
    for numbers <span>$</span> <span>\</span>n <span>-&gt;</span> <span>do</span>
      <span>-- </span><span>Get the cached factor if it exists, otherwise compute it.</span>
      cached <span>&lt;-</span> M.lookup n <span>&lt;$&gt;</span> readIORef cache
      factor <span>&lt;-</span> maybe (caching_compute cache n) pure cached
      <span>-- </span><span>Return n along with a Maybe value containing the factor</span>
      <span>-- </span><span>if the number is composite.</span>
      pure (n, guard (factor <span>/=</span> n) <span>$&gt;</span> factor)
</pre>
</div>

<p>
We’ll note that we don’t actually perform any side effect other than keep a
mutable variable around. There is a special type of side effect object designed
for keeping a mutable variable around, and it’s called <code>State</code>.<label for="fn.10">10</label><span><sup>10</sup> <code>State</code> is
actually just one of many options, depending on what guarantees one wants around
backtracking, multithreading, atomicity, etc. Notably, <code>State</code> is not the
highest-performance alternative, but in our case the factorisation is what takes
time, not state management.</span> We can switch to that type of mutable state instead
of <code>IO</code>, to make it harder to accidentally launch missiles from this function.
</p>

<p>
This will make the <code>cache</code> variable the implicit target of our mutation
(<code>modify</code> and <code>gets</code>), and it will also allow for streaming results. That means
we can plug an infinite list into this function, and it will keep emitting
results one element at a time.<label for="fn.11">11</label><span><sup>11</sup> This depends a little on our choice of data
structure for the cache as well as the type of side effect object we choose for
keeping the mutable variable around. Some combinations will not allow streaming
results, and they tend to have better behaviour and performance characteristics
for finite inputs.</span>
</p>

<p><label>In[46]:</label></p><div>
<pre><span>type</span> <span>FactorCache</span> <span>=</span> <span>M.Map</span> <span>Int</span> <span>Int</span>

<span>factorise</span> <span>::</span> [<span>Int</span>] <span>-&gt;</span> <span>State</span> <span>FactorCache</span> [(<span>Int</span>, <span>Maybe</span> <span>Int</span>)]
<span>factorise</span> numbers <span>=</span>
  for numbers <span>$</span> <span>\</span>n <span>-&gt;</span>
    <span>let</span>
      <span>-- </span><span>Rely on laziness to not compute this value other than</span>
      <span>-- </span><span>where it is used, which is only when the cache is dry.</span>
      computed <span>=</span> greatest_factor n
    <span>in</span> <span>do</span>
      cached <span>&lt;-</span> gets (M.lookup n)
      factor <span>&lt;-</span> maybe (modify (M.insert n computed) <span>$&gt;</span> computed)
        pure cached
      pure (n, guard (factor <span>/=</span> n) <span>$&gt;</span> factor)
</pre>
</div>

<p>
Since we are no longer using <code>IO</code>, we cannot extract the result from this side
effectful function the way we are used to. Instead, can convert its result to a
pure value with the <code>evalState</code> function, seeding the state with an empty
dictionary:
</p>

<p><label>In[47]:</label></p><div>
<pre><span>factored</span> <span>::</span> [(<span>Int</span>, <span>Maybe</span> <span>Int</span>)]
<span>factored</span> <span>=</span> evalState (factorise some_numbers) M.empty
</pre>
</div>

<p>
The result here is a pure, streaming list, meaning we can feed this function an
infinite sequence of numbers and it will keep categorising them – although it
will go faster and faster as its cache fills up.
</p>

<p>
This also illustrates another reason to go for something like <code>State</code> rather
than <code>IO</code>: we can guarantee that any side effects executed as part of a <code>State</code>
object are only visible within that <code>State</code> object, which in turn means we can
convert the side effectful function to a pure function. Haskell is happy to let
us have side effects inside pure functions – as long as we can prove to the
compiler that the side effects will not affect anything outside of those
functions.<label for="fn.12">12</label><span><sup>12</sup> There are many of these specialised side effect types in Haskell
which we can use to get local side effects but which look pure from the
outside.</span>
</p>

<p>
Since <code>numbers</code> is given as an argument to this function and then immediately
used as an argument to <code>for</code>, I’d be tempted at this point to actually go back
to <code>traverse</code> and eta reduce, resulting in the following definition.
</p>

<p><label>In[48]:</label></p><div>
<pre><span>factorise</span> <span>::</span> [<span>Int</span>] <span>-&gt;</span> <span>State</span> <span>FactorCache</span> [(<span>Int</span>, <span>Maybe</span> <span>Int</span>)]
<span>factorise</span> <span>=</span> traverse <span>$</span> <span>\</span>n <span>-&gt;</span>
  <span>let</span>
    computed <span>=</span> greatest_factor n
  <span>in</span> <span>do</span>
    cached <span>&lt;-</span> gets (M.lookup n)
    factor <span>&lt;-</span> maybe (modify (M.insert n computed) <span>$&gt;</span> computed)
      pure cached
    pure (n, guard (factor <span>/=</span> n) <span>$&gt;</span> factor)
</pre>
</div>

<p>
This no longer looks anything like procedural code in other languages. This is
procedural on a higher level: it’s defining a stateful traversal. A stateful
traversal is a distinctly procedural operation, but most other procedural
languages don’t have support for defining stateful traversals. Haskell does,
because Haskell is the greatest procedural language in the world.
</p>
</div>
</section>
<section id="outline-container-things-you-never-need-to-care-about">
<h2 id="things-you-never-need-to-care-about">Things you never need to care about</h2>
<div id="text-org6e7b6ae">
<p>
There are some historic functions you might run across in this context. You
never need these, and you can almost always translate them into their modern,
better equivalents.
</p>

<ul>
<li><code>&gt;&gt;</code> is an old name for <code>*&gt;</code>.</li>
<li><code>return</code> is an old name for <code>pure</code>.</li>
<li><code>map</code> and <code>liftM</code> are old names for <code>fmap</code>.</li>
<li><code>liftM2</code>, <code>liftM3</code>, etc. are old names for <code>liftA2</code>, <code>liftA3</code>, etc.</li>
<li><code>ap</code> is an old name for the <code>&lt;*&gt;</code> operator.</li>
<li><code>msum</code> is an old name for <code>asum</code>.</li>
<li><code>sequence</code> and <code>sequence_</code> are old names for <code>sequenceA</code> and <code>sequenceA_</code>.</li>
<li><code>mapM</code> and <code>mapM_</code> are old names for <code>traverse</code> and <code>traverse_</code>.</li>
<li><code>forM</code> and <code>forM_</code> are old names for <code>for</code> and <code>for_</code>.</li>
</ul>

<p>
Back in the dark ages of the 1990s, we thought monads were the greatest thing
since sorting machines for punch cards. It turns out it is possible extract a
highly useful subset of monads, called <i>applicative functors</i>, and they are
responsible for many of the cool things we did with monads. These old names
generally come from the time before we learned about applicative functors.
</p>
</div>
</section>
<section id="outline-container-appendix-a--avoiding-success-and-uselessness">
<h2 id="appendix-a--avoiding-success-and-uselessness">Appendix A: Avoiding success and uselessness</h2>
<div id="text-org1320073">
<p>
It is sometimes said the Haskell community is ready to do anything to avoid
success. This is a misreading of “avoid success at all costs”. What the phrase
really means is that some languages are ready to sacrifice their values to
become more successful – they aim for success at all costs – and Haskell does
not. It prefers to stick to its values rather than chucking them out for a
moment of fame.
</p>

<p>
“Haskell is useless” was said in a discussion on how different programming
languages approach giving authority to the programmer. At the time it was said,
the popular approach to programming language design was to assume that the
programmer was always right, good, and should be given maximum authority over
their code.<label for="fn.13">13</label><span><sup>13</sup> Think <abbr>php</abbr>, Ruby, Python 2, JavaScript.</span> It has long been
recognised that this sort of freedom often leads to programmers shooting
themselves in the foot, so languages restrict this flexibility by adding
prohibitions into their design. The idea is “you are allowed to do everything
except X, Y, Z.” Some examples:
</p>

<ul>
<li>In C, we can no longer jump to abitrary addresses like we could in
assembly.<label for="fn.14">14</label><span><sup>14</sup> Popular C compilers do have support for this, but it’s not in
the language standard.</span></li>
<li>In Ada, we can no longer think of a pointer as a trenchcoated integer.</li>
<li>In Java, we are no longer allowed to manually deallocate memory.</li>
<li>In Python 3, we can no longer treat a sequence of bytes as text.</li>
</ul>

<p>
With every generation, more restrictions are piled onto previous ones, making
the language safer, but also less powerful – or, some would say, less useful.
</p>

<p>
Haskell approached this from the other direction. It started by saying no to
arbitrary side effects: when computing a return value, a function is only
allowed to rely on its arguments, nothing else. Functions cannot read from the
system environment, and certainly not write to it. This eliminates a large class
of safety problems, but it also makes the language completely useless for almost
everything practical.
</p>

<p>
Then Haskell went on and found a way to allow code to describe side effects
without actually executing them in such a way that they can be executed in the
right order by an external runtime, and Haskell gained some usefulness. This is
in contrast to other languages that started out useful but lost some of their
usefulness with time, in the name of safety: Haskell started out useless, but
has since gained usefulness.
</p>
</div>
</section>
<section id="outline-container-appendix-b--why-fmap-maps-over-both-side-effects-and-lists">
<h2 id="appendix-b--why-fmap-maps-over-both-side-effects-and-lists">Appendix B: Why fmap maps over both side effects and lists</h2>
<div id="text-org78141c3">
<p>
We have already seen that
</p>

<p><label>In[49]:</label></p><div>
<pre><span>fmap</span> <span>::</span> (a <span>-&gt;</span> b) <span>-&gt;</span> <span>IO</span> a <span>-&gt;</span> <span>IO</span> b
</pre>
</div>

<p>
but it is <i>equally true</i> that
</p>

<p><label>In[50]:</label></p><div>
<pre><span>fmap</span> <span>::</span> (a <span>-&gt;</span> b) <span>-&gt;</span> [a] <span>-&gt;</span> [b]
</pre>
</div>

<p>
I.e. <code>fmap</code> can take a pure function and apply it to all elements of a list.
</p>

<p>
The way it works is that <code>fmap</code> is not written to work specifically with side
effects, or lists, or anything else. It is written to work against anything that
is a functor. Its most general type signature is actually
</p>

<p><label>In[51]:</label></p><div>
<pre><span>fmap</span> <span>::</span> <span>Functor</span> f <span>=&gt;</span> (a <span>-&gt;</span> b) <span>-&gt;</span> f a <span>-&gt;</span> f b
</pre>
</div>

<p>
The name <i>functor</i> is a bit nondescriptive, but we can think of it as the name
for all types that support being “mapped over”, in some sense.
</p>

<ul>
<li>Mapping over a side effect object means transforming the result it produces.</li>
<li>Mapping over a list means transforming each element of the list.</li>
<li>Mapping over a nullable value means transforming the value of it, if it exists,
otherwise doing nothing.</li>
</ul>

<p>
Lists, side effect objects, and nullable values are, therefore, functors. Thus,
they all support the <code>fmap</code> operation.
</p>
</div>
</section>
<section id="outline-container-appendix-c--foldable-and-traversable">
<h2 id="appendix-c--foldable-and-traversable">Appendix C: Foldable and Traversable</h2>
<div id="text-orga432767">
<p>
The result-preserving variants of the functions we’ve discussed – <code>sequenceA</code>,
<code>traverse</code>, etc. – guarantee that the data structure produced by side effect
they create will have the same shape as the original data structure we passed
in. This is easy e.g. in the case of a list, because given the same number of
items, the list has the same shape regardless of what we put into it. However,
for some data structures, this is a more severe requirement. Typical examples
are sets and search trees: their structure depends on their contents. When we
<code>traverse</code> a search tree and construct a new search tree with the values
produced by the side effect, we might end up getting a completely different tree
shape than what we started from.
</p>

<p>
In contrast, if we are not interested in getting back the values produced by
side effects, we don’t need to be able to reconstruct the same data structure we
started with. All that takes is being able to iterate the elements of the data
structure, and this can be supported by more data structures – including sets
and search trees. This is why <code>sequenceA_</code>, <code>traverse_</code>, and friends can work
with more collection types.
</p>

<p>
One hack if we want the values produced by the side effect, but do not care
about the structure, is to first convert the collection to a list (this only
requires being able to iterate the collection) and then call <code>traverse</code> on that
list. Then we have retained the results, but lost the original structure of the
collection. Such is life if we work with structures that do not support traversal.
</p>

<p>
The technical names here are <i>foldable</i> (can produce elements one at a time in a
specified order) and <i>traversable</i> (can reconstruct its structure even when
given new elements.) All traversables are foldable, but not all foldables are
traversable.
</p>
</div>
</section>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Forgejo: A self-hosted lightweight software forge (223 pts)]]></title>
            <link>https://forgejo.org/</link>
            <guid>42753523</guid>
            <pubDate>Sun, 19 Jan 2025 04:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forgejo.org/">https://forgejo.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42753523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <div> <hgroup>  <p>
Beyond coding.
<span>We forge.</span> </p> </hgroup> <div> <picture> <source srcset="
								https://forgejo.org/_astro/mascot-dark.1omhhgvT_1wP5Ng.webp 407w,
								https://forgejo.org/_astro/mascot-dark.1omhhgvT_FT0Ui.webp 814w,
								https://forgejo.org/_astro/mascot-dark.1omhhgvT_1RR3an.webp 1629w,
							" type="image/webp" sizes="(max-width: 767px) 407px, (max-width: 1479px) 814px, 1629px"> <source srcset="
								https://forgejo.org/_astro/mascot-dark.1omhhgvT_Z1a8nnm.png 407w,
								https://forgejo.org/_astro/mascot-dark.1omhhgvT_Z214sgk.png 814w,
								https://forgejo.org/_astro/mascot-dark.1omhhgvT_ZAasCY.png 1629w,
							" type="image/png" sizes="(max-width: 767px) 407px, (max-width: 1479px) 814px, 1629px"> <img src="https://forgejo.org/_astro/mascot-dark.1omhhgvT_Zm0N2n.webp" alt="" loading="eager" width="1629" height="1273" decoding="async"> </picture> </div> <div> <p> <span>Forgejo</span>
is a self-hosted lightweight software forge.<br>
Easy to install and low maintenance, it just does the job.
</p> <p>
Brought to you by an inclusive community under the umbrella of
<a href="https://docs.codeberg.org/getting-started/what-is-codeberg/#what-is-codeberg-e.v.%3F">Codeberg e.V.</a>, a democratic non-profit organization, Forgejo can be trusted to be exclusively Free Software. You can
						create an account on
<a href="https://codeberg.org/">Codeberg</a>
and
<a href="https://codeberg.org/forgejo-contrib/delightful-forgejo#public-instances">other instances</a>
or download it to self-host your own. It focuses on security, scaling, federation and privacy. Learn more about
<a href="https://forgejo.org/compare/">how it compares with other forges</a>.
</p>  </div> </div> <div id="features"> <div> <!-- <p class="text-base mb-4 text-primary-600 dark:text-steel-200 font-semibold tracking-wide uppercase">
				Highlights
			</p> --> <h2>
Forge great software with Forgejo
</h2> <p>
Take back control of your software development process, self-host your projects and get everyone involved in
				delivering quality software on the same page.
</p> </div> <div> <div> <div> <h3> Simple software project management </h3> <p><strong>Ease of use</strong> is important to get things done efficiently. Forgejo’s user experience is designed for <strong>collaboration</strong> and <strong>productivity</strong>.</p> </div><div> <h3> Self-hosted alternative to GitHub </h3> <p><strong>Liberate your software</strong> from proprietary shackles. Forgejo offers a familiar environment to GitHub users, allowing smooth transition to a <strong>platform you own</strong>.</p> </div><div> <h3> Easy to install and maintain </h3> <p>Hosting your own software forge does not require expert skills. With Forgejo you can control your server with <strong>minimal effort</strong>.</p> </div> </div><div> <div> <h3> Lightweight and performant </h3> <p>With a <strong>rich feature set</strong>, Forgejo still has a <strong>low server profile</strong> and requires <strong>an order of magnitude less resources</strong> than other forges.</p> </div><div> <h3> Guaranteed 100% Free Software </h3> <p>Forgejo will always be <strong>Free and Open Source Software</strong>. Furthermore we exclusively use Free Software for our own project development.</p> </div><div> <h3> Beyond coding, we forge ahead </h3> <p>An exciting future awaits. We will innovate the Software Forge and enable <strong>collaborative</strong> software development facilitated by <strong>decentralized</strong> platforms.</p> </div> </div> </div> </div> <div> <h2>
Get Involved
</h2> <p> <strong>
Forgejo consists of motivated people, and we are looking forward to
<a href="https://forgejo.org/docs/next/contributor/">your contribution</a> </strong>.<br>
Feel free to help in the domains of
<a href="https://forgejo.org/docs/next/contributor/localization/">localization</a>,
<a href="https://codeberg.org/forgejo/forgejo/issues">code, federation, releases management</a>,
<a href="https://codeberg.org/forgejo/user-research/">user research</a>,
<a href="https://codeberg.org/forgejo/design">UX and usability</a>,
<a href="https://codeberg.org/forgejo/code-of-conduct/issues">community management</a>,
<a href="https://codeberg.org/forgejo/docs/issues">documentation</a>,
<a href="https://codeberg.org/forgejo/website/issues">web design</a>,
<a href="https://codeberg.org/forgejo/governance/issues">governance</a> and more.
</p> <p> <a href="https://codeberg.org/forgejo/forgejo" target="_blank" rel="noopener"> <svg width="1em" height="1em" viewBox="0 0 24 24" data-icon="tabler:git-merge">  <use xlink:href="#ai:tabler:git-merge"></use>  </svg> Contribute on Codeberg
</a> <a href="https://liberapay.com/forgejo" target="_blank" rel="noopener"> <svg width="1em" height="1em" viewBox="0 0 24 24" data-icon="mdi:heart">  <symbol id="ai:mdi:heart"><path fill="currentColor" d="m12 21.35l-1.45-1.32C5.4 15.36 2 12.27 2 8.5C2 5.41 4.42 3 7.5 3c1.74 0 3.41.81 4.5 2.08C13.09 3.81 14.76 3 16.5 3C19.58 3 22 5.41 22 8.5c0 3.77-3.4 6.86-8.55 11.53z"></path></symbol><use xlink:href="#ai:mdi:heart"></use>  </svg> Donate
</a> </p> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TikTok goes dark in the US (905 pts)]]></title>
            <link>https://techcrunch.com/2025/01/18/tiktok-goes-dark-in-the-u-s/</link>
            <guid>42753396</guid>
            <pubDate>Sun, 19 Jan 2025 03:51:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/18/tiktok-goes-dark-in-the-u-s/">https://techcrunch.com/2025/01/18/tiktok-goes-dark-in-the-u-s/</a>, See on <a href="https://news.ycombinator.com/item?id=42753396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<div>
			<div>
<p id="speakable-summary">TikTok has gone dark in the U.S., the result of a federal law that bans the popular short-form video app for millions of Americans — at least for now.</p>

<p>TikTok users began receiving a message about the ban around 10:30 p.m. Eastern. As of Saturday evening, the app was also no longer available in the Apple or Google Play app store.</p>







<p>“Sorry, TikTok isn’t available right now,” the message reads. “A law banning TikTok has been enacted in the U.S. Unfortunately, that means you can’t use TikTok for now.”</p>

<p>The message also suggests this may only be a temporary disappearance. Tiktok credits President-elect Donald Trump for indicating “he will work with us on a solution to reinstate TikTok once he takes office,” with users urged to “stay tuned!”</p>

<figure><img loading="lazy" decoding="async" width="1796" height="1264" src="https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?w=680" alt="" srcset="https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png 1796w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=150,106 150w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=300,211 300w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=768,541 768w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=680,479 680w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=1200,845 1200w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=1280,901 1280w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=430,303 430w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=720,507 720w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=900,633 900w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=800,563 800w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=1536,1081 1536w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=668,470 668w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=533,375 533w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=877,617 877w, https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2025-01-18-at-8.44.16PM.png?resize=708,498 708w" sizes="auto, (max-width: 1796px) 100vw, 1796px"></figure>

<p>The company warned earlier this week the app’s disappearance was imminent, <a href="https://techcrunch.com/2025/01/18/tiktok-says-it-will-go-dark-sunday-unless-biden-offers-definitive-statement/">saying Friday that it would “go dark”</a> unless President Joe Biden’s administration made a “definitive statement” that it wouldn’t enforce the ban.</p>

<p>Biden signed the law in April, requiring TikTok’s owner ByteDance to sell the app or see it banned in the United States, due to concerns over potential Chinese surveillance and propaganda. And while efforts to force ByteDance to divest go back to Trump’s first administration, he has taken a very different tone recently. Trump <a href="https://techcrunch.com/2024/12/28/trump-asks-supreme-court-to-pause-imminent-tiktok-ban/">asked the Supreme Court to delay the ban</a> and said he would <a rel="nofollow" href="https://www.nbcnews.com/politics/donald-trump/trump-likely-give-tiktok-90-day-extension-avoid-ban-rcna188258">“most likely”</a> give the company a 90-day extension.</p>

<p>And while the Supreme Court issued a ruling <a href="https://techcrunch.com/2025/01/17/supreme-court-upholds-tiktok-ban/">upholding the law Friday</a>, the Biden administration seemed inclined to<a href="https://techcrunch.com/2025/01/18/tiktok-says-it-will-go-dark-sunday-unless-biden-offers-definitive-statement/"> leave the app’s fate in the hands of the next president</a>. White House Press Secretary Karine Jean-Pierre noted that with Sunday being Biden’s last day in office, “actions to implement the law simply must fall to the next Administration.” Deputy Attorney General Lisa Monaco issuing a similar statement that “the next phase of this effort — implementing and ensuring compliance with the law after it goes into effect on January 19 — will be a process that plays out over time.”</p>


<p>TikTok, however, suggested that this was not enough to assurance for “critical service providers” to continue listing or hosting the app in the US, unless the Biden administration made the aforementioned “definitive statement.” Jean-Pierre called TikTok’s response “a stunt” and claimed there’s “no reason for TikTok or other companies to take actions in the next few days before the Trump administration takes office on Monday.”</p>

<p>Stunt or not, TikTok is gone for now.</p>
</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
			
<div>
	
	
	
	

	
<div>
	<p>
		Kyle Wiggers is a senior reporter at TechCrunch with a special interest in artificial intelligence. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Brooklyn with his partner, a piano educator, and dabbles in piano himself. occasionally — if mostly unsuccessfully.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/kyle-wiggers/" data-event="button" href="https://techcrunch.com/author/kyle-wiggers/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>
			
<div>
	
	
	
	

	
<div>
	<p>Anthony Ha is TechCrunch’s weekend editor. Previously, he worked as a tech reporter at Adweek, a senior editor at VentureBeat, a local government reporter at the Hollister Free Lance, and vice president of content at a VC firm. He lives in New York City.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/anthony-ha/" data-event="button" href="https://techcrunch.com/author/anthony-ha/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>
	</div>


			


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div><div>
		<div>
	<div>
		<div>
			<h3>Newsletters</h3>
			
		</div>
		<p>Subscribe for the industry’s biggest tech news</p>
	</div>
	<form method="POST" action="/">
		
	</form>
	
</div>


		
		<h2>Related</h2>
		

		
		
		

		
		<div>

<h2>Latest in Social</h2>




</div>
		

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yek: Serialize your code repo (or part of it) to feed into any LLM (107 pts)]]></title>
            <link>https://github.com/bodo-run/yek</link>
            <guid>42753302</guid>
            <pubDate>Sun, 19 Jan 2025 03:24:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bodo-run/yek">https://github.com/bodo-run/yek</a>, See on <a href="https://news.ycombinator.com/item?id=42753302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>yek</code></h2><a id="user-content-yek" aria-label="Permalink: yek" href="#yek"></a></p>
<p dir="auto">A <a href="#performance">fast</a> Rust based tool to read text-based files in a repository or directory, chunk them, and serialize them for LLM consumption. By default, the tool:</p>
<ul dir="auto">
<li>Uses <code>.gitignore</code> rules to skip unwanted files.</li>
<li>Uses the Git history to infer what files are important.</li>
<li>Infers additional ignore patterns (binary, large, etc.).</li>
<li>Splits content into chunks based on either approximate "token" count or byte size.</li>
<li>Automatically detects if output is being piped and streams content instead of writing to files.</li>
<li>Supports processing multiple directories in a single command.</li>
<li>Configurable via a <code>yek.toml</code> file.</li>
</ul>
<p dir="auto">Yek (<a href="https://fa.wikipedia.org/wiki/%DB%B1" rel="nofollow">يک</a>) means "One" in Farsi/Persian.</p>
<p dir="auto">Consider having a simple repo like this:</p>
<div data-snippet-clipboard-copy-content=".
├── README.md
├── src
│   ├── main.rs
│   └── utils.rs
└── tests
    └── test.rs"><pre><code>.
├── README.md
├── src
│   ├── main.rs
│   └── utils.rs
└── tests
    └── test.rs
</code></pre></div>
<p dir="auto">Running <code>yek</code> in this directory will produce a single file and write it to the temp directory with the following content:</p>
<div dir="auto" data-snippet-clipboard-copy-content=">>>> README.md
... content of README.md ...
>>>> tests/test.rs
... content of tests/test.rs ...
>>>> src/utils.rs
... content of src/utils.rs ...
>>>> src/main.rs
... rest of the file ..."><pre>&gt;&gt;&gt;&gt; README.md
... content of README.md ...
&gt;&gt;&gt;&gt; tests/test.rs
... content of tests/test.rs ...
&gt;&gt;&gt;&gt; src/utils.rs
... content of src/utils.rs ...
&gt;&gt;&gt;&gt; src/main.rs
... rest of the file ...</pre></div>
<blockquote>
<p dir="auto"><code>yek</code> will prioritize more important files to come last in the output. This is useful for LLM consumption.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">For Unix-like systems (macOS, Linux):</p>

<div dir="auto" data-snippet-clipboard-copy-content="curl -fsSL https://bodo.run/yek.sh | bash"><pre>curl -fsSL https://bodo.run/yek.sh <span>|</span> bash</pre></div>

<p dir="auto">For Windows (PowerShell):</p>

<div dir="auto" data-snippet-clipboard-copy-content="irm https://bodo.run/yek.ps1 | iex"><pre>irm https:<span>//</span>bodo.run<span>/</span>yek.ps1 <span>|</span> iex</pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<ol dir="auto">
<li><a href="https://www.rust-lang.org/tools/install" rel="nofollow">Install Rust</a>.</li>
<li>Clone this repository.</li>
<li>Run <code>make macos</code> or <code>make linux</code> to build for your platform (both run <code>cargo build --release</code>).</li>
<li>Add to your PATH:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="export PATH=$(pwd)/target/release:$PATH"><pre><span>export</span> PATH=<span><span>$(</span>pwd<span>)</span></span>/target/release:<span>$PATH</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><code>yek</code> has sensible defaults, you can simply run <code>yek</code> in a directory to serialize the entire repository. It will serialize all files in the repository into chunks of 10MB by default. The file will be written to the temp directory and file path will be printed to the console.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Process current directory and write to temp directory:</p>

<p dir="auto">Pipe output to clipboard (macOS):</p>

<p dir="auto">Cap the max size to 128K tokens and only process the <code>src</code> directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="yek --max-size 128K --tokens src/"><pre>yek --max-size 128K --tokens src/</pre></div>
<p dir="auto">Cap the max size to 100KB and only process the <code>src</code> directory, writing to a specific directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="yek --max-size 100KB --output-dir /tmp/yek src/"><pre>yek --max-size 100KB --output-dir /tmp/yek src/</pre></div>
<p dir="auto">Process multiple directories:</p>

<p dir="auto">Process multiple repositories:</p>
<div dir="auto" data-snippet-clipboard-copy-content="yek ~/code/project1 ~/code/project2"><pre>yek <span>~</span>/code/project1 <span>~</span>/code/project2</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Help</h3><a id="user-content-help" aria-label="Permalink: Help" href="#help"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="yek --help

Repository content chunker and serializer for LLM consumption

Usage: yek [OPTIONS] [directories]...

Arguments:
  [directories]...  Directories to process [default: .]

Options:
      --max-size <max-size>      Maximum size per chunk (e.g. '10MB', '128KB', '1GB') [default: 10MB]
      --tokens                   Count size in tokens instead of bytes
      --debug                    Enable debug output
      --output-dir <output-dir>  Output directory for chunks
  -h, --help                     Print help
  -V, --version                  Print version"><pre>yek --help

Repository content chunker and serializer <span>for</span> LLM consumption

Usage: yek [OPTIONS] [directories]...

Arguments:
  [directories]...  Directories to process [default: .]

Options:
      --max-size <span>&lt;</span>max-size<span>&gt;</span>      Maximum size per chunk (e.g. <span><span>'</span>10MB<span>'</span></span>, <span><span>'</span>128KB<span>'</span></span>, <span><span>'</span>1GB<span>'</span></span>) [default: 10MB]
      --tokens                   Count size <span>in</span> tokens instead of bytes
      --debug                    Enable debug output
      --output-dir <span>&lt;</span>output-dir<span>&gt;</span>  Output directory <span>for</span> chunks
  -h, --help                     Print <span>help</span>
  -V, --version                  Print version</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration File</h2><a id="user-content-configuration-file" aria-label="Permalink: Configuration File" href="#configuration-file"></a></p>
<p dir="auto">You can place a file called <code>yek.toml</code> at your project root or pass a custom path via <code>--config</code>. The configuration file allows you to:</p>
<ol dir="auto">
<li>Add custom ignore patterns</li>
<li>Define file priority rules for processing order</li>
<li>Add additional binary file extensions to ignore (extends the built-in list)</li>
<li>Configure Git-based priority boost</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example <code>yek.toml</code></h3><a id="user-content-example-yektoml" aria-label="Permalink: Example yek.toml" href="#example-yektoml"></a></p>
<p dir="auto">This is optional, you can configure the <code>yek.toml</code> file at the root of your project.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add patterns to ignore (in addition to .gitignore)
[ignore_patterns]
patterns = [
  &quot;node_modules/&quot;,
  &quot;\\.next/&quot;,
  &quot;my_custom_folder/&quot;
]

# Configure Git-based priority boost (optional)
git_boost_max = 50  # Maximum score boost based on Git history (default: 100)

# Define priority rules for processing order
# Higher scores are processed first
[[priority_rules]]
score = 100
patterns = [&quot;^src/lib/&quot;]

[[priority_rules]]
score = 90
patterns = [&quot;^src/&quot;]

[[priority_rules]]
score = 80
patterns = [&quot;^docs/&quot;]

# Add additional binary file extensions to ignore
# These extend the built-in list (.jpg, .png, .exe, etc.)
binary_extensions = [
  &quot;.blend&quot;,  # Blender files
  &quot;.fbx&quot;,    # 3D model files
  &quot;.max&quot;,    # 3ds Max files
  &quot;.psd&quot;,    # Photoshop files
]"><pre><span><span>#</span> Add patterns to ignore (in addition to .gitignore)</span>
[<span>ignore_patterns</span>]
<span>patterns</span> = [
  <span><span>"</span>node_modules/<span>"</span></span>,
  <span><span>"</span><span>\\</span>.next/<span>"</span></span>,
  <span><span>"</span>my_custom_folder/<span>"</span></span>
]

<span><span>#</span> Configure Git-based priority boost (optional)</span>
<span>git_boost_max</span> = <span>50</span>  <span><span>#</span> Maximum score boost based on Git history (default: 100)</span>

<span><span>#</span> Define priority rules for processing order</span>
<span><span>#</span> Higher scores are processed first</span>
[[<span>priority_rules</span>]]
<span>score</span> = <span>100</span>
<span>patterns</span> = [<span><span>"</span>^src/lib/<span>"</span></span>]

[[<span>priority_rules</span>]]
<span>score</span> = <span>90</span>
<span>patterns</span> = [<span><span>"</span>^src/<span>"</span></span>]

[[<span>priority_rules</span>]]
<span>score</span> = <span>80</span>
<span>patterns</span> = [<span><span>"</span>^docs/<span>"</span></span>]

<span><span>#</span> Add additional binary file extensions to ignore</span>
<span><span>#</span> These extend the built-in list (.jpg, .png, .exe, etc.)</span>
<span>binary_extensions</span> = [
  <span><span>"</span>.blend<span>"</span></span>,  <span><span>#</span> Blender files</span>
  <span><span>"</span>.fbx<span>"</span></span>,    <span><span>#</span> 3D model files</span>
  <span><span>"</span>.max<span>"</span></span>,    <span><span>#</span> 3ds Max files</span>
  <span><span>"</span>.psd<span>"</span></span>,    <span><span>#</span> Photoshop files</span>
]</pre></div>
<p dir="auto">All configuration keys are optional. By default:</p>
<ul dir="auto">
<li>No extra ignore patterns</li>
<li>All files have equal priority (score: 1)</li>
<li>Git-based priority boost maximum is 100</li>
<li>Common binary file extensions are ignored (.jpg, .png, .exe, etc. - see source for full list)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto"><code>yek</code> is fast. It's written in Rust and does many things in parallel to speed up processing.</p>
<p dir="auto">Here is a benchmark comparing it to <a href="https://github.com/jxnl/repomix">Repomix</a> serializing the <a href="https://github.com/vercel/next.js">Next.js</a> project:</p>
<div dir="auto" data-snippet-clipboard-copy-content="time yek
Executed in    5.19 secs    fish           external
   usr time    2.85 secs   54.00 micros    2.85 secs
   sys time    6.31 secs  629.00 micros    6.31 secs"><pre><span>time</span> yek
Executed <span>in</span>    5.19 secs    fish           external
   usr <span>time</span>    2.85 secs   54.00 micros    2.85 secs
   sys <span>time</span>    6.31 secs  629.00 micros    6.31 secs</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="time repomix
Executed in   22.24 mins    fish           external
   usr time   21.99 mins    0.18 millis   21.99 mins
   sys time    0.23 mins    1.72 millis    0.23 mins"><pre><span>time</span> repomix
Executed <span>in</span>   22.24 mins    fish           external
   usr <span>time</span>   21.99 mins    0.18 millis   21.99 mins
   sys <span>time</span>    0.23 mins    1.72 millis    0.23 mins</pre></div>
<p dir="auto"><code>yek</code> is <strong>230x faster</strong> than <code>repomix</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">See <a href="https://github.com/bodo-run/yek/issues?q=type:%22Feature%22">proposed features</a>. I am open to accepting new feature requests. Please write a detailed proposal to discuss new features.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Unix spell ran in 64kb RAM (175 pts)]]></title>
            <link>https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram</link>
            <guid>42752604</guid>
            <pubDate>Sun, 19 Jan 2025 00:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram">https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram</a>, See on <a href="https://news.ycombinator.com/item?id=42752604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>How do you fit a 250kB dictionary in 64kB of RAM and still perform fast lookups? For reference, even with modern compression techniques like gzip -9, you can't compress this file below 85kB.</p><div><p><span>In the 1970s, Douglas McIlroy faced this exact challenge while implementing the spell checker for Unix at AT&amp;T. The constraints of the PDP-11 computer meant the entire dictionary needed to fit in just 64kB of RAM. A seemingly impossible task.</span></p><p><span>Instead of relying on generic compression techniques, he took advantage of the properties of the data and developed a compression algorithm that came within 0.03 bits of the theoretical limit of possible compression. To this day, it remains unbeaten. </span></p></div><p>The story of Unix spell is more than just historical curiosity. It's a masterclass in engineering under constraints: how to analyze a problem from first principles, leverage mathematical insights, and design elegant solutions that work within strict resource limits.</p><p>If you're short on time, here's the key engineering story:</p><ul><li><p>The Unix spell started in the 1970s as an afternoon prototype by Steve Johnson at AT&amp;T, before Douglas McIlroy rewrote it to improve its performance and accuracy.</p></li><li><p>McIlroy's first innovation was a clever linguistics-based stemming algorithm that reduced the dictionary to just 25,000 words while improving accuracy.</p></li><li><p>For fast lookups, he initially used a Bloom filter—perhaps one of its first production uses. Interestingly, Dennis Ritchie provided the implementation. They tuned it to have such a low false positive rate that they could skip actual dictionary lookups.</p></li><li><p>When the dictionary grew to 30,000 words, the Bloom filter approach became impractical, leading to innovative hash compression techniques.</p></li><li><p>They computed that 27-bit hash codes would keep collision probability acceptably low, but needed compression.</p></li><li><p>McIlroy's solution was to store differences between sorted hash codes, after discovering these differences followed a geometric distribution.</p></li><li><p>Using Golomb's code, a compression scheme designed for geometric distributions, he achieved 13.60 bits per word—remarkably close to the theoretical minimum of 13.57 bits.</p></li><li><p>Finally, he partitioned the compressed data to speed up lookups, trading a small memory increase (final size ~14 bits per word) for significantly faster performance.</p></li></ul><p>The rest of the article expands each of these points and gives a detailed explanation with all the math and logic behind them.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg" width="800" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;A PDP-11 machine, source: Wikipedia&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="A PDP-11 machine, source: Wikipedia" title="A PDP-11 machine, source: Wikipedia" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce523ed3-b3aa-4cf3-b06a-4d67e70d838a_800x529.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>A PDP-11 machine, source: Wikipedia</figcaption></figure></div><p>In order to secure funding for Unix, Ken Thompson and Dennis Ritchie pitched Unix as a text processing system for the patents department to AT&amp;T. Naturally, a text processing system needed a spell checker as well. </p><p>The first version of Unix spell was written by Steve Johnson in 1975 which was  a prototype. Jon Bentley mentions that Steve wrote it in one afternoon. Even though it worked, it was not very accurate. </p><p>It was pretty simple. It would split the input file into a stream of words, do some light preprocessing such as remove numbers and special characters, convert to lower case, then sort, unique, and finally pass the list to the spell program which would simply check for the existence of those words in a dictionary on the disk.</p><p>Because of its simplistic implementation, it was not very accurate, and also slow because of dictionary lookups on the disk.</p><p>After seeing the adoption of the initial version, Douglas McIlroy took up the project to rewrite it with the goal of improving the accuracy and performance of the tool. He worked on two separate fronts both involving some very clever engineering:</p><ul><li><p>Building an affix removal algorithm for reducing words to their stems, and a compact dictionary consisting of the stem words</p></li><li><p>A compact data structure for loading the dictionary into memory for doing fast lookups</p></li></ul><p>This article is going to be focused on the data structure design part, but let’s spend a section to get an overview on the affix removal algorithm to see how it worked.</p><p>Using a full fledged dictionary for doing lookups was slow because the computers those days had only a few kilobytes of main memory and using disk based lookups was even more slower. </p><p>Douglas McIlroy came up with the idea of an algorithm which would iteratively remove common prefixes and suffixes from a word and look up a dictionary to see if the reduced word is present in it or not. The algorithm would follow the affix removal process until there were no affixes left to remove and if even after this the word was not present in the dictionary, then it would be flagged as a misspelling.</p><p>For instance, the algorithm would reduce the word “misrepresented” to “present” by removing the prefixes “mis”, “re”, and the suffix “ed”. And because “present” is a valid word in the dictionary, it would not flag it as a misspelling.</p><p>This affix removal technique was not 100% accurate and would sometimes let misspelled words pass through. But, such occurrences were deemed acceptable at that time. He also implemented a bunch of exceptions to these rules to avoid some of the common errors. </p><p>Overall, this algorithm resulted in a very compact dictionary. The final dictionary consisted of 25,000 words, which seemed possible to load into memory with a well engineered data structure.</p><p>Let’s move on to discussing how he managed to implement in-memory dictionary lookups with just 64 kB of memory.</p><div><p>Bloom published his work on Bloom filter in 1970 while the Unix spell was developed in the mid-1970s. At this time, Bloom filter was not even called Bloom filter. In his paper, Douglas calls it a “superimposed code scheme”. </p><p>Interestingly, the Bloom filter implementation he used was given to him by Dennis Ritchie.</p></div><p>Even though the dictionary size was 25,000 words, it was still not possible to load it as it is in just 64kB of RAM. Besides, it also needed fast lookups. </p><p><span>The first data structure that Douglas used was a Bloom filter. In the paper he doesn’t call it Bloom filter, instead he refers to it as a “superimposed coding scheme”, attributed to </span><a href="https://dl.acm.org/doi/pdf/10.1145/362686.362692" rel="">Bloom’s paper from 1970</a><span>. Interestingly, he gives the credit for the implementation of the Bloom filter he used to Dennis Ritchie.</span></p><p>A Bloom filter consists of a bit table initialized to all zeros. To add an item to the Bloom filter, you apply multiple hash functions to the item. Each hash function generates an index in the table, and that bit index is set to 1. If k hash functions are used, then, k different bit indices are turned on in the table.</p><blockquote><h5><em><span>For a more detailed explanation of Bloom filter, please check out </span><a href="https://blog.codingconfessions.com/p/bloom-filters-and-beyond" rel="">my article on Bloom filters</a><span>.</span></em></h5></blockquote><p><span>Looking up an item, whether it exists in the table or not, requires the same procedure. You need to apply the k hash functions, and for each of them check if the corresponding bit is set to 1 in the table or not. If even one of the bits is not on, then it means that the item is not present in the dataset. However, if all the bits are set, then it indicates that the item </span><em>might</em><span> be present, but this may also be a false positive. </span></p><p>False positives can occur because of hash collisions. When querying for an item, we cannot be 100% sure if a bit is on in the table because of the query item, or because of a hash collision with another item. </p><p>When using Bloom filter, you need to implement a strategy to handle false positives. For instance, in this case it could mean doing a full dictionary search. But that would defeat the whole purpose of using a Bloom filter, which was to save memory and do fast dictionary lookups. In the case of a spell checker, most of the words exist in the dictionary and only a fraction of words are misspelled, so we would be checking the full dictionary quite a lot.</p><p><span>However, a Bloom filter can be tuned to achieve a desired false positive rate. The following formula computes the false positive probability for a Bloom filter with a given size </span><code>n</code><span>, number of inserted items </span><code>m</code><span>, and number of hash functions </span><code>k</code><span>. </span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{q} \approx \left( 1 - e^{-\frac{kn}{m}} \right) \\
&amp;\text{where:} &amp; \\
&amp;\text{q}:\ \text{Probability that a bit is 1} \\
&amp;m: \ \text{Number of bits in the Bloom filter} \\
&amp;k: \ \text{Number of hash functions used} \\
&amp;n: \ \text{Number of elements inserted}
\end{align*}
\)</span></p></div><p>In his paper, Douglas mentions that a false positive probability of 1 in 2000 was acceptable to them, which meant that for such a low false positive rate, they did not need to consult the dictionary. </p><p>As they had a dictionary of 25,000 items, the number of items was fixed. They fixed the bit table size at 400,000 bits because of the limited amount of memory. Based on these factors, using 11 hash functions allowed them to keep the false positive rate at around 1/2000.</p><p>They used the Bloom filter based spell implementation for a while. In the paper, Douglas mentions that even though the false positive rate was acceptable, in the wild, they were encountering a lot of new words that needed to be added to the dictionary. This led to the dictionary size going up from 25,000 to 30,000.</p><p>However, for a dictionary of this size, their Bloom filter required a bigger bit table size which was not possible for them. As a result, Douglas looked for alternate data structure designs to be able to fit a dictionary of 30,000 words in memory with similar lookup performance and accuracy.</p><p>As the dictionary size exploded from 25,000 to 30,000, Douglas needed a more memory efficient data structure to hold the dictionary in memory. </p><p>A hash table was an attractive solution, but it would have consumed much more memory than a Bloom filter because it requires storing the hash, as well as, the actual words to handle collisions.</p><p>Instead of a full hash table, Douglas decided to store just the hashes of the words. The lookup required computing the hash of the input word, and then checking for its existence in the hashes using a scan. </p><p>One intuition for doing so might have been that the individual words can be of varying lengths, but a hash function will naturally compress them down to a fixed number of bits, and that may possibly allow them to fit the hashes in memory.</p><p>But hashes can collide, so they needed a large enough hash code to have an acceptably low probability of collisions.</p><p><span>If each word in the dictionary is hashed to a hash code of size </span><code>b</code><span> bits, then there are </span><code>2^b</code><span> total possible hash codes in that space. If the size of the dictionary is </span><code>v</code><span> words, then the probability of a hash collision can be computed as:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
\text{P(hash collision)} = \frac{v}{2^b}
\end{align*}\)</span></p></div><p>They had a dictionary of 30,000 words, which is ~2^15 words. Moreover, he mentions that a collision probability of 1 in 2^12 was acceptable to them. This gave a hash code size of 27 bits.</p><p>But 27-bit hash codes were too big: with 2^15 words, they needed 2^15 * 27 bits of memory, while the PDP-11 had only 2^15 * 16 bits (64kB) of RAM—compression was essential. </p><p>Before implementing any compression algorithm, we need to know what is the theoretical minimum number of bits we can achieve to compress this piece of data. It acts as a benchmark to tell us how well we are able to compress the data.</p><p><span>This theoretical minimum is computed using the information content of the event which generated the data we are trying to compress. This concept comes from </span><a href="https://en.wikipedia.org/wiki/Information_theory" rel="">information theory</a><span> which is the underpinning foundation for all of the data compression techniques. </span></p><p><span>The basic idea behind </span><a href="https://en.wikipedia.org/wiki/Information_content" rel="">information content</a><span> is to use the probability of an event to determine how many bits are needed to encode it without loss of information. </span></p><p>A highly likely event carries less information (for instance a 100% probable event has no information and needs 0 bits to encode), while a less probable event contains much more information and needs more bits. There is an inverse relationship between the probability of an event and its information content, which leads to the following formula:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{I(E)} = \log_2\left(\frac{1}{P(E)}\right) \\
&amp;\text{Or, I(E) = } -\log_2{P(E)} \\
&amp;\text{where P(E) is } \text{probability of event E}
\end{align*}\)</span></p></div><p>Now, to compute the information content of a set of hash codes, we need to figure out the probability of generating them.</p><p><span>If the size of a hash code is </span><code>b</code><span> bits, then there are a total of </span><code>2^b</code><span> possible hash codes in that space. Out of that, we are selecting a set of </span><code>v</code><span> unique hash codes. </span></p><div data-component-name="Latex"><p><span>\(

\begin{align*}


\text{The total number of ways to select such sets = }\binom{2^b}{v}
\end{align*}
\)</span></p></div><p>Therefore, the probability of any one of these sets of being generated is:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
\text{P} = 
\frac{1}{\binom{2^b}{v}}
\end{align*}
\)</span></p></div><p>Thus, the information content of these hash codes is:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;I(E) = - \log_2\left(\frac{1}{\binom{2^b}{v}}\right) \\
&amp;= \log_2\left(\binom{2^b}{v}\right) \\
&amp;= \log_2\left(\frac{(2^b)!}{v! \, (2^b - v)!}\right)
\end{align*}\)</span></p></div><p><span>To simplify things, we can use </span><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation" rel="">Stirling’s approximation</a><span>:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{Stirling's approximation: } \quad \log_2(n!) \approx n \log_2(n) - n \log_2(e), \\
\end{align*}
\)</span></p></div><p><span>The paper makes another simplifying assumption that the number of words in the dictionary (30,000) is much smaller than the total number of hash codes (2^27), i.e., </span><code>v « 2^b</code><span>, this allows them to simplify </span><code>(2^b - v)</code><span> as  </span><code>2^b</code><span> in the above computation. </span></p><p>Using these two approximations leads to the following formula for the information content:</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;I(E) \approx v\left[b - \log_2\left(\frac{v}{e}\right)\right]
\end{align*}\)</span></p></div><p><span>Plugging in </span><code>v=30,000</code><span> and </span><code>b=27</code><span>, the minimum number of bits needed to encode a single hash code turns out to be 13.57, which was ~50% shorter than the original hash codes, and within the capacity of the PDP-11’s memory.</span></p><p><span>At this point they knew how much compression they could achieve but the bigger question was how to get there. Instead of compressing the raw hash codes, what if they computed and stored the differences between successive hash codes (in their sorted order)? This is similar to how </span><a href="https://en.wikipedia.org/wiki/Delta_encoding" rel="">delta encoding</a><span> works, but not quite the same.</span></p><p>There were a couple of advantages of working with hash differences.</p><ul><li><p>By definition the differences were smaller than the raw hash codes</p></li><li><p>And many of the difference value would repeat because the difference of several hash codes might be the same. </p></li></ul><p>This implied that it was easier to compress these differences than the hash codes. </p><p>Hash differences were computed by sorting the hash codes and taking differences between consecutive values.</p><p>For instance:</p><pre><code>sorted hash codes: 5, 14, 21, 32, 55, 67
hash differences: 5, 9, 7, 11, 23, 12</code></pre><p>Let’s also see how the lookup of a word worked when they stored hash differences instead of the actual value.</p><p>To check if a word exists in the dictionary or not, they would compute the hash of the word and check for its existence in the dictionary via a simple algorithm. </p><pre><code>lookup(input_hashcode) -&gt; bool:
  sum = hash_differences[0]
  i = 1
  while True:
    sum += hash_differences[i]
    if sum == input_hashcode:
      return True
    if sum &gt; input_hashcode:
      return False
    i += 1</code></pre><p>Now, let’s discuss how they came up with a compression scheme for this data.</p><p>The basic principle behind lossless compression is to assign shorter codes to symbols with higher probabilities, and longer codes to symbols with lower probabilities. This makes sense because symbols with higher probabilities tend to occur more frequently in the data and assigning them shorter codes means higher compression rate.</p><p>But this requires computing the probability distribution of all the symbols in the data, and then using it to generate compressed codes. The probability distribution table is needed at decompression time as well to perform the decoding. </p><p>This had two problems for Douglas:</p><ul><li><p>Holding a probability distribution table for ~30,000 symbols in memory would have taken away any compression advantage he was getting from compression itself. So he needed a scheme which was free of this requirement.</p></li><li><p>Computing the probabilities of the hash differences would have been time expensive. All the 30,000 possible hash difference values, their sums and counts would not have been possible to keep in memory for computing their probabilities. So, it would have required an expensive disk based data structure to compute these probabilities.</p></li></ul><p>But McIlroy came up with an elegant solution by recognizing that the hash differences followed a geometric distribution, enabling an efficient compression scheme. Let’s first understand how these hash difference values are geometrically distributed.</p><p><span>The </span><a href="https://en.wikipedia.org/wiki/Geometric_distribution" rel="">geometric distribution</a><span> is a discrete probability distribution which is used to model scenarios where we conduct an experiment until we get a success. For instance, rolling a six-faced die until we get a “1” forms a geometric distribution, with the probability of success being 1/6. A simpler example is tossing a coin until we get a head. </span></p><p><span>If the probability of failure is </span><code>p</code><span>, the probability of success is </span><code>q</code><span>, and if success occurs in the </span><code>kth</code><span> trial, then the </span><a href="https://en.wikipedia.org/wiki/Probability_mass_function" rel="">probability mass function</a><span> of the geometric distribution is given by the following formula:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;P = p^kq
\end{align*}\)</span></p></div><p>Now, let's understand how the hash difference values map to this distribution.</p><p><span>As each hash code is </span><code>b</code><span> bits wide, we have a space of </span><code>2^b</code><span> points. And we have </span><code>v</code><span> hash codes spread out in this space. The probability of any point in this space containing a hash code is </span><code>q=v/2^b</code><span>, and the probability of a point being empty is </span><code>p=1-(v/2^b)</code><span>.</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{P(a point containing a hash code)} = q = \frac{v}{2^b} \\
&amp;\text{P(an empty point)} = p = 1 - \left(\frac{v}{2^b}\right)
\end{align*}\)</span></p></div><p><span>But we are interested in modelling the distribution of hash difference values, rather than the hash codes themselves. A hash difference </span><code>k</code><span> occurs when two consecutive hash values in the sorted sequence are </span><code>k</code><span> positions apart. For instance, if we have two successive hash code values 20 and 25, then the hash difference is 5.</span></p><p><span>What's the probability of seeing a hash difference of </span><code>k</code><span>? Given any hash value </span><code>h</code><span> in our space:</span></p><ul><li><p><span>We need the next </span><code>k-1</code><span> positions after </span><code>h</code><span> to be empty</span></p></li><li><p><span>And then we need a hash value at position </span><code>h+k</code></p></li><li><p><span>The probability of </span><code>k-1</code><span> empty positions is </span><code>p^(k-1)</code></p></li><li><p><span>The probability of a hash value at position </span><code>h+k</code><span> is </span><code>q</code></p></li></ul><p><span>Therefore, the probability of a hash difference of </span><code>k</code><span> is:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{P(difference=k) =} p^\left(k-1\right)q
\end{align*}\)</span></p></div><p>This follows exactly the form of a geometric distribution!</p><blockquote><h5><em>If you read the spell paper, you will find that the author takes a different route to arrive at this conclusion. He models the generation of the hash codes as a Poisson process and proceeds from there.</em></h5></blockquote><p><span>But, what is the point of modelling this as a geometric distribution? It turns out, there is a very simple and efficient </span><a href="https://en.wikipedia.org/wiki/Run-length_encoding" rel="">run-length encoding</a><span> scheme for geometrically distributed integers given by Golomb in his 1965 </span><a href="https://web.stanford.edu/class/ee398a/handouts/papers/Golomb%20-%20Run-Length%20Codes%20-%20IT66.pdf" rel="">paper</a><span>. Let’s see how it works.</span></p><p>Golomb’s code is a simple run-length encoding scheme for geometrically distributed integers which was used by Douglas to compress the hash differences. It takes advantage of the fact that geometrically distributed values have an exponentially decaying probability of success, which can be leveraged for performing compression.</p><p><span>What do we mean by that? Recall that in the case of geometric distribution, the probability of success after </span><code>k</code><span> trials is given as:</span></p><p><span>Let’s say, we find an integer </span><code>m</code><span>, such that </span><code>p^m = 1/2</code><span>. Then, it means that the probability of success after </span><code>k + m</code><span> trials is:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;\text{P(success after k + m trials)} = p^\left(k + m\right)q = \frac{1}{2}p^kq
\end{align*}\)</span></p></div><p><span>In other words, the probability of getting success in </span><code>k + m</code><span> trials is half of that of  the probability of getting a success in </span><code>m</code><span> trials. And this probability continues to half every </span><code>m</code><span> successive trials. This is an exponentially decaying distribution of probabilities.</span></p><p><span>Let's consider a fair coin toss example where we toss the coin until a head occurs. For a fair coin, </span><code>p = q = 1/2</code><span>.</span></p><p><span>Here, we have </span><code>m = 1</code><span>, because </span><code>p^1 = 1/2</code><span>. It means that the probabilities decay by half every trial:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;P(k=1) = \frac{1}{2} \\
&amp;P(k=2) = \frac{1}{4} \\
&amp;P(k=3) = \frac{1}{8} \\
&amp;P(k=4) = \frac{1}{16}


\end{align*}\)</span></p></div><p><span>Let’s take another example of a biased coin, such that </span><code>p=1/sqrt(2) = 0.707</code><span>, and </span><code>q=0.293</code><span>.</span></p><p><span>Here, we have </span><code>m=2</code><span>, because </span><code>p^2 = 1/2</code><span>. In this case the probabilities decay after blocks of size 2.</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;P(k=1) = 0.293 \\
&amp;P(k=2) = 0.207 \\
&amp;P(k=3) = 0.116 \\
&amp;P(k=4) = 0.104 \\
&amp;P(k=5) = 0.073 \\
&amp;P(k=6) = 0.051


\end{align*}\)</span></p></div><p><span>You can see that the probabilities decay by half for every even value of </span><code>k</code><span>. For instance:</span></p><div data-component-name="Latex"><p><span>\(\frac{P(k=2)}{P(k=4)} = \frac{P(K=4)}{P(k=6)} \approx 0.5\)</span></p></div><p><span>This pattern of exponential decay allows us to group the hash difference values in blocks of size </span><code>m</code><span>. Each value within a block gets a code of size </span><code>k</code><span> bits, while the next block gets codes of size </span><code>k + 1</code><span> bits. The reasoning behind it is rooted in information theory.</span></p><p><span>The minimum number of bits required to encode the outcome of an event is given by its information content which is </span><code>-log₂(p)</code><span>, where </span><code>p</code><span> is the probability of that event. </span></p><p>It means that if an event has probability 1/2, it needs 1 bit code, an event with probability 1/4 needs 2 bits, an event with probability 1/8 needs 3 bit codes and so on.</p><p><span>We can leverage this for geometrically distributed values by arranging them in blocks of size </span><code>m</code><span>. The values within a given block can be assigned codes of equal length, and the values in the subsequent block gets codes one bit wider for its </span><code>m</code><span> values.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png" width="680" height="165" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:165,&quot;width&quot;:680,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20471,&quot;alt&quot;:&quot;Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes." title="Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ced331b-4cbe-4fb9-808b-43673c752884_680x165.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Arranging hash code differences in blocks of sizes m, where each block’s probability is half of that of its predecessor. If the predecessor block gets k bit wide codes, the next block gets k+1 bit wide codes.</figcaption></figure></div><p>It turns out, formation of blocks with increasingly larger codes also leads to a beautiful self-similar bit pattern with properties that make it very easy to generate such codes. Let’s see how this self-similar pattern forms.</p><p>Self-similar pattern essentially means that the codes at a specific index within a block repeats itself at the same index in the next block, with one padding bit added on its left.</p><p>For instance, if the code at the 2nd position in the first block is 0001, then the 2nd code in the 2nd block will be 10001. Similarly, the 2nd position code in the 3rd block will be 110001, and so on. </p><p>This self-similar pattern naturally emerges because of the enforcement of 1 bit longer codes in each successive block. Let us see a concrete example.</p><p><span>Let’s say, our block size is </span><code>m=5</code><span> and the codes in the first block are </span><code>k=4</code><span> bits wide. </span></p><p><span>If the first code in the first block is </span><code>0110</code><span>, then we can generate the codes for the rest of the block by adding 1 to the previous code value. The codes for the first block will look like this:</span></p><pre><code>block-1 codes: 0110 0111 1000 1001 1010</code></pre><p><span>As you can see, the first block ends at the code </span><code>1010</code><span>. The natural value for the next code should be </span><code>1011</code><span>, but because this code lies in the next block, its code has to be 1 bit larger. As a result we need to left shift it by one bit, which makes it </span><code>10110</code><span>. And if you notice the least 4 bits of this code are the same as the first code in the first block. The following diagram highlights this visually.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png" width="581" height="511" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/db717247-7d87-4196-8c38-be44f82fe747_581x511.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:511,&quot;width&quot;:581,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:37780,&quot;alt&quot;:&quot;An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block" title="An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb717247-7d87-4196-8c38-be44f82fe747_581x511.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>An example of self-similar codes. The first code in the first block are same as least 4 bits of the first code in the 2nd block</figcaption></figure></div><pre><code>Another thing worth Noticing here is that we are obtaining the first value of the 2nd block by left shifting it by 1 bit, which means its least significant bit (LSB) is always 0. 

For the self-similar patterns to form, the first code of the first block should also always have its LSB set to 0. This implies that this first code is always an even number of the form 2x. </code></pre><p><span>The self-similar code has a couple of additional properties which provide an intuitive way to figure out the minimum bit width </span><code>k</code><span> for the codes in the first block. Let’s see how.</span></p><p><span>To figure out the minimum number of bits needed to encode the first block, let’s assume the first encoded value in the first block is </span><code>2x</code><span>. </span></p><p><span>Then the first code of the 2nd block </span><em>should</em><span> be </span><code>2x + m</code><span>. However, because the codes in the next block need to be 1 bit wider, this value gets shifted to the left by 1 bit, which makes it </span><code>2(2x + m)</code><span>.</span></p><blockquote><h6><em>Left shifting a value by 1 bit doubles it.</em></h6></blockquote><p><span>The self-similarity pattern gives rise to another way to think about these codes. If the first code in the first block is </span><code>2x</code><span>, then in the next block:</span></p><ul><li><p><span>We want the same pattern (</span><code>2x</code><span>)</span></p></li><li><p>But with an extra bit on the left</p></li><li><p><span>Adding a bit on the left is equivalent to adding </span><code>2^k</code></p></li><li><p><span>So the code becomes </span><code>2^k + 2x</code></p></li></ul><p>Combining these two relations gives us the following equation</p><div data-component-name="Latex"><p><span>\(\begin{align*}
&amp;2^k + 2x = 2(2x + m) \\
&amp;\text{Or, } 2^k = 2m + 2x \\
&amp;\text{Here, x is a nonnegative integer, so we can simplify the above to:} \\
&amp;2^k \ge 2m
\end{align*}\)</span></p></div><p><span>By solving for the smallest integer value of </span><code>k</code><span> we can get the code width of the first block.</span></p><p><span>The same equation also gives us the value of </span><code>x</code><span>:</span></p><div data-component-name="Latex"><p><span>\(\begin{align*}
x = 2^\left(k-1\right) - m = 2^{\log_2(m)} - m
\end{align*}\)</span></p></div><p>Knowing the values of m, k, and x gives way for a simple encoding algorithm.</p><p>The paper by Golomb gave a different algorithm for encoding and decoding, but the Unix spell code used a slightly complicated but more efficient algorithm. I describe the algorithm as implemented in the Unix spell.</p><p>Let’s understand how to encode a value using Golomb’s code. Recall that we have:</p><ul><li><p><span>Initial bit width </span><code>k</code></p></li><li><p><span>Block size </span><code>m</code></p></li><li><p><span>First code in first block is </span><code>2x</code><span>, where </span><code>x = 2^(k-1) - m</code></p></li></ul><p>Here’s the encoding algorithm:</p><pre><code><code>def encode(value):
    # Case 1: Values less than x
    # These get shorter codes of length k-1
    # Because we have unused bit patterns available
    if value &lt; x:
        return value, k-1  # return (code, length)
    
    # Case 2: Values &gt;= x
    # Need to find which block they belong to
    value = value - x     # adjust relative to first code
    y = 1                 # tracks block number through bit shifts
    length = k           # start with k bits
    
    # Find block by repeatedly subtracting block size
    while value &gt;= m:    # m is block size
        value -= m       # move to next block
        y = y &lt;&lt; 1      # add padding bit for next block
        length += 1     # each block needs one more bit
    
    # Generate final code:
    # (y-1) &lt;&lt; k creates padding bits based on block number
    # x*2 adds offset for the first code
    # value adds position within current block
    code = ((y-1) &lt;&lt; k) + (x*2) + value
    return code, length
</code></code></pre><p>The following figure shows two examples to illustrate how it works in practice:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png" width="939" height="1121" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1121,&quot;width&quot;:939,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:105065,&quot;alt&quot;:&quot;Examples of how the encoding algorithm works for values 2 and 8&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Examples of how the encoding algorithm works for values 2 and 8" title="Examples of how the encoding algorithm works for values 2 and 8" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f59257e-3633-4a1a-baee-1bbf3ccde036_939x1121.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Examples of how the encoding algorithm works for values 2 and 8</figcaption></figure></div><p><span>You can find the original Unix svr4 implementation of this algorithm </span><a href="https://github.com/calmsacibis995/svr4-src/blob/main/cmd/spell/huff.c#L105" rel="">here</a><span>.</span></p><p>The decoding process is also not that complicated. Given a code, we need to:</p><ol><li><p><span>First look at its top </span><code>k-1</code><span> bits (call this w):</span></p><ul><li><p><span>If </span><code>w</code><span> is less than </span><code>x</code><span>, then this is a shorter code</span></p></li><li><p><span>The decoded value is simply </span><code>w</code><span> itself</span></p></li></ul></li><li><p><span>If </span><code>w ≥ x</code><span>, then we need to </span></p><ul><li><p>Include one more bit into w</p></li><li><p><span>Look at the least significant </span><code>k</code><span> bits of </span><code>w</code><span>: call it </span><code>u</code></p></li><li><p><span>if </span><code>u &lt; 2x + m</code><span>:</span></p><ul><li><p><code>value = x + u + (s - 1)m</code></p></li><li><p><span>where </span><code>s</code><span> is the number of extra bits included into w</span></p></li></ul></li><li><p>else:</p><ul><li><p><span>keep including more bits into </span><code>w</code><span> until </span><code>u &lt; 2x + m</code></p></li></ul></li></ul></li></ol><p><span>You can find the original Unix svr4 implementation of the decode algorithm </span><a href="https://github.com/calmsacibis995/svr4-src/blob/main/cmd/spell/huff.c#L83" rel="">here</a><span>.</span></p><p>So how well this technique was able to compress the hash differences?</p><p>Recall that the theoretical limit of compression was 13.57 bits per word. Golomb codes managed to achieve an expected code length of 13.60, remarkably close to this theoretical minimum.</p><p>However, looking up a value in this compressed dictionary was quite slow. It required starting from the beginning, decoding and summing values until finding or exceeding the desired hash code.</p><p><span>To speed this up, the final Unix spell implementation partitioned the table of differences into </span><code>M</code><span> bins. This allowed them to first locate the correct bin, and then only scan within that bin, speeding up the search by a factor of </span><code>M</code><span>.</span></p><p><span>This partitioning scheme required storing additional pointers to the bins, adding </span><code>log₂M</code><span> bits per word to the storage requirement. The total storage increased to about 14 bits per word, but this was an acceptable trade-off: it was still within their memory budget while providing significantly faster lookups.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://blog.codingconfessions.com/p/how-unix-spell-ran-in-64kb-ram?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>The Unix spell command is a fascinating piece of engineering history that emerged from the severe memory constraints of the PDP-11. What makes it particularly interesting is how it evolved from a simple disk-based dictionary lookup to an elegant solution combining multiple computer science concepts:</p><ul><li><p>Probabilistic data structures (Bloom filters)</p></li><li><p>Information theory (optimal bit width calculations)</p></li><li><p>Probability theory (geometric distribution)</p></li><li><p>Compression algorithms (Golomb coding)</p></li></ul><p>The engineering journey is particularly instructive:</p><ol><li><p>Started with Bloom filters achieving acceptable false positive rates</p></li><li><p>When dictionary size grew, switched to compressed hashing where:</p><ul><li><p>They computed theoretical minimum bits needed</p></li><li><p>Recognized patterns in hash differences</p></li><li><p>Used Golomb's code to achieve near-optimal compression</p></li><li><p>Added binning for faster lookups with minimal space overhead</p></li></ul></li></ol><p>Even though modern spell checkers use different techniques like edit distance and language models, the engineering insights from Unix spell remain valuable. It shows how deep understanding of theoretical concepts combined with practical constraints can lead to efficient and elegant solutions.</p><p>Most importantly, it demonstrates that some of the best innovations happen when we are resource constrained, forcing us to think deeper about our problems rather than throwing more hardware at them.</p><ul><li><p><a href="https://ia800601.us.archive.org/11/items/development-of-spelling-list/Image092317125441_text.pdf" rel="">Development of a Spelling List by Douglas McIlroy, 1982</a></p></li><li><p><a href="https://web.stanford.edu/class/ee398a/handouts/papers/Golomb%20-%20Run-Length%20Codes%20-%20IT66.pdf" rel="">Run-length Encodings by Golomb, 1965</a></p></li><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/362686.362692" rel="">Space/Time Trade-offs in Hash Coding with Allowable Errors by Bloom, 1970</a></p></li><li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3532.315102" rel="">A Spelling Checker by Jon Bentley, 1985</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/History_of_Unix#1970s" rel="">History of Unix</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/Geometric_distribution" rel="">Geometric Distribution</a></p></li><li><p><a href="https://blog.codingconfessions.com/p/bloom-filters-and-beyond" rel="">Bloom filter</a></p></li><li><p><a href="https://github.com/calmsacibis995/svr4-src/tree/main/cmd/spell" rel="">Unix Spell Source</a><span> </span></p></li></ul><p><em>If you find my work interesting and valuable, you can support me by opting for a paid subscription (it’s $6.40 monthly/$58 annual). As a bonus you get access to monthly live sessions, and all the past recordings.</em></p><p><strong>Subscribed</strong></p><p><em><span>Many people report failed payments, or don’t want a recurring subscription. For that I also have a </span><a href="https://buymeacoffee.com/codeconfessions" rel="">buymeacoffee page</a><span>. Where you can buy me a coffee or become a member. I will upgrade you to a paid subscription for the equivalent duration here.</span></em></p><p data-attrs="{&quot;url&quot;:&quot;https://buymeacoffee.com/codeconfessions&quot;,&quot;text&quot;:&quot;Buy me a coffee&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://buymeacoffee.com/codeconfessions" rel=""><span>Buy me a coffee</span></a></p><p><em>I also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here.</em></p><p data-attrs="{&quot;url&quot;:&quot;https://github.com/sponsors/abhinav-upadhyay&quot;,&quot;text&quot;:&quot;Sponsor me on GitHub&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://github.com/sponsors/abhinav-upadhyay" rel=""><span>Sponsor me on GitHub</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nation-scale Matrix deployments will fail using the community version of Synapse (120 pts)]]></title>
            <link>https://mastodon.matrix.org/@element/113842786942364269</link>
            <guid>42752402</guid>
            <pubDate>Sat, 18 Jan 2025 23:52:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.matrix.org/@element/113842786942364269">https://mastodon.matrix.org/@element/113842786942364269</a>, See on <a href="https://news.ycombinator.com/item?id=42752402">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Pharaoh's Tomb HD – A Remake Made in JavaScript with Kaplay (107 pts)]]></title>
            <link>https://pt-hd.iocaihost.me/</link>
            <guid>42752023</guid>
            <pubDate>Sat, 18 Jan 2025 22:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pt-hd.iocaihost.me/">https://pt-hd.iocaihost.me/</a>, See on <a href="https://news.ycombinator.com/item?id=42752023">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Kalman Filter Tutorial (325 pts)]]></title>
            <link>https://www.kalmanfilter.net/default.aspx</link>
            <guid>42751690</guid>
            <pubDate>Sat, 18 Jan 2025 21:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kalmanfilter.net/default.aspx">https://www.kalmanfilter.net/default.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=42751690">Hacker News</a></p>
Couldn't get https://www.kalmanfilter.net/default.aspx: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Perplexity AI submits bid to merge with TikTok (110 pts)]]></title>
            <link>https://techcrunch.com/2025/01/18/perplexity-ai-submits-bid-to-merge-with-tiktok/</link>
            <guid>42751649</guid>
            <pubDate>Sat, 18 Jan 2025 21:42:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/18/perplexity-ai-submits-bid-to-merge-with-tiktok/">https://techcrunch.com/2025/01/18/perplexity-ai-submits-bid-to-merge-with-tiktok/</a>, See on <a href="https://news.ycombinator.com/item?id=42751649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<div>
			<div>
<p id="speakable-summary">With a TikTok ban looming in the United States, Perplexity AI is the latest bidder hoping to give the video app a new corporate home.</p>

<p>CNBC first <a rel="nofollow" href="https://www.cnbc.com/2025/01/18/perplexity-ai-makes-a-bid-to-merge-with-tiktok-us.html">reported on Perplexity’s interest</a>. A source with knowledge of the offer confirmed to TechCrunch that Perplexity (whose CEO Aravind Srinivas is pictured above) has submitted a bid to merge with TikTok US.</p>







<p>The source also confirmed other details about the bid — that it would create a new entity combining Perplexity, TikTok US, and new equity partners; that most investors in TikTok’s parent company ByteDance would be able to retain their equity; and that by merging, Perplexity hopes to bring more video to its AI search engine.</p>

<p>A law requiring ByteDance to either sell TikTok or see it banned in the US is set to take effect on Sunday, January 19. That will be President Joe Biden’s last day in office, and officials from his administration have said that <a href="https://techcrunch.com/2025/01/18/tiktok-says-it-will-go-dark-sunday-unless-biden-offers-definitive-statement/">it will leave the actual implementation of the ban</a> “to the next Administration.”</p>

<p>Meanwhile, President-elect Donald Trump, who will be inaugurated on Monday, said he would <a rel="nofollow" href="https://www.nbcnews.com/politics/donald-trump/trump-likely-give-tiktok-90-day-extension-avoid-ban-rcna188258">“most likely” give TikTok a 90-day extension</a>, and TikTok’s CEO <a href="https://techcrunch.com/2025/01/17/tiktok-ceo-responds-to-trump-thanks-him-for-trying-to-solve-us-ban/">posted a video thanking Trump </a>for his efforts.</p>

<p>However, TikTok said that without more explicit assurances of non-enforcement from the Biden administration, <a href="https://techcrunch.com/2025/01/18/tiktok-says-it-will-go-dark-sunday-unless-biden-offers-definitive-statement/">it will be “forced to go dark”</a> on Sunday.</p>

<p>Despite a number of buyers expressing interest in TikTok, ByteDance has said repeatedly that it does not intend to sell. (The company described a report that <a href="https://techcrunch.com/2025/01/13/china-is-reportedly-open-to-elon-musk-acquiring-tiktok-us/">the Chinese government is open to an acquisition by Elon Musk</a> as “pure fiction.”) CNBC reports that Perplexity is hoping it can overcome those reservations by proposing a merger rather than a sale.</p>


<p>TechCrunch has reached out to TikTok and Perplexity AI for comment.</p>
</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
	
	
	
	

	
<div>
	<p>Anthony Ha is TechCrunch’s weekend editor. Previously, he worked as a tech reporter at Adweek, a senior editor at VentureBeat, a local government reporter at the Hollister Free Lance, and vice president of content at a VC firm. He lives in New York City.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/anthony-ha/" data-event="button" href="https://techcrunch.com/author/anthony-ha/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>


			


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div><div>
		<div>
	<div>
		<div>
			<h3>Newsletters</h3>
			
		</div>
		<p>Subscribe for the industry’s biggest tech news</p>
	</div>
	<form method="POST" action="/">
		
	</form>
	
</div>


		
		<h2>Related</h2>
		

		
		
		

		
		<div>

<h2>Latest in Government &amp; Policy</h2>




</div>
		

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WASM GC isn't ready for realtime graphics (130 pts)]]></title>
            <link>https://dthompson.us/posts/wasm-gc-isnt-ready-for-realtime-graphics.html</link>
            <guid>42750781</guid>
            <pubDate>Sat, 18 Jan 2025 19:36:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dthompson.us/posts/wasm-gc-isnt-ready-for-realtime-graphics.html">https://dthompson.us/posts/wasm-gc-isnt-ready-for-realtime-graphics.html</a>, See on <a href="https://news.ycombinator.com/item?id=42750781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Wasm GC is a wonderful thing that is now available in all major web
browsers since slowpoke Safari/WebKit finally shipped it in December.
It provides a hierarchy of heap allocated reference types and a set of
instructions to operate on them.  Wasm GC enables managed memory
languages to take advantage of the advanced garbage collectors inside
web browser engines.  It’s now possible to implement a managed memory
language without having to ship a GC inside the binary.  The benefits
are smaller binaries, better performance, and better integration with
the host runtime.</p><p>However, Wasm GC has some serious drawbacks when compared to linear
memory. I enjoy playing around with realtime graphics programming in
my free time, but I was disappointed to discover that Wasm GC just
isn’t a good fit for that right now.  I decided to write this post
because I’d like to see Wasm GC on more or less equal footing with
linear memory when it comes to binary data manipulation.</p><h2>Hello triangle</h2><p>For starters, let's take a look at what a <a href="https://learnopengl.com/Getting-started/Hello-Triangle">“hello
triangle”</a>
WebGL demo looks like with Wasm GC.  I’ll use
<a href="https://spritely.institute/hoot">Hoot</a>, the Scheme to Wasm compiler
that I work on, to build it.</p><p>Below is a Scheme program that declares imports for the subset of the
WebGL, HTML5 Canvas, etc. APIs that are necessary and then renders a
single triangle:</p><pre><code><span>(</span><span>use-modules</span> <span>(</span><span>hoot</span> <span>ffi</span><span>)</span><span>)</span>

<span>;; Document
</span><span>(</span><span>define-foreign</span> <span>get-element-by-id</span>
  <span>"document"</span> <span>"getElementById"</span>
  <span>(</span><span>ref</span> <span>string</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>null</span> <span>extern</span><span>)</span><span>)</span>

<span>;; Element
</span><span>(</span><span>define-foreign</span> <span>element-width</span>
  <span>"element"</span> <span>"width"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>i32</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>element-height</span>
  <span>"element"</span> <span>"height"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>i32</span><span>)</span>

<span>;; Canvas
</span><span>(</span><span>define-foreign</span> <span>get-canvas-context</span>
  <span>"canvas"</span> <span>"getContext"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>string</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>null</span> <span>extern</span><span>)</span><span>)</span>

<span>;; WebGL
</span><span>(</span><span>define</span> <span>GL_VERTEX_SHADER</span> <span>35633</span><span>)</span>
<span>(</span><span>define</span> <span>GL_FRAGMENT_SHADER</span> <span>35632</span><span>)</span>
<span>(</span><span>define</span> <span>GL_COMPILE_STATUS</span> <span>35713</span><span>)</span>
<span>(</span><span>define</span> <span>GL_LINK_STATUS</span> <span>35714</span><span>)</span>
<span>(</span><span>define</span> <span>GL_ARRAY_BUFFER</span> <span>34962</span><span>)</span>
<span>(</span><span>define</span> <span>GL_STATIC_DRAW</span> <span>35044</span><span>)</span>
<span>(</span><span>define</span> <span>GL_COLOR_BUFFER_BIT</span> <span>16384</span><span>)</span>
<span>(</span><span>define</span> <span>GL_TRIANGLES</span> <span>4</span><span>)</span>
<span>(</span><span>define</span> <span>GL_FLOAT</span> <span>5126</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-create-shader</span>
  <span>"gl"</span> <span>"createShader"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-delete-shader</span>
  <span>"gl"</span> <span>"deleteShader"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-shader-source</span>
  <span>"gl"</span> <span>"shaderSource"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>string</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-compile-shader</span>
  <span>"gl"</span> <span>"compileShader"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-get-shader-parameter</span>
  <span>"gl"</span> <span>"getShaderParameter"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>-&gt;</span> <span>i32</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-get-shader-info-log</span>
  <span>"gl"</span> <span>"getShaderInfoLog"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>string</span><span>)</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-create-program</span>
  <span>"gl"</span> <span>"createProgram"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-delete-program</span>
  <span>"gl"</span> <span>"deleteProgram"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-attach-shader</span>
  <span>"gl"</span> <span>"attachShader"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-link-program</span>
  <span>"gl"</span> <span>"linkProgram"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-use-program</span>
  <span>"gl"</span> <span>"useProgram"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-get-program-parameter</span>
  <span>"gl"</span> <span>"getProgramParameter"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>-&gt;</span> <span>i32</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-get-program-info-log</span>
  <span>"gl"</span> <span>"getProgramInfoLog"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>string</span><span>)</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-create-buffer</span>
  <span>"gl"</span> <span>"createBuffer"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-delete-buffer</span>
  <span>"gl"</span> <span>"deleteBuffer"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-bind-buffer</span>
  <span>"gl"</span> <span>"bindBuffer"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-buffer-data</span>
  <span>"gl"</span> <span>"bufferData"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>(</span><span>ref</span> <span>eq</span><span>)</span> <span>i32</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-enable-vertex-attrib-array</span>
  <span>"gl"</span> <span>"enableVertexAttribArray"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-vertex-attrib-pointer</span>
  <span>"gl"</span> <span>"vertexAttribPointer"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-draw-arrays</span>
  <span>"gl"</span> <span>"drawArrays"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-viewport</span>
  <span>"gl"</span> <span>"viewport"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>i32</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-clear-color</span>
  <span>"gl"</span> <span>"clearColor"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>f64</span> <span>f64</span> <span>f64</span> <span>f64</span> <span>-&gt;</span> <span>none</span><span>)</span>
<span>(</span><span>define-foreign</span> <span>gl-clear</span>
  <span>"gl"</span> <span>"clear"</span>
  <span>(</span><span>ref</span> <span>extern</span><span>)</span> <span>i32</span> <span>-&gt;</span> <span>none</span><span>)</span>

<span>(</span><span>define</span> <span>(</span><span>compile-shader</span> <span>gl</span> <span>type</span> <span>source</span><span>)</span>
  <span>(</span><span>let</span> <span>(</span><span>(</span><span>shader</span> <span>(</span><span>gl-create-shader</span> <span>gl</span> <span>type</span><span>)</span><span>)</span><span>)</span>
    <span>(</span><span>gl-shader-source</span> <span>gl</span> <span>shader</span> <span>source</span><span>)</span>
    <span>(</span><span>gl-compile-shader</span> <span>gl</span> <span>shader</span><span>)</span>
    <span>(</span><span>unless</span> <span>(</span><span>=</span> <span>(</span><span>gl-get-shader-parameter</span> <span>gl</span> <span>shader</span> <span>GL_COMPILE_STATUS</span><span>)</span> <span>1</span><span>)</span>
      <span>(</span><span>let</span> <span>(</span><span>(</span><span>info</span> <span>(</span><span>gl-get-shader-info-log</span> <span>gl</span> <span>shader</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>gl-delete-shader</span> <span>gl</span> <span>shader</span><span>)</span>
        <span>(</span><span>error</span> <span>"shader compilation failed"</span> <span>info</span><span>)</span><span>)</span><span>)</span>
    <span>shader</span><span>)</span><span>)</span>

<span>(</span><span>define</span> <span>(</span><span>link-shader</span> <span>gl</span> <span>vertex-shader</span> <span>fragment-shader</span><span>)</span>
  <span>(</span><span>let</span> <span>(</span><span>(</span><span>program</span> <span>(</span><span>gl-create-program</span> <span>gl</span><span>)</span><span>)</span><span>)</span>
    <span>(</span><span>gl-attach-shader</span> <span>gl</span> <span>program</span> <span>vertex-shader</span><span>)</span>
    <span>(</span><span>gl-attach-shader</span> <span>gl</span> <span>program</span> <span>fragment-shader</span><span>)</span>
    <span>(</span><span>gl-link-program</span> <span>gl</span> <span>program</span><span>)</span>
    <span>(</span><span>unless</span> <span>(</span><span>=</span> <span>(</span><span>gl-get-program-parameter</span> <span>gl</span> <span>program</span> <span>GL_LINK_STATUS</span><span>)</span> <span>1</span><span>)</span>
      <span>(</span><span>let</span> <span>(</span><span>(</span><span>info</span> <span>(</span><span>gl-get-program-info-log</span> <span>gl</span> <span>program</span><span>)</span><span>)</span><span>)</span>
        <span>(</span><span>gl-delete-program</span> <span>gl</span> <span>program</span><span>)</span>
        <span>(</span><span>error</span> <span>"program linking failed"</span> <span>info</span><span>)</span><span>)</span><span>)</span>
    <span>program</span><span>)</span><span>)</span>

<span>;; Setup GL context
</span><span>(</span><span>define</span> <span>canvas</span> <span>(</span><span>get-element-by-id</span> <span>"canvas"</span><span>)</span><span>)</span>
<span>(</span><span>define</span> <span>gl</span> <span>(</span><span>get-canvas-context</span> <span>canvas</span> <span>"webgl"</span><span>)</span><span>)</span>
<span>(</span><span>when</span> <span>(</span><span>external-null?</span> <span>gl</span><span>)</span>
  <span>(</span><span>error</span> <span>"unable to create WebGL context"</span><span>)</span><span>)</span>

<span>;; Compile shader
</span><span>(</span><span>define</span> <span>vertex-shader-source</span>
  <span>"attribute vec2 position;
attribute vec3 color;
varying vec3 fragColor;

void main() {
  gl_Position = vec4(position, 0.0, 1.0);
  fragColor = color;
}"</span><span>)</span>
<span>(</span><span>define</span> <span>fragment-shader-source</span>
  <span>"precision mediump float;

varying vec3 fragColor;

void main() {
  gl_FragColor = vec4(fragColor, 1);
}"</span><span>)</span>
<span>(</span><span>define</span> <span>vertex-shader</span>
  <span>(</span><span>compile-shader</span> <span>gl</span> <span>GL_VERTEX_SHADER</span> <span>vertex-shader-source</span><span>)</span><span>)</span>
<span>(</span><span>define</span> <span>fragment-shader</span>
  <span>(</span><span>compile-shader</span> <span>gl</span> <span>GL_FRAGMENT_SHADER</span> <span>fragment-shader-source</span><span>)</span><span>)</span>
<span>(</span><span>define</span> <span>shader</span> <span>(</span><span>link-shader</span> <span>gl</span> <span>vertex-shader</span> <span>fragment-shader</span><span>)</span><span>)</span>

<span>;; Create vertex buffer
</span><span>(</span><span>define</span> <span>stride</span> <span>(</span><span>*</span> <span>4</span> <span>5</span><span>)</span><span>)</span>
<span>(</span><span>define</span> <span>buffer</span> <span>(</span><span>gl-create-buffer</span> <span>gl</span><span>)</span><span>)</span>
<span>(</span><span>gl-bind-buffer</span> <span>gl</span> <span>GL_ARRAY_BUFFER</span> <span>buffer</span><span>)</span>
<span>(</span><span>gl-buffer-data</span> <span>gl</span> <span>GL_ARRAY_BUFFER</span>
                <span>#f32</span><span>(</span><span>-1.0</span> <span>-1.0</span>
                      <span>1.0</span>  <span>0.0</span>  <span>0.0</span>
                      <span>1.0</span> <span>-1.0</span>
                      <span>0.0</span>  <span>1.0</span>  <span>0.0</span>
                      <span>0.0</span>  <span>1.0</span>
                      <span>0.0</span>  <span>0.0</span>  <span>1.0</span><span>)</span>
                <span>GL_STATIC_DRAW</span><span>)</span>

<span>;; Draw
</span><span>(</span><span>gl-viewport</span> <span>gl</span> <span>0</span> <span>0</span> <span>(</span><span>element-width</span> <span>canvas</span><span>)</span> <span>(</span><span>element-height</span> <span>canvas</span><span>)</span><span>)</span>
<span>(</span><span>gl-clear</span> <span>gl</span> <span>GL_COLOR_BUFFER_BIT</span><span>)</span>
<span>(</span><span>gl-use-program</span> <span>gl</span> <span>shader</span><span>)</span>
<span>(</span><span>gl-enable-vertex-attrib-array</span> <span>gl</span> <span>0</span><span>)</span>
<span>(</span><span>gl-vertex-attrib-pointer</span> <span>gl</span> <span>0</span> <span>2</span> <span>GL_FLOAT</span> <span>0</span> <span>stride</span> <span>0</span><span>)</span>
<span>(</span><span>gl-enable-vertex-attrib-array</span> <span>gl</span> <span>1</span><span>)</span>
<span>(</span><span>gl-vertex-attrib-pointer</span> <span>gl</span> <span>1</span> <span>3</span> <span>GL_FLOAT</span> <span>0</span> <span>stride</span> <span>8</span><span>)</span>
<span>(</span><span>gl-draw-arrays</span> <span>gl</span> <span>GL_TRIANGLES</span> <span>0</span> <span>3</span><span>)</span></code></pre><p>Note that in Scheme, the equivalent of a <code>Uint8Array</code> is a
<em>bytevector</em>.  Hoot uses a packed array, an <code>(array i8)</code> specifically,
for the contents of a bytevector.</p><p>And here is the JavaScript code necessary to boot the resulting Wasm
binary:</p><pre><code><span>window</span><span>.</span><span>addEventListener</span><span>(</span><span>"load"</span><span>,</span> <span>async</span> <span>(</span><span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
  <span>function</span> <span>bytevectorToUint8Array</span><span>(</span><span>bv</span><span>)</span> <span>{</span>
    <span>let</span> <span>len</span> <span>=</span> <span>reflect</span><span>.</span><span>bytevector_length</span><span>(</span><span>bv</span><span>)</span><span>;</span>
    <span>let</span> <span>array</span> <span>=</span> <span>new</span> <span>Uint8Array</span><span>(</span><span>len</span><span>)</span><span>;</span>
    <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>len</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
      <span>array</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>reflect</span><span>.</span><span>bytevector_ref</span><span>(</span><span>bv</span><span>,</span> <span>i</span><span>)</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>array</span><span>;</span>
  <span>}</span>

  <span>let</span> <span>mod</span> <span>=</span> <span>await</span> <span>SchemeModule</span><span>.</span><span>fetch_and_instantiate</span><span>(</span><span>"triangle.wasm"</span><span>,</span> <span>{</span>
    <span>reflect_wasm_dir</span><span>:</span> <span>'reflect-wasm'</span><span>,</span>
    <span>user_imports</span><span>:</span> <span>{</span>
      <span>document</span><span>:</span> <span>{</span>
        <span>getElementById</span><span>:</span> <span>(</span><span>id</span><span>)</span> <span>=</span><span>&gt;</span> <span>document</span><span>.</span><span>getElementById</span><span>(</span><span>id</span><span>)</span>
      <span>}</span><span>,</span>
      <span>element</span><span>:</span> <span>{</span>
        <span>width</span><span>:</span> <span>(</span><span>elem</span><span>)</span> <span>=</span><span>&gt;</span> <span>elem</span><span>.</span><span>width</span><span>,</span>
        <span>height</span><span>:</span> <span>(</span><span>elem</span><span>)</span> <span>=</span><span>&gt;</span> <span>elem</span><span>.</span><span>height</span>
      <span>}</span><span>,</span>
      <span>canvas</span><span>:</span> <span>{</span>
        <span>getContext</span><span>:</span> <span>(</span><span>elem</span><span>,</span> <span>type</span><span>)</span> <span>=</span><span>&gt;</span> <span>elem</span><span>.</span><span>getContext</span><span>(</span><span>type</span><span>)</span>
      <span>}</span><span>,</span>
      <span>gl</span><span>:</span> <span>{</span>
        <span>createShader</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>type</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>createShader</span><span>(</span><span>type</span><span>)</span><span>,</span>
        <span>deleteShader</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>shader</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>deleteShader</span><span>(</span><span>shader</span><span>)</span><span>,</span>
        <span>shaderSource</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>shader</span><span>,</span> <span>source</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>shaderSource</span><span>(</span><span>shader</span><span>,</span> <span>source</span><span>)</span><span>,</span>
        <span>compileShader</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>shader</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>compileShader</span><span>(</span><span>shader</span><span>)</span><span>,</span>
        <span>getShaderParameter</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>shader</span><span>,</span> <span>param</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>getShaderParameter</span><span>(</span><span>shader</span><span>,</span> <span>param</span><span>)</span><span>,</span>
        <span>getShaderInfoLog</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>shader</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>getShaderInfoLog</span><span>(</span><span>shader</span><span>)</span><span>,</span>
        <span>createProgram</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>type</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>createProgram</span><span>(</span><span>type</span><span>)</span><span>,</span>
        <span>deleteProgram</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>program</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>deleteProgram</span><span>(</span><span>program</span><span>)</span><span>,</span>
        <span>attachShader</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>program</span><span>,</span> <span>shader</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>attachShader</span><span>(</span><span>program</span><span>,</span> <span>shader</span><span>)</span><span>,</span>
        <span>linkProgram</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>program</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>linkProgram</span><span>(</span><span>program</span><span>)</span><span>,</span>
        <span>useProgram</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>program</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>useProgram</span><span>(</span><span>program</span><span>)</span><span>,</span>
        <span>getProgramParameter</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>program</span><span>,</span> <span>param</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>getProgramParameter</span><span>(</span><span>program</span><span>,</span> <span>param</span><span>)</span><span>,</span>
        <span>getProgramInfoLog</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>program</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>getProgramInfoLog</span><span>(</span><span>program</span><span>)</span><span>,</span>
        <span>createBuffer</span><span>:</span> <span>(</span><span>gl</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>createBuffer</span><span>(</span><span>)</span><span>,</span>
        <span>deleteBuffer</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>buffer</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>deleteBuffer</span><span>(</span><span>buffer</span><span>)</span><span>,</span>
        <span>bindBuffer</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>target</span><span>,</span> <span>buffer</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>bindBuffer</span><span>(</span><span>target</span><span>,</span> <span>buffer</span><span>)</span><span>,</span>
        <span>bufferData</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>buffer</span><span>,</span> <span>data</span><span>,</span> <span>usage</span><span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
          <span>let</span> <span>bv</span> <span>=</span> <span>new</span> <span>Bytevector</span><span>(</span><span>reflect</span><span>,</span> <span>data</span><span>)</span><span>;</span>
          <span>gl</span><span>.</span><span>bufferData</span><span>(</span><span>buffer</span><span>,</span> <span>bytevectorToUint8Array</span><span>(</span><span>bv</span><span>)</span><span>,</span> <span>usage</span><span>)</span><span>;</span>
        <span>}</span><span>,</span>
        <span>enableVertexAttribArray</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>index</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>enableVertexAttribArray</span><span>(</span><span>index</span><span>)</span><span>,</span>
        <span>vertexAttribPointer</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>index</span><span>,</span> <span>size</span><span>,</span> <span>type</span><span>,</span> <span>normalized</span><span>,</span> <span>stride</span><span>,</span> <span>offset</span><span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
          <span>gl</span><span>.</span><span>vertexAttribPointer</span><span>(</span><span>index</span><span>,</span> <span>size</span><span>,</span> <span>type</span><span>,</span> <span>normalized</span><span>,</span> <span>stride</span><span>,</span> <span>offset</span><span>)</span><span>;</span>
        <span>}</span><span>,</span>
        <span>drawArrays</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>mode</span><span>,</span> <span>first</span><span>,</span> <span>count</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>drawArrays</span><span>(</span><span>mode</span><span>,</span> <span>first</span><span>,</span> <span>count</span><span>)</span><span>,</span>
        <span>viewport</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>,</span> <span>w</span><span>,</span> <span>h</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>viewport</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>,</span> <span>w</span><span>,</span> <span>h</span><span>)</span><span>,</span>
        <span>clearColor</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>r</span><span>,</span> <span>g</span><span>,</span> <span>b</span><span>,</span> <span>a</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>clearColor</span><span>(</span><span>r</span><span>,</span> <span>g</span><span>,</span> <span>b</span><span>,</span> <span>a</span><span>)</span><span>,</span>
        <span>clear</span><span>:</span> <span>(</span><span>gl</span><span>,</span> <span>mask</span><span>)</span> <span>=</span><span>&gt;</span> <span>gl</span><span>.</span><span>clear</span><span>(</span><span>mask</span><span>)</span>
      <span>}</span>
    <span>}</span>
  <span>}</span><span>)</span><span>;</span>
  <span>let</span> <span>reflect</span> <span>=</span> <span>await</span> <span>mod</span><span>.</span><span>reflect</span><span>(</span><span>{</span> <span>reflect_wasm_dir</span><span>:</span> <span>'reflect-wasm'</span> <span>}</span><span>)</span><span>;</span>
  <span>let</span> <span>proc</span> <span>=</span> <span>new</span> <span>Procedure</span><span>(</span><span>reflect</span><span>,</span> <span>mod</span><span>.</span><span>get_export</span><span>(</span><span>"$load"</span><span>)</span><span>.</span><span>value</span><span>)</span><span>;</span>
  <span>proc</span><span>.</span><span>call</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span></code></pre><h2>Hello problems</h2><p>There are two major performance issues with this program.  One is
visible in the source above, the other is hidden in the language
implementation.</p><h3>Heap objects are opaque on the other side</h3><p>Wasm GC heap objects are <em>opaque</em> to the host.  Likewise, heap objects
from the host are opaque to the Wasm guest.  Thus the contents of an
<code>(array i8)</code> object are not visible from JavaScript and the contents
of a <code>Uint8Array</code> are not visible from Wasm.  This is a good security
property in the general case, but it’s a hinderance in this specific
case.</p><p>Let’s say we have an <code>(array i8)</code> full of vertex data we want to put
into a WebGL buffer.  To do this, we must make one JS-&gt;Wasm call <em>for
each byte</em> in the array and store it into a <code>Uint8Array</code>.  This is
what the <code>bytevectorToUint8Array</code> function above is doing.  Copying
any significant amount of data per frame is going to tank performance.
Hope you aren’t trying to stream vertex data!</p><p>Contrast the previous paragraph with Wasm linear memory.  A
<code>WebAssembly.Memory</code> object can be <a href="https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory/buffer">easily accessed from
JavaScript</a>
as an <code>ArrayBuffer</code>.  To get a blob of vertex data out of a memory
object, you just need to know the byte offset and length and you’re
good to go.  There are many Wasm linear memory applications using
WebGL successfully.</p><h3>Manipulating multi-byte binary data is inefficient</h3><p>To read a multi-byte number such as an unsigned 32-bit integer from an
<code>(array i8)</code>, you have to fetch each individual byte and combine them
together.  Here’s a self-contained example that uses Guile-flavored
WAT format:</p><pre><code><span>(</span><span>module</span>
 <span>(</span><span>type</span> <span>$bytevector</span> <span>(</span><span>array</span> <span>i8</span><span>)</span><span>)</span>
 <span>(</span><span>data</span> <span>$init</span> <span>#u32</span><span>(</span><span>123456789</span><span>)</span><span>)</span>
 <span>(</span><span>func</span> <span>(</span><span>export</span> <span>"main"</span><span>)</span> <span>(</span><span>result</span> <span>i32</span><span>)</span>
       <span>(</span><span>local</span> <span>$a</span> <span>(</span><span>ref</span> <span>$bytevector</span><span>)</span><span>)</span>
       <span>(</span><span>local.set</span> <span>$a</span> <span>(</span><span>array.new_data</span> <span>$bytevector</span> <span>$init</span>
                                     <span>(</span><span>i32.const</span> <span>0</span><span>)</span>
                                     <span>(</span><span>i32.const</span> <span>4</span><span>)</span><span>)</span><span>)</span>
       <span>(</span><span>array.get_u</span> <span>$bytevector</span> <span>(</span><span>local.get</span> <span>$a</span><span>)</span> <span>(</span><span>i32.const</span> <span>0</span><span>)</span><span>)</span>
       <span>(</span><span>i32.shl</span> <span>(</span><span>array.get_u</span> <span>$bytevector</span> <span>(</span><span>local.get</span> <span>$a</span><span>)</span> <span>(</span><span>i32.const</span> <span>1</span><span>)</span><span>)</span>
                <span>(</span><span>i32.const</span> <span>8</span><span>)</span><span>)</span>
       <span>(</span><span>i32.or</span><span>)</span>
       <span>(</span><span>i32.shl</span> <span>(</span><span>array.get_u</span> <span>$bytevector</span> <span>(</span><span>local.get</span> <span>$a</span><span>)</span> <span>(</span><span>i32.const</span> <span>2</span><span>)</span><span>)</span>
                <span>(</span><span>i32.const</span> <span>16</span><span>)</span><span>)</span>
       <span>(</span><span>i32.or</span><span>)</span>
       <span>(</span><span>i32.shl</span> <span>(</span><span>array.get_u</span> <span>$bytevector</span> <span>(</span><span>local.get</span> <span>$a</span><span>)</span> <span>(</span><span>i32.const</span> <span>3</span><span>)</span><span>)</span>
                <span>(</span><span>i32.const</span> <span>24</span><span>)</span><span>)</span>
       <span>(</span><span>i32.or</span><span>)</span><span>)</span><span>)</span></code></pre><p>By contrast, Wasm linear memory needs but a single <code>i32.load</code>
instruction:</p><pre><code><span>(</span><span>module</span>
 <span>(</span><span>memory</span> <span>1</span><span>)</span>
 <span>(</span><span>func</span> <span>(</span><span>export</span> <span>"main"</span><span>)</span> <span>(</span><span>result</span> <span>i32</span><span>)</span>
       <span>(</span><span>i32.store</span> <span>(</span><span>i32.const</span> <span>0</span><span>)</span> <span>(</span><span>i32.const</span> <span>123456789</span><span>)</span><span>)</span>
       <span>(</span><span>i32.load</span> <span>(</span><span>i32.const</span> <span>0</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Easy peasy.  Not only is it less code, it's a lot more efficient.</p><p>The triangle example above uses static vertex data stuffed into
bytevector literals, so it doesn’t hit this problem, but real programs
that have dynamic buffer data will be slower than their linear memory
equivalents.</p><h2>Unsatisfying workarounds</h2><p>There’s no way around the multi-byte problem at the moment, but for
byte access from JavaScript there are some things we could try to work
with what we have been given.  Spoiler alert: None of them are
pleasant.</p><h3>Use Uint8Array from the host</h3><p>This approach makes all binary operations from within the Wasm binary
slow since we’d have to cross the Wasm-&gt;JS bridge for each read/write.
Since most of the binary data manipulation is happening in the Wasm
module, this approach will just make things slower overall.</p><h3>Use linear memory for bytevectors</h3><p>This would require a little <code>malloc</code>/<code>free</code> implementation and a way
to reclaim memory for GC'd bytevectors.  You could register every
bytevector in a <code>FinalizationRegistry</code> in order to be notified upon GC
and <code>free</code> the memory.  Now you have to deal with memory
fragmentation.  This is Wasm GC, we shouldn’t have to do any of this!</p><h3>Use linear memory as a scratch space</h3><p>This avoids crossing the Wasm/JS boundary for each byte, but still
involves a byte-by-byte copy from <code>(array i8)</code> to linear memory within
the Wasm module.  So far this feels like the least worst option, but
the extra copy is still going to greatly reduce throughput.</p><h2>Wasm GC needs some fixin'</h2><p>I’ve used realtime graphics as an example because it’s a use case that
is very sensitive to performance issues, but this unfortunate need to
copy binary data byte-by-byte is also the reason why <a href="https://wingolog.org/archives/2023/10/19/requiem-for-a-stringref">strings are
trash</a>
on Wasm GC right now.
<a href="https://github.com/WebAssembly/stringref">Stringref</a> is a good
proposal and the Wasm community group made a mistake by rejecting it.</p><p>Anyway, there has been some discussion about both
<a href="https://github.com/WebAssembly/gc/issues/395">multi-byte</a> and
<a href="https://github.com/WebAssembly/gc/issues/568"><code>ArrayBuffer</code></a> access
on GitHub, but as far as I can tell neither issue is anywhere close to
a resolution.</p><p>Can these things be implemented efficiently?  How can the need for
direct access to packed arrays from JS be reconciled with Wasm heap
object opaqueness?  I hope the Wasm community group can arrive at
solutions sooner than later because it will take a long time to get
the proposal(s) to phase 4 and shipped in all browsers, perhaps years.
I am getting by making simple things with HTML5 Canvas but it would be
a shame to be effectively shut out from using WebGPU when it finally
reaches stable browser releases.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Skymont: Intel's E-Cores reach for the Sky (120 pts)]]></title>
            <link>https://chipsandcheese.com/p/skymont-intels-e-cores-reach-for-the-sky</link>
            <guid>42750734</guid>
            <pubDate>Sat, 18 Jan 2025 19:27:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/skymont-intels-e-cores-reach-for-the-sky">https://chipsandcheese.com/p/skymont-intels-e-cores-reach-for-the-sky</a>, See on <a href="https://news.ycombinator.com/item?id=42750734">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>The 2020s were a fun time for Intel’s Atom line, which went from an afterthought to playing a major role across Intel’s high performance client offerings. Intel’s latest mobile chip, codenamed Lunar Lake, sees Intel’s high performance P-Cores ditch SMT. With P-Cores focusing even more on single threaded performance, E-Cores will play an even more prominent role in boosting multithreaded performance. With Intel facing stronger competition than ever in the laptop scene, power efficiency is also important. Skymont is Intel’s latest E-Core architecture, and replaces Crestmont in the outgoing Meteor Lake mobile chips.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32534" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg" width="688" height="385" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:385,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32534&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7f2c9b1-db97-431b-84ff-b9192e3ac3a3_1912x1070.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In Lunar Lake, Skymont serves both of those goals. Meteor Lake’s two levels of E-Cores have been combined into one in Lunar Lake, designed both to boost multithreaded performance and handle low priority background tasks. Meteor Lake boosted multithreaded performance with Crestmont E-Cores attached to the ring bus and L3. Two other low power Crestmont cores sat on a low power island to handle background tasks and let the higher performance cores power down more often.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32532" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg" width="688" height="386" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:386,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32532&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71655a0b-dafd-495e-b6f2-051a7792275d_1914x1075.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>Lunar Lake’s quad core Skymont cluster sits on a low power island, letting it handle light background tasks without waking the P-Cores. But it’s also on the same TSMC N3B process node as the P-Cores, and gets an improved cache hierarchy compared to Meteor Lake’s rather weak low power E-Cores. That combination lets Skymont serve both roles with a single core variant. Intel evidently decided more core levels wasn’t a good thing. They also decided just four E-Cores would be adequate, so Skymont has some big shoes to fill.</p><p>Skymont is a substantial step over its predecessor, Crestmont. At a high level, it’s an eight-wide out-of-order core. In many areas, it’s not far off Intel’s P-Cores or latest members of AMD’s Zen line. Skymont of course can’t run at the same clock speeds and can’t compete in absolute performance. However, it’s a good showcase of how sophisticated a density optimized core can get on a modern process node.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32518" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg" width="688" height="353" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:353,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32518&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76792650-42ed-4bb4-86ce-cd5c991f2a23_2200x1129.jpeg 1456w" sizes="100vw"></picture></div></a><figcaption>Keep in mind these block diagrams are an approximation. Digging into a distributed scheduler layout is a nightmare and very error prone</figcaption></figure></div><p>Digging deeper, Skymont is a clear relative of Crestmont. Both use a distributed scheduler layout with a lot of ports, and have distinguishing features like a clustered frontend. However, the scope of Skymont’s changes is pretty huge.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/2024/05/13/meteor-lakes-e-cores-crestmont-makes-incremental-progress/crestmont_drawio/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg" width="688" height="417" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:417,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/2024/05/13/meteor-lakes-e-cores-crestmont-makes-incremental-progress/crestmont_drawio/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6557b51-5832-4758-88fb-121c13efc6a2_1876x1138.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Impressively, Intel was able to deliver Skymont at the same time as Lion Cove, which also delivers large changes over its predecessor. Intel fighting hard to retain the laptop market that it nearly dominated a decade ago, and Lunar Lake is the result of a big company steaming ahead with all boilers lit.</p><p>Branch prediction accuracy affects both performance and power, because wasted work costs both power and core resources downstream. When faced with crazy long random patterns, Skymont doesn’t do quite as well as Intel or AMD’s latest high performance cores. However, it’s a clear step ahead of Crestmont.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skt_branchhist/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png" width="688" height="335" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:335,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skt_branchhist/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96cd2f88-20c0-46ce-b7e4-87bf01ff34e7_1100x536.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If there are a few branches in play, Skymont can deal with a longer repeating random pattern. With 512 branches each going through a different random pattern, Skymont falls apart after the random pattern is longer than 48, a good improvement over 16 on Crestmont. Perhaps Intel has given Skymont’s predictor a lot more storage for branch history.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/2024/05/13/meteor-lakes-e-cores-crestmont-makes-incremental-progress/crestmont_branchhist/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png" width="688" height="405" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:405,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/2024/05/13/meteor-lakes-e-cores-crestmont-makes-incremental-progress/crestmont_branchhist/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4f89390-2c78-48ee-b610-f045009fa2be_1164x686.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Getting a branch’s direction right is only one part of the picture. The point of a branch predictor is to go fast and minimize delays from control flow dependencies. To do so, modern branch predictor can run far ahead of instruction fetch, queueing up cache miss requests and using memory level parallelism to mitigate L2 or even L3 latency. Caching branch targets is crucial because it lets the predictor tell where a branch goes without waiting for it to arrive at the core. Skymont can cache up to 8K branch targets in its last level branch target buffer.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skt_btb/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png" width="688" height="394" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c00aea46-c505-4acd-9ed8-086d165bb594_872x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skt_btb/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc00aea46-c505-4acd-9ed8-086d165bb594_872x500.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Crestmont for comparison has a 6K entry last level BTB. Both E-Cores have smaller BTBs than recent high performance cores. Golden Cove has a 12K entry BTB, and AMD’s Zen 5 has a massive 24K BTB entries. Still, 8K BTB entries is nothing to scoff at. It’s more entries than older high performance cores like Sunny Cove or Zen 2.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/2024/05/13/meteor-lakes-e-cores-crestmont-makes-incremental-progress/crestmont_btb/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png" width="688" height="383" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:383,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/2024/05/13/meteor-lakes-e-cores-crestmont-makes-incremental-progress/crestmont_btb/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F236829cf-97bc-4d35-b1ad-8861e1e3ed53_962x536.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Cache speed matters too, because taken branch latency can reduce frontend throughput. Skymont and Crestmont both have a first level 1024 entry BTB capable of zero bubble taken branches (single cycle latency). It’s faster than the 3-4 cycle latency L2 BTB, but can’t do two taken branches per cycle like Intel and AMD’s latest cores.</p><p>Returns are predicted via a return stack, because they usually go back to a corresponding call site. Skymont has a very deep 128 entry return stack, a feature inherited from Crestmont. Calls and returns have to be spaced by at least one cacheline to utilize this return stack, which complicated microbenchmarking but shouldn’t affect typical applications.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_returnstack/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png" width="688" height="391" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:391,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_returnstack/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ea1ca88-069f-4d5a-a53b-efb29848f5f6_880x500.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>For comparison, AMD’s Zen 5 has 2×52 entry return stacks, one for each thread. Skymont’s P-Core companion, Lion Cove, has a 24 entry return stack.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_spec_bpu_accuracy/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png" width="688" height="348" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:348,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_spec_bpu_accuracy/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1ad11b7-c874-4ce8-b9b4-50908810c64b_1495x757.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In SPEC CPU2017’s workloads, Skymont posts a decent improvement in branch prediction accuracy over its predecessor. The geometric mean of branch prediction accuracy across all subtests goes up from 98.09% to 98.21%. It may seem like a minor step forward, but that’s because many of SPEC CPU2017’s workloads were easy to predict in the first place. Difficult benchmarks like 541.leela, 505.mcf, and 526.blender saw 4.83%, 5%, and 13.58% reductions in branch MPKI respectively.</p><p>Leapfrogging fetch and decode clusters have been a distinguishing feature of Intel’s E-Core line ever since Tremont. Skymont doubles down by adding another decode cluster, for a total of three clusters capable of decoding a total of nine instructions per cycle. Unlike AMD and Intel’s high performance cores, there is no micro-op cache or loop buffer. The fetch and decode path is the primary and only method of instruction delivery.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_ifetch8/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png" width="688" height="304" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:304,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_ifetch8/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68c60a41-9532-47f6-b648-bd60d6fb1bd5_1358x601.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>With 8-byte NOPs, instruction fetch bandwidth maxes out at 48 bytes per cycle. It’s a clear improvement over Crestmont, though I wonder why it’s not better on both. Each decode cluster can fetch 32 bytes per cycle, and the three decode slots in each cluster should only need 24 bytes per cycle in this test. But here, each cluster seems to cap out at 16 bytes per cycle. Lion Cove and Zen 5 can use their micro-op caches to achieve 64 bytes per cycle in this test.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_ifetch4/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png" width="688" height="308" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:308,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_ifetch4/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe135f45a-e5f5-4b82-9c45-4565cae002a5_1357x608.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>4 byte NOPs more closely correspond to average instruction length in integer code, and usually moves the bottleneck to the decoders rather than instruction fetch bandwidth. Even though Skymont has 9 decode slots, sustained throughput is limited to 8 instructions per cycle by the subsequent rename stage.</p><p>For larger code footprints, Skymont and Crestmont can run code from L2 at just under 20 bytes per cycle. That’s good for 4 IPC with 4 byte instructions, which is still plenty considering IPC is often limited to far less than that by factors like backend memory latency.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32579" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg" width="688" height="384" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:384,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32579&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3369719a-0d23-4988-ba78-713339795e3d_1368x763.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Decoders feed micro-ops into queues in front of the renamer, which help absorb short duration delays. Besides a 3-wide decoder, a frontend cluster includes its own micro-op queue. Copy-pasting the cluster again brings micro-op queue capacity from 2×32 = 64 on Crestmont to 3×32 = 96 entries in Skymont. It’s nowhere near the 192 entry micro-op queue capacity on Lion Cove, but it does get close to some older cores. For reference, Zen 4 has a 144 entry micro-op queue.</p><p>Next, the renamer stage sends micro-ops to the backend, allocating entries in all the necessary resources in the process. The renamer in Intel’s E-Cores simultaneously reads from all of the frontend clusters’s micro-op queues, putting the instruction stream back in-order for register renaming. Besides register renaming, the rename/allocate stage often performs other optimizations to break false dependenices and expose more parallelism to the backend.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png" width="1073" height="514" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:514,&quot;width&quot;:1073,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:53944,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b696be8-8579-4632-9fc9-81119dea422d_1073x514.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Skymont can eliminate register to register MOVs and recognize zeroing idioms at full rate. That doesn’t always apply, as subtracting a register from itself no longer seems to be recognized. Compared to Crestmont, Skymont has also gained Golden Cove’s ability to do simple math completely within the renamer. Integer adds with an immediate can execute at more than one per cycle. It’s not as crazy as Golden Cove’s six dependent increments or add-immediates per cycle, but it could still speed up tiny loops by resolving the loop increment dependency chain faster.</p><p>It’s hard to understate the size of Skymont’s out-of-order execution engine. Its reorder buffer (ROB) has 416 entries, up from 256 in Crestmont. The ROB is an in-order list of all in-flight instructions, and an upper bound on how many micro-ops the core has in flight. For that reason, I think ROB capacity gives an idea of the reordering capacity that designers were targeting. Skymont’s ROB is larger than Sunny Cove or Zen 4’s, and not far off the 512 entry ROB in Golden Cove.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32575" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png" width="688" height="285" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0845404-e10b-4892-8cba-d222a73851ec_1072x444.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:285,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32575&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0845404-e10b-4892-8cba-d222a73851ec_1072x444.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Other structures have to be sized appropriately because the rename/allocate stage is in-order, and will stall the moment it hits an instruction it can’t allocate resources for. Skymont’s register files and store queue see substantial size increases, though not by as much as ROB capacity growth.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png" width="1061" height="451" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:451,&quot;width&quot;:1061,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72372,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04d7c771-f211-477a-8087-4cc0633aab95_1061x451.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Crestmont had generously sized register files capable of covering the majority of its ROB capacity. Intel may have decided that was overkill, and rebalanced Skymont’s backend resources to increase reordering capacity in a more efficient way.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32583" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png" width="688" height="496" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:496,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32583&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507b0d8b-fe0c-427e-906c-9e485a8eed9b_708x510.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>An interesting example is Skymont’s reordering capacity for branches, which has dropped compared to Crestmont. A 96 entry branch order buffer can cover 23% of Skymont’s ROB capacity. I wonder if Intel might be cutting it a bit close here, because a couple of SPEC CPU2017 integer workloads have more frequent branches than that. But Intel’s engineers are obviously looking at more than just SPEC, and I trust they know what they’re doing.</p><p>Compared to Crestmont, Skymont’s integer schedulers grow from 16 to 20 entries. There are still four schedulers, but each now has two ports instead of one, letting them feed an extra integer ALU. That doubles Skymont’s throughput for basic operations like integer adds compared to Crestmont.</p><p><span>Back in an </span><a href="https://chipsandcheese.com/2024/07/15/a-video-interview-with-mike-clark-chief-architect-of-zen-at-amd/" rel="">interview</a><span> with Cheese, AMD’s Mike Clark mentioned a unified scheduler can avoid situations where several ops become ready on one scheduling queue, but have to sit in line for that queue’s ALU port. I wonder if putting two ALU ports on each queue is a way around that. If three ALU ops suddenly become ready on one of Skymont’s schedulers, they would start execution over two cycles instead of three.</span></p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32637" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png" width="688" height="317" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:317,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32637&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee8ee3ab-e1ab-4b76-95e3-1f595849ae9a_781x360.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Less commonly used units like integer multipliers and shifters were not scaled up, and execution throughput for those operations remains similar to Crestmont. Skymont however has improved 64-bit integer multiplication latency, which drops from 5 to 4 cycles. It’s not as fast as Zen 5’s 3 cycles, but it’s good to see Intel find ways to cut pipeline stages here and there.</p><p>Skymont gains an extra branch port, letting it handle three not-taken branches per cycle. Throughput for taken branches is still limited to one per cycle because the branch predictor can’t go any faster.</p><p>Floating point and vector execution has never been a strength of Intel’s E-Core line, but Skymont does get substantial upgrades. It now has a quad-pipe FPU. All four pipes can handle basic floating point and integer vector instructions, creating a setup reminiscent of Cortex X2 or Qualcomm Oryon’s. Compared to Crestmont, Intel has grown both the scheduling and non-scheduling queues by a significant amount.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32628" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png" width="688" height="417" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:417,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32628&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b0a7f5f-1a16-4750-9971-f144141df403_900x546.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As before, execution units remain 128-bits wide and 256-bit instructions execute as two micro-ops. A 256-bit vector result will also consume two 128-bit entries in the vector register file. Scheduling capacity for 256-bit operations remains mediocre despite what the numbers above would imply.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32634" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png" width="688" height="321" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:321,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32634&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b6ace75-fa65-4ed3-b2f1-21e33dcc59b2_913x426.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Some of the microbenchmark runs targeting Skymont’s FP/vector unit, using variations of Henry Wong’s </span><a href="https://blog.stuffedcow.net/2013/05/measuring-rob-capacity/" rel="">methodology</a><span> to measure structure sizes</span></figcaption></figure></div><p><span>From measurements, it feels like </span><code>vaddps</code><span>‘s micro-ops can’t get into the non-scheduling queue the way scalar </span><code>addss</code><span> ones can. So, scheduling capacity for those 256-bit packed FP adds is half of the scheduler capacity. It’s better than Crestmont, where I could get 23 </span><code>vaddps</code><span> instructions in flight. That measurement suggests Crestmont is using its non-scheduling queue, though not to its fullest extent.</span></p><p><span>I thought that could be a restriction with instructions that have to be broken into two micro-ops. However, </span><code>cvtsi2ss</code><span> (convert integer to floating point) is handled as a single micro-op and can’t use the non-scheduling queue on either architecture. Perhaps something about Intel’s non-scheduling queue prevents it from being used in certain situations.</span></p><h5>Denormal Behavior</h5><p><span>We </span><a href="https://chipsandcheese.com/2024/06/15/intel-details-skymont/" rel="">previously covered Intel’s presentation on Skymont</a><span>, where they discussed improved denormal handling to prevent “glass jaw” performance behavior. From testing on Skymont, there is indeed no penalty when an operation on normalized floating point numbers produces a subnormal result.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png" width="1063" height="342" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:342,&quot;width&quot;:1063,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:50508,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53906198-a6aa-4c45-9d27-4965c822c371_1063x342.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Curiously, Lion Cove has not received the same fast-path denormal handling hardware. This sort of &gt;100 cycle penalty above can be easily avoided by disabling denormal handling via the flush-to-zero and denormals-are-zero flags. But it’s funny that Intel’s P-Cores have a “glass jaw” corner case that E-Cores now handle quite well.</p><h5>Execution Latency</h5><p>Skymont brings floating point execution latencies down across the board. As with Crestmont, there’s no latency penalty for using 256-bit vectors over 128-bit ones. 256-bit vector integer operations could sometimes see extra latency on Crestmont, possibly because it had an odd number of vector integer ALUs. Skymont improves there, and has no problem achieving single cycle latency for vector integer adds.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png" width="1067" height="362" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:362,&quot;width&quot;:1067,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:52182,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57596407-fb2e-4939-bbd0-a380aa3ab870_1067x362.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Floating point division is not pipelined and has an average latency of 2.5 cycles on Skymont. That’s a match for Zen 5, and better than 5 cycles on Crestmont.</p><p>Address generation gets handled by a massive seven AGUs on Skymont. Three generate load addresses, and four handle store addresses. In both areas, Skymont again gets a big upgrade over Crestmont.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32646" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg" width="479" height="364" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:364,&quot;width&quot;:479,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32646&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70270165-1395-48ab-8a6e-591e658068cf_479x364.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Skymont’s four store AGUs may seem like overkill because the data cache can only handle two stores per cycle. However, more AGUs can help find memory dependencies faster.</p><p>As for finding those memory dependencies, Skymont behaves similarly to Crestmont. Dependencies appear to be checked at 4B granularity. Either half of a 64-bit store can be forwarded to a dependent 32-bit load. Crestmont could do zero latency forwarding for an exact address match if both the load and store are 64B aligned. Skymont expands that to cover more cases, though I don’t see any clear pattern to zero latency forwarding works. Even when it doesn’t, Skymont’s 2 cycle forwarding latency is faster than Crestmont’s 3-7 cycle latency.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32655" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png" width="688" height="329" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:329,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32655&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F058dc8b6-11bd-49b0-93b0-6829f4cdc377_2560x1225.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Forwarding the upper half of a 64-bit store to a dependent 32-bit load takes 7-8 cycles, a bit longer than 6-7 cycles on Crestmont. Other overlap cases cause the forwarding mechanism to fail with a 14-15 cycle penalty, which is worse than Crestmont’s 11-12 cycle penalty. Just as with Crestmont, that penalty applies if a load and store access the same 4B aligned region, even if they don’t actually overlap. Intel and AMD’s high performance cores don’t suffer from that false dependency case, and generally have more flexible store forwarding mechanisms.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32679" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png" width="688" height="331" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:331,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32679&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7068bdbf-31dc-4956-a797-a6e41cc1eb3e_2560x1231.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>For example, Lion Cove and Zen 5 can do store forwarding across cacheline boundaries. Even when there are no memory dependencies, Zen 5 is notably more robust with misaligned accesses. A misaligned store executes over two cycles on Skymont, Crestmont, and Lion Cove, but can execute over a single cycle on Zen 5.</p><p>The load/store unit also has to translate program-visible virtual addresses to physical addresses that correspond to locations in DRAM. Operating systems set up multi-level tables to tell CPUs how they should map virtual addresses to physical ones, but accessing those tables (called a page walk) would introduce a lot of extra latency. To counter this, CPUs cache frequently used address translations in Translation Lookaside Buffers, or TLBs.</p><p>Skymont continues to have a rather small 48 entry L1 DTLB. However, L2 TLB capacity increases from 3072 to 4096 entries. The L2 TLB on Skymont is 4-way set associative, and accessing it costs an extra 9 cycles of latency jus like on Crestmont. Neither E-Core has a particularly fast L2 TLB, but getting a translation from it is far better than doing a page walk. Intel’s priority here seems to be avoiding expensive page walks rather than improving speed for TLB hits.</p><p>For context, Skymont matches AMD’s Zen 5 in L2 TLB entry count, though AMD’s 16-way set associative L2 TLB should make conflict misses less common. Skymont’s P-Core companion, Lion Cove, only has 2048 L2 TLB entries.</p><p>Data cache latency has regressed to four cycles on Skymont, compared to three cycles on Crestmont. Skymont’s L2 cache is more impressive, dropping latency from 20 to 19 cycles while providing twice as much capacity.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32626" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png" width="688" height="363" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/952f275b-2649-4640-b638-720fa7740054_1030x544.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:363,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32626&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F952f275b-2649-4640-b638-720fa7740054_1030x544.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>L2 misses head to a 8 MB memory side cache. Estimating memory side cache latency is difficult because testing with a 8 MB array will result in about half the accesses hitting in L2, assuming a non-LRU L2 replacement policy. At the 8 MB test size though, latency is 59.5 ns or 214 cycles. Since error from hitting L2 will only cause the test to underestimate memory side cache latency, Lunar Lake’s memory side cache has at least that much latency when accessed from the E-Core cluster.</p><p>59.5 ns is nearly as high as DRAM access latency on older systems. For example, the AMD FX-8350 has 61.7 ns of latency with a 1 GB test size and 2 MB pages. Lunar Lake’s memory side cache is very much closer to memory than CPU cores as its name suggests, and doesn’t provide comparable performance to Meteor Lake’s 24 MB L3.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32650" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png" width="481" height="289" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:289,&quot;width&quot;:481,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32650&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd9fd7a-eb90-439f-bc40-b030654dbf96_481x289.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As with Lunar Lake’s P-Cores, DRAM latency measurements are complicated by the memory controller staying in a low power state if there isn’t enough traffic. A simple memory latency test sees about 170 ns with a 1 GB test size, but that drops to 133 ns if another core pulls just over 8 GB/s of bandwidth. It’s better than 175 ns or 153 ns on Meteor Lake’s low power and standard Crestmont cores respectively, but nowhere near as good as desktop DDR5.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32669" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png" width="688" height="308" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:308,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32669&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff838c6fb-71fc-4187-97d5-7bd818a869b8_1330x595.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Skymont has a first level cache bandwidth advantage thanks to its additional 128-bit load port, but things are more evenly matched at lower cache levels. A single Skymont core can sustain 28-29 bytes per cycle from L2, a slight improvement over Crestmont’s 25-26 bytes per cycle. Skymont’s larger L2 is also appreciated. For larger test sizes, Crestmont benefits from Meteor Lake’s 24 MB L3. Lunar Lake’s 8 MB memory side cache really doesn’t help much. Usually memory bandwidth from a single core is latency limited, so this is another hint that latency is very high on Lunar Lake’s memory side cache.</p><p>Low power Crestmont is in a completely different class, and not in a good way. Its low clock speed and lack of a L3 cache put it far behind. Meteor Lake’s higher memory latency puts the proverbial nail in the coffin for low power Crestmont’s performance.</p><p>Multithreaded applications generally demand more bandwidth, and here Skymont posts clearer L2 bandwidth gains against its predecessor. Crestmont’s L2 could only provide 64 bytes per cycle, and that bandwidth has to be shared by all four cores in a cluster. Skymont increases L2 bandwidth to 128 bytes per cycle, doubling L2 bandwidth available to each core.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32685" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png" width="688" height="357" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:357,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32685&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63cd0db9-ac67-4268-b9cb-f7444a1afbc2_1321x685.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Outside the cluster, Crestmont’s L3 cache plays out against Skymont’s memory side cache. Neither has a significant advantage, and both E-Core clusters appear to be latency rather than bandwidth limited. Once the test gets out of Meteor Lake’s L3 though, the Skymont cluster can access a lot more DRAM bandwidth. However, AMD Strix Point’s Zen 5c cluster can achieve 61 GB/s from DRAM, limited only by the Infinity Fabric link.</p><p>Intel compares Lunar Lake’s Skymont cores to Meteor Lake’s low power Crestmont Cores, and it’s easy to understand why. Skymont sits on a low power island just as low power Crestmont does on Meteor Lake. Both cores aim to offload background tasks, letting higher performance cores sleep.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/hc2024_skymont_perf_slide/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png" width="688" height="386" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:386,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/hc2024_skymont_perf_slide/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a8de722-d978-42ce-8ad3-ae69556ea0dd_1919x1078.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Against low power Crestmont, Skymont benefits from better caching and better clocks courtesy of TSMC’s leading edge N3B process. Unsurprisingly, Skymont posts a huge 78.3% performance advantage in SPEC CPU2017’s integer suite. Skymont’s lead grows to a staggering 83.8% in the floating point suite.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_spec/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png" width="688" height="458" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:458,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_spec/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d0158c2-435d-4184-8077-7fd04b58377f_713x475.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But in my opinion, comparisons against low power Crestmont only provide half the picture. Skymont is meant to boost multithreaded performance too, like Crestmont Cores on. Meteor Lake uses Crestmont cores on the Intel 4 Compute Tile to fill that role. Therefore, a comparison against those Compute Tile Crestmont cores is also appropriate.</p><p>That comparison is less favorable to Skymont, which only manages a 0.68% performance gain in SPEC CPU2017’s integer tests. I consider that within margin of error, and even if it weren’t, no one would notice that in practice. Gains in the floating point suite are better at 15.7%, but I’d like to see double digit gains in both suites for such a large architecture change. I suspect Skymont would indeed provide double digit percentage gains given identical cache setups. However, giving Crestmont a 24 MB L3 and a 100 MHz clock speed advantage seems to be enough to cancel out Skymont’s improved architecture.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_specint/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png" width="688" height="732" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dc829d15-4334-43a2-8564-f61c3551460e_754x802.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:732,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_specint/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdc829d15-4334-43a2-8564-f61c3551460e_754x802.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>AMD’s Zen 5c is another appropriate comparison, as the Strix Point mobile chip features eight of those to improve multithreaded performance. Low clock speed on AMD’s density optimized core lets Skymont lead by a hair in SPEC CPU2017, though I still consider a 1.5% gain within margin of error. The floating point suite sees AMD’s Zen 5c pull ahead by a margin of 20.4%, partially because Zen 5c destroys Intel’s E-Cores on 503.bwaves and 549.fotonik3d.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/skymont_specfp/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png" width="688" height="789" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:789,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/skymont_specfp/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc489827-b0ba-4b2b-9877-590525d6b6e0_752x862.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>While Skymont turns in a decent showing against its predecessor in SPEC CPU2017’s floating point suite, I feel SPEC’s reliance on compiler code generation doesn’t paint a complete picture. Software projects often use </span><a href="https://gitlab.com/AOMediaCodec/SVT-AV1/-/tree/master/Source/Lib/ASM_AVX2?ref_type=heads" rel="">intrinsics </a><span>or </span><a href="https://code.videolan.org/videolan/x264/-/tree/master/common/x86" rel="">assembly </a><span>if they require high performance. libx264 is one of them. Software video encoding offers better quality than hardware encoders by being more computationally expensive, and even the older H.264 codec is no exception.</span></p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32663" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png" width="481" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:481,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32663&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c740770-27e9-48ce-852d-10131bb7c8d2_481x288.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In this workload, the Core Ultra 7 258V’s quad core Skymont cluster loses to a quad core Crestmont cluster on the Core Ultra 7 155H. Crestmont wins by just over 5%, though performance counters reported a 8.1% IPC advantage for Crestmont (1.46 IPC vs 1.35 IPC on Skymont). Skymont’s branch predictor does shine through, delivering 97.52% accuracy compared to Crestmont’s 97.35%. But it’s not enough to push Skymont ahead.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32692" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png" width="481" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:481,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32692&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b79d32f-12b6-48f0-8d98-684a5cc357b7_481x288.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Using event 0x2E and unit masks 0x4F/0x41 for LLC references and misses</figcaption></figure></div><p>Skymont’s cache setup is a clear culprit. While Intel hasn’t documented performance counters for Skymont or Lion Cove yet, they do have a set of “architectural” performance events introduced with the Core Duo and Core Solo generation. Those are guaranteed to work on subsequent Intel CPUs, and include events for longest latency cache references and misses. What those events map to can vary of course. From testing, Skymont considers the 4 MB L2 the “longest latency cache”. And those events show Skymont sees far more misses with its 4 MB L2 than Crestmont does with its 24 MB L3.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32667" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png" width="481" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/73377467-065d-44d9-b9a6-69e622074960_481x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:481,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32667&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73377467-065d-44d9-b9a6-69e622074960_481x288.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Tested using the Broadwell binary</figcaption></figure></div><p>Y-Cruncher is a very heavily vectorized program that calculates Pi digits. Skymont really shows its advantage in this workload, posting a 1.56x speedup over its predecessor. Skymont also averaged 1.81 instructions per active core cycle, again a huge improvement over Cresmtont’s 1.22 IPC.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32707" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png" width="688" height="593" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:593,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32707&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F129f4cb6-cdf7-436a-a223-2d1cacded84e_710x612.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>IPC, from performance counters. High IPC tasks tend to benefit more from a wider core, while lower IPC ones benefit from mitigating whatever the biggest bottleneck is (often backend memory access latency or frontend latency)</figcaption></figure></div><p><span>Performance-wise, Skymont seems to be at its best in high IPC workloads with a small cache footprint. For example Skymont beats Crestmont by 20.8% in 548.exchange2, a workload that </span><a href="https://chipsandcheese.com/2024/09/19/running-spec-cpu2017-at-chips-and-cheese/" rel="">fits in Zen 4’s 32 KB L1D cache</a><span>. There, Crestmont’s already high 3.39 IPC increases to 4.21 with Skymont’s improvements. Conversely 520.omnetpp sees 10.38 MPKI on Zen 4’s 1 MB L2, and 1.42 MPKI for its 32 MB L3. On that test, Skymont drops to 0.54 IPC, while Crestmont holds up better at 0.62 IPC. However if a workload is really cache unfriendly, Skymont’s ability to pull more memory bandwidth can show through. I suspect that’s what happens in Y-Cruncher and 549.fotonik3d, as both are very memory bandwidth bound on other architectures. There, Skymont posts huge gains.</span></p><p>Skymont is a huge step over Crestmont. Nearly every part of the core is beefed up in some way, and often significantly so. After seeing both P-Cores and E-Cores barely change from Alder Lake to Meteor Lake, I’m happy to see massive progress on both of Intel’s core lines. Assessing Lion Cove was straightforward because the core delivered a typical gen-on-gen improvement despite its changed cache hierarchy. Skymont’s situation is more complicated.</p><p>Certainly Lunar Lake’s Skymont cores crush low power Crestmont. But standard Crestmont already posts huge gains over its low power variant, thanks to a better cache setup and better process node. Despite massive architecture improvements, Skymont’s performance is hit or miss compared to Crestmont. Lunar Lake’s different cache hierarchy plays a large role in this, and highlights the difficulties in having one core setup play both the low power and multithreaded performance roles. It also highlights the massive role caches play in CPU performance. Even a dramatically improved core can struggle to deliver gains if the cache subsystem doesn’t keep up. That’s especially important with LPDDR5X, which has high latency and can be a handicap in low core count workloads.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32701" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg" width="688" height="387" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:387,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32701&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5571849e-7494-4488-83bf-1cbd5cb8a2be_1593x895.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Slide from Intel’s Hot Chips 2024 presentation on Lunar Lake, showing the Skymont cluster mostly containing a Teams workload</figcaption></figure></div><p>That said, Lunar Lake’s aims for better power efficiency, not necessarily for better performance. Skymont’s implementation in Lunar Lake is consistent with targeting lower power. Four Skymont cores are not going to do well against either eight standard Crestmont cores in Meteor Lake or eight Zen 5c cores in AMD’s Strix Point. But they do stand a better chance of containing low intensity workloads like videoconferencing, which are just a bit too demanding for Meteor Lake’s low power Crestmont cluster. Perhaps Lunar Lake’s Skymont cluster focuses harder on containing such workloads. Boosting multithreaded performance is a secondary concern, and landing in the same performance ballpark as Crestmont is good enough.</p><div><figure><a target="_blank" href="https://chipsandcheese.com/?attachment_id=32705" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg" width="688" height="388" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:388,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://chipsandcheese.com/?attachment_id=32705&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d29ae1e-51e6-4703-9648-e62ba85ab471_1585x895.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>While the LPE-Core cluster on Meteor Lake does a comparatively poor job</figcaption></figure></div><p>When performance really matters, Lion Cove comes into play and does deliver a typical gen-on-gen improvement over Redwood Cove. Thus Lunar Cove should improve performance across a decent range of consumer applications, while also improving power efficiency in long duration, low intensity tasks. Intel designed their chip to meet those goals and I think their decisions made a lot of sense. But I was personally curious about how Skymont’s huge architecture changes would play out. Lunar Lake unfortunately isn’t the right place to evaluate that.</p><p><span>Again, we would like to thank ASUS for sending us over a Zenbook S 14 for review and if you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;</span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span>&nbsp;or our&nbsp;</span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;</span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon's AI crawler is making my Git server unstable (566 pts)]]></title>
            <link>https://xeiaso.net/notes/2025/amazon-crawler/</link>
            <guid>42750420</guid>
            <pubDate>Sat, 18 Jan 2025 18:48:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xeiaso.net/notes/2025/amazon-crawler/">https://xeiaso.net/notes/2025/amazon-crawler/</a>, See on <a href="https://news.ycombinator.com/item?id=42750420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
    
    
    <p>
        Published on <time datetime="2025-01-17">01/17/2025</time>, 259 words, 1 minutes to read
    </p>

    
        <p>Please, just stop.</p>
    

    
        
            
    

    

        
    

    

    <p>EDIT(2025-01-18 19:00 UTC):</p>
<p>I give up. I moved the Gitea server back behind my VPN. I'm working on a proof of work reverse proxy to protect my server from bots in the future. I'll have it back up soon.</p>
<hr>
<p>EDIT(2025-01-17 17:50 UTC):</p>
<p>I added this snippet to the ingress config:</p>
<pre><code><span><span>nginx.ingress.kubernetes.io/configuration-snippet</span><span>:</span> <span>|</span><span>
</span></span><span><span>  if ($http_user_agent ~* "(Amazon)" ){
</span></span><span><span>    return 418;
</span></span><span><span>  }</span>
</span></code></pre>
<p>The bots are still hammering away from a different IP every time. About 10% of the requests do not have the amazonbot user agent. I'm at a loss for what to do next.</p>
<p>I hate the future.</p>
<hr>
<p>Hi all. This is a different kind of post. This is not informative. This is a cry for help.</p>
<p>To whoever runs AmazonBot, please add <code>git.xeserv.us</code> to your list of blocked domains. If you know anyone at Amazon, please forward this to them and ask them to forward it to the AmazonBot team.</p>
<p>Should you want to crawl my git server for some reason, please reach out to me so we can arrange for payment for hardware upgrades commensurate to your egregious resource usage.</p>
<p>I don't want to have to close off my Gitea server to the public, but I will if I have to. It's futile to block AI crawler bots because they lie, change their user agent, use residential IP addresses as proxies, and more. I just want the requests to stop.</p>
<p>I have already configured the <code>robots.txt</code> file to block all bots:</p>
<pre><code><span>User-agent: *
</span><span>Disallow: /
</span></code></pre>
<p>What else do I need to do?</p>

    <hr>

    

    

    <p>Facts and circumstances may have changed since publication. Please contact me before jumping to conclusions if something seems wrong or unclear.</p>

    <p>Tags: </p>
</article>
        </div><div>
            <p>Copyright 2012-2025 Xe Iaso. Any and all opinions listed here are my own and
                not representative of any of my employers, past, future, and/or present.</p>
            
            <p>Served by xesite v4 (/app/xesite) with site version 
                        <a href="https://github.com/Xe/site/commit/96544596603771529cfae4ebf51f00d1dce18b6e">96544596</a>
                    , source code available <a href="https://github.com/Xe/site">here</a>.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VS Code Pets (431 pts)]]></title>
            <link>https://github.com/tonybaloney/vscode-pets</link>
            <guid>42750195</guid>
            <pubDate>Sat, 18 Jan 2025 18:17:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tonybaloney/vscode-pets">https://github.com/tonybaloney/vscode-pets</a>, See on <a href="https://news.ycombinator.com/item?id=42750195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">VS Code Pets</h2><a id="user-content-vs-code-pets" aria-label="Permalink: VS Code Pets" href="#vs-code-pets"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tonybaloney/vscode-pets/raw/main/icon.png"><img src="https://github.com/tonybaloney/vscode-pets/raw/main/icon.png" alt="icon"></a></p>
</div>    
<div dir="auto"><p>
    Puts a small, bored cat, an enthusiastic dog, a feisty snake, a rubber duck, or Clippy 📎 in your code editor to boost productivity.
    </p><p>
    
    <a href="https://github.com/tonybaloney/vscode-pets/issues/new?assignees=&amp;labels=feature&amp;template=bug_report.md&amp;title=">Report a Bug</a>
    ·
    <a href="https://github.com/tonybaloney/vscode-pets/issues/new?assignees=&amp;labels=feature&amp;template=feature_request.md&amp;title=">Request feature</a></p></div>
<p dir="auto"><a href="https://marketplace.visualstudio.com/items?itemName=tonybaloney.vscode-pets&amp;WT.mc_id=python-17801-anthonyshaw" rel="nofollow"><img src="https://camo.githubusercontent.com/ff039a862a07110ef39ff2a0c7e8511c474f979ef20b8617fbb06b6e4e59bd8f/68747470733a2f2f696d672e736869656c64732e696f2f76697375616c2d73747564696f2d6d61726b6574706c6163652f762f746f6e7962616c6f6e65792e7673636f64652d706574733f636f6c6f723d626c7565266c6f676f3d76697375616c2d73747564696f" alt="Visual Studio Marketplace Version" data-canonical-src="https://img.shields.io/visual-studio-marketplace/v/tonybaloney.vscode-pets?color=blue&amp;logo=visual-studio"></a>
<a href="https://marketplace.visualstudio.com/items?itemName=tonybaloney.vscode-pets&amp;WT.mc_id=python-17801-anthonyshaw" rel="nofollow"><img src="https://camo.githubusercontent.com/e13577de308cf988b3246a7b6749a588b699524f776e353ef83f7b930fb52477/68747470733a2f2f696d672e736869656c64732e696f2f76697375616c2d73747564696f2d6d61726b6574706c6163652f692f746f6e7962616c6f6e65792e7673636f64652d706574733f6c6f676f3d76697375616c73747564696f" alt="Visual Studio Marketplace Installs" data-canonical-src="https://img.shields.io/visual-studio-marketplace/i/tonybaloney.vscode-pets?logo=visualstudio"></a>
<a href="https://marketplace.visualstudio.com/items?itemName=tonybaloney.vscode-pets&amp;WT.mc_id=python-17801-anthonyshaw" rel="nofollow"><img src="https://camo.githubusercontent.com/3cb2825243f710557ae49e2648ed067c481dc7ae0ef021b1974ac7fa77713a09/68747470733a2f2f696d672e736869656c64732e696f2f76697375616c2d73747564696f2d6d61726b6574706c6163652f642f746f6e7962616c6f6e65792e7673636f64652d706574733f6c6f676f3d76697375616c73747564696f" alt="Visual Studio Marketplace Downloads" data-canonical-src="https://img.shields.io/visual-studio-marketplace/d/tonybaloney.vscode-pets?logo=visualstudio"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tonybaloney/vscode-pets/raw/main/docs/source/_static/screenshot.gif"><img src="https://github.com/tonybaloney/vscode-pets/raw/main/docs/source/_static/screenshot.gif" alt="screenshot" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Install this extension from the <a href="https://marketplace.visualstudio.com/items?itemName=tonybaloney.vscode-pets&amp;WT.mc_id=python-17801-anthonyshaw" rel="nofollow">VS Code marketplace</a>.</p>
<p dir="auto">OR</p>
<p dir="auto">With VS Code open, search for <code>vscode-pets</code> in the extension panel (<code>Ctrl+Shift+X</code> on Windows/Linux or <code>Cmd(⌘)+Shift+X</code> on MacOS) and click install.</p>
<p dir="auto">OR</p>
<p dir="auto">With VS Code open, launch VS Code Quick Open (<code>Ctrl+P</code> on Windows/Linux or <code>Cmd(⌘)+P</code> on MacOS), paste the following command, and press enter.</p>
<p dir="auto"><code>ext install tonybaloney.vscode-pets</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using VS Code Pets</h2><a id="user-content-using-vs-code-pets" aria-label="Permalink: Using VS Code Pets" href="#using-vs-code-pets"></a></p>
<p dir="auto">Congrats on installing joy! Enjoy interacting with these cute pixelated pets. Read below to get a full understanding of this extension. Not convinced? Watch our extension spotlight on <a href="https://www.youtube.com/watch?v=aE6Ifj_KstI" rel="nofollow">Visual Studio Code</a>.</p>
<p dir="auto">After installing, open the command palette with <code>Ctrl+Shift+P</code> on Windows/Linux or <code>Cmd(⌘)+Shift+P</code> on MacOS.</p>
<p dir="auto">Run the "Start pet coding session" command (<code>vscode-pets.start</code>) to see a cat in VS Code:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tonybaloney/vscode-pets/raw/main/docs/source/_static/pet-in-default-explorer.png"><img src="https://github.com/tonybaloney/vscode-pets/raw/main/docs/source/_static/pet-in-default-explorer.png" alt="Default view"></a></p>
<p dir="auto"><a href="https://tonybaloney.github.io/vscode-pets" rel="nofollow">Now checkout the documentation to see what else is possible!</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Translation</h2><a id="user-content-translation" aria-label="Permalink: Translation" href="#translation"></a></p>
<p dir="auto">Visit the <a href="https://crowdin.com/project/vscode-pets" rel="nofollow">Crowdin Project</a> in case you'd like to help with the translations. It will be synced automatically to the repository. You can also request a new language in the <a href="https://crowdin.com/project/vscode-pets/discussions" rel="nofollow">Discussions</a> section.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">The cat animations were designed by <a href="https://seethingswarm.itch.io/catset" rel="nofollow">seethingswarm</a>. The dog media assets for this extension were designed by <a href="https://nvph-studio.itch.io/dog-animation-4-different-dogs" rel="nofollow">NVPH Studio</a>.</p>
<p dir="auto">The forest theme was designed by <a href="https://edermunizz.itch.io/free-pixel-art-forest" rel="nofollow">edermunizz</a>. The castle assets were created using artwork by <a href="https://guttykreum.itch.io/gothic-castle-game-assets" rel="nofollow">GuttyKreum</a>.</p>
<p dir="auto"><a href="https://twitter.com/marcduiker" rel="nofollow">Marc Duiker</a> created the Clippy, Rocky, Zappy, rubber duck, snake, cockatiel, Ferris the crab, and Mod the dotnet bot media assets.</p>
<p dir="auto"><a href="https://twitter.com/pixelthen" rel="nofollow">Elthen</a> created the fox media assets.</p>
<p dir="auto"><a href="https://www.aldeka.net/" rel="nofollow">Karen Rustad Tölva</a> designed the original concept of Ferris the crab.</p>
<p dir="auto"><a href="https://github.com/kevin2huang">Kevin Huang</a> created the Akita inu media assets.</p>
<p dir="auto">The turtle animations were designed by enkeefe using <a href="https://www.pixilart.com/draw" rel="nofollow">Pixelart</a>.</p>
<p dir="auto">The horse animations were adapted by <a href="https://github.com/thechriskent">Chris Kent</a> from assets by <a href="https://onfe.itch.io/horse-sprite-with-rider-asset-pack" rel="nofollow">Onfe</a>.</p>
<p dir="auto"><a href="https://github.com/WoofWoof0">Kennet Shin</a> created the snail media assets.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thank you</h2><a id="user-content-thank-you" aria-label="Permalink: Thank you" href="#thank-you"></a></p>
<p dir="auto">Thanks to all the <a href="https://github.com/tonybaloney/vscode-pets/graphs/contributors">contributors</a> to this project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[O1 isn't a chat model (and that's the point) (148 pts)]]></title>
            <link>https://www.latent.space/p/o1-skill-issue</link>
            <guid>42750096</guid>
            <pubDate>Sat, 18 Jan 2025 18:04:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latent.space/p/o1-skill-issue">https://www.latent.space/p/o1-skill-issue</a>, See on <a href="https://news.ycombinator.com/item?id=42750096">Hacker News</a></p>
Couldn't get https://www.latent.space/p/o1-skill-issue: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Interactive systemd – a better way to work with systemd units (474 pts)]]></title>
            <link>https://isd-project.github.io/isd/</link>
            <guid>42749402</guid>
            <pubDate>Sat, 18 Jan 2025 16:22:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isd-project.github.io/isd/">https://isd-project.github.io/isd/</a>, See on <a href="https://news.ycombinator.com/item?id=42749402">Hacker News</a></p>
Couldn't get https://isd-project.github.io/isd/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dusa Programming Language (Finite-Choice Logic Programming) (155 pts)]]></title>
            <link>https://dusa.rocks/docs/</link>
            <guid>42749147</guid>
            <pubDate>Sat, 18 Jan 2025 15:45:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dusa.rocks/docs/">https://dusa.rocks/docs/</a>, See on <a href="https://news.ycombinator.com/item?id=42749147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Dusa is a logic programming language designed by
<a href="https://typesafety.net/rob/">Rob Simmons</a> and
<a href="https://www.khoury.northeastern.edu/home/cmartens/">Chris Martens</a>,
the first implementation of finite-choice logic programming.</p>
<ul>
<li>If you’ve heard of Datalog (as implemented in systems like
<a href="https://souffle-lang.github.io/program">Soufflé</a>), you may want to start by
reading about how <a href="https://dusa.rocks/docs/introductions/datalog/">Dusa is datalog</a>.</li>
<li>If you’ve heard of answer set programming (as implemented in systems
like <a href="https://potassco.org/">Potassco</a>), you may want to start by reading
about how <a href="https://dusa.rocks/docs/introductions/asp/">Dusa is answer set programming</a>.</li>
<li>If you have no familarity with either of these, that’s okay too! You may
want to start by reading about how
<a href="https://dusa.rocks/docs/introductions/graph/">Dusa is a graph exploration language</a>.
Then you can take a look at some of the other introductions, or
<a href="https://dusa.rocks/">fiddle with some of the default examples</a>.</li>
<li>If you’re interested in the mathematics of finite-choice logic programming,
the paper
<a href="https://popl25.sigplan.org/details/POPL-2025-popl-research-papers/13/Finite-Choice-Logic-Programming">Finite-Choice Logic Programming</a>
by Martens, Simmons, and Michael Arntzenius may be of interest.</li>
</ul>
<p>The easiest way to use Dusa is in our <a href="https://dusa.rocks/">web editor</a>.
Dusa is also available as a command-line utility and JavaScript API via the
<a href="https://www.npmjs.com/package/dusa">Node package manager</a>.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why do bees die when they sting you? (303 pts)]]></title>
            <link>https://www.subanima.org/bees/</link>
            <guid>42749069</guid>
            <pubDate>Sat, 18 Jan 2025 15:32:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.subanima.org/bees/">https://www.subanima.org/bees/</a>, See on <a href="https://news.ycombinator.com/item?id=42749069">Hacker News</a></p>
Couldn't get https://www.subanima.org/bees/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>