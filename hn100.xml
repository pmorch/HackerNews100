<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 22 Dec 2025 13:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[If you don't design your career, someone else will (2014) (152 pts)]]></title>
            <link>https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/</link>
            <guid>46352930</guid>
            <pubDate>Mon, 22 Dec 2025 10:27:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/">https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/</a>, See on <a href="https://news.ycombinator.com/item?id=46352930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

	<p>A client once responded to one of my questions by saying, “Oh Greg, I am too busy living to think <a href="https://gregmckeown.com/about/">about</a> life!” His off-the-cuff comment named a trap all of us fall into sometimes. In just one example, it is easy to become so consumed&nbsp;<em>in&nbsp;</em>our careers we fail to really think&nbsp;<em><a href="https://gregmckeown.com/about/">about</a>&nbsp;</em>our careers.</p>
<p>To avoid this trap, I suggest carving out a couple of hours over the holiday break to follow these simple steps for reflecting on your career.</p>
<p><strong>Step 1: <a href="https://gregmckeown.com/books/effortless/review/">Review</a> the last 12 months</strong>. <a href="https://gregmckeown.com/books/effortless/review/">Review</a> the year, month by month. Make a list of where you spent your time: include your major projects, responsibilities and accomplishments. No need to overcomplicate this.</p>
<p><strong>Step 2: Ask, “What is the <a href="https://gregmckeown.com/news/">news</a>?”</strong>&nbsp;Look over your list and reflect on what is&nbsp;<em>really</em>&nbsp;going on. Think like a journalist and ask yourself: Why does this matter? What are the trends here? What happens if these trends continue?</p>
<p><strong>Step 3: Ask “What would I do in my career if I could do</strong><strong>&nbsp;<em>anything</em>?”&nbsp;</strong>Just brainstorm with no voice of criticism to hold you back. Just write out all the ideas that come to mind.</p>
<p><strong>Step 4: Go back and spend a bit more time on Step 3.</strong>&nbsp;Too often we begin our career planning with our second best option in mind. We have a sense of what we would most love to do but we&nbsp;<em>immediately</em>&nbsp;push it aside. Why? Typically because “it is not realistic” which is code for, “I can’t make money doing this.” In this economy—in any economy—I understand why making money is critical. However, sometimes we pass by legitimate career paths because we set them aside too quickly.</p>
<p><strong>Step 5: Write down six objectives for the next 12 months.</strong>&nbsp;Make a list of the top six items you would like to accomplish in your career this year and place them in priority order.</p>
<p><strong>Step 6: Cross off the bottom five.</strong>&nbsp;Once you’re back to the whirlwind of work you’ll benefit from having a single “true north” career objective for the year.</p>
<p><strong>Step 7: Make an action plan for this month.</strong>&nbsp;Make a list of some quick wins you’d like to have in place over the next 3-4 weeks.</p>
<p><strong>Step 8: Decide what you will say no to.</strong>&nbsp;Make a list of the “good” things that will keep you from achieving your one “great” career objective. Think <a href="https://gregmckeown.com/about/">about</a> how to delete, defer or delegate these other tasks.&nbsp;<a href="http://en.wikipedia.org/wiki/Ralph_Waldo_Emerson" target="_blank">Ralph Waldo Emerson</a>&nbsp;said, “The crime which bankrupts men and nations is that of turning aside from one’s main purpose to serve a job here and there.”</p>
<p>Many years ago I followed this process and, without exaggeration, it changed the course of my life. The insight I gained led me to quit law school, leave England and move to America and start down the path as a teacher and author. You’re reading this because of that choice. It remains the single most important career decision of my life.</p>
<p>Two hours spent wisely over the next couple of weeks could easily improve the quality of your life over the 8760 hours of the next year–and perhaps far beyond. After all,&nbsp;<em>if we don’t design our careers, someone else will.</em></p>

	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Android apps using Rust and Iced (115 pts)]]></title>
            <link>https://github.com/ibaryshnikov/android-iced-example</link>
            <guid>46350641</guid>
            <pubDate>Mon, 22 Dec 2025 02:14:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ibaryshnikov/android-iced-example">https://github.com/ibaryshnikov/android-iced-example</a>, See on <a href="https://news.ycombinator.com/item?id=46350641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Example of building android app with iced</h2><a id="user-content-example-of-building-android-app-with-iced" aria-label="Permalink: Example of building android app with iced" href="#example-of-building-android-app-with-iced"></a></p>
<p dir="auto">There are <a href="https://github.com/ibaryshnikov/android-iced-example/blob/master/NativeActivity">NativeActivity</a> and <a href="https://github.com/ibaryshnikov/android-iced-example/blob/master/GameActivity">GameActivity</a> examples here.</p>
<p dir="auto">Based on several other examples:</p>
<ul dir="auto">
<li><code>na-mainloop</code> and <code>agdk-mainloop</code> from
<a href="https://github.com/rust-mobile/android-activity/tree/v0.6.0/examples">android-activity</a></li>
<li><a href="https://github.com/rust-mobile/rust-android-examples/tree/main/na-winit-wgpu">na-winit-wgpu</a>
from <code>rust-android-examples</code></li>
<li><a href="https://github.com/iced-rs/iced/tree/0.14/examples/integration">integration</a>
from <code>iced</code></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Preview</h2><a id="user-content-preview" aria-label="Permalink: Preview" href="#preview"></a></p>
<p dir="auto">iced integration example</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/pixel_1.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/pixel_1.png" alt="Pixel first screenshot"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/pixel_2.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/pixel_2.png" alt="Pixed second screenshot"></a></p>
<p dir="auto">You can also run most of the examples from iced.
For this omit the scene rendering part and set the background of the root container.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Watch</h2><a id="user-content-watch" aria-label="Permalink: Watch" href="#watch"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/watch_1.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/watch_1.png" alt="Watch first"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/watch_2.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/watch_2.png" alt="Watch second"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/watch_3.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/watch_3.png" alt="Watch third"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Text input</h2><a id="user-content-text-input" aria-label="Permalink: Text input" href="#text-input"></a></p>
<p dir="auto">Text input partially works, unresolved issues:</p>
<ul dir="auto">
<li>window doesn't resize on show/hide soft keyboard</li>
<li>how to change input language of soft keyboard</li>
<li>ime is not supported</li>
</ul>
<p dir="auto">Copy/paste and show/hide soft keyboard is implemented by calling Java</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibaryshnikov/android-iced-example/blob/master/pixel_3.png"><img src="https://github.com/ibaryshnikov/android-iced-example/raw/master/pixel_3.png" alt="Pixel third screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and running</h2><a id="user-content-building-and-running" aria-label="Permalink: Building and running" href="#building-and-running"></a></p>
<p dir="auto">Check <code>android-activity</code> crate for detailed instructions.
During my tests I was running the following command and using android studio afterwards:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export ANDROID_NDK_HOME=&quot;path/to/ndk&quot;
export ANDROID_HOME=&quot;path/to/sdk&quot;

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build"><pre><span>export</span> ANDROID_NDK_HOME=<span><span>"</span>path/to/ndk<span>"</span></span>
<span>export</span> ANDROID_HOME=<span><span>"</span>path/to/sdk<span>"</span></span>

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build</pre></div>
<p dir="auto">My setup is the following:</p>
<ul dir="auto">
<li>archlinux 6.9.6</li>
<li>jdk-openjdk 22</li>
<li>target api 35</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">Thanks to <code>android-activity</code> we can already build android apps in Rust, and
key crates such as <code>winit</code> and <code>wgpu</code> also support building for android.
<code>iced</code> doesn't support android out of the box, but it can be integrated with
existing graphics pipelines, as shown in
<a href="https://github.com/iced-rs/iced/tree/0.13.1/examples/integration">integration</a> example.
As a result, it was possible to convert existing example running <code>winit</code> + <code>wgpu</code> to
use <code>iced</code> on top.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disney Imagineering Debuts Next-Generation Robotic Character, Olaf (215 pts)]]></title>
            <link>https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</link>
            <guid>46348847</guid>
            <pubDate>Sun, 21 Dec 2025 21:46:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/">https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</a>, See on <a href="https://news.ycombinator.com/item?id=46348847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <p>Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ <em>Frozen</em>. </p>

<p>This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&nbsp;</p>

<div>
<figure><img decoding="async" width="1620" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1620x1080.jpg" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1620x1080.jpg 1620w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-647x431.jpg 647w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-212x141.jpg 212w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-768x512.jpg 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1536x1024.jpg 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3-1024x683.jpg 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-3.jpg 1920w" sizes="(max-width: 1620px) 100vw, 1620px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>


<h2>Innovation at the Core: From Screen to Reality</h2>


<p>From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&nbsp;</p>



<p>Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&nbsp;&nbsp;</p>

<div>
<figure><img decoding="async" width="1920" height="1079" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6.png" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6.png 1920w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-767x431.png 767w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-250x141.png 250w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-768x432.png 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-1536x863.png 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-848x477.png 848w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-6-1024x575.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>


<h2>The Technology Behind the Magic</h2>


<p>Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&nbsp;</p>

<p>Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&nbsp;&nbsp;</p>

<div>
<figure><img loading="lazy" decoding="async" width="1913" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1913x1080.png" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1913x1080.png 1913w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-763x431.png 763w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-250x141.png 250w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-768x434.png 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1536x867.png 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4-1024x578.png 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-4.png 1920w" sizes="(max-width: 1913px) 100vw, 1913px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>

<p>While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists. </p>

<p>It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&nbsp;&nbsp;</p>

<div>
<figure><img loading="lazy" decoding="async" width="1909" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1909x1080.png" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1909x1080.png 1909w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-762x431.png 762w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-250x141.png 250w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-768x434.png 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1536x869.png 1536w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5-1024x579.png 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-5.png 1920w" sizes="(max-width: 1909px) 100vw, 1909px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>

<p>Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&nbsp;</p>

<p>Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&nbsp;&nbsp;</p>

<p>The BDX Droids, <a href="https://disneyparksblog.com/disney-experiences/herbie-from-the-fantastic-four-first-steps-at-disneyland/" target="_blank" rel="noreferrer noopener">self-balancing H.E.R.B.I.E.</a>, and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world. </p>

<div>
<figure><img loading="lazy" decoding="async" width="720" height="1080" src="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-720x1080.jpg" alt="Robotic Olaf Marks New Era of Disney Innovation" srcset="https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-720x1080.jpg 720w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-287x431.jpg 287w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-94x141.jpg 94w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-768x1152.jpg 768w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-1024x1536.jpg 1024w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-1365x2048.jpg 1365w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-512x768.jpg 512w, https://disneyparksblog.com/app/uploads/2025/11/2025-WDI-Olaf-Robotic-Character-2-scaled.jpg 1707w" sizes="(max-width: 720px) 100vw, 720px"><figcaption><em>Prototype-completed design varies</em>.</figcaption></figure></div>


<h2>Where Guests Can See Olaf</h2>


<p>Olaf will soon venture out into the unknown, eager to see guests at:</p>

<div>
<ul>
<li>Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.</li>



<li>Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.</li>
</ul>
</div>

<p>Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp; Development, came to life at in the latest episode of <a href="https://www.youtube.com/watch?v=EoPN02bmzrE" target="_blank" rel="noreferrer noopener"><em>We Call It Imagineering</em>.</a></p>


            
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A guide to local coding models (469 pts)]]></title>
            <link>https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</link>
            <guid>46348329</guid>
            <pubDate>Sun, 21 Dec 2025 20:55:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude">https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</a>, See on <a href="https://news.ycombinator.com/item?id=46348329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!fARn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!fARn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!fARn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!fARn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:80981,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!fARn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!fARn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!fARn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb194c53f-7b2b-4f74-942b-5cf1c59b32aa_1920x1080.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em>[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.</em></p><p><em><span>[Edit 2] This hypothesis was </span><strong>actually wrong</strong><span> and thank you to everyone who commented! </span></em></p><p><em>Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.</em></p><p><em><span>I’m </span><strong>not </strong><span>editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below. </span></em></p><p><em>There is one takeaway this article provides that definitely holds true:</em></p><ul><li><p><em>Local models are far more capable than they’re given credit for, even for coding.</em></p></li></ul><p><em>It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.</em></p><p><em><strong>But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.</strong></em></p><p><em>This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:</em></p><ul><li><p><em>While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.</em></p></li><li><p><em><span>I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. </span><strong>I would not recommend running local models as a company</strong><span> instead of giving employees access to a tool like Claude Code.</span></em></p></li><li><p><em>While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.</em></p></li></ul><p><em>So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.</em></p><p>Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).</p><p>So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!msVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!msVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 424w, https://substackcdn.com/image/fetch/$s_!msVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 848w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1272w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!msVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png" width="618" height="394.7759197324415" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:1196,&quot;resizeWidth&quot;:618,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!msVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 424w, https://substackcdn.com/image/fetch/$s_!msVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 848w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1272w, https://substackcdn.com/image/fetch/$s_!msVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c9d7143-3d10-4898-923a-7bc517ca615a_1196x764.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that </span><strong><span>my hypothesis was </span><s>correct, with nuance</s></strong><span>,</span><strong> not correct </strong><span>[see edit 2 above] which I’ll get into later in this article.</span></p><p>In this article, we cover:</p><ul><li><p>Why local models matter and the benefits they provide.</p></li><li><p>How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.</p></li><li><p>Walk through setting up your own local coding model and tool step-by-step.</p></li></ul><p>Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.</p><p>If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.</p><ul><li><p><strong>Local coding models are very capable</strong><span>. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. </span><strong>[Edited to add in this next part]</strong><span> Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.</span></p></li><li><p><strong>Tools matter a lot</strong><span>. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.</span></p></li><li><p><strong>There’s a lot to consider when you’re actually working within hardware constraints.</strong><span> We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.</span></p></li><li><p><strong>Google threw a wrench into my hypothesis</strong><span>. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below. </span></p></li></ul><p>If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.</p><p><span>You might wonder why local models are worth investing in at all. The obvious answer is </span><strong>cost</strong><span>. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!OUrN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!OUrN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!OUrN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:140073,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!OUrN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!OUrN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c71b176-e011-4694-915e-217a2e3ce9b5_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>First: </span><strong>Reliability</strong><span>. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.</span></p><p><span>Second: </span><strong>Local models can apply to </strong><em><strong>far </strong></em><strong>more applications</strong><span>. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.</span></p><p>With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.</p><p><span>Finally: </span><strong>Availability. </strong><span>Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).</span></p><p>While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LxTS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LxTS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LxTS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:136854,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LxTS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!LxTS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95d992f8-225f-4b06-8fb9-e9d7544cf2d5_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.</p><p>Local AI has two parts that eat up your memory: The model itself and the model’s context window.</p><p>The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).</p><p>The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.</p><p>When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.</p><p>The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.</p><p>This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!cVCW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!cVCW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!cVCW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109124,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!cVCW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!cVCW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F954df13c-8e66-4c72-a6a1-1182619a5e2b_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works </span><a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list" rel="">here</a><span>.</span></p><p><span>The second trick is quantizing the values you’re working with. </span><a href="https://www.byteplus.com/en/what-is/quantization" rel="">Quantization means converting a continuous set of values into a smaller amount of distinct values</a><span>. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.</span></p><p>You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.</p><p><span>We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it </span><a href="https://arxiv.org/pdf/2510.10964" rel="">causes the model to forget details in long reasoning traces</a><span>. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.</span></p><p>In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.</p><p>Here are a few more factors to understand when setting up a local coding model on your hardware:</p><p>Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).</p><p>You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.</p><p>Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.</p><p>MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.</p><p>Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.</p><p>In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.</p><p>This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.</p><p>There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.</p><p>The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.</p><p><span>There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are </span><a href="https://opencode.ai/" rel="">OpenCode</a><span>, </span><a href="https://aider.chat/" rel="">Aider</a><span>, </span><a href="https://github.com/QwenLM/qwen-code" rel="">Qwen Code</a><span>, </span><a href="https://roocode.com/" rel="">Roo Code</a><span>, and </span><a href="https://www.continue.dev/" rel="">Continue</a><span>. Make sure to use a tool compatible with </span><a href="https://bentoml.com/llm/llm-inference-basics/openai-compatible-api" rel="">OpenAI’s API standard</a><span>. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.</span></p><p><span>I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that </span><strong>tooling matters a lot</strong><span>. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.</span></p><p>If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.</p><p>For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.</p><p>For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.</p><p><span>I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with </span><em>a ton</em><span> of RAM. For serving local coding models, more is always better.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!4IEy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!4IEy!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!4IEy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:133148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!4IEy!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!4IEy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a82350e-d1e0-420d-b4f3-7ea2343d3407_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Xqrl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Xqrl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:171397,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Xqrl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Xqrl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F52524f56-954d-4bd9-8014-0bfb55cc2812_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>I’ve shared my </span><a href="https://github.com/loganthorneloe/modelfiles" rel="">modelfiles repo</a><span> for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.</span></p><ol><li><p><span>Install </span><a href="https://github.com/ml-explore/mlx" rel="">MLX</a><span> or download </span><a href="https://ollama.com/download" rel="">Ollama</a><span> (the rest of this guide will continue with MLX but details for serving on Ollama can be found </span><a href="https://docs.ollama.com/quickstart" rel="">here</a><span>).</span></p></li><li><p>Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.</p></li><li><p>Run pip install -U mlx-lm to install MLX for serving community models.</p></li><li><p>Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).</p></li><li><p><span>Download </span><a href="https://github.com/QwenLM/qwen-code" rel="">Qwen Code</a><span>. You might need to install Node Package Manager for this. I recommend using </span><a href="https://github.com/nvm-sh/nvm" rel="">Node Version Manager</a><span> (nvm) for managing your npm version.</span></p></li><li><p>Set up your tool to access an OpenAI compatible API by entering the following settings:</p><ol><li><p><span>Base URL: </span><a href="http://localhost:8080/v1" rel="">http://localhost:8080/v1</a><span> (should be the default MLX serves your model at)</span></p></li><li><p>API Key: mlx</p></li><li><p>Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).</p></li></ol></li><li><p>Voila! Your coding model tool should be working with your local coding model.</p></li></ol><p><span>I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run </span><strong>very </strong><span>slowly.</span></p><p><strong>One tip I have for using local coding models</strong><span>: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.</span></p><p><span>My original hypothesis was: </span><strong>Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.</strong></p><p><span>I would argue that</span><s>—yes!—</s><strong>no </strong><span>[see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.</span></p><p><span>[This paragraph was added in after initial release of this article] It’s important to note that local models will </span><strong>not</strong><span> reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.</span></p><p><strong>It’s also important to note that local models are only going to get better and smaller</strong><span>. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.</span></p><p>From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!wAV2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!wAV2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!wAV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:162037,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.aiforswes.com/i/182132050?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!wAV2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wAV2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F922da413-1147-4a89-bbd0-fefdd78bc8cb_1920x1080.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.</p><p>Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.</p><p><span>However, this is foiled a bit now that </span><a href="https://blog.google/products/gemini/gemini-3-flash/" rel="">Gemini 3 Flash</a><span> was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.</span></p><p>I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.</p><p>Thanks for reading!</p><p><strong>Always be (machine) learning,</strong></p><p><strong>Logan</strong></p><p data-attrs="{&quot;url&quot;:&quot;https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[More on whether useful quantum computing is “imminent” (102 pts)]]></title>
            <link>https://scottaaronson.blog/?p=9425</link>
            <guid>46348318</guid>
            <pubDate>Sun, 21 Dec 2025 20:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scottaaronson.blog/?p=9425">https://scottaaronson.blog/?p=9425</a>, See on <a href="https://news.ycombinator.com/item?id=46348318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-9425">
				
<p>These days, the most common question I get goes something like this:</p>



<blockquote>
<p>A decade ago, you told people that scalable quantum computing wasn’t imminent. Now, though, you claim it plausibly <em>is</em> imminent. Why have you reversed yourself??</p>
</blockquote>



<p>I appreciated the friend of mine who paraphrased this as follows: “A decade ago you said you were 35. Now you say you’re 45. Explain yourself!”</p>



<hr>



<p>A couple weeks ago, I was delighted to attend <a href="https://q2b.qcware.com/conference/2025-silicon-valley">Q2B</a> in Santa Clara, where I gave a keynote talk entitled <a href="https://www.scottaaronson.com/talks/works.pptx">“Why I Think Quantum Computing Works”</a> (link goes to the PowerPoint slides).  This is one of the most optimistic talks I’ve ever given.  But mostly that’s just because, uncharacteristically for me, here I gave short shrift to the challenge of broadening the class of problems that achieve huge quantum speedups, and just focused on the experimental milestones achieved over the past year.  With every experimental milestone, the little voice in my head that asks “but what if Gil Kalai turned out to be right after all? what if scalable QC <em>wasn’t</em> possible?” grows quieter, until now it can barely be heard.</p>



<p>Going to Q2B was extremely helpful in giving me a sense of the current state of the field.  Ryan Babbush gave a <em>superb</em> overview (I couldn’t have improved a word) of the current status of quantum algorithms, while John Preskill’s annual where-we-stand talk was “magisterial” as usual (that’s the word I’ve long used for his talks), making mine look like just a warmup act for his.  Meanwhile, Quantinuum took a victory lap, boasting of their recent successes in a way that I considered basically justified.</p>



<hr>



<p>After returning from Q2B, I then did an hour-long <a href="https://www.youtube.com/watch?si=T9u5MjX9xwCY9zeJ&amp;v=0_7SH3Eons0&amp;feature=youtu.be">podcast</a> with “The Quantum Bull” on the topic “How Close Are We to Fault-Tolerant Quantum Computing?”  You can watch it here:</p>



<figure><p>
<iframe title="Scott Aaronson on the Possibility of Fault-Tolerant Quantum Computing by 2028" width="500" height="281" src="https://www.youtube.com/embed/0_7SH3Eons0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>As far as I remember, this is the first YouTube interview I’ve ever done that concentrates entirely on the current state of the QC race, skipping any attempt to explain amplitudes, interference, and other basic concepts.  Despite (or conceivably because?) of that, I’m happy with how this interview turned out.  Watch if you want to know my detailed current views on hardware—as always, I recommend 2x speed.</p>



<p>Or for those who don’t have the half hour, a quick summary:</p>



<ul>
<li>In quantum computing, there are the large companies and startups that might succeed or might fail, but are at least trying to solve the real technical problems, and some of them are making amazing progress. And then there are the companies that have optimized for doing IPOs, getting astronomical valuations, and selling a narrative to retail investors and governments about how quantum computing is poised to revolutionize optimization and machine learning and finance. Right now, I see these two sets of companies as <strong>almost entirely disjoint from each other</strong>.<br></li>



<li>The interview also contains my most direct condemnation yet of some of the wild misrepresentations that IonQ, in particular, has made to governments about what QC will be good for (“unlike AI, quantum computers won’t hallucinate because they’re deterministic!”)<br></li>



<li>The two approaches that had the most impressive demonstrations in the past year are trapped ions (especially Quantinuum but also Oxford Ionics) and superconducting qubits (especially Google but also IBM), and perhaps also neutral atoms (especially QuEra but also Infleqtion and Atom Computing).<br></li>



<li>Contrary to a misconception that refuses to die, I <em>haven’t</em> dramatically changed my views on any of these matters. As I have for a quarter century, I continue to profess a lot of confidence in the basic principles of quantum computing theory worked out in the mid-1990s, and I <em>also</em> continue to profess ignorance of exactly how many years it will take to realize those principles in the lab, and of which hardware approach will get there first.<br></li>



<li>But yeah, <em>of course</em> I update in response to developments on the ground, because it would be insane not to! And 2025 was clearly a year that met or exceeded my expectations on hardware, with multiple platforms now boasting &gt;99.9% fidelity two-qubit gates, at or above the theoretical threshold for fault-tolerance. This year updated me in favor of taking more seriously the aggressive pronouncements—the “roadmaps”—of Google, Quantinuum, QuEra, PsiQuantum, and other companies about where they could be in 2028 or 2029.<br></li>



<li>One more time for those in the back: the main <em>known</em> applications of quantum computers remain (1) the simulation of quantum physics and chemistry themselves, (2) breaking a lot of currently deployed cryptography, and (3) eventually, achieving some <em>modest</em> benefits for optimization, machine learning, and other areas (but it will probably be a while before those modest benefits win out in practice).  To be sure, the detailed list of quantum speedups expands over time (as new quantum algorithms get discovered) and also contracts over time (as some of the quantum algorithms get dequantized).  But the list of known applications “from 30,000 feet” remains fairly close to what it was a quarter century ago, after you hack away the dense thickets of obfuscation and hype.</li>
</ul>



<hr>



<p>I’m going to close this post with a warning.  When Frisch and Peierls wrote their <a href="https://en.wikipedia.org/wiki/Frisch%E2%80%93Peierls_memorandum">now-famous memo</a> in March 1940, estimating the mass of Uranium-235 that would be needed for a fission bomb, they didn’t publish it in a journal, but communicated the result through military channels only.  As recently as February 1939, Frisch and Meitner had <a href="https://www.nature.com/articles/143239a0">published in <em>Nature</em></a> their theoretical explanation of recent experiments, showing that the uranium nucleus could fission when bombarded by neutrons.  But by 1940, Frisch and Peierls realized that the time for open publication of these matters had passed.</p>



<p>Similarly, at some point, the people doing detailed estimates of how many physical qubits and gates it’ll take to break actually deployed cryptosystems using Shor’s algorithm are going to stop publishing those estimates, if for no other reason than the risk of giving too much information to adversaries. Indeed, for all we know, that point may have been passed already. This is the clearest warning that I can offer in public right now about the urgency of migrating to post-quantum cryptosystems, a process that I’m grateful is already underway.</p>



<hr>



<p><strong><mark>Update:</mark></strong> Someone on Twitter who’s “long $IONQ” says he’ll be <a href="https://x.com/SPAC_Infleqtion/status/2002826241146302651">posting about and investigating me</a> every day, never resting until UT Austin fires me, in order to punish me for slandering IonQ and other “pure play” SPAC IPO quantum companies. And also, because I’ve been anti-Trump and pro-Biden. He confabulates that I must be trying to profit from my stance (eg by shorting the companies I criticize), it being inconceivable to him that anyone would say anything purely because they care about what’s true.</p>

		
				
				<p>
					<small>
						This entry was posted
												on Sunday, December 21st, 2025 at 11:34 am						and is filed under <a href="https://scottaaronson.blog/?cat=10" rel="category">Adventures in Meatspace</a>, <a href="https://scottaaronson.blog/?cat=4" rel="category">Quantum</a>, <a href="https://scottaaronson.blog/?cat=17" rel="category">Speaking Truth to Parallelism</a>.
						You can follow any responses to this entry through the <a href="https://scottaaronson.blog/?feed=rss2&amp;p=9425">RSS 2.0</a> feed.

													You can <a href="#respond">leave a response</a>, or <a href="https://scottaaronson.blog/wp-trackback.php?p=9425" rel="trackback">trackback</a> from your own site.

						
					</small>
				</p>

			</div><p>You can use rich HTML in comments!  You can also use basic TeX, by enclosing it within <span>$$ $$</span> for displayed equations or <span>\( \)</span> for inline equations.</p><p>
	After two decades of mostly-open comments, in July 2024 <i>Shtetl-Optimized</i> transitioned to the following policy:
	
</p><p>All comments are treated, by default, as personal missives to me, Scott Aaronson---with no expectation either that they'll appear on the blog or that I'll reply to them.

</p><p>At my leisure and discretion, and in consultation with the <a href="https://scottaaronson.blog/?p=6576"><i>Shtetl-Optimized</i> Committee of Guardians</a>, I'll put on the blog a curated selection of comments that I judge to be particularly interesting or to move the topic forward, and I'll do my best to answer those.  But it will be more like Letters to the Editor.  Anyone who feels unjustly censored is welcome to the rest of the Internet.

</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rue: Higher level than Rust, lower level than Go (175 pts)]]></title>
            <link>https://rue-lang.dev/</link>
            <guid>46348262</guid>
            <pubDate>Sun, 21 Dec 2025 20:46:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rue-lang.dev/">https://rue-lang.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46348262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Higher level than Rust, lower level than Go</p></div><div><div><h2>Memory Safe</h2><p>No garbage collector, no manual memory management. A work in progress, though.</p></div><div><h2>Simple Syntax</h2><p>Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.</p></div><div><h2>Fast Compilation</h2><p>Direct compilation to native code.</p></div></div><div><h2>Hello, Rue</h2><div><pre data-lang="rust"><code data-lang="rust"><span><span><span>//</span> It's a classic for a reason
</span></span><span><span><span><span>fn</span> </span><span>fib</span></span><span><span><span>(</span><span>n</span><span>:</span> <span>i32</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>i32</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>if</span> n <span>&lt;=</span> <span>1</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        n
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span> <span>else</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        <span>fib</span><span><span>(</span>n <span>-</span> <span>1</span></span><span><span>)</span></span> <span>+</span> <span>fib</span><span><span>(</span>n <span>-</span> <span>2</span></span><span><span>)</span></span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>i32</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span><span>//</span> Print the first 20 Fibonacci numbers
</span></span></span></span><span><span><span>    <span>let</span> <span>mut</span> i <span>=</span> <span>0</span><span>;</span>
</span></span></span><span><span><span>    <span>while</span> i <span>&lt;</span> <span>20</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        <span>@</span><span>dbg</span><span><span>(</span><span>fib</span><span><span>(</span>i</span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span><span><span><span><span>        i <span>=</span> i <span>+</span> <span>1</span><span>;</span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    <span><span>//</span> Return fib(10) = 55
</span></span></span></span><span><span><span>    <span>fib</span><span><span>(</span><span>10</span></span><span><span>)</span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I can't upgrade to Windows 11, now leave me alone (486 pts)]]></title>
            <link>https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone</link>
            <guid>46347108</guid>
            <pubDate>Sun, 21 Dec 2025 18:43:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone">https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone</a>, See on <a href="https://news.ycombinator.com/item?id=46347108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="articleBody">
			<h2>Microsoft won't let you dismiss the upgrade notification</h2>

<p>So support for Windows 10 has ended. Yes, millions of users are still on it. One of my main laptops runs Windows 10. I can't update to Windows 11 because of the hardware requirements. It's not that I don't have enough RAM, storage, or CPU power. The hardware limitation is specifically TPM 2.0.</p>

<p>What is TPM 2.0, you say? It stands for Trusted Platform Module. It's basically a security chip on the motherboard that enables some security features. It's good and all, but Windows says my laptop doesn't support it. Great! Now leave me alone.</p>

<p>Well, every time I turn on my computer, I get a reminder that I need to update to Windows 11. OK, at this point a Windows machine only belongs to you in name. Microsoft can run arbitrary code on it. They already ran the code to decide that my computer doesn't support Windows 11. So why do they keep bothering me?</p>

<p><a href="https://cdn.idiallo.com/images/assets/daily/92/eol_big.jpg"><img src="https://cdn.idiallo.com/images/assets/daily/92/eol.jpg" alt="Windows 10 end of life announcement"></a>
</p>

<p>Fine, I'm frustrated. That's why I'm complaining. I've accepted the fact that my powerful, yet 10-year-old laptop won't get the latest update. But if Microsoft's own systems have determined my hardware is incompatible, why are they harassing? I'll just have to dismiss this notification and call it a day.</p>

<p>But wait a minute. How do I dismiss it?</p>

<p><img src="https://cdn.idiallo.com/images/assets/daily/92/buttons.jpg" alt="remind me later or learn more">
</p>

<p>I cannot dismiss it. I can only be reminded later or... I have to learn more. If I click "remind me later," I'm basically telling Microsoft that I consent to being shown the same message again whenever they feel like it. If I click "learn more"? I'm taken to the <a href="https://www.microsoft.com/en-us/windows/laptop-buying-guide">Windows Store</a>, where I'm shown ads for different laptops I can buy instead. Apparently, I'm also probably giving them consent to show me this ad the next time I log in.</p>

<p><img src="https://cdn.idiallo.com/images/assets/daily/92/buy.jpg" alt="windows laptop buying guide">
</p>

<p>It's one thing to be at the forefront of enshittification, but Microsoft is now <a href="https://idiallo.com/blog/hostile-not-enshittification">actively hostile to its users</a>. I've written about this <a href="https://idiallo.com/byte-size/say-no-to-onedrive-backup">passive-aggressive illusion of choice</a> before. They are basically asking "Do you want to buy a new laptop?" And the options they are presenting are "Yes" and "OK."</p>

<p>This isn't a bug. This is intentional design. Microsoft has deliberately removed the ability to decline.</p>

<h2>Dear Microsoft</h2>

<p>Listen. You said my device doesn't support Windows 11. You're right. Now leave me alone. I have another device running Windows 11. It's festered with ads, and you're trying everything in your power to get me to create a Microsoft account.</p>

<p>I paid for that computer. I also paid for a pro version of the OS. I don't want OneDrive. I don't want to sign up with my Microsoft account. Whether I use my computer online or offline is none of your business. In fact, if you want me to create an account on your servers, you are first required to register your OS on my own website. The terms and conditions are simple. Every time you perform any network access, you have to send a copy of the payload and response back to my server. Either that, or you're in breach of my terms.</p>

<p><strong>Notes:</strong></p>

<p>By the way, the application showing this notification is called <strong>Reusable UX Interaction Manager</strong> sometimes. Other times it appears as <strong>Campaign Manager</strong>.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mullvad VPN: "This is a Chat Control 3.0 attempt." (597 pts)]]></title>
            <link>https://mastodon.online/@mullvadnet/115742530333573065</link>
            <guid>46347080</guid>
            <pubDate>Sun, 21 Dec 2025 18:39:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.online/@mullvadnet/115742530333573065">https://mastodon.online/@mullvadnet/115742530333573065</a>, See on <a href="https://news.ycombinator.com/item?id=46347080">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[You’re not burnt out, you’re existentially starving (305 pts)]]></title>
            <link>https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/</link>
            <guid>46346958</guid>
            <pubDate>Sun, 21 Dec 2025 18:28:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/">https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/</a>, See on <a href="https://news.ycombinator.com/item?id=46346958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p><strong><em><mark>“Those who have a ‘Why’ to live, can bear with almost any ‘How’.”</mark></em></strong></p>



<p><em>― Viktor Frankl quoting Friedrich Nietzsche, <a href="https://en.wikipedia.org/wiki/Man's_Search_for_Meaning" target="_blank" rel="noreferrer noopener">Man’s Search for Meaning</a></em></p>



<p><strong><mark>Let me guess:</mark></strong></p>



<ol>
<li><strong>Your life is going pretty darn well by any objective metric.</strong>
<ul>
<li>Nice place to live. More than enough stuff. Family and friends who love you.</li>
</ul>
</li>



<li><strong>But you’re tired, burnt out, and more.</strong>
<ul>
<li>It feels like you’re stuck in the ordinary when all you want to do is chase greatness.</li>
</ul>
</li>
</ol>



<p><strong><mark>Viktor Frankl calls this feeling the “existential vacuum”</mark></strong> in his famous book <em>Man’s Search for Meaning</em>. Frankl was a psychologist who survived the Holocaust, and in this book he explains that the inmates who survived with him found and focused on a higher purpose in life, like caring for other inmates and promising to stay alive to reconnect with loved ones outside the camps. But these survivors also struggled in their new lives after the war, desperately searching for meaning when every decision was no longer life or death.</p>



<p><strong>Frankl realized that this existential anxiety is not a nuisance to eliminate</strong>, but actually an important signal pointing us towards our need for meaning. Similarly, while Friedrich Nietzsche would argue that life inherently lacks meaning, he’d also implore us to zoom out and find our highest purpose now:</p>



<p><em>“</em><strong><em>This is the most effective way: to let the youthful soul look back on life with the question, ‘What have you up to now truly loved, what has drawn your soul upward, mastered it and blessed it too?’… <mark>for your true being lies not deeply hidden within you, but an infinite height above you, or at least above that which you commonly take to be yourself.</mark></em></strong><em>“</em></p>



<p><em>— Friedrich Nietzsche, <a href="https://en.wikipedia.org/wiki/Untimely_Meditations" target="_blank" rel="noreferrer noopener">Untimely Meditations</a>, 1874</em></p>



<p><strong><mark>Nihilists get both Nietzsche and YOLO wrong.</mark> Neither mean that you give up. Instead, both mean that your efforts are everything.</strong></p>



<p>So when you get those Sunday Scaries, the existential anxiety that <em>your</em> time is ending and the rest of your life is spent working for someone else, the answer isn’t escapism.</p>



<p>Instead, visualize your ideal self, the truest childhood dream of who you wanted to be when you grew up. What would that person be doing now? Go do that thing!</p>



<p><strong><mark>When facing the existential vacuum, there’s only one way out —&nbsp;up, towards your highest purpose.</mark></strong></p>



<hr>



<p><strong><mark>On a 0-10 scale, how happy did you feel when you started working this Monday?</mark></strong></p>



<p><strong><mark>Why wasn’t your answer a 10?</mark></strong></p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="900" height="600" data-attachment-id="10027" data-permalink="https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/mondays-happiness-scale/" data-orig-file="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?fit=1536%2C1024&amp;ssl=1" data-orig-size="1536,1024" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Mondays Happiness Scale" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?fit=300%2C200&amp;ssl=1" data-large-file="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?fit=900%2C600&amp;ssl=1" src="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=900%2C600&amp;ssl=1" alt="" srcset="https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?w=1536&amp;ssl=1 1536w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=1024%2C683&amp;ssl=1 1024w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=768%2C512&amp;ssl=1 768w, https://i0.wp.com/neilthanedar.com/wp-content/uploads/Mondays-Happiness-Scale.png?resize=1200%2C800&amp;ssl=1 1200w" sizes="(max-width: 900px) 100vw, 900px"></figure>



<p><strong>You got the great job. You built the startup. You took the vacations. But that’s not what you really needed.</strong> You kept coming back Monday after Monday realizing you were doing the same job again.</p>



<p><strong>So you tried to improve yourself.</strong> You optimized your morning routine. You perfected your productivity system. You bought a sleep mask and mouth tape. Yet you’re still dragging yourself out of bed each Monday morning tired and unmotivated.</p>



<p><strong><mark>We’re optimizing for less suffering instead of more meaning.</mark></strong> We’ve confused comfort with fulfillment. And we’re getting really, really good at it. Millennials are the first generation in history to expect our jobs to provide a higher meaning beyond survival. That’s a good thing. It means that the essentials of life are nearly universally available now.</p>



<p><strong>But, as I write in my book <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em>:</strong></p>



<p><em>“</em><strong><em>The last two hundred years of progress pulled most of the world’s population over the poverty line. The next hundred years is about lifting everyone above the abundance line… <mark>Positive Politics seeks to democratize this abundance.</mark></em></strong><em>“</em></p>



<p><strong>Those of us who have already achieved abundance in our own lives now have two responsibilities:</strong></p>



<ol>
<li>Spread that abundance to as many other people as possible.</li>



<li>Find something more meaningful to do than chase more stuff.</li>
</ol>



<hr>



<p><em><mark>“</mark></em><strong><mark><em>The existential vacuum is a widespread phenomenon of the twentieth century</em></mark></strong><em><mark>“</mark></em></p>



<p><em>― Viktor Frankl, Man’s Search for Meaning</em></p>



<p><strong>When I was a kid, I knew exactly what I wanted to do&nbsp;— <a href="https://neilthanedar.com/the-most-important-job-in-the-world/" target="_blank" rel="noreferrer noopener">the most important job in the world</a>.</strong> And I wasn’t afraid to tell you either. At five years old, I would talk your ear off about training to be goalie for the St. Louis Blues. By seven, it was astronaut for NASA. By eleven, it was President of the United States. Then middle school hit, I got made fun of more than a few times, and that voice went silent.</p>



<p>After three startups, three nonprofits, and especially three kids knocked the imposter syndrome out of me, I spent a lot of time training my inner voice to get loud again. And what I heard reinforced what I knew all along —&nbsp;that my highest purpose is way above where I commonly take myself now.</p>



<p><strong>Imposter syndrome can be a good thing.</strong> That external voice saying “this is not you” may actually be telling you the truth. I got into the testing lab industry to save our family business. Fifteen years and three startups later, I had become “the lab expert” to the world. But I cringed at that label. First, there was no room to grow. I had already done it. I didn’t want to be eighty and still running labs. Second, and most importantly, I knew that my skills could be used for much more than money.</p>



<p>I’d love to say I transformed overnight, but really it took 5+ years from 2020 to 2025 for me to fully embody my new identity. You can see it in my writing, which became much more ambitious in 2020, when I relaunched this site and started <a href="https://neilthanedar.com/ideas/" target="_blank" rel="noreferrer noopener">blogging</a> consistently. That led to my <em><a href="https://worldsbiggestproblems.com/" target="_blank" rel="noreferrer noopener">World’s Biggest Problems</a></em> project, which convinced me that <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em> is the #1 solution we need now!</p>



<p><strong><mark>There are two key components to my highest mission now:</mark></strong></p>



<ol>
<li>Help people find their highest purpose.</li>



<li>Be a model for <a href="https://share.evernote.com/note/ab3c0eec-663b-716e-6fa9-35efc848b3ee">the pursuit of greatness</a>.</li>
</ol>



<p><strong>That means consistently chasing my highest purpose —&nbsp;helping ambitious optimists get into politics!</strong> After nearly a decade of doing this behind the scenes as a political volunteer and advisor, 2025 was the first year where I went full-time in politics. Leading <a href="https://mcfn.org/" target="_blank" rel="noreferrer noopener">MCFN</a> and publishing <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em> at the same time was a ton of work. But <strong>nothing energizes me more than fighting two of the biggest battles in the world now&nbsp;—&nbsp;anticorruption and Positive Politics!</strong></p>



<p><strong>I love politics because it’s full of meta solutions —&nbsp;solutions that create more solutions.</strong> My Positive Politics Accelerator is a classic example —&nbsp;recruiting and training more ambitious optimists into politics will lead to them making positive political change at all levels of government. But I’ve also tackled challenges like independent testing with startups and led a nonprofit to drive investigative journalism.</p>



<p>There are so many paths to positive impact, including politics, startups, nonprofits, medicine, law, education, science, engineering, journalism, art, faith, parenting, mentorship, and more! Choose the path that both best fits you now and is pointed towards your long-term highest purpose.</p>



<hr>



<p><strong><mark>I woke up today so excited to get to work thinking it was Monday morning already.</mark></strong> Instead of jumping right into it, I spent all morning making breakfast and playing with my kids, then wrote this post. When I’m writing about something personal, 1,000+ words can easily flow for me in an afternoon. This part will be done just in time to go to a nerf battle birthday party with my boys and their friends.</p>



<p><strong>Both the hustle and anti-hustle cultures get it wrong.</strong> Working long hours isn’t inherently good or bad. If I really had to count how much I’m “on” vs. doing whatever I want, it’s easy 100+ hours per week. But that includes everything from investigative journalism and operations work for MCFN, social media and speaking events for <em>Positive Politics</em>, reading and writing for my site, and 40+ hours every week with my kids.</p>



<p><strong><mark>I want to help more ambitious optimists chase your highest potential!</mark></strong> Whether the best solution is in startups, politics, nonprofits, science, crypto, or some new technology that’s yet to be invented, I’m happy to point you where I think you’ll be most powerful. I’ve thought, written, and worked on many of these ideas in my 15+ year career.</p>



<p>Now with 10+ years of writing for <a href="https://neilthanedar.com/ideas" target="_blank" rel="noreferrer noopener">my site</a> and my <em><a href="https://positivepoliticsbook.com/" target="_blank" rel="noreferrer noopener">Positive Politics</a></em> and <em><a href="https://worldsbiggestproblems.com/" target="_blank" rel="noreferrer noopener">World’s Biggest Problems</a></em> projects, I’ve focused on publicly inspiring more people to take on these challenges too. We should be flexible on how we solve the problems but firm in our resolve to consistently organize people and launch solutions.</p>



<p>As Steve Jobs said, “Life can be much broader once you discover one simple fact, and that is <strong><mark>everything around you that you call ‘life’ was made up by people that were no smarter than you… You can change it, you can mold it</mark></strong>… the most important thing…is to shake off this erroneous notion that life is there and you’re just going to live in it, versus embrace it, change it, improve it, make your mark upon it… <strong>Once you learn that, you’ll never be the same again.</strong>”</p>



<p><strong>Remember how it felt as a young child to openly tell the world about your dream job?</strong> Find the work that makes you feel this way and jump on whatever rung of that career ladder you can start now. The pay may be a little lower, but the existential payoff will be exponentially higher for the rest of your life.</p>



<p><strong><mark>You don’t have to go all-in right away!</mark></strong> In fact, after a long diet of low existential work, it’s probably best to ease into public work. You can even volunteer one hour or less per week for a political campaign or nonprofit to get started. Pick the smallest first step, and do it. Not in January, now. Do it before the end of the year. And see how different you feel when 2026 starts!</p>



<p><strong>And you don’t have to choose politics like me!</strong> Do you have the next great ambitious optimistic science fiction novel in your head? That book could spark movies and movements that positively change millions of lives! Choose the path will inspire and energize you for decades!</p>



<p><strong><mark>What matters most is you go straight towards your highest potential right now.</mark></strong> Pause once a month to make sure you’re still on the right track. Stop once a year to triple-check you’re on the right track. But never get off this path towards your highest potential. Anything else will starve you existentially.</p>



<p><strong><mark>When you truly chase your highest potential, everything you thought was burnout will melt away.</mark></strong> Because you weren’t suffering from too much work, you were suffering from too little truly important work. Like a boy who thought he was full until dessert arrives, you’ll suddenly find your hunger return!</p>



<hr>



<p><strong><em><mark>If you’re sick of politics as usual and ready to change the system, join Positive Politics!</mark></em></strong></p>



<ul>
<li><strong><em>Buy the book: <a href="https://positivepoliticsbook.com/"><span>positivepoliticsbook.com</span></a></em></strong></li>



<li><strong><em>Join the accelerator: <a href="https://positivepolitics.org/apply"><span>positivepolitics.org/apply</span></a></em></strong></li>
</ul>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Logging Sucks (537 pts)]]></title>
            <link>https://loggingsucks.com/</link>
            <guid>46346796</guid>
            <pubDate>Sun, 21 Dec 2025 18:09:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://loggingsucks.com/">https://loggingsucks.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46346796">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
        <div>
          
          <p>And here's how to make it better.</p>
          
        </div>
        <div>
              <p><span>2024-12-20T03:14:22.847Z</span> <span>INFO</span> <span>HttpServer started successfully binding=0.0.0.0:3000 pid=28471 env=production version=2.4.1 node_env=production cluster_mode=enabled workers=4</span></p>
              <p><span>2024-12-20T03:14:22.912Z</span> <span>debug</span> <span>PostgreSQL connection pool initialized host=db.internal:5432 database=main pool_size=20 ssl_mode=require idle_timeout=10000ms max_lifetime=1800000ms</span></p>
              <p><span>2024-12-20T03:14:23.156Z</span> <span>INFO</span> <span>Incoming request method=GET path=/api/v1/users/me ip=192.168.1.42 user_agent="Mozilla/5.0" request_id=req_8f7a2b3c trace_id=abc123def456</span></p>
              <p><span>2024-12-20T03:14:23.201Z</span> <span>debug</span> <span>JWT token validation started issuer=auth.company.com audience=api.company.com exp=1703044800 iat=1703041200 sub=user_abc123 scope="read write"</span></p>
              <p><span>2024-12-20T03:14:23.445Z</span> <span>WARN</span> <span>Slow database query detected duration_ms=847 query="SELECT u.*, o.name FROM users u JOIN orgs o ON u.org_id = o.id WHERE u.org_id = $1 AND u.deleted_at IS NULL" rows_returned=2847</span></p>
              <p><span>2024-12-20T03:14:23.892Z</span> <span>debug</span> <span>Redis cache lookup failed key=users:org_12345:list:v2 ttl_seconds=3600 fallback_strategy=database cache_cluster=redis-prod-01 latency_ms=2</span></p>
              <p><span>2024-12-20T03:14:24.156Z</span> <span>info</span> <span>Request completed successfully status=200 duration_ms=1247 bytes_sent=48291 request_id=req_8f7a2b3c cache_hit=false db_queries=3 external_calls=1</span></p>
              <p><span>2024-12-20T03:14:24.312Z</span> <span>ERROR</span> <span>Database connection pool exhausted active_connections=20 waiting_requests=147 timeout_ms=30000 service=postgres suggestion="Consider increasing pool_size or optimizing queries"</span></p>
              <p><span>2024-12-20T03:14:24.445Z</span> <span>warn</span> <span>Retrying failed HTTP request attempt=1 max_attempts=3 backoff_ms=100 error_code=ETIMEDOUT target_service=payment-gateway endpoint=/v1/charges circuit_state=closed</span></p>
              <p><span>2024-12-20T03:14:25.112Z</span> <span>INFO</span> <span>Circuit breaker state transition service=payment-api previous_state=closed current_state=open failure_count=5 failure_threshold=5 reset_timeout_ms=30000</span></p>
              <p><span>2024-12-20T03:14:25.445Z</span> <span>debug</span> <span>Background job executed successfully job_id=job_9x8w7v6u type=weekly_email_digest duration_ms=2341 emails_sent=1847 failures=3 queue=default priority=low</span></p>
              <p><span>2024-12-20T03:14:26.201Z</span> <span>ERROR</span> <span>Memory pressure critical heap_used_bytes=1932735283 heap_limit_bytes=2147483648 gc_pause_ms=847 gc_type=major rss_bytes=2415919104 external_bytes=8847291</span></p>
              <p><span>2024-12-20T03:14:26.556Z</span> <span>WARN</span> <span>Rate limit threshold approaching user_id=user_abc123 current_requests=890 limit=1000 window_seconds=60 remaining=110 reset_at=2024-12-20T03:15:00Z</span></p>
              <p><span>2024-12-20T03:14:27.112Z</span> <span>info</span> <span>WebSocket connection established client_id=ws_7f8g9h2j protocol=wss rooms=["team_updates","notifications","presence"] user_id=user_abc123 ip=192.168.1.42</span></p>
              <p><span>2024-12-20T03:14:27.445Z</span> <span>debug</span> <span>Kafka message consumed successfully topic=user-events partition=3 offset=1847291 key=user_abc123 consumer_group=api-consumers lag=12 processing_time_ms=45</span></p>
              <p><span>2024-12-20T03:14:28.112Z</span> <span>INFO</span> <span>Health check passed service=api-gateway uptime_seconds=847291 active_connections=142 memory_usage_percent=73 cpu_usage_percent=45 status=healthy version=2.4.1</span></p>
              <p><span>2024-12-20T03:14:28.556Z</span> <span>debug</span> <span>S3 upload completed bucket=company-uploads key=avatars/user_abc123/profile.jpg size_bytes=245891 content_type=image/jpeg duration_ms=892 region=us-east-1</span></p>
              <p><span>2024-12-20T03:14:29.112Z</span> <span>warn</span> <span>Deprecated API version detected endpoint=/api/v1/legacy/users version=v1 recommended_version=v3 deprecation_date=2025-01-15 client_id=mobile-app-ios</span></p>
              <!-- Duplicate for seamless loop -->
              <p><span>2024-12-20T03:14:22.847Z</span> <span>INFO</span> <span>HttpServer started successfully binding=0.0.0.0:3000 pid=28471 env=production version=2.4.1 node_env=production cluster_mode=enabled workers=4</span></p>
              <p><span>2024-12-20T03:14:22.912Z</span> <span>debug</span> <span>PostgreSQL connection pool initialized host=db.internal:5432 database=main pool_size=20 ssl_mode=require idle_timeout=10000ms max_lifetime=1800000ms</span></p>
              <p><span>2024-12-20T03:14:23.156Z</span> <span>INFO</span> <span>Incoming request method=GET path=/api/v1/users/me ip=192.168.1.42 user_agent="Mozilla/5.0" request_id=req_8f7a2b3c trace_id=abc123def456</span></p>
              <p><span>2024-12-20T03:14:23.201Z</span> <span>debug</span> <span>JWT token validation started issuer=auth.company.com audience=api.company.com exp=1703044800 iat=1703041200 sub=user_abc123 scope="read write"</span></p>
              <p><span>2024-12-20T03:14:23.445Z</span> <span>WARN</span> <span>Slow database query detected duration_ms=847 query="SELECT u.*, o.name FROM users u JOIN orgs o ON u.org_id = o.id WHERE u.org_id = $1 AND u.deleted_at IS NULL" rows_returned=2847</span></p>
              <p><span>2024-12-20T03:14:23.892Z</span> <span>debug</span> <span>Redis cache lookup failed key=users:org_12345:list:v2 ttl_seconds=3600 fallback_strategy=database cache_cluster=redis-prod-01 latency_ms=2</span></p>
              <p><span>2024-12-20T03:14:24.156Z</span> <span>info</span> <span>Request completed successfully status=200 duration_ms=1247 bytes_sent=48291 request_id=req_8f7a2b3c cache_hit=false db_queries=3 external_calls=1</span></p>
              <p><span>2024-12-20T03:14:24.312Z</span> <span>ERROR</span> <span>Database connection pool exhausted active_connections=20 waiting_requests=147 timeout_ms=30000 service=postgres suggestion="Consider increasing pool_size or optimizing queries"</span></p>
              <p><span>2024-12-20T03:14:24.445Z</span> <span>warn</span> <span>Retrying failed HTTP request attempt=1 max_attempts=3 backoff_ms=100 error_code=ETIMEDOUT target_service=payment-gateway endpoint=/v1/charges circuit_state=closed</span></p>
              <p><span>2024-12-20T03:14:25.112Z</span> <span>INFO</span> <span>Circuit breaker state transition service=payment-api previous_state=closed current_state=open failure_count=5 failure_threshold=5 reset_timeout_ms=30000</span></p>
              <p><span>2024-12-20T03:14:25.445Z</span> <span>debug</span> <span>Background job executed successfully job_id=job_9x8w7v6u type=weekly_email_digest duration_ms=2341 emails_sent=1847 failures=3 queue=default priority=low</span></p>
              <p><span>2024-12-20T03:14:26.201Z</span> <span>ERROR</span> <span>Memory pressure critical heap_used_bytes=1932735283 heap_limit_bytes=2147483648 gc_pause_ms=847 gc_type=major rss_bytes=2415919104 external_bytes=8847291</span></p>
              <p><span>2024-12-20T03:14:26.556Z</span> <span>WARN</span> <span>Rate limit threshold approaching user_id=user_abc123 current_requests=890 limit=1000 window_seconds=60 remaining=110 reset_at=2024-12-20T03:15:00Z</span></p>
              <p><span>2024-12-20T03:14:27.112Z</span> <span>info</span> <span>WebSocket connection established client_id=ws_7f8g9h2j protocol=wss rooms=["team_updates","notifications","presence"] user_id=user_abc123 ip=192.168.1.42</span></p>
              <p><span>2024-12-20T03:14:27.445Z</span> <span>debug</span> <span>Kafka message consumed successfully topic=user-events partition=3 offset=1847291 key=user_abc123 consumer_group=api-consumers lag=12 processing_time_ms=45</span></p>
              <p><span>2024-12-20T03:14:28.112Z</span> <span>INFO</span> <span>Health check passed service=api-gateway uptime_seconds=847291 active_connections=142 memory_usage_percent=73 cpu_usage_percent=45 status=healthy version=2.4.1</span></p>
              <p><span>2024-12-20T03:14:28.556Z</span> <span>debug</span> <span>S3 upload completed bucket=company-uploads key=avatars/user_abc123/profile.jpg size_bytes=245891 content_type=image/jpeg duration_ms=892 region=us-east-1</span></p>
              <p><span>2024-12-20T03:14:29.112Z</span> <span>warn</span> <span>Deprecated API version detected endpoint=/api/v1/legacy/users version=v1 recommended_version=v3 deprecation_date=2025-01-15 client_id=mobile-app-ios</span></p>
            </div>
      </div>

    <div>
        <p>Your logs are lying to you. Not maliciously. They're just not equipped to tell the truth.</p>
<p>You've probably spent hours grep-ing through logs trying to understand why a user couldn't check out, why that webhook failed, or why your p99 latency spiked at 3am. You found nothing useful. Just timestamps and vague messages that mock you with their uselessness.</p>
<p>This isn't your fault. <strong>Logging, as it's commonly practiced, is fundamentally broken.</strong> And no, slapping OpenTelemetry on your codebase won't magically fix it.</p>
<p>Let me show you what's wrong, and more importantly, how to fix it.</p>

<h2>The Core Problem</h2>
<p>Logs were designed for a different era. An era of monoliths, single servers, and problems you could reproduce locally. Today, a single user request might touch 15 services, 3 databases, 2 caches, and a message queue. Your logs are still acting like it's 2005.</p>
<p>Here's what a typical logging setup looks like:</p>
<div id="log-chaos-simulator">
    <p>The Log Chaos Simulator</p>
    <p>Loading interactive demo...</p>
  </div>
<p>That's 13 log lines for a single successful request. Now multiply that by 10,000 concurrent users. You've got 130,000 log lines per second. Most of them saying absolutely nothing useful.</p>
<p>But here's the real problem: when something goes wrong, these logs won't help you. They're missing the one thing you need: <strong>context</strong>.</p>

<h2>Why String Search is Broken</h2>
<p>When a user reports "I can't complete my purchase," your first instinct is to search your logs. You type their email, or maybe their user ID, and hit enter.</p>
<div id="futile-search">
    <p>The Futile Search</p>
    <p>Loading interactive demo...</p>
  </div>
<p>String search treats logs as bags of characters. It has no understanding of structure, no concept of relationships, no way to correlate events across services.</p>
<ul>When you search for "user-123", you might find it logged 47 different ways across your codebase:
<li><code>user-123</code></li>
<li><code>user_id=user-123</code></li>
<li><code>{"userId": "user-123"}</code></li>
<li><code>[USER:user-123]</code></li>
<li><code>processing user: user-123</code></li></ul>
<p>And those are just the logs that <em>include</em> the user ID. What about the downstream service that only logged the order ID? Now you need a second search. And a third. You're playing detective with one hand tied behind your back.</p>
<blockquote>The fundamental problem: logs are optimized for <em>writing</em>, not for <em>querying</em>.</blockquote>
<p>Developers write <code>console.log("Payment failed")</code> because it's easy in the moment. Nobody thinks about the poor soul who'll be searching for this at 2am during an outage.</p>

<h2>Let's Define Some Terms</h2>
<p>Before I show you the fix, let me define some terms. These get thrown around a lot, often incorrectly.</p>
<p><strong>Structured Logging</strong>: Logs emitted as key-value pairs (usually JSON) instead of plain strings. <code>{"event": "payment_failed", "user_id": "123"}</code> instead of <code>"Payment failed for user 123"</code>. Structured logging is necessary but not sufficient.</p>
<p><strong>Cardinality</strong>: The number of unique values a field can have. <code>user_id</code> has high cardinality (millions of unique values). <code>http_method</code> has low cardinality (GET, POST, PUT, DELETE, etc.). High cardinality fields are what make logs actually useful for debugging.</p>
<p><strong>Dimensionality</strong>: The number of fields in your log event. A log with 5 fields has low dimensionality. A log with 50 fields has high dimensionality. More dimensions = more questions you can answer.</p>
<p><strong>Wide Event</strong>: A single, context-rich log event emitted per request per service. Instead of 13 log lines for one request, you emit 1 line with 50+ fields containing everything you might need to debug.</p>
<p><strong>Canonical Log Line</strong>: Another term for wide event, popularized by Stripe. One log line per request that serves as the authoritative record of what happened.</p>
<div id="cardinality-explorer">
    <p>Cardinality Explorer</p>
    <p>Loading interactive demo...</p>
  </div>

<h2>OpenTelemetry Won't Save You</h2>
<p>I see this take constantly: "Just use OpenTelemetry and your observability problems are solved."</p>
<p>No. OpenTelemetry is a <strong>protocol and a set of SDKs</strong>. It standardizes how telemetry data (logs, traces, metrics) is collected and exported. This is genuinely useful: it means you're not locked into a specific vendor's format.</p>
<p>But here's what OpenTelemetry does NOT do:</p>
<p>1. <strong>It doesn't decide what to log.</strong> You still have to instrument your code deliberately.<br>2. <strong>It doesn't add business context.</strong> If you don't add the user's subscription tier, their cart value, or the feature flags enabled, OTel won't magically know.<br>3. <strong>It doesn't fix your mental model.</strong> If you're still thinking in terms of "log statements," you'll just emit bad telemetry in a standardized format.</p>
<div id="otel-reality-check">
    <p>The OTel Reality Check</p>
    <p>Loading interactive demo...</p>
  </div>
<p>OpenTelemetry is a delivery mechanism. It doesn't know that <code>user-789</code> is a premium customer who's been with you for 3 years and just tried to spend $160. <strong>You</strong> have to tell it.</p>

<h2>The Fix: Wide Events / Canonical Log Lines</h2>
<p>Here's the mental model shift that changes everything:</p>
<blockquote>Instead of logging <em>what your code is doing</em>, log <em>what happened to this request</em>.</blockquote>
<p>Stop thinking about logs as a debugging diary. Start thinking about them as a structured record of business events.</p>
<p>For each request, emit <strong>one wide event</strong> per service hop. This event should contain every piece of context that might be useful for debugging. Not just what went wrong, but the full picture of the request.</p>
<div id="wide-event-builder">
    <p>Build a Wide Event</p>
    <p>Loading interactive demo...</p>
  </div>
<p>Here's what a wide event looks like in practice:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "timestamp"</span><span>: </span><span>"2025-01-15T10:23:45.612Z"</span><span>,</span></span>
<span><span>  "request_id"</span><span>: </span><span>"req_8bf7ec2d"</span><span>,</span></span>
<span><span>  "trace_id"</span><span>: </span><span>"abc123"</span><span>,</span></span>
<span></span>
<span><span>  "service"</span><span>: </span><span>"checkout-service"</span><span>,</span></span>
<span><span>  "version"</span><span>: </span><span>"2.4.1"</span><span>,</span></span>
<span><span>  "deployment_id"</span><span>: </span><span>"deploy_789"</span><span>,</span></span>
<span><span>  "region"</span><span>: </span><span>"us-east-1"</span><span>,</span></span>
<span></span>
<span><span>  "method"</span><span>: </span><span>"POST"</span><span>,</span></span>
<span><span>  "path"</span><span>: </span><span>"/api/checkout"</span><span>,</span></span>
<span><span>  "status_code"</span><span>: </span><span>500</span><span>,</span></span>
<span><span>  "duration_ms"</span><span>: </span><span>1247</span><span>,</span></span>
<span></span>
<span><span>  "user"</span><span>: {</span></span>
<span><span>    "id"</span><span>: </span><span>"user_456"</span><span>,</span></span>
<span><span>    "subscription"</span><span>: </span><span>"premium"</span><span>,</span></span>
<span><span>    "account_age_days"</span><span>: </span><span>847</span><span>,</span></span>
<span><span>    "lifetime_value_cents"</span><span>: </span><span>284700</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "cart"</span><span>: {</span></span>
<span><span>    "id"</span><span>: </span><span>"cart_xyz"</span><span>,</span></span>
<span><span>    "item_count"</span><span>: </span><span>3</span><span>,</span></span>
<span><span>    "total_cents"</span><span>: </span><span>15999</span><span>,</span></span>
<span><span>    "coupon_applied"</span><span>: </span><span>"SAVE20"</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "payment"</span><span>: {</span></span>
<span><span>    "method"</span><span>: </span><span>"card"</span><span>,</span></span>
<span><span>    "provider"</span><span>: </span><span>"stripe"</span><span>,</span></span>
<span><span>    "latency_ms"</span><span>: </span><span>1089</span><span>,</span></span>
<span><span>    "attempt"</span><span>: </span><span>3</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "error"</span><span>: {</span></span>
<span><span>    "type"</span><span>: </span><span>"PaymentError"</span><span>,</span></span>
<span><span>    "code"</span><span>: </span><span>"card_declined"</span><span>,</span></span>
<span><span>    "message"</span><span>: </span><span>"Card declined by issuer"</span><span>,</span></span>
<span><span>    "retriable"</span><span>: </span><span>false</span><span>,</span></span>
<span><span>    "stripe_decline_code"</span><span>: </span><span>"insufficient_funds"</span></span>
<span><span>  },</span></span>
<span></span>
<span><span>  "feature_flags"</span><span>: {</span></span>
<span><span>    "new_checkout_flow"</span><span>: </span><span>true</span><span>,</span></span>
<span><span>    "express_payment"</span><span>: </span><span>false</span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></pre>
<ul>One event. Everything you need. When this user complains, you search for <code>user_id = "user_456"</code> and you instantly know:
<li>They're a premium customer (high priority)</li>
<li>They've been with you for over 2 years (very high priority)</li>
<li>The payment failed on the 3rd attempt</li>
<li>The actual reason: insufficient funds</li>
<li>They were using the new checkout flow (potential correlation?)</li></ul>
<p>No grep-ing. No guessing. No second search.</p>

<h2>The Queries You Can Now Run</h2>
<p>With wide events, you're not searching text anymore. You're querying structured data. The difference is night and day.</p>
<div id="query-playground">
    <p>Query Playground</p>
    <p>Loading interactive demo...</p>
  </div>
<p>This is the superpower of wide events combined with high-cardinality, high-dimensionality data. You're not searching logs anymore. You're running analytics on your production traffic.</p>

<h2>Implementing Wide Events</h2>
<p>Here's a practical implementation pattern. The key insight: build the event throughout the request lifecycle, then emit once at the end.</p>
<pre tabindex="0"><code><span><span>// middleware/wideEvent.ts</span></span>
<span><span>export</span><span> function</span><span> wideEventMiddleware</span><span>() {</span></span>
<span><span>  return</span><span> async</span><span> (</span><span>ctx</span><span>, </span><span>next</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>    const</span><span> startTime</span><span> =</span><span> Date.</span><span>now</span><span>();</span></span>
<span></span>
<span><span>    // Initialize the wide event with request context</span></span>
<span><span>    const</span><span> event</span><span>:</span><span> Record</span><span>&lt;</span><span>string</span><span>, </span><span>unknown</span><span>&gt; </span><span>=</span><span> {</span></span>
<span><span>      request_id: ctx.</span><span>get</span><span>(</span><span>'requestId'</span><span>),</span></span>
<span><span>      timestamp: </span><span>new</span><span> Date</span><span>().</span><span>toISOString</span><span>(),</span></span>
<span><span>      method: ctx.req.method,</span></span>
<span><span>      path: ctx.req.path,</span></span>
<span><span>      service: process.env.</span><span>SERVICE_NAME</span><span>,</span></span>
<span><span>      version: process.env.</span><span>SERVICE_VERSION</span><span>,</span></span>
<span><span>      deployment_id: process.env.</span><span>DEPLOYMENT_ID</span><span>,</span></span>
<span><span>      region: process.env.</span><span>REGION</span><span>,</span></span>
<span><span>    };</span></span>
<span></span>
<span><span>    // Make the event accessible to handlers</span></span>
<span><span>    ctx.</span><span>set</span><span>(</span><span>'wideEvent'</span><span>, event);</span></span>
<span></span>
<span><span>    try</span><span> {</span></span>
<span><span>      await</span><span> next</span><span>();</span></span>
<span><span>      event.status_code </span><span>=</span><span> ctx.res.status;</span></span>
<span><span>      event.outcome </span><span>=</span><span> 'success'</span><span>;</span></span>
<span><span>    } </span><span>catch</span><span> (error) {</span></span>
<span><span>      event.status_code </span><span>=</span><span> 500</span><span>;</span></span>
<span><span>      event.outcome </span><span>=</span><span> 'error'</span><span>;</span></span>
<span><span>      event.error </span><span>=</span><span> {</span></span>
<span><span>        type: error.name,</span></span>
<span><span>        message: error.message,</span></span>
<span><span>        code: error.code,</span></span>
<span><span>        retriable: error.retriable </span><span>??</span><span> false</span><span>,</span></span>
<span><span>      };</span></span>
<span><span>      throw</span><span> error;</span></span>
<span><span>    } </span><span>finally</span><span> {</span></span>
<span><span>      event.duration_ms </span><span>=</span><span> Date.</span><span>now</span><span>() </span><span>-</span><span> startTime;</span></span>
<span></span>
<span><span>      // Emit the wide event</span></span>
<span><span>      logger.</span><span>info</span><span>(event);</span></span>
<span><span>    }</span></span>
<span><span>  };</span></span>
<span><span>}</span></span></code></pre>
<p>Then in your handlers, you enrich the event with business context:</p>
<pre tabindex="0"><code><span><span>app.</span><span>post</span><span>(</span><span>'/checkout'</span><span>, </span><span>async</span><span> (</span><span>ctx</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>  const</span><span> event</span><span> =</span><span> ctx.</span><span>get</span><span>(</span><span>'wideEvent'</span><span>);</span></span>
<span><span>  const</span><span> user</span><span> =</span><span> ctx.</span><span>get</span><span>(</span><span>'user'</span><span>);</span></span>
<span></span>
<span><span>  // Add user context</span></span>
<span><span>  event.user </span><span>=</span><span> {</span></span>
<span><span>    id: user.id,</span></span>
<span><span>    subscription: user.subscription,</span></span>
<span><span>    account_age_days: </span><span>daysSince</span><span>(user.createdAt),</span></span>
<span><span>    lifetime_value_cents: user.ltv,</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  // Add business context as you process</span></span>
<span><span>  const</span><span> cart</span><span> =</span><span> await</span><span> getCart</span><span>(user.id);</span></span>
<span><span>  event.cart </span><span>=</span><span> {</span></span>
<span><span>    id: cart.id,</span></span>
<span><span>    item_count: cart.items.</span><span>length</span><span>,</span></span>
<span><span>    total_cents: cart.total,</span></span>
<span><span>    coupon_applied: cart.coupon?.code,</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  // Process payment</span></span>
<span><span>  const</span><span> paymentStart</span><span> =</span><span> Date.</span><span>now</span><span>();</span></span>
<span><span>  const</span><span> payment</span><span> =</span><span> await</span><span> processPayment</span><span>(cart, user);</span></span>
<span></span>
<span><span>  event.payment </span><span>=</span><span> {</span></span>
<span><span>    method: payment.method,</span></span>
<span><span>    provider: payment.provider,</span></span>
<span><span>    latency_ms: Date.</span><span>now</span><span>() </span><span>-</span><span> paymentStart,</span></span>
<span><span>    attempt: payment.attemptNumber,</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  // If payment fails, add error details</span></span>
<span><span>  if</span><span> (payment.error) {</span></span>
<span><span>    event.error </span><span>=</span><span> {</span></span>
<span><span>      type: </span><span>'PaymentError'</span><span>,</span></span>
<span><span>      code: payment.error.code,</span></span>
<span><span>      stripe_decline_code: payment.error.declineCode,</span></span>
<span><span>    };</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  return</span><span> ctx.</span><span>json</span><span>({ orderId: payment.orderId });</span></span>
<span><span>});</span></span></code></pre>
<div id="event-lifecycle">
    <p>Wide Event Builder Simulator</p>
    <p>Loading interactive demo...</p>
  </div>

<h2>Sampling: Keeping Costs Under Control</h2>
<p>"But Boris," I hear you saying, "if I log 50 fields per request at 10,000 requests per second, my observability bill will bankrupt me."</p>
<p>Valid concern. This is where <strong>sampling</strong> comes in.</p>
<p><strong>Sampling</strong> means keeping only a percentage of your events. Instead of storing 100% of traffic, you might store 10% or 1%. At scale, this is the only way to stay sane (and solvent).</p>
<p>But naive sampling is dangerous. If you randomly sample 1% of traffic, you might accidentally drop the one request that explains your outage.</p>
<div id="sampling-trap">
    <p>The Sampling Trap</p>
    <p>Loading interactive demo...</p>
  </div>
<h3>Tail Sampling</h3>
<p><strong>Tail sampling</strong> means you make the sampling decision <em>after</em> the request completes, based on its outcome.</p>
<p>The rules are simple:<br>1. <strong>Always keep errors.</strong> 100% of 500s, exceptions, and failures get stored.<br>2. <strong>Always keep slow requests.</strong> Anything above your p99 latency threshold.<br>3. <strong>Always keep specific users.</strong> VIP customers, internal testing accounts, flagged sessions.<br>4. <strong>Randomly sample the rest.</strong> Happy, fast requests? Keep 1-5%.</p>
<p>This gives you the best of both worlds: manageable costs, but you never lose the events that matter.</p>
<pre tabindex="0"><code><span><span>// Tail sampling decision function</span></span>
<span><span>function</span><span> shouldSample</span><span>(</span><span>event</span><span>:</span><span> WideEvent</span><span>)</span><span>:</span><span> boolean</span><span> {</span></span>
<span><span>  // Always keep errors</span></span>
<span><span>  if</span><span> (event.status_code </span><span>&gt;=</span><span> 500</span><span>) </span><span>return</span><span> true</span><span>;</span></span>
<span><span>  if</span><span> (event.error) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Always keep slow requests (above p99)</span></span>
<span><span>  if</span><span> (event.duration_ms </span><span>&gt;</span><span> 2000</span><span>) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Always keep VIP users</span></span>
<span><span>  if</span><span> (event.user?.subscription </span><span>===</span><span> 'enterprise'</span><span>) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Always keep requests with specific feature flags (debugging rollouts)</span></span>
<span><span>  if</span><span> (event.feature_flags?.new_checkout_flow) </span><span>return</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  // Random sample the rest at 5%</span></span>
<span><span>  return</span><span> Math.</span><span>random</span><span>() </span><span>&lt;</span><span> 0.05</span><span>;</span></span>
<span><span>}</span></span></code></pre>

<h2>Misconceptions</h2>
<h3>"Structured logging is the same as wide events"</h3>
<p>No. Structured logging means your logs are JSON instead of strings. That's table stakes. Wide events are a <em>philosophy</em>: one comprehensive event per request, with all context attached. You can have structured logs that are still useless (5 fields, no user context, scattered across 20 log lines).</p>
<h3>"We already use OpenTelemetry, so we're good"</h3>
<p>You're using a delivery mechanism. OpenTelemetry doesn't decide what to capture. You do. Most OTel implementations I've seen capture the bare minimum: span name, duration, status. That's not enough. You need to deliberately instrument with business context.</p>
<h3>"This is just tracing with extra steps"</h3>
<p>Tracing gives you request flow across services (which service called which). Wide events give you context <em>within</em> a service. They're complementary. <strong>Ideally, your wide events ARE your trace spans, enriched with all the context you need</strong>.</p>
<h3>"Logs are for debugging, metrics are for dashboards"</h3>
<p>This distinction is artificial and harmful. Wide events can power both. Query them for debugging. Aggregate them for dashboards. The data is the same, just different views.</p>
<h3>"High-cardinality data is expensive and slow"</h3>
<p>It's expensive on <em>legacy logging systems</em> built for low-cardinality string search. Modern columnar databases (ClickHouse, BigQuery, etc.) are specifically designed for high-cardinality, high-dimensionality data. The tooling has caught up. Your practices should too.</p>
<h2>The Payoff</h2>
<p>When you implement wide events properly, debugging transforms from archaeology to analytics.</p>
<p>Instead of: <em>"The user said checkout failed. Let me grep through 50 services and hope I find something."</em></p>
<p>You get: <em>"Show me all checkout failures for premium users in the last hour where the new checkout flow was enabled, grouped by error code."</em></p>
<p>One query. Sub-second results. Root cause identified.</p>
<p>Your logs stop lying to you. They start telling the truth. The whole truth.</p>

<p>Complete the form below to get a personalized report on your stack. I'll tell you what's working, what's not, and where you can save money. I genuinely want to hear about your logging nightmares :)</p>
      </div>

    <div id="signup">
        <p>Free in 30 seconds</p>
        <h2>Get your stack roasted.<br>Get a plan to fix it.</h2>
        <p>Answer a few questions and I'll send you a personalized report with:</p>
        <ul>
          <li>Where wide events would have the biggest impact on your stack</li>
          <li>What to log (and what to stop logging)</li>
          <li>Which tools are worth the cost (and which aren't)</li>
          <li>Quick wins you can ship this week</li>
        </ul>
        <p>Questions? Logging horror stories? Drop them in the comments below.</p>

        <div>
          
          
          <p>Your logs are just as empty.</p>
        </div>

        

        <div id="instant-analysis">
          <h3>Your stack has been judged.</h3>
          
          <p>Check your email for a detailed analysis.</p>
        </div>
      </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get an AI code review in 10 seconds (128 pts)]]></title>
            <link>https://oldmanrahul.com/2025/12/19/ai-code-review-trick/</link>
            <guid>46346391</guid>
            <pubDate>Sun, 21 Dec 2025 17:21:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oldmanrahul.com/2025/12/19/ai-code-review-trick/">https://oldmanrahul.com/2025/12/19/ai-code-review-trick/</a>, See on <a href="https://news.ycombinator.com/item?id=46346391">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><article><p>Here’s a trick I don’t see enough people using:</p><blockquote><p>Add <code>.diff</code> to the end of any PR URL and copy&amp;paste into a LLM</p></blockquote><p>You can get an instant feedback on any GitHub PR.</p><p><strong>No Copilot Enterprise. No browser extensions. No special tooling.</strong></p><p>That’s it.</p><h2 id="example">Example</h2><ol><li>PR Link: <code>https://github.com/RahulPrabha/oldmanrahul.com/pull/11</code></li><li>Add <code>.diff</code> to the end: <code>https://github.com/RahulPrabha/oldmanrahul.com/pull/11.diff</code></li><li>Copy the raw diff</li><li>Paste it into Claude, ChatGPT, or any LLM (Maybe add a short instuction like: <code>please review.</code>)</li></ol><h2 id="so-no-more-human-reviewers">So no more human reviewers?</h2><p>This isn’t a replacement for a real code review by a peer. But it’s a great way to get a first pass in short order.</p><p>Before you ping a teammate, run your PR through an LLM. You’ll catch obvious issues, get suggestions for edge cases you missed, and show up to the real review with cleaner code.</p><p>It’ll shorten your cycle times and be <a href="https://simonwillison.net/2025/Dec/18/code-proven-to-work/">a courtesy to others.</a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autoland saves King Air, everyone reported safe (236 pts)]]></title>
            <link>https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/</link>
            <guid>46346214</guid>
            <pubDate>Sun, 21 Dec 2025 16:57:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/">https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/</a>, See on <a href="https://news.ycombinator.com/item?id=46346214">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-td-block-uid="tdi_84">
<p>Garmin has confirmed the first emergency use of its <a href="https://avbrief.com/more-garmin-autothrottle-safety-backstop-for-big-king-airs/">Autoland</a> system occurred on Saturday in Colorado. “Garmin can confirm that an emergency Autoland activation occurred at Rocky Mountain Metropolitan Airport in Broomfield, Colorado,” the company said in a statement Sunday. “The Autoland took place on Sat., Dec. 20, resulting in a successful landing. We look forward to sharing additional details at the appropriate time.” Social media posts from flight tracking hobbyists reported a King Air 200 squawked 7700 about 2 p.m. local time today. The Autoland system was initiated and landed the aircraft at Rocky Mountain Metropolitan Airport near Denver. A recording from <a href="https://www.liveatc.net/archive.php?" rel="nofollow noopener" target="_blank">LiveATC’s feed</a> of the airport’s tower frequency includes a robotic female voice declaring a pilot incapacitation and the intention to land on Runway 30. The tape is below and first mention of the incident by ATC is at about 5:00. The Autoland system announces its intentions at about 11:10. (The time stamps are approximate.) There is no word on the condition of the pilot but social media posts suggest all aboard were safe.</p>



<figure><audio controls="" src="https://avbrief.com/wp-content/uploads/2025/12/autoland.mp3"></audio></figure>



<p>The aircraft, <a href="https://www.flightaware.com/live/flight/N479BR/history/20251220/1910Z/KASE/KBJC" rel="nofollow noopener" target="_blank">N479BR</a>, was being operated by Buffalo River Outfitters from Aspen to Rocky Mountain Metropolitan. It’s not clear how many people were on board. The system appeared to work flawlessly, and the controller at Rocky Mountain Metropolitan seemed to take it in stride, accommodating as many requests as he could before shutting down the airport for the landing. We’ll have more detail on this as it becomes available. </p>



<p>Larry Anglisano recorded this <a href="https://avbrief.com/king-air-emergency-autoland-demo/" data-type="post" data-id="6834">video demonstration of the Autoland system</a> in the Beechcraft King Air.</p>



<p>A reader was at the airport Saturday and shared this video that he had posted to Instagram.</p>



<blockquote data-service="instagram" data-category="marketing" data-placeholder-image="https://avbrief.com/wp-content/plugins/complianz-gdpr-premium/assets/images/placeholders/instagram-minimal.jpg" data-instgrm-captioned="" data-instgrm-permalink="https://www.instagram.com/p/DSgDCLlEfbw/?utm_source=ig_embed&amp;utm_campaign=loading" data-instgrm-version="14"></blockquote>

</div><div data-td-block-uid="tdi_92"><p><a href="https://avbrief.com/author/russ/" title="Russ Niles"><img src="https://avbrief.com/wp-content/uploads/2025/08/russ-niles-150x150.jpg" width="96" height="96" srcset="https://avbrief.com/wp-content/uploads/2025/08/russ-niles-300x300.jpg 2x" alt="Russ Niles"></a></p><div><p><a href="https://avbrief.com/author/russ/">Russ Niles</a></p><p>Russ Niles is Editor-in-Chief of AvBrief.com. He has been a pilot for 30 years and an aviation journalist since 2003. He and his wife Marni live in southern British Columbia where they also operate a small winery.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Books mentioned on Hacker News in 2025 (497 pts)]]></title>
            <link>https://hackernews-readings-613604506318.us-west1.run.app</link>
            <guid>46345897</guid>
            <pubDate>Sun, 21 Dec 2025 16:21:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackernews-readings-613604506318.us-west1.run.app">https://hackernews-readings-613604506318.us-west1.run.app</a>, See on <a href="https://news.ycombinator.com/item?id=46345897">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: WalletWallet – create Apple passes from anything (407 pts)]]></title>
            <link>https://walletwallet.alen.ro/</link>
            <guid>46345745</guid>
            <pubDate>Sun, 21 Dec 2025 16:04:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://walletwallet.alen.ro/">https://walletwallet.alen.ro/</a>, See on <a href="https://news.ycombinator.com/item?id=46345745">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><span></span>
            Totally free          </p>

          

          <p>
            A simple utility to convert physical barcodes into digital passes for Apple Wallet®. Entirely free and runs directly from your browser.
          </p>

          <div>
            <p><span>1</span>
              <span>Enter your membership or loyalty card barcode data.</span>
            </p>
            <p><span>2</span>
              <span>Configure the appearance and titles for your pass.</span>
            </p>
            <p><span>3</span>
              <span>Download and open the file to add it to your Wallet.</span>
            </p>
          </div>

          <div>
             <p><i></i>
               <span>No Sign-up</span>
             </p>
             <p><i></i>
               <span>Private</span>
             </p>
             <p><i></i>
               <span>No Install</span>
             </p>
          </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[E.W.Dijkstra Archive (138 pts)]]></title>
            <link>https://www.cs.utexas.edu/~EWD/welcome.html</link>
            <guid>46345523</guid>
            <pubDate>Sun, 21 Dec 2025 15:29:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.utexas.edu/~EWD/welcome.html">https://www.cs.utexas.edu/~EWD/welcome.html</a>, See on <a href="https://news.ycombinator.com/item?id=46345523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div height="3739" colspan="2" xpos="123">
								<table>
									<tbody><tr>
										<td><img src="https://www.cs.utexas.edu/~EWD/EWDwww.jpg" alt="Photo of Edsger W. Dijkstra" height="306" width="237"><span size="-3">(<i>photo</i> ©2002 <i>Hamilton Richards</i>)</span></td>
										<td></td>
										<td><br>
											Edsger Wybe Dijkstra was one of the most influential members of computing science’s founding generation. Among the domains in which his scientific contributions are fundamental are
											<ul>
												<li type="disc">algorithm design
												</li><li type="disc">programming languages
												</li><li type="disc">program design
												</li><li type="disc">operating systems
												</li><li type="disc">distributed processing
												</li><li type="disc">formal specification and verification
												</li><li type="disc">design of mathematical arguments
											</li></ul>
											<p>In addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry.</p>
											<p>During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra’s contributions brought him many prizes and awards, including computing science’s highest honor, the ACM Turing Award.</p>
										</td>
									</tr>
								</tbody></table>
								<hr>
								<h2>The Manuscripts</h2>
								<p>Like most of us, Dijkstra always believed it a scientist’s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as “EWDs”, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra’s writings, the informal circulation of many of the EWDs eventually reached into the thousands.</p>
								<p>Although most of Dijkstra’s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, you’ll need a copy of <a href="http://www.adobe.com/products/acrobat/readstep.html">Acrobat Reader</a>). We hope you will find it convenient, useful, inspiring, and enjoyable.</p>
								<p>The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at <a href="http://www.lib.utexas.edu/taro/utcah/00378/cah-00378.html#a0">The Center for American History</a> of The University of Texas at Austin.</p>
								<h3>Indexes</h3>
								<p>Each manuscript file is accessible through either of two indexes:</p>
								<p><b><span size="+1">0. <a href="https://www.cs.utexas.edu/~EWD/indexBibTeX.html">BibTeX</a> index. </span></b>Each entry includes all the available bibliographic data.</p>
								<p><b><span size="+1">1. Ad-hoc indexes. </span></b>These contain titles only, but are faster if you know what you’re looking for.</p>
								<blockquote>
									<p><a href="https://www.cs.utexas.edu/~EWD/indexChron.html"><b>EWD-numbered documents</b></a><b> </b>(This index gives an approximate correspondence between manuscripts’ EWD numbers and the year in which they appeared.)</p>
									<p><a href="https://www.cs.utexas.edu/~EWD/indexMCreps.html"><b>Technical reports</b></a> from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)</p>
									<p><a href="https://www.cs.utexas.edu/~EWD/PhDthesis/PhDthesis.PDF" target="_blank"><b>PhD thesis</b></a><b> (5.3 MB)</b></p>
									<p><a href="https://www.cs.utexas.edu/~EWD/indexOtherDocs.html"><b>Other documents</b></a></p>
								</blockquote>
								<p>You can find a table relating EWD numbers to publication years <a href="https://www.cs.utexas.edu/~EWD/indexChron.html">here</a>.</p>
								<p>Many of the privately circulated manuscripts collected here were subsequently published; their <a href="https://www.cs.utexas.edu/~EWD/copyrights.html">copyrights</a> are held by their respective publishers.</p>
								<h3>Transcripts and translations</h3>
								<p>A growing number of the PDF bitmap documents have been <a href="https://www.cs.utexas.edu/~EWD/transcriptions/transcriptions.html">transcribed</a> to make them searchable and accessible to visitors who are visually impaired.</p>
								<p>A few of the manuscripts written in Dutch have been <a href="https://www.cs.utexas.edu/~EWD/translations/translations.html">translated</a> into English, and one —EWD1036— has been translated into <a title="Spanish translation of EWD 1036" href="http://www.smaldone.com.ar/documentos/ewd/sobre_la_crueldad.html">Spanish</a>. EWD28  has been translated from English into <a href="https://www.cs.utexas.edu/~EWD/translations/EWD28%20Russian.html">Russian</a>.</p>
								<p>For these transcriptions and translations we are grateful to over sixty <a href="https://www.cs.utexas.edu/~EWD/transcriptions/transcribers.html">contributors</a>. <span color="#ff0033"><b>Volunteers</b></span> willing to transcribe manuscripts are always <a href="https://www.cs.utexas.edu/~EWD/transcriptions/invitation.html">welcome</a> (<b>Note</b>: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).</p>
								<p><b>Proofreading</b> Each transcription gets a cursory scan as it’s prepared for uploading, but since a web page can always be updated, I don’t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please 
									
								</p>
								
								<h3>Links between EWDs</h3>
								<p>A compilation of <a href="https://www.cs.utexas.edu/~EWD/AnnotationsEtc/Michaelis.html" target="_blank">cross-references</a> has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it.</p>
								<p>Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didn’t provide the backward references, we indicate the relationship by "see also" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to 
									
								</p>
								
								
								<h3>Summaries</h3>
								<p>We have begun adding <a href="https://www.cs.utexas.edu/~EWD/Summaries/Summaries.html">summaries</a> of the EWDs. This innovation was suggested by Günter Rote, who contributed the first dozen summaries. Additional contributions of summaries—especially summaries in English of EWDs in Dutch—are most welcome.</p>
								<hr>
								<h3>Copyrights</h3>
								<p>Copyrights in most EWDs  are  held by his children, one of whom —



                                
—  handles  requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed <a href="https://www.cs.utexas.edu/~EWD/copyrightsDetail.html" title="Copyrights">here</a>, and each one is provided with a cover page identifying the copyright holder.</p>
								<p>Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, <a href="http://www.cah.utexas.edu/research/permission_publish.php" title="BCAH permission policies">the Center’s policies</a> are also applicable.</p>
								<hr>
								<h2>Video and audio</h2>
								<p>In addition to the manuscripts, you may enjoy some <a href="https://www.cs.utexas.edu/~EWD/video-audio/video-audio.html">recordings</a> of Dijkstra lectures and interviews.</p>
								<hr>
								<h2>About Dijkstra and his work</h2>
								<p>An <a href="https://www.cs.utexas.edu/~EWD/misc/vanVlissingenInterview.html">interview</a> with Dijkstra (Spanish translation <a href="https://www.cs.utexas.edu/~EWD/misc/vanVlissingenEntrevista.html">here</a>) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a <a href="http://digitalundivide.blogspot.com/2005/12/ewd-personal-reflection.html">personal reflection</a> on “Dijkstra’s sense of what computer science and programming are and what they aren’t.”</p>
								<p>Another interview was conducted by Philip L. Frana in August 2001. A <a title="link to Dijkstra oral-history interview" href="https://hdl.handle.net/11299/107247">transcript</a> is available in the on-line collection of the Charles Babbage Institute.</p>
								<p>To mark the occasion of Dijkstra’s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, <i>In Pursuit of Simplicity</i>, which took place on his birthday in May 2000. The symposium’s <a href="https://www.cs.utexas.edu/~EWD/symposiumProgram.pdf">program</a> (10 MB) contains an outline of Dijkstra’s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a <a href="http://www.cs.utexas.edu/users/EWD/videos/EWD6.mpg" target="_blank">video</a>.</p>
								<p>Dijkstra’s death in August 2002 was marked by many <a href="https://www.cs.utexas.edu/~EWD/obituaries/obits.html">obituaries</a> and memorials, including the Computer Sciences department’s <a href="http://www.cs.utexas.edu/users/EWD/memorial/" target="_blank">memorial celebration.</a></p>
								<p>A <a title="Maarten van Emden: I remember Edsger Dijkstra" href="http://vanemden.wordpress.com/2008/05/06/i-remember-edsger-dijkstra-1930-2002/">remembrance</a> of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it).</p>
								<p>In 2021 Krzysztof R. Apt and Tony Hoare edited a 
									<a name="commemoration" href="https://www.cs.utexas.edu/~EWD/commemoration/EWD-commemoration-2021.pdf">commemoration of Edsger Dijkstra</a>
									written by more than twenty computer scientists who knew him as a colleague, teacher, and friend.</p>
								<p>A <a title="E.G.Daylight's Dijkstra blog" href="http://www.dijkstrascry.com/">blog</a> devoted to Dijkstra’s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An <a title="article: Dijkstra's Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s" href="http://www.dijkstrascry.com/node/4">article</a> by Daylight, “Dijkstra’s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,” appeared in <i>The Computer Journal</i>, March 2011.</p>
								<p>In his blog A Programmer’s Place, Maarten van Emden has an <a title="link to van Emden's blog entry" href="http://vanemden.wordpress.com/2011/01/15/another-scoop-by-dijkstra/#more-366">entry entitled “Another scoop by Dijkstra?”.</a> The entry describes Dijkstra’s “remarkable insight [in “<a href="https://www.cs.utexas.edu/~EWD/ewd02xx/EWD249.PDF">Notes on Structured Programming” (EWD 249)</a>] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)” by applying the Assembly-line Principle.</p>
								<p>The <a href="http://www.podc.org/dijkstra/">Edsger W. Dijkstra Prize in Distributed Computing</a> honors Dijkstra’s “foundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.” </p>
								<hr>
								<h2><b>The Dijkstra Memorial Lectures</b></h2>
								<p>A <a title="link to the index page of the Dijkstra Memorial Lectures" href="https://www.cs.utexas.edu/~EWD/DijkstraMemorialLectures/lectureIndex.html">series of annual lectures</a> in memory of Dijkstra commenced at The University of Texas in October 2010.</p>
								<hr>
								<h2>About this site</h2>
								<p>Recent significant changes in the site are listed <a href="https://www.cs.utexas.edu/~EWD/recentChanges.html">here</a>; the most recent change was posted on 30 March 2021.</p>
								<p>The folks who contributed most significantly to the site’s creation are acknowledged <a href="https://www.cs.utexas.edu/~EWD/acknowledgments.html">here</a>.</p>
								<p>Comments and suggestions about the site are always welcome; please email them to the<br>
									
								</p>
								
								<hr>
								<h2>Related site</h2>
								<p>If you find this site interesting, you may also be interested in another site:</p>
								<blockquote>
									<p><a href="http://www.mathmeth.com/" target="_blank">Discipline in Thought</a> which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others).</p>
								</blockquote>
								<hr>
								
								<p><i><span size="-1">Revised <csobj format="MedDate" h="16" region="15" t="DateTime" w="95">
								  <!-- #BeginDate format:En2 -->2020-01-12<!-- #EndDate -->
								</csobj>
								</span></i></p>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CO2 batteries that store grid energy take off globally (277 pts)]]></title>
            <link>https://spectrum.ieee.org/co2-battery-energy-storage</link>
            <guid>46345506</guid>
            <pubDate>Sun, 21 Dec 2025 15:27:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/co2-battery-energy-storage">https://spectrum.ieee.org/co2-battery-energy-storage</a>, See on <a href="https://news.ycombinator.com/item?id=46345506">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Energy Dome</span> began operating its 20-megawatt, long-duration energy<u><span>-</span></u><span>storage </span>facility in July 2025 in Ottana, Sardinia. In 2026, replicas of the system will begin popping up on multiple continents. </p><div data-headline="Grid-Scale Bubble Batteries Will Soon Be Everywhere"><p><strong><sub></sub>This giant bubble on</strong> the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasn’t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the dome’s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until it’s needed.</p><p>Developed by the Milan-based company <a href="https://energydome.com/" target="_blank">Energy Dome</a>, the bubble and its surrounding machinery demonstrate a first-of-its-kind “CO2 Battery,” as the company calls it. The facility compresses and expands CO<sub>2</sub> daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.</p><p>We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere there’s 5 hectares of flat land.</p><p>The first to build one outside of Sardinia will be one of India’s largest power companies, <a href="https://ntpc.co.in/" target="_blank">NTPC Limited</a>. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility <a href="https://www.alliantenergy.com/" target="_blank">Alliant Energy</a> received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.</p><p>And <a href="https://spectrum.ieee.org/tag/google">Google</a> <a href="https://blog.google/outreach-initiatives/sustainability/long-term-energy-storage/" target="_blank">likes the concept</a> so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling <a href="https://spectrum.ieee.org/tag/data-centers">data centers</a> with round-the-clock <a href="https://spectrum.ieee.org/tag/clean-energy">clean energy</a>, even when the sun isn’t shining or the wind isn’t blowing. The partnership with Energy Dome, announced in July, marked Google’s first investment in long-duration <a href="https://spectrum.ieee.org/tag/energy-storage">energy storage</a>.</p><p>“We’ve been scanning the globe seeking different solutions,” says <a href="https://www.linkedin.com/in/ainhoa-anda/en?originalSubdomain=fr" target="_blank">Ainhoa Anda</a>, Google’s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. “So standardization is really important, and this is one of the aspects that we really like” about Energy Dome, she says. “They can really plug and play this.”</p><p>Google will prioritize placing the Energy Dome facilities where they’ll have the most impact on <a href="https://spectrum.ieee.org/tag/decarbonization">decarbonization</a> and grid reliability, and where there’s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Google’s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.</p><p>Anda says Google expects to help the technology “reach a massive commercial stage.”</p><h2>Getting creative with long-duration energy storage</h2><p>All this excitement is based on Energy Dome’s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transition’s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.</p><p>When sun and wind are abundant, solar and <a href="https://spectrum.ieee.org/tag/wind-farms">wind farms</a> tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.</p><p>The problem is that even the best new grid-scale storage systems on the market—mainly <a href="https://spectrum.ieee.org/tag/lithium-ion">lithium-ion</a> batteries—provide only about 4 to 8 hours of storage. That’s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.</p><p> <img alt="A series of large cylindrical tanks of different diameters, each as tall a multistory building" data-rm-shortcode-id="8aacfbdeb1de61583aea6f1a8341e6e6" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-series-of-large-cylindrical-tanks-of-different-diameters-each-as-tall-a-multistory-building.jpg?id=62599073&amp;width=980" height="1333" id="6b5ba" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-series-of-large-cylindrical-tanks-of-different-diameters-each-as-tall-a-multistory-building.jpg?id=62599073&amp;width=980" width="2000"> <small placeholder="Add Photo Caption...">After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. </small><small placeholder="Add Photo Credit...">Luigi Avantaggiato</small></p><p>Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually aren’t economically viable. Other grid-scale <a href="https://spectrum.ieee.org/what-energy-storage-would-have-to-cost-for-a-renewable-grid" target="_self">battery chemistries and approaches</a> are in development, such as sodium-based, iron-air, and <a href="https://spectrum.ieee.org/its-big-and-longlived-and-it-wont-catch-fire-the-vanadium-redoxflow-battery" target="_self">vanadium redox flow batteries</a>. But the energy density, costs, degradation, and <a href="https://spectrum.ieee.org/natron-sodium-ion-battery-failure" target="_self">funding</a> complications have challenged the developers of those alternatives.</p><p>Researchers have also experimented with storing energy by <a href="https://spectrum.ieee.org/hydrostor-stymied" target="_self">compressing air</a>, <a href="https://spectrum.ieee.org/could-storing-electricity-in-whitehot-blocks-give-supercheap-renewables-storage" target="_self">heating up blocks</a> or<a href="https://spectrum.ieee.org/thermal-energy" target="_self"> sand</a>, <a href="https://spectrum.ieee.org/hydrogen-storage-grid-scale" target="_self">using hydrogen</a> or<a href="https://spectrum.ieee.org/methanol-energy-storage" target="_self"> methanol</a>, <a href="https://spectrum.ieee.org/arpa-e-summit-2024" target="_self">pressurizing water deep underground</a>, and even <a href="https://spectrum.ieee.org/gravity-energy-storage-will-show-its-potential-in-2021" target="_self">dangling heavy objects in the air and dropping them</a>. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.</p><p>The tried-and-true grid-scale storage option—<a href="https://spectrum.ieee.org/a-big-hydro-project-in-big-sky-country" target="_self">pumped hydro</a>, in which water is pumped between reservoirs at different elevations—lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.</p><p>CO2 Batteries check a lot of boxes that other approaches don’t. They don’t need special topography like pumped-hydro reservoirs do. They don’t need critical <a href="https://spectrum.ieee.org/tag/minerals">minerals</a> like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as <a href="https://spectrum.ieee.org/tag/lithium-ion-batteries">lithium-ion batteries</a>. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.</p><p><a href="https://spectrum.ieee.org/tag/china">China</a> has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO<sub>2</sub>-based energy-storage facility in the Xinjiang region of northwest China. Media reports <a href="https://www.seetaoe.com/details/242600.html#:~:text=The%20project%20is%20a%20new,reliability%20of%20the%20power%20grid." target="_blank">show renderings</a> of domes but give <a href="https://www.bloominglobal.com/media/detail/worlds-largest-compressed-carbon-dioxide-energy-storage-project-tops-out" target="_blank">widely varying storage capacities</a>—including 100 MW and 1,000 MW. The Chinese companies did not respond to <em><em>IEEE Spectrum</em></em>’s requests for information.</p><p>“What I can say is that they are developing something very, very similar [to Energy Dome’s CO2 Battery] but quite large in scale,” says <a href="https://www.linkedin.com/in/claudio-spadacini/?originalSubdomain=it" target="_blank">Claudio Spadacini</a>, Energy Dome’s founder and CEO. The Chinese companies “are good, they are super fast, and they have a lot of money,” he says.</p><h2>Why is Google investing in CO2 Batteries?</h2><p>When I visited Energy Dome’s Sardinia facility in October, the CO<sub>2</sub> had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO<sub>2</sub>, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.</p><p>Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.</p><p>“This is incredible,” I said to my guide, <a href="https://www.linkedin.com/in/mario-torchio/?locale=it_IT" target="_blank">Mario Torchio</a>, Energy Dome’s global marketing and communications director.</p><p>“It is. But it’s physics,” he said.</p><p>Outside the dome, a series of machines connected by undulating pipes moves the CO<sub>2</sub> out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO<sub>2</sub> to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.</p><p>To discharge the battery, the process reverses. The liquid CO<sub>2</sub> is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.</p><p> <img alt="Workers in hard hats and a series of connected pipes and tanks with the quilted-looking exterior of the dome in the background  " data-rm-shortcode-id="14ccdb3a8a61303def0bf31138e7667c" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/workers-in-hard-hats-and-a-series-of-connected-pipes-and-tanks-with-the-quilted-looking-exterior-of-the-dome-in-the-background.jpg?id=62599071&amp;width=980" height="1334" id="8db3a" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/workers-in-hard-hats-and-a-series-of-connected-pipes-and-tanks-with-the-quilted-looking-exterior-of-the-dome-in-the-background.jpg?id=62599071&amp;width=980" width="2000"> <small placeholder="Add Photo Caption...">Energy Dome engineers inspect the dryer system, which keeps the gaseous CO₂ in the dome at optimal dryness levels at all times.</small><small placeholder="Add Photo Credit...">Luigi Avantaggiato</small></p><p>It’s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. “How we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensing…can really cut costs and increase the efficiency,” he says.</p><p>The company uses pure, purpose-made CO<sub>2</sub> instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.</p><h2>What happens if the dome is punctured?</h2><p>On the downside, Energy Dome’s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.</p><p>And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a day’s warning of severe weather, the company can just compress and store the CO<sub>2</sub> in the tanks and then deflate the outer dome, he says.</p><p>If the worst happens and the dome is punctured, 2,000 tonnes of CO<sub>2</sub> will enter the atmosphere. That’s equivalent to the emissions of about 15 round-trip flights between New York and London on a <a href="https://spectrum.ieee.org/tag/boeing">Boeing</a> 777. “It’s negligible compared to the emissions of a coal plant,” Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.</p><p>Worth the risk? The companies lining up to build these systems seem to think so. <span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARIN Public Incident Report – 4.10 Misissuance Error (137 pts)]]></title>
            <link>https://www.arin.net/announcements/20251212/</link>
            <guid>46345444</guid>
            <pubDate>Sun, 21 Dec 2025 15:19:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arin.net/announcements/20251212/">https://www.arin.net/announcements/20251212/</a>, See on <a href="https://news.ycombinator.com/item?id=46345444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-text">
      <p><strong>Posted: Friday, 12 December 2025</strong> <br>
      <a href="#related" data-bs-toggle="collapse" role="button" aria-expanded="false">ARIN </a></p>
      
      
      <h2 id="executive-summary">Executive Summary</h2>
<p>On 2 December 2025, an IPv4 block 23.150.164.0/24, correctly allocated to the Original Customer, was inadvertently removed and reissued to the Requesting Customer during a 4.10 allocation process. This error stemmed from the current manual and partially offline 4.10 inventory process.</p>
<p>The incorrect state persisted until 9 December 2025, when the Original Customer reported the issue. ARIN restored the 23.150.164.0/24 to the Original Customer, issued a replacement /24 to the Requesting Customer, coordinated withdrawal of the incorrect route announcement, and notified the affected parties.</p>
<p>This incident highlights known weaknesses in ARIN’s current Internet Number Resources (INR) Inventory handling for 4.10 transition space and underscores the need to complete the transition to a fully automated, integrated online inventory architecture.</p>
<h2 id="incident-description">Incident Description</h2>
<p>Following the current allocation process for 4.10 space, an RSD analyst:</p>
<ul>
<li>Relied on legacy/manual 4.10 inventory artifacts, including a flat file and sparse allocation spreadsheet</li>
<li>Did not recognize indicators in ARIN Online showing that 23.150.164.0/24 was already allocated to the Original Customer</li>
<li>Removed 23.150.164.0/24 from the Original Customer</li>
<li>Reissued that same /24 to the Requesting Customer</li>
</ul>
<p>As a result, the registration record and associated ROAs for the Original Customer were deleted in error, and the /24 appeared as allocated to the Requesting Customer in ARIN’s systems.</p>
<h2 id="customer-impact-and-risk">Customer Impact and Risk</h2>
<ul>
<li>The /24 was removed from the Original Customer account and assigned to another organization.</li>
<li>The ROA associated with the block was removed and had to be recreated after restoration.</li>
<li>The block was announced by a third-party provider under the incorrect registration, introducing risk of routing conflict and confusion.</li>
<li>The incorrect state persisted for approximately seven days before detection.</li>
<li>The Original Customer reported the issue via Ask ARIN and a Help Desk call on 9 December 2025.</li>
</ul>
<p>The customer has not provided a technical impact statement.</p>
<h2 id="timeline-of-events">Timeline of Events</h2>
<p><em>(All event times are ET – Eastern Time)</em></p>
<h3 id="25-26-november-2025">25–26 November 2025</h3>
<ul>
<li>25 November, 11:59 AM – 4.10 space request received from the Requesting Customer.</li>
<li>26 November, 6:25 AM – Ticket assigned to an RSD Analyst for processing.</li>
</ul>
<h3 id="2-december-2025-incident-occurs">2 December 2025 – Incident Occurs</h3>
<ul>
<li>12:10 PM – Ticket approved for issuance of 4.10 space by designated RSD Analyst.</li>
<li>~12:10–12:30 PM – In the process of fulfilling the 4.10 request, the designated Analyst:
<ul>
<li>Opened the e-black-book (an offline Excel-based inventory file, separate from the primary online inventory system), reviewed the existing 4.10 allocations, and selected 23.150.164.0 as the next available sparse entry.</li>
<li>Returned to the ARIN Online management application and queried for 23.150.164.0 based on the entry identified in the e-black-book. At this time the analyst did not recognize that the /24 was already allocated to the Original Customer.</li>
<li>Performed a block split and deleted 23.150.164.0/24 – not recognizing that it was allocated to the Original Customer – which removed associated registry services (ROAs, reverse DNS, etc.).</li>
<li>Issued 23.150.164.0/24 to the Requesting Customer.</li>
</ul>
</li>
</ul>
<h3 id="2-9-december-2025-incorrect-state-persists">2–9 December 2025 – Incorrect State Persists</h3>
<ul>
<li>The /24 remained misregistered.</li>
<li>The Requesting Customer upstream provider announced the block.</li>
<li>No automated detection of the error occurred.</li>
</ul>
<h3 id="9-december-2025-detection-and-resolution">9 December 2025 – Detection and Resolution</h3>
<ul>
<li>10:12 AM – The Original Customer submitted an Ask ARIN ticket regarding the problem.</li>
<li>10:14 AM – The Original Customer contacted the Help Desk; escalation to Director at 10:20 AM.</li>
<li>10:20–10:30 AM – Director reviewed block history and directed corrective actions.</li>
<li>10:30 AM – Director and CXO approved:
<ul>
<li>Removal of the /24 from the Requesting Customer</li>
<li>Issuance of a replacement /24 to the Requesting Customer</li>
<li>Restoration of 23.150.164.0/24 to the Original Customer</li>
<li>Coordination of route withdrawal</li>
<li>Update of inaccurate POC information</li>
</ul>
</li>
<li>10:44 AM – First notification email sent to the Requesting Customer.</li>
<li>10:54 AM – Second email sent noting invalid phone contact.</li>
<li>12:01 PM – Corrective actions completed.</li>
</ul>
<h2 id="root-cause">Root Cause</h2>
<p>A manual 4.10 workflow that relies on a combination of online systems and offline flat files/spreadsheets for inventory management allowed a current customer allocation to be mistakenly identified as available for issuance. This reliance on offline spreadsheets is a legacy constraint where post-runout 4.10 inventory is maintained outside the primary online system to keep it reserved. The lack of a unified view of inventory and related business-rule-driven system controls enabled the error to proceed without detection.</p>
<h2 id="contributing-factors">Contributing Factors</h2>
<ul>
<li>Hybrid inventory architecture (online + offline) for 4.10 space.</li>
<li>Sparse allocation methods implemented through manual tools rather than integrated system logic.</li>
<li>Generic warning messages that are not routing aware or business-rule driven.</li>
<li>High demand on analysts to catch procedural errors in a manual “swivel chair” workflow.</li>
</ul>
<h2 id="mitigation-plan-and-next-steps">Mitigation Plan and Next Steps</h2>
<h3 id="immediate-near-term-controls-completed">Immediate / Near-Term Controls (Completed)</h3>
<ul>
<li><strong>Updated Process Controls (completed)</strong>
<ul>
<li>RSD has implemented additional process controls that require a dual review for all ticketing type workflows that include a network delete.</li>
<li>Only a limited set of experienced analysts are permitted to perform this function.</li>
<li>Reviews and approvals are performed at set times each day with a second reviewer involved for any ticket that includes a delete step.</li>
</ul>
</li>
<li><strong>Updated 4.10 Issuing Playbook</strong>
<ul>
<li>Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes:
<ul>
<li>Required checks for existing allocations and ROAs</li>
<li>Explicit verification steps prior to any delete/reissue action</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="system-and-architecture-improvements-medium-term">System and Architecture Improvements (Medium-Term)</h3>
<p><strong>Accelerate the ongoing INR Inventory Management Roadmap item:</strong> This incident reinforces the urgency of the architecture work already underway (reviewed Oct 2025) to move legacy offline inventories into a modern, online architecture. Specific alignment actions include:</p>
<ul>
<li><strong>Stronger business rule–based warnings and controls</strong>
<ul>
<li>Enhance warning logic when issuing or modifying 4.10 space to include:
<ul>
<li>Clear alerts if the /24 is already allocated to an Org</li>
<li>Clear alerts if active ROAs exist for the exact block or covering prefix</li>
</ul>
</li>
<li>Implement system controls for resource types and staff roles, with flags and audit trails for review and auditing.</li>
<li>Replace generic, non–ROA-aware warnings that are easily treated as noise.</li>
</ul>
</li>
<li><strong>Continue Engineering solution for offline inventory</strong>
<ul>
<li>Move offline 4.10 and microallocation inventories, and the viip file for IPv6, into the integrated online inventory architecture.</li>
<li>Eliminate reliance on separate spreadsheets and flat files for production issuing.</li>
<li>Implement business-rule-driven warnings for existing allocations and ROAs</li>
<li>Introduce role-based controls, flags, and audit trails</li>
</ul>
</li>
<li><strong>Advance the “Updated Resource Status Taxonomy” work</strong>
<ul>
<li>Ensure 4.10 status and history are fully visible and consistent inside the primary system.</li>
<li>Provide analysts with a clear, unified view of current holder, status, and ROA/IRR context.</li>
</ul>
</li>
<li><strong>Fast-track automation of all inventory issuing</strong>
<ul>
<li>Reduce or eliminate manual issuing where possible, with priority for higher-risk categories such as 4.10 and RPKI-covered space.</li>
</ul>
</li>
</ul>
<p>Regards,</p>
<p>American Registry for Internet Numbers (ARIN)</p>

      





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reasons Not to Become Famous (2020) (156 pts)]]></title>
            <link>https://tim.blog/2020/02/02/reasons-to-not-become-famous/</link>
            <guid>46345341</guid>
            <pubDate>Sun, 21 Dec 2025 15:07:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tim.blog/2020/02/02/reasons-to-not-become-famous/">https://tim.blog/2020/02/02/reasons-to-not-become-famous/</a>, See on <a href="https://news.ycombinator.com/item?id=46345341">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img fetchpriority="high" decoding="async" width="2128" height="1416" src="https://i1.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?fit=600%2C399&amp;ssl=1" alt="" srcset="https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?w=2128&amp;ssl=1 2128w, https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?resize=1024%2C681&amp;ssl=1 1024w, https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?resize=768%2C511&amp;ssl=1 768w, https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?resize=1536%2C1022&amp;ssl=1 1536w, https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?resize=2048%2C1363&amp;ssl=1 2048w, https://i0.wp.com/tim.blog/wp-content/uploads/2020/02/Tim_November_2008.jpg?resize=360%2C240&amp;ssl=1 360w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption><strong>November of 2008.</strong> I had more hair, a flip phone, and absolutely no idea what was coming.</figcaption></figure>



<blockquote>
<p>Let the cymbals of popularity tinkle still. Let the butterflies of fame glitter with their wings. I shall envy neither their music nor their colors.</p>
<cite>— John Adams<br><em>Letters of John Adams Addressed to His Wife</em></cite></blockquote>



<p>“If I’m not famous by 30, I might as well put a bullet in my head.”</p>



<p>That’s an actual sentence I spoke to one of my closest friends. At the time, I was 28.</p>



<p>Fortunately, unlike <a href="https://tim.blog/2015/05/06/how-to-commit-suicide/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">during my darkest period in college</a>, I wasn’t serious about suicide. Nonetheless, the sentiment was real. I felt like I somehow needed fame. In retrospect, there was a lot of self-loathing from tough childhood experiences, and I desperately hoped that love from without (i.e., from masses of other people) would somehow make up for hate from within.</p>



<p>As luck would have it, I got to test this hypothesis.</p>



<p><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/4-Hour-Workweek-Escape-Live-Anywhere/dp/0307465357/?tag=offsitoftimfe-20" target="_blank"><em>The 4-Hour Workweek</em></a>, my first book, was published in 2007. It hit the <em>New York Times</em> Hardcover Business bestseller list, where it stayed for an unbroken four years and four months. It was quickly translated into approximately 40 languages, and shit went bonkers. Everything changed.</p>



<p>I was 29.</p>



<p>Soon, I was engulfed in a hailstorm of both great and terrible things, and I was utterly unprepared for any of it.</p>



<p>To kick off this post, let’s start with a real example from 2010. I vividly remember the day I received an email from someone we’ll call “James.” James was a frequent commenter on my blog, and we’d become friendly over time. He was a great guy and a huge help to other readers. I’d given him advice, he’d built a few successful businesses, and we’d developed a nice virtual rapport. That day in 2010, however, I actually received an email from James’ longtime assistant. It was succinct: “James learned so much from you, and he instructed me to give you this video.” I clicked on the attachment. James popped up. He was clearly agitated and clenching his jaw, making contorted faces and speaking strangely. He thanked me for all of my help over the years and explained that it had helped him through some very dark times. He finished by saying that he was sorry, but that he had to end things. That’s when he turned off the video and killed himself.</p>



<p>This experience profoundly fucked me up for a long period of time.</p>



<p>Suffice to say, I didn’t realize that this type of thing was part of the Faustian fame-seeking bargain.</p>



<h3><strong>THE 30,000-FOOT VIEW</strong></h3>



<p>Now it’s 2020. 13 years, 5 books, 1,000+ blog posts, and nearly 500M podcast downloads later, I’ve learned a few things about the promises and perils of seeking fame.</p>



<p>And I say “seeking fame” deliberately, because—let’s be honest—I’m not <em>really</em> famous. Beyoncé and Brad Pitt are truly famous. They cannot walk around in public anywhere in the world. I am a micro public figure with a monthly audience in the millions or tens of millions. There are legions&nbsp;of people on Instagram alone with audiences of this size. New platforms offer new speed. Some previous unknowns on TikTok, for example, have attracted millions of followers in a matter of <em>weeks</em>.</p>



<p>If you suddenly had 100,000 or 1,000,000 or 10,000,000 more followers, what might happen?</p>



<p>I thought I knew, and I was naive.</p>



<p>This post will explore a lot of things. Chief among them will be answering the question: if you win the popularity game, what might you expect?</p>



<p>I’ll mention some of the rewards and upsides, which can be incredible. I will also talk about some of the risks and downsides, which can be horrifying.</p>



<p>My hope is that this post will help people better understand the wall their ladder is leaning against… before they spend years climbing towards the top. Or, in a world of TikTok-like acceleration, before they let the genie out of the bottle without thinking it through.</p>



<p>If you’re interested in building a large audience to become rich and famous, some warnings and recommendations are in order. If you’re interested in building a large audience you also truly care about and with whom you are vulnerable, even more precautionary tales are in order.</p>



<h3><strong>ON THE BRIGHT SIDE, SOME VERY REAL BENEFITS</strong></h3>



<p>Let’s cover some of the great stuff first.</p>



<p>One could easily argue that the national exposure that accompanied <a href="https://www.amazon.com/4-Hour-Workweek-Escape-Live-Anywhere/dp/0307465357/?tag=offsitoftimfe-20" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>The 4-Hour Workweek</em></a> and <a href="https://tim.blog/books" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">later books</a> was a necessary ingredient for:</p>



<ul>
<li>Meeting many of my now dear friends, including <a target="_blank" href="http://instagram.com/kevinrose">Kevin R</a><a aria-label="o (opens in a new tab)" href="http://instagram.com/kevinrose" target="_blank" rel="noreferrer noopener">o</a><a target="_blank" href="http://instagram.com/kevinrose">se</a>, <a aria-label=" (opens in a new tab)" href="http://ma.tt/" target="_blank" rel="noreferrer noopener">Matt Mullenweg</a>, and many others. These are people I hope will be my close brothers and sisters for life. That led to…</li>



<li>Being able to invest in <a aria-label=" (opens in a new tab)" href="https://angel.co/tim" target="_blank" rel="noreferrer noopener">dozens of early-stage technology deals</a>, the proceeds of which then allowed…</li>



<li>Helping <a aria-label=" (opens in a new tab)" href="https://tim.blog/about/causes/" target="_blank" rel="noreferrer noopener">many causes and organizations</a> that are making a real, positive dent in the world, including education (<a aria-label=" (opens in a new tab)" href="https://www.newsday.com/long-island/education/author-tim-ferriss-flash-funds-145-school-projects-on-li-1.11567795" target="_blank" rel="noreferrer noopener">DonorsChoose</a>, <a aria-label=" (opens in a new tab)" href="https://www.wsj.com/articles/silicon-valley-venture-capitalists-help-connect-low-income-students-with-elite-colleges-1431535621" target="_blank" rel="noreferrer noopener">QuestBridge</a>, etc.); <a aria-label=" (opens in a new tab)" href="https://www.nytimes.com/2019/09/04/science/psychedelic-drugs-hopkins-depression.html?searchResultPosition=1" target="_blank" rel="noreferrer noopener">scientific research aimed at treatments for chronic depression, PTSD, and other “intractable” psychiatric conditions</a> (Johns Hopkins, <a target="_blank" href="https://clinicaltrials.ucsf.edu/trial/NCT02950467">UCSF</a>, etc.); and more.</li>



<li>Launching projects to aid the above (e.g., <a aria-label=" (opens in a new tab)" href="https://tim.blog/2019/03/09/trip-of-compassion/" target="_blank" rel="noreferrer noopener"><em>Trip of Compassion</em> documentary</a>)</li>
</ul>



<p>And then there are the occasional fringe benefits, like getting tables at busy restaurants, getting free samples of products (although “free” often ends up being the most expensive), and so on.</p>



<p>Many of the things I’m proudest of in life would have been difficult or impossible to accomplish without a large audience. For that, I owe every one of my readers and listeners a huge debt of gratitude.</p>



<p>Using fame as a lever, however, can be tricky.&nbsp;</p>



<p>First off, what type of “fame” do you want? In concrete terms, what would “successful” look like and over what period of time? From 0–100%, how confident are you that you can convert exposure to income? If more than 0%, what evidence do you have to suggest that your strategy will work? Do you have a plan for becoming <em>un</em>famous if you don’t like it?</p>



<p>During my college years, one of my dorm mate’s dads was a famous Hollywood producer. He once said to me, “You want everyone to know your name and no one to know your face.”</p>



<p>Taking it a step further, we could quote Bill Murray: </p>



<blockquote>
<p>I always want to say to people who want to be rich and famous: ‘try being rich first.’ See if that doesn’t cover most of it. There’s not much downside to being rich, other than paying taxes and having your relatives ask you for money. But when you become famous, you end up with a 24-hour job. . . . The only good thing about fame is that I’ve gotten out of a couple of speeding tickets. I’ve gotten into a restaurant when I didn’t have a suit and tie on. That’s really about it.</p>
</blockquote>



<p>But how could this be true? It seems like a farce. At the very least, it must be an exaggeration, right?</p>



<p>To wrap your head around what “famous” really means, there is one metaphor that might help.</p>



<h3><strong>THE TRIBE, THE VILLAGE, THE CITY — THIS IS IMPORTANT</strong></h3>



<p>Here’s an email I received in July of 2007:</p>



<blockquote>
<p>[Your sport] shows that you are a hypocrite to profess helping others with your book. You are showing a grave example of the White horseman to our children. Shame on you. Shame on you… Shame. And Wickedness… It is the most evil war on earth, the one for blood spectacle for those who would entertain by whoring themselves prostituting violence to those who seek and lust to watch inhumanity. You are an evil one who has gained the world and lost your soul.</p>
</blockquote>



<p>What did I do or say that caused this? Was it in response to a how-to article on clubbing baby seals? </p>



<p>Not quite. It was in response to my blog post highlighting the non-profit <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.donorschoose.org/" target="_blank">DonorsChoose.org</a>, which I’ve advised for 10+ years. The explicit goal? To raise money for under-funded public school classrooms. In the introduction, I happened to mention that the founder and CEO of DonorsChoose was my wrestling partner in high school. That’s it.&nbsp;&nbsp;</p>



<p>This same “White horseman” reader proceeded to send me more than a dozen increasingly threatening emails, concluding with “I shall deliver you on judgment day.”</p>



<p>Was that a death threat? Was there anything I should do or could do about it? I’d never dealt with such things, and I didn’t know. But I did know one thing: it was very scary and completely out of the blue.</p>



<p>That week, I shared the above story with a female career blogger. She laughed and said soberly, “Welcome to the party.” She got an average of one death threat and one sex request/threat per week. At the time, our audiences were roughly the same size.</p>



<p>This brings me to the topic of audience size and the metaphor of the tribe, the village, and the city.</p>



<p>Think back to your 5th-grade class. In my case, there were 20–30 kids. Was there anyone totally off the rails in your class? For most of you, there’s a decent chance kids seemed pretty sane. It’s a small sample size.</p>



<p>Next, think back to your freshman year in high school. In my case, there were a few hundred kids. Was there anyone volatile or unbalanced? I can think of at least a handful who were prone to violence and made me uneasy. There were fights. Some kids brought knives to school. There was even a kid rumored to enjoy torturing animals. Keep in mind: this high school was in the same town as my elementary school. What changed? The sample size was larger.</p>



<p>Flash forward to my life in July of 2007, less than three months after the publication of my first book.&nbsp;</p>



<p>In that short span of time, my monthly blog audience had exploded from a small group of friends (20–30?) to the current size of Providence, Rhode Island (180,–200,000 people). Well, let’s dig into that. What do we know of Providence? Here’s one snippet from Wikipedia, and bolding is mine:</p>



<blockquote>
<p><strong>Compared to the national average, Providence has an average rate of violent crime and a higher rate of property crime per 100,000 inhabitants. In 2010, there were 15 murders, down from 24 in 2009. </strong>In 2010, Providence fared better regarding violent crime than most of its peer cities. Springfield, Massachusetts, has approximately 20,000 fewer residents than Providence but reported 15 murders in 2009, the same number of homicides as Providence but a slightly higher rate per capita.</p>
</blockquote>



<p><strong>The point is this: you don’t need to do anything <em>wrong</em></strong><strong> to get death threats, rape threats, etc. You just need a big enough audience. </strong>Think of yourself as the leader of a tribe or the mayor of a city.</p>



<p>The averages will dictate that you get a certain number of crazies, con artists, extortionists, possible (or actual) murderers, and so on. In fairness, we should also include a certain number of geniuses, a certain number of good Samaritans, and so on. Sure, your subject matter and content matters, but it doesn’t matter as much as you’d like to think.</p>



<p>To recap: the bigger the population, the more opportunities and problems you will have. A small, self-contained town in Idaho might not have a Pulitzer Prize winner among its residents, but it probably doesn’t need a SWAT team either.</p>



<p>Now, here we are in 2020.&nbsp;&nbsp;</p>



<p>My monthly audience is larger than the size of New York City (NYC).&nbsp;&nbsp;</p>



<p>For fun, Google “New York City” and click on “News.” On some level, those are the dynamics—good and bad—you will need to deal with if your audience is that large.</p>



<p>But let’s assume you only have 100 or 1,000 followers. You should still wonder: At any given time, how many of these people might go off of their meds? And how many of the remaining folks will simply wake up on the wrong side of the bed today, feeling the need to lash out at someone? The answer will never be zero.</p>



<h3><strong>ON THE DARKER SIDE, SOME VERY REAL ISSUES</strong></h3>



<div><p>To quote Henry David Thoreau, “The cost of a thing is the amount of what I will call life which is required to be exchanged for it, immediately or in the long run.” (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/Walden-Civil-Disobedience-Henry-Thoreau/dp/0451532163/?tag=offsitoftimfe-20" target="_blank"><em>Walden</em></a>) </p><p>With that in mind, let’s look at some very common downsides of exposure. Nearly all of my friends who have audiences of 1M or more have personal stories for every category I’ll describe. </p></div>



<div><p>If you’ve ever wondered why many celebrities disappear for a period of time, sometimes years, it’s often in the hopes that the below will fade or go away. Sadly, it’s very hard to put the toothpaste back in the toothpaste tube once you have a large Google footprint. </p><p>Best to be aware in advance. Here be dragons…</p></div>



<ul>
<li><strong>Stalkers. </strong>One example to set the tone: Back when I lived in SF, a fan on the East Coast thought I was sending him secret, personalized messages embedded in my public Facebook posts. He believed I was asking him to move into my house and work for me. He told his co-workers, who were worried he’d go postal, so they reported him to the CEO, who reached out to me. It was a close call, and I got lucky. This particular employee had already bought plane tickets for the following week, intending to fly to SF to find me. I got the FBI involved, his family staged an intervention, and, lo and behold, he had gone off of his meds for psychiatric disorders.  Another example from 2008, a year after my first book came out. That’s  when the first person showed up at my door looking for me. I’d just closed on my first home, a cute little townhouse near Sunnyvale, CA. The random visits didn’t happen sooner, as I’d been renting up to that point.<p> Many more people followed. My little townhouse was cute, but it was totally unprotected: no gate, no nothing. Eventually, one male stalker ended up hanging out in front of my house nonstop, taking pictures and posting them on social media with comments like “Too bad Tim Ferriss isn’t home. I missed him again!” Things snowballed from there, and I had to sell the house and move. When traveling, I’ve also had to stop posting photos to social until well after the fact. Why? I’ve had people triangulate the city I’m visiting, call <em>every</em> hotel in the city to ask for a registered guest with my last name, and then <em>fly</em> to the country to find me and/or my family. I’ve since learned to use pseudonyms, but we’ll get to that later…</p></li>
</ul>



<ul>
<li><strong>Death threats.&nbsp; </strong>I get regular death threats, and this is common for public figures. I would estimate I get at least one per month via some channel. Sometimes they’re related to extortion (coming later), but they’re most often from people who are mentally unstable. What are they angry about? Once again, therein lies the rub: it is rarely in response to anything that I’ve said or done. That is the scariest thing, and it’s also why the tribe-village-city metaphor is so apt. The people sending death threats are normally suffering from psychotic episodes, and there is nothing you can do to prevent them. <p>One example: A few years ago, I received a text message from an unknown number with “I know what you did. I’m going to make you pay.” I have no idea how they got my number, but it went on and on in nebulous terms. I engaged and took screenshots, trying to figure out who it was and what the hell was going on. Since they kept texting, I was able to gather that it was a woman (or someone claiming to be), and she said, “You humiliated me, and now it’s your turn for pain. I know you’re speaking at SXSW, and everyone is going to know and see.” Fortunately, I had enough data to get lawyers, private investigators, and law enforcement involved. It also meant that I had armed security at SXSW that year, and I was constantly on pins and needles, waiting for the other shoe to drop. So…. In the end, did I learn who it was? I did. It was a middle-aged mother living in rural Texas with her husband and two kids. I’D NEVER MET HER, NOR HAD ANY CONTACT WITH HER.</p><p>Just months before this happened, two well-known YouTubers in Austin, Texas, had a fan drive 11 hours from New Mexico to their house with a car full of guns. He intended to kill at least one of them. He broke into their home at 4am and hunted for them from room to room, .45-caliber handgun in hand. They hid in a closet and frantically called 911. From<a aria-label=" (opens in a new tab)" href="https://www.krqe.com/news/police-albuquerque-man-targeted-texas-youtube-celebrities/" target="_blank" rel="noreferrer noopener"> related media coverage</a>: “They’re a popular Texas couple on YouTube, but they never thought that would put their lives in danger. That is until an Albuquerque stalker showed up at their house in the middle of the night with a gun and bad intentions.” Fortunately, the police arrived, and the intruder ended up dead, but it could’ve easily ended differently. In some cases, the intended target gets blown away before they even realize what’s happening. Ironically, it’s often the diehard fans professing love who kill them, not “haters” of any type.</p><p>Given how often I get threats, and how truly dangerous it can be, I decided to get a concealed carry permit and carry concealed firearms. I wanted to avoid this, and I wish it weren’t the case, but here we are (P.S. Thanks for the <a aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Frangible_bullet" target="_blank" rel="noreferrer noopener">frangible round</a> recommendation, <a aria-label=" (opens in a new tab)" href="https://tim.blog/2015/09/25/jocko-willink/" target="_blank" rel="noreferrer noopener">Jocko</a>).</p><p>I also trained my girlfriend to use a Taser, which relates to the next category…</p></li>
</ul>



<ul>
<li><strong>Harassment of family members and loved ones.</strong> <p>There are at least two categories of people who will want to find you: fundamentally nice people (albeit overenthusiastic), and fundamentally malevolent people. I hate to put it that way, but I’ve learned that there are people in this world who derive great pleasure from hurting or threatening others.</p><p>If either group can’t easily get to you—whether to find you or harm you—they will often go after your family and loved ones. </p><p>If they’re an attacker, they will go for what they perceive to be your weakest link. This is precisely why I never mention the names of my closest friends or girlfriends, unless they are public figures already.</p><p>Of all the issues in this post, this one upsets me the most. In some respects, I invited this upon myself with my decisions, but none of my loved ones asked for it. Even to write about this aspect makes me furious, so I’ll keep this bullet short.</p></li>
</ul>



<ul>
<li><strong>Dating woes. </strong>As you might imagine, dating can be a quagmire of liabilities and bear traps. It could be someone hoping to write a clickbait article about their date with you (obviously without disclosing such), or it could be much worse. If you’re a female, this is where things can once again become physically dangerous. If you’re a male, this is where things can become legally dangerous. There are many predators for both sides, and it can make you lose your faith in humanity.</li>
</ul>



<ul>
<li><strong>Extortion attempts.</strong> <p>I could write an entire blog post about this topic. One simple example: In 2019, my team and I received a threat. In essence: “Pay me X now, or I will DDOS your site.” Since a DDOS is a technological attack on a website, and I’m confident in the strength of the <a aria-label=" (opens in a new tab)" href="https://automattic.com/" target="_blank" rel="noreferrer noopener">Automattic</a> hosting infrastructure, we decided not to respond. The extortionist didn’t like our silence and replied with a bomb threat. This was shortly after the<a aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Austin_serial_bombings" target="_blank" rel="noreferrer noopener"> Austin serial bombings</a>, which had killed two people, so I escalated to forensic analysis, investigators, law enforcement, etc. I’ve been very good at tracking down extortionists, I don’t negotiate with terrorists, and I’m more than happy to have public battles if I’m in the right, but… it’s all a huge energy suck. The most common form of extortion is some variety of “Unless you give me X, I’m going to say Y about you.” Fortunately, I’ve spent years deliberately talking about controversial topics and disclosing uncomfortable personal stories. In part, this has been to avoid the temptation to create a squeaky-clean public persona. It also robs would-be extortionists of a lot of common ammo.</p><p>If you don’t have <em>your own</em> ammo, this category can be catastrophic. In other words, if you have more fame than resources, you paint yourself into a vulnerable corner. If you have fewer options and fewer allies, you’ll be attractive to predators.</p></li>
</ul>



<ul>
<li><strong>Desperation messages and pleas for help. </strong>This is a sad category, much like the the suicide video story in the introduction to this piece. It’s one thing to get an “I committed suicide and I’m letting you know” note, which is absolutely awful. It’s quite another to get a message with something like: “You’re my last hope. I have no one else to ask. If you can’t help me with X, Y, and Z in the next 48 hours, I’m going to kill myself.” I have received dozens of these. In the beginning, I tried to help everyone and became horribly enmeshed. This never failed to end in misery and countless sleepless nights. Now, the senders of such notes are referred to suicide hotlines (e.g., 1 (800) 273-8255 in the US; <a aria-label=" (opens in a new tab)" href="http://www.suicide.org/international-suicide-hotlines.html" target="_blank" rel="noreferrer noopener">a list of international hotlines</a> [<a aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/List_of_suicide_crisis_lines" target="_blank" rel="noreferrer noopener">alternative link</a>]) and a post I wrote entitled “<a aria-label=" (opens in a new tab)" href="https://tim.blog/2015/05/06/how-to-commit-suicide/" target="_blank" rel="noreferrer noopener">Some Practical Thoughts on Suicide</a>.” I owe many thanks to <a target="_blank" href="https://en.wikipedia.org/wiki/Violet_Blue">Violet Blue</a> for her moral and tactical support with many of these situations. Thank you, Violet. This is very rough terrain. The more you operate in the world of how-to advice, and the more vulnerable you are with your audience, the more of these you will receive.</li>
</ul>



<ul>
<li><strong>Kidnapping.</strong> <p>If you appear semi-famous online, guess what? Even if you’re not rich, it can be assumed that you have enough money to make a nice ransom. There are places where kidnapping is an established industry, and professionals do this on a regular basis. The US is generally safe, but if you’re flying overseas, you should be aware of a few things.&nbsp;</p><p>For example, if you use a car service, give them a fake name (and nothing cute like “James Bond,” which will blow it) that they’ll use on the sign or iPad to find you at luggage claim. Here’s why: it’s common practice for organized crime to have an arrangement to buy flight manifests from airport employees. This means that the potential kidnappers, much like a Michelin three-star restaurant, will Google every name associated with every seat to figure out exactly who is who. If you appear to make an attractive target, they will then go to the airport an hour before you land, find the driver with your name on a sign, and pay or threaten them to leave. They then replace your driver with their own driver, who now holds the sign and waits for you. B’bye! This can take other forms too. Once in Central Asia, I had a driver show up at my hotel to take me to the airport, but… he used my real name, and I’d given the car service a fake name. To buy time, I asked him to wait while I made a few phone calls. About 10 minutes later, the real driver showed up to take me to the airport, using the designated pseudonym. The first fraudulent driver took off, and to this day, I have no idea how he knew where I was staying or when I was leaving. But it bears repeating: there are professionals who do this, and they will be very good at what they do.</p></li>
</ul>



<ul>
<li><strong>Impersonation, identity theft, etc. </strong><p>The more visible you are, the more people will attempt to impersonate you or your employees. This could be to hack a website, access a bank account, get a SSN, or otherwise. Companies or fly-by-night entrepreneurs will also use your name and face to sell everything from web services and e-books to shady info products and penis pills (sadly, all real examples). This is something that my lawyers deal with on a weekly basis. It’s non-stop. For both reputational and liability reasons, it’s important to track and guard against much of this.</p></li>
</ul>



<ul>
<li><strong>Attack and clickbait media.</strong>There are a lot of amazing writers and media professionals with rock-solid ethics. Many of my dear friends are journalists in this camp. On the flip side, there are increasingly large numbers of bad actors due to perverse incentives created by the click-baity, fast-is-the-new-good digital playing field. <p>Remember the tribe-village-city metaphor? Multiply your target audience size by two. Now recall the percentage of that audience that might be angry or off of their meds. Next, double that percentage to include those who will do gray-area things to advance their careers. Last, give <em>all</em> of those people a job—or contributor status—at a media outlet.</p><p>What a fucking mess.</p><p>If you don’t like shitty Twitter comments, or if nasty Facebook remarks get under your skin, just wait until you get your first hatchet job profile piece. It won’t be the last, so brush up on your Stoic philosophy.</p><p>This is particularly demoralizing when a piece is full of misquotes, even after you’ve corrected fact-checkers via phone (oops!). Pro tip: use email for fact-checking, my friends. </p><p>Speaking of friends…</p></li>
</ul>



<ul>
<li><strong>“Friends” with ulterior motives.</strong>Once you have a decent sized audience or “platform,” the majority of people who want to grab coffee, ask mutual friends for an intro, or—especially—offer you unsolicited favors will have ulterior motives. It took me a long time to accept this, and I paid a hefty tax for being Pollyannaish.<p>To be clear: I don’t mind pitches, as long as they come upfront. What I can’t stand is fakery to get in someone’s good graces over months, followed with a surprise of “Oh, I’ve been meaning to tell you about my new book coming out in a few weeks” and similar shenanigans. This has happened to me more times than I can count, and it feels dirty and gross.</p><p>This is one of the main reasons for my ongoing blanket policies, like <a aria-label=" (opens in a new tab)" href="https://tim.blog/2020/01/20/one-decision-that-removes-100-decisions/" target="_blank" rel="noreferrer noopener">a commitment to not reading any new books published in 2020</a>. It’s also one of the reasons that the majority of my closest friends are not in the public eye.</p><p>Be wary of anyone who just “wants to get to know you.” 99 times out of 100, that will be untrue.</p></li>
</ul>



<ul>
<li><strong>Invasions of privacy.</strong> <p>For all of the reasons in this post (and many more), if you’re doing anything public, you should <em>never </em>have anything mailed to where you live. If you <a aria-label=" (opens in a new tab)" href="https://twitter.com/nathanbarry/status/1223279622755799040" target="_blank" rel="noreferrer noopener">violate that even once</a>, it’s likely that your name and associated address will end up in company or government databases. Those mailing lists are then rented and traded as revenue streams, and it all ultimately ends up searchable. Remember the story of the Austin YouTubers hunted in their own home? Don’t be them.</p><p>For safety, unless you want to take huge risks, use a UPS Store or other off-site mailing address for everything. This is a must-have, not a nice-to-have.</p></li>
</ul>



<h3><strong>IN CONCLUSION</strong></h3>



<div><p>It’s been a wild ride. </p><p>Lest it appear otherwise, this is not intended to be a woe-is-me post. I’ve been very fortunate, and I love my life.</p></div>



<p>That said, all of the above have created heightened levels of anxiety that I didn’t anticipate. I’m lucky to have the support of my family and friends, my girlfriend, and<a href="https://www.instagram.com/p/B4vzyNTHtmx/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"> my guardian and fluffball, Molly</a>. I simply couldn’t handle it otherwise.</p>



<div><p>Would I have listened to all these warnings in advance? Would it have changed my behavior? I don’t know. Perhaps not. Unless you’ve lived it, it might seem like someone is being gifted a Bugatti and complaining about gas mileage.</p><p>The entire experience reminds me of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Blind_men_and_an_elephant" target="_blank">the parable of the blind men and the elephant</a>. This is a parable that has been told across different cultures since at least the 1st millennium BCE:<br><em><br>It is a story of a group of blind men, who have never come across an elephant before and who learn and conceptualize what the elephant is like by touching it. Each blind man feels a different part of the elephant</em>’<em>s body, but only one part, such as the side or the tusk. They then describe the elephant based on their limited experience, and their descriptions of the elephant are different from each other. In some versions, they come to suspect that the other person is dishonest, and they come to blows. The moral of the parable is that humans have a tendency to claim absolute truth based on their limited, subjective experience as they ignore other people</em>’<em>s limited, subjective experiences which may be equally true.</em></p></div>



<div><p>Before 2007, I was the blind men. </p><p>Here and there, I’d feel the ears (A celebrity in a cover story! Wow! Must be nice!), the tail (Fancy cars in a photo shoot!), or the tusk (Lifestyles of the Rich and Famous!).</p></div>



<p>Only now do I have some idea of what it’s like to be the elephant itself. No matter what part you grab beforehand, you can’t fully appreciate the scope of experience until you’re <em>in</em> it.</p>



<p><strong>If I’ve learned anything, it is this: fame will not fix your problems.&nbsp;&nbsp;</strong></p>



<div><p>Instead, fame is likely to magnify all of your insecurities and exaggerate all of your fears. It’s like picking up a fire extinguisher for your pain that ends up being a canister of gasoline.&nbsp;</p><p>If you think you have problems that fame will fix, I implore you to work on the inside first. At the very least, work on both in equal measure. I’ve found books like <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/Awareness-Opportunities-Reality-Anthony-Mello/dp/0385249373/?tag=offsitoftimfe-20" target="_blank"><em>Awareness</em></a> and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/Radical-Acceptance-Embracing-Heart-Buddha/dp/0553380990/?tag=offsitoftimfe-20" target="_blank"><em>Radical Acceptance</em></a> to be helpful.</p></div>



<p>If you don’t, you will end up with sand slipping through your fingers, leaving you with the same feelings of emptiness. Only now, along with disappointment, you will have the new challenges described in this post.</p>



<p>I also highly recommend reading Kevin Kelly’s essay entitled “<a rel="noreferrer noopener" aria-label="1,000 True Fans (opens in a new tab)" href="https://kk.org/thetechnium/1000-true-fans/" target="_blank">1,000 True Fans</a>.” Is it possible that being “famous” to the right 1,000 people could get you to your goals faster—and be healthier—than seeking the adoration and validation of millions? I tend to think so.</p>



<p>But then again…</p>



<p>Does that mean no one should pursue the path of Great Fame or tempt the sirens of the Great Public? I can’t say that. My intention is simply to shine light upon some of the hazards that such a journey entails.&nbsp; </p>



<p>Perhaps—just perhaps—you should give stardom a shot.</p>



<p>After all, as Jim Carrey has said: </p>



<p>“I think everybody should get rich and famous, and do everything they ever dreamed of, so they can see that it’s not the answer.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Structured outputs create false confidence (143 pts)]]></title>
            <link>https://boundaryml.com/blog/structured-outputs-create-false-confidence</link>
            <guid>46345333</guid>
            <pubDate>Sun, 21 Dec 2025 15:06:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boundaryml.com/blog/structured-outputs-create-false-confidence">https://boundaryml.com/blog/structured-outputs-create-false-confidence</a>, See on <a href="https://news.ycombinator.com/item?id=46345333">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Three ways to solve problems (132 pts)]]></title>
            <link>https://andreasfragner.com/writing/three-ways-to-solve-problems</link>
            <guid>46345125</guid>
            <pubDate>Sun, 21 Dec 2025 14:35:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andreasfragner.com/writing/three-ways-to-solve-problems">https://andreasfragner.com/writing/three-ways-to-solve-problems</a>, See on <a href="https://news.ycombinator.com/item?id=46345125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-50405b9eafa5af39c384">
  <p>One of my favorite definitions of a problem comes from the late Gerald Weinberg [1]</p><blockquote><p>A problem is the difference between things as perceived and desired.</p></blockquote><p>This definition is great because it’s actionable. It tells you that there are three ways to approach a problem:</p><ul data-rte-list="default"><li><p>Move the world towards the desired state</p></li><li><p>Change your perception of the current state</p></li><li><p>Change your desired state</p></li></ul><p>Points two and three seem like cop-outs at first — you basically avoid solving the problem. But they often turn out to be not just viable but optimal since they force you to re-frame and re-contextualize the problem.</p><p>Changing your desired state allows you to solve a different, possibly easier problem. For example, rather than solving the full problem (to get to the original desired state) you might find that a partial solution (the new desired state) gets you 80% there at 20% of the cost.</p><p>Changing your perception of the current state, you might realize it’s close enough to the desired state and so the problem doesn’t need to be dealt with at all right now. Deciding not to solve a problem can also be a solution.</p><p>As a strategy, deciding not to solve a problem or to solve a different version seems generally underutilized. I suspect this is largely because (a) we’re bad at understanding tradeoffs and quantifying opportunity costs, and (b) because it’s just hard to say no to people who feel strongly about reaching a desired state but might lack the full picture. For example, startups most often have messy finances and HR at the beginning. Not having good accounting or employment contracts is definitely a problem but one with lower weight than failing to ship product or hit growth targets. So deciding to minimally solve for it at first is the rational move. There will be pressure to do it to a high standard from the get-go — from investors or people on your team — but you have to resist it.</p><p>As a founder you’re constantly confronted with these kinds of tradeoffs and being good at mastering them is underrated I think. It takes discipline and clarity of mind to say no in the face of pressure. It’s especially hard when the pressure comes from yourself — when your own high, uncompromising standards get in the way. This is where I think repeat founders set themselves apart from first-time ones.</p><p>Similar situation as a product manager. All products have all sorts of problems all the time — missing features, broken edge cases, high-friction UI, bad UX, etc. But reaching the desired state (perfect product) is neither possible nor sensible. The key is to say no to solving 90% of the problems you have and to focus on the 10% that really matter.<br></p>
</div></div>]]></description>
        </item>
    </channel>
</rss>