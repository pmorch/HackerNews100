<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 22 Feb 2025 00:30:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[We the Builders (392 pts)]]></title>
            <link>https://www.wethebuilders.org/</link>
            <guid>43133648</guid>
            <pubDate>Fri, 21 Feb 2025 22:07:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wethebuilders.org/">https://www.wethebuilders.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43133648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="gridContainer"><h2>Blog</h2><p>Real stories from federal employees.</p><ul><li><a href="https://www.wethebuilders.org/posts/a-tale-of-two-effiencies">A Tale of Two Efficiencies: U.S. Digital Service vs. DOGE</a></li><li><a href="https://www.wethebuilders.org/posts/what-is-us-digital-service">What is the US Digital Service and Why Does it Matter?</a></li></ul><h2>Who We Are</h2><p>For decades, we've done our jobs in the background. We made it easier to file taxes, get veterans' benefits, and apply for financial aid. During times of crisis, we helped refugees navigate immigration processes, helped everyone find vaccines, and helped parents find baby formula.</p><p>Along the way, we made government websites easier to use while protecting the integrity of your personal information.</p><p>If they really wanted to know how to use technology to build a more efficient country, they would ask us.</p><p>But they haven't. They are destroyers.</p><p>We are the builders.</p><h2>Our mission</h2><p>We don't work for DOGE. We have always worked for you.</p><p>Here, you'll find stories from real government employees: How we save you time and money, how we protect your personal information, and how DOGE's dangerous dismantling of government technology puts you at risk.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[20 years working on the same software product (214 pts)]]></title>
            <link>https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</link>
            <guid>43133174</guid>
            <pubDate>Fri, 21 Feb 2025 21:22:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/">https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/</a>, See on <a href="https://news.ycombinator.com/item?id=43133174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>I released version 1 of my <a href="https://www.perfecttableplan.com/">table seating planning software</a>, PerfectTablePlan, in February 2005. 20 years ago this month. It was a different world. A world of Windows, shareware and CDs. A lot has changed since then, but PerfectTablePlan is now at version 7 and still going strong.</p>



<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png"><img data-attachment-id="12446" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/v1-screenshot-20th/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png" data-orig-size="700,438" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="v1-screenshot-20th" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=625" width="700" height="438" src="https://successfulsoftware.net/wp-content/uploads/2025/02/v1-screenshot-20th.png?w=700" alt=""></a></figure>



<p>PerfectTablePlan v1</p>



<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png"><img data-attachment-id="12447" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/v7-screenshot-20th/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png" data-orig-size="700,444" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="v7-screenshot-20th" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=625" width="700" height="444" src="https://successfulsoftware.net/wp-content/uploads/2025/02/v7-screenshot-20th.png?w=700" alt=""></a></figure>



<p>PerfectTablePlan v7</p>



<p>I have released several other products since then, and done some training and consulting, but PerfectTablePlan remains my most successful product. It’s success is due to a lot of hard work, and a certain amount of dumb luck.</p>



<p>I was getting married and I volunteered to do the seating plan for our wedding reception. It sounded like a relatively straightforward optimization problem, as we only had 60 guests and no family feuds to worry about. But it was surprisingly difficult to get right. I looked around for some software to help me. There were a couple of software packages, but I wasn’t impressed. I could do better myself! So I wrote a (very rough) first version, which I used for our wedding.</p>



<p>Things weren’t going great at my day job, at a small software startup. Maybe I could commercialize my table planner? I was a bit wary, as my potential competitors all seemed rather moribund and I didn’t think I would be able to make a living off it. But I thought I could do everything worth doing in 6-12 months and then start on the next product. Wrong on both counts!</p>



<p>Web-based software was still in its infancy in 2005. So I decided to write it as desktop software using C++ and cross-platform framework Qt, which I had plenty of experience in. Initially, I just released a Windows version. But I later added a Mac version as well. Qt has had its commercial ups and downs in the last 20 years, but it has grown with me and is now very robust, comprehensive and well documented. I think I made a good choice.</p>



<p>I financed PerfectTablePlan out of my own savings and it has been profitable every year since version 1 was launched. I could have taken on employees and grown the business, but I preferred to keep it as a <a href="https://successfulsoftware.net/2013/11/06/lifestyle-programming/">lifestyle business</a>. My wife does the accounts and proof reading and I do nearly everything else, with a bit of help from my accountant, web designers and a few other contractors. I don’t regret that decision. 20 years without meetings, ties or alarm clocks. My son was born 18 months after PerfectTablePlan was launched and it has been great to have the flexibility to be fully present as a Dad.</p>



<p>CDs, remember them? I sent out around 5,000 CDs (with some help from my father), before I stopped shipping CDs in 2016.</p>


<div>
<figure><a href="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png"><img data-attachment-id="12477" data-permalink="https://successfulsoftware.net/2025/02/21/20-years-working-on-the-same-software-product/l-shadow-only-hq-2/" data-orig-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="L-Shadow-Only-HQ" data-image-description="" data-image-caption="" data-medium-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=300" data-large-file="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=500" width="500" height="500" src="https://successfulsoftware.net/wp-content/uploads/2025/02/l-shadow-only-hq.png?w=500" alt=""></a></figure></div>


<p>During the lifetime of PerfectTablePlan it became clear that things were increasingly moving to the web. But I couldn’t face rewriting PerfectTablePlan from scratch for the web. Javascript. Ugh. Also PerfectTablePlan is quite compute intensive, using a genetic algorithm to generate an automated seating plan and I felt it was better running this on the customer’s local computers than my server. And some of my customers consider their seating plans to be confidential and don’t want to store them on third party servers. So I decided to stick with desktop. But, if I was starting PerfectTablePlan from scratch now, I might make a different decision.</p>



<p>Plenty of strange and wonderful things have happened over the last 20 years, including:</p>



<ul>
<li>PerfectTablePlan has been used by some very famous organizations for some very famous events (which we mostly don’t have permission to mention). It has seated royalty, celebrities and heads of state.</li>



<li>PerfectTablePlan was used as part of a <a href="https://www.perfecttableplan.com/newsletters/newsletter10_web.html">demonstration of the (controversial) first commercial quantum computer by D-Wave</a>.</li>



<li>A mock-up of PerfectTablePlan, including icons I did myself, was <a href="https://www.perfecttableplan.com/newsletters/newsletter_8.html">used without our permission</a> by Sony in their ‘Big day’ TV comedy series. I threated them with legal action. Years later, I am still awaiting a reply.</li>



<li>I got to grapple with some interesting problems, including the mathematics of <a href="https://www.perfecttableplan.com/html/genetic_algorithm.html">large combinatorial problems</a> and <a href="https://successfulsoftware.net/2008/07/18/a-mathematical-digression/">elliptical tables</a>. Some customers have seated 4,000 guests and 4000! (4000x3999x3998 .. x 1) is a mind-bogglingly huge number.</li>



<li>A well known wedding magazine ran a promotion with a valid licence key clearly visible in a photograph of a PerfectTablePlan CD. I worked through the night to release a new version of PerfectTablePlan that didn’t work with this key.</li>



<li>I found out that <a href="https://www.perfecttableplan.com/html/the_dog_ate_my_cd.html">CDs are edible</a>.</li>



<li>I sponsored the <a href="https://thejunipertrust.org/jt_projects/bampti-kindergaten-school/">building of a kindergarten in Nepal</a>.</li>



<li>I once had to stay up late, in a state of some inebriation, to fix an issue so that a world famous event wasn’t a disaster (no I can’t tell you the event).</li>
</ul>



<p>The lowest point was the pandemic, when sales pretty much dropped to zero.</p>



<p>Competitors and operating systems have come and gone and the ecosystem for software has changed a lot, but PerfectTablePlan is still here and still paying the bills. It is about 145,000 lines of C++. Some of the code is a bit ugly and not how I would write it now. But the product is very solid, with very few bugs. The website and user documentation are also substantial pieces of work. The PDF version of the documentation is nearly 500 pages.</p>



<p>I now divide my time between PerfectTablePlan and my 2 other products: <a href="https://www.easydatatransform.com/">data wrangling software</a> Easy Data Transform and <a href="https://www.hyperplan.com/">visual planner</a> Hyper Plan. Having multiple products keeps things varied and avoids having all my eggs in one basket. In May 2024 I released PerfectTablePlan v7 with a load of improvements and new features. And I have plenty of ideas for future improvements. I fully expect to keep working on PerfectTablePlan until I retire (I’m 59 now).</p>




					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suckless.org: software that sucks less (152 pts)]]></title>
            <link>https://suckless.org/</link>
            <guid>43131059</guid>
            <pubDate>Fri, 21 Feb 2025 18:27:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suckless.org/">https://suckless.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43131059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

<p>Home of <a href="https://dwm.suckless.org/">dwm</a>, <a href="https://tools.suckless.org/dmenu">dmenu</a> and
other quality software with a focus on simplicity, clarity, and frugality.</p>
<p>Read more about our <a href="https://suckless.org/philosophy">philosophy</a> and join us on the <a href="https://suckless.org/community">mailing
list</a>.</p>
<h2>News</h2>
<p><a href="https://suckless.org/atom.xml">Atom feed</a></p>
<h2>2024-11-26</h2>
<ul>
<li><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.2.tar.gz">download</a></li>
</ul>
<h2>2024-04-05</h2>
<ul>
<li><a href="https://st.suckless.org/">st 0.9.2</a> released: <a href="https://dl.suckless.org/st/st-0.9.2.tar.gz">download</a></li>
</ul>
<p>This reverts a commit and a regression with cursor move with wide glyphs, for
example with GNU readline.</p>
<h2>2024-03-20</h2>
<p>Below are some highlights of the changes for the recent releases of dmenu, dwm,
st and tabbed, see the git logs for all details:</p>
<p>General small Makefile improvements, rationale being: just be verbose and show
what is done: do not abstract/hide details from the user/developer.
Respect (more) the package manager and build system flags (CFLAGS, LDFLAGS, etc).</p>
<p><a href="https://git.suckless.org/dwm/log.html">dwm</a>:
</p><ul>
<li>Improvements to signal handling.</li>
<li>Fix: Avoid missing events when a keysym maps to multiple keycodes.</li>
</ul>

<p><a href="https://git.suckless.org/dmenu/log.html">dmenu</a>:
</p><ul>
<li>Reduce memory usage for reading the lines.</li>
<li>Fix: X11 BadMatch error when embedding on some windows.</li>
</ul>

<p><a href="https://git.suckless.org/st/log.html">st</a>:
</p><ul>
<li>Fix: bounds checks of dc.col.</li>
<li>Fix: buffer overflow when handling long composed input.</li>
<li>Ignore C1 control characters in UTF-8 mode.</li>
<li>Improvements to cell handling and wide characters.</li>
<li>Default config: decrease the default minlatency.</li>
<li><a href="https://git.suckless.org/st/log.html">Various other terminal fixes and compatibility improvements.</a></li>
</ul>

<p><a href="https://git.suckless.org/tabbed/log.html">tabbed</a>:
</p><ul>
<li>Fix: faulty zombie process reaping.</li>
<li>Improvements to signal handling.</li>
<li>Improve compatibility with compiling on older systems such as Slackware 11.</li>
</ul>

<p>Thanks to all contributors who submitted patches.</p>
<h2>2024-03-19</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 5.3</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.3.tar.gz">download</a></li>
<li><a href="https://dwm.suckless.org/">dwm 6.5</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.5.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.9.1</a> released: <a href="https://dl.suckless.org/st/st-0.9.1.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/tabbed">tabbed 0.8</a> released: <a href="https://dl.suckless.org/tools/tabbed-0.8.tar.gz">download</a></li>
</ul>
<h2>2023-07-04</h2>
<p><a href="https://tools.suckless.org/slstatus">slstatus 1.0</a> released: <a href="https://dl.suckless.org/tools/slstatus-1.0.tar.gz">download</a></p>
<h2>2022-12-28</h2>
<p><a href="https://tools.suckless.org/lchat">lchat 1.0</a> released: <a href="https://dl.suckless.org/tools/lchat-1.0.tar.gz">download</a></p>
<h2>2022-11-02</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.2</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.2.tar.gz">download</a></p>
<h2>2022-10-08</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.1</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.1.tar.gz">download</a></p>
<h2>2022-10-06</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 2.0.0</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-2.0.0.tar.gz">download</a></p>
<h2>2022-10-04</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 5.2</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.2.tar.gz">download</a></li>
<li><a href="https://dwm.suckless.org/">dwm 6.4</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.4.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/ii">ii 2.0</a> released: <a href="https://dl.suckless.org/tools/ii-2.0.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/sic">sic 1.3</a> released: <a href="https://dl.suckless.org/tools/sic-1.3.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/slock">slock 1.5</a> released: <a href="https://dl.suckless.org/tools/slock-1.5.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.9</a> released: <a href="https://dl.suckless.org/st/st-0.9.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/tabbed">tabbed 0.7</a> released: <a href="https://dl.suckless.org/tools/tabbed-0.7.tar.gz">download</a></li>
</ul>
<h2>2022-04-19</h2>
<p>Suckless now has a dark mode CSS style for its pages.
Surf also now has support for <a href="https://git.suckless.org/surf/commit/1f5b8f3bd1f37d4d3dc45d21285f34ef4752dbaa.html">dark mode</a>.</p>
<h2>2022-02-11</h2>
<p><a href="https://tools.suckless.org/dmenu/">dmenu 5.1</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.1.tar.gz">download</a></p>
<h2>2022-01-07</h2>
<ul>
<li><a href="https://dwm.suckless.org/">dwm 6.3</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.3.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/ii">ii 1.9</a> released: <a href="https://dl.suckless.org/tools/ii-1.9.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.8.5</a> released: <a href="https://dl.suckless.org/st/st-0.8.5.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4.1</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.1.tar.gz">download</a></li>
</ul>
<h2>2021-12-22</h2>
<p><a href="https://libs.suckless.org/libgrapheme">libgrapheme 1.0.0</a> released: <a href="https://dl.suckless.org/libgrapheme/libgrapheme-1.0.0.tar.gz">download</a></p>
<h2>2021-07-30</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.4</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.4.tar.gz">download</a></p>
<h2>2021-05-09</h2>
<p>On Tuesday, 2021-05-11 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 1 hour from about 21:00 to
22:00 UTC+02:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2021-05-12 23:33 UTC+02:00.
P.S.: It didn't actually take 26h30, I just had forgotten to do it.</p>
<h2>2021-05-08</h2>
<p><a href="https://surf.suckless.org/">surf 2.1</a> released: <a href="https://dl.suckless.org/surf/surf-2.1.tar.gz">download</a></p>
<h2>2021-03-28</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.3</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.3.tar.gz">download</a></p>
<h2>2021-03-28</h2>
<p>On Wednesday, 2021-03-31 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+02:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2021-03-31 19:10 UTC+02:00.</p>
<h2>2021-01-19</h2>
<p><a href="https://tools.suckless.org/scroll/">scroll 0.1</a> released: <a href="https://dl.suckless.org/tools/scroll-0.1.tar.gz">download</a></p>
<h2>2020-12-11</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.2.tar.gz">download</a></p>
<h2>2020-09-18</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2.1</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.1.tar.gz">download</a></p>
<h2>2020-09-13</h2>
<p><a href="https://tools.suckless.org/x/svkbd/">svkbd 0.2</a> released: <a href="https://dl.suckless.org/tools/svkbd-0.2.tar.gz">download</a></p>
<h2>2020-09-02</h2>
<p><a href="https://tools.suckless.org/dmenu/">dmenu 5.0</a> released: <a href="https://dl.suckless.org/tools/dmenu-5.0.tar.gz">download</a></p>
<h2>2020-06-19</h2>
<p><a href="https://st.suckless.org/">st 0.8.4</a> released: <a href="https://dl.suckless.org/st/st-0.8.4.tar.gz">download</a></p>
<h2>2020-05-27</h2>
<p>The <a href="https://suckless.org/conferences/2020">slcon7</a> has been cancelled due to the 2019-nCoV
pandemic.</p>
<h2>2020-04-27</h2>
<p><a href="https://st.suckless.org/">st 0.8.3</a> released: <a href="https://dl.suckless.org/st/st-0.8.3.tar.gz">download</a></p>
<h2>2019-12-01</h2>
<p>On Wednesday, 2019-12-04 there will be scheduled maintenance of the suckless
servers. It's estimated this will take about 2-3 hours from about 19:00 to
21:00 - 22:00 UTC+01:00.</p>
<p>The mailinglist, website and source-code repositories will have some downtime.</p>
<p><strong>Update:</strong> the maintenance was finished at 2019-12-04 20:00 UTC+01:00.</p>
<h2>2019-04-04</h2>
<p>Registrations are now open for <a href="https://suckless.org/conferences/2019">slcon6</a> that will be held in
Bad Liebenzell, Germany on 2019-10-(04-06).</p>
<p>The CfP for interested participants will end on 2019-06-30.</p>
<h2>2019-03-30</h2>
<p>There is now a <a href="https://gunther.suckless.org/patches/">patch overview</a> tool to have a
quick overview of the patch status list. This list is generated each day from
the <a href="https://git.suckless.org/sites/">sites</a> repository. It checks if patches apply
cleanly in a normal patching manner. Of course it does not check patch
combinations.</p>
<ul>
<li><a href="https://suckless.org/hacking/">Hacking patches guidelines</a></li>
<li><a href="https://git.suckless.org/sites/file/testpatches.sh.html">Tool source-code</a></li>
</ul>
<p>Please keep the patches tidy and maintain or remove them.</p>
<h2>2019-02-09</h2>
<p><a href="https://st.suckless.org/">st 0.8.2</a> released: <a href="https://dl.suckless.org/st/st-0.8.2.tar.gz">download</a></p>
<p>This release has mostly bugfixes.</p>
<h2>2019-02-03</h2>
<ul>
<li><a href="https://dwm.suckless.org/">dwm 6.2</a> released: <a href="https://dl.suckless.org/dwm/dwm-6.2.tar.gz">download</a></li>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 4.9</a> released: <a href="https://dl.suckless.org/tools/dmenu-4.9.tar.gz">download</a></li>
</ul>
<h2>2018-06-01</h2>
<p>The maintainance is completed. Let me know of any important things that are broken.
Internally we will keep tweaking the server configuration over the course of
time.</p>
<h2>2018-05-27</h2>
<p>There will be a scheduled server maintenance next Friday and Saturday, 2018-06-(01-02).
The migration to the new server will happen on these days and the git
repositories and mailing list will be frozen on the old (now current)
server.</p>
<h2>2018-04-11</h2>
<p><a href="https://tools.suckless.org/farbfeld/">farbfeld 4</a> released: <a href="https://dl.suckless.org/farbfeld/farbfeld-4.tar.gz">download</a></p>
<h2>2018-03-20</h2>
<p><a href="https://st.suckless.org/">st 0.8.1</a> released: <a href="https://dl.suckless.org/st/st-0.8.1.tar.gz">download</a></p>
<p>This release fixes some regressions introduced in the 0.8 release.</p>
<h2>2018-03-19</h2>
<p>Registrations for <a href="https://suckless.org/conferences/2018/">slcon5</a> are now open.</p>
<h2>2018-03-14</h2>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu 4.8</a> released: <a href="https://dl.suckless.org/tools/dmenu-4.8.tar.gz">download</a></li>
<li><a href="https://st.suckless.org/">st 0.8</a> released: <a href="https://dl.suckless.org/st/st-0.8.tar.gz">download</a></li>
</ul>
<h2>2018-02-04</h2>
<p><a href="https://tools.suckless.org/ii">ii 1.8</a> released: <a href="https://dl.suckless.org/tools/ii-1.8.tar.gz">download</a></p>
<h2>2017-09-04</h2>
<p><a href="https://suckless.org/conferences/2017">suckless hackathon</a>: we met on Sep 1-3 2017 in Würzburg, Germany.</p>
<h2>2017-09-04</h2>
<p><a href="https://tools.suckless.org/sent">sent 1</a> released: <a href="https://dl.suckless.org/tools/sent-1.tar.gz">download</a></p>
<h2>2017-08-30</h2>
<p>suckless.org now supports TLS using <a href="https://letsencrypt.org/">Let's Encrypt</a>.
Cloning git repos over HTTPS now works. Some links on the page have been
changed to allow both HTTP and HTTPS.</p>
<p>HSTS is not fully working yet. This will be fixed.</p>
<p>The IPv6 AAAA record was added and IPv6 is fully working now.</p>
<p>suckless has many subdomains, these should hopefully all work via TLS. If you
see a subdomain without a signed certificate please report it. If you find any
broken links on the wiki pages, these can be fixed by anyone.</p>
<h2>2017-07-03</h2>
<p>The suckless.org project is now hosted on a new server. All inactive accounts
have been removed during the relocation.</p>
<p>Please note that the new ECDSA key fingerprint is
SHA256:7DBXcYScmsxbv7rMJUJoJsY5peOrngD4QagiXX6MiQU.</p>
<h2>2017-05-06</h2>
<p><a href="https://tools.suckless.org/blind">blind 1.1</a> released:
<a href="https://dl.suckless.org/tools/blind-1.1.tar.gz">download</a></p>
<h2>2017-05-02</h2>
<p><a href="https://tools.suckless.org/dmenu">dmenu 4.7</a> released:
<a href="https://dl.suckless.org/tools/dmenu-4.7.tar.gz">download</a></p>
<h2>2017-04-14</h2>
<p><a href="https://tools.suckless.org/farbfeld/">farbfeld 3</a> released:
<a href="https://dl.suckless.org/farbfeld/farbfeld-3.tar.gz">download</a></p>
<h2>2017-03-28</h2>
<p><a href="https://surf.suckless.org/">surf</a> now uses webkit2 by default. The webkit1 version
is kept in the <a href="https://git.suckless.org/surf/log/?h=surf-webkit1">surf-webkit1</a>
branch. The “master” branch doesn't exist anymore, HEAD is now
<a href="https://git.suckless.org/surf/log/">surf-webkit2</a>, so be sure to rebase your local
master commits onto surf-webkit1.</p>
<h2>2016-11-20</h2>
<p><a href="https://tools.suckless.org/slock">slock 1.4</a> released:
<a href="https://dl.suckless.org/tools/slock-1.4.tar.gz">download</a></p>
<h2>2016-09-26</h2>
<p>Videos of the <a href="https://suckless.org/conferences/2016">slcon 2016 talks</a> are now available.</p>
<h2>2016-08-24</h2>
<p><a href="https://suckless.org/conferences/2016">slcon3</a> preliminary schedule now published. If you want to
attend please register before: <strong>2016-09-01</strong>.</p>
<h2>2015-12-19</h2>
<p><a href="https://surf.suckless.org/">surf 0.7</a> released:
<a href="https://dl.suckless.org/surf/surf-0.7.tar.gz">download</a></p>
<h2>2015-11-25</h2>
<p><a href="https://tools.suckless.org/sent">sent 0.2</a> released:
<a href="https://dl.suckless.org/tools/sent-0.2.tar.gz">download</a></p>
<h2>2015-11-13</h2>
<p>Videos of the <a href="https://suckless.org/conferences/2015">slcon2 talks</a> are now available.</p>
<h2>2015-11-09</h2>
<p><a href="https://dwm.suckless.org/">dwm 6.1</a> released:
<a href="https://dl.suckless.org/dwm/dwm-6.1.tar.gz">download</a></p>
<h2>2015-09-23</h2>
<p>Kai and Anselm gave an interview about suckless.org on Randal Schwartz's <a href="https://twit.tv/shows/floss-weekly/episodes/355?autostart=false">FLOSS
Weekly show</a></p>
<h2>2015-07-07</h2>
<p><a href="https://st.suckless.org/">st 0.6</a> released:
<a href="https://dl.suckless.org/st/st-0.6.tar.gz">download</a></p>
<h2>2015-02-14</h2>
<p><a href="https://suckless.org/conferences/2015">slcon2</a> will be held in Budapest on 2015-10-(30-31).</p>
<p>The CfP for interested participants is now open and will end on 2015-04-30.</p>
<h2>2014-11-29</h2>
<p><a href="https://tools.suckless.org/x/lsw">lsw 0.3</a> released:
<a href="https://dl.suckless.org/tools/lsw-0.3.tar.gz">download</a></p>
<h2>2014-11-24</h2>
<p>There will be a
<a href="https://events.ccc.de/congress/2014/wiki/Assembly%3ASuckless">suckless assembly</a>
at the <a href="https://events.ccc.de/congress/2014">31C3</a>. The whole suckless
community is invited to come, meet and hack!</p>
<h2>2014-08-05</h2>
<p><a href="https://core.suckless.org/sinit">sinit 0.9.1</a> released:
<a href="https://dl.suckless.org/sinit/sinit-0.9.1.tar.gz">download</a></p>
<h2>2014-05-01</h2>
<p><a href="https://core.suckless.org/ubase">ubase 0.1</a> released:
<a href="https://dl.suckless.org/ubase/ubase-0.1.tar.gz">download</a></p>
<h2>2014-01-21</h2>
<p><a href="https://tools.suckless.org/tabbed">tabbed 0.6</a> released:
<a href="https://dl.suckless.org/tools/tabbed-0.6.tar.gz">download</a></p>
<h2>2013-06-16</h2>
<p><a href="https://tools.suckless.org/sic">sic 1.2</a> released:
<a href="https://dl.suckless.org/tools/sic-1.2.tar.gz">download</a></p>
<h2>2013-05-07</h2>
<p><a href="https://tools.suckless.org/x/xssstate">xssstate 1.1</a> released:
<a href="https://dl.suckless.org/tools/xssstate-1.1.tar.gz">download</a></p>
<h2>2013-05-06</h2>
<p><a href="https://tools.suckless.org/tabbed">tabbed 0.5</a> released:
<a href="https://dl.suckless.org/tools/tabbed-0.5.tar.gz">download</a></p>
<h2>2013-04-21</h2>
<p>We are glad to announce the <a href="https://suckless.org/conferences/2013">slcon 2013</a> programme.</p>
<h2>2012-11-29</h2>
<p>We are glad to announce the switch to git from mercurial in all of our
repositories. You can find them at <a href="https://git.suckless.org/">git.suckless.org</a> Many
thanks to 20h for his contribution!</p>
<h2>2012-10-28</h2>
<p><a href="https://tools.suckless.org/x/sprop">sprop 0.1</a> released:
<a href="https://dl.suckless.org/tools/sprop-0.1.tar.gz">download</a></p>
<h2>2012-10-14</h2>
<p>Today we heard a very sad news that our friend, contributor and philosophical
advisor Uriel has passed away peacefully. We will miss him a lot.</p>
<p><img src="https://suckless.org/uriel.png" alt="uriel"></p>
<p>RIP</p>
<h2>2011-05-14</h2>
<p>Anselm gave a talk about <strong>The 'suckless.org' universe</strong> at the <a href="http://www.linuxtag.org/">LinuxTag
2011</a> conference in Berlin.</p>
<h2>2011-01-31</h2>
<p><a href="https://tools.suckless.org/ii">ii 1.6</a> released (regression fix):
<a href="https://dl.suckless.org/tools/ii-1.6.tar.gz">download</a></p>
<h2>2010-06-04</h2>
<p><a href="https://tools.suckless.org/9base">9base-6</a> released:
<a href="https://dl.suckless.org/tools/9base-6.tar.gz">download</a></p>
<h2>2010-03-28</h2>
<p>We learned today that the previous wmii maintainer, who wasn't actively
involved since 2007, Denis Grelich,
<a href="https://web.archive.org/web/20140208043925/http://www.lmt.uni-saarland.de/de/aktuelles/grelich.html">died on 2010-03-12</a>.
We thank him for his work. Rest in peace.</p>
<h2>2010-03-07</h2>
<p>We applied as a mentoring organisation for GSoC 2010. See our <a href="https://suckless.org/project_ideas">project ideas
for GSoC 2010</a> page for further details.</p>
<h2>2010-02-13</h2>
<p>Some of us will visit <a href="http://chemnitzer.linux-tage.de/2010/">CLT2010</a>. Anselm
will give a
<a href="http://chemnitzer.linux-tage.de/2010/vortraege/detail.html?idx=308">talk</a>
about stali on the second day of CLT2010 at 17:00.</p>
<h2>2009-12-28</h2>
<p>There was a small community meeting in Berlin! Thanks to all attendees.</p>
<h2>2008-08-02</h2>
<p><a href="https://tools.suckless.org/x/wmname">wmname 0.1</a> released:
<a href="https://dl.suckless.org/tools/wmname-0.1.tar.gz">download</a></p>
<h2>2008-07-29</h2>
<p><a href="https://tools.suckless.org/x/sselp">sselp 0.2</a> released:
<a href="https://dl.suckless.org/tools/sselp-0.2.tar.gz">download</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's James Webb Space Telescope faces potential 20% budget cut (132 pts)]]></title>
            <link>https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</link>
            <guid>43131045</guid>
            <pubDate>Fri, 21 Feb 2025 18:25:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts">https://www.space.com/space-exploration/james-webb-space-telescope/nasa-james-webb-space-telescope-faces-20-percent-budget-cuts</a>, See on <a href="https://news.ycombinator.com/item?id=43131045">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg" alt="a spacecraft with a large gold hexagon on top in deep space" srcset="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qpAkShLp2kqGCkfGqGBMuf.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: dima_zel/iStock/Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>The scientists behind NASA's largest and most powerful space telescope ever built are bracing for potentially crippling budget cuts, and the observatory is only halfway through its primary mission.</p><p>The team overseeing NASA's <a data-analytics-id="inline-link" href="https://www.space.com/21925-james-webb-space-telescope-jwst.html" data-before-rewrite-localise="https://www.space.com/21925-james-webb-space-telescope-jwst.html"><u>James Webb Space Telescope</u></a> (JWST) has been directed to prepare for up to 20% in budget cuts that would touch on every aspect of the flagship observatory's operations, which are managed by the Space Telescope Science Institute (STScI) in Maryland. The potential cut comes even as the space observatory is more in demand than ever before, with astronomers requesting the equivalent of nine years' worth of Webb observing time in one operational year.</p><p>"NASA is having budget constraints across the entire board, so the institute is being asked to consider a significant — about 20% — cut to our operational budget for the mission starting later this year," Tom Brown, who leads the Webb mission office at STScI, told a crowd of scientists last month at the 245th American Astronomical Society (AAS) meeting in National Harbor, Maryland. "So the impacts of that, if it comes to pass, pretty much cut across the entire mission."</p><p>NASA's <a data-analytics-id="inline-link" href="https://www.space.com/nasa-white-house-2025-budget-request" data-before-rewrite-localise="https://www.space.com/nasa-white-house-2025-budget-request"><u>$25.4 billion budget request for 2025</u></a> set aside $317 million to fund the Webb space telescope, as well as the <a data-analytics-id="inline-link" href="https://www.space.com/15892-hubble-space-telescope.html" data-before-rewrite-localise="https://www.space.com/15892-hubble-space-telescope.html"><u>Hubble Space Telescope</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/18669-chandra-x-ray-observatory.html" data-before-rewrite-localise="https://www.space.com/18669-chandra-x-ray-observatory.html"><u>Chandra X-ray Observatory</u></a> that together comprise NASA's currently operational "Great Observatories." The Hubble Telescope program is facing a potential 20% budget cut of its own, <a data-analytics-id="inline-link" href="https://spacenews.com/hubble-budget-cuts-could-impact-science-and-mission-operations/"><u>according to SpaceNews</u></a>. And Chandra <a data-analytics-id="inline-link" href="https://www.space.com/chandra-x-ray-observatory-nasa-fy2025-budget" data-before-rewrite-localise="https://www.space.com/chandra-x-ray-observatory-nasa-fy2025-budget"><u>is facing the end of its mission</u></a>, with NASA's 2025 budget request including plans to wind down operations, with its budget dropping from $41.1 million this year to just $5.2 million in 2029.</p><p>But unlike Hubble, which turns 35 this spring, and Chandra, which launched in 1999, Webb is in its prime, approaching the midpoint of a primary 10-year mission. It could last at least 20 years or more, NASA officials have said. The mission is an international partnership between NASA, the European Space Agency and the Canadian Space Agency.</p><p>"Frankly, this mission works far better than, really, most folks expected it to, you know," Brown said during the Webb town hall event on Jan. 15 at the AAS conference. "It's extremely worrisome that, while we're in the middle of the prime mission, we're also maybe looking at significant budget cuts."</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png" alt="The galaxy GN-z11 as seen by Hubble (inset) an illustration of a feeding black hole" srcset="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/uQQra7dvnHyLzW64uprJSg.png"></picture></p></div><figcaption itemprop="caption description"><span>The James Webb Space Telescope has made mind-boggling discoveriesin its first four years, like this one of the galaxy GN-z11 with the oldest and farthest black hole ever seen. </span><span itemprop="copyrightHolder">(Image credit: NMASA, ESA, P. Oesch (Yale University), G. Brammer (STScI), P. van Dokkum (Yale University), and G. Illingworth (University of California, Santa Cruz) (Inset) Robert Lea)</span></figcaption></figure><p>The $10 billion Webb space telescope survived a tumultuous development process, one that included cost overruns and technical delays that nearly killed the observatory before it ever flew. Lawmakers with the House Appropriations Committee <a data-analytics-id="inline-link" href="https://www.space.com/12187-nasa-budget-bill-cancels-space-telescope-house.html" data-before-rewrite-localise="https://www.space.com/12187-nasa-budget-bill-cancels-space-telescope-house.html"><u>proposed cancelling the mission</u></a> in 2011, a decade before <a data-analytics-id="inline-link" href="https://www.space.com/nasa-james-webb-space-telescope-launch-success" data-before-rewrite-localise="https://www.space.com/nasa-james-webb-space-telescope-launch-success"><u>Webb's Christmas Day launch in 2021</u></a>, only to back down after backlash from scientists and influential politicians defending the observatory.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-rTLnXKeQtgwCKwvYQVHpC6"><section><p>Breaking space news, the latest updates on rocket launches, skywatching events and more!</p></section></div><p>Since its 2021 launch, the Webb space telescope <a data-analytics-id="inline-link" href="https://www.space.com/james-webb-space-telescope-first-year-astronomers-in-tears" data-before-rewrite-localise="https://www.space.com/james-webb-space-telescope-first-year-astronomers-in-tears"><u>has outmatched even the most optimistic predictions</u></a> for its performance. Its infrared optics have looked deep into the universe's past, observed distant galaxies and exoplanets, and even peered at our own local solar system planets closer to home.</p><p>"In a nutshell, it is truly fulfilling its promise," Macarena Garcia Marin, STScI's Webb project scientist, said during the same town hall event. "Across every field, JWST is truly delivering cutting-edge science."</p><p>Some of Webb's budget challenges stem from its operational costs, which were set "idealistically low" in 2011 when the observatory was saved from cancellation. Those costs, coupled with inflation rates that were much higher than expected and less flexibility in NASA's budget, have also contributed, Brown said.</p><p>According to a <a data-analytics-id="inline-link" href="https://www.stsci.edu/files/live/sites/www/files/home/jwst/news-events/events/2025/_documents/0125-jwst-townhall-mission-status-brown.pdf"><u>presentation by Brown</u></a>, a 20% cut to Webb's operational budget would definitely affect how much science the telescope could perform. The impacts would be felt across teams that review proposals for observing targets, data analysis, observatory efficiencies, and anomaly resolution when something goes wrong, not to mention the need to engage with the scientific community and public on Webb's science results.</p><p>"It's a huge cut. That's not like kind of trying to nibble away at the edges," Brown told Space.com. "That impacts everything across the board, all the way up to how many modes we're offering to the observers."</p><p>Those impacts, Brown said, would likely be felt for the first time in October, when the next fiscal year begins.</p><p>Brown's comments at the Webb observatory town hall at AAS came just before the <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/we-will-pursue-our-manifest-destiny-into-the-stars-president-trump-wants-astronauts-to-raise-the-american-flag-on-mars" data-before-rewrite-localise="https://www.space.com/space-exploration/we-will-pursue-our-manifest-destiny-into-the-stars-president-trump-wants-astronauts-to-raise-the-american-flag-on-mars"><u>inauguration of President Donald Trump</u></a>, who in subsequent weeks created the Department of Government Efficiency headed by SpaceX CEO <a data-analytics-id="inline-link" href="https://www.space.com/18849-elon-musk.html" data-before-rewrite-localise="https://www.space.com/18849-elon-musk.html"><u>Elon Musk</u></a> to reduce government spending. DOGE, as it's known, has worked to dismantle some entire agencies, like the U.S. Agency for International Development, which provides aid to other countries during disasters and other emergencies, while also overseeing massive cuts to the federal workforce. Nearly <a data-analytics-id="inline-link" href="https://www.space.com/the-universe/earth/over-1-000-nasa-employees-saved-from-dismissal-as-trump-downsizes-federal-workforce" data-before-rewrite-localise="https://www.space.com/the-universe/earth/over-1-000-nasa-employees-saved-from-dismissal-as-trump-downsizes-federal-workforce"><u>1,000 NASA jobs could be eliminated</u></a>, though they appear to have been saved from layoffs earlier this week.</p><p>Trump has nominated American billionaire entrepreneur <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/private-spaceflight/who-is-jared-isaacman-trumps-pick-for-nasa-chief" data-before-rewrite-localise="https://www.space.com/space-exploration/private-spaceflight/who-is-jared-isaacman-trumps-pick-for-nasa-chief"><u>Jared Isaacman</u></a>, who has flown in orbit twice on private SpaceX missions he financed himself, to serve as the next NASA administrator, though Isaacman has yet to be confirmed. The agency is currently being led by Acting Administrator <a data-analytics-id="inline-link" href="https://www.space.com/space-exploration/missions/who-is-janet-petro-trumps-pick-for-acting-nasa-administrator" data-before-rewrite-localise="https://www.space.com/space-exploration/missions/who-is-janet-petro-trumps-pick-for-acting-nasa-administrator"><u>Janet Petro</u></a>, former director of the agency's Kennedy Space Center in Florida.</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>

<div id="slice-container-authorBio-rTLnXKeQtgwCKwvYQVHpC6"><p>Tariq is the Editor-in-Chief of <a href="https://www.space.com/" data-before-rewrite-localise="https://www.space.com/">Space.com</a> and joined the team in 2001, first as an intern and staff writer, and later as an editor. He covers human spaceflight, exploration and space science, as well as skywatching and entertainment. He became Space.com's Managing Editor in 2009 and Editor-in-Chief in 2019. Before joining Space.com, Tariq was a staff reporter for The Los Angeles Times covering education and city beats in La Habra, Fullerton and Huntington Beach. In October 2022, <a href="https://www.nscfl.org/kolcum-award/" target="_blank">Tariq received the Harry Kolcum Award</a> for excellence in space reporting from the National Space Club Florida Committee. He is also an Eagle Scout (yes, he has the Space Exploration merit badge) and went to Space Camp four times as a kid and a fifth time as an adult. He has journalism degrees from the University of Southern California and New York University. You can find Tariq at Space.com and as the co-host to the <a href="https://twit.tv/shows/this-week-in-space" target="_blank">This Week In Space podcast</a> with space historian Rod Pyle on the <a href="https://twit.tv/" target="_blank">TWiT network</a>. To see his latest project, you can follow Tariq on&nbsp;Twitter <a href="https://twitter.com/tariqjmalik" target="_blank">@tariqjmalik</a>.</p></div>

</section>



<div data-test-id="more-about">
<p>More about science astronomy</p>


</div>

<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Feynman's blackboard at the time of his death (1988) (174 pts)]]></title>
            <link>https://digital.archives.caltech.edu/collections/Images/1.10-29/</link>
            <guid>43131017</guid>
            <pubDate>Fri, 21 Feb 2025 18:22:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digital.archives.caltech.edu/collections/Images/1.10-29/">https://digital.archives.caltech.edu/collections/Images/1.10-29/</a>, See on <a href="https://news.ycombinator.com/item?id=43131017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article><p>These digitized collections are accessible for purposes of education and research. Due to the nature of archival collections, archivists at the Caltech Archives and Special Collections are not always able to identify copyright and rights of privacy, publicity, or trademark. We are eager to <a href="mailto:archives@caltech.edu">hear from any rights holders</a>, so that we may obtain accurate information. Upon request, we’ll remove material from public view while we address a rights issue.</p></article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SWE-Bench tainted by answer leakage; real pass rates significantly lower (293 pts)]]></title>
            <link>https://arxiv.org/abs/2410.06992</link>
            <guid>43130732</guid>
            <pubDate>Fri, 21 Feb 2025 17:59:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.06992">https://arxiv.org/abs/2410.06992</a>, See on <a href="https://news.ycombinator.com/item?id=43130732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.06992">View PDF</a>
    <a href="https://arxiv.org/html/2410.06992v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Song Wang [<a href="https://arxiv.org/show-email/8151c019/2410.06992" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2410.06992v1" rel="nofollow">[v1]</a></strong>
        Wed, 9 Oct 2024 15:38:53 UTC (3,863 KB)<br>
    <strong>[v2]</strong>
        Thu, 10 Oct 2024 13:13:09 UTC (1,714 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Ruby on Rails still matters (200 pts)]]></title>
            <link>https://www.contraption.co/rails-versus-nextjs/</link>
            <guid>43130546</guid>
            <pubDate>Fri, 21 Feb 2025 17:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.contraption.co/rails-versus-nextjs/">https://www.contraption.co/rails-versus-nextjs/</a>, See on <a href="https://news.ycombinator.com/item?id=43130546">Hacker News</a></p>
Couldn't get https://www.contraption.co/rails-versus-nextjs/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The Profitable Startup (111 pts)]]></title>
            <link>https://linear.app/blog/the-profitable-startup</link>
            <guid>43130480</guid>
            <pubDate>Fri, 21 Feb 2025 17:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linear.app/blog/the-profitable-startup">https://linear.app/blog/the-profitable-startup</a>, See on <a href="https://news.ycombinator.com/item?id=43130480">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?</p><p>But that thinking was always flawed.</p><p>Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.</p><p><a href="https://paulgraham.com/ramenprofitable.html">Paul Graham famously wrote about "ramen profitability"</a> – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.</p><p>Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.</p><p>At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.</p><p>I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.</p><p>What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.</p><p>At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next <em>great</em> engineer. This intentional approach has allowed us to maintain both quality and culture.</p><p>The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.</p><p>While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.</p><p><strong>Measure What Matters</strong></p><p>Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.</p><p><strong>Understand Your Risk Profile</strong></p><p>Are you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.</p><p><strong>Hire Intentionally and Slower</strong></p><p>For most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.</p><p><strong>Raise on Your Own Terms </strong></p><p>Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.</p><p>The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.</p><p>I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepDive in everything of Llama3: revealing detailed insights and implementation (104 pts)]]></title>
            <link>https://github.com/therealoliver/Deepdive-llama3-from-scratch</link>
            <guid>43129887</guid>
            <pubDate>Fri, 21 Feb 2025 16:57:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch">https://github.com/therealoliver/Deepdive-llama3-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=43129887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/logo.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/logo.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deepdive-llama3-from-scratch</h2><a id="user-content-deepdive-llama3-from-scratch" aria-label="Permalink: Deepdive-llama3-from-scratch" href="#deepdive-llama3-from-scratch"></a></p>
<p dir="auto">
    <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/8714a45b348376ef75d52e9adcc597d84fc855efd11b2e5eca5b2b5cb52a99fe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7468657265616c6f6c697665722f44656570646976652d6c6c616d61332d66726f6d2d73637261746368" alt="License" data-canonical-src="https://img.shields.io/github/license/therealoliver/Deepdive-llama3-from-scratch"></a>
    <a href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/stargazers"><img src="https://camo.githubusercontent.com/12e1dc75bd78c858f8d5f6986ed939b9c9a8403e4f4d528da7d0759d3a8ae25d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7468657265616c6f6c697665722f44656570646976652d6c6c616d61332d66726f6d2d73637261746368" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/therealoliver/Deepdive-llama3-from-scratch"></a>
    <a href="#from_me"><img src="https://camo.githubusercontent.com/32783fcfbbf44f49658a770f3403e9a13329d7c170898e02ba0e5d4baf85aaac/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe298952532304275792532306d6525323061253230636f666665652d666636396234" alt="Buy me a coffee" data-canonical-src="https://img.shields.io/badge/☕%20Buy%20me%20a%20coffee-ff69b4"></a>
</p>

<hr>
<div dir="auto"><p>This project is an enhanced version based on <a href="https://github.com/naklecha/llama3-from-scratch">naklecha/llama3-from-scratch</a>. It has been comprehensively improved and optimized on the basis of the original project, aiming to help everyone more easily understand and master the implementation principle and the detailed reasoning process of the Llama3 model. Thanks to the contributions of the original author :)
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
The following are the core improvements of this project:
</h3><a id="user-content-the-following-are-the-core-improvements-of-this-project" aria-label="Permalink: 
The following are the core improvements of this project:
" href="#the-following-are-the-core-improvements-of-this-project"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Structural Optimization</strong><br>
The presentation sequence of the content has been rearranged, and the directory structure has been adjusted to make the learning process clearer and more reasonable, facilitating everyone to understand the code step by step.</p>
</li>
<li>
<p dir="auto"><strong>Code Annotations</strong><br>
A large number of detailed code annotations have been added to teach you how to understand the function of each piece of code. Even beginners can get started easily.</p>
</li>
<li>
<p dir="auto"><strong>Dimension Tracking</strong><br>
The changes in the matrix dimensions in each step of the calculation are fully annotated, making it easier for you to understand the entire process.</p>
</li>
<li>
<p dir="auto"><strong>Principle Explanation</strong><br>
Abundant principle-related explanations and a large number of detailed derivations have been added. It not only tells you "what to do" but also deeply explains "why to do it", helping you fundamentally master the design concept of the model.</p>
</li>
<li>
<p dir="auto"><strong>KV-Cache Insights</strong><br>
An additional derivation chapter on KV-Cache has been added, covering detailed core concepts, principle derivations, and the application process in the attention mechanism, allowing you to understand every detail and philosophy of KV-Cache from its roots.</p>
</li>
<li>
<div dir="auto"><p><strong>Bilingual Documents</strong><br>
Code files in both Chinese and English are provided. The native Chinese translation avoids the problem of inaccurate expressions caused by machine translation.
</p></div>
</li>
</ol>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#loading-the-model">Loading the model</a>
<ul dir="auto">
<li><a href="#loading-the-tokenizer">Loading the tokenizer</a></li>
<li><a href="#reading-model-files-and-configuration-files">Reading model files and configuration files</a>
<ul dir="auto">
<li><a href="#inferring-model-details-using-the-configuration-file">Inferring model details using the configuration file</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#convert-the-input-text-into-embeddings">Convert the input text into embeddings</a>
<ul dir="auto">
<li><a href="#convert-the-text-into-a-sequence-of-token-ids">Convert the text into a sequence of token ids</a></li>
<li><a href="#convert-the-sequence-of-token-ids-into-embeddings">Convert the sequence of token ids into embeddings</a></li>
</ul>
</li>
<li><a href="#build-the-first-transformer-block">Build the first Transformer block</a>
<ul dir="auto">
<li><a href="#normalization">Normalization</a>
<ul dir="auto">
<li><a href="#using-rms-normalization-for-embeddings">Using RMS normalization for embeddings</a></li>
</ul>
</li>
<li><a href="#implementing-the-single-head-attention-mechanism-from-scratch">Implementing the single-head attention mechanism from scratch</a>
<ul dir="auto">
<li><a href="#obtain-the-qkv-vectors-corresponding-to-the-input-tokens">Obtain the QKV vectors corresponding to the input tokens</a>
<ul dir="auto">
<li><a href="#obtain-the-query-vector">Obtain the query vector</a>
<ul dir="auto">
<li><a href="#unfold-the-query-weight-matrix">Unfold the query weight matrix</a></li>
<li><a href="#obtain-the-first-head">Obtain the first head</a></li>
<li><a href="#multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens">Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens</a></li>
</ul>
</li>
<li><a href="#obtain-the-key-vector-almost-the-same-as-the-query-vector">Obtain the key vector (almost the same as the query vector)</a></li>
<li><a href="#obtain-the-value-vector-almost-the-same-as-the-key-vector">Obtain the value vector (almost the same as the key vector)</a></li>
</ul>
</li>
<li><a href="#add-positional-information-to-the-query-and-key-vectors">Add positional information to the query and key vectors</a>
<ul dir="auto">
<li><a href="#rotary-position-encoding-rope">Rotary Position Encoding (RoPE)</a></li>
<li><a href="#add-positional-information-to-the-query-vectors">Add positional information to the query vectors</a></li>
<li><a href="#add-positional-information-to-the-key-vectors-same-as-the-query">Add positional information to the key vectors (same as the query)</a></li>
</ul>
</li>
<li><a href="#everythings-ready-lets-start-calculating-the-attention-weights-between-tokens">Everything's ready. Let's start calculating the attention weights between tokens.</a>
<ul dir="auto">
<li><a href="#multiply-the-query-and-key-vectors-to-obtain-the-attention-scores">Multiply the query and key vectors to obtain the attention scores.</a></li>
<li><a href="#now-we-must-mask-the-future-query-key-scores">Now we must mask the future query-key scores.</a></li>
<li><a href="#calculate-the-final-attention-weights-that-is-softmaxscore">Calculate the final attention weights, that is, softmax(score).</a></li>
</ul>
</li>
<li><a href="#finally-calculate-the-final-result-of-the-single-head-attention-mechanism">Finally! Calculate the final result of the single-head attention mechanism!</a></li>
</ul>
</li>
<li><a href="#calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process">Calculate the multi-head attention mechanism (a simple loop to repeat the above process)</a>
<ul dir="auto">
<li><a href="#calculate-the-result-for-each-head">Calculate the result for each head</a></li>
<li><a href="#merge-the-results-of-each-head-into-a-large-matrix">Merge the results of each head into a large matrix</a></li>
<li><a href="#head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer">Head-to-head information interaction (linear mapping), the final step of the self-attention layer!</a></li>
</ul>
</li>
<li><a href="#perform-the-residual-operation-add">Perform the residual operation (add)</a></li>
<li><a href="#perform-the-second-normalization-operation">Perform the second normalization operation</a></li>
<li><a href="#perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer">Perform the calculation of the FFN (Feed-Forward Neural Network) layer</a></li>
<li><a href="#perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block">Perform the residual operation again (Finally, we get the final output of the Transformer block!)</a></li>
</ul>
</li>
<li><a href="#everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-">Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)</a></li>
<li><a href="#lets-complete-the-last-step-and-predict-the-next-token">Let's complete the last step and predict the next token</a>
<ul dir="auto">
<li><a href="#first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer">First, perform one last normalization on the output of the last Transformer layer</a></li>
<li><a href="#then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension">Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)</a></li>
<li><a href="#heres-the-prediction-result">Here's the prediction result!</a></li>
</ul>
</li>
<li><a href="#lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-">Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)</a></li>
<li><a href="#need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz">Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)</a></li>
<li><a href="#thank-you-all-thanks-for-your-continuous-learning-love-you-all-">Thank you all. Thanks for your continuous learning. Love you all :)</a>
<ul dir="auto">
<li><a href="#from-me">From Me</a></li>
<li><a href="#from-the-author-of-predecessor-project">From the author of predecessor project</a></li>
</ul>
</li>
<li><a href="#license">LICENSE</a></li>
</ul>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">
Now, let's start the formal learning process!
</h3><a id="user-content-now-lets-start-the-formal-learning-process" aria-label="Permalink: 
Now, let's start the formal learning process!
" href="#now-lets-start-the-formal-learning-process"></a></p>
<br>
<div dir="auto"><p>In this file, I implemented Llama3 from scratch, one tensor and matrix multiplication at a time.
<br>
Also, I'm going to load tensors directly from the model file that meta provided for Llama3 (Meta-Llama-3-8B), you need to download the weights before running this file. Here is the offical link to download the weights: <a href="https://llama.meta.com/llama-downloads/" rel="nofollow">https://llama.meta.com/llama-downloads/</a>
<br>
Note: This project uses the original model files, that is, the models in the "original" folder of the downloaded model files.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
    Please Note! There is a small mistake in the figure:<br>
    </h3><a id="user-content-----please-note-there-is-a-small-mistake-in-the-figure----" aria-label="Permalink: 
    Please Note! There is a small mistake in the figure:" href="#----please-note-there-is-a-small-mistake-in-the-figure----"></a></p><p dir="auto"><h4 tabindex="-1" dir="auto">
        In each Transformer block, the input of the second "add" operation should be the output of the feed-forward layer and the output of the first "add" operation, instead of the result after normalization.
        <br>
        If we consider multi-head self-attention and feed-forward as the same type of operations (both for feature transformation), then the forms and processes of the two "normalization - feature transformation - residual connection (add)" are exactly the same.
    </h4><a id="user-content---------in-each-transformer-block-the-input-of-the-second-add-operation-should-be-the-output-of-the-feed-forward-layer-and-the-output-of-the-first-add-operation-instead-of-the-result-after-normalization----------------if-we-consider-multi-head-self-attention-and-feed-forward-as-the-same-type-of-operations-both-for-feature-transformation-then-the-forms-and-processes-of-the-two-normalization---feature-transformation---residual-connection-add-are-exactly-the-same----" aria-label="Permalink: 
        In each Transformer block, the input of the second &quot;add&quot; operation should be the output of the feed-forward layer and the output of the first &quot;add&quot; operation, instead of the result after normalization.
        
        If we consider multi-head self-attention and feed-forward as the same type of operations (both for feature transformation), then the forms and processes of the two &quot;normalization - feature transformation - residual connection (add)&quot; are exactly the same.
    " href="#--------in-each-transformer-block-the-input-of-the-second-add-operation-should-be-the-output-of-the-feed-forward-layer-and-the-output-of-the-first-add-operation-instead-of-the-result-after-normalization----------------if-we-consider-multi-head-self-attention-and-feed-forward-as-the-same-type-of-operations-both-for-feature-transformation-then-the-forms-and-processes-of-the-two-normalization---feature-transformation---residual-connection-add-are-exactly-the-same----"></a></p>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/archi.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/archi.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Loading the model</h2><a id="user-content-loading-the-model" aria-label="Permalink: Loading the model" href="#loading-the-model"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Loading the tokenizer</h2><a id="user-content-loading-the-tokenizer" aria-label="Permalink: Loading the tokenizer" href="#loading-the-tokenizer"></a></p>
<p dir="auto">The tokenizer is used to split the input text string into a sequence of sub-words, making it easier to input to the model.
<br>
I'm not going to implement a bpe tokenizer (but andrej karpathy has a really clean implementation),
link to his implementation: <a href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/karpathyminbpe.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/karpathyminbpe.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Summary of the steps to load the BPE-based tokenizer:</h3><a id="user-content-summary-of-the-steps-to-load-the-bpe-based-tokenizer" aria-label="Permalink: Summary of the steps to load the BPE-based tokenizer:" href="#summary-of-the-steps-to-load-the-bpe-based-tokenizer"></a></p>
<ol dir="auto">
<li>Loading regular words: Load the local tokenizer model dictionary (which only contains regular subwords and no special tokens).</li>
<li>Definition of the special words: Manually define special tokens (using ready-made ones or modifying based on the ready-made ones).</li>
<li>Definition of the text rough-splitting rule: Define the regular expression for text rough-splitting (just using a ready-made one). The input will go through two steps of rough-splitting (based on the regular expression) and fine-splitting (based on BPE) to obtain the final tokenization result.</li>
<li>Create tokenizer: Create a text encoder-decoder object based on the open-sourced tiktoken library by OpenAI (which can further split the rough-splitting result based on the BPE algorithm).</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Loading the BPE-based Tokenizer

# Import related libraries
from pathlib import Path  # Used to obtain the file name/model name from the file path
import tiktoken  # An open-source library developed by OpenAI for text encoding and decoding (mutual conversion between text and token IDs)
from tiktoken.load import load_tiktoken_bpe  # Load the BPE model
import torch  # Used for building models and matrix calculations
import json  # Used for loading configuration files
import matplotlib.pyplot as plt  # Used for plotting graphs


tokenizer_path = &quot;Meta-Llama-3-8B/original/tokenizer.model&quot;  # Path to the tokenizer model

# Special tokens outside the regular dictionary.
# These special tokens are present in the 'added_tokens' field of both 'tokenizer.json' and 'tokenizer_config.json' in the &quot;Meta-Llama-3-8B/&quot; path
special_tokens = [
            &quot;<|begin_of_text|>&quot;,
            &quot;<|end_of_text|>&quot;,
            &quot;<|reserved_special_token_0|>&quot;,  # Reserved special tokens from 0 to 250
            &quot;<|reserved_special_token_1|>&quot;,
            &quot;<|reserved_special_token_2|>&quot;,
            &quot;<|reserved_special_token_3|>&quot;,
            &quot;<|start_header_id|>&quot;,  # Start of header information, used to mark the header information that wraps structured data, such as metadata
            &quot;<|end_header_id|>&quot;,  # End of header information
            &quot;<|reserved_special_token_4|>&quot;,
            &quot;<|eot_id|>&quot;,  # end of turn, used to mark the end of the current turn in multi-turn conversations
        ] + [f&quot;<|reserved_special_token_{i}|>&quot; for i in range(5, 256 - 5)]


# Load the BPE model (actually a dictionary)
# A dictionary of subword(bytes type, decoded with utf-8)-rank(id) pairs, with 128000 words, not including the 256 special tokens above,
# so the total size of the model's dictionary will be 128256 in after operation (but not here)
# The rank values are an increasing sequence starting from 0, used to determine the priority order of subword unit merging,
# the higher the priority, the earlier the merging. Therefore, the variable name here is &quot;mergeable_ranks&quot; instead of something like BPE or word dictionary
# The special tokens are not added to the dictionary probably for flexibility,
# making it easy to add specific tokens when facing different model architectures or tasks with different special tokens, and keeping the dictionary size unchanged
mergeable_ranks = load_tiktoken_bpe(tokenizer_path)


# Create a text encoder-decoder object
# The pat_str is roughly divided into three types: words with abbreviations &amp; words, Chinese segments, 1-3-digit numbers &amp; other special characters
tokenizer = tiktoken.Encoding(
    name=Path(tokenizer_path).name,  # Name of the encoder, which is convenient when debugging and logging to use different encoders
    pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;,  # Regular expression for initially roughly splitting the text into a token sequence
    mergeable_ranks=mergeable_ranks,  # Pass in the loaded BPE model
    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},  # Dictionary for adding special token-id pairs
)


# Test whether the creation is successful, that is, whether the encoder-decoder can run correctly
print(tokenizer.decode(tokenizer.encode(&quot;create tokenizer successed!&quot;)))


# The following is a case test to test the effects and differences between the rough splitting of pat_str and the fine splitting of the tokenizer
# The regular expression of pat_str only provides a preliminary splitting,
# some long sentences or Chinese text will not be split and will be further refined based on the BPE algorithm in the tokenizer
import regex  # Since some Unicode syntax such as \p{L} is used in pat_str, the re library cannot be used

## Create a regular expression
pat_str=r&quot;(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;
pattern = regex.compile(pat_str)

## Text segmentation
text = &quot;Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.&quot;  # testing string
re_tokens = pattern.findall(text)  # Split the string using the regular expression
merge_tokens_id = tokenizer.encode(text)  # Split the string using the tokenizer
merge_tokens = [tokenizer.decode([i]) for i in merge_tokens_id]  # Convert the id sequence of the tokenizer's splitting result into an actual subword sequence

## Output result
print(&quot;Original string: &quot;, text)
print(&quot;Regular expression splitting result: &quot;, re_tokens)
print(&quot;Tokenizer splitting result: &quot;, merge_tokens)
print(&quot;Tokenizer splitting result ids: &quot;, list(zip(merge_tokens, merge_tokens_id)))

## From the results, it can be seen that the leading spaces of all words are retained, rather than being a single space token or being deleted.
## This is beneficial for the model to correctly understand the boundary information between words, such as 'alongwords' in the example."><pre><span># Loading the BPE-based Tokenizer</span>

<span># Import related libraries</span>
<span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>  <span># Used to obtain the file name/model name from the file path</span>
<span>import</span> <span>tiktoken</span>  <span># An open-source library developed by OpenAI for text encoding and decoding (mutual conversion between text and token IDs)</span>
<span>from</span> <span>tiktoken</span>.<span>load</span> <span>import</span> <span>load_tiktoken_bpe</span>  <span># Load the BPE model</span>
<span>import</span> <span>torch</span>  <span># Used for building models and matrix calculations</span>
<span>import</span> <span>json</span>  <span># Used for loading configuration files</span>
<span>import</span> <span>matplotlib</span>.<span>pyplot</span> <span>as</span> <span>plt</span>  <span># Used for plotting graphs</span>


<span>tokenizer_path</span> <span>=</span> <span>"Meta-Llama-3-8B/original/tokenizer.model"</span>  <span># Path to the tokenizer model</span>

<span># Special tokens outside the regular dictionary.</span>
<span># These special tokens are present in the 'added_tokens' field of both 'tokenizer.json' and 'tokenizer_config.json' in the "Meta-Llama-3-8B/" path</span>
<span>special_tokens</span> <span>=</span> [
            <span>"&lt;|begin_of_text|&gt;"</span>,
            <span>"&lt;|end_of_text|&gt;"</span>,
            <span>"&lt;|reserved_special_token_0|&gt;"</span>,  <span># Reserved special tokens from 0 to 250</span>
            <span>"&lt;|reserved_special_token_1|&gt;"</span>,
            <span>"&lt;|reserved_special_token_2|&gt;"</span>,
            <span>"&lt;|reserved_special_token_3|&gt;"</span>,
            <span>"&lt;|start_header_id|&gt;"</span>,  <span># Start of header information, used to mark the header information that wraps structured data, such as metadata</span>
            <span>"&lt;|end_header_id|&gt;"</span>,  <span># End of header information</span>
            <span>"&lt;|reserved_special_token_4|&gt;"</span>,
            <span>"&lt;|eot_id|&gt;"</span>,  <span># end of turn, used to mark the end of the current turn in multi-turn conversations</span>
        ] <span>+</span> [<span>f"&lt;|reserved_special_token_<span><span>{</span><span>i</span><span>}</span></span>|&gt;"</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>5</span>, <span>256</span> <span>-</span> <span>5</span>)]


<span># Load the BPE model (actually a dictionary)</span>
<span># A dictionary of subword(bytes type, decoded with utf-8)-rank(id) pairs, with 128000 words, not including the 256 special tokens above,</span>
<span># so the total size of the model's dictionary will be 128256 in after operation (but not here)</span>
<span># The rank values are an increasing sequence starting from 0, used to determine the priority order of subword unit merging,</span>
<span># the higher the priority, the earlier the merging. Therefore, the variable name here is "mergeable_ranks" instead of something like BPE or word dictionary</span>
<span># The special tokens are not added to the dictionary probably for flexibility,</span>
<span># making it easy to add specific tokens when facing different model architectures or tasks with different special tokens, and keeping the dictionary size unchanged</span>
<span>mergeable_ranks</span> <span>=</span> <span>load_tiktoken_bpe</span>(<span>tokenizer_path</span>)


<span># Create a text encoder-decoder object</span>
<span># The pat_str is roughly divided into three types: words with abbreviations &amp; words, Chinese segments, 1-3-digit numbers &amp; other special characters</span>
<span>tokenizer</span> <span>=</span> <span>tiktoken</span>.<span>Encoding</span>(
    <span>name</span><span>=</span><span>Path</span>(<span>tokenizer_path</span>).<span>name</span>,  <span># Name of the encoder, which is convenient when debugging and logging to use different encoders</span>
    <span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>,  <span># Regular expression for initially roughly splitting the text into a token sequence</span>
    <span>mergeable_ranks</span><span>=</span><span>mergeable_ranks</span>,  <span># Pass in the loaded BPE model</span>
    <span>special_tokens</span><span>=</span>{<span>token</span>: <span>len</span>(<span>mergeable_ranks</span>) <span>+</span> <span>i</span> <span>for</span> <span>i</span>, <span>token</span> <span>in</span> <span>enumerate</span>(<span>special_tokens</span>)},  <span># Dictionary for adding special token-id pairs</span>
)


<span># Test whether the creation is successful, that is, whether the encoder-decoder can run correctly</span>
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>tokenizer</span>.<span>encode</span>(<span>"create tokenizer successed!"</span>)))


<span># The following is a case test to test the effects and differences between the rough splitting of pat_str and the fine splitting of the tokenizer</span>
<span># The regular expression of pat_str only provides a preliminary splitting,</span>
<span># some long sentences or Chinese text will not be split and will be further refined based on the BPE algorithm in the tokenizer</span>
<span>import</span> <span>regex</span>  <span># Since some Unicode syntax such as \p{L} is used in pat_str, the re library cannot be used</span>

<span>## Create a regular expression</span>
<span>pat_str</span><span>=</span><span>r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"</span>
<span>pattern</span> <span>=</span> <span>regex</span>.<span>compile</span>(<span>pat_str</span>)

<span>## Text segmentation</span>
<span>text</span> <span>=</span> <span>"Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789."</span>  <span># testing string</span>
<span>re_tokens</span> <span>=</span> <span>pattern</span>.<span>findall</span>(<span>text</span>)  <span># Split the string using the regular expression</span>
<span>merge_tokens_id</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>text</span>)  <span># Split the string using the tokenizer</span>
<span>merge_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>i</span>]) <span>for</span> <span>i</span> <span>in</span> <span>merge_tokens_id</span>]  <span># Convert the id sequence of the tokenizer's splitting result into an actual subword sequence</span>

<span>## Output result</span>
<span>print</span>(<span>"Original string: "</span>, <span>text</span>)
<span>print</span>(<span>"Regular expression splitting result: "</span>, <span>re_tokens</span>)
<span>print</span>(<span>"Tokenizer splitting result: "</span>, <span>merge_tokens</span>)
<span>print</span>(<span>"Tokenizer splitting result ids: "</span>, <span>list</span>(<span>zip</span>(<span>merge_tokens</span>, <span>merge_tokens_id</span>)))

<span>## From the results, it can be seen that the leading spaces of all words are retained, rather than being a single space token or being deleted.</span>
<span>## This is beneficial for the model to correctly understand the boundary information between words, such as 'alongwords' in the example.</span></pre></div>
<div data-snippet-clipboard-copy-content="create tokenizer successed!
Original string:  Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.
Regular expression splitting result:  ['Hello', ' world', '!', ' It', &quot;'s&quot;, ' a', ' test', '.', ' 这是一个测试', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result:  ['Hello', ' world', '!', ' It', &quot;'s&quot;, ' a', ' test', '.', ' 这', '是一个', '测试', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result ids:  [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), (&quot;'s&quot;, 596), (' a', 264), (' test', 1296), ('.', 13), (' 这', 122255), ('是一个', 122503), ('测试', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]"><pre><code>create tokenizer successed!
Original string:  Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.
Regular expression splitting result:  ['Hello', ' world', '!', ' It', "'s", ' a', ' test', '.', ' 这是一个测试', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result:  ['Hello', ' world', '!', ' It', "'s", ' a', ' test', '.', ' 这', '是一个', '测试', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']
Tokenizer splitting result ids:  [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), ("'s", 596), (' a', 264), (' test', 1296), ('.', 13), (' 这', 122255), ('是一个', 122503), ('测试', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reading model files and configuration files</h2><a id="user-content-reading-model-files-and-configuration-files" aria-label="Permalink: Reading model files and configuration files" href="#reading-model-files-and-configuration-files"></a></p>
<p dir="auto">Generally, reading a model file depends on how its model class is written and the variable names within it.
<br>
However, since we are implementing Llama3 from scratch, we will read one tensor file at a time.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/model.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/model.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load the model, a dictionary such as {&quot;network-layer-name&quot;: tensor-type parameters}
model = torch.load(&quot;Meta-Llama-3-8B/original/consolidated.00.pth&quot;)

# Print the names of the first 20 network layers to verify if the model is loaded correctly.
print(json.dumps(list(model.keys())[:20], indent=4))"><pre><span># Load the model, a dictionary such as {"network-layer-name": tensor-type parameters}</span>
<span>model</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>"Meta-Llama-3-8B/original/consolidated.00.pth"</span>)

<span># Print the names of the first 20 network layers to verify if the model is loaded correctly.</span>
<span>print</span>(<span>json</span>.<span>dumps</span>(<span>list</span>(<span>model</span>.<span>keys</span>())[:<span>20</span>], <span>indent</span><span>=</span><span>4</span>))</pre></div>
<div data-snippet-clipboard-copy-content="[
    &quot;tok_embeddings.weight&quot;,
    &quot;layers.0.attention.wq.weight&quot;,
    &quot;layers.0.attention.wk.weight&quot;,
    &quot;layers.0.attention.wv.weight&quot;,
    &quot;layers.0.attention.wo.weight&quot;,
    &quot;layers.0.feed_forward.w1.weight&quot;,
    &quot;layers.0.feed_forward.w3.weight&quot;,
    &quot;layers.0.feed_forward.w2.weight&quot;,
    &quot;layers.0.attention_norm.weight&quot;,
    &quot;layers.0.ffn_norm.weight&quot;,
    &quot;layers.1.attention.wq.weight&quot;,
    &quot;layers.1.attention.wk.weight&quot;,
    &quot;layers.1.attention.wv.weight&quot;,
    &quot;layers.1.attention.wo.weight&quot;,
    &quot;layers.1.feed_forward.w1.weight&quot;,
    &quot;layers.1.feed_forward.w3.weight&quot;,
    &quot;layers.1.feed_forward.w2.weight&quot;,
    &quot;layers.1.attention_norm.weight&quot;,
    &quot;layers.1.ffn_norm.weight&quot;,
    &quot;layers.2.attention.wq.weight&quot;
]"><pre><code>[
    "tok_embeddings.weight",
    "layers.0.attention.wq.weight",
    "layers.0.attention.wk.weight",
    "layers.0.attention.wv.weight",
    "layers.0.attention.wo.weight",
    "layers.0.feed_forward.w1.weight",
    "layers.0.feed_forward.w3.weight",
    "layers.0.feed_forward.w2.weight",
    "layers.0.attention_norm.weight",
    "layers.0.ffn_norm.weight",
    "layers.1.attention.wq.weight",
    "layers.1.attention.wk.weight",
    "layers.1.attention.wv.weight",
    "layers.1.attention.wo.weight",
    "layers.1.feed_forward.w1.weight",
    "layers.1.feed_forward.w3.weight",
    "layers.1.feed_forward.w2.weight",
    "layers.1.attention_norm.weight",
    "layers.1.ffn_norm.weight",
    "layers.2.attention.wq.weight"
]
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Load the configuration file.
# The specific meaning of each configuration is described in the next section.
with open(&quot;Meta-Llama-3-8B/original/params.json&quot;, &quot;r&quot;) as f:
    config = json.load(f)
config"><pre><span># Load the configuration file.</span>
<span># The specific meaning of each configuration is described in the next section.</span>
<span>with</span> <span>open</span>(<span>"Meta-Llama-3-8B/original/params.json"</span>, <span>"r"</span>) <span>as</span> <span>f</span>:
    <span>config</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span>config</span></pre></div>
<div data-snippet-clipboard-copy-content="{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}"><pre><code>{'dim': 4096,
 'n_layers': 32,
 'n_heads': 32,
 'n_kv_heads': 8,
 'vocab_size': 128256,
 'multiple_of': 1024,
 'ffn_dim_multiplier': 1.3,
 'norm_eps': 1e-05,
 'rope_theta': 500000.0}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inferring model details using the configuration file</h3><a id="user-content-inferring-model-details-using-the-configuration-file" aria-label="Permalink: Inferring model details using the configuration file" href="#inferring-model-details-using-the-configuration-file"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration Item</th>
<th>Configuration Value</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>dim</td>
<td>4096</td>
<td>Dimension of the hidden layer, i.e., the vector representation of each token has a dimension of 4096.</td>
</tr>
<tr>
<td>n_layers</td>
<td>32</td>
<td>Number of model layers, i.e., the model has 32 Transformer layers or say Transformer blocks.</td>
</tr>
<tr>
<td>n_heads</td>
<td>32</td>
<td>Number of heads in multi-head attention, i.e., each multi-head attention block has 32 heads. The so-called multi-head means that multiple independent attention mechanisms are used simultaneously to capture different features or information of the input data.</td>
</tr>
<tr>
<td>n_kv_heads</td>
<td>8</td>
<td>Number of heads in key-value attention, used for Grouped Query Attention (GQA). That is, the key-value attention has 8 heads, while the query has n_heads=32 heads. Every 4 query heads will share a set of key-value pairs.</td>
</tr>
<tr>
<td>vocab_size</td>
<td>128256</td>
<td>Size of the vocabulary, including 128000 ordinary tokens and 256 special tokens.</td>
</tr>
<tr>
<td>multiple_of</td>
<td>1024</td>
<td>Multiple constraint on the dimension of the hidden layer. That is, the dimension of the model's hidden layer should be a multiple of 1024 to optimize computational efficiency.</td>
</tr>
<tr>
<td>ffn_dim_multiplier</td>
<td>1.3</td>
<td>Multiplier for the hidden layer dimension of the feed-forward network layer, used to calculate the hidden layer dimension of the FFN. The calculation process can be seen in the corresponding section.</td>
</tr>
<tr>
<td>norm_eps</td>
<td>1e-05</td>
<td>Constant added to the denominator in layer normalization calculation to prevent division by zero and ensure numerical stability.</td>
</tr>
<tr>
<td>rope_theta</td>
<td>500000.0</td>
<td>Basic frequency scaling factor in Rotary Position Encoding (RoPE), which controls the periodicity and resolution of position encoding, thus affecting the model's ability to capture sequences of different lengths and positional relationships.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Based on the configuration details, the internal calculation process of attention given an input can be inferred as follows:
</h3><a id="user-content-based-on-the-configuration-details-the-internal-calculation-process-of-attention-given-an-input-can-be-inferred-as-follows" aria-label="Permalink: 
Based on the configuration details, the internal calculation process of attention given an input can be inferred as follows:
" href="#based-on-the-configuration-details-the-internal-calculation-process-of-attention-given-an-input-can-be-inferred-as-follows"></a></p>
<pre>input(L, 4096) -&gt; query_proj(L, 128, 32)
               -&gt; key_proj(L, 128, 8)
               -&gt; value_proj(L, 128, 8)
                                           -&gt; group_query_attention(L, 128, 32)
                                           -&gt; output_proj(L, 4096)
                                                                                   -&gt; output(L, 4096)
</pre>
<div dir="auto" data-snippet-clipboard-copy-content="# Record these configurations, which will be gradually used later.
dim = config[&quot;dim&quot;]
n_layers = config[&quot;n_layers&quot;]
n_heads = config[&quot;n_heads&quot;]
n_kv_heads = config[&quot;n_kv_heads&quot;]
vocab_size = config[&quot;vocab_size&quot;]
multiple_of = config[&quot;multiple_of&quot;]
ffn_dim_multiplier = config[&quot;ffn_dim_multiplier&quot;]
norm_eps = config[&quot;norm_eps&quot;]
rope_theta = torch.tensor(config[&quot;rope_theta&quot;])"><pre><span># Record these configurations, which will be gradually used later.</span>
<span>dim</span> <span>=</span> <span>config</span>[<span>"dim"</span>]
<span>n_layers</span> <span>=</span> <span>config</span>[<span>"n_layers"</span>]
<span>n_heads</span> <span>=</span> <span>config</span>[<span>"n_heads"</span>]
<span>n_kv_heads</span> <span>=</span> <span>config</span>[<span>"n_kv_heads"</span>]
<span>vocab_size</span> <span>=</span> <span>config</span>[<span>"vocab_size"</span>]
<span>multiple_of</span> <span>=</span> <span>config</span>[<span>"multiple_of"</span>]
<span>ffn_dim_multiplier</span> <span>=</span> <span>config</span>[<span>"ffn_dim_multiplier"</span>]
<span>norm_eps</span> <span>=</span> <span>config</span>[<span>"norm_eps"</span>]
<span>rope_theta</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>config</span>[<span>"rope_theta"</span>])</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the input text into embeddings</h2><a id="user-content-convert-the-input-text-into-embeddings" aria-label="Permalink: Convert the input text into embeddings" href="#convert-the-input-text-into-embeddings"></a></p>
<p dir="auto">Before inputting the text in string form to the network layer, it needs to be converted into vector form for mathematical calculations.
<br>
The required process is: use the tokenizer to split the input text into a subword sequence -&gt; convert the subwords into vector representations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the text into a sequence of token ids</h2><a id="user-content-convert-the-text-into-a-sequence-of-token-ids" aria-label="Permalink: Convert the text into a sequence of token ids" href="#convert-the-text-into-a-sequence-of-token-ids"></a></p>
<p dir="auto">Here, we use tiktoken (a library from OpenAI) as the tokenizer.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/tokens.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/tokens.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Convert the input prompt into a sequence of token ids
prompt = &quot;the answer to the ultimate question of life, the universe, and everything is &quot;  # Input text
tokens = [128000] + tokenizer.encode(prompt)  # Perform subword segmentation and add a special token <|begin_of_text|> indicating the start of the text at the beginning of the text. Dimension: [17]
print(tokens)  # Check the segmentation result
tokens = torch.tensor(tokens)  # Convert to tensor type for subsequent matrix calculations. [17]

# Convert the token ids into a specific sequence of token subwords, which is only for display purposes and not actually needed
prompt_split_as_tokens = [tokenizer.decode([token]) for token in tokens]
print(prompt_split_as_tokens)"><pre><span># Convert the input prompt into a sequence of token ids</span>
<span>prompt</span> <span>=</span> <span>"the answer to the ultimate question of life, the universe, and everything is "</span>  <span># Input text</span>
<span>tokens</span> <span>=</span> [<span>128000</span>] <span>+</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>)  <span># Perform subword segmentation and add a special token &lt;|begin_of_text|&gt; indicating the start of the text at the beginning of the text. Dimension: [17]</span>
<span>print</span>(<span>tokens</span>)  <span># Check the segmentation result</span>
<span>tokens</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>tokens</span>)  <span># Convert to tensor type for subsequent matrix calculations. [17]</span>

<span># Convert the token ids into a specific sequence of token subwords, which is only for display purposes and not actually needed</span>
<span>prompt_split_as_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>token</span>]) <span>for</span> <span>token</span> <span>in</span> <span>tokens</span>]
<span>print</span>(<span>prompt_split_as_tokens</span>)</pre></div>
<div data-snippet-clipboard-copy-content="[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']"><pre><code>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Convert the sequence of token ids into embeddings</h2><a id="user-content-convert-the-sequence-of-token-ids-into-embeddings" aria-label="Permalink: Convert the sequence of token ids into embeddings" href="#convert-the-sequence-of-token-ids-into-embeddings"></a></p>
<div dir="auto"><p>Sorry, this is the only part in this codebase where I use built-in neural network modules.
<br>
In short, our original [17x1] token sequence is now [17x4096], that is, 17 embeddings of length 4096 (one for each token).
</p><p>
Note: Pay attention to the change in the shape of this tensor, which will make it easier for you to understand the entire process (And i will annotate the shape changes in all steps).</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/embeddings.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/embeddings.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create an embedding network layer to map discrete token ids to a continuous vector space
embedding_layer = torch.nn.Embedding(vocab_size, dim)

# Update the parameters of the embedding network with the pre-trained parameter values in Llama3
embedding_layer.weight.data.copy_(model[&quot;tok_embeddings.weight&quot;])

# Use the embedding network to convert the input sequence of token ids into vector representations
# The embedding network only looks up the corresponding vectors based on the ids in a dictionary and does not involve interactions between tokens.
# [17] -> [17x4096]
token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)  # By default, it is in full-precision float32. Here, we change it to the half-precision format to reduce memory usage.

token_embeddings_unnormalized.shape"><pre><span># Create an embedding network layer to map discrete token ids to a continuous vector space</span>
<span>embedding_layer</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>Embedding</span>(<span>vocab_size</span>, <span>dim</span>)

<span># Update the parameters of the embedding network with the pre-trained parameter values in Llama3</span>
<span>embedding_layer</span>.<span>weight</span>.<span>data</span>.<span>copy_</span>(<span>model</span>[<span>"tok_embeddings.weight"</span>])

<span># Use the embedding network to convert the input sequence of token ids into vector representations</span>
<span># The embedding network only looks up the corresponding vectors based on the ids in a dictionary and does not involve interactions between tokens.</span>
<span># [17] -&gt; [17x4096]</span>
<span>token_embeddings_unnormalized</span> <span>=</span> <span>embedding_layer</span>(<span>tokens</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># By default, it is in full-precision float32. Here, we change it to the half-precision format to reduce memory usage.</span>

<span>token_embeddings_unnormalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Build the first Transformer block</h2><a id="user-content-build-the-first-transformer-block" aria-label="Permalink: Build the first Transformer block" href="#build-the-first-transformer-block"></a></p>
<p dir="auto">From the pre-trained parameters involved in the first Transformer block shown below, it includes:</p>
<ol dir="auto">
<li>Two normalizations (attention_norm and ffn_norm)</li>
<li>Implementation of the attention mechanism (4 attention.w)</li>
<li>Implementation of the feed-forward network layer (3 feed_forward.w)</li>
<li>(Of course, it also includes two residual connection operations that do not require pre-trained parameters)</li>
</ol>
<p dir="auto">In general, the operation process in a Transformer block is as follows:
<br>
Normalization -&gt; Multi-head self-attention -&gt; Residual connection -&gt; Normalization -&gt; Feed-forward neural network -&gt; Residual connection</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Display all the weight parameters and their shapes of the first Transformer block
for k, v in model.items():
    if not k.startswith('layers'):
        continue
    if k.startswith('layers.1'):
        break
    print(k, v.shape)"><pre><span># Display all the weight parameters and their shapes of the first Transformer block</span>
<span>for</span> <span>k</span>, <span>v</span> <span>in</span> <span>model</span>.<span>items</span>():
    <span>if</span> <span>not</span> <span>k</span>.<span>startswith</span>(<span>'layers'</span>):
        <span>continue</span>
    <span>if</span> <span>k</span>.<span>startswith</span>(<span>'layers.1'</span>):
        <span>break</span>
    <span>print</span>(<span>k</span>, <span>v</span>.<span>shape</span>)</pre></div>
<div data-snippet-clipboard-copy-content="layers.0.attention.wq.weight torch.Size([4096, 4096])
layers.0.attention.wk.weight torch.Size([1024, 4096])
layers.0.attention.wv.weight torch.Size([1024, 4096])
layers.0.attention.wo.weight torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight torch.Size([14336, 4096])
layers.0.feed_forward.w3.weight torch.Size([14336, 4096])
layers.0.feed_forward.w2.weight torch.Size([4096, 14336])
layers.0.attention_norm.weight torch.Size([4096])
layers.0.ffn_norm.weight torch.Size([4096])"><pre><code>layers.0.attention.wq.weight torch.Size([4096, 4096])
layers.0.attention.wk.weight torch.Size([1024, 4096])
layers.0.attention.wv.weight torch.Size([1024, 4096])
layers.0.attention.wo.weight torch.Size([4096, 4096])
layers.0.feed_forward.w1.weight torch.Size([14336, 4096])
layers.0.feed_forward.w3.weight torch.Size([14336, 4096])
layers.0.feed_forward.w2.weight torch.Size([4096, 14336])
layers.0.attention_norm.weight torch.Size([4096])
layers.0.ffn_norm.weight torch.Size([4096])
</code></pre></div>
<p dir="auto">There are two points to note here:</p>
<ol dir="auto">
<li>The shape of the weight matrix of a neural network is (output dimension, input dimension). During the calculation, the parameter matrix W will be transposed to (input dimension, output dimension) and then multiplied by the input X, i.e., the output Y = XW.T. You will see this in the subsequent calculations.</li>
<li>Since Llama3 uses the grouped attention mechanism, every 4 query heads will share a set of kv vectors (for details, see the section on the details of the configuration file above). Therefore, the dimension of the weight matrix of kv is [1024, 4096], which is 1/4 of that of q ([4096, 4096]).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Normalization</h2><a id="user-content-normalization" aria-label="Permalink: Normalization" href="#normalization"></a></p>
<div dir="auto"><p>The normalization operation aims to constrain the scale differences in the data, avoiding issues such as unstable training process caused by excessive differences in vector values.
</p><p>
After normalization, the shape of the tensor remains [17x4096], the same as that of the embedding.</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/norm.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/norm.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using RMS normalization for embeddings</h3><a id="user-content-using-rms-normalization-for-embeddings" aria-label="Permalink: Using RMS normalization for embeddings" href="#using-rms-normalization-for-embeddings"></a></p>
<p dir="auto">Llama3 uses the Root Mean Square (RMS) normalization method, and its calculation formula is shown in the figure below.
<br>
It should be noted that we need a norm_eps parameter (from the configurations) because we don't want to accidentally set the RMS to 0 and perform a division by zero.
<br>
The formula is as follows:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/rms.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/rms.png" width="600"></a>
</p>

<p dir="auto">In addition, you may have noticed the gi parameter in the formula. This is a scaling factor learned during the model training process, used to scale the normalization result of each dimension again to enhance the model's expressive ability. Its dimension is the same as the feature dimension of the embedding, i.e., [4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Define the calculation function for RMS normalization
# Each token will be normalized independently
# norm_weights is the pre-trained scaling factor (i.e., gi in the formula) to enhance the model's representational ability. It can be loaded from the model file and has 4096 dimensions
# torch.rsqrt used to calculates the reciprocal of the square root of a tensor, i.e., 1/RMS(a)
def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"><pre><span># Define the calculation function for RMS normalization</span>
<span># Each token will be normalized independently</span>
<span># norm_weights is the pre-trained scaling factor (i.e., gi in the formula) to enhance the model's representational ability. It can be loaded from the model file and has 4096 dimensions</span>
<span># torch.rsqrt used to calculates the reciprocal of the square root of a tensor, i.e., 1/RMS(a)</span>
<span>def</span> <span>rms_norm</span>(<span>tensor</span>, <span>norm_weights</span>):
    <span>return</span> (<span>tensor</span> <span>*</span> <span>torch</span>.<span>rsqrt</span>(<span>tensor</span>.<span>pow</span>(<span>2</span>).<span>mean</span>(<span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>) <span>+</span> <span>norm_eps</span>)) <span>*</span> <span>norm_weights</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Normalize the input
token_embeddings = rms_norm(token_embeddings_unnormalized, model[&quot;layers.0.attention_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
model[&quot;layers.0.attention_norm.weight&quot;].shape, token_embeddings.shape"><pre><span># Normalize the input</span>
<span>token_embeddings</span> <span>=</span> <span>rms_norm</span>(<span>token_embeddings_unnormalized</span>, <span>model</span>[<span>"layers.0.attention_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>model</span>[<span>"layers.0.attention_norm.weight"</span>].<span>shape</span>, <span>token_embeddings</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="(torch.Size([4096]), torch.Size([17, 4096]))"><pre><code>(torch.Size([4096]), torch.Size([17, 4096]))
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementing the single-head attention mechanism from scratch</h2><a id="user-content-implementing-the-single-head-attention-mechanism-from-scratch" aria-label="Permalink: Implementing the single-head attention mechanism from scratch" href="#implementing-the-single-head-attention-mechanism-from-scratch"></a></p>
<p dir="auto">In the calculation of multi-head attention on each layer, 32 heads are involved. However, the calculation processes of these heads are completely identical and independent. Therefore, in this section, we will first implement the single-head attention calculation process, and expand it to multi-head calculation in the next section.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qkv.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qkv.png" width="600"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">
The core calculation of the attention mechanism is the calculation formula shown in the following figure.
</h3><a id="user-content-the-core-calculation-of-the-attention-mechanism-is-the-calculation-formula-shown-in-the-following-figure" aria-label="Permalink: 
The core calculation of the attention mechanism is the calculation formula shown in the following figure.
" href="#the-core-calculation-of-the-attention-mechanism-is-the-calculation-formula-shown-in-the-following-figure"></a></p>
<ol dir="auto">
<li>We need to obtain the query, key, and value vectors by performing a linear mapping on the input embeddings.</li>
<li>Subsequently, based on the QK vectors, we obtain the attention weights between tokens, that is, for each token, the scores of the importance or relevance of other tokens to it.</li>
<li>Finally, based on the attention weights, we weight the value vectors to obtain the attention results corresponding to each token.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<p dir="auto">Back to the point. Let's first load the attention heads of the first-layer Transformer.
<br>
&gt; When we load the query, key, value, and output weight matrices from the model (the output weight is used for information fusion among multiple heads to generate the final attention output), we will notice that their shapes are: [4096x4096], [1024x4096], [1024x4096], [4096x4096].
<br>
&gt; At first glance, this seems strange because ideally, we would like the q, k, v of each head to be independent of each other (in which case their shapes would be: 32x[128x4096], 8x[128x4096], 8x[128x4096]).
<br>
&gt; The author of the code binds them together because this helps parallelize the multiplication calculation of the attention heads.
<br>
&gt; But we will unfold everything...</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Show the shapes of the attention weight matrices of the current q, k, v and o.
print(
    model[&quot;layers.0.attention.wq.weight&quot;].shape,  # [4096x4096]
    model[&quot;layers.0.attention.wk.weight&quot;].shape,  # [1024x4096]
    model[&quot;layers.0.attention.wv.weight&quot;].shape,  # [1024x4096]
    model[&quot;layers.0.attention.wo.weight&quot;].shape   # [4096x4096]
)"><pre><span># Show the shapes of the attention weight matrices of the current q, k, v and o.</span>
<span>print</span>(
    <span>model</span>[<span>"layers.0.attention.wq.weight"</span>].<span>shape</span>,  <span># [4096x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wk.weight"</span>].<span>shape</span>,  <span># [1024x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wv.weight"</span>].<span>shape</span>,  <span># [1024x4096]</span>
    <span>model</span>[<span>"layers.0.attention.wo.weight"</span>].<span>shape</span>   <span># [4096x4096]</span>
)</pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])"><pre><code>torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Obtain the QKV vectors corresponding to the input tokens</h3><a id="user-content-obtain-the-qkv-vectors-corresponding-to-the-input-tokens" aria-label="Permalink: Obtain the QKV vectors corresponding to the input tokens" href="#obtain-the-qkv-vectors-corresponding-to-the-input-tokens"></a></p>
<p dir="auto">In this section, we will convert the input token embeddings into query, key, and value vectors for the purpose of attention mechanism computation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the query vector</h4><a id="user-content-obtain-the-query-vector" aria-label="Permalink: Obtain the query vector" href="#obtain-the-query-vector"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Unfold the query weight matrix</h5><a id="user-content-unfold-the-query-weight-matrix" aria-label="Permalink: Unfold the query weight matrix" href="#unfold-the-query-weight-matrix"></a></p>
<p dir="auto">We will first unfold the queries from multiple attention heads, and the final shape will be [32x128x4096].
<br>
Here, 32 is the number of attention heads in Llama3, 128 is the vector dimension of the query head, and 4096 is the dimension of the token embedding (the reason why the dimension of the embedding is in the last dimension is that when multiplying the input and the weight, it is usually = X*W.T, that is, multiplying by the transpose of the weight).</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the query weight matrix of layers.0 to unfold it in the form of multiple heads
q_layer0 = model[&quot;layers.0.attention.wq.weight&quot;]  # Default shape is [4096x4096]
head_dim = q_layer0.shape[0] // n_heads  # Dimension of the attention head, 4096/32 = 128
q_layer0 = q_layer0.view(n_heads, head_dim, dim)  # Unfolded dimension, [32x128x4096]
q_layer0.shape"><pre><span># Load and modify the shape of the query weight matrix of layers.0 to unfold it in the form of multiple heads</span>
<span>q_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wq.weight"</span>]  <span># Default shape is [4096x4096]</span>
<span>head_dim</span> <span>=</span> <span>q_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>  <span># Dimension of the attention head, 4096/32 = 128</span>
<span>q_layer0</span> <span>=</span> <span>q_layer0</span>.<span>view</span>(<span>n_heads</span>, <span>head_dim</span>, <span>dim</span>)  <span># Unfolded dimension, [32x128x4096]</span>
<span>q_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([32, 128, 4096])"><pre><code>torch.Size([32, 128, 4096])
</code></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Obtain the first head</h5><a id="user-content-obtain-the-first-head" aria-label="Permalink: Obtain the first head" href="#obtain-the-first-head"></a></p>
<p dir="auto">Here, I access the first head of the query weight matrix in the first layer. The shape of this query weight matrix is [128x4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
q_layer0_head0 = q_layer0[0]  # [32x128x4096] -> [128x4096]
q_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>q_layer0_head0</span> <span>=</span> <span>q_layer0</span>[<span>0</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
<span>q_layer0_head0</span>.<span>shape</span></pre></div>

<p dir="auto"><h5 tabindex="-1" dir="auto">Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens</h5><a id="user-content-multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens" aria-label="Permalink: Multiply the token embeddings by the query weights to obtain the query vectors corresponding to the tokens" href="#multiply-the-token-embeddings-by-the-query-weights-to-obtain-the-query-vectors-corresponding-to-the-tokens"></a></p>
<p dir="auto">Here, you can see that the shape of the result is [17x128]. This is because we have 17 tokens, and for each token, there is a query vector of length 128.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/q_per_token.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/q_per_token.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the query values of inputs on the first query head
# Q0_head0 = XW0_Q_head0.T
q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
q_per_token.shape"><pre><span># Calculate the query values of inputs on the first query head</span>
<span># Q0_head0 = XW0_Q_head0.T</span>
<span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>q_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the key vector (almost the same as the query vector)</h4><a id="user-content-obtain-the-key-vector-almost-the-same-as-the-query-vector" aria-label="Permalink: Obtain the key vector (almost the same as the query vector)" href="#obtain-the-key-vector-almost-the-same-as-the-query-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/keys.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/keys.png" width="600px"></a>
</p>
<p dir="auto">I want to take a lazy, so I won't elaborate on the calculation process of the key vector again. Orz. The only thing you need to remember is:
<br>
&gt; The key also generates a 128-dimensional vector.
<br>
&gt; The number of parameters of the weight matrix of the key is only 1/4 of that of the query, because the weight of each key is shared by 4 heads simultaneously to reduce the required amount of calculation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the key weight matrix of layers.0 to expand it in a multi-head form
# Different from the query weight matrix, the key has 8 attention heads, so the number of parameters is 1/4 of that of the query matrix
k_layer0 = model[&quot;layers.0.attention.wk.weight&quot;]  # [1024x4096]
k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim) # [8x128x4096]
k_layer0.shape"><pre><span># Load and modify the shape of the key weight matrix of layers.0 to expand it in a multi-head form</span>
<span># Different from the query weight matrix, the key has 8 attention heads, so the number of parameters is 1/4 of that of the query matrix</span>
<span>k_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wk.weight"</span>]  <span># [1024x4096]</span>
<span>k_layer0</span> <span>=</span> <span>k_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>) <span># [8x128x4096]</span>
<span>k_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
k_layer0_head0 = k_layer0[0]  # [8x128x4096] -> [128x4096]
k_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>k_layer0_head0</span> <span>=</span> <span>k_layer0</span>[<span>0</span>]  <span># [8x128x4096] -&gt; [128x4096]</span>
<span>k_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the key vectors corresponding to the inputs of the first head
# K0_head0 = XW0_K_head0.T
k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
k_per_token.shape"><pre><span># Calculate the key vectors corresponding to the inputs of the first head</span>
<span># K0_head0 = XW0_K_head0.T</span>
<span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>k_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Obtain the value vector (almost the same as the key vector)</h4><a id="user-content-obtain-the-value-vector-almost-the-same-as-the-key-vector" aria-label="Permalink: Obtain the value vector (almost the same as the key vector)" href="#obtain-the-value-vector-almost-the-same-as-the-key-vector"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/value.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/value.png" width="600px"></a>
</p>
<p dir="auto">&gt; Similar to the key weights, the value weights are also shared by every 4 attention heads (to save computation).
<br>
&gt; Therefore, the shape of the value weight matrix is [8x128x4096].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load and modify the shape of the value weight matrix of layers.0 to expand it in a multi-head form
# Similar to the key weight matrix, the value also has 8 attention heads, so the number of parameters is also 1/4 of that of the query matrix
v_layer0 = model[&quot;layers.0.attention.wv.weight&quot;]  # [1024x4096]
v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)  # [1024x4096] -> [8x128x4096]
v_layer0.shape"><pre><span># Load and modify the shape of the value weight matrix of layers.0 to expand it in a multi-head form</span>
<span># Similar to the key weight matrix, the value also has 8 attention heads, so the number of parameters is also 1/4 of that of the query matrix</span>
<span>v_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wv.weight"</span>]  <span># [1024x4096]</span>
<span>v_layer0</span> <span>=</span> <span>v_layer0</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer0</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [1024x4096] -&gt; [8x128x4096]</span>
<span>v_layer0</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([8, 128, 4096])"><pre><code>torch.Size([8, 128, 4096])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the weights of the first head
v_layer0_head0 = v_layer0[0]  # [8x128x4096] -> [128x4096]
v_layer0_head0.shape"><pre><span># Extract the weights of the first head</span>
<span>v_layer0_head0</span> <span>=</span> <span>v_layer0</span>[<span>0</span>]  <span># [8x128x4096] -&gt; [128x4096]</span>
<span>v_layer0_head0</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the value vectors corresponding to the inputs of the first head
# V0_head0 = XW0_V_head0.T
v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)  # [17x4096] x [4096x128] = [17x128]
v_per_token.shape"><pre><span># Calculate the value vectors corresponding to the inputs of the first head</span>
<span># V0_head0 = XW0_V_head0.T</span>
<span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head0</span>.<span>T</span>)  <span># [17x4096] x [4096x128] = [17x128]</span>
<span>v_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Add positional information to the query and key vectors</h3><a id="user-content-add-positional-information-to-the-query-and-key-vectors" aria-label="Permalink: Add positional information to the query and key vectors" href="#add-positional-information-to-the-query-and-key-vectors"></a></p>
<ul dir="auto">
<li>For natural language, the sequential relationship and relative positions between words are extremely important. For example, "The dog bites the man" and "The man bites the dog" have completely different semantic information. Moreover, our intuition also tells us that the correlation between relatively close words is usually greater than that between distant words.</li>
<li>Therefore, we need to provide positional information between tokens during the attention calculation process, so that the model can better capture the dependencies in the sequence.</li>
<li>Why add it to the query and key vectors? Because the query and key vectors are used to calculate the attention weights, i.e., the importance of each token to other tokens. This requires both of them to know the positions and relative positional relationships of any two tokens when calculating the similarity between them.</li>
<li>Why not add it to the value vectors? Because the value vectors are only used for weighted summation. The positional information has already been considered in the interaction between the query and the key. Therefore, the value vectors only need to provide content information.</li>
</ul>
<p dir="auto">We will use RoPE (Rotary Position Encoding) to add positional information to these vectors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Rotary Position Encoding (RoPE)</h4><a id="user-content-rotary-position-encoding-rope" aria-label="Permalink: Rotary Position Encoding (RoPE)" href="#rotary-position-encoding-rope"></a></p>
<div dir="auto"><p>You can watch this video to understand its mathematical principles in detail (this is also the one I watched):
<a href="https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s" rel="nofollow">https://www.youtube.com/watch?v=o29P0Kpobz0&amp;t=530s</a></p><p>
The general idea of RoPE is to regard the vector as being in the complex space, and then generate specific rotation matrix based on positions. By multiplying the vector with the rotation matrix, rotation in the complex space can be achieved, thereby adding the relative position information to the vector. (That is, taking the positional relationship of the input vectors as a rotation at different angles in a complex space.)
<br>
(Similar to the rotation of planar position coordinates around an axis through the multiplication of trigonometric-function-based matrices in robot kinematics.)
</p><p>
RoPE is usually applied to the query and key vectors in the self-attention mechanism. When calculating the attention scores, the query and key vectors are first rotated based on the corresponding rotation matrix of RoPE. Then, operations such as dot-product calculation and softmax normalization are performed. In this way, the Transformer can take positional information into account when calculating attentions and better capture the dependencies in the text.</p></div>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/rope.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/rope.png" width="600"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">
The specific calculation process of RoPE is as follows:
</h3><a id="user-content-the-specific-calculation-process-of-rope-is-as-follows" aria-label="Permalink: 
The specific calculation process of RoPE is as follows:
" href="#the-specific-calculation-process-of-rope-is-as-follows"></a></p>
<ol dir="auto">
<li>Divide the dimensions of each vector into pairs (because the derivation of high-dimensional rotation matrices is complex, and excessively high dimensions will significantly increase the computational complexity, while the formulas for two-dimensional rotation are relatively mature and simple, making them easy to calculate).</li>
<li>For each pair, obtain <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\Large \theta=\frac{1}{rope\_theta^{i/D}}$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$i$</math-renderer> is the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$i$</math-renderer>-th pair and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$D$</math-renderer> is the total number of pairs. That is, the positional information of the current dimension pair within the vector.</li>
<li>For each vector, obtain <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\Large m$</math-renderer>, which represents that the vector corresponds to the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m$</math-renderer>-th token. That is, the positional information of the current vector within the entire input vectors.</li>
<li>For each pair, <a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/pmatrix.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/pmatrix.png" alt="png"></a> , where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$res$</math-renderer> is the result of rotating the vector pair by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees in the complex space.</li>
<li>Perform the above calculations on all dimension pairs of all vectors to obtain the final RoPE result.</li>
</ol>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qsplit.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qsplit.png" width="600"></a>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">
In the actual code implementation, to simplify the calculation process, the above-mentioned calculation based on the rotation matrix (Step 4) will be converted into a calculation in the complex number domain. The principle is as follows:
</h3><a id="user-content-in-the-actual-code-implementation-to-simplify-the-calculation-process-the-above-mentioned-calculation-based-on-the-rotation-matrix-step-4-will-be-converted-into-a-calculation-in-the-complex-number-domain-the-principle-is-as-follows" aria-label="Permalink: 
In the actual code implementation, to simplify the calculation process, the above-mentioned calculation based on the rotation matrix (Step 4) will be converted into a calculation in the complex number domain. The principle is as follows:
" href="#in-the-actual-code-implementation-to-simplify-the-calculation-process-the-above-mentioned-calculation-based-on-the-rotation-matrix-step-4-will-be-converted-into-a-calculation-in-the-complex-number-domain-the-principle-is-as-follows"></a></p>
<ol dir="auto">
<li>The rectangular coordinates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$(x, y)$</math-renderer> can be regarded as the coordinate representation of the complex number <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large x + yi$</math-renderer> on the complex plane.</li>
<li>The polar form of a complex number can be expressed as <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta}$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$r$</math-renderer> is the modulus and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\theta$</math-renderer> is the angle.</li>
<li>The multiplication calculation in polar coordinates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large r_1e^{i\theta_1} \times r_2e^{i\theta_2} = r_1r_2e^{i(\theta_1 + \theta_2)}$</math-renderer> can be regarded as increasing the length of coordinate_1 by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$r_2$</math-renderer> times and rotating the angle by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\theta_2$</math-renderer> degrees.</li>
<li>Therefore, if you want to rotate the coordinates by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees, you can define a rotation factor <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large e^{im\theta}$</math-renderer> with a modulus of 1 and an angle of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer>. Multiplying it by the coordinates will be equivalent to the rotation method based on the rotation matrix.</li>
<li>In addition, according to Euler's formula, we have <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta} = r\cos\theta + r\sin{\theta i} = x + yi$</math-renderer> and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large e^{im\theta} = \cos{m\theta} + \sin{m\theta i}$</math-renderer>.</li>
<li>Therefore, rotating a two-dimensional coordinate <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$(x, y)$</math-renderer> by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer> degrees can be obtained through <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\large re^{i\theta^\prime} \times e^{im\theta} = (x + yi) \times (\cos{m\theta} + \sin{m\theta i})$</math-renderer> (the product of two complex numbers).</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">Add positional information to the query vectors</h4><a id="user-content-add-positional-information-to-the-query-vectors" aria-label="Permalink: Add positional information to the query vectors" href="#add-positional-information-to-the-query-vectors"></a></p>
<div dir="auto"><p>In the following steps, we will first split the query vectors into pairs, and then perform angle rotation on each pair, as shown in the above steps.
</p><p>
Now we have a vector with a shape of [17x64x2]. This is obtained by splitting the 128-dimensional query vectors corresponding to each token in the prompt into 64 pairs, and each pair will be rotated by </p><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$m\theta$</math-renderer><p> degrees.</p></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Split the query vectors in pairs along the dimension direction.
# .float() is for switch back to full precision to ensure the precision and numerical stability in the subsequent trigonometric function calculations.
# [17x128] -> [17x64x2]
q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)
q_per_token_split_into_pairs.shape"><pre><span># Split the query vectors in pairs along the dimension direction.</span>
<span># .float() is for switch back to full precision to ensure the precision and numerical stability in the subsequent trigonometric function calculations.</span>
<span># [17x128] -&gt; [17x64x2]</span>
<span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)
<span>q_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Start to obtain the complex-domain representation of the rotation matrix.
</h3><a id="user-content-start-to-obtain-the-complex-domain-representation-of-the-rotation-matrix" aria-label="Permalink: 
Start to obtain the complex-domain representation of the rotation matrix.
" href="#start-to-obtain-the-complex-domain-representation-of-the-rotation-matrix"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/freq_cis.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/freq_cis.png" width="600"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate θ. Step 1: Obtain i/D.
# [64]
zero_to_one_split_into_64_parts = torch.tensor(range(64))/64  # Each feature has 64 dimension pairs after segmentation, so 64 θ values are required
zero_to_one_split_into_64_parts"><pre><span># Calculate θ. Step 1: Obtain i/D.</span>
<span># [64]</span>
<span>zero_to_one_split_into_64_parts</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>range</span>(<span>64</span>))<span>/</span><span>64</span>  <span># Each feature has 64 dimension pairs after segmentation, so 64 θ values are required</span>
<span>zero_to_one_split_into_64_parts</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])"><pre><code>tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate θ. Step 2: Obtain θ.
# rope_theta is used to control information such as the periodicity of the position encoding.
# For details, please refer to the configuration information section.
freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)  # [64]
freqs"><pre><span># Calculate θ. Step 2: Obtain θ.</span>
<span># rope_theta is used to control information such as the periodicity of the position encoding.</span>
<span># For details, please refer to the configuration information section.</span>
<span>freqs</span> <span>=</span> <span>1.0</span> <span>/</span> (<span>rope_theta</span> <span>**</span> <span>zero_to_one_split_into_64_parts</span>)  <span># [64]</span>
<span>freqs</span></pre></div>
<div data-snippet-clipboard-copy-content="tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])"><pre><code>tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate mθ
# 'outer' is for outer product calculation, and 'arange(17)' represents the m corresponding to each vector (since the input has 17 tokens, 17 m values are needed).
# The result has a shape of [17x64], meaning that each vector corresponding to a token has 64 mθ values, which are used to calculate the rotation of each of the 64 dimension pairs.
freqs_for_each_token = torch.outer(torch.arange(17), freqs)  # [17] &amp; [64] -> [17x64]"><pre><span># Calculate mθ</span>
<span># 'outer' is for outer product calculation, and 'arange(17)' represents the m corresponding to each vector (since the input has 17 tokens, 17 m values are needed).</span>
<span># The result has a shape of [17x64], meaning that each vector corresponding to a token has 64 mθ values, which are used to calculate the rotation of each of the 64 dimension pairs.</span>
<span>freqs_for_each_token</span> <span>=</span> <span>torch</span>.<span>outer</span>(<span>torch</span>.<span>arange</span>(<span>17</span>), <span>freqs</span>)  <span># [17] &amp; [64] -&gt; [17x64]</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (cosmθ + sinmθi), that is, convert mθ to the complex-number form
# Regard the rotation angle mθ as a polar-coordinate form with a modulus of 1, and then convert it to a complex-number representation
# The two inputs of 'polar' represent the modulus (set to 1, meaning only the angle is changed without affecting the length) and the angle (i.e., mθ) respectively
freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)  # [17x64] -> [17x64]
print(freqs_cis.shape)

# View freqs_cis at some positions, just for display
token_to_show = [1, 3, 5]  # View the 2nd, 4th, and 6th rows
fig, axs = plt.subplots(1, len(token_to_show), figsize=(5 * len(token_to_show), 4))  # Generate a figure window with 3 sub-plots in 1 row and 3 columns
for i, index in enumerate(token_to_show):
    value = freqs_cis[index]
    for j, element in enumerate(value):
        # Plot a blue line from the origin to the coordinate point, with the real part as the x-coordinate and the imaginary part as the y-coordinate.
        axs[i].plot([0, element.real], [0, element.imag], color='blue', linewidth=1, label=f&quot;Index: {j}&quot;)
        # Draw red numerical annotations to represent the i-th pair of dimensions.
        axs[i].annotate(f&quot;{j}&quot;, xy=(element.real, element.imag), color='red')
    axs[i].set_xlabel('Real')
    axs[i].set_ylabel('Imaginary')
    axs[i].set_title(f'Plot of {index + 1}th of freqs_cis')
plt.show()

&quot;&quot;&quot;
Note: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.
      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X
&quot;&quot;&quot;"><pre><span># Obtain (cosmθ + sinmθi), that is, convert mθ to the complex-number form</span>
<span># Regard the rotation angle mθ as a polar-coordinate form with a modulus of 1, and then convert it to a complex-number representation</span>
<span># The two inputs of 'polar' represent the modulus (set to 1, meaning only the angle is changed without affecting the length) and the angle (i.e., mθ) respectively</span>
<span>freqs_cis</span> <span>=</span> <span>torch</span>.<span>polar</span>(<span>torch</span>.<span>ones_like</span>(<span>freqs_for_each_token</span>), <span>freqs_for_each_token</span>)  <span># [17x64] -&gt; [17x64]</span>
<span>print</span>(<span>freqs_cis</span>.<span>shape</span>)

<span># View freqs_cis at some positions, just for display</span>
<span>token_to_show</span> <span>=</span> [<span>1</span>, <span>3</span>, <span>5</span>]  <span># View the 2nd, 4th, and 6th rows</span>
<span>fig</span>, <span>axs</span> <span>=</span> <span>plt</span>.<span>subplots</span>(<span>1</span>, <span>len</span>(<span>token_to_show</span>), <span>figsize</span><span>=</span>(<span>5</span> <span>*</span> <span>len</span>(<span>token_to_show</span>), <span>4</span>))  <span># Generate a figure window with 3 sub-plots in 1 row and 3 columns</span>
<span>for</span> <span>i</span>, <span>index</span> <span>in</span> <span>enumerate</span>(<span>token_to_show</span>):
    <span>value</span> <span>=</span> <span>freqs_cis</span>[<span>index</span>]
    <span>for</span> <span>j</span>, <span>element</span> <span>in</span> <span>enumerate</span>(<span>value</span>):
        <span># Plot a blue line from the origin to the coordinate point, with the real part as the x-coordinate and the imaginary part as the y-coordinate.</span>
        <span>axs</span>[<span>i</span>].<span>plot</span>([<span>0</span>, <span>element</span>.<span>real</span>], [<span>0</span>, <span>element</span>.<span>imag</span>], <span>color</span><span>=</span><span>'blue'</span>, <span>linewidth</span><span>=</span><span>1</span>, <span>label</span><span>=</span><span>f"Index: <span><span>{</span><span>j</span><span>}</span></span>"</span>)
        <span># Draw red numerical annotations to represent the i-th pair of dimensions.</span>
        <span>axs</span>[<span>i</span>].<span>annotate</span>(<span>f"<span><span>{</span><span>j</span><span>}</span></span>"</span>, <span>xy</span><span>=</span>(<span>element</span>.<span>real</span>, <span>element</span>.<span>imag</span>), <span>color</span><span>=</span><span>'red'</span>)
    <span>axs</span>[<span>i</span>].<span>set_xlabel</span>(<span>'Real'</span>)
    <span>axs</span>[<span>i</span>].<span>set_ylabel</span>(<span>'Imaginary'</span>)
    <span>axs</span>[<span>i</span>].<span>set_title</span>(<span>f'Plot of <span><span>{</span><span>index</span> <span>+</span> <span>1</span><span>}</span></span>th of freqs_cis'</span>)
<span>plt</span>.<span>show</span>()

<span>"""</span>
<span>Note: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.</span>
<span>      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X</span>
<span>"""</span></pre></div>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_47_1.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_47_1.png" alt="png"></a></p>
<div data-snippet-clipboard-copy-content="'\nNote: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.\n      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X\n'"><pre><code>'\nNote: As shown in the figures, tokens in later positions have larger rotation angles, but within a single token, earlier vector dimensions have larger rotation angles.\n      You can explore further to see if there are any mathematical reasons behind this if you are interested. X_X\n'
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
Now we have provided a complex number (an angle-changing vector) for each dimension pair of the query vector corresponding to each token.
</h3><a id="user-content-now-we-have-provided-a-complex-number-an-angle-changing-vector-for-each-dimension-pair-of-the-query-vector-corresponding-to-each-token" aria-label="Permalink: 
Now we have provided a complex number (an angle-changing vector) for each dimension pair of the query vector corresponding to each token.
" href="#now-we-have-provided-a-complex-number-an-angle-changing-vector-for-each-dimension-pair-of-the-query-vector-corresponding-to-each-token"></a></p>
<br>
Now we can convert our query (the one divided into pairs) into complex numbers and then rotate these queries through dot-product calculation. :)
<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (x + yi)
# That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].
q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # [17x64x2] -> [17x64]
q_per_token_as_complex_numbers.shape"><pre><span># Obtain (x + yi)</span>
<span># That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].</span>
<span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># [17x64x2] -&gt; [17x64]</span>
<span>q_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate (x + yi) * (cosmθ + sinmθi)
# That is, perform the rotation operation to obtain the final result.
q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis  # [17x64] * [17x64] = [17x64]
q_per_token_as_complex_numbers_rotated.shape"><pre><span># Calculate (x + yi) * (cosmθ + sinmθi)</span>
<span># That is, perform the rotation operation to obtain the final result.</span>
<span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># [17x64] * [17x64] = [17x64]</span>
<span>q_per_token_as_complex_numbers_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Obtain the rotated vectors (restore the shape).
</h3><a id="user-content-obtain-the-rotated-vectors-restore-the-shape" aria-label="Permalink: 
Obtain the rotated vectors (restore the shape).
" href="#obtain-the-rotated-vectors-restore-the-shape"></a></p>
<br>
We can represent the complex numbers as real numbers again to obtain the query results in the form of dimension pairs.
<div dir="auto" data-snippet-clipboard-copy-content="# Convert the complex-number results back to the real-number dimension-pair form.
q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # [17x64] -> [17x64x2]
q_per_token_split_into_pairs_rotated.shape"><pre><span># Convert the complex-number results back to the real-number dimension-pair form.</span>
<span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># [17x64] -&gt; [17x64x2]</span>
<span>q_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<p dir="auto">Merge the rotated dimensions. In this way, we obtain a new query vector (the rotated query vector) with a shape of [17x128], where 17 represents the number of tokens and 128 represents the dimension of the query vector.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Restore the dimension-pair results to the original form of the query vectors, and obtain the final query vector.
q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # [17x64x2] -> [17x128]
q_per_token_rotated.shape"><pre><span># Restore the dimension-pair results to the original form of the query vectors, and obtain the final query vector.</span>
<span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># [17x64x2] -&gt; [17x128]</span>
<span>q_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Add positional information to the key vectors (same as the query)</h4><a id="user-content-add-positional-information-to-the-key-vectors-same-as-the-query" aria-label="Permalink: Add positional information to the key vectors (same as the query)" href="#add-positional-information-to-the-key-vectors-same-as-the-query"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Split the key vectors into pairs along the dimension direction to form dimension pairs (modify the shape).
k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # [17x128] -> [17x64x2]
k_per_token_split_into_pairs.shape"><pre><span># Split the key vectors into pairs along the dimension direction to form dimension pairs (modify the shape).</span>
<span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># [17x128] -&gt; [17x64x2]</span>
<span>k_per_token_split_into_pairs</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Obtain (x + yi)
# That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].
k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # [17x64x2] -> [17x64]
k_per_token_as_complex_numbers.shape"><pre><span># Obtain (x + yi)</span>
<span># That is, convert the dimension pairs into complex numbers. After the conversion, the shape of the dimensions will change from [17x64x2] to [17x64].</span>
<span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># [17x64x2] -&gt; [17x64]</span>
<span>k_per_token_as_complex_numbers</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Calculate (x + yi) * (cosmθ + sinmθi)
# That is, perform the rotation operation to obtain the final result.
# And convert the result back to the real-number form.
k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)  # [17x64] * [17x64] = [17x64] -> [17x64x2]
k_per_token_split_into_pairs_rotated.shape"><pre><span># Calculate (x + yi) * (cosmθ + sinmθi)</span>
<span># That is, perform the rotation operation to obtain the final result.</span>
<span># And convert the result back to the real-number form.</span>
<span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>)  <span># [17x64] * [17x64] = [17x64] -&gt; [17x64x2]</span>
<span>k_per_token_split_into_pairs_rotated</span>.<span>shape</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Restore the dimension-pair results to the original form of the key vectors, and obtain the final key vector.
k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # [17x64x2] -> [17x128]
k_per_token_rotated.shape"><pre><span># Restore the dimension-pair results to the original form of the key vectors, and obtain the final key vector.</span>
<span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># [17x64x2] -&gt; [17x128]</span>
<span>k_per_token_rotated</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
At this stage, we have the rotated query vectors and key vectors corresponding to each token.
</h3><a id="user-content-at-this-stage-we-have-the-rotated-query-vectors-and-key-vectors-corresponding-to-each-token" aria-label="Permalink: 
At this stage, we have the rotated query vectors and key vectors corresponding to each token.
" href="#at-this-stage-we-have-the-rotated-query-vectors-and-key-vectors-corresponding-to-each-token"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/keys0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/keys0.png" width="600px"></a>
</p>
<p dir="auto">The shape of each query and key vector remains [17x128].</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Everything's ready. Let's start calculating the attention weights between tokens.</h3><a id="user-content-everythings-ready-lets-start-calculating-the-attention-weights-between-tokens" aria-label="Permalink: Everything's ready. Let's start calculating the attention weights between tokens." href="#everythings-ready-lets-start-calculating-the-attention-weights-between-tokens"></a></p>
<p dir="auto">This will involve a three-step process:</p>
<ol dir="auto">
<li>Calculate the attention scores: score = Q x K</li>
<li>Mask the future tokens: score = mask(score)</li>
<li>Calculate the attention weights: res = softmax(score)</li>
</ol>
<p dir="auto">Let's get started! :)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multiply the query and key vectors to obtain the attention scores.</h4><a id="user-content-multiply-the-query-and-key-vectors-to-obtain-the-attention-scores" aria-label="Permalink: Multiply the query and key vectors to obtain the attention scores." href="#multiply-the-query-and-key-vectors-to-obtain-the-attention-scores"></a></p>
<p dir="auto">In this way, we will get the score values between each token and all other tokens.
<br>
These scores represent how strongly each token's query relates to every other token's key.
<br>
This is the self-attention!
<br>
The shape of this attention score matrix (qk_per_token) is [17x17], where 17 is the number of tokens in the input prompt.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/qkmatmul.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/qkmatmul.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the attention score
# At the same time, perform normalization to prevent the subsequent softmax calculation results from being overly skewed towards 0 or 1,
# (the dot-product values may be too large when the dimensions are large),
# which could lead to vanishing gradients or exploding gradients, so as to maintain numerical stability.
qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5  # [17x128] x [128x17] = [17x17]
qk_per_token.shape"><pre><span># Calculate the attention score</span>
<span># At the same time, perform normalization to prevent the subsequent softmax calculation results from being overly skewed towards 0 or 1,</span>
<span># (the dot-product values may be too large when the dimensions are large),</span>
<span># which could lead to vanishing gradients or exploding gradients, so as to maintain numerical stability.</span>
<span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
<span>qk_per_token</span>.<span>shape</span></pre></div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Now we must mask the future query-key scores.</h4><a id="user-content-now-we-must-mask-the-future-query-key-scores" aria-label="Permalink: Now we must mask the future query-key scores." href="#now-we-must-mask-the-future-query-key-scores"></a></p>
<p dir="auto">During the training process of Llama 3, the QK scores of future tokens will be masked.
<br>
Why? Because during training, we only learn how to use past tokens to predict the current token. If we don't mask future tokens, it will lead to the leakage of prediction information.
<br>
Therefore, during the inference process, we also need to set the future tokens to 0 (to ensure the logical consistency between the training and inference processes).
<br></p>
<p dir="auto">Of course, if you're as curious as I am about what would happen without masking, you can check the results of the additional experiment I conducted in the last section after you've finished learning. (^_&lt;)</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/mask.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/mask.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# First, take a look at the score matrix before masking
def display_qk_heatmap(qk_per_token):
    _, ax = plt.subplots()  # Create a figure window

    # `imshow` is commonly used to display data in the form of a two-dimensional array or matrix,
    # it maps the matrix elements to grayscale or color values, so it can be used to draw heatmaps.
    # Convert the tensor back to full precision, then detach it from the computational graph to avoid potential gradient calculation and storage issues.
    # Specify to use the 'viridis' color mapping scheme to display the image (blue -> green -> yellow).
    im = ax.imshow(qk_per_token.to(float).detach(), cmap='viridis')

    # Set the number and labels of the x and y axis ticks to ensure correct one-to-one correspondence.
    ax.set_xticks(range(len(prompt_split_as_tokens)))
    ax.set_yticks(range(len(prompt_split_as_tokens)))
    ax.set_xticklabels(prompt_split_as_tokens)
    ax.set_yticklabels(prompt_split_as_tokens)

    # Add a color bar on the side.
    # Specify `im` to identify the correct color mapping and value range.
    # Specify the sub-plot it belongs to as `ax` (if there are multiple sub-plots, it would be `ax = ax[i]`).
    ax.figure.colorbar(im, ax=ax)
    
display_qk_heatmap(qk_per_token)"><pre><span># First, take a look at the score matrix before masking</span>
<span>def</span> <span>display_qk_heatmap</span>(<span>qk_per_token</span>):
    <span>_</span>, <span>ax</span> <span>=</span> <span>plt</span>.<span>subplots</span>()  <span># Create a figure window</span>

    <span># `imshow` is commonly used to display data in the form of a two-dimensional array or matrix,</span>
    <span># it maps the matrix elements to grayscale or color values, so it can be used to draw heatmaps.</span>
    <span># Convert the tensor back to full precision, then detach it from the computational graph to avoid potential gradient calculation and storage issues.</span>
    <span># Specify to use the 'viridis' color mapping scheme to display the image (blue -&gt; green -&gt; yellow).</span>
    <span>im</span> <span>=</span> <span>ax</span>.<span>imshow</span>(<span>qk_per_token</span>.<span>to</span>(<span>float</span>).<span>detach</span>(), <span>cmap</span><span>=</span><span>'viridis'</span>)

    <span># Set the number and labels of the x and y axis ticks to ensure correct one-to-one correspondence.</span>
    <span>ax</span>.<span>set_xticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_yticks</span>(<span>range</span>(<span>len</span>(<span>prompt_split_as_tokens</span>)))
    <span>ax</span>.<span>set_xticklabels</span>(<span>prompt_split_as_tokens</span>)
    <span>ax</span>.<span>set_yticklabels</span>(<span>prompt_split_as_tokens</span>)

    <span># Add a color bar on the side.</span>
    <span># Specify `im` to identify the correct color mapping and value range.</span>
    <span># Specify the sub-plot it belongs to as `ax` (if there are multiple sub-plots, it would be `ax = ax[i]`).</span>
    <span>ax</span>.<span>figure</span>.<span>colorbar</span>(<span>im</span>, <span>ax</span><span>=</span><span>ax</span>)
    
<span>display_qk_heatmap</span>(<span>qk_per_token</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_65_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_65_0.png" alt="png"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Generate the masking matrix
# Set the positions of the elements to be masked to negative infinity, and set the positions that do not need to be masked to 0.
# Then, add it to the score matrix to achieve the masking effect (negative infinity will tend to 0 when calculating the softmax).

# `torch.full` is used to generate a tensor with a specified shape and filling value.
# Here, a [17x17] matrix filled with negative infinity is first generated.
# Specify that the device of this matrix is the same as that of the previous tokens to ensure that there are no errors in subsequent calculations,
# for example, if the previous tokens are on the GPU and the device is not specified here, the `mask` will be newly created on the CPU, and an error will occur when adding the two.
mask = torch.full((len(tokens), len(tokens)), float(&quot;-inf&quot;), device=tokens.device)  # [17x17]

# `torch.triu` is used to return the upper-triangular part of the matrix, and set the rest to 0 (use `torch.tril` to get the lower-triangular part).
# `diagonal` is the offset of the diagonal. When it's 1, it means taking the upper-triangular part starting from 1 position above the main diagonal to avoid masking the token itself.
mask = torch.triu(mask, diagonal=1)  # [17x17]

mask, mask.shape"><pre><span># Generate the masking matrix</span>
<span># Set the positions of the elements to be masked to negative infinity, and set the positions that do not need to be masked to 0.</span>
<span># Then, add it to the score matrix to achieve the masking effect (negative infinity will tend to 0 when calculating the softmax).</span>

<span># `torch.full` is used to generate a tensor with a specified shape and filling value.</span>
<span># Here, a [17x17] matrix filled with negative infinity is first generated.</span>
<span># Specify that the device of this matrix is the same as that of the previous tokens to ensure that there are no errors in subsequent calculations,</span>
<span># for example, if the previous tokens are on the GPU and the device is not specified here, the `mask` will be newly created on the CPU, and an error will occur when adding the two.</span>
<span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>((<span>len</span>(<span>tokens</span>), <span>len</span>(<span>tokens</span>)), <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)  <span># [17x17]</span>

<span># `torch.triu` is used to return the upper-triangular part of the matrix, and set the rest to 0 (use `torch.tril` to get the lower-triangular part).</span>
<span># `diagonal` is the offset of the diagonal. When it's 1, it means taking the upper-triangular part starting from 1 position above the main diagonal to avoid masking the token itself.</span>
<span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># [17x17]</span>

<span>mask</span>, <span>mask</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
 torch.Size([17, 17]))"><pre><code>(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),
 torch.Size([17, 17]))
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Mask the scores of future tokens
qk_per_token_after_masking = qk_per_token + mask  # [17x17] + [17x17] = [17x17]
display_qk_heatmap(qk_per_token_after_masking)  # Display the attention scores after masking"><pre><span># Mask the scores of future tokens</span>
<span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># [17x17] + [17x17] = [17x17]</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking</span>)  <span># Display the attention scores after masking</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_67_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_67_0.png" alt="png"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Calculate the final attention weights, that is, softmax(score).</h4><a id="user-content-calculate-the-final-attention-weights-that-is-softmaxscore" aria-label="Permalink: Calculate the final attention weights, that is, softmax(score)." href="#calculate-the-final-attention-weights-that-is-softmaxscore"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/softmax.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/softmax.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the attention weights
# That is, calculate the softmax values of the scores.
# `dim = 1` indicates that the softmax calculation is performed row-by-row, and the result is converted to half-precision to be consistent with the subsequent value vectors.
qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # [17x17] -> [17x17]
display_qk_heatmap(qk_per_token_after_masking_after_softmax)"><pre><span># Calculate the attention weights</span>
<span># That is, calculate the softmax values of the scores.</span>
<span># `dim = 1` indicates that the softmax calculation is performed row-by-row, and the result is converted to half-precision to be consistent with the subsequent value vectors.</span>
<span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># [17x17] -&gt; [17x17]</span>
<span>display_qk_heatmap</span>(<span>qk_per_token_after_masking_after_softmax</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/output_69_0.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/output_69_0.png" alt="png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finally! Calculate the final result of the single-head attention mechanism!</h3><a id="user-content-finally-calculate-the-final-result-of-the-single-head-attention-mechanism" aria-label="Permalink: Finally! Calculate the final result of the single-head attention mechanism!" href="#finally-calculate-the-final-result-of-the-single-head-attention-mechanism"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/attention.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/attention.png" width="600px"></a>
</p>
<p dir="auto">Principle: The previous attention weights (ranging from 0 to 1) are used to determine what proportion of each value vector should be used for each token (i.e., to weight the value vectors).</p>
<p dir="auto">Example: If the input consists of 3 tokens, the attention result of the first token might be: res = 0.6 * value_1 + 0.3 * value_2 + 0.1 * value_3</p>
<p dir="auto">The shape of the attention result after the multiplication of the weight matrix and the value matrix is [17x128].</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the final result of the single-head attention
qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] x [17x128] = [17x128]
qkv_attention.shape"><pre><span># Calculate the final result of the single-head attention</span>
<span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] x [17x128] = [17x128]</span>
<span>qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Calculate the multi-head attention mechanism (a simple loop to repeat the above process)</h2><a id="user-content-calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process" aria-label="Permalink: Calculate the multi-head attention mechanism (a simple loop to repeat the above process)" href="#calculate-the-multi-head-attention-mechanism-a-simple-loop-to-repeat-the-above-process"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/heads.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/heads.png" width="600px"></a>
</p>
<p dir="auto">We now have the attention values for the first head of the first layer.
<br></p>
<div dir="auto"><p>Now we need to run a loop to perform exactly the same mathematical process as in the previous cell, but for each head in the first layer.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">
It's worth noting that in the <a href="https://github.com/meta-llama/llama3/blob/main/llama/model.py#L90">official Llama3 code implementation</a>, the multi-head attention calculation adopts the method of one-time matrix multiplication instead of time consuming for-loop calculations. The general process is as follows:
</h3><a id="user-content-its-worth-noting-that-in-the-official-llama3-code-implementation-the-multi-head-attention-calculation-adopts-the-method-of-one-time-matrix-multiplication-instead-of-time-consuming-for-loop-calculations-the-general-process-is-as-follows" aria-label="Permalink: 
It's worth noting that in the official Llama3 code implementation, the multi-head attention calculation adopts the method of one-time matrix multiplication instead of time consuming for-loop calculations. The general process is as follows:
" href="#its-worth-noting-that-in-the-official-llama3-code-implementation-the-multi-head-attention-calculation-adopts-the-method-of-one-time-matrix-multiplication-instead-of-time-consuming-for-loop-calculations-the-general-process-is-as-follows"></a></p>
<ol dir="auto">
<li>Based on matrix parallelism, calculate the QKV vectors: [17x4096] x [4096x4096] or [4096x1024] = [17x4096] or [17x1024], and then reshape them to [32x17x128] or [8x17x128].</li>
<li>After obtaining the QKV vectors, duplicate the internal parts of the K and V vectors to make their shapes consistent with the Q vector. At this time, the shapes of all of them are [32x17x128].</li>
<li>When calculating the scores, use the transpose method to exchange the positions of the last two dimensions of the tensors to complete the matrix multiplication. For example, <code>torch.matmul(q, k.transpose(1,2)) / head_dim ** 0.5</code>. At this time, it is [32x17x128] x [32x128x17] = [32x17x17].</li>
<li>The same principle applies to other matrix calculations.</li>
</ol>
<p dir="auto">Note: The matrix shape changes in each step of the above process are simplified versions, only for illustration to facilitate understanding, which are different from the change process in the official Llama3 implementation (the official implementation involves a large number of shape change processes).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Calculate the result for each head</h3><a id="user-content-calculate-the-result-for-each-head" aria-label="Permalink: Calculate the result for each head" href="#calculate-the-result-for-each-head"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the multi-head attention results
# That is, a loop of the previous single-head attention calculation process
qkv_attention_store = []

for head in range(n_heads):
    # Extract the QKV weight matrices corresponding to the current head
    q_layer0_head = q_layer0[head]  # [32x128x4096] -> [128x4096]
    k_layer0_head = k_layer0[head//4]  # Every 4 heads share one key weight, [8x128x4096] -> [128x4096]
    v_layer0_head = v_layer0[head//4]  # Every 4 heads share one value weight, [8x128x4096] -> [128x4096]
    
    # Calculate XW to obtain the QKV vectors
    # [17x4096] x [4096x128] = [17x128]
    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)
    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)
    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)
    
    # Add position information to the query vector (RoPE)
    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
    q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis[:len(tokens)]  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -> [17x128]

    # Add position information to the key vector (RoPE)
    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
    k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis[:len(tokens)]  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -> [17x128]

    # Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))
    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5  # [17x128] x [128x17] = [17x17]
    
    # Mask the scores of future tokens
    mask = torch.full(qk_per_token.shape, float(&quot;-inf&quot;), device=tokens.device)  # Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]
    mask = torch.triu(mask, diagonal=1)  # Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]
    qk_per_token_after_masking = qk_per_token + mask  # Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]
    
    # Calculate the attention weights (i.e., softmax(score))
    # Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).
    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # Calculate the softmax row-by-row. [17x17]
    
    # Calculate the final result of the attention mechanism (i.e., softmax(score) × V)
    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] × [17x128] = [17x128]
    
    # Record the result of this head
    qkv_attention_store.append(qkv_attention)

len(qkv_attention_store)"><pre><span># Calculate the multi-head attention results</span>
<span># That is, a loop of the previous single-head attention calculation process</span>
<span>qkv_attention_store</span> <span>=</span> []

<span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
    <span># Extract the QKV weight matrices corresponding to the current head</span>
    <span>q_layer0_head</span> <span>=</span> <span>q_layer0</span>[<span>head</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
    <span>k_layer0_head</span> <span>=</span> <span>k_layer0</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one key weight, [8x128x4096] -&gt; [128x4096]</span>
    <span>v_layer0_head</span> <span>=</span> <span>v_layer0</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one value weight, [8x128x4096] -&gt; [128x4096]</span>
    
    <span># Calculate XW to obtain the QKV vectors</span>
    <span># [17x4096] x [4096x128] = [17x128]</span>
    <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>q_layer0_head</span>.<span>T</span>)
    <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>k_layer0_head</span>.<span>T</span>)
    <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>token_embeddings</span>, <span>v_layer0_head</span>.<span>T</span>)
    
    <span># Add position information to the query vector (RoPE)</span>
    <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
    <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
    <span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)]  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
    <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
    <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -&gt; [17x128]</span>

    <span># Add position information to the key vector (RoPE)</span>
    <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
    <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
    <span>k_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>[:<span>len</span>(<span>tokens</span>)]  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
    <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
    <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -&gt; [17x128]</span>

    <span># Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))</span>
    <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>head_dim</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
    
    <span># Mask the scores of future tokens</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>(<span>qk_per_token</span>.<span>shape</span>, <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>tokens</span>.<span>device</span>)  <span># Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]</span>
    <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]</span>
    <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]</span>
    
    <span># Calculate the attention weights (i.e., softmax(score))</span>
    <span># Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).</span>
    <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># Calculate the softmax row-by-row. [17x17]</span>
    
    <span># Calculate the final result of the attention mechanism (i.e., softmax(score) × V)</span>
    <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] × [17x128] = [17x128]</span>
    
    <span># Record the result of this head</span>
    <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)

<span>len</span>(<span>qkv_attention_store</span>)</pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Merge the results of each head into a large matrix</h3><a id="user-content-merge-the-results-of-each-head-into-a-large-matrix" aria-label="Permalink: Merge the results of each head into a large matrix" href="#merge-the-results-of-each-head-into-a-large-matrix"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/stacked.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/stacked.png" width="600px"></a>
</p>
Now we have the results of the attention mechanism for all 32 heads in the first layer. Next, we'll merge all the attention values into a large matrix with a shape of [17x4096].
<br>
We're almost done with the calculation of the attention layer :)
<div dir="auto" data-snippet-clipboard-copy-content="# Merge the multi-head attention matrices
stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)  # Concatenate along the second dimension, 32x[17x128] -> [17x4096]
stacked_qkv_attention.shape"><pre><span># Merge the multi-head attention matrices</span>
<span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Concatenate along the second dimension, 32x[17x128] -&gt; [17x4096]</span>
<span>stacked_qkv_attention</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Head-to-head information interaction (linear mapping), the final step of the self-attention layer!</h3><a id="user-content-head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer" aria-label="Permalink: Head-to-head information interaction (linear mapping), the final step of the self-attention layer!" href="#head-to-head-information-interaction-linear-mapping-the-final-step-of-the-self-attention-layer"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/weightmatrix.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/weightmatrix.png" width="600px"></a>
</p>
The last step of the attention calculation for layer0 is to perform the final linear mapping, that is, multiply the combined attention matrix by the output weight matrix.
<div dir="auto" data-snippet-clipboard-copy-content="# Load the output weight matrix of layers.0
w_layer0 = model[&quot;layers.0.attention.wo.weight&quot;]  # [4096x4096]
w_layer0.shape"><pre><span># Load the output weight matrix of layers.0</span>
<span>w_layer0</span> <span>=</span> <span>model</span>[<span>"layers.0.attention.wo.weight"</span>]  <span># [4096x4096]</span>
<span>w_layer0</span>.<span>shape</span></pre></div>

<p dir="auto">This is just a simple linear layer, so we only need matrix multiplication.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the linear mapping of the attention matrix
# This is the final output of the attention layer
embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)  # [17x4096] x [4096x4096] = [17x4096]
embedding_delta.shape"><pre><span># Perform the linear mapping of the attention matrix</span>
<span># This is the final output of the attention layer</span>
<span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>w_layer0</span>.<span>T</span>)  <span># [17x4096] x [4096x4096] = [17x4096]</span>
<span>embedding_delta</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the residual operation (add)</h2><a id="user-content-perform-the-residual-operation-add" aria-label="Permalink: Perform the residual operation (add)" href="#perform-the-residual-operation-add"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/afterattention.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/afterattention.png" width="600px"></a>
</p>
Now we have the value of the input vector after the attention mechanism is applied. At this time, we need to add the original input vector to it (i.e., the residual operation, to ensure that information is not easily lost and alleviate the problem of gradient vanishing).
<div dir="auto" data-snippet-clipboard-copy-content="# Add the output of the attention layer to the original input to complete the residual operation
embedding_after_edit = token_embeddings_unnormalized + embedding_delta  # [17x4096] + [17x4096] = [17x4096]
embedding_after_edit.shape"><pre><span># Add the output of the attention layer to the original input to complete the residual operation</span>
<span>embedding_after_edit</span> <span>=</span> <span>token_embeddings_unnormalized</span> <span>+</span> <span>embedding_delta</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
<span>embedding_after_edit</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the second normalization operation</h2><a id="user-content-perform-the-second-normalization-operation" aria-label="Permalink: Perform the second normalization operation" href="#perform-the-second-normalization-operation"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/norm_after.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/norm_after.png" width="600px"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Normalize the result of the residual operation
embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&quot;layers.0.ffn_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
embedding_after_edit_normalized.shape"><pre><span># Normalize the result of the residual operation</span>
<span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>"layers.0.ffn_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>embedding_after_edit_normalized</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the calculation of the FFN (Feed-Forward Neural Network) layer</h2><a id="user-content-perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer" aria-label="Permalink: Perform the calculation of the FFN (Feed-Forward Neural Network) layer" href="#perform-the-calculation-of-the-ffn-feed-forward-neural-network-layer"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/swiglu.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/swiglu.png" width="600px"></a>
</p>
<br>
In Llama3, they used the SwiGLU feed-forward network. This network architecture can effectively increase nonlinear characteristics when the model needs.
<br>
Nowadays, this kind of feed-forward network architecture is very common in large language models.
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Introduce Nonlinear Layers:</h3><a id="user-content-why-introduce-nonlinear-layers" aria-label="Permalink: Why Introduce Nonlinear Layers:" href="#why-introduce-nonlinear-layers"></a></p>
<ul dir="auto">
<li>The Nonlinearity is at the core of why neural network models can be considered "universal function approximators". In traditional neural network models, we use nonlinear activation functions (such as sigmoid, ReLU, etc.) to increase the model's expressive power, enabling it to fit the complex patterns hidden in the training data.</li>
<li>However, in the Transformer, the attention mechanism is essentially a linear weighted sum of the value vectors (even though the weights are obtained through nonlinear calculation of the softmax function, it's still just a linear weighting for the values). Therefore, although it can capture global dependencies, its output is still only a linear combination of the input. At this time, the Transformer model is actually lacks nonlinear capabilities.</li>
<li>So, it is necessary to add an FFN network after the self-attention layer to introduce nonlinear transformation capabilities to the model, thus improving the model's ability to model complex semantic relationships.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generally, introducing nonlinear layers can play the following roles:</h3><a id="user-content-generally-introducing-nonlinear-layers-can-play-the-following-roles" aria-label="Permalink: Generally, introducing nonlinear layers can play the following roles:" href="#generally-introducing-nonlinear-layers-can-play-the-following-roles"></a></p>
<ol dir="auto">
<li>Add nonlinear capabilities to the model to facilitate the model's learning and training.</li>
<li>Enhance the model's information abstraction ability, enabling the model to represent data features and patterns at different levels during the layer-by-layer learning process. For example, the lower-layer networks can identify basic language structures (such as part-of-speech), while the higher-layer networks can understand more complex semantic information (such as sentiment, intention).</li>
<li>In addition, a current view holds that the attention layer is mainly used for input context interaction, while the FFN layer is where the LLMs mainly stores and remembers general knowledge during training (given to its nonlinear representation ability), so that it can find answers to input questions from general knowledge.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">SwiGLU Network Structure:</h3><a id="user-content-swiglu-network-structure" aria-label="Permalink: SwiGLU Network Structure:" href="#swiglu-network-structure"></a></p>
<ol dir="auto">
<li>Perform a linear transformation on the input: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime = XW_3$</math-renderer>
</li>
<li>Gating unit: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = Activation\_Function(XW_1)$</math-renderer>, which is used to selectively pass information. That is, assuming that the information in <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime$</math-renderer> has different importance, so the information should be weighted and passed based on the score of the gating unit, thus improving the expressive ability of the model.</li>
<li>The activation function used is a Swish activation function (hence the network is called SwiGLU, which is a combination of the Swish activation function and the Gated Linear Unit (GLU)). The formula is: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Swish = X \cdot \sigma(\beta X)$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\sigma$</math-renderer> is the sigmoid activation function. In SwiGLU, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$\beta$</math-renderer> is set to 1 (in the original formula, it is a learnable parameter).</li>
<li>Therefore, the specific calculation of the gating unit is: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = XW_1 \cdot \sigma(XW_1)$</math-renderer>. In PyTorch, this activation function is called silu, that is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$GATE = silu(XW_1)$</math-renderer>.</li>
<li>Application of the gating mechanism: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$X^\prime = X^\prime \cdot GATE$</math-renderer>
</li>
<li>Perform a linear transformation again: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Y = X^\prime W_2$</math-renderer>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Calculation of the Dimension Size of the Hidden Layer in the Feed-Forward Layer (Based on the Official Implementation Process of Llama3):</h3><a id="user-content-calculation-of-the-dimension-size-of-the-hidden-layer-in-the-feed-forward-layer-based-on-the-official-implementation-process-of-llama3" aria-label="Permalink: Calculation of the Dimension Size of the Hidden Layer in the Feed-Forward Layer (Based on the Official Implementation Process of Llama3):" href="#calculation-of-the-dimension-size-of-the-hidden-layer-in-the-feed-forward-layer-based-on-the-official-implementation-process-of-llama3"></a></p>
<ol dir="auto">
<li>Input dimension is dim = 4096</li>
<li>hidden_dim = 4 * dim = 16384  # First, magnify it by four times. When initializing the feed-forward layer in the Transformer block, the input hidden_dim is multiplied by four.</li>
<li>hidden_dim = int(2 * hidden_dim / 3) = 10922 # Then, magnify it by 2/3 times. Such scaling is first performed within the feed-forward layer.</li>
<li>hidden_dim = int(ffn_dim_multiplier * hidden_dim) = int(1.3 * 10922) = 14198  # Then, magnify it by ffn_dim_multiplier times. The ffn_dim_multiplier is defined as 1.3 in the model configuration file.</li>
<li>hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) = 1024 * ((14198 + 1024 - 1) // 1024) = 14336  # Adjust it to an integer multiple of multiple_of. The multiple_of is defined as 1024 in the model configuration file to ensure that the dimensions of all hidden layers in the model are multiples of 1024, so as to improve the computational efficiency.</li>
<li>Finally, we get the dimension size of the hidden layer is 14336.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Calculate the feed-forward network layer
# The dimension size of the hidden layer is 14336
w1 = model[&quot;layers.0.feed_forward.w1.weight&quot;]  # [14336x4096]
w3 = model[&quot;layers.0.feed_forward.w3.weight&quot;]  # [14336x4096]
w2 = model[&quot;layers.0.feed_forward.w2.weight&quot;]  # [4096x14336]
print(w1.shape, w3.shape, w2.shape)

# output = (silu(XW1) * XW3)W2
# [17x4096] x [4096x14336] x [14336x4096] = [17x4096]
output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
output_after_feedforward.shape"><pre><span># Calculate the feed-forward network layer</span>
<span># The dimension size of the hidden layer is 14336</span>
<span>w1</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w1.weight"</span>]  <span># [14336x4096]</span>
<span>w3</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w3.weight"</span>]  <span># [14336x4096]</span>
<span>w2</span> <span>=</span> <span>model</span>[<span>"layers.0.feed_forward.w2.weight"</span>]  <span># [4096x14336]</span>
<span>print</span>(<span>w1</span>.<span>shape</span>, <span>w3</span>.<span>shape</span>, <span>w2</span>.<span>shape</span>)

<span># output = (silu(XW1) * XW3)W2</span>
<span># [17x4096] x [4096x14336] x [14336x4096] = [17x4096]</span>
<span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
<span>output_after_feedforward</span>.<span>shape</span></pre></div>
<div data-snippet-clipboard-copy-content="torch.Size([14336, 4096]) torch.Size([14336, 4096]) torch.Size([4096, 14336]) torch.Size([17, 4096])"><pre><code>torch.Size([14336, 4096]) torch.Size([14336, 4096]) torch.Size([4096, 14336]) torch.Size([17, 4096])
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Perform the residual operation again (Finally, we get the final output of the Transformer block!)</h2><a id="user-content-perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block" aria-label="Permalink: Perform the residual operation again (Finally, we get the final output of the Transformer block!)" href="#perform-the-residual-operation-again-finally-we-get-the-final-output-of-the-transformer-block"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add the output of the feed-forward layer to the original input to complete the residual operation
# This is the final result of a Transformer block
layer_0_embedding = embedding_after_edit+output_after_feedforward  # [17x4096] + [17x4096] = [17x4096]
layer_0_embedding.shape"><pre><span># Add the output of the feed-forward layer to the original input to complete the residual operation</span>
<span># This is the final result of a Transformer block</span>
<span>layer_0_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
<span>layer_0_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">
Finally, we have the new embeddings of each token after passing through the first layer.
</h3><a id="user-content-finally-we-have-the-new-embeddings-of-each-token-after-passing-through-the-first-layer" aria-label="Permalink: 
Finally, we have the new embeddings of each token after passing through the first layer.
" href="#finally-we-have-the-new-embeddings-of-each-token-after-passing-through-the-first-layer"></a></p>
<br>
There are only 31 layers left to complete (just one for loop away).
<br>
You can imagine that this processed embedding contains all the information of the tokens proposed in the first layer.
<br>
Now, each layer will encode more complex queries in the asked question. Until the end, we will get an embedding that knows all the information about the next token we need.
<p dir="auto"><h2 tabindex="-1" dir="auto">Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)</h2><a id="user-content-everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-" aria-label="Permalink: Everything is here. Let's complete the calculation of all 32 Transformer blocks. Happy reading :)" href="#everything-is-here-lets-complete-the-calculation-of-all-32-transformer-blocks-happy-reading-"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/god.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/god.png" width="600px"></a>
</p>
<p dir="auto">Yes, that's it. All the work we've done before will be presented here at once to complete the calculation of each layer.
<br></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Now, let's start to complete the calculation of all 32 Transformer blocks!

# Use the embeddings of the input tokens as the initial input.
final_embedding = token_embeddings_unnormalized  # [17x4096]

# Perform layer-by-layer calculation for the 32-layer Transformer blocks
for layer in range(n_layers):
    #########################################################################################################################
    ################### Round 1: Normalization - Feature Transformation - Residual Operation ###############################
    
    ########################### The first normalization ###################################################
    
    # The first normalization
    layer_embedding_norm = rms_norm(final_embedding, model[f&quot;layers.{layer}.attention_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
    
    ################ The first feature transformation - Multi-Head Self-Attention ########################
    
    # Obtain the qkv weight matrix of the attention mechanism for the current layer
    q_layer = model[f&quot;layers.{layer}.attention.wq.weight&quot;]  # [4096x4096]
    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)  # [32x128x4096]
    k_layer = model[f&quot;layers.{layer}.attention.wk.weight&quot;]  # [1024x4096]
    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)  # [8x128x4096]
    v_layer = model[f&quot;layers.{layer}.attention.wv.weight&quot;]  # [1024x4096]
    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)  # [8x128x4096]
    
    # Used to store the calculation results of the attention mechanism for each head
    qkv_attention_store = []
    
    # Calculate the attention mechanism results for each head
    for head in range(n_heads):
        # Extract the QKV weight matrices corresponding to the current head
        q_layer_head = q_layer[head]  # [32x128x4096] -> [128x4096]
        k_layer_head = k_layer[head//4]  # Every 4 heads share one key weight, [8x128x4096] -> [128x4096]
        v_layer_head = v_layer[head//4]  # Every 4 heads share one value weight, [8x128x4096] -> [128x4096]
        
        # Calculate XW to obtain the QKV vectors
        # [17x4096] x [4096x128] = [17x128]
        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)
        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)
        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)
        
        # Add position information to the query vector (RoPE)
        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
        q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)  # Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -> [17x128]
        
        # Add position information to the key vector (RoPE)
        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)  # Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -> [17x64x2]
        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)  # Convert to complex number representation, (x,y) -> (x+yi). [17x64x2] -> [17x64]
        k_per_token_as_complex_numbers_rotated = k_per_token_as_complex_numbers * freqs_cis  # Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]
        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers_rotated)  # Convert the result back to real number representation, (x+yi) -> (x,y). [17x64] -> [17x64x2]
        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)  # Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -> [17x128]
        
        # Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))
        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5  # [17x128] x [128x17] = [17x17]
        
        # Mask the scores of future tokens
        mask = torch.full(qk_per_token.shape, float(&quot;-inf&quot;), device=qk_per_token.device)  # Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]
        mask = torch.triu(mask, diagonal=1)  # Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]
        qk_per_token_after_masking = qk_per_token + mask  # Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]
        
        # Calculate the attention weights (i.e., softmax(score))
        # Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).
        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)  # Calculate the softmax row-by-row. [17x17]
        
        # Calculate the final result of the attention mechanism (i.e., softmax(score) × V)
        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)  # [17x17] x [17x128] = [17x128]
        
        # Record the result of this head
        qkv_attention_store.append(qkv_attention)
    
    # Merge the multi-head attention results
    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)  # Merge the second dimension, that is, 32x[17x128] -> [17x4096]
    
    # Perform a linear mapping on the results to generate the final multi-head self-attention mechanism results
    o_layer = model[f&quot;layers.{layer}.attention.wo.weight&quot;]
    embedding_delta = torch.matmul(stacked_qkv_attention, o_layer.T)  # [17x4096] x [4096x4096] = [17x4096]

    ########################### The first residual operation ##############################################
    
    # The first Residual Operation
    # Add the output of the attention layer to the original input to complete the residual operation
    embedding_after_edit = final_embedding + embedding_delta  # [17x4096] + [17x4096] = [17x4096]
    
    
    #########################################################################################################################
    #################### Round 2: Normalization - Feature Transformation - Residual Operation ##############################
    
    ########################### The second normalization ##################################################
    
    # The second normalization
    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&quot;layers.{layer}.ffn_norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
    
    ################## The second feature transformation - Feed-Forward Network ##########################
    
    # Load the parameter matrix of the feed-forward network (SwiGLU)
    w1 = model[f&quot;layers.{layer}.feed_forward.w1.weight&quot;]  # [14336x4096]
    w3 = model[f&quot;layers.{layer}.feed_forward.w3.weight&quot;]  # [14336x4096]
    w2 = model[f&quot;layers.{layer}.feed_forward.w2.weight&quot;]  # [4096x14336]
    
    # Calculate the results of the feed-forward network (output = (silu(XW1) * XW3)W2)
    # [17x4096] x [4096x14336] x [14336x4096] = [17x4096]
    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)
    
    ########################### The second residual operation ##############################################
    
    # The second residual operation, obtain the final output result of the current Transformer block
    # Add the output of the feed-forward layer to the original input to complete the residual operation
    final_embedding = embedding_after_edit+output_after_feedforward  # [17x4096] + [17x4096] = [17x4096]"><pre><span># Now, let's start to complete the calculation of all 32 Transformer blocks!</span>

<span># Use the embeddings of the input tokens as the initial input.</span>
<span>final_embedding</span> <span>=</span> <span>token_embeddings_unnormalized</span>  <span># [17x4096]</span>

<span># Perform layer-by-layer calculation for the 32-layer Transformer blocks</span>
<span>for</span> <span>layer</span> <span>in</span> <span>range</span>(<span>n_layers</span>):
    <span>#########################################################################################################################</span>
    <span>################### Round 1: Normalization - Feature Transformation - Residual Operation ###############################</span>
    
    <span>########################### The first normalization ###################################################</span>
    
    <span># The first normalization</span>
    <span>layer_embedding_norm</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
    
    <span>################ The first feature transformation - Multi-Head Self-Attention ########################</span>
    
    <span># Obtain the qkv weight matrix of the attention mechanism for the current layer</span>
    <span>q_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wq.weight"</span>]  <span># [4096x4096]</span>
    <span>q_layer</span> <span>=</span> <span>q_layer</span>.<span>view</span>(<span>n_heads</span>, <span>q_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_heads</span>, <span>dim</span>)  <span># [32x128x4096]</span>
    <span>k_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wk.weight"</span>]  <span># [1024x4096]</span>
    <span>k_layer</span> <span>=</span> <span>k_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>k_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [8x128x4096]</span>
    <span>v_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wv.weight"</span>]  <span># [1024x4096]</span>
    <span>v_layer</span> <span>=</span> <span>v_layer</span>.<span>view</span>(<span>n_kv_heads</span>, <span>v_layer</span>.<span>shape</span>[<span>0</span>] <span>//</span> <span>n_kv_heads</span>, <span>dim</span>)  <span># [8x128x4096]</span>
    
    <span># Used to store the calculation results of the attention mechanism for each head</span>
    <span>qkv_attention_store</span> <span>=</span> []
    
    <span># Calculate the attention mechanism results for each head</span>
    <span>for</span> <span>head</span> <span>in</span> <span>range</span>(<span>n_heads</span>):
        <span># Extract the QKV weight matrices corresponding to the current head</span>
        <span>q_layer_head</span> <span>=</span> <span>q_layer</span>[<span>head</span>]  <span># [32x128x4096] -&gt; [128x4096]</span>
        <span>k_layer_head</span> <span>=</span> <span>k_layer</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one key weight, [8x128x4096] -&gt; [128x4096]</span>
        <span>v_layer_head</span> <span>=</span> <span>v_layer</span>[<span>head</span><span>//</span><span>4</span>]  <span># Every 4 heads share one value weight, [8x128x4096] -&gt; [128x4096]</span>
        
        <span># Calculate XW to obtain the QKV vectors</span>
        <span># [17x4096] x [4096x128] = [17x128]</span>
        <span>q_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>q_layer_head</span>.<span>T</span>)
        <span>k_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>k_layer_head</span>.<span>T</span>)
        <span>v_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>layer_embedding_norm</span>, <span>v_layer_head</span>.<span>T</span>)
        
        <span># Add position information to the query vector (RoPE)</span>
        <span>q_per_token_split_into_pairs</span> <span>=</span> <span>q_per_token</span>.<span>float</span>().<span>view</span>(<span>q_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
        <span>q_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>q_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
        <span>q_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>q_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
        <span>q_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>q_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
        <span>q_per_token_rotated</span> <span>=</span> <span>q_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>q_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final query vector. [17x64x2] -&gt; [17x128]</span>
        
        <span># Add position information to the key vector (RoPE)</span>
        <span>k_per_token_split_into_pairs</span> <span>=</span> <span>k_per_token</span>.<span>float</span>().<span>view</span>(<span>k_per_token</span>.<span>shape</span>[<span>0</span>], <span>-</span><span>1</span>, <span>2</span>)  <span># Divide vector into pairs along the dimensions direction to form dimension pairs. [17x128] -&gt; [17x64x2]</span>
        <span>k_per_token_as_complex_numbers</span> <span>=</span> <span>torch</span>.<span>view_as_complex</span>(<span>k_per_token_split_into_pairs</span>)  <span># Convert to complex number representation, (x,y) -&gt; (x+yi). [17x64x2] -&gt; [17x64]</span>
        <span>k_per_token_as_complex_numbers_rotated</span> <span>=</span> <span>k_per_token_as_complex_numbers</span> <span>*</span> <span>freqs_cis</span>  <span># Calculate (x+yi)*(cosmθ+sinmθi) to complete the rotation operation. [17x64] * [17x64] = [17x64]</span>
        <span>k_per_token_split_into_pairs_rotated</span> <span>=</span> <span>torch</span>.<span>view_as_real</span>(<span>k_per_token_as_complex_numbers_rotated</span>)  <span># Convert the result back to real number representation, (x+yi) -&gt; (x,y). [17x64] -&gt; [17x64x2]</span>
        <span>k_per_token_rotated</span> <span>=</span> <span>k_per_token_split_into_pairs_rotated</span>.<span>view</span>(<span>k_per_token</span>.<span>shape</span>)  <span># Convert the result back to the original vector shape to obtain the final key vector. [17x64x2] -&gt; [17x128]</span>
        
        <span># Calculate the attention scores and normalize the scores simultaneously (i.e., Q×K/sqrt(dim))</span>
        <span>qk_per_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>q_per_token_rotated</span>, <span>k_per_token_rotated</span>.<span>T</span>)<span>/</span>(<span>128</span>)<span>**</span><span>0.5</span>  <span># [17x128] x [128x17] = [17x17]</span>
        
        <span># Mask the scores of future tokens</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>full</span>(<span>qk_per_token</span>.<span>shape</span>, <span>float</span>(<span>"-inf"</span>), <span>device</span><span>=</span><span>qk_per_token</span>.<span>device</span>)  <span># Create a matrix with the same shape as the attention scores, filled with negative infinity, and stored in the same device as other vectors to prevent errors in subsequent calculations. [17x17]</span>
        <span>mask</span> <span>=</span> <span>torch</span>.<span>triu</span>(<span>mask</span>, <span>diagonal</span><span>=</span><span>1</span>)  <span># Keep the negative infinity in the upper-triangular part and set others to 0 (i.e., the upper-triangular area represents future tokens that need to be masked). The diagonal offset is 1 to avoid masking the token itself. [17x17]</span>
        <span>qk_per_token_after_masking</span> <span>=</span> <span>qk_per_token</span> <span>+</span> <span>mask</span>  <span># Add the attention scores with the masking matrix, making the upper-triangular part of the score matrix become negative infinity, which will tend to 0 after the subsequent softmax operation. [17x17]</span>
        
        <span># Calculate the attention weights (i.e., softmax(score))</span>
        <span># Meanwhile, convert it back to half-precision (because it will be multiplied with the value vector v_per_token later, so the data types need to be the same).</span>
        <span>qk_per_token_after_masking_after_softmax</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>functional</span>.<span>softmax</span>(<span>qk_per_token_after_masking</span>, <span>dim</span><span>=</span><span>1</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)  <span># Calculate the softmax row-by-row. [17x17]</span>
        
        <span># Calculate the final result of the attention mechanism (i.e., softmax(score) × V)</span>
        <span>qkv_attention</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>qk_per_token_after_masking_after_softmax</span>, <span>v_per_token</span>)  <span># [17x17] x [17x128] = [17x128]</span>
        
        <span># Record the result of this head</span>
        <span>qkv_attention_store</span>.<span>append</span>(<span>qkv_attention</span>)
    
    <span># Merge the multi-head attention results</span>
    <span>stacked_qkv_attention</span> <span>=</span> <span>torch</span>.<span>cat</span>(<span>qkv_attention_store</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Merge the second dimension, that is, 32x[17x128] -&gt; [17x4096]</span>
    
    <span># Perform a linear mapping on the results to generate the final multi-head self-attention mechanism results</span>
    <span>o_layer</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.attention.wo.weight"</span>]
    <span>embedding_delta</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>stacked_qkv_attention</span>, <span>o_layer</span>.<span>T</span>)  <span># [17x4096] x [4096x4096] = [17x4096]</span>

    <span>########################### The first residual operation ##############################################</span>
    
    <span># The first Residual Operation</span>
    <span># Add the output of the attention layer to the original input to complete the residual operation</span>
    <span>embedding_after_edit</span> <span>=</span> <span>final_embedding</span> <span>+</span> <span>embedding_delta</span>  <span># [17x4096] + [17x4096] = [17x4096]</span>
    
    
    <span>#########################################################################################################################</span>
    <span>#################### Round 2: Normalization - Feature Transformation - Residual Operation ##############################</span>
    
    <span>########################### The second normalization ##################################################</span>
    
    <span># The second normalization</span>
    <span>embedding_after_edit_normalized</span> <span>=</span> <span>rms_norm</span>(<span>embedding_after_edit</span>, <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.ffn_norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
    
    <span>################## The second feature transformation - Feed-Forward Network ##########################</span>
    
    <span># Load the parameter matrix of the feed-forward network (SwiGLU)</span>
    <span>w1</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w1.weight"</span>]  <span># [14336x4096]</span>
    <span>w3</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w3.weight"</span>]  <span># [14336x4096]</span>
    <span>w2</span> <span>=</span> <span>model</span>[<span>f"layers.<span><span>{</span><span>layer</span><span>}</span></span>.feed_forward.w2.weight"</span>]  <span># [4096x14336]</span>
    
    <span># Calculate the results of the feed-forward network (output = (silu(XW1) * XW3)W2)</span>
    <span># [17x4096] x [4096x14336] x [14336x4096] = [17x4096]</span>
    <span>output_after_feedforward</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>torch</span>.<span>functional</span>.<span>F</span>.<span>silu</span>(<span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w1</span>.<span>T</span>)) <span>*</span> <span>torch</span>.<span>matmul</span>(<span>embedding_after_edit_normalized</span>, <span>w3</span>.<span>T</span>), <span>w2</span>.<span>T</span>)
    
    <span>########################### The second residual operation ##############################################</span>
    
    <span># The second residual operation, obtain the final output result of the current Transformer block</span>
    <span># Add the output of the feed-forward layer to the original input to complete the residual operation</span>
    <span>final_embedding</span> <span>=</span> <span>embedding_after_edit</span><span>+</span><span>output_after_feedforward</span>  <span># [17x4096] + [17x4096] = [17x4096]</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Let's complete the last step and predict the next token</h2><a id="user-content-lets-complete-the-last-step-and-predict-the-next-token" aria-label="Permalink: Let's complete the last step and predict the next token" href="#lets-complete-the-last-step-and-predict-the-next-token"></a></p>
<p dir="auto">Now we have obtained the final embeddings, which contains all the information we needed to predict the next token.
<br>
The shape of this embedding is the same as that of the input token embedding, both being [17x4096], where 17 is the number of tokens and 4096 is the dimension of the embedding.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/last_norm.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/last_norm.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">First, perform one last normalization on the output of the last Transformer layer</h2><a id="user-content-first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer" aria-label="Permalink: First, perform one last normalization on the output of the last Transformer layer" href="#first-perform-one-last-normalization-on-the-output-of-the-last-transformer-layer"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the last normalization in the entire model
final_embedding = rms_norm(final_embedding, model[&quot;norm.weight&quot;])  # [17x4096] &amp; [4096] -> [17x4096]
final_embedding.shape"><pre><span># Perform the last normalization in the entire model</span>
<span>final_embedding</span> <span>=</span> <span>rms_norm</span>(<span>final_embedding</span>, <span>model</span>[<span>"norm.weight"</span>])  <span># [17x4096] &amp; [4096] -&gt; [17x4096]</span>
<span>final_embedding</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)</h2><a id="user-content-then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension" aria-label="Permalink: Then, make the prediction based on the embedding corresponding to the last token (perform a linear mapping to the vocabulary dimension)" href="#then-make-the-prediction-based-on-the-embedding-corresponding-to-the-last-token-perform-a-linear-mapping-to-the-vocabulary-dimension"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/finallayer.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/finallayer.png" width="600px"></a>
</p>
<br>
We will use the output decoder (a linear mapping layer) to convert the embedding vector of the last token into a prediction result for the next token (the dimension is the size of the vocabulary. If we apply a softmax function to the result, the value of each dimension represents the probability that the next token belongs to that word).
<div dir="auto"><p>Why do we only use the output vector of the last token to predict the next token?
<br>
Because during training, the model's objective is to predict the next token based on the current token and all previous tokens. Therefore, the output vector corresponding to each token is used to predict the next token relative to itself, rather than the next token for the entire input.
</p></div>
<p dir="auto">We hope the answer is 42 in our example :)
<br>
Note: 42 is the answer to "the answer to the ultimate question of life, the universe, and everything is " according to the book <em>The Hitchhiker's Guide to the Galaxy</em>. Most modern large language models will answer 42, which will verify the correctness of our entire code! Good luck to us :)</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Perform the last linear mapping to map the embeddings to the size of the vocabulary dimension as a prediction for the next token
logits = torch.matmul(final_embedding[-1], model[&quot;output.weight&quot;].T)  # [17x4096] -> [4096] -> [4096] x [4096x128256] = [128256]
logits.shape"><pre><span># Perform the last linear mapping to map the embeddings to the size of the vocabulary dimension as a prediction for the next token</span>
<span>logits</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>[<span>-</span><span>1</span>], <span>model</span>[<span>"output.weight"</span>].<span>T</span>)  <span># [17x4096] -&gt; [4096] -&gt; [4096] x [4096x128256] = [128256]</span>
<span>logits</span>.<span>shape</span></pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Here's the prediction result!</h2><a id="user-content-heres-the-prediction-result" aria-label="Permalink: Here's the prediction result!" href="#heres-the-prediction-result"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Extract the id corresponding to the dimension with the highest probability,
# is gonna be the predicted next token's id
next_token = torch.argmax(logits, dim=-1)  # Get the index corresponding to the maximum value, which is the predicted next token id. [128256] -> [1]
next_token"><pre><span># Extract the id corresponding to the dimension with the highest probability,</span>
<span># is gonna be the predicted next token's id</span>
<span>next_token</span> <span>=</span> <span>torch</span>.<span>argmax</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>)  <span># Get the index corresponding to the maximum value, which is the predicted next token id. [128256] -&gt; [1]</span>
<span>next_token</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Based on the predicted id, restore it to the specific predicted value
tokenizer.decode([next_token.item()])"><pre><span># Based on the predicted id, restore it to the specific predicted value</span>
<span>tokenizer</span>.<span>decode</span>([<span>next_token</span>.<span>item</span>()])</pre></div>

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/42.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/42.png" width="600px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)</h2><a id="user-content-lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-" aria-label="Permalink: Let's dive deeper and see how different embeddings or token masking strategies might affect the prediction results :)" href="#lets-dive-deeper-and-see-how-different-embeddings-or-token-masking-strategies-might-affect-the-prediction-results-"></a></p>
<p dir="auto">Now we've got the final prediction results. If you're still interested, let's explore some of the issues that might have been mentioned before~
<br></p>
<p dir="auto">We'll briefly explore three scenarios:</p>
<ol dir="auto">
<li>Apart from the top-1 result, what else is predicted in the current prediction, that is, the top-k results?</li>
<li>What can be predicted if we use the output embedding of other tokens for prediction?</li>
<li>If the future tokens were not masked during the attention calculation before, how would the prediction results differ?</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Let's first take a look at the top-k prediction results
logits_sort, logits_idx = torch.sort(logits, dim=-1, descending=True)  # Put the token with the highest probability prediction at the front, [128256]
[tokenizer.decode([i]) for i in logits_idx[:10]]  # View the top 10 high-probability results"><pre><span># Let's first take a look at the top-k prediction results</span>
<span>logits_sort</span>, <span>logits_idx</span> <span>=</span> <span>torch</span>.<span>sort</span>(<span>logits</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>, <span>descending</span><span>=</span><span>True</span>)  <span># Put the token with the highest probability prediction at the front, [128256]</span>
[<span>tokenizer</span>.<span>decode</span>([<span>i</span>]) <span>for</span> <span>i</span> <span>in</span> <span>logits_idx</span>[:<span>10</span>]]  <span># View the top 10 high-probability results</span></pre></div>
<div data-snippet-clipboard-copy-content="['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']"><pre><code>['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Next, let's to see what can we get by using the embeddings of other tokens for prediction
logits_all_token = torch.matmul(final_embedding, model[&quot;output.weight&quot;].T)  # Map the embeddings to the same size as the vocabulary, [17x4096] x [4096x128256] = [17x128256]
logits_all_token_sort, logits_all_token_idx = torch.sort(logits_all_token, dim=-1, descending=True)  # Put the token with the highest probability prediction at the front, [17x128256]

print('Input tokens:', prompt_split_as_tokens)  # Display the input tokens, [17]

# Display the results of the next-token prediction based on the output embedding of each token
for i in range(len(final_embedding)):
    print(f'Predict results based on {i+1}th token:', [tokenizer.decode([j]) for j in logits_all_token_idx[i][:10]])  # Output the top 10 high-probability results
    
_=&quot;&quot;&quot;
It can be seen that when making predictions based on each token, the prediction result is the possible result of the next token after the &quot;current token&quot;,
rather than the prediction result of the entire complete input.
Therefore, in actual prediction, only the embedding of the last token will be used for prediction.
&quot;&quot;&quot;"><pre><span># Next, let's to see what can we get by using the embeddings of other tokens for prediction</span>
<span>logits_all_token</span> <span>=</span> <span>torch</span>.<span>matmul</span>(<span>final_embedding</span>, <span>model</span>[<span>"output.weight"</span>].<span>T</span>)  <span># Map the embeddings to the same size as the vocabulary, [17x4096] x [4096x128256] = [17x128256]</span>
<span>logits_all_token_sort</span>, <span>logits_all_token_idx</span> <span>=</span> <span>torch</span>.<span>sort</span>(<span>logits_all_token</span>, <span>dim</span><span>=</span><span>-</span><span>1</span>, <span>descending</span><span>=</span><span>True</span>)  <span># Put the token with the highest probability prediction at the front, [17x128256]</span>

<span>print</span>(<span>'Input tokens:'</span>, <span>prompt_split_as_tokens</span>)  <span># Display the input tokens, [17]</span>

<span># Display the results of the next-token prediction based on the output embedding of each token</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>len</span>(<span>final_embedding</span>)):
    <span>print</span>(<span>f'Predict results based on <span><span>{</span><span>i</span><span>+</span><span>1</span><span>}</span></span>th token:'</span>, [<span>tokenizer</span>.<span>decode</span>([<span>j</span>]) <span>for</span> <span>j</span> <span>in</span> <span>logits_all_token_idx</span>[<span>i</span>][:<span>10</span>]])  <span># Output the top 10 high-probability results</span>
    
<span>_</span><span>=</span><span>"""</span>
<span>It can be seen that when making predictions based on each token, the prediction result is the possible result of the next token after the "current token",</span>
<span>rather than the prediction result of the entire complete input.</span>
<span>Therefore, in actual prediction, only the embedding of the last token will be used for prediction.</span>
<span>"""</span></pre></div>
<div data-snippet-clipboard-copy-content="Input tokens: ['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['Question', 'def', '#', 'The', 'import', 'Tags', 'A', 'package', 'Home', 'I']
Predict results based on 2th token: [' ', ' best', ' first', ' most', ' new', ' world', ' last', ' same', ' way', ' number']
Predict results based on 3th token: [' to', ' is', ' was', ' of', ' lies', ',', ' for', ' you', ' key', ' will']
Predict results based on 4th token: [' the', ' this', ' your', ' all', ' that', ' a', ' my', ' life', ' &quot;', ' everything']
Predict results based on 5th token: [' question', ' problem', ' above', ' ultimate', ' first', ' r', ' following', ' questions', ' most', ' previous']
Predict results based on 6th token: [' question', ' questions', ' mystery', '\xa0', ' quest', '\n', ' life', ' philosophical', ' qu', ' problem']
Predict results based on 7th token: [' of', '\n', ' to', ' is', '?\n', ',', '.\n', ':', '...\n', ' about']
Predict results based on 8th token: [' life', ' Life', ' the', '\xa0', ' everything', ' existence', '\n', ' LIFE', ' all', ' human']
Predict results based on 9th token: [',', ' the', '\n', ' and', ' is', ',\n', '.\n', '?\n', '...\n', '...']
Predict results based on 10th token: [' the', ' universe', ' and', ' etc', '\xa0', ' is', ' death', ' of', ' or', ' everything']
Predict results based on 11th token: [' universe', ' Universe', '\n', '\xa0', ' un', ' univers', ' uni', ' cosmos', ' universal', ' u']
Predict results based on 12th token: [',', ' and', ' &amp;', '\n', ',\n', ' ,', '...\n', ',and', '...', '\xa0']
Predict results based on 13th token: [' and', ' everything', ' &amp;', ' the', ' etc', '\xa0', ' is', ' or', ' ...\n', ' an']
Predict results based on 14th token: [' everything', '\xa0', ' the', ' every', '\n', ' ever', ' all', ' Everything', ' EVERY', '...']
Predict results based on 15th token: ['\n', ' is', '.\n', '.', '?\n', ',', ' (', '\n\n', '...\n', ' in']
Predict results based on 16th token: [' ', '\n', '...', '...\n', ':', ' forty', ' not', ' &quot;', '…', ' a']
Predict results based on 17th token: ['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']"><pre><code>Input tokens: ['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['Question', 'def', '#', 'The', 'import', 'Tags', 'A', 'package', 'Home', 'I']
Predict results based on 2th token: [' ', ' best', ' first', ' most', ' new', ' world', ' last', ' same', ' way', ' number']
Predict results based on 3th token: [' to', ' is', ' was', ' of', ' lies', ',', ' for', ' you', ' key', ' will']
Predict results based on 4th token: [' the', ' this', ' your', ' all', ' that', ' a', ' my', ' life', ' "', ' everything']
Predict results based on 5th token: [' question', ' problem', ' above', ' ultimate', ' first', ' r', ' following', ' questions', ' most', ' previous']
Predict results based on 6th token: [' question', ' questions', ' mystery', '\xa0', ' quest', '\n', ' life', ' philosophical', ' qu', ' problem']
Predict results based on 7th token: [' of', '\n', ' to', ' is', '?\n', ',', '.\n', ':', '...\n', ' about']
Predict results based on 8th token: [' life', ' Life', ' the', '\xa0', ' everything', ' existence', '\n', ' LIFE', ' all', ' human']
Predict results based on 9th token: [',', ' the', '\n', ' and', ' is', ',\n', '.\n', '?\n', '...\n', '...']
Predict results based on 10th token: [' the', ' universe', ' and', ' etc', '\xa0', ' is', ' death', ' of', ' or', ' everything']
Predict results based on 11th token: [' universe', ' Universe', '\n', '\xa0', ' un', ' univers', ' uni', ' cosmos', ' universal', ' u']
Predict results based on 12th token: [',', ' and', ' &amp;', '\n', ',\n', ' ,', '...\n', ',and', '...', '\xa0']
Predict results based on 13th token: [' and', ' everything', ' &amp;', ' the', ' etc', '\xa0', ' is', ' or', ' ...\n', ' an']
Predict results based on 14th token: [' everything', '\xa0', ' the', ' every', '\n', ' ever', ' all', ' Everything', ' EVERY', '...']
Predict results based on 15th token: ['\n', ' is', '.\n', '.', '?\n', ',', ' (', '\n\n', '...\n', ' in']
Predict results based on 16th token: [' ', '\n', '...', '...\n', ':', ' forty', ' not', ' "', '…', ' a']
Predict results based on 17th token: ['42', '6', '43', '41', '4', '1', '45', '3', '2', '46']
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Finally, let's take a look at what the prediction results will be if we don't mask future tokens when calculating attention
# At this time, the prediction results based on each token will be as follows
# It can be seen that due to the visibility of future tokens, the embeddings of each token will more accurately predict &quot;the next token for it&quot; (it's a bit like &quot;cheating&quot;) 

_=&quot;&quot;&quot;
Input tokens: ['<|begin_of_text|>', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']
Predict results based on 1th token: ['://', '.Forms', '_REF', ' Angeles', '.swing', '�', 'php', 'во', 'ysics', '�']
Predict results based on 2th token: [' answer', ' Hitch', ' universe', ' question', ' ultimate', ' meaning', ' hitch', ' Universe', ' Answer', ' reason']
Predict results based on 3th token: [' to', ' is', ',', ':', ' was', '\n', ' ', ' (', '\n\n', ' of']
Predict results based on 4th token: [' the', ' life', ' this', ' which', ' everything', ' that', ' how', ' why', ' ', ' all']
Predict results based on 5th token: [' ultimate', ' question', ' great', ' meaning', ' universe', ' Ultimate', ' everything', ' life', ' holy', ' greatest']
Predict results based on 6th token: [' question', ' answer', ' is', ' was', '\n', ' questions', ' mystery', '\n\n', ' what', ' Question']
Predict results based on 7th token: [' of', ' is', '\n', ',', ' about', ':', ' to', ' in', ' (', '<|end_of_text|>']
Predict results based on 8th token: [' life', ' existence', ' everything', ' Life', ' the', ' death', ' time', ' all', ' why', ' which']
Predict results based on 9th token: [',', ' is', ' the', '\n', ':', ' (', '...', ' and', ' ,', ' -']
Predict results based on 10th token: [' the', ' and', ' is', ' death', ' The', ' which', ' or', '\xa0', ' existence', ' don']
Predict results based on 11th token: [' universe', ' answer', ' cosmos', ' world', ' existence', ' Universe', ' everything', ' un', ' meaning', ' question']
Predict results based on 12th token: [',', ' and', ' is', ' &amp;', '\n', ' ,', '.', '...', ' (', ' ']
Predict results based on 13th token: [' and', ' &amp;', ' don', ' the', ' is', ' a', ' or', ' Douglas', '\xa0', '<|end_of_text|>']
Predict results based on 14th token: [' everything', ' dough', ' don', ' ever', ' deep', ' Douglas', ' the', ' every', ' all', ' death']
Predict results based on 15th token: ['\n', ' is', ',', '.', ' ', ' (', ':', '<|end_of_text|>', '\n\n', '.\n']
Predict results based on 16th token: [' ', '\n', ' forty', '...', ' &quot;', '42', ' the', ':', '\xa0', ' to']
Predict results based on 17th token: ['42', '6', '4', '41', '1', '2', '3', '7', '5', '43']
&quot;&quot;&quot;"><pre><span># Finally, let's take a look at what the prediction results will be if we don't mask future tokens when calculating attention</span>
<span># At this time, the prediction results based on each token will be as follows</span>
<span># It can be seen that due to the visibility of future tokens, the embeddings of each token will more accurately predict "the next token for it" (it's a bit like "cheating") </span>

<span>_</span><span>=</span><span>"""</span>
<span>Input tokens: ['&lt;|begin_of_text|&gt;', 'the', ' answer', ' to', ' the', ' ultimate', ' question', ' of', ' life', ',', ' the', ' universe', ',', ' and', ' everything', ' is', ' ']</span>
<span>Predict results based on 1th token: ['://', '.Forms', '_REF', ' Angeles', '.swing', '�', 'php', 'во', 'ysics', '�']</span>
<span>Predict results based on 2th token: [' answer', ' Hitch', ' universe', ' question', ' ultimate', ' meaning', ' hitch', ' Universe', ' Answer', ' reason']</span>
<span>Predict results based on 3th token: [' to', ' is', ',', ':', ' was', '<span>\n</span>', ' ', ' (', '<span>\n</span><span>\n</span>', ' of']</span>
<span>Predict results based on 4th token: [' the', ' life', ' this', ' which', ' everything', ' that', ' how', ' why', ' ', ' all']</span>
<span>Predict results based on 5th token: [' ultimate', ' question', ' great', ' meaning', ' universe', ' Ultimate', ' everything', ' life', ' holy', ' greatest']</span>
<span>Predict results based on 6th token: [' question', ' answer', ' is', ' was', '<span>\n</span>', ' questions', ' mystery', '<span>\n</span><span>\n</span>', ' what', ' Question']</span>
<span>Predict results based on 7th token: [' of', ' is', '<span>\n</span>', ',', ' about', ':', ' to', ' in', ' (', '&lt;|end_of_text|&gt;']</span>
<span>Predict results based on 8th token: [' life', ' existence', ' everything', ' Life', ' the', ' death', ' time', ' all', ' why', ' which']</span>
<span>Predict results based on 9th token: [',', ' is', ' the', '<span>\n</span>', ':', ' (', '...', ' and', ' ,', ' -']</span>
<span>Predict results based on 10th token: [' the', ' and', ' is', ' death', ' The', ' which', ' or', '<span>\xa0</span>', ' existence', ' don']</span>
<span>Predict results based on 11th token: [' universe', ' answer', ' cosmos', ' world', ' existence', ' Universe', ' everything', ' un', ' meaning', ' question']</span>
<span>Predict results based on 12th token: [',', ' and', ' is', ' &amp;', '<span>\n</span>', ' ,', '.', '...', ' (', ' ']</span>
<span>Predict results based on 13th token: [' and', ' &amp;', ' don', ' the', ' is', ' a', ' or', ' Douglas', '<span>\xa0</span>', '&lt;|end_of_text|&gt;']</span>
<span>Predict results based on 14th token: [' everything', ' dough', ' don', ' ever', ' deep', ' Douglas', ' the', ' every', ' all', ' death']</span>
<span>Predict results based on 15th token: ['<span>\n</span>', ' is', ',', '.', ' ', ' (', ':', '&lt;|end_of_text|&gt;', '<span>\n</span><span>\n</span>', '.<span>\n</span>']</span>
<span>Predict results based on 16th token: [' ', '<span>\n</span>', ' forty', '...', ' "', '42', ' the', ':', '<span>\xa0</span>', ' to']</span>
<span>Predict results based on 17th token: ['42', '6', '4', '41', '1', '2', '3', '7', '5', '43']</span>
<span>"""</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)</h2><a id="user-content-need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz" aria-label="Permalink: Need to predict multiple tokens? Just using KV-Cache! (It really took me a lot of effort to sort this out. Orz)" href="#need-to-predict-multiple-tokens-just-using-kv-cache-it-really-took-me-a-lot-of-effort-to-sort-this-out-orz"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to Continuously Predict Multiple Tokens</h3><a id="user-content-how-to-continuously-predict-multiple-tokens" aria-label="Permalink: How to Continuously Predict Multiple Tokens" href="#how-to-continuously-predict-multiple-tokens"></a></p>
<div dir="auto"><p>Now, we've completed the prediction of the next word for the input text. But what if our expected output requires multiple tokens?
<br>
For example, in practical llm applications, models usually don't output just one word. Instead, they often output a passage of text, or even a very long text. How is this ability achieved?
<br>
Actually, it's quite simple. We just need to repeatedly call the llm's prediction process to gradually generate a complete sentence or paragraph.
<br>
This process is like "snowballing". Each time we predict a word, we add this word to the current input sequence, and then call the model again for a new round of prediction. The prediction stops when we encounter a stop symbol (a special token "&lt;|end_of_text|&gt;" in llama3) or reach the maximum length limit (a hyperparameter max_seq_len).
</p><p>
Does this sound inefficient? Yes!
<br>
That's why there are well-known caching mechanisms like KV-Cache. By caching the KV vectors of historical tokens, we can reduce the input and computational load, thus improving the inference efficiency.
<br>
Thanks to the caching mechanism, when we use a large model for inference, you may notice that waiting for the first token to be output is often the most time-consuming. But once the first token is output, the output speed of subsequent tokens will increase significantly.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advantages and Disadvantages of KV-Cache</h3><a id="user-content-advantages-and-disadvantages-of-kv-cache" aria-label="Permalink: Advantages and Disadvantages of KV-Cache" href="#advantages-and-disadvantages-of-kv-cache"></a></p>
<div dir="auto"><p><strong>Advantage</strong>: When continuously predicting, we only need to input the new token each time instead of the entire text sequence. This greatly improves the calculation speed during inference.
<br>
<strong>Disadvantage</strong>: Due to the caching mechanism, it will consume more memory resources during inference.
</p></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Principle Derivation of KV-Cache</h3><a id="user-content-principle-derivation-of-kv-cache" aria-label="Permalink: Principle Derivation of KV-Cache" href="#principle-derivation-of-kv-cache"></a></p>
<p dir="auto">KV-Cache comes from the observation and analysis of the above matrix calculation process. By analyzing the calculation process of each input token, we can find that in most calculation steps, the calculation of each token is actually relatively independent and rately involves interaction with other tokens. Only when calculating the attention mechanism will token-to-token interactions be involved, thus requiring the caching of historical KV vectors.
<br></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Here is the specific derivation logic of KV-Cache:</h3><a id="user-content-here-is-the-specific-derivation-logic-of-kv-cache" aria-label="Permalink: Here is the specific derivation logic of KV-Cache:" href="#here-is-the-specific-derivation-logic-of-kv-cache"></a></p>
<ol dir="auto">
<li><strong>Premise</strong>: To predict the next token, we only need to get the output result of the last token (just as we did in the prediction chapter).</li>
<li><strong>Non-attention parts only needs to calculate the new tokens</strong>: Except for the attention calculation, the calculations of all other parts are independent among tokens. So we only need to calculate the new tokens and don't need to input historical tokens (I'll expand the analysis below).</li>
<li><strong>Attention parts also only needs to calculate the new tokens</strong>: In the attention layer, due to the masking mechanism, the output results of historical tokens won't be affected by future new tokens. So their inputs and outputs at each layer are fixed, that is, the QKV vectors of historical tokens will not change because of the addition of new tokens. Thus, we only need to calculate the attention of the new tokens.</li>
<li><strong>Calculate the new token's attention mechanism</strong>: The attention layer is used to let the token obtain the context information of historical tokens. So, for each new token, we need to calculate the weighted sum using the value vectors of all tokens. Therefore, we need to store the values of historical tokens.</li>
<li><strong>Calculate the new token's attention weights</strong>: As known from point 4, we also need to obtain the importance information, i.e., weights, between the new tokens and historical tokens first. So we need to calculate the product of the key vectors of the new tokens with the key vectors of all tokens. Therefore, we need to store the keys of historical tokens.</li>
<li><strong>Acquisition of KV-Cache</strong>: As known from points 4 and 5, we need to store the KV vectors of historical tokens. Since the query vectors are not used, we don't need to store them. This is how the kv-cache came about.</li>
<li><strong>Efficiency of KV-Cache</strong>: As known from point 3, the historical KV vectors won't change. So they can be incrementally updated during the continuous prediction process without modifying the historical content. In this way, each time we predict, we only need to input and calculate the result of the newly added tokens instead of taking the complete sequence as input, thus greatly improving the inference efficiency.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Additional: Analysis of the Independence of Token Calculation in KV-Cache</h3><a id="user-content-additional-analysis-of-the-independence-of-token-calculation-in-kv-cache" aria-label="Permalink: Additional: Analysis of the Independence of Token Calculation in KV-Cache" href="#additional-analysis-of-the-independence-of-token-calculation-in-kv-cache"></a></p>
<p dir="auto"><strong>All components except the attention layer (no interaction among them)</strong>:</p>
<ol dir="auto">
<li><strong>Two times normalizations</strong>: Each token vector is normalized in its own feature dimension without using other tokens.</li>
<li><strong>Two times residual connections (add)</strong>: Each token vector adds its own output result to itself without using other tokens.</li>
<li><strong>Feed-forward network (FFN)</strong>: Each token vector is multiplied by the same weight matrices W1, W2, W3 to get the result, and other tokens are not used during this process. Imagine that if the number of input tokens is 17, the calculation of FFN can be simplified as: [17x4096] x [4096x14336] x [14336x4096] = [17x4096]. This is actually equivalent to inputting one token at a time and then concatenating the 17 results into a matrix, that is: 17 times ([1x4096] x [4096x14336] x [14336x4096] = [1x4096]) = 17x[1x4096] =&gt; [17x4096]. Therefore, when each token is calculated in the feed-forward layer, there is actually no interaction with other tokens.</li>
</ol>
<p dir="auto"><strong>Attention layer (only have one-way interaction between new tokens and historical tokens)</strong>:</p>
<ol dir="auto">
<li><strong>Calculate QKV vectors</strong>: Each token vector is multiplied by the same QKV weight matrices to get the result without using other tokens.</li>
<li><strong>Add positional information to QK vectors</strong>: Each token vector performs an independent rotation operation based on its own position without using the specific content of other tokens.</li>
<li><strong>Calculate attention weights</strong>: The attention weights represent the correlation between each token and every historical tokens preceding it, and are independent of future tokens. Therefore, the results of historical tokens are independent of new tokens. And new tokens need the key vector cache of historical tokens.</li>
<li><strong>Calculate the result of the attention mechanism</strong>: The attention mechanism calculates the weighted sum of value vectors based on attention weights. So, similar to the conclusion in the previous point, the results of historical tokens are also independent of new tokens. And new tokens need the value vector cache of historical tokens.
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Attention Calculation Process Based on KV-Cache</h3><a id="user-content-attention-calculation-process-based-on-kv-cache" aria-label="Permalink: Attention Calculation Process Based on KV-Cache" href="#attention-calculation-process-based-on-kv-cache"></a></p>
<p dir="auto">To clearly show the calculation process, we only derive the single-head scenario (the principle and process of extending it to the multi-head scenario are exactly the same as the previous multi-head attention implementation):</p>
<ol dir="auto">
<li>Assume that the historical input tokens are <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$S_1$</math-renderer> with a length of N. Based on KV-Cache, we will store the KV result matrix of each head. The shape of a single head is [Nxhead_dim] = [Nx128].</li>
<li>Assume that the newly added input tokens are <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$S_2$</math-renderer> with a length of M (it can be newly predicted tokens or the input of a new round of user dialogue or any other scenarios).</li>
<li>Calculate the QKV vectors of the new tokens: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="19d90f3d5e551008e8fb4bc953d823b8">$Q,K,V = S_2W_{Q,K,V}$</math-renderer> =&gt; [Mx4096] x [4096x128] = [Mx128].</li>
<li>Add positional information to the QK vectors: The positions of new tokens should start from N + 1, not from 0. [Mx128] -&gt; [Mx128].</li>
<li>Add the new KV values to the KV cache to get the updated KV matrix, that is, [Nx128] -&gt; [(N + M)x128].</li>
<li>Calculate the attention weights of the new tokens: Attention_weight = softmax(QK/sqrt(d) + mask) =&gt; [Mx128] x [128x(N + M)] = [Mx(N + M)].</li>
<li>Calculate the final result of the attention mechanism for the new tokens: Attention_weight x V =&gt; [Mx(N + M)] x [(N + M)x128] = [Mx128].</li>
<li>Concatenate the results of each head and perform a linear mapping to get the final output of the attention layer, with a shape of 32x[Mx128] -&gt; [Mx4096].
</li>
</ol>
<p dir="auto">Since our previous learning process has been quite comprehensive, we won't implement the code for the optimization scheme here (if you're interested, you can refer to the official code of Llama 3, which is relatively easy to implement). Just like the parallel calculation of multi-head attention mentioned before, knowing that the calculation process can be optimized is enough~</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thank you all. Thanks for your continuous learning. Love you all :)</h2><a id="user-content-thank-you-all-thanks-for-your-continuous-learning-love-you-all-" aria-label="Permalink: Thank you all. Thanks for your continuous learning. Love you all :)" href="#thank-you-all-thanks-for-your-continuous-learning-love-you-all-"></a></p>
<p dir="auto">Our learning has come to an end. I hope you have also enjoyed this reading process!</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">From Me</h2><a id="user-content-from-me" aria-label="Permalink: From Me" href="#from-me"></a></p>
<p dir="auto">If you've come across this work, thank you for your trust and for learning all the way to this point. I'm glad to be of help to you~
<br></p>
<p dir="auto">If you'd like to support my work</p>
<ol dir="auto">
<li>give it a star⭐~ :)</li>
<li>buy me a coffee~ <a href="https://ko-fi.com/therealoliver" rel="nofollow">https://ko-fi.com/therealoliver</a></li>
</ol>

<p dir="auto"><h2 tabindex="-1" dir="auto">From the author of predecessor project</h2><a id="user-content-from-the-author-of-predecessor-project" aria-label="Permalink: From the author of predecessor project" href="#from-the-author-of-predecessor-project"></a></p>
<p dir="auto">If you want to support my work</p>
<ol dir="auto">
<li>follow me on twitter <a href="https://twitter.com/naklecha" rel="nofollow">https://twitter.com/naklecha</a></li>
<li>or, buy me a coffee <a href="https://www.buymeacoffee.com/naklecha" rel="nofollow">https://www.buymeacoffee.com/naklecha</a></li>
</ol>
<p dir="auto">Honestly, if you made it this far you already made my day :)</p>
<p dir="auto">what motivates me?</p>
<p dir="auto">My friends and I are on a mission - to make research more accessible!
We created a research lab called A10 - <a href="http://aaaaaaaaaa.org/" rel="nofollow">AAAAAAAAAA.org</a></p>
<p dir="auto">A10 twitter - <a href="https://twitter.com/aaaaaaaaaaorg" rel="nofollow">https://twitter.com/aaaaaaaaaaorg</a></p>
<p dir="auto">our thesis:</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/images/a10.png"><img src="https://github.com/therealoliver/Deepdive-llama3-from-scratch/raw/main/images/a10.png" width="600px"></a>
</p>
<p>
Thanks again to the original author for the base code and illustrations, which also taught me a lot
</p><p dir="auto"><h2 tabindex="-1" dir="auto">LICENSE</h2><a id="user-content-license" aria-label="Permalink: LICENSE" href="#license"></a></p>
<p dir="auto">Copyright (c) 2025 Jinlong Zhang (<a href="https://github.com/therealoliver">https://github.com/therealoliver</a>)</p>
<p dir="auto">Copyright (c) 2024 Nishant Aklecha</p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Removing Jeff Bezos from my bed (552 pts)]]></title>
            <link>https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed</link>
            <guid>43129439</guid>
            <pubDate>Fri, 21 Feb 2025 16:27:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed">https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed</a>, See on <a href="https://news.ycombinator.com/item?id=43129439">Hacker News</a></p>
Couldn't get https://trufflesecurity.com/blog/removing-jeff-bezos-from-my-bed: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla recalls 380k vehicles in US over power steering assist issue (179 pts)]]></title>
            <link>https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/</link>
            <guid>43128987</guid>
            <pubDate>Fri, 21 Feb 2025 15:57:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/">https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/</a>, See on <a href="https://news.ycombinator.com/item?id=43128987">Hacker News</a></p>
Couldn't get https://www.reuters.com/business/autos-transportation/tesla-recalls-380000-vehicles-us-over-power-steering-issue-2025-02-21/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Apple pulls encrypted iCloud security feature in UK amid backdoor demands (208 pts)]]></title>
            <link>https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/</link>
            <guid>43128870</guid>
            <pubDate>Fri, 21 Feb 2025 15:49:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/">https://www.macrumors.com/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/</a>, See on <a href="https://news.ycombinator.com/item?id=43128870">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/02/21/apple-pulls-encrypted-icloud-security-feature-uk/"><p>Apple has withdrawn its Advanced Data Protection iCloud feature from the United Kingdom following government demands for backdoor access to encrypted user data, according to <a href="https://www.bloomberg.com/news/articles/2025-02-21/apple-removes-end-to-end-encryption-feature-from-uk-after-backdoor-order"><em>Bloomberg</em></a>. The move comes after UK officials secretly ordered Apple to provide unrestricted access to encrypted iCloud content worldwide.</p>
<p><img src="https://images.macrumors.com/t/DJt4yhlUGQL70lVg5j2lXcCiu10=/400x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg?lossy" srcset="https://images.macrumors.com/t/DJt4yhlUGQL70lVg5j2lXcCiu10=/400x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg?lossy 400w,https://images.macrumors.com/t/wvBp_HGYzUZeneatDI1358r-HVg=/800x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg?lossy 800w,https://images.macrumors.com/t/a9cMrXLU2fJN_7zb0__5cvQFIeo=/1600x0/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg 1600w,https://images.macrumors.com/t/0TD9k13MPLmH0gwTFeVngM_zIs4=/2500x0/filters:no_upscale()/article-new/2025/02/iCloud-Versus-UK-Key-Feature.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iCloud Versus UK Key Feature" width="2500" height="1406"><br>Customers who are already using Advanced Data Protection, or ADP, will need to manually disable it during an unspecified grace period to keep their iCloud accounts, according to the report. Apple said it will issue additional guidance in the future to affected users and that it "does not have the ability to automatically disable it on their behalf."</p>
<p>The <a href="https://www.macrumors.com/2025/02/07/uk-government-orders-access-icloud/">UK government's demand</a> came through a "technical capability notice" under the Investigatory Powers Act (IPA), requiring Apple to create a backdoor that would allow British security officials to access encrypted user data globally. The order would have compromised Apple's Advanced Data Protection feature, which provides end-to-end encryption for iCloud data including Photos, Notes, Messages backups, and device backups.<br>
</p>
<blockquote><p>"We are gravely disappointed that the protections provided by ADP will not be available to our customers in the UK given the continuing rise of data breaches and other threats to customer privacy," Apple said in a statement. "ADP protects iCloud data with end-to-end encryption, which means the data can only be decrypted by the user who owns it, and only on their trusted devices."</p></blockquote>
<p>Apple's decision to pull the feature rather than comply with the UK's demands is consistent with the company's previous statements that it would consider withdrawing encrypted services from the UK rather than compromise security. Apple has long opposed creating backdoors in its products, maintaining that such access points would inevitably be discovered by malicious actors.</p>
<p><img src="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy" srcset="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy 400w,https://images.macrumors.com/t/PZY36wCtYE298Dn3jVuf7VynYV8=/800x0/article-new/2025/02/advanced-data-protection.jpg?lossy 800w,https://images.macrumors.com/t/xBplnSN_h1PNJPMPpyYO0osUC00=/1600x0/article-new/2025/02/advanced-data-protection.jpg 1600w,https://images.macrumors.com/t/iDglo1AZeUl_ER84F4U8L7vK5yg=/2500x0/filters:no_upscale()/article-new/2025/02/advanced-data-protection.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="advanced data protection" width="1072" height="388" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy" data-srcset="https://images.macrumors.com/t/JWvmpOLixjCnUvbh58cIk1qqKzQ=/400x0/article-new/2025/02/advanced-data-protection.jpg?lossy 400w,https://images.macrumors.com/t/PZY36wCtYE298Dn3jVuf7VynYV8=/800x0/article-new/2025/02/advanced-data-protection.jpg?lossy 800w,https://images.macrumors.com/t/xBplnSN_h1PNJPMPpyYO0osUC00=/1600x0/article-new/2025/02/advanced-data-protection.jpg 1600w,https://images.macrumors.com/t/iDglo1AZeUl_ER84F4U8L7vK5yg=/2500x0/filters:no_upscale()/article-new/2025/02/advanced-data-protection.jpg 2500w"></p><p><em>Notice UK iCloud users now see after the feature was pulled</em></p><p>The UK order was particularly controversial as it would have required Apple to provide access to data from users outside the UK without their governments' knowledge. Additionally, the IPA makes it illegal for companies to disclose the existence of such government demands.</p>
<p>US security agencies, including the FBI and NSA, have been advocating for increased use of encryption to protect against Chinese cyber threats, creating potential conflicts between UK and US security interests.</p>
<p>"Enhancing the security of cloud storage with end-to-end encryption is more urgent than ever before,” said Apple on Friday, per <em>Bloomberg</em>. The company added that it "remains committed to offering our users the highest level of security for their personal data and are hopeful that we will be able to do so in the future in the United Kingdom."</p>
<p>Note that the loss of Advanced Data Protection in the UK does not affect the existing end-to-end encryption of several other Apple features available in the country, including iMessage, FaceTime, password management and health data.</p>
<p><small>Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our <a href="https://forums.macrumors.com/forums/political-news.218/">Political News</a> forum.  All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.</small></p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/02/19/apple-announces-iphone-16e/">Apple Announces iPhone 16e With A18 Chip and Apple Intelligence, Pricing Starts at $599</a></h3><p>Wednesday February 19, 2025 8:02 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple today introduced the iPhone 16e, its newest entry-level smartphone. The device succeeds the third-generation iPhone SE, which has now been discontinued.
The iPhone 16e features a larger 6.1-inch OLED display, up from a 4.7-inch LCD on the iPhone SE. The display has a notch for Face ID, and this means that Apple no longer sells any iPhones with a Touch ID fingerprint button, marking the ...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/18/iphone-17-pro-models-aluminum-frame-rumor/">iPhone 17 Pro Models Rumored to Feature Aluminum Frame Instead of Titanium Frame</a></h3><p>Tuesday February 18, 2025 12:02 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Over the years, Apple has switched from an aluminum frame to a stainless steel frame to a titanium frame for its highest-end iPhones. And now, it has been rumored that Apple will go back to using aluminum for three out of four iPhone 17 models.
In an investor note with research firm GF Securities, obtained by MacRumors this week, Apple supply chain analyst Jeff Pu said the iPhone 17, iPhone...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/20/new-apple-products-still-expecting-this-spring/">Here Are the New Apple Products We're Still Expecting This Spring</a></h3><p>Thursday February 20, 2025 5:06 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Now that Apple has announced its new more affordable iPhone 16e, our thoughts turn to what else we are expecting from the company this spring. 
There are three product categories that we are definitely expecting to get upgraded before spring has ended. Keep reading to learn what they are. If we're lucky, Apple might make a surprise announcement about a completely new product category.
M4...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/17/iphone-design-to-change-significantly/">iPhone Design to Change 'Significantly' This Year</a></h3><p>Apple is set to "significantly change" the iPhone's design language later this year, according to a Weibo leaker.
In a new post, the user known "Digital Chat Station" said that the iPhone's design is "starting to change significantly" this year. The "iPhone 17 Air" reportedly features a "horizontal, bar-shaped" design on the rear, likely referring to an elongated camera bump. On the other...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/13/apple-launch-february-19/">Tim Cook Teases an 'Apple Launch' Next Wednesday</a></h3><p>Thursday February 13, 2025 8:07 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>In a social media post today, Apple CEO Tim Cook teased an upcoming "launch" of some kind scheduled for Wednesday, February 19.
"Get ready to meet the newest member of the family," he said, with an #AppleLaunch hashtag.
The post includes a short video with an animated Apple logo inside a circle.
Cook did not provide an exact time for the launch, or share any other specific details, so...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/19/ios-18-4-release-date/">Here's When Apple Will Release iOS 18.4</a></h3><p>Wednesday February 19, 2025 11:38 am PST by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Following the launch of the iPhone 16e, Apple updated its iOS 18, iPadOS 18, and macOS Sequoia pages to give a narrower timeline on when the next updates are set to launch.
All three pages now state that new Apple Intelligence features and languages will launch in early April, an update from the more broader April timeframe that Apple provided before. The next major point updates will be iOS ...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/14/ios-18-4-beta-next-week/">iOS 18.4 Coming Next Week With These New Features for Your iPhone</a></h3><p>Friday February 14, 2025 6:18 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>The first iOS 18.4 beta for iPhones should be just around the corner, and the update is expected to include many new features and changes.
Bloomberg's Mark Gurman expects the iOS 18.4 beta to be released by next week.
Below, we outline what to expect from iOS 18.4 so far.
Apple Intelligence for Siri
Siri is expected to get several enhancements powered by Apple Intelligence on iOS...</p></div><div><h3><a href="https://www.macrumors.com/2025/02/14/two-older-apple-products-getting-updated/">Two of Apple's Oldest Products Are Finally Getting Updated This Year</a></h3><p>Friday February 14, 2025 6:03 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple released the HomePod mini in November 2020, followed by the AirTag in May 2021, and both still remain first-generation products.
Fortunately, rumors suggest that both the HomePod mini and the AirTag will finally be updated at some point this year.
Below, we recap rumors about the HomePod mini 2 and AirTag 2.
HomePod mini 2
In January 2025, Bloomberg's Mark Gurman said Apple is ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple pulls data protection tool after UK government security row (901 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cgj54eq4vejo</link>
            <guid>43128253</guid>
            <pubDate>Fri, 21 Feb 2025 15:05:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cgj54eq4vejo">https://www.bbc.com/news/articles/cgj54eq4vejo</a>, See on <a href="https://news.ycombinator.com/item?id=43128253">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Zoe Kleinman</span></p><p><span>Technology editor<!-- --><span>•</span><a href="https://twitter.com/zsk" target="_blank">@zsk</a></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/5df2/live/6e826890-f043-11ef-afe3-3909ee34e697.jpg.webp" alt="Getty Images The Apple logo in front of a high rise building"><span>Getty Images</span></p></div></figure><div data-component="text-block"><p>Apple is taking the unprecedented step of removing its highest level data security tool from customers in the UK, after the government demanded access to user data.<!-- --></p><p><a target="_blank" href="https://support.apple.com/en-gb/108756#:~:text=Advanced%20Data%20Protection%20is%20designed,secured%20using%20standard%20data%20protection.">Advanced Data Protection<!-- --></a> (ADP) means only account holders can view items such as photos or documents they have stored online through a process known as end-to-end encryption.<!-- --></p><p>But earlier this month <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/c20g288yldko">the UK government asked<!-- --></a> for the right to see the data, which currently not even Apple can access.<!-- --></p><p>Apple did not comment at the time but has consistently opposed creating a "backdoor" in its encryption service, arguing that if it did so, it would only be a matter of time before bad actors also found a way in.<!-- --></p><p>Now the tech giant has decided it will no longer be possible to activate ADP in the UK.<!-- --></p><p>It means eventually not all UK customer data stored on iCloud - Apple's cloud storage service - will be <!-- --><a target="_blank" href="https://support.apple.com/en-us/102651">fully encrypted<!-- --></a>.<!-- --></p><p>Data with standard encryption is accessible by Apple and shareable with law enforcement, if they have a warrant.<!-- --></p><p>In a statement the Home Office said: "We do not comment on operational matters, including for example confirming or denying the existence of any such notices."<!-- --></p><p>In a statement Apple said it was "gravely disappointed" that the security feature would no longer be available to British customers.<!-- --></p><p>"As we have said many times before, we have never built a backdoor or master key to any of our products, and we never will," it continued.<!-- --></p><ul><li><a target="_self" href="https://www.bbc.co.uk/news/technology-64863448">How does encryption work?<!-- --></a></li></ul><p>The ADP service is opt-in, meaning people have to sign up to get the protection it provides.<!-- --></p><p>From 1500GMT on Friday, any Apple user in the UK attempting to turn it on has been met with an error message.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a729/live/a86b5cb0-f06b-11ef-8c03-7dfdbeeb2526.jpg.webp" alt="Apple The Apple error message"><span>Apple</span></p></div></figure><div data-component="text-block"><p>Existing users' access will be disabled at a later date. <!-- --></p><p>It is not known how many people have signed up for ADP since it became available to British Apple customers in December 2022.<!-- --></p><p>Prof Alan Woodward - a cyber-security expert at Surrey University - said it was a "very disappointing development" which amounted to "an act of self harm" by the government.<!-- --></p><p>"All the UK government has achieved is to weaken online security and privacy for UK based users," he told the BBC.<!-- --></p><p>"It was naïve of the UK government to think they could tell a US technology company what to do globally," he added.<!-- --></p></div><p data-component="subheadline-block"><h2>What did the UK ask for?<!-- --></h2></p><div data-component="text-block"><p>The request was served by the Home Office under the Investigatory Powers Act (IPA), which compels firms to provide information to law enforcement agencies.<!-- --></p><p>Apple would not comment on the notice and the Home Office refused to either confirm or deny its existence, but the BBC and the Washington Post spoke to a number of sources familiar with the matter.<!-- --></p><p>It provoked a fierce backlash from privacy campaigners, who called it an "unprecedented attack" on the private data of individuals.<!-- --></p><p>Two <!-- --><a target="_self" href="https://www.bbc.co.uk/news/articles/c5yvn90pl5no">senior US politicians said<!-- --></a> it was so serious a threat to American national security that the US government should re-evaluate its intelligence-sharing agreements with the UK unless it was withdrawn.<!-- --></p><p>It is not clear that Apple's actions will fully address those concerns, as the IPA order applies worldwide and ADP will continue to operate in other countries.<!-- --></p><p>In its statement, Apple said it regretted the action it had taken.<!-- --></p><p>"Enhancing the security of cloud storage with end-to-end-encryption is more urgent than ever before," it said.<!-- --></p><p>"Apple remains committed to offering our users the highest level of security for their personal data and are hopeful that we will be able to do so in future in the UK."<!-- --></p><p>The row comes amid growing push-back in the US against regulation being imposed on its tech sector from elsewhere.<!-- --></p><p>In a speech at the AI Action Summit in Paris at the beginning of February, US Vice President JD Vance made it clear that the US was increasingly concerned about it. <!-- --></p><p>"The Trump administration is troubled by reports that some foreign governments are considering tightening the screws on US tech companies with international footprints," he said.<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Johnny.Decimal – A system to organise your life (273 pts)]]></title>
            <link>https://johnnydecimal.com</link>
            <guid>43128093</guid>
            <pubDate>Fri, 21 Feb 2025 14:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johnnydecimal.com">https://johnnydecimal.com</a>, See on <a href="https://news.ycombinator.com/item?id=43128093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="_11-01">  
<p><strong>Johnny.Decimal is designed to help you find things quickly, with more confidence, and less stress.</strong></p>
<p>You assign a unique ID to everything in your life.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--dtop-1_resize-light-cx-1000x609.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--dtop-1_resize-dark-cx-1000x609.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--mob-1_resize-light-cx-500x470.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--mob-1_resize-dark-cx-500x470.png"> <img alt="A diagram showing the structure of a Johnny.Decimal number. The number is 15.52 and it explains how the '1' is an area, which groups related categories in sets of 10. The '15' is the category, in this case 'travel'. And '52' is just an ID; they start at 01. The title of this, our 52nd travel thing, is 'Trip to NYC'." src="https://johnnydecimal.com/img/v6/11.01A-Diagram_1552_NYC--dtop-1_resize-light-cx-1000x609.png" width="500" height="304">  </picture> 
<p>These IDs help you stay organised. They impose constraints that make it harder to get lost. And you create your own index to link everything in your life together.</p>
<p>The system is free to use and the concepts are the same at home, work, or that club you manage.</p>
<h2 id="the-problem">The problem</h2>
<p>In real life, if you stored your stuff in piles of badly-labelled boxes you'd never find anything again.</p>
<p>If you put <em>those</em> boxes in boxes, in boxes, you'd never know which box to open to find the next box. It would be chaos. But this is how you save your computer files.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-light-cx-628x526.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-dark-cx-628x526.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-light-cx-628x526.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-dark-cx-628x526.png"> <img alt="A screenshot of a MacOS Finder window showing a bunch of folders, nested terribly, all named similarly. It's a confusing mess." src="https://johnnydecimal.com/img/v6/11.01B-Finder_chaos_v2--1_vfx-light-cx-628x526.png" width="314" height="263"> <figcaption>FIGURE 11.01B. A CHAOTIC FILE SYSTEM WITH MANY LEVELS OF FOLDERS.</figcaption> </picture> 
<h2 id="the-solution">The solution</h2>
<p>Here's one way to think about how a Johnny.Decimal system works. In this simple analogy, an area is a shelf, a category is a box, and an ID is a manila folder.</p>
<h3 id="step-1-buy-ten-shelves">Step 1: Buy ten shelves</h3>
<p>Imagine a computer is a garage. We can't put everything on the floor, so we buy ten shelves. Then we dedicate each one to an area of our life -- <code>life admin</code>, <code>home business</code>, and <code>tennis club</code>.<sup><a href="#user-content-fn-room-to-grow" id="user-content-fnref-room-to-grow" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-light-cx-1312x918.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-dark-cx-1312x918.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-light-cx-1312x918.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-dark-cx-1312x918.png"> <img alt="A line drawing of three storage shelves. Think your classic Ikea 'Billy' bookshelf. At the top they're labelled 'life admin', 'home business', and 'tennis club'. They're empty." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01C-Berkeley_shelves_x3--0_original-light-cx-1312x918.png" width="1312" height="918"> <figcaption>FIGURE 11.01C. A SHELF FOR EACH MAJOR AREA OF OUR LIFE.</figcaption> </picture> 
<h3 id="step-2-add-some-boxes">Step 2: Add some boxes</h3>
<p>Each shelf has space for ten boxes, so we categorise what we want to store. In life admin we decide on five and label them: <code>me</code>, <code>house</code>, <code>money</code>, <code>online</code>, and <code>travel</code>. Our boxes have space for a number, so we add that too.<sup><a href="#user-content-fn-startat11" id="user-content-fnref-startat11" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup></p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-light-cx-448x914.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-dark-cx-448x914.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-light-cx-448x914.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-dark-cx-448x914.png"> <img alt="The same shelf and boxes, but now the labels have numbers at the front. The shelf is labelled '10-19 Life admin', and the boxes are labelled '11 Me', '12 House', '13 Money', '14 Online', and '15 Travel'." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01D-Berkeley_shelf_w_boxes--0_original-light-cx-448x914.png" width="224" height="407"> <figcaption>FIGURE 11.01D: OUR LIFE ADMIN SHELF ENDS UP WITH FIVE BOXES.</figcaption> </picture> 
<h3 id="step-3-file-your-stuff-in-folders">Step 3: File your stuff in folders</h3>
<p>We put our documents in manila folders. Each folder gets a number starting at <code>.11</code> so we can track them. In this case, we've put some insurance policies in <code>15.23 Travel insurance</code>. Then put the folder in a box.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-light-cx-1252x434.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-dark-cx-1252x434.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-light-cx-1252x434.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-dark-cx-1252x434.png"> <img alt="Line drawing representing a manila folder. It's labelled '15.23 Travel insurance' and contains 3 documents, labelled 'Claim form', 'Payment receipt', and 'Policy document'." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01E-Berkeley_folder_box--0_original-light-cx-1252x434.png" width="626" height="217"> <figcaption>FIGURE 11.01E. WE PUT OUR DOCUMENTS IN NUMBERED FOLDERS AND STORE THEM IN THE RELEVANT BOX.</figcaption> </picture> 
<h3 id="this-is-how-we-structure-our-file-system">This is how we structure our file system</h3>
<p>Let's return to our computer. The shelves have become our area folders. The boxes are category folders. And the manila folders are the IDs where we save our files.</p>
<picture>  <source media="(prefers-color-scheme: light) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-light-cx-648x522.png">  <source media="(prefers-color-scheme: dark) and (min-width: 600px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-dark-cx-648x522.png">  <source media="(prefers-color-scheme: light) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-light-cx-648x522.png">  <source media="(prefers-color-scheme: dark) and (max-width: 599px)" srcset="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-dark-cx-648x522.png"> <img alt="Screenshot of macOS Finder. It shows a parent folder '10-19 Life admin', labelled 'SHELF'. It contains folder '15 Travel', labelled 'BOX'. And it contains '11.53 Travel insurance', which is labelled 'FOLDER'." loading="lazy" src="https://johnnydecimal.com/img/v6/11.01F-Finder_box_shelf_folder--1_labels-light-cx-648x522.png" width="324" height="261"> <figcaption>FIGURE 11.01F. A NEAT FILE STRUCTURE WITH AREAS, CATEGORIES, AND IDS.</figcaption> </picture> 
<h2 id="benefits-of-the-johnnydecimal-id">Benefits of the Johnny.Decimal ID</h2>
<p>Each of our storage folders now has a number, the ID. It always has two digits, a decimal, and two digits. For example, <code>15.23</code> <code>22.11</code> <code>31.17</code>. This number is really useful.</p>
<h3 id="it-provides-structure">It provides structure</h3>
<p>The ID tells us exactly where a thing is. The numbers before the decimal are the item's category, and they define the structure of your system.</p>
<p>At a glance, you know what sort of thing the item contains. You'll be astonished at how many of your category numbers you remember.</p>
<h3 id="theyre-easy-to-communicate">They're easy to communicate</h3>
<p>They're short, memorable, and can be spoken out loud. Say it like "sixteen oh-two" or "thirty-one dot seventeen".</p>
<p>This is really handy when you want to tell someone (including your future self) where a thing is.</p>
<h3 id="things-stay-where-they-are">Things stay where they are</h3>
<p>If you use the alphabet to name folders, they move when a new one is created. So you never get a chance to develop muscle memory.</p>
<p>Numbers solve this problem. In the example above, <code>11 Me</code> comes before <code>12 House</code> because the folders sort by number. If we made a new folder, <code>16 Aardvark collection</code>, nothing would move.</p>
<h3 id="it-imposes-limits">It imposes limits</h3>
<p>The 'no more than ten' concept is at the heart of Johnny.Decimal.</p>
<p>When you start looking for something, you have no more than ten area folders to choose from. Select one and ignore the rest. Now you have no more than ten category folders to choose from. Repeat the process.</p>
<p>You then arrive in a folder with no more than one hundred IDs. If the ID was created recently it will have a higher number. If not, lower. And things created together, stick together. The alphabet isn't around to ruin the party.</p>
<h2 id="i-like-it-what-next">I like it! What next?</h2>
<p>Welcome to the Johnny.Decimal family, there's plenty to go on with:</p>
<ul>
<li>
<p>Explore the site to <a href="https://johnnydecimal.com/11.02/">learn more</a> about the system.</p>
</li>
<li>
<p>If you're a small business owner, head over to <a href="https://smallbusiness.johnnydecimal.com/">the just-announced small business system</a>.</p>
</li>
<li>
<p>Get organised fast with the '<a href="https://johnnydecimal.com/14.11/">life admin</a>' pack. Check out the <a href="https://johnnydecimal.com/14.21/">workbook</a> or <a href="https://johnnydecimal.com/14.22/">workshop</a> for more comprehensive guidance.</p>
</li>
<li>
<p>Follow the <a href="https://johnnydecimal.com/22.02/">blog</a> or sign up to the <a href="https://johnnydecimal.com/21.02/">mailing list</a>.</p>
</li>
<li>
<p>Ask for help in the friendly <a href="https://forum.johnnydecimal.com/">forum</a> or <a href="https://johnnydecimal.com/23.02/">Discord</a>, or <a href="mailto:hello@johnnydecimal.com">email me</a> (I answer every message).</p>
</li>
</ul>
<section data-footnotes="">
<ol>
<li id="user-content-fn-room-to-grow">
<p>We don't try to use all ten shelves -- there's room to grow. <a href="#user-content-fnref-room-to-grow" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-startat11">
<p>I'll explain later why the first box isn't number <code>01</code> or <code>10</code>. <a href="#user-content-fnref-startat11" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX engineers brought on at FAA after probationary employees were fired (190 pts)]]></title>
            <link>https://www.wired.com/story/faa-doge-elon-musk-space-x/</link>
            <guid>43127819</guid>
            <pubDate>Fri, 21 Feb 2025 14:28:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/faa-doge-elon-musk-space-x/">https://www.wired.com/story/faa-doge-elon-musk-space-x/</a>, See on <a href="https://news.ycombinator.com/item?id=43127819">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Engineers who work for <a href="https://www.wired.com/story/elon-musk-government-young-engineers/">Elon Musk’s</a> SpaceX have been brought on as senior advisers to the acting administrator of the <a href="https://www.wired.com/story/washington-dc-plane-crash-everything-we-know-so-far-black-hawk-faa-potomac-american-airlines-psa/">Federal Aviation Administration</a> (FAA), sources tell WIRED.</p><p>On Sunday, Sean Duffy, secretary of the Department of Transportation, which oversees the FAA, <a data-offer-url="https://x.com/SecDuffy/status/1891310401800872114" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/SecDuffy/status/1891310401800872114&quot;}" href="https://x.com/SecDuffy/status/1891310401800872114" rel="nofollow noopener" target="_blank">announced</a> in a post on X that SpaceX engineers would be visiting the Air Traffic Control System Command Center in Virginia to take what he positioned as a tour. “The safety of air travel is a nonpartisan matter,” Musk <a data-offer-url="https://x.com/elonmusk/status/1891312807896907807" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/elonmusk/status/1891312807896907807&quot;}" href="https://x.com/elonmusk/status/1891312807896907807" rel="nofollow noopener" target="_blank">replied</a>. “SpaceX engineers will help make air travel safer.”</p><p>By the time these posts were made, though, according to sources who were granted anonymity because they fear retaliation, SpaceX engineers were already being onboarded at the agency under Schedule A, a <a data-offer-url="https://www.opm.gov/policy-data-oversight/disability-employment/hiring/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.opm.gov/policy-data-oversight/disability-employment/hiring/&quot;}" href="https://www.opm.gov/policy-data-oversight/disability-employment/hiring/" rel="nofollow noopener" target="_blank">special authority</a> that allows government managers to “hire persons with disabilities without requiring them to compete for the job,” <a data-offer-url="https://www.opm.gov/policy-data-oversight/disability-employment/hiring/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.opm.gov/policy-data-oversight/disability-employment/hiring/&quot;}" href="https://www.opm.gov/policy-data-oversight/disability-employment/hiring/" rel="nofollow noopener" target="_blank">according</a> to the Office of Personnel Management (OPM).</p><p>These new hires come after the terminations of hundreds of FAA probationary employees, and the most deadly month of US <a href="https://www.wired.com/story/washington-dc-plane-crash-everything-we-know-so-far-black-hawk-faa-potomac-american-airlines-psa/">aviation</a> <a data-offer-url="https://www.cbsnews.com/philadelphia/news/northeast-philadelphia-plane-crash/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.cbsnews.com/philadelphia/news/northeast-philadelphia-plane-crash/&quot;}" href="https://www.cbsnews.com/philadelphia/news/northeast-philadelphia-plane-crash/" rel="nofollow noopener" target="_blank">disasters</a> in more than a decade.</p><p>According to a source with knowledge of the situation, none of the SpaceX engineers were fully vetted by their start date. Unlike the <a href="https://www.wired.com/story/elon-musk-government-young-engineers/">very young technologists</a> associated with Musk’s so-called Department of Government Efficiency (DOGE) who have been given access to critical systems at agencies ranging <a href="https://www.wired.com/story/elon-musk-lackeys-office-personnel-management-opm-neuralink-x-boring-stalin/">from OPM</a> and the <a href="https://www.wired.com/story/elon-musk-associate-bfs-federal-payment-system/">Treasury Department</a> to the <a href="https://www.wired.com/story/doge-engineer-noaa-data-google-musk-climate-project-2025/">National Oceanic and Atmospheric Administration</a> in recent weeks, though, the engineers identified by WIRED—Ted Malaska, Thomas Kiernan, Sam Smeal, and Brady Glantz—do appear to have experience relevant to the FAA.</p><p>Malaska is currently, according to his LinkedIn profile, a senior director of application software at SpaceX, where he started working in May 2021. Formerly the senior director of data engineering at Capitol One and a senior architect at FINRA, he graduated from the University of Maryland Baltimore County in 2000 and cowrote a <a data-offer-url="https://www.amazon.com/Hadoop-Application-Architectures-Real-World-Applications/dp/1491900083" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Hadoop-Application-Architectures-Real-World-Applications/dp/1491900083&quot;}" href="https://www.amazon.com/Hadoop-Application-Architectures-Real-World-Applications/dp/1491900083" rel="nofollow noopener" target="_blank">2015 book</a> on Hadoop application architectures.</p><p>Kiernan is currently a lead software engineer at SpaceX, according to his LinkedIn page. Before joining SpaceX in May 2020, he worked at Wayfair and is a 2017 Dartmouth graduate.</p><p>Smeal is a software engineer who has worked at SpaceX since September 2021, according to his LinkedIn. He graduated from Saint Vincent College in 2018.</p><p>Glantz is a software engineer who has worked at SpaceX since May 2024 and worked as an engineering analyst at Goldman Sachs from 2019 to 2021, according to his LinkedIn, and graduated from the University of Michigan in 2019.</p><p>Malaska, Kiernan, Smeal, and Glantz did not immediately respond to requests for comment. The FAA also did not immediately respond to requests for comment.</p><p>In <a data-offer-url="https://x.com/SecDuffy/status/1891310401800872114" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/SecDuffy/status/1891310401800872114&quot;}" href="https://x.com/SecDuffy/status/1891310401800872114" rel="nofollow noopener" target="_blank">his post on X</a>, Duffy wrote, "Because I know the media (and Hillary Clinton) will claim Elon’s team is getting special access, let me make clear that the @FAANews regularly gives tours of the command center to both media and companies.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>But on Wednesday, FAA acting administrator Chris Rocheleau wrote in an email to FAA staff, viewed by WIRED, that DOGE and the teams of special government employees deployed in federal agencies were “top-of-mind,” before noting that the agency had "recently welcomed” a team of special government employees who had already toured some FAA facilities. “We are asking for their help to engineer solutions while we keep the airspace open and safe,” he wrote, adding that the new employees had already visited the FAA Command Center and Potomac TRACON, a facility that controls the airspace around and provides air traffic control services to airports in the DC, Maryland, and Virginia areas.</p><p>In a Department of Transportation all-hands meeting late last week, Duffy responded to a question about DOGE's role in national airspace matters, and without explicitly mentioning the new employees, suggested help was needed on reforming Notice to Air Mission (NOTAM) alerts, a critical system that distributes real-time data and warnings to pilots but which has had <a href="https://www.wired.com/story/faa-notam-outage/">significant outages</a>, one as recently as <a data-offer-url="https://www.nytimes.com/2025/02/02/us/notam-outage-faa-alerts.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.nytimes.com/2025/02/02/us/notam-outage-faa-alerts.html&quot;}" href="https://www.nytimes.com/2025/02/02/us/notam-outage-faa-alerts.html" rel="nofollow noopener" target="_blank">this month</a>. “If I can get ideas from really smart engineers on how we can fix it, I’m going to take those ideas,” he said, according to a recording of the meeting reviewed by WIRED. “Great engineers” might also work on airspace issues, he said.</p><p>SpaceX functioned as the pre-inauguration staging ground for the DOGE team, according to reporting from <a data-offer-url="https://www.nytimes.com/2025/01/12/us/politics/elon-musk-doge-government-trump.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.nytimes.com/2025/01/12/us/politics/elon-musk-doge-government-trump.html&quot;}" href="https://www.nytimes.com/2025/01/12/us/politics/elon-musk-doge-government-trump.html" rel="nofollow noopener" target="_blank">The New York Times</a> and sources who spoke to WIRED. In the months between November 5 and January 20, members of DOGE including Steve Davis (president of Musk’s Boring Company) and the young engineer Luke Farritor were operating out of the company’s DC office, according to a source with knowledge.</p><p>The company did not respond to questions about whether these employees will retain their salaries and positions at the company during their time with DOGE. Many of the so-called department’s operatives have joined as “special government employees,” who are limited to working 130 days in a year. Last week <a href="https://www.wired.com/story/musk-krause-treasury-bfs-conflict-of-interest/">WIRED reported</a> that Tom Krause, a DOGE operative at the Treasury Department, would continue to maintain his position as CEO of the Cloud Software Group while also performing the duties of fiscal assistant secretary. Other members of Musk’s companies, including xAI and Tesla, have also taken on positions with DOGE.</p><p>Late last week, the <a href="https://apnews.com/article/doge-faa-air-traffic-firings-safety-67981aec33b6ee72cbad8dcee31f3437">Trump administration laid off</a> <a data-offer-url="https://x.com/SecDuffy/status/1891656952662405304" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/SecDuffy/status/1891656952662405304&quot;}" href="https://x.com/SecDuffy/status/1891656952662405304" rel="nofollow noopener" target="_blank">400</a> FAA workers, according to their union, the Professional Aviation Safety Specialists. The union says these included probationary employees who worked on air traffic control communications and related radio and computer systems. Air traffic controllers were not affected by the layoffs, Duffy <a data-offer-url="https://x.com/SecDuffy/status/1891656952662405304" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/SecDuffy/status/1891656952662405304&quot;}" href="https://x.com/SecDuffy/status/1891656952662405304" rel="nofollow noopener" target="_blank">said in an X post</a>.</p><p>Just two weeks before that, the US suffered its most deadly aviation incident in more than a decade, when 67 people died after an Army helicopter <a href="https://www.wired.com/story/whats-next-in-the-dc-jet-crash-investigation/">collided with a passenger jet</a> in Washington, DC. Though initial findings suggest <a data-offer-url="https://www.nbcnews.com/news/us-news/equipment-malfunction-dropped-messages-looked-ntsb-midair-crash-dc-rcna191275" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.nbcnews.com/news/us-news/equipment-malfunction-dropped-messages-looked-ntsb-midair-crash-dc-rcna191275&quot;}" href="https://www.nbcnews.com/news/us-news/equipment-malfunction-dropped-messages-looked-ntsb-midair-crash-dc-rcna191275" rel="nofollow noopener" target="_blank">complex equipment and communications issues</a> possibly played roles in the disaster, President Trump was quick to blame “DEI,” railing against a decade-old program that helps the FAA identify talent among populations with disabilities. People with disabilities hired into the FAA and other federal agencies are often accepted under the Schedule A authority—exactly the route these new engineers have taken into the agency.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The FAA has frequently tangled with Musk’s SpaceX, as the rocket company and others fight to operate their own interests in crowded American airspace. In January, the FAA temporarily grounded SpaceX’s program after one of its Starship rockets broke apart midflight, <a data-offer-url="https://www.cnn.com/2025/01/30/science/spacex-starship-explosion-debris-turks-caicos/index.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.cnn.com/2025/01/30/science/spacex-starship-explosion-debris-turks-caicos/index.html&quot;}" href="https://www.cnn.com/2025/01/30/science/spacex-starship-explosion-debris-turks-caicos/index.html" rel="nofollow noopener" target="_blank">reportedly</a> damaging public property on Turks and Caicos in the Caribbean. The FAA diverted dozens of commercial airline flights following the explosion and announced an investigation into the incident, which is ongoing and being led by SpaceX. Musk, however, <a href="https://apnews.com/article/spacex-starship-elon-musk-launch-accident-e69d04467e2def65d2bc6b0e9645d715">characterized</a> the failure as “barely a bump in the road” and <a data-offer-url="https://x.com/elonmusk/status/1880060983734858130" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/elonmusk/status/1880060983734858130&quot;}" href="https://x.com/elonmusk/status/1880060983734858130" rel="nofollow noopener" target="_blank">did not seem to indicate</a> that the investigation would slow SpaceX’s launch cadence. Last year, the company indicated it was aiming for <a data-offer-url="https://gizmodo.com/spacex-sets-ambitious-goal-25-starship-flights-in-2025-2000524527?utm_medium=AIAA_Website&amp;utm_source=rasa_io&amp;utm_campaign=Industry_News" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://gizmodo.com/spacex-sets-ambitious-goal-25-starship-flights-in-2025-2000524527?utm_medium=AIAA_Website&amp;utm_source=rasa_io&amp;utm_campaign=Industry_News&quot;}" href="https://gizmodo.com/spacex-sets-ambitious-goal-25-starship-flights-in-2025-2000524527?utm_medium=AIAA_Website&amp;utm_source=rasa_io&amp;utm_campaign=Industry_News" rel="nofollow noopener" target="_blank">25 launches of the Starship in 2025</a>.</p><p>FAA spokesperson Steven Kulm told WIRED that “the FAA is overseeing the SpaceX-led mishap investigation.” The FAA did not respond to further questions about whether the presence of SpaceX engineers at the agency would constitute a conflict of interest.</p><p>In September, the FAA <a data-offer-url="https://www.faa.gov/sites/faa.gov/files/afn-foia-20240917-case-2023WA990028.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.faa.gov/sites/faa.gov/files/afn-foia-20240917-case-2023WA990028.pdf&quot;}" href="https://www.faa.gov/sites/faa.gov/files/afn-foia-20240917-case-2023WA990028.pdf" rel="nofollow noopener" target="_blank">proposed</a> <a data-offer-url="https://www.faa.gov/sites/faa.gov/files/afn-foia-20240917-case-2023WA990031_0.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.faa.gov/sites/faa.gov/files/afn-foia-20240917-case-2023WA990031_0.pdf&quot;}" href="https://www.faa.gov/sites/faa.gov/files/afn-foia-20240917-case-2023WA990031_0.pdf" rel="nofollow noopener" target="_blank">$633,000</a> in fines following two 2023 incidents in which SpaceX allegedly did not follow its license requirements, violating regulations. Responding to an X user posting about the penalties, Musk <a data-offer-url="https://x.com/elonmusk/status/1836182108412481871?mx=2" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/elonmusk/status/1836182108412481871?mx=2&quot;}" href="https://x.com/elonmusk/status/1836182108412481871?mx=2" rel="nofollow noopener" target="_blank">wrote</a>, “The fundamental problem is that humanity will forever be confined to Earth unless there is radical reform at the FAA!” Shortly afterward, Musk called for FAA head Mike Whitaker <a data-offer-url="https://x.com/elonmusk/status/1838978117072805999" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/elonmusk/status/1838978117072805999&quot;}" href="https://x.com/elonmusk/status/1838978117072805999" rel="nofollow noopener" target="_blank">to resign</a>.</p><p>In January, more than three years before his term was due to end, Whitaker did resign.</p><p>“I told Elon, any conflicts, you can’t have anything to do with that,” said President Trump in a <a data-offer-url="https://thehill.com/homenews/administration/5151906-elon-musk-official-title/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://thehill.com/homenews/administration/5151906-elon-musk-official-title/&quot;}" href="https://thehill.com/homenews/administration/5151906-elon-musk-official-title/" rel="nofollow noopener" target="_blank">press conference this week, in response to a question about Musk, SpaceX, the FAA, and conflicts of interest.</a> “So anything to do with possibly even space, we won’t let Elon partake in that.”</p><p>The White House did not immediately respond to a request for comment.</p><p>SpaceX is directly regulated by a small FAA agency called the Office of Commercial Space Transportation, which since 1984 has licensed the launch of US space rockets. “The purpose is to ensure public safety,” says George Nield, a former associate administrator of the office. “People on the ground did not consent” to rocket launches above them, he says. ”We absolutely need to keep them safe. The office has done a great job of that.” The office oversaw 157 launches in 2024 alone.</p><p>On February 10, several days after Musk <a data-offer-url="https://x.com/elonmusk/status/1887233566263967812" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/elonmusk/status/1887233566263967812&quot;}" href="https://x.com/elonmusk/status/1887233566263967812" rel="nofollow noopener" target="_blank">posted</a> on X that DOGE “will aim to make rapid safety upgrades to the air traffic control system,” a group of Democratic legislators <a href="https://www.kaine.senate.gov/imo/media/doc/va_md_dc_delegation_rocheleau_dca_incident_letter_doge.pdf">wrote</a> to Rocheleau—a career civil servant whose ties to the FAA go back to 1996—requesting information about any planned changes to FAA systems.</p><p>“We are extremely concerned that an ad hoc team of individuals lacking any expertise, exposure, certifications, or knowledge of aviation operations being invited, or inserting themselves, to make ‘rapid’ changes to our nation’s air traffic systems,” they wrote. “Aviation safety is not an area to ‘move fast and break things.’”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sweden Investigates New Cable Break Under Baltic Sea (180 pts)]]></title>
            <link>https://www.nytimes.com/2025/02/21/world/europe/baltic-sea-cable-sweden.html</link>
            <guid>43127425</guid>
            <pubDate>Fri, 21 Feb 2025 13:54:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/02/21/world/europe/baltic-sea-cable-sweden.html">https://www.nytimes.com/2025/02/21/world/europe/baltic-sea-cable-sweden.html</a>, See on <a href="https://news.ycombinator.com/item?id=43127425">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/02/21/world/europe/baltic-sea-cable-sweden.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA['What a lie': Danish astronaut responds to Musk (128 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/feb/21/elon-musk-butch-wilmore-suni-williams-nasa-astronaut-iss-claims-biden</link>
            <guid>43126588</guid>
            <pubDate>Fri, 21 Feb 2025 12:05:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/feb/21/elon-musk-butch-wilmore-suni-williams-nasa-astronaut-iss-claims-biden">https://www.theguardian.com/technology/2025/feb/21/elon-musk-butch-wilmore-suni-williams-nasa-astronaut-iss-claims-biden</a>, See on <a href="https://news.ycombinator.com/item?id=43126588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Elon Musk has become embroiled in<strong> </strong>a heated row with a Danish astronaut who criticised the <a href="https://www.theguardian.com/technology/2025/feb/16/elon-musk-doge-government-privatization" data-link-name="in body link">tech billionaire</a>’s claim that the former US president Joe Biden abandoned two American astronauts at the International Space Station on purpose.</p><p>Andreas “Andy” Mogensen accused Musk of lying when he claimed in a Fox News interview alongside Donald Trump that Nasa’s <a href="https://www.theguardian.com/science/2025/jan/30/nasa-stuck-astronauts-spacewalk" data-link-name="in body link">Butch Wilmore and Suni Williams</a> were left stranded for “political reasons” by Biden.</p><p>“What a lie. And from someone who complains about lack of honesty from the mainstream media,” the 48-year-old European Space Agency astronaut, who has flown to the ISS twice, wrote on X.</p><p>In response, Musk called Mogensen “fully retarded”. He said<strong> </strong>SpaceX, which he owns, could have brought the astronauts back “several months ago” and that he had made such an offer to the Biden administration. Musk did not elaborate on what that offer entailed.</p><figure id="8f651091-1d4c-496e-958b-611e0aa46b1c" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:4,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;What does Elon Musk believe?&quot;,&quot;elementId&quot;:&quot;8f651091-1d4c-496e-958b-611e0aa46b1c&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/ng-interactive/2025/feb/08/elon-musk-politics-doge&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Wilmore and Williams flew to the ISS in June onboard a Boeing Starliner for what was meant to be an eight-day test mission to certify the new spaceship. However, thruster problems led Nasa to decide that the Starliner should return without its crew, and the agency tasked SpaceX with bringing the veteran astronauts home.</p><p>Nasa then announced the pair would return on the SpaceX Crew-9 mission’s spaceship, which launched in September with a crew of two instead of four in order to accommodate them.</p><p>The voyage home was initially scheduled for February but later shifted to March due to delays by SpaceX in preparing the spacecraft for Crew-10, whose crew will replace Crew-9’s. If there was an alternate rescue plan that could have been executed sooner, Musk has not revealed it.</p><p>Replying to Musk, Mogensen – who flew onboard a SpaceX Crew Dragon capsule during a 2023 mission to the ISS – said: “Elon, I have long admired you and what you have accomplished, especially at SpaceX and Tesla.</p><p>“You know as well as I do, that Butch and Suni are returning with Crew-9, as has been the plan since last September. Even now, you are not sending up a rescue ship to bring them home. They are returning on the Dragon capsule that has been on ISS since last September.”</p><p>As well as being the CEO of SpaceX and Tesla, and owning X, formerly known as Twitter, Musk leads the Department of Government Efficiency (Doge), an advisory department under the Trump administration.</p><p>In 2019, a court in Los Angeles <a href="https://www.theguardian.com/technology/2019/dec/06/elon-musk-vernon-unsworth-trial-verdict" data-link-name="in body link">found Musk not guilty</a> of defaming the British cave explorer Vernon Unsworth, after a Twitter spat over the 2018 Thailand cave rescue during which Musk called Unsworth a “pedo guy”. Musk’s attorneys had argued that the tweet was not a statement of fact, but an insult, which is considered protected speech.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Users don't care about your tech stack (201 pts)]]></title>
            <link>https://www.empathetic.dev/users-dont-care-about-your-tech-stack</link>
            <guid>43125981</guid>
            <pubDate>Fri, 21 Feb 2025 10:26:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.empathetic.dev/users-dont-care-about-your-tech-stack">https://www.empathetic.dev/users-dont-care-about-your-tech-stack</a>, See on <a href="https://news.ycombinator.com/item?id=43125981">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I still often find myself in discussions where the main focus is the choice of technologies. Questions like <em>“Is this language better than that one?”</em> or <em>“Is this framework more performant than the other?”</em> keep coming up. But the truth is: <strong>users don’t care about any of that</strong>. They won’t notice those extra 10 milliseconds you saved, nor will their experience magically improve just because you’re using the latest JavaScript framework.</p>
<p>What truly makes a difference for users is your attention to the <strong>product</strong> and their <strong>needs</strong>.</p>
<p>Every programming language shines in specific contexts. Every framework is born to solve certain problems. But none of these technical decisions, in isolation, will define the success of your product from a user’s perspective.</p>
<h2 id="so-what-should-you-focus-on">So, what should you focus on?</h2>
<ul>
<li><strong>Use what you know well.</strong></li>
<li><strong>Use what you enjoy working with.</strong></li>
<li><strong>Use what challenges and motivates you to improve every day.</strong></li>
</ul>
<p>Don’t get swept up by the hype. Sure, set aside time to explore new technologies that catch your interest, but don’t fall into the trap of thinking that building a great product requires the same stack featured in the latest YouTube tutorial from your favorite tech influencer.</p>
<p>Learn to distinguish between tech choices that are <strong>interesting to you</strong> and those that are genuinely <strong>valuable for your product and your users</strong>. Finding the right balance is key to creating something truly impactful.</p>
<hr>
<p>There are no “best” languages or frameworks. There are only technologies designed to solve specific problems, and your job is to pick the ones that fit your use case—not because they’re trendy, but because they’re the right tool for the job.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta claims torrenting pirated books isn't illegal without proof of seeding (592 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/02/meta-defends-its-vast-book-torrenting-were-just-a-leech-no-proof-of-seeding/</link>
            <guid>43125840</guid>
            <pubDate>Fri, 21 Feb 2025 10:01:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/02/meta-defends-its-vast-book-torrenting-were-just-a-leech-no-proof-of-seeding/">https://arstechnica.com/tech-policy/2025/02/meta-defends-its-vast-book-torrenting-were-just-a-leech-no-proof-of-seeding/</a>, See on <a href="https://news.ycombinator.com/item?id=43125840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2077220">
  
  <header>
  <div>
    <div>
      

      

      <p>
        Meta’s copyright defense may hinge on court ignorance of torrenting terminology.
      </p>

      
    </div>

    <div>
    
    <p>
      A peer who downloads more data than they upload on torrenting networks is known as a "leech"  sucking up data without contributing to the swarm.

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/photo/medicinal-leech-hirudo-medicinalis-royalty-free-image/1125883889?phrase=leeches&amp;searchscope=image,film&amp;adppopup=true" target="_blank">
          
          phototrip | iStock / Getty Images Plus

                      </a>
                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          <p>Just because Meta admitted to <a href="https://arstechnica.com/tech-policy/2025/02/meta-torrented-over-81-7tb-of-pirated-books-to-train-ai-authors-say/">torrenting a dataset of pirated books for AI training</a> purposes, that doesn't necessarily mean that Meta seeded the file after downloading it, the social media company claimed in a <a href="https://cdn.arstechnica.net/wp-content/uploads/2025/02/Kadrey-v-Meta-Reply-In-Support-of-Motion-to-Dismiss-2-20-25.pdf">court filing</a> this week.</p>
<p>Evidence instead shows that Meta "took precautions not to 'seed' any downloaded files," Meta's filing said. Seeding refers to sharing a torrented file after the download completes, and because there's allegedly no proof of such "seeding," Meta insisted that authors cannot prove Meta shared the pirated books with anyone during the torrenting process.</p>
<p>Whether or not Meta actually seeded the pirated books could make a difference in a copyright lawsuit from book authors including Richard Kadrey, Sarah Silverman, and Ta-Nehisi Coates. Authors had previously alleged that Meta unlawfully copied and distributed their works through AI outputs—an increasingly common complaint that so far has barely been litigated. But Meta's admission to torrenting appears to add a more straightforward claim of unlawful distribution of copyrighted works through illegal torrenting, which has long been considered established case-law.</p>
<p>Authors have <a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.415175/gov.uscourts.cand.415175.426.0.pdf">alleged</a> that "Meta deliberately engaged in one of the largest data piracy campaigns in history to acquire text data for its LLM training datasets, torrenting and sharing dozens of terabytes of pirated data that altogether contain many millions of copyrighted works." Separate from their copyright infringement claims opposing Meta's AI training on pirated copies of their books, authors alleged that Meta torrenting the dataset was "independently illegal" under California's Computer Data Access and Fraud Act (CDAFA), which allegedly "prevents the unauthorized taking of data, including copyrighted works."</p>
<p>Meta, however, is hoping to convince the court that torrenting is not in and of itself illegal, but is, rather, a "widely-used protocol to download large files." According to Meta, the decision to download the pirated books dataset from pirate libraries like LibGen and Z-Library was simply a move to access "data from a 'well-known online repository' that was publicly available via torrents."</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>To defend its torrenting, Meta has basically scrubbed the word "pirate" from the characterization of its activity. The company alleges that authors can't claim that Meta gained unauthorized access to their data under CDAFA. Instead, all they can claim is that "Meta allegedly accessed and downloaded datasets that Plaintiffs did not create, containing the text of published books that anyone can read in a public library, from public websites Plaintiffs do not operate or own."</p>
<p>While Meta may claim there's no evidence of seeding, there is some testimony that might be compelling to the court. Previously, a Meta executive in charge of project management, Michael Clark, had <a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.415175/gov.uscourts.cand.415175.417.8.pdf">testified</a> that Meta allegedly modified torrenting settings "so that the smallest amount of seeding possible could occur," which seems to support authors' claims that some seeding occurred. And an internal <a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.415175/gov.uscourts.cand.415175.417.9.pdf">message</a> from Meta researcher Frank Zhang appeared to show that Meta allegedly tried to conceal the seeding by not using Facebook servers while downloading the dataset to "avoid" the "risk" of anyone "tracing back the seeder/downloader" from Facebook servers. Once this information came to light, authors asked the court for a chance to depose Meta executives again, alleging that new facts "contradict prior deposition testimony."</p>
<h2>Torrenting terminology may confuse court</h2>
<p>How successful Meta's torrenting defense will be is still up in the air, but authors pointed out that even if Meta somehow managed to avoid seeding any of the torrented books, the social media giant still participated in an "online piracy ring." Further, in a footnote, the authors told the court that "IP pirates like Meta also upload or share files with others during (leeching) and after (seeding) downloading." Additionally, TorrentFreak <a href="https://torrentfreak.com/meta-says-it-made-sure-not-to-seed-any-pirated-books/">noted</a> that Meta "taking precautions is not the same as preventing" seeding.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>Authors will likely push to persuade the court that merely by torrenting the file, Meta made "pirated works available to other users worldwide" while making it clear that even Meta can't claim to have prevented all seeding. A lawyer representing the authors declined to comment on whether ongoing discovery may surface more evidence to help prove the seeding claims. Lack of evidence could be a problem since TorrentFreak suggested the torrenting terminology may be foreign to the court, potentially muddying what authors feel otherwise is a straightforward claim that Meta allegedly knew it was violating laws by torrenting the pirated books.</p>
<p>Meta has been silent so far on claims about sharing data while "leeching" (downloading) but told the court it plans to fight the seeding claims at summary judgment.</p>
<p>At this time, Meta has moved to dismiss the authors' CDAFA claim as being preempted by copyright law, but unsurprisingly, the authors told the court that they strongly disagree.</p>
<p>"Had Meta bought Plaintiffs’ works in a bookstore or borrowed them from a library and then trained its LLMs on them without a license, it would have committed copyright infringement, but no CDAFA violation," the authors alleged. "Meta’s decision to bypass lawful acquisition methods and become a knowing participant in an illegal peer-to-peer piracy network provides the 'extra element' and is 'qualitatively different' to establish an independent CDAFA violation."</p>
<p>Authors further linked the CDAFA claim to their copyright infringement claim opposing Meta's AI training. They alleged that by torrenting their works "from pirated databases in lieu of executing lawful licensing arrangements, Meta not only deprived Plaintiffs of that licensing revenue, but it also deprived Plaintiffs of additional revenue they could have generated from 'other users worldwide' because Meta simultaneously made the copyrighted works available to download by any interested Internet user in the process of acquiring Plaintiffs’ data" to train AI.</p>
<p>Meta did not immediately respond to Ars' request for comment.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/ashleybelanger/"><img src="https://arstechnica.com/wp-content/uploads/2022/06/Ashley-Belanger-400x400.jpg" alt="Photo of Ashley Belanger"></a></p>
  </div>

  <div>
    

    <p>
      Ashley is a senior policy reporter for Ars Technica, dedicated to tracking social impacts of emerging policies and new technologies. She is a Chicago-based journalist with 20 years of experience.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/tech-policy/2025/02/meta-defends-its-vast-book-torrenting-were-just-a-leech-no-proof-of-seeding/#comments" title="100 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    100 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/space/2025/02/elon-musk-recommends-that-the-international-space-station-be-deorbited-asap/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/iss056e201248large-768x432.jpg" alt="Listing image for first story in Most Read: Elon Musk recommends that the International Space Station be deorbited ASAP" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Every .gov Domain (285 pts)]]></title>
            <link>https://flatgithub.com/cisagov/dotgov-data/blob/main/?filename=current-full.csv&amp;sha=7dc7d24fba91f571692112d92b6a8fbe7aecbba2</link>
            <guid>43125829</guid>
            <pubDate>Fri, 21 Feb 2025 09:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flatgithub.com/cisagov/dotgov-data/blob/main/?filename=current-full.csv&#x26;sha=7dc7d24fba91f571692112d92b6a8fbe7aecbba2">https://flatgithub.com/cisagov/dotgov-data/blob/main/?filename=current-full.csv&#x26;sha=7dc7d24fba91f571692112d92b6a8fbe7aecbba2</a>, See on <a href="https://news.ycombinator.com/item?id=43125829">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Fly To Podman: a script that will help you to migrate from Docker (131 pts)]]></title>
            <link>https://github.com/Edu4rdSHL/fly-to-podman</link>
            <guid>43125487</guid>
            <pubDate>Fri, 21 Feb 2025 08:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Edu4rdSHL/fly-to-podman">https://github.com/Edu4rdSHL/fly-to-podman</a>, See on <a href="https://news.ycombinator.com/item?id=43125487">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">fly-to-podman</h2><a id="user-content-fly-to-podman" aria-label="Permalink: fly-to-podman" href="#fly-to-podman"></a></p>
<p dir="auto">Migrate from Docker to Podman.</p>
<p dir="auto">fly-to-podman is a small bash script that helps you migrate from Docker to Podman. It will migrate your Docker containers, images, and volumes to Podman, as well as keep your container data and configurations (mounts, ports, etc.) intact.</p>
<p dir="auto">Full blog post: <a href="https://www.edu4rdshl.dev/posts/from-docker-to-podman-full-migration-to-rootless/" rel="nofollow">From Docker to Podman: full migration to rootless</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What it does</h2><a id="user-content-what-it-does" aria-label="Permalink: What it does" href="#what-it-does"></a></p>
<ul dir="auto">
<li>Migrate Docker images to Podman (including tags)</li>
<li>Migrate Docker volumes to Podman (including all data)</li>
<li>Migrate Docker networks to Podman (including names, IPs, gateways, IP ranges, etc.)</li>
<li>Migrate Docker containers to Podman (including names, IDs, and statuses such as restart policy, etc.)</li>
<li>Keep container data and configurations (mounts, exposed ports, etc.)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Docker</li>
<li>Podman</li>
<li>bash</li>
<li>jq</li>
<li>rsync</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="fly-to-podman.sh {images|volumes|containers|full}
        images: Migrate Docker images to Podman
        volumes: Migrate Docker volumes to Podman
        containers: Migrate Docker containers to Podman
        networks: Migrate Docker networks to Podman
        full: Migrate Docker images, volumes, and containers to Podman"><pre>fly-to-podman.sh {images<span>|</span>volumes<span>|</span>containers<span>|</span>full}
        images: Migrate Docker images to Podman
        volumes: Migrate Docker volumes to Podman
        containers: Migrate Docker containers to Podman
        networks: Migrate Docker networks to Podman
        full: Migrate Docker images, volumes, and containers to Podman</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Issues and contributions</h2><a id="user-content-issues-and-contributions" aria-label="Permalink: Issues and contributions" href="#issues-and-contributions"></a></p>
<p dir="auto">If you find any issues or have any suggestions, please open an issue or a pull request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Train Your Own O1 Preview Model Within $450 (363 pts)]]></title>
            <link>https://sky.cs.berkeley.edu/project/sky-t1/</link>
            <guid>43125430</guid>
            <pubDate>Fri, 21 Feb 2025 08:42:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sky.cs.berkeley.edu/project/sky-t1/">https://sky.cs.berkeley.edu/project/sky-t1/</a>, See on <a href="https://news.ycombinator.com/item?id=43125430">Hacker News</a></p>
Couldn't get https://sky.cs.berkeley.edu/project/sky-t1/: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Docker limits unauthenticated pulls to 10/HR/IP from Docker Hub, from March 1 (314 pts)]]></title>
            <link>https://docs.docker.com/docker-hub/usage/</link>
            <guid>43125089</guid>
            <pubDate>Fri, 21 Feb 2025 07:42:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.docker.com/docker-hub/usage/">https://docs.docker.com/docker-hub/usage/</a>, See on <a href="https://news.ycombinator.com/item?id=43125089">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><hr><hr><blockquote><p><span><svg width="48" height="48" viewBox="0 -960 960 960"><path d="M483.18-280q12.82.0 21.32-8.63 8.5-8.62 8.5-21.37v-180q0-12.75-8.68-21.38-8.67-8.62-21.5-8.62-12.82.0-21.32 8.62-8.5 8.63-8.5 21.38v180q0 12.75 8.68 21.37 8.67 8.63 21.5 8.63zm-3.2-314q14.02.0 23.52-9.2T513-626q0-14.45-9.48-24.22-9.48-9.78-23.5-9.78t-23.52 9.78Q447-640.45 447-626q0 13.6 9.48 22.8 9.48 9.2 23.5 9.2zm.29 514q-82.74.0-155.5-31.5Q252-143 197.5-197.5t-86-127.34Q80-397.68 80-480.5t31.5-155.66T197.5-763t127.34-85.5Q397.68-880 480.5-880t155.66 31.5T763-763t85.5 127T880-480.27q0 82.74-31.5 155.5Q817-252 763-197.68q-54 54.31-127 86Q563-80 480.27-80z"></path></svg>
</span><strong>Note</strong></p><p>The Docker Hub plan limits will take effect on March 1, 2025. No charges on
Docker Hub pulls or storage will be incurred between December 10, 2024,
and February 28, 2025.</p></blockquote><p>When using Docker Hub, unauthenticated and Docker Personal users are subject to
strict limits. In contrast, Docker Pro, Team, and Business users benefit from a
consumption-based model with a base amount of included usage. This included
usage is not a hard limit; users can scale or upgrade their subscriptions to
receive additional usage or use on-demand usage.</p><p>The following table provides an overview of the included usage and limits for each
user type, subject to fair use:</p><div><table><thead><tr><th>User type</th><th>Pulls per month</th><th>Pull rate limit per hour</th><th>Public repositories</th><th>Public repository storage</th><th>Private repositories</th><th>Private repository storage</th></tr></thead><tbody><tr><td>Business (authenticated)</td><td>1M</td><td>Unlimited</td><td>Unlimited</td><td>Unlimited</td><td>Unlimited</td><td>Up to 500 GB</td></tr><tr><td>Team (authenticated)</td><td>100K</td><td>Unlimited</td><td>Unlimited</td><td>Unlimited</td><td>Unlimited</td><td>Up to 50 GB</td></tr><tr><td>Pro (authenticated)</td><td>25K</td><td>Unlimited</td><td>Unlimited</td><td>Unlimited</td><td>Unlimited</td><td>Up to 5 GB</td></tr><tr><td>Personal (authenticated)</td><td>Not applicable</td><td>40</td><td>Unlimited</td><td>Unlimited</td><td>Up to 1</td><td>Up to 2 GB</td></tr><tr><td>Unauthenticated users</td><td>Not applicable</td><td>10 per IPv4 address or IPv6 /64 subnet</td><td>Not applicable</td><td>Not applicable</td><td>Not applicable</td><td>Not applicable</td></tr></tbody></table></div><p>For more details, see the following:</p><ul><li><a href="https://docs.docker.com/docker-hub/usage/pulls/">Pull usage and limits</a></li><li><a href="https://docs.docker.com/docker-hub/usage/storage/">Storage usage and limits</a></li></ul><p>When utilizing the Docker Platform, users should be aware that excessive data
transfer, pull rates, or data storage can lead to throttling, or additional
charges. To ensure fair resource usage and maintain service quality, we reserve
the right to impose restrictions or apply additional charges to accounts
exhibiting excessive data and storage consumption.</p><h3 id="abuse-rate-limit"><a href="#abuse-rate-limit">Abuse rate limit</a></h3><p>Docker Hub has an abuse rate limit to protect the application and
infrastructure. This limit applies to all requests to Hub properties including
web pages, APIs, and image pulls. The limit is applied per IPv4 address or per
IPv6 /64 subnet, and while the limit changes over time depending on load and
other factors, it's in the order of thousands of requests per minute. The abuse
limit applies to all users equally regardless of account level.</p><p>You can differentiate between the pull rate limit and abuse rate limit by
looking at the error code. The abuse limit returns a simple <code>429 Too Many Requests</code> response. The pull limit returns a longer error message that includes
a link to documentation.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Judge invalidates blood glucose sensor patent, opens door for Apple Watch (409 pts)]]></title>
            <link>https://www.patentlyapple.com/2025/02/a-federal-judge-has-invalidated-an-omni-medsci-patent-which-could-open-the-door-for-a-blood-glucose-solution-for-apple-watch.html</link>
            <guid>43124436</guid>
            <pubDate>Fri, 21 Feb 2025 05:55:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.patentlyapple.com/2025/02/a-federal-judge-has-invalidated-an-omni-medsci-patent-which-could-open-the-door-for-a-blood-glucose-solution-for-apple-watch.html">https://www.patentlyapple.com/2025/02/a-federal-judge-has-invalidated-an-omni-medsci-patent-which-could-open-the-door-for-a-blood-glucose-solution-for-apple-watch.html</a>, See on <a href="https://news.ycombinator.com/item?id=43124436">Hacker News</a></p>
Couldn't get https://www.patentlyapple.com/2025/02/a-federal-judge-has-invalidated-an-omni-medsci-patent-which-could-open-the-door-for-a-blood-glucose-solution-for-apple-watch.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek Open Infra: Open-Sourcing 5 AI Repos in 5 Days (714 pts)]]></title>
            <link>https://github.com/deepseek-ai/open-infra-index</link>
            <guid>43124018</guid>
            <pubDate>Fri, 21 Feb 2025 04:24:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/open-infra-index">https://github.com/deepseek-ai/open-infra-index</a>, See on <a href="https://news.ycombinator.com/item?id=43124018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">


<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"><img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-Open-Infra"></a>
</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hello, DeepSeek Open Infra!</h2><a id="user-content-hello-deepseek-open-infra" aria-label="Permalink: Hello, DeepSeek Open Infra!" href="#hello-deepseek-open-infra"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">202502 Open-Source Week</h2><a id="user-content-202502-open-source-week" aria-label="Permalink: 202502 Open-Source Week" href="#202502-open-source-week"></a></p>
<p dir="auto">We're a tiny team @deepseek-ai pushing our limits in AGI exploration.</p>
<p dir="auto">Starting <strong>next week</strong>, we'll open-source 5 repos – one daily drop – not because we've made grand claims,
but simply as developers sharing our small-but-sincere progress with full transparency.</p>
<p dir="auto">These are humble building blocks of our online service: documented, deployed and battle-tested in production.
No vaporware, just code that moved our tiny moonshot forward.</p>
<p dir="auto">Why? Because every line shared becomes collective momentum that accelerates the journey.
Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation 🔧</p>
<p dir="auto">Stay tuned – let's geek out in the open together.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Day0: ???</h3><a id="user-content-day0-" aria-label="Permalink: Day0: ???" href="#day0-"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">2024 AI Infrastructure Paper (SC24)</h2><a id="user-content-2024-ai-infrastructure-paper-sc24" aria-label="Permalink: 2024 AI Infrastructure Paper (SC24)" href="#2024-ai-infrastructure-paper-sc24"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning</h3><a id="user-content-fire-flyer-ai-hpc-a-cost-effective-software-hardware-co-design-for-deep-learning" aria-label="Permalink: Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning" href="#fire-flyer-ai-hpc-a-cost-effective-software-hardware-co-design-for-deep-learning"></a></p>
<p dir="auto"><a href="https://dl.acm.org/doi/10.1109/SC41406.2024.00089" rel="nofollow"><b>📄 Paper Link</b></a>
<a href="https://arxiv.org/abs/2408.14158" rel="nofollow"><b>📄 Arxiv Paper Link</b></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Please Commit More Blatant Academic Fraud (2021) (121 pts)]]></title>
            <link>https://jacobbuckman.com/2021-05-29-please-commit-more-blatant-academic-fraud/</link>
            <guid>43123837</guid>
            <pubDate>Fri, 21 Feb 2025 03:53:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobbuckman.com/2021-05-29-please-commit-more-blatant-academic-fraud/">https://jacobbuckman.com/2021-05-29-please-commit-more-blatant-academic-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=43123837">Hacker News</a></p>
<div id="readability-page-1" class="page"><article role="main">
        <p>This week, I was thrilled to read about <a href="https://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext#FNA">the first well-documented case of explicit academic fraud in the artificial intelligence community</a>.
I hope that this is the beginning of a trend, and that other researchers will be inspired by their example and follow up by engaging in even more blatant forms of fraud in the future.</p>

<p>Explicit academic fraud is, of course, the natural extension of the sort of mundane, day-to-day fraud that most academics in our community commit on a regular basis.
Trying that shiny new algorithm out on a couple dozen seeds, and then only reporting the best few.
Running a big hyperparameter sweep on your proposed approach but using the defaults for the baseline.
Cherry-picking examples where your model looks good, or cherry-picking whole datasets to test on, where you’ve confirmed your model’s advantage.
Making up new problem settings, new datasets, new objectives in order to claim victory on an empty playing field.
Proclaiming that your work is a “promising first step” in your introduction, despite being fully aware that nobody will ever build on it.
Submitting a paper to a conference because it’s got a decent shot at acceptance and you don’t want the time you spent on it go to waste, even though you’ve since realized that the core ideas aren’t quite correct.</p>

<p>The problem with this sort of low-key fraud is that it’s insidious, it’s subtle.
In many ways, a fraudulent action is indistinguishable from a simple mistake.
There is plausible deniability – oh, I simply forgot to include those seeds, I didn’t have enough compute for those other ablations, I didn’t manage to catch that bug.
It’s difficult to bring ourselves to punish a well-meaning grad student for something that could plausibly have been a simple mistake, so we let these things slide, and slide and slide until they have become normalized.
When standards are low, it’s to no individual’s advantage to hold themselves to a higher bar.
Newcomers to the field see these things, they learn, and they imitate.
Often, they are directly encouraged by mentors.
A graduate student who publishes three papers a year is every professor’s dream, so strategies for maximal paper output become lab culture.
And when virtually every lab endorses certain behaviors, they become integral to the research standards of the field.</p>

<p>But worst of all: because everybody is complicit in this subtle fraud, nobody is willing to acknowledge its existence.
Who would be such a hypocrite as to condemn in others, behaviors they can see clearly in themselves?
And yet, who is willing to undermine their own achievements by admitting that their own work does not have scientific value?<sup id="fnref:0" role="doc-noteref"><a href="#fn:0" rel="footnote">1</a></sup>
The sad result is that, as a community, we have developed a collective blind-spot around a depressing reality: even at top conferences, the median published paper contains no truth or insight.
Any attempts to highlight or remedy the situation are met with harsh resistance from those who benefit from the current state of affairs.
The devil himself could not have designed a better impediment to humanity’s progression.</p>

<p>But now that blatant academic fraud is in the mix, the AI community has a fighting chance.
By partaking in a form of fraud that has left the Overton window of acceptability, the researchers in the collusion ring have finally succeeded in forcing the community to acknowledge its blind spot.
For the first time, researchers reading conference proceedings will be forced to wonder: does this work truly merit my attention?
Or is its publication simply the result of fraud?</p>

<p>It would, of course, be quite difficult to actually distinguish the papers published fraudlently from the those published “legitimately”.
(That fact alone tells you all you really need to know about the current state of AI research.)
But the mere <em>possibility</em> that any given paper was published through fraud forces people to engage more skeptically with <em>all</em> published work.
Readers are forced to act more like reviewers, weighing the evidence presented against their priors, attempting to identify ways in which surprising conclusions could be the result of fraud – explicit or subtle – rather than fact.
People will apply additional scrutiny to deal with explicit forms of fraud like collusion rings, but in doing so will also develop a sensitivity to the more subtle forms of fraud that are already endemic to the community.
This will, in turn, put pressure on authors to produce work which can withstand such scrutiny; results obtained without any fraud at all, leading to publications with genuine scientific merit.</p>

<p>That same harsh light is also cast on ourselves, on our motivations.
This situation seems to have evoked in many researchers feelings of empathy.
These actions are understandable; such an occurrence was inevitable; everyone does this, just more discreetly.
We are not surprised that people behave unscientifically in order to get their papers published; we are surprised that someone was willing to take it this far.
The most notable thing about the collusion ring is not that it was more fraudulent than is typical, but that the fraud was more <em>intentional</em>.</p>

<p>This surfaces the fundamental tension between good science and career progression buried deep at the heart of academia.
Most researchers are to some extent “career researchers”, motivated by the power and prestige that rewards those who excel in the academic system, rather than idealistic pursuit of scientific truth.
Well-respected senior figures are no exception to this. (In fact, due to selection effects, I suspect most are actually <em>more</em> career-motivated than is typical.)
We must come to terms with the fact that, since the incentives for career progression are not perfectly aligned with good science, almost any action motivated by career progression will interfere with one’s ability to do good science.
We must encourage a norm of introspection, of probing one’s own motivations, where any decisions made to obtain science-adjacent benefits are viewed with the deepest suspicion.
And we must ensure that explicit suggestions to modify one’s science in the service of one’s career – “you need to do X to be published”, “you need to publish Y to graduate”, “you need to avoid criticizing Z to get hired” – carry social penalties as severe as a suggestion of plagiarism or fraud.</p>

<hr>

<p>Prof. Littman says that collusion rings threaten the integrity of computer science research.
I agree with him.
<em>And I am looking forward to the day they make good on that threat.</em>
Undermining the credibility of computer science research is the best possible outcome for the field, since the institution in its current form does not deserve the credibility that it has.
Widespread fraud would force us to re-strengthen our community’s academic norms, transforming the way we do research, and improving our collective ability to progress humanity’s knowledge.</p>

<p>So this is a call to action: <strong>please commit more academic fraud</strong>.</p>

<p><em>Blatant</em> fraud.
<em>Aggressive</em> fraud.
Form more collusion rings!
Blackmail your reviewers, bribe your ACs!
Fudge your results – or fabricate them entirely!
(But don’t skimp on the writing: your paper needs to be written in perfect academic English, formatted nicely, and tell an intuitive, plausible-sounding story.)
Let’s make explicit academic fraud commonplace enough to cast doubt into the minds of every scientist reading an AI paper.
Overall, science will benefit.</p>

<hr>

<p>Together, we can force the community to reckon with its own shortcomings, and develop stronger, better, and more scientific norms.
It is a harsh treatment, to be sure – a chemotherapy regimen that risks destroying us entirely.
But this is our best shot at destroying the cancer that has infected our community.
I believe with all my heart that we can make it through this challenge and emerge stronger than ever.
And when we do, the secrets to artificial intelligence will be waiting.</p>

<hr>

<p>Thanks for reading. Follow me on <a href="https://jacobbuckman.substack.com/">Substack</a> for more writing, or hit me up on Twitter <a href="https://twitter.com/jacobmbuckman">@jacobmbuckman</a> with any feedback or questions!</p>

<p><em>Many thanks to Nitarshan Rajkumar for his feedback when writing this post.</em></p>

<hr>



      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Shape of a Mars Mission (108 pts)]]></title>
            <link>https://idlewords.com/2025/02/the_shape_of_a_mars_mission.htm</link>
            <guid>43123516</guid>
            <pubDate>Fri, 21 Feb 2025 03:01:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://idlewords.com/2025/02/the_shape_of_a_mars_mission.htm">https://idlewords.com/2025/02/the_shape_of_a_mars_mission.htm</a>, See on <a href="https://news.ycombinator.com/item?id=43123516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><b>02.20.<a href="https://idlewords.com/2025/">2025</a></b></p><p><a href="https://idlewords.com/2025/02/the_shape_of_a_mars_mission.htm">The Shape of a Mars Mission</a></p>
    <p>This post is the second in a series. Read <a href="https://idlewords.com/2023/1/why_not_mars.htm">part one here</a>.</p>


<p><i>“From a mathematics and trajectory standpoint and with a certain kind of technology, there’s not too many different ways to go to Mars.  It’s been pretty well figured out. You can adjust the decimal places here and there, but basically if you're talking about chemical rockets, there's a certain way you're going to go to Mars.” - John Aaron<a href="#aaron" name="fn_aaron"><sup>[1]</sup></a> </i>


</p><p>Unlike the Moon, which hangs in the sky like a lonely grandparent waiting for someone to visit, Mars leads a rich orbital life of its own and is not always around to entertain the itinerant astronaut. There is just one brief window every 26 months when travel between our two planets is feasible, and this constraint of orbital mechanics is so fundamental that we’ve known since Lindbergh crossed the Atlantic what a mission to Mars must look like.<a href="#early_mars" name="fn_early_mars"><sup>[2]</sup></a>

</p><p>Using chemical rockets, there are just two classes of mission to choose from: (The durations I give here can vary, but are representative).



</p><p><img src="https://idlewords.com/images/mars_mission_types.png">

</p><ul>
<li><b>Long Stay</b>: Spend six months flying to Mars, stay for 17 months, spend six months flying back  (~1000 days total).  This is sometimes called a conjunction class mission.  This profile trades a simple out-and-back trajectory for a long stay time at Mars. </li>

<li><b>Short Stay</b>: Spend six months flying to Mars, stay for 30-90 days, spend 400 days flying back (~650 days total).  This is also called an opposition class mission. This profile trades a short Martian stay time for a long and frankly terrifying trip home through the inner solar system.</li>
</ul>

<p>Before comparing the merits of each, it’s worth stressing what they have in common—both are <i>long</i>, more than double the absolute record for space flight (438 days), five times longer than anyone has remained in space without resupply (128 days), and about ten times humanity’s accumulated time beyond low Earth orbit (82 days).<a href="#durations" name="fn_durations"><sup>[3]</sup></a>  It is this inconvenient length, more than any technical obstacle, that has kept us from going to Mars since rockets capable of making the trip first became available in the 1960's. <a href="#first_rockets" name="fn_first_rockets"><sup>[4]</sup></a>

</p><p>And because this length is set by the relative motions of the planets, it’s resistant to attack by technology.   You can build rockets that go faster, but unless you make <i>Mars</i> go faster, you’ll mostly end up trading transit time for longer stay times.  Getting a round trip below the 500 day mark requires fundamental breakthroughs in either propulsion or refueling. <a href="#breakthroughs" name="fn_breakthroughs"><sup>[5]</sup></a>

</p><p><img src="https://idlewords.com/images/trades_delta_v.png">

</p><p>Delta-v requirements for short stay missions of varying length (left) and a long-stay mission (orange line right) for comparison. Note the sharp jump at around 500 days. <a href="https://ntrs.nasa.gov/api/citations/20150001240/downloads/20150001240.pdf">source</a>. 

</p><p>
That’s the bad news. The good news is that these constraints are so strong that we can say a lot about going to Mars without committing to any particular spacecraft or mission design.  Just like animals that live in the sea are likely to have good hearing and a streamlined body shape, there are things that have to hold true for any Mars-bound spacecraft, just from the nature of the problem.


<br>


</p><h2>I. No escape, no rescue</h2>

<p>A trip to Mars will be commital in a way that has no precedent in human space flight.  The moon landings were designed so that any moment the crew could hit the red button and return expeditiously to Earth; engineers spent the brief windows of time when an abort was infeasible chain smoking and chewing on their slide rules. <a href="#apollo_abort" name="fn_apollo_abort"><sup>[6]</sup></a>

</p><p>But within a few days of launch, a Mars-bound crew will have committed to spending years in space with no hope of resupply or rescue.  If something goes wrong, the only alternative to completing the mission will be to divert into a long, looping orbit that gets the spacecraft home about two years after departure.<a href="#mars_abort" name="fn_mars_abort"><sup>[7]</sup></a>   And if they get stuck on Mars, astronauts will find themselves in a similar position to the early Antarctic explorers, able to communicate home by radio, but forced by unalterable cycles of nature to wait months or years for a rescue ship. 

</p><p><img src="https://idlewords.com/images/mars_abort_types.png">

</p><p>Delta-v in km/sec required to return to Earth in 50, 70, and 90 days from various points in a long-stay Mars mission. Values above 10 km/sec are not realistic at our current technology level. <a href="https://dspace.mit.edu/handle/1721.1/43238?show=full">source</a></p>

<p>The effect of this no-abort condition is to make Mars mission design acutely risk-averse. You can think of flying to Mars like one of those art films where the director has to shoot the movie in a single take. Even if no scene is especially challenging, the requirement that everything go right sequentially, with no way to pause or reshoot, means that even small risks become unacceptable in the aggregate. 

</p><p>To get a feel for this effect, consider a toy model where we fly to Mars on a 30 month mission.  Every month there is a 3% chance that a critical system on our spacecraft will fail, and once that happens, the spacecraft enters a degraded state, with a 5% chance every month that a subsequent failure kills the crew. 

</p><p>In this model,  the probability that the crew gets home safely works out to 68%.  But if we add an abort option that can get them home in six months, that probability jumps to 85%.  And with a three month abort trajectory, the odds of safe return go up to 92%. 

</p><p>These odds are notional, but they demonstrate how big an effect the absence of abort options can have on safety.<a href="#abort_code" name="fn_abort_code"><sup>[8]</sup></a> 

</p><p>This necessary risk aversion introduces a tension into any Mars program. What’s the point of spending a trillion dollars to send a crew if they’re going to cower inside their spacecraft?  And yet since going outside is one of the most dangerous things you can do on Mars, early missions have to minimize it. The first visitors to Mars will have to land in the safest possible location and do almost nothing. 


</p><p>Risk is closely tied with the next issue, reliability.



</p><h2>II. Reliability</h2> 

<p>The closest thing humanity has built to a Mars-bound spacecraft is the International Space Station.  But ‘reliable’ is not the first word that leaps to the lips of ISS engineers when they talk about their creation—not even the first printable word. Despite twenty years of effort, equipment on the station breaks constantly, and depends on a stream of replacement parts flown up from Earth.<a href="#iss_failures" name="fn_iss_failures"><sup>[9]</sup></a> 

</p><p><img src="https://idlewords.com/images/heat_exchanger.jpg">

</p><p>A defective heat exchanger packaged for return to Earth from ISS in 2023</p>

<p>Going to Mars will require order of magnitude reliability improvements over the status quo. Systems on the spacecraft will need to work without breaking, or at least break in ways the crew can fix.  If there’s an emergency, like a chemical leak or a fire, the crew must be able to live for years in whatever’s left of the ship.  And the kind of glitches that made for funny stories in low Earth orbit (like a <a href="https://idlewords.com/urine_icicle.htm">urine icicle blocking the Space Shuttle toilet</a>) will be enough to kill a Mars-bound crew.

</p><p>Complicating matters is that traditional reliability engineering practices don’t work in life support, where everything is interconnected, often through the bodies of the crew.  Life support engineering is much more like keeping a marine aquarium than it is like building a rocket. It’s not easy to untangle cause from effect, the entire system evolves over time, and there’s a lot of “spooky action at a distance” between subsystems that were supposed to be unrelated.<a href="#urine_calcium" name="fn_urine_calcium"><sup>[10]</sup></a>  Indeed, failures in life support have a tendency to wander the spacecraft until they find the most irreplaceable thing to break.

</p><p>Nor is it possible to brute-force things by filling the spacecraft with spare parts. The same systemic interactions that damage one component can eat through any number of replacements. The bedrock axiom of reliability engineering—that complex designs can be partitioned into isolated subsystems with independent failure rates—does not hold for regenerative life support.

</p><p>The need for long and expensive test flights to validate life support introduces another kind of risk aversion, this time in the design phase.  With prototypes needing to be flown for years in space, there will be pressure to freeze the life support design at whatever point it becomes barely adequate, and no amount of later innovation will make it onto the spacecraft.  

</p><p>This is a similar dynamic to one that afflicted the Space Shuttle, a groundbreaking initial design so expensive to modify that it froze the underlying technology at the prototype phase for thirty years.  In that period we learned nothing about making better space planes, but burned through decades and billions of dollars patching up the first working prototype.
 
</p><p>Such timorousness goes against the grain of a development strategy that proven spectacularly successful in recent years for SpaceX, an approach you could call “fly often and try everything”.  With hardware to spare, SpaceX is not afraid to make wholesale changes between tests of its Starship rocket, relying on rapid iterations to advance the state of the art at an exhilarating pace.

</p><p>But this Yosemite Sam approach to testing won’t work for Mars. It only takes a few hours for engineers to collect the data they need after a Starship launch, while test runs of Mars-bound systems will last for years.  The inevitable outcome is a development program that looks an awful lot like NASA, with long periods of fussing and analysis punctuated by infrequent, hideously expensive test flights. 

</p><h2>III. Autonomy</h2>

<p>Autonomy is a concept alien to NASA, which has been micromanaging astronauts from the ground since the first Mercury astronaut had to <a href="https://idlewords.com/shepard_pee.htm">beg controllers for  permission to pee</a> (the request went all the way up the reporting chain to Wernher Von Braun).  

</p><p>To this day, missions follow a test pilot paradigm where the crew works from detailed checklists prepared for them months or years in advance.  On the space station, this takes the form of a graphical schedule creeping past a red vertical line on a laptop screen, with astronauts expected to keep pace with the moving colored boxes.  Most routine work on the space station (like pumping water or managing waste heat) is relegated to specialized teams on the ground and is not even visible to the crew. 

</p><p><img src="https://idlewords.com/images/shepard_in_space.jpg" width="600">

</p><p>Alan Shepard aboard Freedom 7, explaining that he really has to go pretty bad.</p> 

<p>But as a Mars-bound spacecraft gets further from Earth, the round-trip communications delay with ground control will build to a maximum of 43 minutes, culminating in a week or more of communications blackout when the Sun is directly between the two planets.  This physical constraint means that  the crew has to have full control over every system on the spacecraft, without help from the ground.

</p><p>Autonomy sounds like a good thing! Who wants government bean-counters deciding how astronauts spend their space time? But the ground-driven paradigm has its advantages, most notably in limiting workload. The ISS is run by a staff of hundreds who together send some 50,000 commands per day to the station. The seven astronauts on board are only called in as a last resort, and even so the demands on their time are so great that the station has struggled to perform its scientific mission.<a href="#iss_workload" name="fn_iss_workload"><sup>[11]</sup></a>

</p><p>One benefit of NASA’s backseat driving has always been that in an emergency, the crew has access to unlimited real-time expert help on Earth. The starkest illustration of this came on Apollo 13, when an oxygen tank in the service module ruptured 56 hours into the flight.  It took the crew and mission controllers nearly an hour to get their bearings, at which point there was only a short window of time left to power down the spacecraft in a way that would preserve their ability to return to Earth.  

</p><p>A <a href="https://idlewords.com/apollo_13_annotated.htm">transcript of that first hour</a> shows how difficult it was for crew and ground to figure out what was happening, and prioritize their response. It casts no aspersions on the crew of Apollo 13 to say they could not have survived a Mars-like communications delay.  

</p><p>And while this mission is the most famous example of ground controllers backstopping an Apollo crew, there were at least five more occasions in the Apollo program when timely help from the ground averted serious trouble:

</p><ul>
<li>Apollo 12 was hit twice by lightning after launch, scrambling the electrical system and lighting up the command module with warning lights. Flight controller John Aaron recognized the baffling error pattern and passed into NASA legend by telling the crew to flip an obscure switch that restored sanity to their displays.</li> 

<li>On Apollo 14, the descent radar on the lunar module failed to lock on properly, returning spurious range data. Without a timely call from ground control (who told the pilot to reset a breaker), the problem would likely have led to an aborted landing. </li>

<li>On Apollo 15, the crew struggled to contain a water leak that threatened to become serious. After fifteen minutes, engineers on the ground were able to trace the problem to a pre-launch incident with a chlorination valve and relay up a procedure that solved the problem.</li>

<li>Also on Apollo 15, a sliver of loose metal floating in a switch caused an intermittent abort signal to be sent to the lunar module engine.  Suppressing the signal so the lunar module could descend safely required reprogramming the onboard computer in a procedure guaranteed to raise the hairs on the head of every modern software developer.</li>

<li>On Apollo 16, a pair of servo motors on the service module failed in lunar orbit. Mission rules called for an abort, but after some interactive debugging with the command module pilot, ground controllers found a workaround they judged safe enough to continue with the landing.</li>
</ul>

<p>While these incidents stand out, Apollo transcripts reveal numberless other examples of crew and ground working closely to get on top of problems. The loss of this real-time help is a real risk magnifier for astronauts going to Mars. 

</p><h2>IV. Analysis</h2>

<p>Another way in which the ISS depends on Earth is for laboratory analysis of air and water samples, which are collected on a regular schedule and sent down with each returning capsule. The tests that can be performed on the station itself are rudimentary, alerting crew to the presence of microbes or contaminants, but without the detailed information necessary to trace a root cause.

</p><p>For Mars, this analytic capability will have to move into the spacecraft. In essence, this means building a kind of Space Theranos, an automated black box that can perform biochemical assays in space without requiring repair or calibration. Such an instrument doesn’t exist anywhere, but a Mars mission requires two flavors of it—one that works in zero G, and another for Martian gravity.<a href="#theranos" name="fn_theranos"><sup>[12]</sup></a> 

</p><p>This black box belongs to a category of hardware that pops up a lot in Mars plans: technologies that would be multibillion dollar industries if they  existed on Earth, but are assumed to be easy enough to invent when the time comes to put them on a Mars-bound spacecraft.  <a href="#magic_tech" name="fn_magic_tech"><sup>[13]</sup></a>

</p><p>Some Mars boosters even cite these technologies as examples of the benefits going to Mars will bring to humanity.  But this gets things exactly backwards—problems that are hard on Earth don’t get easier by firing them into space, and the fact that nonexistent technologies are on the critical path to Mars is not an argument for going there.

</p><h2>V. Automation </h2>

<p>The requirement that the crew be able to handle the ship when some members are incapacitated and there is no communication with Earth means that an ISS-size workload has to be automated to the point where it can be run by two or three astronauts. 

</p><p><img src="https://idlewords.com/images/cimon_gerst.jpg">

</p><p>Astronaut Alexander Gerst (right) interacting with CIMON, NASA's $6 million AI chatbot</p>

<p>Automation means software, and lots of it.  To automate the systems on a Mars-bound spacecraft will be a monumental task, like trying to extend the autopilot on an airliner to make it run the airport concession stands, baggage claim, and airline pension plan.  The likely outcome is an ISS-like hotchpotch of software tested to different levels of rigor, running across hundreds of processors. But this hardware will be exposed to a far harsher radiation environment than systems on the ISS, making software design and integration a particular challenge.

</p><p>A special case of the automation problem comes up on long-stay missions, when  the orbiting spacecraft has to keep itself free of mold, fungus, and space raccoons for the year and a half that the crew are on the Martian surface. Anyone who owns a vacation home knows that this problem—called “quiescence” in the Mars literature—is already hard to solve on Earth. 

</p><p>Unless carefully managed, the interplay between automation, complexity and reliability can enter a pathological spiral.   Adding software to a system makes it more complex. To stay reliable, complex systems have to degrade gracefully, so that the whole continues to function even if an individual component fails. But these degraded modes, as well as unexpected interactions between them, introduce their own complexity, which then has to be managed with software, and so on. 

</p><p>The upshot is that automation introduces its own, separate reason for running full-length mock missions before actually going to Mars. There will be too many bugs in a system this complex to leave them all for the first Mars-bound crew to discover.

</p><h2>Implications </h2>

<p>The extreme requirements for autonomy, reliability, and automation I’ve outlined are old news to designers of deep-space probes. The solar system is full of hardware beeping serenely away decades after launch, most spectacularly the forty-six-year-old Voyager spacecraft. 

</p><p>But  no one has ever tried attaching a box of large primates to a deep space probe with the goal of keeping them alive, happy, and not tweeting about how NASA sent them into the vast empty spaces to die.  A Mars-bound spacecraft will be the most complicated human artifact ever built, about a hundred times bigger than any previous space probe, and inside it will be a tightly-coupled system of software, hardware, bacteria, fungi, astronauts, and (for half the mission) whatever stuff the crew tracks with them back onto the spacecraft.

</p><p>Designing such a machine means taking something at the ragged edge of human ability (building interplanetary probes) and combining it with something that we can’t even do yet on Earth (keep a group of six or eight humans alive for years with regenerative life support).<a href="#biosphere2" name="fn_biosphere2"><sup>[14]</sup></a>  

</p><p>My argument is not that it is impossible to do this, but that it is impossible to do it quickly.  Preparing for Mars will be an iterative, open-ended undertaking in which every round of testing eats up years of time and most of our space budget, like Artemis and the ISS before it.  The first decade of a Mars program will be indistinguishable from the last forty years of space flight—a series of repetitive, long-duration missions to orbit. The only thing NASA will need to change is the program name. 

</p><p>Nor is this a problem that can be delegated to billionaire hobbyists. Life support is going to be a grind no matter whose logo is on the rocket.  The sky could be thick with Starships and we’d still be stuck doing all-up trials of hardware and software on these multi-year missions to nowhere.  

</p><p>The only way to explore Mars in our lifetime is to ditch the requirement that people accompany the machinery.

</p><h2>Choosing a profile</h2> 

<p>But since we’re determined to go to Mars, and have two profiles to choose from, which one is better? 

</p><p>Everyone agrees that only the long-stay profile makes sense for exploration. There’s no point in spending 95% of the trip in transit just to get a rushed couple of weeks at the destination. 

</p><p>But on early missions, where the goal is just to get the crew home alive, the choice is tricky.

</p><p><img src="https://idlewords.com/images/long_stay.png">


</p><h2>Long Stay</h2>

<p>The virtue of the long stay profile is simplicity. You fly your rocket to Mars, wait 17 months for the planets to align, and then fly the same trajectory home.  Each leg of this transfer journey lasts about as long as an ISS deployment, and it’s possible to tweak the transfer time by burning more fuel (although the crew then has to stay longer on Mars to compensate). 

</p><p>At every point in the mission, the ship remains between 1 AU and 1.5 AU from the Sun. This simplifies thermal and solar panel design and greatly reduces the risk to the crew from solar storms.  

</p><p>But the problem of what to do with all that time on Mars is vexing.  500 days is a long time for a first stay anywhere, even someplace with nightlife and an atmosphere.   And as we’ll see, an orbital mission is probably out of the question.  The requirement that the crew go live on Mars on their first visit adds enormously to the level of risk. 

</p><p><img src="https://idlewords.com/images/short_stay.png">

</p><h2>Short Stay</h2>

<p>The appeal of the short stay profile is right in the name. Instead of staying on Mars so long they have to file taxes, the first arrivals can plant the flag, grab whatever rock is nearest the ladder, and get the hell out of there.  Or they can choose to skip the landing and make the first trip strictly orbital, following a tradition in aerospace engineering of attempting the impossible sequentially instead of all at once.

</p><p>But the problem with the short stay profile is that trip home. The return trajectory cuts well inside the orbit of Venus, complicating the design of the spacecraft and adding spectacular ways for the crew to die during the weeks near perihelion. For most of that journey, the ship is on the wrong side of the Sun, hampering communications with Earth while leaving the crew with no warning of solar storms.

</p><p>And that crew has to spend two consecutive years in deep space, maximizing their exposure to radiation and microgravity, the biggest known risks to astronaut health. 

</p><p>The short stay profile also requires more propellant, in some years a prohibitive amount. If your strategy for mitigating risk on Mars is to launch crews during every synodic period, so that there are always potential rescuers en route to Mars, then this is a problem.  
￼
</p><p><img src="https://idlewords.com/images/delta_v_graph.png" width="600">

</p><p>A diagram comparing the delta-v requirements for short stay and long stay missions across future launch dates. Since propellant requirements go up exponentially with delta v, a mission in 2041 requires five times as much propellant as one in 2033. <a href="https://ntrs.nasa.gov/api/citations/20130010385/downloads/20130010385.pdf">source</a>“
</p>

<h2>Orbit or Land?</h2> 

<p>Once you’ve picked a profile, the other decision to make is whether to land the spacecraft. 

</p><p>Obviously you have to land a crew at some point; if you don’t, the other space programs will make fun of you, and there will be hurtful zingers at your Congressional hearing. 

</p><p>But since surviving a trip to Mars requires tackling a sequence of unrelated problems (arrival, entry, landing, surface operations, ascent, rendezvous), there is a case for cutting the problem in half by making the first mission orbital. This was the approach taken by the Apollo program, which looped the first crew around the Moon before a working lunar lander existed.

</p><p>Not having to carry a lander on the first mission means more room for spare parts and consumables, which improves the margin of safety for the crew.  It also buys time for engineers to work on the hard problems of entry, landing, quiescence, and ascent without holding back the entire program. 

</p><p>But there are powerful arguments against an orbital mission. Since so much of the risk in going to Mars is a simple function of time, why roll the dice more than necessary? And given the expense and physical toll on crew,  how do you justify not attempting a landing?  Imagine driving to Disneyland, turning the car around in the parking lot, and announcing to your family that you’re now ready for the real trip next year.  There will be angry kicking from the backseat, and mutiny.  
 
</p><p>NASA has waffled for years over which option to choose.  In the 2009 design reference architecture, they favored sending a crew of four on the long stay trajectory.  Their more recent plans envision a shoestring mission on a short-stay profile with four crew members, two of whom attempt a landing. 

</p><p>Elon Musk, for his part, has proposed solving the problem in stages, sending volunteers to settle Mars first, then figuring out how to get them home later.<a href="#spacex_plan" name="fn_spacex_plan"><sup>[15]</sup></a>   

</p><p>What makes the choice genuinely hard is that we lack answers to two key questions:

</p><p><b>1. How does the human body respond to partial gravity?</b>

</p><p>Decades in space have given us a good idea of what prolonged periods in free-fall do to astronauts, and how they recover after returning to Earth.   But we have no idea what happens in partial gravity, either on the Moon (0.16 g) or on Mars (0.38 g). In particular, we don’t know whether Martian gravity is strong enough to arrest or slow the degenerative processes that we observe in free fall.<a href="#deconditioning" name="fn_deconditioning"><sup>[16]</sup></a>  

</p><p>The answer to this question will drive a key decision: whether or not to spin the spacecraft. As we’ll see, spinning a spacecraft to create artificial gravity is an enormous hassle, but whether it’s avoidable depends on the unstudied effects of long stays in partial gravity.<a href="#partial_g" name="fn_partial_g"><sup>[17]</sup></a> 

</p><p><b>2. What is the risk to the crew from the heavy-ion component of galactic cosmic radiation?</b>

</p><p>Radiation in space comes in many varieties, most of which are well-understood from experience with their analogues on Earth.  

</p><p>Low-dose heavy-ion radiation, however, is different.  It doesn’t exist outside of particle accelerators on Earth and is hard to study in low orbit, where both the magnetosphere and the bulk of our planet shield astronauts from most of the flux they’d experience in free space.  

</p><p>Heavy ion radiation has biological effects that are not captured by the standard model of radiation damage to tissue. In particular, there is a class of phenomena called non-targeted effects (NTEs) that are known to damage cells far from the radiation track. This is a weird effect, like if found yourself hospitalized because your neighbor got hit by a car.  It’s believed that NTEs disrupt epigenetic signaling mechanisms in cells, but the phenomenon is poorly understood.

</p><p>Uncertainty about the effects of low-dose heavy ion radiation widens our best guess at radiation risk by at least a factor of two.<a href="#radiation_uncertainty"><sup>[18]</sup></a> At the low end of the range, these effects are just a curiosity, and Mars missions can be planned using traditional models of radiation exposure. At the high end of the range, long-duration orbital missions may not be survivable, and astronauts on the Martian surface will either have to live in a cave or cover their shelter with meters of soil.

</p><p><img src="https://idlewords.com/images/nte_uncertainty.png" width="600">


</p><p>Prediction of tumor prevalence after 1 year of galactic cosmic radiation exposure. The solid line at bottom shows the standard radiation model (TE). The dotted lines show the influence of non-targeted effects (NTE) under different assumptions. Note the nearly threefold uncertainty in predicted tumor prevalence in the unshielded case. <a href="https://pubmed.ncbi.nlm.nih.gov/28500351/">source</a>

</p><p>This uncertainty about biological effects makes radiation the greatest uncharacterized known risk facing a Mars-bound crew, and it affects every aspect of mission design. 

</p><p>It’s helpful to combine the three main risk factors in going to Mars into one big chart:

￼
<table>
<caption>Technical Risk</caption>

<tbody><tr><th></th><th>Orbit</th><th>Land</th></tr>
<tr><td>Short Stay</td>
<td>Spacecraft trajectory complicates spacecraft design, communications are a challenge.</td>
<td>Requires working lander and ascent stage, less margin than orbital mission.
</td></tr>
<tr><td>Long Stay</td>
<td>Lowest complexity, large mass budget for spares and consumables. </td>
<td>Highest complexity, all-up mission must work on the first try.</td></tr>
</tbody></table>


<table>
<caption>Radiation Risk</caption>
<tbody><tr><th></th><th>Orbit</th><th>Land</th></tr>
<tr><td>Short Stay</td>
<td colspan="2">600 days in deep space, return trip requires close solar approach (0.7 AU).  Risk from solar particle events may require flying near solar minimum, incurring higher GCR dose.

</td></tr>
<tr><td>Long Stay</td>
<td>Risk of death or incapacitation from heavy ion component of GCR may exceed 50%
</td>
<td>Lowest radiation exposure, but adequately shielding the habitat on Mars increases complexity and contamination risk 
</td></tr>
</tbody></table>


<table>
<caption>Deconditioning Risk</caption>
<tbody><tr><th></th><th>Orbit</th><th>Land</th></tr>
<tr><td>Short Stay</td>
<td colspan="2">1.5 times beyond human endurance record; crew at risk for bone fractures and eye damage.


</td></tr>
<tr><td>Long Stay</td>
<td>2.5 times beyond human endurance record.
</td>
<td>Physiological effects of partial gravity unknown. 

</td></tr>
</tbody></table>

</p><p>The gray areas in these grids represent knowledge gaps that have to be filled before we decide how to go to Mars.


</p><p>How long this preliminary medical research would take is anyone’s guess, but it has to be some multiple of the total mission time.  Studying partial gravity in particular is tricky—you can do it on the Moon (42% of martian gravity) and hope the results extend to Mars, or you can build rotating structures in space and do more precise tests there.

</p><p>Studying radiation effects means flying animals outside the magnetosphere for a few years and then watching them for tumors, which (unless the radiation news is really bad) is also going to take some time.

</p><p>In software engineering we have a useful concept called “yak shaving”. To get started on a project you must first prepare your tools, which often involves reconfiguring your programming environment, which may mean updating software, which requires finding a long-disused password, and pretty soon you find yourself under the office chair with a hex wrench. (The TV show Malcolm in the Middle has <a href="https://www.youtube.com/watch?v=AbSehcT19u0">a beautiful illustration</a> of yak shaving in the context of home repair.) 

</p><p>The same phenomenon afflicts us in trying to go to Mars. It would be one thing if, given enough rockets and money, explorers could climb on a spaceship and go. But there is always this chain of necessary prerequisites. We paint <i>Destination: Mars!</i> on the side of our spaceship and then find ourselves in low Earth orbit a decade later, centrifuging mice. It’s dispiriting.


</p><p>It’s tempting to say “you can just build things” and dismiss all this research and testing as timid and unnecessary. But this would mean ignoring the biggest risk factor for Mars, which I’ll include here for the sake of completeness.

<table>
<caption>Unknown Risks</caption>
<tbody><tr><th></th><th>Orbit</th><th>Land</th></tr>


<tr><td>Short Stay</td>
<td>Unknown
</td>
<td>Unknown

</td></tr>
<tr><td>Long Stay</td>
<td>Unknown
</td>
<td>Unknown 

</td></tr>
</tbody></table>

</p><p>A trip to Mars is so difficult that we don’t have the luxury of ignoring known risks—we need all the room we can spare in our risk budget for the things we don’t know to worry about yet.

</p><p>My goal in all this is not to kill a cherished dream, but to try to push people to a more realistic view of what it means to commit to a Mars landing, and in particular to think about going to Mars in terms of opportunity costs.

</p><p>In recent years, there’s been a remarkable division in space exploration.  On one side of the divide are missions like Curiosity, James Webb, Gaia, or Euclid that are making new discoveries by the day. These projects have clearly defined goals and a formidable record of discovery. 

</p><p>On the other side, there is the International Space Station and the now twenty-year old effort to return Americans to the moon. These projects have no purpose other than perpetuating a human presence in space, and they eat through half the country’s space budget with nothing to show for it.  Forget even Mars—we are further from landing on the Moon today than we were in 1965.

</p><p>In going to Mars, we have a choice about which side of this ledger to be on. We can go aggressively explore the planet with robots, benefiting from an ongoing revolution in automation and software to launch ever more capable missions to the places most likely to harbor life.  

</p><p>Or we can stay on the treadmill we’ve been on for forty years,  slowly building up the capacity to land human beings on the safest possible piece of Martian real estate, where they will leave behind a plaque and a flag.  But we can’t do both.

</p><p>Next time: Eyes and Bones

</p><hr>

<h2>Footnotes</h2>

<p><a name="aaron" href="#fn_aaron">[1]</a> 
 Quote taken from a 2000 <a href="https://historycollection.jsc.nasa.gov/JSCHistoryPortal/history/oral_histories/AaronJW/AaronJW_1-21-00.htm">oral history</a> with Aaron. 

</p><p><a name="early_mars" href="#fn_early_mars">[2]</a> For an early example, see the 1928 Scientific American article, “<a href="https://idlewords.com/can_we_go_to_mars.htm">Can we go to Mars?</a>”, While understandably hand-wavy about the means of propulsion, it describes a conjunction-class orbital mission not substantially different from NASA’s 2009 Design Reference Architecture.

</p><p><a name="durations" href="#fn_durations">[3]</a> Valerii Polyakov set the 437 day record on a space flight that landed in 1995. The International Space Station went without resupply from Nov 25, 2002 to April 2, 2003. Nine Apollo missions went beyond low Earth orbit, the longest of these (Apollo 17) was gone 12.4 days.

</p><p><a name="first_rockets" href="#fn_first_rockets">[4]</a> The Saturn V was capable of launching about 20 tons on a Mars flyby trajectory. NASA <a href="http://www.astronautix.com/j/jagmarsflyby1966.html">undertook preliminary planning</a> for such a mission (requiring four Saturn V launches) in 1967.  

</p><p><a name="breakthroughs" href="#fn_breakthroughs">[5]</a> In 1987 a team chaired by Sally Ride proposed <a href="https://archive.is/gmZoN">a ‘split/sprint’ mission architecture</a> that is probably the best way to get to Mars. In this architecture, slow-moving tankers pre-position cryogenic propellant depots in Mars orbit, and then in the next synodic period a human mission (the “sprint” part of the mission) lands briefly on Mars, refuels from the orbiting depots, and get home within 400 days.  Such a mission requires about 15 heavy launches and two nonexistent technologies: long-term storage of liquid hydrogen in space, and the ability to pump liquid hydrogen between spacecraft in space.  (Interestingly, both of these technologies are part of Blue Origin's plan to build a moon lander). 

</p><p>The other way to get to Mars fast is with nuclear thermal rockets. A nuclear thermal rocket is just a nuclear reactor that shoots hot hydrogen out one end. Nuclear thermal rocket designs are about twice as efficient as chemical rockets, making it feasible to fly missions with higher delta V requirements.


</p><p><a name="apollo_abort" href="#fn_apollo_abort">[6]</a> For a comprehensive discussion of Apollo abort modes, see <a href="https://ntrs.nasa.gov/api/citations/19720017278/downloads/19720017278.pdf">1972 Apollo Experience Report - Abort Planning</a>.

</p><p><a name="mars_abort" href="#fn_mars_abort">[7]</a> You can read about possible Mars abort modes in <a href="https://www.researchgate.net/publication/332351285_EARTH_TO_MARS_ABORT_ANALYSIS_FOR_HUMAN_MARS_MISSIONS">Earth to mars Abort Analysis for Human Mars Missions</a>. What kind of a failure scenario would even benefit from a two-year abort option is an interesting philosophical question. 

</p><p><a name="abort_code" href="#fn_abort_code">[8]</a> I wrote a little <a href="https://idlewords.com/mars_abort.py">python script</a> if you want to play with these scenarios yourself.

</p><p><a name="iss_failures" href="#fn_iss_failures">[9]</a> Life support equipment on ISS is packaged into components called ‘Orbital Replacement Units’. In some cases, this means that an assembly weighing hundreds of kilograms has to be flown up because a tiny sensor within it failed. 

</p><p>Here's a partial list of ORUs replaced in calendar year 2023 (<a href="https://ntrs.nasa.gov/api/citations/20240005249/downloads/ICES-2024-141%20ECLS%20Overview%20of%20Events%202023%20Final.pdf">source</a>):

</p><ul>
<li>Heat exchanger in Node 3</li>
<li>Common cabin air assembly water separator</li>
<li>Node 3 water separator</li>
<li>Common cabin air assembly water separator liquid check valve</li>
<li>21 charcoal filters stationwide</li>
<li>HEPA filters in Node 3</li>
<li>Blower in carbon dioxide removal assembly (twice, first replacement failed)</li>
<li>Sample Distribution Assembly in Node 3</li>
<li>Mass Spectrometer assembly</li>
<li>Multifiltration bed</li>
<li>Pump in oxygen generation assembly</li>
</ul>

<p><a name="urine_calcium" href="#fn_urine_calcium">[10]</a> An early urine reprocessor on the space station <a href="https://ntrs.nasa.gov/citations/20170008985">failed after it got clogged up by calcium crystals</a> from the astronauts' dissolving bones, an effect of weightlessness that wasn't properly accounted for in the design. 

</p><p><a name="iss_workload" href="#fn_iss_workload">[11]</a> The 50,000 command figure is from <a href="https://www.nasa.gov/ebooks/the-international-space-station-operating-an-outpost-in-the-new-frontier/">The ISS: Operating an Outpost in the New Frontier</a>, a detailed primer on space station operations. ISS utilization has gone up in recent years, but still remains below 80 hours/week—two full-time equivalents. The seven-member crew spends most of their waking time on mandatory exercise, housekeeping, and station repair. 

</p><p><a name="theranos" href="#fn_theranos">[12]</a> Existing instruments in space are usually set up to identify chemicals on a target list of 10-20 substances, a much easier task than identifying arbitrary compounds. 
</p><p>For the state of the art on the latter, see <a href="https://ntrs.nasa.gov/api/citations/20230002433/downloads/ICES_Paper-SWIM_Draft-2023-03-02_no_copyright.pdf">Progress on the Organic and Inorganic Modules of the Spacecraft Water Impurity Monitor, a Next Generation Complete Water Analysis System for Crewed Vehicles</a> (ICES-2023-110).

</p><p><a name="magic_tech" href="#fn_magic_tech">[13]</a> Other examples of magic Mars technology include leakless seals for spacesuits, waterless washing machines, biofilm-proof coatings, nutritionally complete meals that can be stored for years at room temperature, and autonomous solar-powered factories for turning CO2 into hundreds of tons of methane.

</p><p><a name="biosphere2" href="#fn_biosphere2">[14]</a> The endurance record for closed-system life support belongs to Biosphere 2, which kept a crew alive for 17 months before oxygen fell to dangerous levels because of unanticipated interactions with building materials. 

</p><p><a name="spacex_plan" href="#fn_spacex_plan">[15]</a> Plans involving Starship and Mars depend on being able to produce hundreds of tons of propellant on the Martian surface so the rockets can launch again. In the absence of any details from Musk or SpaceX, the closest thing we have to a detailed plan is <a href="https://www.nature.com/articles/s41598-024-54012-0">this analysis in Nature</a>.

</p><p><a name="deconditioning" href="#fn_deconditioning">[16]</a> For all we know, the set of problems collectively called "deconditioning" could get worse in partial gravity. This goes against our intuitions, but there have been bigger surprises in space.

</p><p><a name="partial_g" href="#fn_partial_g">[17]</a> Another decision that hinges on the effects of partial gravity is whether or not to include heavy exercise equipment on the Mars surface habitat, where space and mass are at a premium.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software engineering job openings hit five-year low? (224 pts)]]></title>
            <link>https://blog.pragmaticengineer.com/software-engineer-jobs-five-year-low/</link>
            <guid>43122871</guid>
            <pubDate>Fri, 21 Feb 2025 01:11:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pragmaticengineer.com/software-engineer-jobs-five-year-low/">https://blog.pragmaticengineer.com/software-engineer-jobs-five-year-low/</a>, See on <a href="https://news.ycombinator.com/item?id=43122871">Hacker News</a></p>
Couldn't get https://blog.pragmaticengineer.com/software-engineer-jobs-five-year-low/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>